//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-35583870
// Cuda compilation tools, release 12.8, V12.8.93
// Based on NVVM 7.0.1
//

.version 8.7
.target compute_75
.address_size 64

	// .globl	unary_neg_f32
.global .align 4 .b8 __cudart_i2opi_f[24] = {65, 144, 67, 60, 153, 149, 98, 219, 192, 221, 52, 245, 209, 87, 39, 252, 41, 21, 68, 78, 110, 131, 249, 162};

.visible .entry unary_neg_f32(
	.param .u64 unary_neg_f32_param_0,
	.param .u64 unary_neg_f32_param_1,
	.param .u32 unary_neg_f32_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<3>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<8>;


	ld.param.u64 	%rd1, [unary_neg_f32_param_0];
	ld.param.u64 	%rd2, [unary_neg_f32_param_1];
	ld.param.u32 	%r2, [unary_neg_f32_param_2];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB0_2;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.s32 	%rd4, %r1, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f32 	%f1, [%rd5];
	neg.f32 	%f2, %f1;
	cvta.to.global.u64 	%rd6, %rd2;
	add.s64 	%rd7, %rd6, %rd4;
	st.global.f32 	[%rd7], %f2;

$L__BB0_2:
	ret;

}
	// .globl	unary_neg_f16
.visible .entry unary_neg_f16(
	.param .u64 unary_neg_f16_param_0,
	.param .u64 unary_neg_f16_param_1,
	.param .u32 unary_neg_f16_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .b16 	%rs<3>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<8>;


	ld.param.u64 	%rd1, [unary_neg_f16_param_0];
	ld.param.u64 	%rd2, [unary_neg_f16_param_1];
	ld.param.u32 	%r2, [unary_neg_f16_param_2];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB1_2;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.s32 	%rd4, %r1, 2;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u16 	%rs2, [%rd5];
	// begin inline asm
	{neg.f16 %rs1,%rs2;
}
	// end inline asm
	cvta.to.global.u64 	%rd6, %rd2;
	add.s64 	%rd7, %rd6, %rd4;
	st.global.u16 	[%rd7], %rs1;

$L__BB1_2:
	ret;

}
	// .globl	unary_abs_f32
.visible .entry unary_abs_f32(
	.param .u64 unary_abs_f32_param_0,
	.param .u64 unary_abs_f32_param_1,
	.param .u32 unary_abs_f32_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<3>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<8>;


	ld.param.u64 	%rd1, [unary_abs_f32_param_0];
	ld.param.u64 	%rd2, [unary_abs_f32_param_1];
	ld.param.u32 	%r2, [unary_abs_f32_param_2];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB2_2;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.s32 	%rd4, %r1, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f32 	%f1, [%rd5];
	abs.f32 	%f2, %f1;
	cvta.to.global.u64 	%rd6, %rd2;
	add.s64 	%rd7, %rd6, %rd4;
	st.global.f32 	[%rd7], %f2;

$L__BB2_2:
	ret;

}
	// .globl	unary_abs_f16
.visible .entry unary_abs_f16(
	.param .u64 unary_abs_f16_param_0,
	.param .u64 unary_abs_f16_param_1,
	.param .u32 unary_abs_f16_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .b16 	%rs<3>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<8>;


	ld.param.u64 	%rd1, [unary_abs_f16_param_0];
	ld.param.u64 	%rd2, [unary_abs_f16_param_1];
	ld.param.u32 	%r2, [unary_abs_f16_param_2];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB3_2;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.s32 	%rd4, %r1, 2;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u16 	%rs2, [%rd5];
	// begin inline asm
	{abs.f16 %rs1,%rs2;
}
	// end inline asm
	cvta.to.global.u64 	%rd6, %rd2;
	add.s64 	%rd7, %rd6, %rd4;
	st.global.u16 	[%rd7], %rs1;

$L__BB3_2:
	ret;

}
	// .globl	unary_sqr_f32
.visible .entry unary_sqr_f32(
	.param .u64 unary_sqr_f32_param_0,
	.param .u64 unary_sqr_f32_param_1,
	.param .u32 unary_sqr_f32_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<3>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<8>;


	ld.param.u64 	%rd1, [unary_sqr_f32_param_0];
	ld.param.u64 	%rd2, [unary_sqr_f32_param_1];
	ld.param.u32 	%r2, [unary_sqr_f32_param_2];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB4_2;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.s32 	%rd4, %r1, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f32 	%f1, [%rd5];
	mul.f32 	%f2, %f1, %f1;
	cvta.to.global.u64 	%rd6, %rd2;
	add.s64 	%rd7, %rd6, %rd4;
	st.global.f32 	[%rd7], %f2;

$L__BB4_2:
	ret;

}
	// .globl	unary_sqr_f16
.visible .entry unary_sqr_f16(
	.param .u64 unary_sqr_f16_param_0,
	.param .u64 unary_sqr_f16_param_1,
	.param .u32 unary_sqr_f16_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .b16 	%rs<4>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<8>;


	ld.param.u64 	%rd1, [unary_sqr_f16_param_0];
	ld.param.u64 	%rd2, [unary_sqr_f16_param_1];
	ld.param.u32 	%r2, [unary_sqr_f16_param_2];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB5_2;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.s32 	%rd4, %r1, 2;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u16 	%rs2, [%rd5];
	// begin inline asm
	{mul.f16 %rs1,%rs2,%rs2;
}
	// end inline asm
	cvta.to.global.u64 	%rd6, %rd2;
	add.s64 	%rd7, %rd6, %rd4;
	st.global.u16 	[%rd7], %rs1;

$L__BB5_2:
	ret;

}
	// .globl	unary_sqrt_f32
.visible .entry unary_sqrt_f32(
	.param .u64 unary_sqrt_f32_param_0,
	.param .u64 unary_sqrt_f32_param_1,
	.param .u32 unary_sqrt_f32_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<3>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<8>;


	ld.param.u64 	%rd1, [unary_sqrt_f32_param_0];
	ld.param.u64 	%rd2, [unary_sqrt_f32_param_1];
	ld.param.u32 	%r2, [unary_sqrt_f32_param_2];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB6_2;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.s32 	%rd4, %r1, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f32 	%f1, [%rd5];
	sqrt.rn.f32 	%f2, %f1;
	cvta.to.global.u64 	%rd6, %rd2;
	add.s64 	%rd7, %rd6, %rd4;
	st.global.f32 	[%rd7], %f2;

$L__BB6_2:
	ret;

}
	// .globl	unary_sqrt_f16
.visible .entry unary_sqrt_f16(
	.param .u64 unary_sqrt_f16_param_0,
	.param .u64 unary_sqrt_f16_param_1,
	.param .u32 unary_sqrt_f16_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .b16 	%rs<3>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<8>;


	ld.param.u64 	%rd1, [unary_sqrt_f16_param_0];
	ld.param.u64 	%rd2, [unary_sqrt_f16_param_1];
	ld.param.u32 	%r2, [unary_sqrt_f16_param_2];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB7_2;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.s32 	%rd4, %r1, 2;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u16 	%rs2, [%rd5];
	// begin inline asm
	{.reg.b32         f;        
 .reg.b16         r;        
  mov.b16         r,%rs2;     
  cvt.f32.f16     f,r;      
  sqrt.approx.ftz.f32   f,f;  
  cvt.rn.f16.f32      r,f;  
  mov.b16         %rs1,r;     
}
	// end inline asm
	cvta.to.global.u64 	%rd6, %rd2;
	add.s64 	%rd7, %rd6, %rd4;
	st.global.u16 	[%rd7], %rs1;

$L__BB7_2:
	ret;

}
	// .globl	unary_rsqrt_f32
.visible .entry unary_rsqrt_f32(
	.param .u64 unary_rsqrt_f32_param_0,
	.param .u64 unary_rsqrt_f32_param_1,
	.param .u32 unary_rsqrt_f32_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<4>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<8>;


	ld.param.u64 	%rd1, [unary_rsqrt_f32_param_0];
	ld.param.u64 	%rd2, [unary_rsqrt_f32_param_1];
	ld.param.u32 	%r2, [unary_rsqrt_f32_param_2];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB8_2;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.s32 	%rd4, %r1, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f32 	%f1, [%rd5];
	sqrt.rn.f32 	%f2, %f1;
	rcp.rn.f32 	%f3, %f2;
	cvta.to.global.u64 	%rd6, %rd2;
	add.s64 	%rd7, %rd6, %rd4;
	st.global.f32 	[%rd7], %f3;

$L__BB8_2:
	ret;

}
	// .globl	unary_rsqrt_f16
.visible .entry unary_rsqrt_f16(
	.param .u64 unary_rsqrt_f16_param_0,
	.param .u64 unary_rsqrt_f16_param_1,
	.param .u32 unary_rsqrt_f16_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .b16 	%rs<3>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<8>;


	ld.param.u64 	%rd1, [unary_rsqrt_f16_param_0];
	ld.param.u64 	%rd2, [unary_rsqrt_f16_param_1];
	ld.param.u32 	%r2, [unary_rsqrt_f16_param_2];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB9_2;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.s32 	%rd4, %r1, 2;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u16 	%rs2, [%rd5];
	// begin inline asm
	{.reg.b32         f;        
 .reg.b16         r;        
  mov.b16         r,%rs2;     
  cvt.f32.f16     f,r;      
  rsqrt.approx.ftz.f32   f,f;  
  cvt.rn.f16.f32      r,f;  
  mov.b16         %rs1,r;     
}
	// end inline asm
	cvta.to.global.u64 	%rd6, %rd2;
	add.s64 	%rd7, %rd6, %rd4;
	st.global.u16 	[%rd7], %rs1;

$L__BB9_2:
	ret;

}
	// .globl	unary_recip_f32
.visible .entry unary_recip_f32(
	.param .u64 unary_recip_f32_param_0,
	.param .u64 unary_recip_f32_param_1,
	.param .u32 unary_recip_f32_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<3>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<8>;


	ld.param.u64 	%rd1, [unary_recip_f32_param_0];
	ld.param.u64 	%rd2, [unary_recip_f32_param_1];
	ld.param.u32 	%r2, [unary_recip_f32_param_2];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB10_2;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.s32 	%rd4, %r1, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f32 	%f1, [%rd5];
	rcp.rn.f32 	%f2, %f1;
	cvta.to.global.u64 	%rd6, %rd2;
	add.s64 	%rd7, %rd6, %rd4;
	st.global.f32 	[%rd7], %f2;

$L__BB10_2:
	ret;

}
	// .globl	unary_recip_f16
.visible .entry unary_recip_f16(
	.param .u64 unary_recip_f16_param_0,
	.param .u64 unary_recip_f16_param_1,
	.param .u32 unary_recip_f16_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .b16 	%rs<3>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<8>;


	ld.param.u64 	%rd1, [unary_recip_f16_param_0];
	ld.param.u64 	%rd2, [unary_recip_f16_param_1];
	ld.param.u32 	%r2, [unary_recip_f16_param_2];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB11_2;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.s32 	%rd4, %r1, 2;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u16 	%rs2, [%rd5];
	// begin inline asm
	{.reg.b32         f;        
 .reg.b16         r;        
  mov.b16         r,%rs2;     
  cvt.f32.f16     f,r;      
  rcp.approx.ftz.f32   f,f;  
  cvt.rn.f16.f32      r,f;  
  mov.b16         %rs1,r;     
}
	// end inline asm
	cvta.to.global.u64 	%rd6, %rd2;
	add.s64 	%rd7, %rd6, %rd4;
	st.global.u16 	[%rd7], %rs1;

$L__BB11_2:
	ret;

}
	// .globl	unary_ceil_f32
.visible .entry unary_ceil_f32(
	.param .u64 unary_ceil_f32_param_0,
	.param .u64 unary_ceil_f32_param_1,
	.param .u32 unary_ceil_f32_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<3>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<8>;


	ld.param.u64 	%rd1, [unary_ceil_f32_param_0];
	ld.param.u64 	%rd2, [unary_ceil_f32_param_1];
	ld.param.u32 	%r2, [unary_ceil_f32_param_2];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB12_2;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.s32 	%rd4, %r1, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f32 	%f1, [%rd5];
	cvt.rpi.f32.f32 	%f2, %f1;
	cvta.to.global.u64 	%rd6, %rd2;
	add.s64 	%rd7, %rd6, %rd4;
	st.global.f32 	[%rd7], %f2;

$L__BB12_2:
	ret;

}
	// .globl	unary_ceil_f16
.visible .entry unary_ceil_f16(
	.param .u64 unary_ceil_f16_param_0,
	.param .u64 unary_ceil_f16_param_1,
	.param .u32 unary_ceil_f16_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .b16 	%rs<3>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<8>;


	ld.param.u64 	%rd1, [unary_ceil_f16_param_0];
	ld.param.u64 	%rd2, [unary_ceil_f16_param_1];
	ld.param.u32 	%r2, [unary_ceil_f16_param_2];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB13_2;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.s32 	%rd4, %r1, 2;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u16 	%rs2, [%rd5];
	// begin inline asm
	cvt.rpi.f16.f16 %rs1, %rs2;
	// end inline asm
	cvta.to.global.u64 	%rd6, %rd2;
	add.s64 	%rd7, %rd6, %rd4;
	st.global.u16 	[%rd7], %rs1;

$L__BB13_2:
	ret;

}
	// .globl	unary_floor_f32
.visible .entry unary_floor_f32(
	.param .u64 unary_floor_f32_param_0,
	.param .u64 unary_floor_f32_param_1,
	.param .u32 unary_floor_f32_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<3>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<8>;


	ld.param.u64 	%rd1, [unary_floor_f32_param_0];
	ld.param.u64 	%rd2, [unary_floor_f32_param_1];
	ld.param.u32 	%r2, [unary_floor_f32_param_2];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB14_2;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.s32 	%rd4, %r1, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f32 	%f1, [%rd5];
	cvt.rmi.f32.f32 	%f2, %f1;
	cvta.to.global.u64 	%rd6, %rd2;
	add.s64 	%rd7, %rd6, %rd4;
	st.global.f32 	[%rd7], %f2;

$L__BB14_2:
	ret;

}
	// .globl	unary_floor_f16
.visible .entry unary_floor_f16(
	.param .u64 unary_floor_f16_param_0,
	.param .u64 unary_floor_f16_param_1,
	.param .u32 unary_floor_f16_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .b16 	%rs<3>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<8>;


	ld.param.u64 	%rd1, [unary_floor_f16_param_0];
	ld.param.u64 	%rd2, [unary_floor_f16_param_1];
	ld.param.u32 	%r2, [unary_floor_f16_param_2];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB15_2;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.s32 	%rd4, %r1, 2;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u16 	%rs2, [%rd5];
	// begin inline asm
	cvt.rmi.f16.f16 %rs1, %rs2;
	// end inline asm
	cvta.to.global.u64 	%rd6, %rd2;
	add.s64 	%rd7, %rd6, %rd4;
	st.global.u16 	[%rd7], %rs1;

$L__BB15_2:
	ret;

}
	// .globl	unary_round_f32
.visible .entry unary_round_f32(
	.param .u64 unary_round_f32_param_0,
	.param .u64 unary_round_f32_param_1,
	.param .u32 unary_round_f32_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<5>;
	.reg .b32 	%r<9>;
	.reg .b64 	%rd<8>;


	ld.param.u64 	%rd1, [unary_round_f32_param_0];
	ld.param.u64 	%rd2, [unary_round_f32_param_1];
	ld.param.u32 	%r2, [unary_round_f32_param_2];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB16_2;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.s32 	%rd4, %r1, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f32 	%f1, [%rd5];
	mov.b32 	%r6, %f1;
	and.b32  	%r7, %r6, -2147483648;
	or.b32  	%r8, %r7, 1056964608;
	mov.b32 	%f2, %r8;
	add.rz.f32 	%f3, %f1, %f2;
	cvt.rzi.f32.f32 	%f4, %f3;
	cvta.to.global.u64 	%rd6, %rd2;
	add.s64 	%rd7, %rd6, %rd4;
	st.global.f32 	[%rd7], %f4;

$L__BB16_2:
	ret;

}
	// .globl	unary_round_f16
.visible .entry unary_round_f16(
	.param .u64 unary_round_f16_param_0,
	.param .u64 unary_round_f16_param_1,
	.param .u32 unary_round_f16_param_2
)
{
	.reg .pred 	%p<3>;
	.reg .b16 	%rs<22>;
	.reg .f32 	%f<3>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<10>;


	ld.param.u64 	%rd2, [unary_round_f16_param_0];
	ld.param.u64 	%rd3, [unary_round_f16_param_1];
	ld.param.u32 	%r2, [unary_round_f16_param_2];
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r4, %r3, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB17_5;

	cvta.to.global.u64 	%rd4, %rd2;
	cvt.s64.s32 	%rd1, %r1;
	mul.wide.s32 	%rd5, %r1, 2;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.u16 	%rs1, [%rd6];
	mov.f32 	%f1, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs6, %f1;}

	// end inline asm
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs1, %rs6;
  selp.u16 %rs7, 1, 0, __$temp3;}
	// end inline asm
	setp.eq.s16 	%p2, %rs7, 0;
	mov.f32 	%f2, 0f3F000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs10, %f2;}

	// end inline asm
	@%p2 bra 	$L__BB17_3;

	// begin inline asm
	{sub.f16 %rs11,%rs1,%rs10;
}
	// end inline asm
	// begin inline asm
	cvt.rpi.f16.f16 %rs21, %rs11;
	// end inline asm
	bra.uni 	$L__BB17_4;

$L__BB17_3:
	// begin inline asm
	{add.f16 %rs16,%rs1,%rs10;
}
	// end inline asm
	// begin inline asm
	cvt.rmi.f16.f16 %rs21, %rs16;
	// end inline asm

$L__BB17_4:
	cvta.to.global.u64 	%rd7, %rd3;
	shl.b64 	%rd8, %rd1, 1;
	add.s64 	%rd9, %rd7, %rd8;
	st.global.u16 	[%rd9], %rs21;

$L__BB17_5:
	ret;

}
	// .globl	unary_rint_f32
.visible .entry unary_rint_f32(
	.param .u64 unary_rint_f32_param_0,
	.param .u64 unary_rint_f32_param_1,
	.param .u32 unary_rint_f32_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<3>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<8>;


	ld.param.u64 	%rd1, [unary_rint_f32_param_0];
	ld.param.u64 	%rd2, [unary_rint_f32_param_1];
	ld.param.u32 	%r2, [unary_rint_f32_param_2];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB18_2;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.s32 	%rd4, %r1, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f32 	%f1, [%rd5];
	cvt.rni.f32.f32 	%f2, %f1;
	cvta.to.global.u64 	%rd6, %rd2;
	add.s64 	%rd7, %rd6, %rd4;
	st.global.f32 	[%rd7], %f2;

$L__BB18_2:
	ret;

}
	// .globl	unary_rint_f16
.visible .entry unary_rint_f16(
	.param .u64 unary_rint_f16_param_0,
	.param .u64 unary_rint_f16_param_1,
	.param .u32 unary_rint_f16_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .b16 	%rs<3>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<8>;


	ld.param.u64 	%rd1, [unary_rint_f16_param_0];
	ld.param.u64 	%rd2, [unary_rint_f16_param_1];
	ld.param.u32 	%r2, [unary_rint_f16_param_2];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB19_2;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.s32 	%rd4, %r1, 2;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u16 	%rs2, [%rd5];
	// begin inline asm
	cvt.rni.f16.f16 %rs1, %rs2;
	// end inline asm
	cvta.to.global.u64 	%rd6, %rd2;
	add.s64 	%rd7, %rd6, %rd4;
	st.global.u16 	[%rd7], %rs1;

$L__BB19_2:
	ret;

}
	// .globl	unary_sin_f32
.visible .entry unary_sin_f32(
	.param .u64 unary_sin_f32_param_0,
	.param .u64 unary_sin_f32_param_1,
	.param .u32 unary_sin_f32_param_2
)
{
	.local .align 4 .b8 	__local_depot20[28];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<12>;
	.reg .f32 	%f<38>;
	.reg .b32 	%r<59>;
	.reg .f64 	%fd<3>;
	.reg .b64 	%rd<31>;


	mov.u64 	%SPL, __local_depot20;
	ld.param.u64 	%rd9, [unary_sin_f32_param_0];
	ld.param.u64 	%rd10, [unary_sin_f32_param_1];
	ld.param.u32 	%r20, [unary_sin_f32_param_2];
	add.u64 	%rd1, %SPL, 0;
	mov.u32 	%r21, %ntid.x;
	mov.u32 	%r22, %ctaid.x;
	mov.u32 	%r23, %tid.x;
	mad.lo.s32 	%r1, %r22, %r21, %r23;
	setp.ge.s32 	%p1, %r1, %r20;
	@%p1 bra 	$L__BB20_14;

	cvta.to.global.u64 	%rd12, %rd9;
	cvt.s64.s32 	%rd2, %r1;
	mul.wide.s32 	%rd13, %r1, 4;
	add.s64 	%rd14, %rd12, %rd13;
	ld.global.f32 	%f1, [%rd14];
	mul.f32 	%f14, %f1, 0f3F22F983;
	cvt.rni.s32.f32 	%r58, %f14;
	cvt.rn.f32.s32 	%f15, %r58;
	mov.f32 	%f16, 0fBFC90FDA;
	fma.rn.f32 	%f17, %f15, %f16, %f1;
	mov.f32 	%f18, 0fB3A22168;
	fma.rn.f32 	%f19, %f15, %f18, %f17;
	mov.f32 	%f20, 0fA7C234C5;
	fma.rn.f32 	%f35, %f15, %f20, %f19;
	abs.f32 	%f3, %f1;
	setp.ltu.f32 	%p2, %f3, 0f47CE4780;
	@%p2 bra 	$L__BB20_9;

	setp.eq.f32 	%p3, %f3, 0f7F800000;
	@%p3 bra 	$L__BB20_8;
	bra.uni 	$L__BB20_3;

$L__BB20_8:
	mov.f32 	%f23, 0f00000000;
	mul.rn.f32 	%f35, %f1, %f23;
	mov.u32 	%r58, 0;
	bra.uni 	$L__BB20_9;

$L__BB20_3:
	mov.b32 	%r3, %f1;
	shr.u32 	%r25, %r3, 23;
	and.b32  	%r26, %r25, 255;
	add.s32 	%r4, %r26, -128;
	shl.b32 	%r27, %r3, 8;
	or.b32  	%r5, %r27, -2147483648;
	shr.u32 	%r6, %r4, 5;
	mov.u64 	%rd30, 0;
	mov.u32 	%r55, 0;
	mov.u64 	%rd29, __cudart_i2opi_f;
	mov.u64 	%rd28, %rd1;

$L__BB20_4:
	.pragma "nounroll";
	ld.global.nc.u32 	%r28, [%rd29];
	mad.wide.u32 	%rd17, %r28, %r5, %rd30;
	shr.u64 	%rd30, %rd17, 32;
	st.local.u32 	[%rd28], %rd17;
	add.s64 	%rd29, %rd29, 4;
	add.s64 	%rd28, %rd28, 4;
	add.s32 	%r55, %r55, 1;
	setp.ne.s32 	%p4, %r55, 6;
	@%p4 bra 	$L__BB20_4;

	st.local.u32 	[%rd1+24], %rd30;
	mov.u32 	%r29, 4;
	sub.s32 	%r9, %r29, %r6;
	mov.u32 	%r30, 6;
	sub.s32 	%r31, %r30, %r6;
	mul.wide.s32 	%rd18, %r31, 4;
	add.s64 	%rd19, %rd1, %rd18;
	ld.local.u32 	%r56, [%rd19];
	ld.local.u32 	%r57, [%rd19+-4];
	and.b32  	%r12, %r4, 31;
	setp.eq.s32 	%p5, %r12, 0;
	@%p5 bra 	$L__BB20_7;

	mov.u32 	%r32, 32;
	sub.s32 	%r33, %r32, %r12;
	shr.u32 	%r34, %r57, %r33;
	shl.b32 	%r35, %r56, %r12;
	add.s32 	%r56, %r34, %r35;
	mul.wide.s32 	%rd20, %r9, 4;
	add.s64 	%rd21, %rd1, %rd20;
	ld.local.u32 	%r36, [%rd21];
	shr.u32 	%r37, %r36, %r33;
	shl.b32 	%r38, %r57, %r12;
	add.s32 	%r57, %r37, %r38;

$L__BB20_7:
	and.b32  	%r39, %r3, -2147483648;
	shr.u32 	%r40, %r57, 30;
	shl.b32 	%r41, %r56, 2;
	or.b32  	%r42, %r40, %r41;
	shr.u32 	%r43, %r42, 31;
	shr.u32 	%r44, %r56, 30;
	add.s32 	%r45, %r43, %r44;
	neg.s32 	%r46, %r45;
	setp.eq.s32 	%p6, %r39, 0;
	selp.b32 	%r58, %r45, %r46, %p6;
	setp.ne.s32 	%p7, %r43, 0;
	xor.b32  	%r47, %r39, -2147483648;
	selp.b32 	%r48, %r47, %r39, %p7;
	selp.b32 	%r49, -1, 0, %p7;
	xor.b32  	%r50, %r42, %r49;
	shl.b32 	%r51, %r57, 2;
	xor.b32  	%r52, %r51, %r49;
	cvt.u64.u32 	%rd22, %r50;
	cvt.u64.u32 	%rd23, %r52;
	bfi.b64 	%rd24, %rd22, %rd23, 32, 32;
	cvt.rn.f64.s64 	%fd1, %rd24;
	mul.f64 	%fd2, %fd1, 0d3BF921FB54442D19;
	cvt.rn.f32.f64 	%f21, %fd2;
	setp.eq.s32 	%p8, %r48, 0;
	neg.f32 	%f22, %f21;
	selp.f32 	%f35, %f21, %f22, %p8;

$L__BB20_9:
	and.b32  	%r19, %r58, 1;
	setp.eq.s32 	%p9, %r19, 0;
	selp.f32 	%f7, %f35, 0f3F800000, %p9;
	mul.rn.f32 	%f8, %f35, %f35;
	mov.f32 	%f36, 0fB94D4153;
	@%p9 bra 	$L__BB20_11;

	mov.f32 	%f25, 0fBAB607ED;
	mov.f32 	%f26, 0f37CBAC00;
	fma.rn.f32 	%f36, %f26, %f8, %f25;

$L__BB20_11:
	selp.f32 	%f27, 0f3C0885E4, 0f3D2AAABB, %p9;
	fma.rn.f32 	%f28, %f36, %f8, %f27;
	selp.f32 	%f29, 0fBE2AAAA8, 0fBEFFFFFF, %p9;
	fma.rn.f32 	%f30, %f28, %f8, %f29;
	mov.f32 	%f31, 0f00000000;
	fma.rn.f32 	%f32, %f8, %f7, %f31;
	fma.rn.f32 	%f37, %f30, %f32, %f7;
	and.b32  	%r54, %r58, 2;
	setp.eq.s32 	%p11, %r54, 0;
	@%p11 bra 	$L__BB20_13;

	mov.f32 	%f34, 0fBF800000;
	fma.rn.f32 	%f37, %f37, %f34, %f31;

$L__BB20_13:
	cvta.to.global.u64 	%rd25, %rd10;
	shl.b64 	%rd26, %rd2, 2;
	add.s64 	%rd27, %rd25, %rd26;
	st.global.f32 	[%rd27], %f37;

$L__BB20_14:
	ret;

}
	// .globl	unary_sin_f16
.visible .entry unary_sin_f16(
	.param .u64 unary_sin_f16_param_0,
	.param .u64 unary_sin_f16_param_1,
	.param .u32 unary_sin_f16_param_2
)
{
	.reg .pred 	%p<4>;
	.reg .b16 	%rs<6>;
	.reg .f32 	%f<23>;
	.reg .b32 	%r<9>;
	.reg .b64 	%rd<8>;


	ld.param.u64 	%rd1, [unary_sin_f16_param_0];
	ld.param.u64 	%rd2, [unary_sin_f16_param_1];
	ld.param.u32 	%r2, [unary_sin_f16_param_2];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB21_2;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.s32 	%rd4, %r1, 2;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u16 	%rs1, [%rd5];
	// begin inline asm
	{  cvt.f32.f16 %f1, %rs1;}

	// end inline asm
	mov.f32 	%f3, 0f4B400000;
	mov.f32 	%f4, 0f3F22F983;
	fma.rn.f32 	%f5, %f1, %f4, %f3;
	mov.b32 	%r6, %f5;
	sub.rn.f32 	%f6, %f5, %f3;
	mov.f32 	%f7, 0fBFC90FDA;
	fma.rn.f32 	%f8, %f6, %f7, %f1;
	mov.f32 	%f9, 0fB3A22168;
	fma.rn.f32 	%f10, %f6, %f9, %f8;
	mul.f32 	%f11, %f10, %f10;
	and.b32  	%r7, %r6, 1;
	setp.eq.b32 	%p2, %r7, 1;
	selp.f32 	%f12, 0f37CCF5CE, 0fB94CA1F9, %p2;
	selp.f32 	%f13, 0fBAB6061A, 0f3C08839E, %p2;
	selp.f32 	%f14, 0f3D2AAAA5, 0fBE2AAAA3, %p2;
	selp.f32 	%f15, 0fBF000000, 0f00000000, %p2;
	selp.f32 	%f16, %f11, %f10, %p2;
	selp.f32 	%f17, 0f3F800000, %f10, %p2;
	fma.rn.f32 	%f18, %f12, %f11, %f13;
	fma.rn.f32 	%f19, %f18, %f11, %f14;
	fma.rn.f32 	%f20, %f19, %f11, %f15;
	fma.rn.f32 	%f21, %f20, %f16, %f17;
	and.b32  	%r8, %r6, 2;
	setp.eq.s32 	%p3, %r8, 0;
	neg.f32 	%f22, %f21;
	selp.f32 	%f2, %f21, %f22, %p3;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs3, %f2;}

	// end inline asm
	// begin inline asm
	{
	  .reg.b16 i,r,t;     
	  mov.b16 r, %rs3;      
	  mov.b16 i, %rs1;      
	  and.b16 t, r, 0x8000U; 
	  abs.f16 r, r;   
	  abs.f16 i, i;   
	{.reg.b16 spc, ulp, p;
  mov.b16 spc,0X32B3U;
  mov.b16 ulp,0x0800U;
  set.eq.f16.f16 p,i, spc;
  fma.rn.f16 r,p,ulp,r;
}
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0X5CB0U;
  mov.b16 ulp,0x9000U;
  set.eq.f16.f16 p,i, spc;
  fma.rn.f16 r,p,ulp,r;
}
  or.b16  r,r,t;      
	  mov.b16 %rs3, r;      
}

	// end inline asm
	cvta.to.global.u64 	%rd6, %rd2;
	add.s64 	%rd7, %rd6, %rd4;
	st.global.u16 	[%rd7], %rs3;

$L__BB21_2:
	ret;

}
	// .globl	unary_sinh_f32
.visible .entry unary_sinh_f32(
	.param .u64 unary_sinh_f32_param_0,
	.param .u64 unary_sinh_f32_param_1,
	.param .u32 unary_sinh_f32_param_2
)
{
	.reg .pred 	%p<5>;
	.reg .f32 	%f<37>;
	.reg .b32 	%r<15>;
	.reg .b64 	%rd<10>;


	ld.param.u64 	%rd2, [unary_sinh_f32_param_0];
	ld.param.u64 	%rd3, [unary_sinh_f32_param_1];
	ld.param.u32 	%r2, [unary_sinh_f32_param_2];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB22_5;

	cvta.to.global.u64 	%rd4, %rd2;
	cvt.s64.s32 	%rd1, %r1;
	mul.wide.s32 	%rd5, %r1, 4;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.f32 	%f1, [%rd6];
	abs.f32 	%f2, %f1;
	setp.ltu.f32 	%p2, %f2, 0f3F800000;
	@%p2 bra 	$L__BB22_3;
	bra.uni 	$L__BB22_2;

$L__BB22_3:
	mul.f32 	%f27, %f1, %f1;
	mov.f32 	%f28, 0f394FFF49;
	mov.f32 	%f29, 0f363D0ADA;
	fma.rn.f32 	%f30, %f29, %f27, %f28;
	mov.f32 	%f31, 0f3C08889A;
	fma.rn.f32 	%f32, %f30, %f27, %f31;
	mov.f32 	%f33, 0f3E2AAAAB;
	fma.rn.f32 	%f34, %f32, %f27, %f33;
	mul.f32 	%f35, %f27, %f34;
	fma.rn.f32 	%f36, %f35, %f1, %f1;
	bra.uni 	$L__BB22_4;

$L__BB22_2:
	mov.f32 	%f6, 0f3FB8AA3B;
	mul.rn.f32 	%f7, %f2, %f6;
	cvt.rzi.f32.f32 	%f8, %f7;
	abs.f32 	%f9, %f8;
	setp.gt.f32 	%p3, %f9, 0f42FC0000;
	mov.b32 	%r6, %f8;
	and.b32  	%r7, %r6, -2147483648;
	or.b32  	%r8, %r7, 1123811328;
	mov.b32 	%f10, %r8;
	selp.f32 	%f11, %f10, %f8, %p3;
	mov.f32 	%f12, 0fBF317218;
	fma.rn.f32 	%f13, %f11, %f12, %f2;
	mov.f32 	%f14, 0f3102E308;
	fma.rn.f32 	%f15, %f11, %f14, %f13;
	mul.f32 	%f16, %f15, 0f3FB8AA3B;
	add.f32 	%f17, %f11, 0f4B40007D;
	mov.b32 	%r9, %f17;
	shl.b32 	%r10, %r9, 23;
	mov.b32 	%f18, %r10;
	ex2.approx.ftz.f32 	%f19, %f16;
	mul.f32 	%f20, %f19, %f18;
	mov.f32 	%f21, 0f3E000000;
	div.approx.f32 	%f22, %f21, %f20;
	neg.f32 	%f23, %f22;
	mov.f32 	%f24, 0f40000000;
	fma.rn.f32 	%f25, %f24, %f20, %f23;
	setp.ge.f32 	%p4, %f2, 0f42B40000;
	selp.f32 	%f26, 0f7F800000, %f25, %p4;
	mov.b32 	%r11, %f26;
	mov.b32 	%r12, %f1;
	and.b32  	%r13, %r12, -2147483648;
	or.b32  	%r14, %r13, %r11;
	mov.b32 	%f36, %r14;

$L__BB22_4:
	cvta.to.global.u64 	%rd7, %rd3;
	shl.b64 	%rd8, %rd1, 2;
	add.s64 	%rd9, %rd7, %rd8;
	st.global.f32 	[%rd9], %f36;

$L__BB22_5:
	ret;

}
	// .globl	unary_sinh_f16
.visible .entry unary_sinh_f16(
	.param .u64 unary_sinh_f16_param_0,
	.param .u64 unary_sinh_f16_param_1,
	.param .u32 unary_sinh_f16_param_2
)
{
	.reg .pred 	%p<4>;
	.reg .b16 	%rs<29>;
	.reg .f32 	%f<15>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<10>;


	ld.param.u64 	%rd2, [unary_sinh_f16_param_0];
	ld.param.u64 	%rd3, [unary_sinh_f16_param_1];
	ld.param.u32 	%r2, [unary_sinh_f16_param_2];
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r4, %r3, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB23_5;

	cvta.to.global.u64 	%rd4, %rd2;
	cvt.s64.s32 	%rd1, %r1;
	mul.wide.s32 	%rd5, %r1, 2;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.u16 	%rs6, [%rd6];
	// begin inline asm
	{.reg.b32         f, C, nZ;       
 .reg.b16         h,r;            
  mov.b16         h,%rs6;           
  cvt.f32.f16     f,h;            
  mov.b32         C, 0x3fb8aa3bU; 
  mov.b32         nZ, 0x80000000U;
  fma.rn.f32      f,f,C,nZ;       
  ex2.approx.ftz.f32  f,f;        
  cvt.rn.f16.f32      r,f;        
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0X1F79U;
  mov.b16 ulp,0x9400U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0X25CFU;
  mov.b16 ulp,0x9400U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0XC13BU;
  mov.b16 ulp,0x0400U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0XC1EFU;
  mov.b16 ulp,0x0200U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
  mov.b16         %rs5,r;           
}
	// end inline asm
	// begin inline asm
	{neg.f16 %rs7,%rs6;
}
	// end inline asm
	// begin inline asm
	{.reg.b32         f, C, nZ;       
 .reg.b16         h,r;            
  mov.b16         h,%rs7;           
  cvt.f32.f16     f,h;            
  mov.b32         C, 0x3fb8aa3bU; 
  mov.b32         nZ, 0x80000000U;
  fma.rn.f32      f,f,C,nZ;       
  ex2.approx.ftz.f32  f,f;        
  cvt.rn.f16.f32      r,f;        
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0X1F79U;
  mov.b16 ulp,0x9400U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0X25CFU;
  mov.b16 ulp,0x9400U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0XC13BU;
  mov.b16 ulp,0x0400U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0XC1EFU;
  mov.b16 ulp,0x0200U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
  mov.b16         %rs9,r;           
}
	// end inline asm
	// begin inline asm
	{sub.f16 %rs11,%rs5,%rs9;
}
	// end inline asm
	mov.f32 	%f5, 0f40000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs14, %f5;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f6, %rs11;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f7, %rs14;}

	// end inline asm
	// begin inline asm
	{rcp.approx.ftz.f32 %f8, %f7;
}
	// end inline asm
	mul.f32 	%f10, %f6, %f8;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs28, %f10;}

	// end inline asm
	// begin inline asm
	{abs.f16 %rs18,%rs28;
}
	// end inline asm
	mov.u16 	%rs22, 143;
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs18, %rs22;
  selp.u16 %rs20, 1, 0, __$temp3;}
	// end inline asm
	setp.eq.s16 	%p2, %rs20, 0;
	@%p2 bra 	$L__BB23_4;

	mov.f32 	%f11, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs23, %f11;}

	// end inline asm
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs23, %rs18;
  selp.u16 %rs24, 1, 0, __$temp3;}
	// end inline asm
	setp.eq.s16 	%p3, %rs24, 0;
	@%p3 bra 	$L__BB23_4;

	neg.f32 	%f13, %f7;
	fma.rn.f32 	%f14, %f13, %f10, %f6;
	fma.rn.f32 	%f12, %f8, %f14, %f10;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs28, %f12;}

	// end inline asm

$L__BB23_4:
	cvta.to.global.u64 	%rd7, %rd3;
	shl.b64 	%rd8, %rd1, 1;
	add.s64 	%rd9, %rd7, %rd8;
	st.global.u16 	[%rd9], %rs28;

$L__BB23_5:
	ret;

}
	// .globl	unary_asin_f32
.visible .entry unary_asin_f32(
	.param .u64 unary_asin_f32_param_0,
	.param .u64 unary_asin_f32_param_1,
	.param .u32 unary_asin_f32_param_2
)
{
	.reg .pred 	%p<5>;
	.reg .f32 	%f<33>;
	.reg .b32 	%r<10>;
	.reg .b64 	%rd<8>;


	ld.param.u64 	%rd1, [unary_asin_f32_param_0];
	ld.param.u64 	%rd2, [unary_asin_f32_param_1];
	ld.param.u32 	%r2, [unary_asin_f32_param_2];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB24_2;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.s32 	%rd4, %r1, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f32 	%f1, [%rd5];
	abs.f32 	%f2, %f1;
	neg.f32 	%f3, %f2;
	mov.f32 	%f4, 0f3F000000;
	fma.rn.f32 	%f5, %f4, %f3, %f4;
	rsqrt.approx.ftz.f32 	%f6, %f5;
	mul.f32 	%f7, %f5, %f6;
	mul.f32 	%f8, %f6, 0f3F000000;
	neg.f32 	%f9, %f7;
	fma.rn.f32 	%f10, %f9, %f8, %f4;
	fma.rn.f32 	%f11, %f7, %f10, %f7;
	setp.eq.f32 	%p2, %f2, 0f3F800000;
	selp.f32 	%f12, 0f00000000, %f11, %p2;
	setp.gt.f32 	%p3, %f2, 0f3F0F5C29;
	selp.f32 	%f13, %f12, %f2, %p3;
	mul.f32 	%f14, %f13, %f13;
	mov.f32 	%f15, 0f3C99CA97;
	mov.f32 	%f16, 0f3D4DD2F7;
	fma.rn.f32 	%f17, %f16, %f14, %f15;
	mov.f32 	%f18, 0f3D3F90E8;
	fma.rn.f32 	%f19, %f17, %f14, %f18;
	mov.f32 	%f20, 0f3D993CCF;
	fma.rn.f32 	%f21, %f19, %f14, %f20;
	mov.f32 	%f22, 0f3E2AAC04;
	fma.rn.f32 	%f23, %f21, %f14, %f22;
	mul.f32 	%f24, %f14, %f23;
	fma.rn.f32 	%f25, %f24, %f13, %f13;
	mul.f32 	%f26, %f25, 0fC0000000;
	mov.f32 	%f27, 0f3FD774EB;
	mov.f32 	%f28, 0f3F6EE581;
	fma.rn.f32 	%f29, %f28, %f27, %f26;
	selp.f32 	%f30, %f29, %f25, %p3;
	setp.le.f32 	%p4, %f30, 0f7F800000;
	mov.b32 	%r6, %f30;
	mov.b32 	%r7, %f1;
	and.b32  	%r8, %r7, -2147483648;
	or.b32  	%r9, %r8, %r6;
	mov.b32 	%f31, %r9;
	selp.f32 	%f32, %f31, %f30, %p4;
	cvta.to.global.u64 	%rd6, %rd2;
	add.s64 	%rd7, %rd6, %rd4;
	st.global.f32 	[%rd7], %f32;

$L__BB24_2:
	ret;

}
	// .globl	unary_asin_f16
.visible .entry unary_asin_f16(
	.param .u64 unary_asin_f16_param_0,
	.param .u64 unary_asin_f16_param_1,
	.param .u32 unary_asin_f16_param_2
)
{
	.reg .pred 	%p<5>;
	.reg .b16 	%rs<3>;
	.reg .f32 	%f<33>;
	.reg .b32 	%r<10>;
	.reg .b64 	%rd<8>;


	ld.param.u64 	%rd1, [unary_asin_f16_param_0];
	ld.param.u64 	%rd2, [unary_asin_f16_param_1];
	ld.param.u32 	%r2, [unary_asin_f16_param_2];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB25_2;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.s32 	%rd4, %r1, 2;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u16 	%rs1, [%rd5];
	// begin inline asm
	{  cvt.f32.f16 %f1, %rs1;}

	// end inline asm
	abs.f32 	%f3, %f1;
	neg.f32 	%f4, %f3;
	mov.f32 	%f5, 0f3F000000;
	fma.rn.f32 	%f6, %f5, %f4, %f5;
	rsqrt.approx.ftz.f32 	%f7, %f6;
	mul.f32 	%f8, %f6, %f7;
	mul.f32 	%f9, %f7, 0f3F000000;
	neg.f32 	%f10, %f8;
	fma.rn.f32 	%f11, %f10, %f9, %f5;
	fma.rn.f32 	%f12, %f8, %f11, %f8;
	setp.eq.f32 	%p2, %f3, 0f3F800000;
	selp.f32 	%f13, 0f00000000, %f12, %p2;
	setp.gt.f32 	%p3, %f3, 0f3F0F5C29;
	selp.f32 	%f14, %f13, %f3, %p3;
	mul.f32 	%f15, %f14, %f14;
	mov.f32 	%f16, 0f3C99CA97;
	mov.f32 	%f17, 0f3D4DD2F7;
	fma.rn.f32 	%f18, %f17, %f15, %f16;
	mov.f32 	%f19, 0f3D3F90E8;
	fma.rn.f32 	%f20, %f18, %f15, %f19;
	mov.f32 	%f21, 0f3D993CCF;
	fma.rn.f32 	%f22, %f20, %f15, %f21;
	mov.f32 	%f23, 0f3E2AAC04;
	fma.rn.f32 	%f24, %f22, %f15, %f23;
	mul.f32 	%f25, %f15, %f24;
	fma.rn.f32 	%f26, %f25, %f14, %f14;
	mul.f32 	%f27, %f26, 0fC0000000;
	mov.f32 	%f28, 0f3FD774EB;
	mov.f32 	%f29, 0f3F6EE581;
	fma.rn.f32 	%f30, %f29, %f28, %f27;
	selp.f32 	%f31, %f30, %f26, %p3;
	setp.le.f32 	%p4, %f31, 0f7F800000;
	mov.b32 	%r6, %f31;
	mov.b32 	%r7, %f1;
	and.b32  	%r8, %r7, -2147483648;
	or.b32  	%r9, %r8, %r6;
	mov.b32 	%f32, %r9;
	selp.f32 	%f2, %f32, %f31, %p4;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs2, %f2;}

	// end inline asm
	cvta.to.global.u64 	%rd6, %rd2;
	add.s64 	%rd7, %rd6, %rd4;
	st.global.u16 	[%rd7], %rs2;

$L__BB25_2:
	ret;

}
	// .globl	unary_asinh_f32
.visible .entry unary_asinh_f32(
	.param .u64 unary_asinh_f32_param_0,
	.param .u64 unary_asinh_f32_param_1,
	.param .u32 unary_asinh_f32_param_2
)
{
	.reg .pred 	%p<7>;
	.reg .f32 	%f<53>;
	.reg .b32 	%r<17>;
	.reg .b64 	%rd<10>;


	ld.param.u64 	%rd2, [unary_asinh_f32_param_0];
	ld.param.u64 	%rd3, [unary_asinh_f32_param_1];
	ld.param.u32 	%r3, [unary_asinh_f32_param_2];
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %ntid.x;
	mov.u32 	%r6, %tid.x;
	mad.lo.s32 	%r1, %r4, %r5, %r6;
	setp.ge.s32 	%p1, %r1, %r3;
	@%p1 bra 	$L__BB26_8;

	cvta.to.global.u64 	%rd4, %rd2;
	cvt.s64.s32 	%rd1, %r1;
	mul.wide.s32 	%rd5, %r1, 4;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.f32 	%f1, [%rd6];
	abs.f32 	%f2, %f1;
	setp.gt.f32 	%p2, %f2, 0f7E800000;
	@%p2 bra 	$L__BB26_6;
	bra.uni 	$L__BB26_2;

$L__BB26_6:
	lg2.approx.f32 	%f46, %f2;
	mul.f32 	%f47, %f46, 0f3F317218;
	mov.f32 	%f48, 0f3F317218;
	add.rn.f32 	%f52, %f48, %f47;
	bra.uni 	$L__BB26_7;

$L__BB26_2:
	rcp.rn.f32 	%f10, %f2;
	mov.f32 	%f11, 0f3F800000;
	fma.rn.f32 	%f12, %f10, %f10, %f11;
	sqrt.rn.f32 	%f13, %f12;
	add.f32 	%f14, %f10, %f13;
	rcp.approx.ftz.f32 	%f15, %f14;
	fma.rn.f32 	%f3, %f2, %f15, %f2;
	add.rz.f32 	%f16, %f3, %f11;
	mov.b32 	%r7, %f16;
	add.s32 	%r8, %r7, -1061158912;
	and.b32  	%r9, %r8, -8388608;
	mov.b32 	%r2, %f3;
	sub.s32 	%r10, %r2, %r9;
	mov.b32 	%f17, %r10;
	mov.u32 	%r11, 1082130432;
	sub.s32 	%r12, %r11, %r9;
	mov.b32 	%f18, %r12;
	mov.f32 	%f19, 0fBF800000;
	mov.f32 	%f20, 0f3E800000;
	fma.rn.f32 	%f21, %f20, %f18, %f19;
	add.f32 	%f22, %f21, %f17;
	cvt.rn.f32.s32 	%f23, %r9;
	mul.f32 	%f24, %f23, 0f34000000;
	mov.f32 	%f25, 0f3DD80012;
	mov.f32 	%f26, 0fBD39BF78;
	fma.rn.f32 	%f27, %f26, %f22, %f25;
	mov.f32 	%f28, 0fBE0778E0;
	fma.rn.f32 	%f29, %f27, %f22, %f28;
	mov.f32 	%f30, 0f3E146475;
	fma.rn.f32 	%f31, %f29, %f22, %f30;
	mov.f32 	%f32, 0fBE2A68DD;
	fma.rn.f32 	%f33, %f31, %f22, %f32;
	mov.f32 	%f34, 0f3E4CAF9E;
	fma.rn.f32 	%f35, %f33, %f22, %f34;
	mov.f32 	%f36, 0fBE800042;
	fma.rn.f32 	%f37, %f35, %f22, %f36;
	mov.f32 	%f38, 0f3EAAAAE6;
	fma.rn.f32 	%f39, %f37, %f22, %f38;
	mov.f32 	%f40, 0fBF000000;
	fma.rn.f32 	%f41, %f39, %f22, %f40;
	mul.f32 	%f42, %f22, %f41;
	fma.rn.f32 	%f43, %f42, %f22, %f22;
	mov.f32 	%f44, 0f3F317218;
	fma.rn.f32 	%f52, %f24, %f44, %f43;
	setp.lt.u32 	%p3, %r2, 2139095040;
	@%p3 bra 	$L__BB26_7;

	setp.lt.s32 	%p4, %r2, -1082130431;
	@%p4 bra 	$L__BB26_5;

	mov.f32 	%f45, 0f7F800000;
	fma.rn.f32 	%f52, %f3, %f45, %f45;

$L__BB26_5:
	setp.eq.f32 	%p5, %f3, 0f00000000;
	selp.f32 	%f52, 0f80000000, %f52, %p5;

$L__BB26_7:
	mov.b32 	%r13, %f1;
	and.b32  	%r14, %r13, -2147483648;
	mov.b32 	%r15, %f52;
	or.b32  	%r16, %r14, %r15;
	mov.b32 	%f49, %r16;
	setp.le.f32 	%p6, %f2, 0f7F800000;
	selp.f32 	%f50, %f49, %f52, %p6;
	cvta.to.global.u64 	%rd7, %rd3;
	shl.b64 	%rd8, %rd1, 2;
	add.s64 	%rd9, %rd7, %rd8;
	st.global.f32 	[%rd9], %f50;

$L__BB26_8:
	ret;

}
	// .globl	unary_asinh_f16
.visible .entry unary_asinh_f16(
	.param .u64 unary_asinh_f16_param_0,
	.param .u64 unary_asinh_f16_param_1,
	.param .u32 unary_asinh_f16_param_2
)
{
	.reg .pred 	%p<7>;
	.reg .b16 	%rs<3>;
	.reg .f32 	%f<54>;
	.reg .b32 	%r<17>;
	.reg .b64 	%rd<10>;


	ld.param.u64 	%rd2, [unary_asinh_f16_param_0];
	ld.param.u64 	%rd3, [unary_asinh_f16_param_1];
	ld.param.u32 	%r3, [unary_asinh_f16_param_2];
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %ntid.x;
	mov.u32 	%r6, %tid.x;
	mad.lo.s32 	%r1, %r4, %r5, %r6;
	setp.ge.s32 	%p1, %r1, %r3;
	@%p1 bra 	$L__BB27_8;

	cvta.to.global.u64 	%rd4, %rd2;
	cvt.s64.s32 	%rd1, %r1;
	mul.wide.s32 	%rd5, %r1, 2;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.u16 	%rs1, [%rd6];
	// begin inline asm
	{  cvt.f32.f16 %f10, %rs1;}

	// end inline asm
	abs.f32 	%f2, %f10;
	setp.gt.f32 	%p2, %f2, 0f7E800000;
	@%p2 bra 	$L__BB27_6;
	bra.uni 	$L__BB27_2;

$L__BB27_6:
	lg2.approx.f32 	%f47, %f2;
	mul.f32 	%f48, %f47, 0f3F317218;
	mov.f32 	%f49, 0f3F317218;
	add.rn.f32 	%f53, %f49, %f48;
	bra.uni 	$L__BB27_7;

$L__BB27_2:
	rcp.rn.f32 	%f11, %f2;
	mov.f32 	%f12, 0f3F800000;
	fma.rn.f32 	%f13, %f11, %f11, %f12;
	sqrt.rn.f32 	%f14, %f13;
	add.f32 	%f15, %f11, %f14;
	rcp.approx.ftz.f32 	%f16, %f15;
	fma.rn.f32 	%f3, %f2, %f16, %f2;
	add.rz.f32 	%f17, %f3, %f12;
	mov.b32 	%r7, %f17;
	add.s32 	%r8, %r7, -1061158912;
	and.b32  	%r9, %r8, -8388608;
	mov.b32 	%r2, %f3;
	sub.s32 	%r10, %r2, %r9;
	mov.b32 	%f18, %r10;
	mov.u32 	%r11, 1082130432;
	sub.s32 	%r12, %r11, %r9;
	mov.b32 	%f19, %r12;
	mov.f32 	%f20, 0fBF800000;
	mov.f32 	%f21, 0f3E800000;
	fma.rn.f32 	%f22, %f21, %f19, %f20;
	add.f32 	%f23, %f22, %f18;
	cvt.rn.f32.s32 	%f24, %r9;
	mul.f32 	%f25, %f24, 0f34000000;
	mov.f32 	%f26, 0f3DD80012;
	mov.f32 	%f27, 0fBD39BF78;
	fma.rn.f32 	%f28, %f27, %f23, %f26;
	mov.f32 	%f29, 0fBE0778E0;
	fma.rn.f32 	%f30, %f28, %f23, %f29;
	mov.f32 	%f31, 0f3E146475;
	fma.rn.f32 	%f32, %f30, %f23, %f31;
	mov.f32 	%f33, 0fBE2A68DD;
	fma.rn.f32 	%f34, %f32, %f23, %f33;
	mov.f32 	%f35, 0f3E4CAF9E;
	fma.rn.f32 	%f36, %f34, %f23, %f35;
	mov.f32 	%f37, 0fBE800042;
	fma.rn.f32 	%f38, %f36, %f23, %f37;
	mov.f32 	%f39, 0f3EAAAAE6;
	fma.rn.f32 	%f40, %f38, %f23, %f39;
	mov.f32 	%f41, 0fBF000000;
	fma.rn.f32 	%f42, %f40, %f23, %f41;
	mul.f32 	%f43, %f23, %f42;
	fma.rn.f32 	%f44, %f43, %f23, %f23;
	mov.f32 	%f45, 0f3F317218;
	fma.rn.f32 	%f53, %f25, %f45, %f44;
	setp.lt.u32 	%p3, %r2, 2139095040;
	@%p3 bra 	$L__BB27_7;

	setp.lt.s32 	%p4, %r2, -1082130431;
	@%p4 bra 	$L__BB27_5;

	mov.f32 	%f46, 0f7F800000;
	fma.rn.f32 	%f53, %f3, %f46, %f46;

$L__BB27_5:
	setp.eq.f32 	%p5, %f3, 0f00000000;
	selp.f32 	%f53, 0f80000000, %f53, %p5;

$L__BB27_7:
	mov.b32 	%r13, %f10;
	and.b32  	%r14, %r13, -2147483648;
	mov.b32 	%r15, %f53;
	or.b32  	%r16, %r14, %r15;
	mov.b32 	%f51, %r16;
	setp.le.f32 	%p6, %f2, 0f7F800000;
	selp.f32 	%f50, %f51, %f53, %p6;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs2, %f50;}

	// end inline asm
	cvta.to.global.u64 	%rd7, %rd3;
	shl.b64 	%rd8, %rd1, 1;
	add.s64 	%rd9, %rd7, %rd8;
	st.global.u16 	[%rd9], %rs2;

$L__BB27_8:
	ret;

}
	// .globl	unary_cos_f32
.visible .entry unary_cos_f32(
	.param .u64 unary_cos_f32_param_0,
	.param .u64 unary_cos_f32_param_1,
	.param .u32 unary_cos_f32_param_2
)
{
	.local .align 4 .b8 	__local_depot28[28];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<12>;
	.reg .f32 	%f<38>;
	.reg .b32 	%r<60>;
	.reg .f64 	%fd<3>;
	.reg .b64 	%rd<31>;


	mov.u64 	%SPL, __local_depot28;
	ld.param.u64 	%rd9, [unary_cos_f32_param_0];
	ld.param.u64 	%rd10, [unary_cos_f32_param_1];
	ld.param.u32 	%r21, [unary_cos_f32_param_2];
	add.u64 	%rd1, %SPL, 0;
	mov.u32 	%r22, %ntid.x;
	mov.u32 	%r23, %ctaid.x;
	mov.u32 	%r24, %tid.x;
	mad.lo.s32 	%r1, %r23, %r22, %r24;
	setp.ge.s32 	%p1, %r1, %r21;
	@%p1 bra 	$L__BB28_14;

	cvta.to.global.u64 	%rd12, %rd9;
	cvt.s64.s32 	%rd2, %r1;
	mul.wide.s32 	%rd13, %r1, 4;
	add.s64 	%rd14, %rd12, %rd13;
	ld.global.f32 	%f1, [%rd14];
	mul.f32 	%f14, %f1, 0f3F22F983;
	cvt.rni.s32.f32 	%r59, %f14;
	cvt.rn.f32.s32 	%f15, %r59;
	mov.f32 	%f16, 0fBFC90FDA;
	fma.rn.f32 	%f17, %f15, %f16, %f1;
	mov.f32 	%f18, 0fB3A22168;
	fma.rn.f32 	%f19, %f15, %f18, %f17;
	mov.f32 	%f20, 0fA7C234C5;
	fma.rn.f32 	%f35, %f15, %f20, %f19;
	abs.f32 	%f3, %f1;
	setp.ltu.f32 	%p2, %f3, 0f47CE4780;
	@%p2 bra 	$L__BB28_9;

	setp.eq.f32 	%p3, %f3, 0f7F800000;
	@%p3 bra 	$L__BB28_8;
	bra.uni 	$L__BB28_3;

$L__BB28_8:
	mov.f32 	%f23, 0f00000000;
	mul.rn.f32 	%f35, %f1, %f23;
	mov.u32 	%r59, 0;
	bra.uni 	$L__BB28_9;

$L__BB28_3:
	mov.b32 	%r3, %f1;
	shr.u32 	%r26, %r3, 23;
	and.b32  	%r27, %r26, 255;
	add.s32 	%r4, %r27, -128;
	shl.b32 	%r28, %r3, 8;
	or.b32  	%r5, %r28, -2147483648;
	shr.u32 	%r6, %r4, 5;
	mov.u64 	%rd30, 0;
	mov.u32 	%r56, 0;
	mov.u64 	%rd29, __cudart_i2opi_f;
	mov.u64 	%rd28, %rd1;

$L__BB28_4:
	.pragma "nounroll";
	ld.global.nc.u32 	%r29, [%rd29];
	mad.wide.u32 	%rd17, %r29, %r5, %rd30;
	shr.u64 	%rd30, %rd17, 32;
	st.local.u32 	[%rd28], %rd17;
	add.s64 	%rd29, %rd29, 4;
	add.s64 	%rd28, %rd28, 4;
	add.s32 	%r56, %r56, 1;
	setp.ne.s32 	%p4, %r56, 6;
	@%p4 bra 	$L__BB28_4;

	st.local.u32 	[%rd1+24], %rd30;
	mov.u32 	%r30, 4;
	sub.s32 	%r9, %r30, %r6;
	mov.u32 	%r31, 6;
	sub.s32 	%r32, %r31, %r6;
	mul.wide.s32 	%rd18, %r32, 4;
	add.s64 	%rd19, %rd1, %rd18;
	ld.local.u32 	%r57, [%rd19];
	ld.local.u32 	%r58, [%rd19+-4];
	and.b32  	%r12, %r4, 31;
	setp.eq.s32 	%p5, %r12, 0;
	@%p5 bra 	$L__BB28_7;

	mov.u32 	%r33, 32;
	sub.s32 	%r34, %r33, %r12;
	shr.u32 	%r35, %r58, %r34;
	shl.b32 	%r36, %r57, %r12;
	add.s32 	%r57, %r35, %r36;
	mul.wide.s32 	%rd20, %r9, 4;
	add.s64 	%rd21, %rd1, %rd20;
	ld.local.u32 	%r37, [%rd21];
	shr.u32 	%r38, %r37, %r34;
	shl.b32 	%r39, %r58, %r12;
	add.s32 	%r58, %r38, %r39;

$L__BB28_7:
	and.b32  	%r40, %r3, -2147483648;
	shr.u32 	%r41, %r58, 30;
	shl.b32 	%r42, %r57, 2;
	or.b32  	%r43, %r41, %r42;
	shr.u32 	%r44, %r43, 31;
	shr.u32 	%r45, %r57, 30;
	add.s32 	%r46, %r44, %r45;
	neg.s32 	%r47, %r46;
	setp.eq.s32 	%p6, %r40, 0;
	selp.b32 	%r59, %r46, %r47, %p6;
	setp.ne.s32 	%p7, %r44, 0;
	xor.b32  	%r48, %r40, -2147483648;
	selp.b32 	%r49, %r48, %r40, %p7;
	selp.b32 	%r50, -1, 0, %p7;
	xor.b32  	%r51, %r43, %r50;
	shl.b32 	%r52, %r58, 2;
	xor.b32  	%r53, %r52, %r50;
	cvt.u64.u32 	%rd22, %r51;
	cvt.u64.u32 	%rd23, %r53;
	bfi.b64 	%rd24, %rd22, %rd23, 32, 32;
	cvt.rn.f64.s64 	%fd1, %rd24;
	mul.f64 	%fd2, %fd1, 0d3BF921FB54442D19;
	cvt.rn.f32.f64 	%f21, %fd2;
	setp.eq.s32 	%p8, %r49, 0;
	neg.f32 	%f22, %f21;
	selp.f32 	%f35, %f21, %f22, %p8;

$L__BB28_9:
	add.s32 	%r19, %r59, 1;
	and.b32  	%r20, %r19, 1;
	setp.eq.s32 	%p9, %r20, 0;
	selp.f32 	%f7, %f35, 0f3F800000, %p9;
	mul.rn.f32 	%f8, %f35, %f35;
	mov.f32 	%f36, 0fB94D4153;
	@%p9 bra 	$L__BB28_11;

	mov.f32 	%f25, 0fBAB607ED;
	mov.f32 	%f26, 0f37CBAC00;
	fma.rn.f32 	%f36, %f26, %f8, %f25;

$L__BB28_11:
	selp.f32 	%f27, 0f3C0885E4, 0f3D2AAABB, %p9;
	fma.rn.f32 	%f28, %f36, %f8, %f27;
	selp.f32 	%f29, 0fBE2AAAA8, 0fBEFFFFFF, %p9;
	fma.rn.f32 	%f30, %f28, %f8, %f29;
	mov.f32 	%f31, 0f00000000;
	fma.rn.f32 	%f32, %f8, %f7, %f31;
	fma.rn.f32 	%f37, %f30, %f32, %f7;
	and.b32  	%r55, %r19, 2;
	setp.eq.s32 	%p11, %r55, 0;
	@%p11 bra 	$L__BB28_13;

	mov.f32 	%f34, 0fBF800000;
	fma.rn.f32 	%f37, %f37, %f34, %f31;

$L__BB28_13:
	cvta.to.global.u64 	%rd25, %rd10;
	shl.b64 	%rd26, %rd2, 2;
	add.s64 	%rd27, %rd25, %rd26;
	st.global.f32 	[%rd27], %f37;

$L__BB28_14:
	ret;

}
	// .globl	unary_cos_f16
.visible .entry unary_cos_f16(
	.param .u64 unary_cos_f16_param_0,
	.param .u64 unary_cos_f16_param_1,
	.param .u32 unary_cos_f16_param_2
)
{
	.reg .pred 	%p<4>;
	.reg .b16 	%rs<6>;
	.reg .f32 	%f<23>;
	.reg .b32 	%r<11>;
	.reg .b64 	%rd<8>;


	ld.param.u64 	%rd1, [unary_cos_f16_param_0];
	ld.param.u64 	%rd2, [unary_cos_f16_param_1];
	ld.param.u32 	%r2, [unary_cos_f16_param_2];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB29_2;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.s32 	%rd4, %r1, 2;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u16 	%rs1, [%rd5];
	// begin inline asm
	{  cvt.f32.f16 %f1, %rs1;}

	// end inline asm
	mov.f32 	%f3, 0f4B400000;
	mov.f32 	%f4, 0f3F22F983;
	fma.rn.f32 	%f5, %f1, %f4, %f3;
	mov.b32 	%r6, %f5;
	sub.rn.f32 	%f6, %f5, %f3;
	mov.f32 	%f7, 0fBFC90FDA;
	fma.rn.f32 	%f8, %f6, %f7, %f1;
	mov.f32 	%f9, 0fB3A22168;
	fma.rn.f32 	%f10, %f6, %f9, %f8;
	and.b32  	%r7, %r6, 3;
	add.s32 	%r8, %r7, 1;
	mul.f32 	%f11, %f10, %f10;
	and.b32  	%r9, %r8, 1;
	setp.eq.b32 	%p2, %r9, 1;
	selp.f32 	%f12, 0f37CCF5CE, 0fB94CA1F9, %p2;
	selp.f32 	%f13, 0fBAB6061A, 0f3C08839E, %p2;
	selp.f32 	%f14, 0f3D2AAAA5, 0fBE2AAAA3, %p2;
	selp.f32 	%f15, 0fBF000000, 0f00000000, %p2;
	selp.f32 	%f16, %f11, %f10, %p2;
	selp.f32 	%f17, 0f3F800000, %f10, %p2;
	fma.rn.f32 	%f18, %f12, %f11, %f13;
	fma.rn.f32 	%f19, %f18, %f11, %f14;
	fma.rn.f32 	%f20, %f19, %f11, %f15;
	fma.rn.f32 	%f21, %f20, %f16, %f17;
	and.b32  	%r10, %r8, 2;
	setp.eq.s32 	%p3, %r10, 0;
	neg.f32 	%f22, %f21;
	selp.f32 	%f2, %f21, %f22, %p3;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs3, %f2;}

	// end inline asm
	// begin inline asm
	{
	  .reg.b16 i,r;        
	  mov.b16 r, %rs3;       
	  mov.b16 i, %rs1;       
	  abs.f16 i, i;        
	{.reg.b16 spc, ulp, p;
  mov.b16 spc,0X2B7CU;
  mov.b16 ulp,0x1000U;
  set.eq.f16.f16 p,i, spc;
  fma.rn.f16 r,p,ulp,r;
}
  mov.b16 %rs3, r;       
}

	// end inline asm
	cvta.to.global.u64 	%rd6, %rd2;
	add.s64 	%rd7, %rd6, %rd4;
	st.global.u16 	[%rd7], %rs3;

$L__BB29_2:
	ret;

}
	// .globl	unary_cosh_f32
.visible .entry unary_cosh_f32(
	.param .u64 unary_cosh_f32_param_0,
	.param .u64 unary_cosh_f32_param_1,
	.param .u32 unary_cosh_f32_param_2
)
{
	.reg .pred 	%p<4>;
	.reg .f32 	%f<23>;
	.reg .b32 	%r<11>;
	.reg .b64 	%rd<8>;


	ld.param.u64 	%rd1, [unary_cosh_f32_param_0];
	ld.param.u64 	%rd2, [unary_cosh_f32_param_1];
	ld.param.u32 	%r2, [unary_cosh_f32_param_2];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB30_2;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.s32 	%rd4, %r1, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f32 	%f1, [%rd5];
	abs.f32 	%f2, %f1;
	mov.f32 	%f3, 0f3FB8AA3B;
	mul.rn.f32 	%f4, %f2, %f3;
	cvt.rzi.f32.f32 	%f5, %f4;
	abs.f32 	%f6, %f5;
	setp.gt.f32 	%p2, %f6, 0f42FC0000;
	mov.b32 	%r6, %f5;
	and.b32  	%r7, %r6, -2147483648;
	or.b32  	%r8, %r7, 1123811328;
	mov.b32 	%f7, %r8;
	selp.f32 	%f8, %f7, %f5, %p2;
	mov.f32 	%f9, 0fBF317218;
	fma.rn.f32 	%f10, %f8, %f9, %f2;
	mov.f32 	%f11, 0f3102E308;
	fma.rn.f32 	%f12, %f8, %f11, %f10;
	mul.f32 	%f13, %f12, 0f3FB8AA3B;
	add.f32 	%f14, %f8, 0f4B40007D;
	mov.b32 	%r9, %f14;
	shl.b32 	%r10, %r9, 23;
	mov.b32 	%f15, %r10;
	ex2.approx.ftz.f32 	%f16, %f13;
	mul.f32 	%f17, %f16, %f15;
	mov.f32 	%f18, 0f3E000000;
	div.approx.f32 	%f19, %f18, %f17;
	mov.f32 	%f20, 0f40000000;
	fma.rn.f32 	%f21, %f20, %f17, %f19;
	setp.ge.f32 	%p3, %f2, 0f42B40000;
	selp.f32 	%f22, 0f7F800000, %f21, %p3;
	cvta.to.global.u64 	%rd6, %rd2;
	add.s64 	%rd7, %rd6, %rd4;
	st.global.f32 	[%rd7], %f22;

$L__BB30_2:
	ret;

}
	// .globl	unary_cosh_f16
.visible .entry unary_cosh_f16(
	.param .u64 unary_cosh_f16_param_0,
	.param .u64 unary_cosh_f16_param_1,
	.param .u32 unary_cosh_f16_param_2
)
{
	.reg .pred 	%p<4>;
	.reg .b16 	%rs<29>;
	.reg .f32 	%f<15>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<10>;


	ld.param.u64 	%rd2, [unary_cosh_f16_param_0];
	ld.param.u64 	%rd3, [unary_cosh_f16_param_1];
	ld.param.u32 	%r2, [unary_cosh_f16_param_2];
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r4, %r3, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB31_5;

	cvta.to.global.u64 	%rd4, %rd2;
	cvt.s64.s32 	%rd1, %r1;
	mul.wide.s32 	%rd5, %r1, 2;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.u16 	%rs6, [%rd6];
	// begin inline asm
	{.reg.b32         f, C, nZ;       
 .reg.b16         h,r;            
  mov.b16         h,%rs6;           
  cvt.f32.f16     f,h;            
  mov.b32         C, 0x3fb8aa3bU; 
  mov.b32         nZ, 0x80000000U;
  fma.rn.f32      f,f,C,nZ;       
  ex2.approx.ftz.f32  f,f;        
  cvt.rn.f16.f32      r,f;        
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0X1F79U;
  mov.b16 ulp,0x9400U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0X25CFU;
  mov.b16 ulp,0x9400U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0XC13BU;
  mov.b16 ulp,0x0400U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0XC1EFU;
  mov.b16 ulp,0x0200U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
  mov.b16         %rs5,r;           
}
	// end inline asm
	// begin inline asm
	{neg.f16 %rs7,%rs6;
}
	// end inline asm
	// begin inline asm
	{.reg.b32         f, C, nZ;       
 .reg.b16         h,r;            
  mov.b16         h,%rs7;           
  cvt.f32.f16     f,h;            
  mov.b32         C, 0x3fb8aa3bU; 
  mov.b32         nZ, 0x80000000U;
  fma.rn.f32      f,f,C,nZ;       
  ex2.approx.ftz.f32  f,f;        
  cvt.rn.f16.f32      r,f;        
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0X1F79U;
  mov.b16 ulp,0x9400U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0X25CFU;
  mov.b16 ulp,0x9400U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0XC13BU;
  mov.b16 ulp,0x0400U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0XC1EFU;
  mov.b16 ulp,0x0200U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
  mov.b16         %rs9,r;           
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs11,%rs5,%rs9;
}
	// end inline asm
	mov.f32 	%f5, 0f40000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs14, %f5;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f6, %rs11;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f7, %rs14;}

	// end inline asm
	// begin inline asm
	{rcp.approx.ftz.f32 %f8, %f7;
}
	// end inline asm
	mul.f32 	%f10, %f6, %f8;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs28, %f10;}

	// end inline asm
	// begin inline asm
	{abs.f16 %rs18,%rs28;
}
	// end inline asm
	mov.u16 	%rs22, 143;
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs18, %rs22;
  selp.u16 %rs20, 1, 0, __$temp3;}
	// end inline asm
	setp.eq.s16 	%p2, %rs20, 0;
	@%p2 bra 	$L__BB31_4;

	mov.f32 	%f11, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs23, %f11;}

	// end inline asm
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs23, %rs18;
  selp.u16 %rs24, 1, 0, __$temp3;}
	// end inline asm
	setp.eq.s16 	%p3, %rs24, 0;
	@%p3 bra 	$L__BB31_4;

	neg.f32 	%f13, %f7;
	fma.rn.f32 	%f14, %f13, %f10, %f6;
	fma.rn.f32 	%f12, %f8, %f14, %f10;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs28, %f12;}

	// end inline asm

$L__BB31_4:
	cvta.to.global.u64 	%rd7, %rd3;
	shl.b64 	%rd8, %rd1, 1;
	add.s64 	%rd9, %rd7, %rd8;
	st.global.u16 	[%rd9], %rs28;

$L__BB31_5:
	ret;

}
	// .globl	unary_acos_f32
.visible .entry unary_acos_f32(
	.param .u64 unary_acos_f32_param_0,
	.param .u64 unary_acos_f32_param_1,
	.param .u32 unary_acos_f32_param_2
)
{
	.reg .pred 	%p<5>;
	.reg .f32 	%f<37>;
	.reg .b32 	%r<10>;
	.reg .b64 	%rd<8>;


	ld.param.u64 	%rd1, [unary_acos_f32_param_0];
	ld.param.u64 	%rd2, [unary_acos_f32_param_1];
	ld.param.u32 	%r2, [unary_acos_f32_param_2];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB32_2;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.s32 	%rd4, %r1, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f32 	%f1, [%rd5];
	abs.f32 	%f2, %f1;
	neg.f32 	%f3, %f2;
	mov.f32 	%f4, 0f3F000000;
	fma.rn.f32 	%f5, %f4, %f3, %f4;
	rsqrt.approx.ftz.f32 	%f6, %f5;
	mul.f32 	%f7, %f5, %f6;
	mul.f32 	%f8, %f6, 0f3F000000;
	neg.f32 	%f9, %f7;
	fma.rn.f32 	%f10, %f9, %f8, %f4;
	fma.rn.f32 	%f11, %f7, %f10, %f7;
	setp.eq.f32 	%p2, %f2, 0f3F800000;
	selp.f32 	%f12, 0f00000000, %f11, %p2;
	setp.gt.f32 	%p3, %f2, 0f3F0F5C29;
	selp.f32 	%f13, %f12, %f2, %p3;
	mov.b32 	%r6, %f13;
	mov.b32 	%r7, %f1;
	and.b32  	%r8, %r7, -2147483648;
	or.b32  	%r9, %r8, %r6;
	mov.b32 	%f14, %r9;
	mul.f32 	%f15, %f14, %f14;
	mov.f32 	%f16, 0f3C8B1ABB;
	mov.f32 	%f17, 0f3D10ECEF;
	fma.rn.f32 	%f18, %f17, %f15, %f16;
	mov.f32 	%f19, 0f3CFC028C;
	fma.rn.f32 	%f20, %f18, %f15, %f19;
	mov.f32 	%f21, 0f3D372139;
	fma.rn.f32 	%f22, %f20, %f15, %f21;
	mov.f32 	%f23, 0f3D9993DB;
	fma.rn.f32 	%f24, %f22, %f15, %f23;
	mov.f32 	%f25, 0f3E2AAAC6;
	fma.rn.f32 	%f26, %f24, %f15, %f25;
	mul.f32 	%f27, %f26, %f15;
	fma.rn.f32 	%f28, %f27, %f14, %f14;
	neg.f32 	%f29, %f28;
	selp.f32 	%f30, %f28, %f29, %p3;
	mov.f32 	%f31, 0f3FD774EB;
	mov.f32 	%f32, 0f3F6EE581;
	fma.rn.f32 	%f33, %f32, %f31, %f30;
	setp.gt.f32 	%p4, %f1, 0f3F0F5C29;
	selp.f32 	%f34, %f28, %f33, %p4;
	add.f32 	%f35, %f34, %f34;
	selp.f32 	%f36, %f35, %f34, %p3;
	cvta.to.global.u64 	%rd6, %rd2;
	add.s64 	%rd7, %rd6, %rd4;
	st.global.f32 	[%rd7], %f36;

$L__BB32_2:
	ret;

}
	// .globl	unary_acos_f16
.visible .entry unary_acos_f16(
	.param .u64 unary_acos_f16_param_0,
	.param .u64 unary_acos_f16_param_1,
	.param .u32 unary_acos_f16_param_2
)
{
	.reg .pred 	%p<5>;
	.reg .b16 	%rs<3>;
	.reg .f32 	%f<37>;
	.reg .b32 	%r<10>;
	.reg .b64 	%rd<8>;


	ld.param.u64 	%rd1, [unary_acos_f16_param_0];
	ld.param.u64 	%rd2, [unary_acos_f16_param_1];
	ld.param.u32 	%r2, [unary_acos_f16_param_2];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB33_2;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.s32 	%rd4, %r1, 2;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u16 	%rs1, [%rd5];
	// begin inline asm
	{  cvt.f32.f16 %f1, %rs1;}

	// end inline asm
	abs.f32 	%f3, %f1;
	neg.f32 	%f4, %f3;
	mov.f32 	%f5, 0f3F000000;
	fma.rn.f32 	%f6, %f5, %f4, %f5;
	rsqrt.approx.ftz.f32 	%f7, %f6;
	mul.f32 	%f8, %f6, %f7;
	mul.f32 	%f9, %f7, 0f3F000000;
	neg.f32 	%f10, %f8;
	fma.rn.f32 	%f11, %f10, %f9, %f5;
	fma.rn.f32 	%f12, %f8, %f11, %f8;
	setp.eq.f32 	%p2, %f3, 0f3F800000;
	selp.f32 	%f13, 0f00000000, %f12, %p2;
	setp.gt.f32 	%p3, %f3, 0f3F0F5C29;
	selp.f32 	%f14, %f13, %f3, %p3;
	mov.b32 	%r6, %f14;
	mov.b32 	%r7, %f1;
	and.b32  	%r8, %r7, -2147483648;
	or.b32  	%r9, %r8, %r6;
	mov.b32 	%f15, %r9;
	mul.f32 	%f16, %f15, %f15;
	mov.f32 	%f17, 0f3C8B1ABB;
	mov.f32 	%f18, 0f3D10ECEF;
	fma.rn.f32 	%f19, %f18, %f16, %f17;
	mov.f32 	%f20, 0f3CFC028C;
	fma.rn.f32 	%f21, %f19, %f16, %f20;
	mov.f32 	%f22, 0f3D372139;
	fma.rn.f32 	%f23, %f21, %f16, %f22;
	mov.f32 	%f24, 0f3D9993DB;
	fma.rn.f32 	%f25, %f23, %f16, %f24;
	mov.f32 	%f26, 0f3E2AAAC6;
	fma.rn.f32 	%f27, %f25, %f16, %f26;
	mul.f32 	%f28, %f27, %f16;
	fma.rn.f32 	%f29, %f28, %f15, %f15;
	neg.f32 	%f30, %f29;
	selp.f32 	%f31, %f29, %f30, %p3;
	mov.f32 	%f32, 0f3FD774EB;
	mov.f32 	%f33, 0f3F6EE581;
	fma.rn.f32 	%f34, %f33, %f32, %f31;
	setp.gt.f32 	%p4, %f1, 0f3F0F5C29;
	selp.f32 	%f35, %f29, %f34, %p4;
	add.f32 	%f36, %f35, %f35;
	selp.f32 	%f2, %f36, %f35, %p3;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs2, %f2;}

	// end inline asm
	cvta.to.global.u64 	%rd6, %rd2;
	add.s64 	%rd7, %rd6, %rd4;
	st.global.u16 	[%rd7], %rs2;

$L__BB33_2:
	ret;

}
	// .globl	unary_acosh_f32
.visible .entry unary_acosh_f32(
	.param .u64 unary_acosh_f32_param_0,
	.param .u64 unary_acosh_f32_param_1,
	.param .u32 unary_acosh_f32_param_2
)
{
	.reg .pred 	%p<9>;
	.reg .f32 	%f<80>;
	.reg .b32 	%r<18>;
	.reg .b64 	%rd<10>;


	ld.param.u64 	%rd2, [unary_acosh_f32_param_0];
	ld.param.u64 	%rd3, [unary_acosh_f32_param_1];
	ld.param.u32 	%r3, [unary_acosh_f32_param_2];
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %ntid.x;
	mov.u32 	%r6, %tid.x;
	mad.lo.s32 	%r1, %r4, %r5, %r6;
	setp.ge.s32 	%p1, %r1, %r3;
	@%p1 bra 	$L__BB34_10;

	cvta.to.global.u64 	%rd4, %rd2;
	cvt.s64.s32 	%rd1, %r1;
	mul.wide.s32 	%rd5, %r1, 4;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.f32 	%f1, [%rd6];
	add.f32 	%f2, %f1, 0fBF800000;
	mov.b32 	%r7, %f2;
	setp.gt.u32 	%p2, %r7, 1258291200;
	@%p2 bra 	$L__BB34_6;
	bra.uni 	$L__BB34_2;

$L__BB34_6:
	setp.lt.f32 	%p6, %f2, 0f00800000;
	mul.f32 	%f48, %f2, 0f4B000000;
	selp.f32 	%f8, %f48, %f2, %p6;
	selp.f32 	%f49, 0fC1B80000, 0f00000000, %p6;
	mov.b32 	%r14, %f8;
	add.s32 	%r15, %r14, -1059760811;
	and.b32  	%r16, %r15, -8388608;
	sub.s32 	%r17, %r14, %r16;
	mov.b32 	%f50, %r17;
	cvt.rn.f32.s32 	%f51, %r16;
	mov.f32 	%f52, 0f34000000;
	fma.rn.f32 	%f53, %f51, %f52, %f49;
	add.f32 	%f54, %f50, 0fBF800000;
	mov.f32 	%f55, 0f3E1039F6;
	mov.f32 	%f56, 0fBE055027;
	fma.rn.f32 	%f57, %f56, %f54, %f55;
	mov.f32 	%f58, 0fBDF8CDCC;
	fma.rn.f32 	%f59, %f57, %f54, %f58;
	mov.f32 	%f60, 0f3E0F2955;
	fma.rn.f32 	%f61, %f59, %f54, %f60;
	mov.f32 	%f62, 0fBE2AD8B9;
	fma.rn.f32 	%f63, %f61, %f54, %f62;
	mov.f32 	%f64, 0f3E4CED0B;
	fma.rn.f32 	%f65, %f63, %f54, %f64;
	mov.f32 	%f66, 0fBE7FFF22;
	fma.rn.f32 	%f67, %f65, %f54, %f66;
	mov.f32 	%f68, 0f3EAAAA78;
	fma.rn.f32 	%f69, %f67, %f54, %f68;
	mov.f32 	%f70, 0fBF000000;
	fma.rn.f32 	%f71, %f69, %f54, %f70;
	mul.f32 	%f72, %f54, %f71;
	fma.rn.f32 	%f73, %f72, %f54, %f54;
	mov.f32 	%f74, 0f3F317218;
	fma.rn.f32 	%f78, %f53, %f74, %f73;
	setp.lt.u32 	%p7, %r14, 2139095040;
	@%p7 bra 	$L__BB34_8;

	mov.f32 	%f75, 0f7F800000;
	fma.rn.f32 	%f78, %f8, %f75, %f75;

$L__BB34_8:
	add.f32 	%f76, %f78, 0f3F317218;
	setp.eq.f32 	%p8, %f8, 0f00000000;
	selp.f32 	%f79, 0fFF800000, %f76, %p8;
	bra.uni 	$L__BB34_9;

$L__BB34_2:
	mul.rz.f32 	%f14, %f1, %f2;
	add.rn.f32 	%f15, %f14, %f2;
	sqrt.rn.f32 	%f16, %f15;
	add.f32 	%f3, %f2, %f16;
	mov.f32 	%f17, 0f3F800000;
	add.rz.f32 	%f18, %f3, %f17;
	mov.b32 	%r8, %f18;
	add.s32 	%r9, %r8, -1061158912;
	and.b32  	%r10, %r9, -8388608;
	mov.b32 	%r2, %f3;
	sub.s32 	%r11, %r2, %r10;
	mov.b32 	%f19, %r11;
	mov.u32 	%r12, 1082130432;
	sub.s32 	%r13, %r12, %r10;
	mov.b32 	%f20, %r13;
	mov.f32 	%f21, 0fBF800000;
	mov.f32 	%f22, 0f3E800000;
	fma.rn.f32 	%f23, %f22, %f20, %f21;
	add.f32 	%f24, %f23, %f19;
	cvt.rn.f32.s32 	%f25, %r10;
	mul.f32 	%f26, %f25, 0f34000000;
	mov.f32 	%f27, 0f3DD80012;
	mov.f32 	%f28, 0fBD39BF78;
	fma.rn.f32 	%f29, %f28, %f24, %f27;
	mov.f32 	%f30, 0fBE0778E0;
	fma.rn.f32 	%f31, %f29, %f24, %f30;
	mov.f32 	%f32, 0f3E146475;
	fma.rn.f32 	%f33, %f31, %f24, %f32;
	mov.f32 	%f34, 0fBE2A68DD;
	fma.rn.f32 	%f35, %f33, %f24, %f34;
	mov.f32 	%f36, 0f3E4CAF9E;
	fma.rn.f32 	%f37, %f35, %f24, %f36;
	mov.f32 	%f38, 0fBE800042;
	fma.rn.f32 	%f39, %f37, %f24, %f38;
	mov.f32 	%f40, 0f3EAAAAE6;
	fma.rn.f32 	%f41, %f39, %f24, %f40;
	mov.f32 	%f42, 0fBF000000;
	fma.rn.f32 	%f43, %f41, %f24, %f42;
	mul.f32 	%f44, %f24, %f43;
	fma.rn.f32 	%f45, %f44, %f24, %f24;
	mov.f32 	%f46, 0f3F317218;
	fma.rn.f32 	%f79, %f26, %f46, %f45;
	setp.lt.u32 	%p3, %r2, 2139095040;
	@%p3 bra 	$L__BB34_9;

	setp.lt.s32 	%p4, %r2, -1082130431;
	@%p4 bra 	$L__BB34_5;

	mov.f32 	%f47, 0f7F800000;
	fma.rn.f32 	%f79, %f3, %f47, %f47;

$L__BB34_5:
	setp.eq.f32 	%p5, %f3, 0f00000000;
	selp.f32 	%f79, 0f80000000, %f79, %p5;

$L__BB34_9:
	cvta.to.global.u64 	%rd7, %rd3;
	shl.b64 	%rd8, %rd1, 2;
	add.s64 	%rd9, %rd7, %rd8;
	st.global.f32 	[%rd9], %f79;

$L__BB34_10:
	ret;

}
	// .globl	unary_acosh_f16
.visible .entry unary_acosh_f16(
	.param .u64 unary_acosh_f16_param_0,
	.param .u64 unary_acosh_f16_param_1,
	.param .u32 unary_acosh_f16_param_2
)
{
	.reg .pred 	%p<9>;
	.reg .b16 	%rs<3>;
	.reg .f32 	%f<82>;
	.reg .b32 	%r<18>;
	.reg .b64 	%rd<10>;


	ld.param.u64 	%rd2, [unary_acosh_f16_param_0];
	ld.param.u64 	%rd3, [unary_acosh_f16_param_1];
	ld.param.u32 	%r3, [unary_acosh_f16_param_2];
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %ntid.x;
	mov.u32 	%r6, %tid.x;
	mad.lo.s32 	%r1, %r4, %r5, %r6;
	setp.ge.s32 	%p1, %r1, %r3;
	@%p1 bra 	$L__BB35_10;

	cvta.to.global.u64 	%rd4, %rd2;
	cvt.s64.s32 	%rd1, %r1;
	mul.wide.s32 	%rd5, %r1, 2;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.u16 	%rs1, [%rd6];
	// begin inline asm
	{  cvt.f32.f16 %f14, %rs1;}

	// end inline asm
	add.f32 	%f2, %f14, 0fBF800000;
	mov.b32 	%r7, %f2;
	setp.gt.u32 	%p2, %r7, 1258291200;
	@%p2 bra 	$L__BB35_6;
	bra.uni 	$L__BB35_2;

$L__BB35_6:
	setp.lt.f32 	%p6, %f2, 0f00800000;
	mul.f32 	%f49, %f2, 0f4B000000;
	selp.f32 	%f8, %f49, %f2, %p6;
	selp.f32 	%f50, 0fC1B80000, 0f00000000, %p6;
	mov.b32 	%r14, %f8;
	add.s32 	%r15, %r14, -1059760811;
	and.b32  	%r16, %r15, -8388608;
	sub.s32 	%r17, %r14, %r16;
	mov.b32 	%f51, %r17;
	cvt.rn.f32.s32 	%f52, %r16;
	mov.f32 	%f53, 0f34000000;
	fma.rn.f32 	%f54, %f52, %f53, %f50;
	add.f32 	%f55, %f51, 0fBF800000;
	mov.f32 	%f56, 0f3E1039F6;
	mov.f32 	%f57, 0fBE055027;
	fma.rn.f32 	%f58, %f57, %f55, %f56;
	mov.f32 	%f59, 0fBDF8CDCC;
	fma.rn.f32 	%f60, %f58, %f55, %f59;
	mov.f32 	%f61, 0f3E0F2955;
	fma.rn.f32 	%f62, %f60, %f55, %f61;
	mov.f32 	%f63, 0fBE2AD8B9;
	fma.rn.f32 	%f64, %f62, %f55, %f63;
	mov.f32 	%f65, 0f3E4CED0B;
	fma.rn.f32 	%f66, %f64, %f55, %f65;
	mov.f32 	%f67, 0fBE7FFF22;
	fma.rn.f32 	%f68, %f66, %f55, %f67;
	mov.f32 	%f69, 0f3EAAAA78;
	fma.rn.f32 	%f70, %f68, %f55, %f69;
	mov.f32 	%f71, 0fBF000000;
	fma.rn.f32 	%f72, %f70, %f55, %f71;
	mul.f32 	%f73, %f55, %f72;
	fma.rn.f32 	%f74, %f73, %f55, %f55;
	mov.f32 	%f75, 0f3F317218;
	fma.rn.f32 	%f80, %f54, %f75, %f74;
	setp.lt.u32 	%p7, %r14, 2139095040;
	@%p7 bra 	$L__BB35_8;

	mov.f32 	%f76, 0f7F800000;
	fma.rn.f32 	%f80, %f8, %f76, %f76;

$L__BB35_8:
	add.f32 	%f77, %f80, 0f3F317218;
	setp.eq.f32 	%p8, %f8, 0f00000000;
	selp.f32 	%f81, 0fFF800000, %f77, %p8;
	bra.uni 	$L__BB35_9;

$L__BB35_2:
	mul.rz.f32 	%f15, %f14, %f2;
	add.rn.f32 	%f16, %f15, %f2;
	sqrt.rn.f32 	%f17, %f16;
	add.f32 	%f3, %f2, %f17;
	mov.f32 	%f18, 0f3F800000;
	add.rz.f32 	%f19, %f3, %f18;
	mov.b32 	%r8, %f19;
	add.s32 	%r9, %r8, -1061158912;
	and.b32  	%r10, %r9, -8388608;
	mov.b32 	%r2, %f3;
	sub.s32 	%r11, %r2, %r10;
	mov.b32 	%f20, %r11;
	mov.u32 	%r12, 1082130432;
	sub.s32 	%r13, %r12, %r10;
	mov.b32 	%f21, %r13;
	mov.f32 	%f22, 0fBF800000;
	mov.f32 	%f23, 0f3E800000;
	fma.rn.f32 	%f24, %f23, %f21, %f22;
	add.f32 	%f25, %f24, %f20;
	cvt.rn.f32.s32 	%f26, %r10;
	mul.f32 	%f27, %f26, 0f34000000;
	mov.f32 	%f28, 0f3DD80012;
	mov.f32 	%f29, 0fBD39BF78;
	fma.rn.f32 	%f30, %f29, %f25, %f28;
	mov.f32 	%f31, 0fBE0778E0;
	fma.rn.f32 	%f32, %f30, %f25, %f31;
	mov.f32 	%f33, 0f3E146475;
	fma.rn.f32 	%f34, %f32, %f25, %f33;
	mov.f32 	%f35, 0fBE2A68DD;
	fma.rn.f32 	%f36, %f34, %f25, %f35;
	mov.f32 	%f37, 0f3E4CAF9E;
	fma.rn.f32 	%f38, %f36, %f25, %f37;
	mov.f32 	%f39, 0fBE800042;
	fma.rn.f32 	%f40, %f38, %f25, %f39;
	mov.f32 	%f41, 0f3EAAAAE6;
	fma.rn.f32 	%f42, %f40, %f25, %f41;
	mov.f32 	%f43, 0fBF000000;
	fma.rn.f32 	%f44, %f42, %f25, %f43;
	mul.f32 	%f45, %f25, %f44;
	fma.rn.f32 	%f46, %f45, %f25, %f25;
	mov.f32 	%f47, 0f3F317218;
	fma.rn.f32 	%f81, %f27, %f47, %f46;
	setp.lt.u32 	%p3, %r2, 2139095040;
	@%p3 bra 	$L__BB35_9;

	setp.lt.s32 	%p4, %r2, -1082130431;
	@%p4 bra 	$L__BB35_5;

	mov.f32 	%f48, 0f7F800000;
	fma.rn.f32 	%f81, %f3, %f48, %f48;

$L__BB35_5:
	setp.eq.f32 	%p5, %f3, 0f00000000;
	selp.f32 	%f81, 0f80000000, %f81, %p5;

$L__BB35_9:
	// begin inline asm
	{  cvt.rn.f16.f32 %rs2, %f81;}

	// end inline asm
	cvta.to.global.u64 	%rd7, %rd3;
	shl.b64 	%rd8, %rd1, 1;
	add.s64 	%rd9, %rd7, %rd8;
	st.global.u16 	[%rd9], %rs2;

$L__BB35_10:
	ret;

}
	// .globl	unary_tan_f32
.visible .entry unary_tan_f32(
	.param .u64 unary_tan_f32_param_0,
	.param .u64 unary_tan_f32_param_1,
	.param .u32 unary_tan_f32_param_2
)
{
	.local .align 4 .b8 	__local_depot36[28];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<11>;
	.reg .f32 	%f<37>;
	.reg .b32 	%r<58>;
	.reg .f64 	%fd<3>;
	.reg .b64 	%rd<31>;


	mov.u64 	%SPL, __local_depot36;
	ld.param.u64 	%rd9, [unary_tan_f32_param_0];
	ld.param.u64 	%rd10, [unary_tan_f32_param_1];
	ld.param.u32 	%r19, [unary_tan_f32_param_2];
	add.u64 	%rd1, %SPL, 0;
	mov.u32 	%r20, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r1, %r21, %r20, %r22;
	setp.ge.s32 	%p1, %r1, %r19;
	@%p1 bra 	$L__BB36_10;

	cvta.to.global.u64 	%rd12, %rd9;
	cvt.s64.s32 	%rd2, %r1;
	mul.wide.s32 	%rd13, %r1, 4;
	add.s64 	%rd14, %rd12, %rd13;
	ld.global.f32 	%f1, [%rd14];
	mul.f32 	%f7, %f1, 0f3F22F983;
	cvt.rni.s32.f32 	%r57, %f7;
	cvt.rn.f32.s32 	%f8, %r57;
	mov.f32 	%f9, 0fBFC90FDA;
	fma.rn.f32 	%f10, %f8, %f9, %f1;
	mov.f32 	%f11, 0fB3A22168;
	fma.rn.f32 	%f12, %f8, %f11, %f10;
	mov.f32 	%f13, 0fA7C234C5;
	fma.rn.f32 	%f36, %f8, %f13, %f12;
	abs.f32 	%f3, %f1;
	setp.ltu.f32 	%p2, %f3, 0f47CE4780;
	@%p2 bra 	$L__BB36_9;

	setp.eq.f32 	%p3, %f3, 0f7F800000;
	@%p3 bra 	$L__BB36_8;
	bra.uni 	$L__BB36_3;

$L__BB36_8:
	mov.f32 	%f16, 0f00000000;
	mul.rn.f32 	%f36, %f1, %f16;
	mov.u32 	%r57, 0;
	bra.uni 	$L__BB36_9;

$L__BB36_3:
	mov.b32 	%r3, %f1;
	shr.u32 	%r24, %r3, 23;
	and.b32  	%r25, %r24, 255;
	add.s32 	%r4, %r25, -128;
	shl.b32 	%r26, %r3, 8;
	or.b32  	%r5, %r26, -2147483648;
	shr.u32 	%r6, %r4, 5;
	mov.u64 	%rd30, 0;
	mov.u32 	%r54, 0;
	mov.u64 	%rd29, __cudart_i2opi_f;
	mov.u64 	%rd28, %rd1;

$L__BB36_4:
	.pragma "nounroll";
	ld.global.nc.u32 	%r27, [%rd29];
	mad.wide.u32 	%rd17, %r27, %r5, %rd30;
	shr.u64 	%rd30, %rd17, 32;
	st.local.u32 	[%rd28], %rd17;
	add.s64 	%rd29, %rd29, 4;
	add.s64 	%rd28, %rd28, 4;
	add.s32 	%r54, %r54, 1;
	setp.ne.s32 	%p4, %r54, 6;
	@%p4 bra 	$L__BB36_4;

	st.local.u32 	[%rd1+24], %rd30;
	mov.u32 	%r28, 4;
	sub.s32 	%r9, %r28, %r6;
	mov.u32 	%r29, 6;
	sub.s32 	%r30, %r29, %r6;
	mul.wide.s32 	%rd18, %r30, 4;
	add.s64 	%rd19, %rd1, %rd18;
	ld.local.u32 	%r55, [%rd19];
	ld.local.u32 	%r56, [%rd19+-4];
	and.b32  	%r12, %r4, 31;
	setp.eq.s32 	%p5, %r12, 0;
	@%p5 bra 	$L__BB36_7;

	mov.u32 	%r31, 32;
	sub.s32 	%r32, %r31, %r12;
	shr.u32 	%r33, %r56, %r32;
	shl.b32 	%r34, %r55, %r12;
	add.s32 	%r55, %r33, %r34;
	mul.wide.s32 	%rd20, %r9, 4;
	add.s64 	%rd21, %rd1, %rd20;
	ld.local.u32 	%r35, [%rd21];
	shr.u32 	%r36, %r35, %r32;
	shl.b32 	%r37, %r56, %r12;
	add.s32 	%r56, %r36, %r37;

$L__BB36_7:
	and.b32  	%r38, %r3, -2147483648;
	shr.u32 	%r39, %r56, 30;
	shl.b32 	%r40, %r55, 2;
	or.b32  	%r41, %r39, %r40;
	shr.u32 	%r42, %r41, 31;
	shr.u32 	%r43, %r55, 30;
	add.s32 	%r44, %r42, %r43;
	neg.s32 	%r45, %r44;
	setp.eq.s32 	%p6, %r38, 0;
	selp.b32 	%r57, %r44, %r45, %p6;
	setp.ne.s32 	%p7, %r42, 0;
	xor.b32  	%r46, %r38, -2147483648;
	selp.b32 	%r47, %r46, %r38, %p7;
	selp.b32 	%r48, -1, 0, %p7;
	xor.b32  	%r49, %r41, %r48;
	shl.b32 	%r50, %r56, 2;
	xor.b32  	%r51, %r50, %r48;
	cvt.u64.u32 	%rd22, %r49;
	cvt.u64.u32 	%rd23, %r51;
	bfi.b64 	%rd24, %rd22, %rd23, 32, 32;
	cvt.rn.f64.s64 	%fd1, %rd24;
	mul.f64 	%fd2, %fd1, 0d3BF921FB54442D19;
	cvt.rn.f32.f64 	%f14, %fd2;
	setp.eq.s32 	%p8, %r47, 0;
	neg.f32 	%f15, %f14;
	selp.f32 	%f36, %f14, %f15, %p8;

$L__BB36_9:
	mul.f32 	%f17, %f36, %f36;
	mov.f32 	%f18, 0f3B560000;
	mov.f32 	%f19, 0f3C190000;
	fma.rn.f32 	%f20, %f19, %f17, %f18;
	mov.f32 	%f21, 0f3CC70000;
	fma.rn.f32 	%f22, %f20, %f17, %f21;
	mov.f32 	%f23, 0f3D5B0000;
	fma.rn.f32 	%f24, %f22, %f17, %f23;
	mov.f32 	%f25, 0f3E089438;
	fma.rn.f32 	%f26, %f24, %f17, %f25;
	mov.f32 	%f27, 0f3EAAAA88;
	fma.rn.f32 	%f28, %f26, %f17, %f27;
	mul.rn.f32 	%f29, %f17, %f36;
	fma.rn.f32 	%f30, %f28, %f29, %f36;
	abs.f32 	%f31, %f36;
	setp.eq.f32 	%p9, %f31, 0f3A00B43C;
	selp.f32 	%f32, %f36, %f30, %p9;
	and.b32  	%r53, %r57, 1;
	setp.eq.b32 	%p10, %r53, 1;
	neg.f32 	%f33, %f32;
	rcp.approx.ftz.f32 	%f34, %f33;
	selp.f32 	%f35, %f34, %f32, %p10;
	cvta.to.global.u64 	%rd25, %rd10;
	shl.b64 	%rd26, %rd2, 2;
	add.s64 	%rd27, %rd25, %rd26;
	st.global.f32 	[%rd27], %f35;

$L__BB36_10:
	ret;

}
	// .globl	unary_tan_f16
.visible .entry unary_tan_f16(
	.param .u64 unary_tan_f16_param_0,
	.param .u64 unary_tan_f16_param_1,
	.param .u32 unary_tan_f16_param_2
)
{
	.reg .pred 	%p<8>;
	.reg .b16 	%rs<29>;
	.reg .f32 	%f<54>;
	.reg .b32 	%r<14>;
	.reg .b64 	%rd<10>;


	ld.param.u64 	%rd2, [unary_tan_f16_param_0];
	ld.param.u64 	%rd3, [unary_tan_f16_param_1];
	ld.param.u32 	%r2, [unary_tan_f16_param_2];
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r4, %r3, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB37_5;

	cvta.to.global.u64 	%rd4, %rd2;
	cvt.s64.s32 	%rd1, %r1;
	mul.wide.s32 	%rd5, %r1, 2;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.u16 	%rs5, [%rd6];
	// begin inline asm
	{  cvt.f32.f16 %f5, %rs5;}

	// end inline asm
	mov.f32 	%f14, 0f4B400000;
	mov.f32 	%f15, 0f3F22F983;
	fma.rn.f32 	%f16, %f5, %f15, %f14;
	mov.b32 	%r6, %f16;
	sub.rn.f32 	%f17, %f16, %f14;
	mov.f32 	%f18, 0fBFC90FDA;
	fma.rn.f32 	%f19, %f17, %f18, %f5;
	mov.f32 	%f20, 0fB3A22168;
	fma.rn.f32 	%f21, %f17, %f20, %f19;
	mul.f32 	%f22, %f21, %f21;
	and.b32  	%r7, %r6, 1;
	setp.eq.b32 	%p2, %r7, 1;
	selp.f32 	%f23, 0f37CCF5CE, 0fB94CA1F9, %p2;
	selp.f32 	%f24, 0fBAB6061A, 0f3C08839E, %p2;
	selp.f32 	%f25, 0f3D2AAAA5, 0fBE2AAAA3, %p2;
	selp.f32 	%f26, 0fBF000000, 0f00000000, %p2;
	selp.f32 	%f27, %f22, %f21, %p2;
	selp.f32 	%f28, 0f3F800000, %f21, %p2;
	fma.rn.f32 	%f29, %f23, %f22, %f24;
	fma.rn.f32 	%f30, %f29, %f22, %f25;
	fma.rn.f32 	%f31, %f30, %f22, %f26;
	fma.rn.f32 	%f32, %f31, %f27, %f28;
	and.b32  	%r8, %r6, 2;
	setp.eq.s32 	%p3, %r8, 0;
	neg.f32 	%f33, %f32;
	selp.f32 	%f6, %f32, %f33, %p3;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs7, %f6;}

	// end inline asm
	// begin inline asm
	{
	  .reg.b16 i,r,t;     
	  mov.b16 r, %rs7;      
	  mov.b16 i, %rs5;      
	  and.b16 t, r, 0x8000U; 
	  abs.f16 r, r;   
	  abs.f16 i, i;   
	{.reg.b16 spc, ulp, p;
  mov.b16 spc,0X32B3U;
  mov.b16 ulp,0x0800U;
  set.eq.f16.f16 p,i, spc;
  fma.rn.f16 r,p,ulp,r;
}
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0X5CB0U;
  mov.b16 ulp,0x9000U;
  set.eq.f16.f16 p,i, spc;
  fma.rn.f16 r,p,ulp,r;
}
  or.b16  r,r,t;      
	  mov.b16 %rs7, r;      
}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f7, %rs5;}

	// end inline asm
	fma.rn.f32 	%f34, %f7, %f15, %f14;
	mov.b32 	%r9, %f34;
	sub.rn.f32 	%f35, %f34, %f14;
	fma.rn.f32 	%f36, %f35, %f18, %f7;
	fma.rn.f32 	%f37, %f35, %f20, %f36;
	and.b32  	%r10, %r9, 3;
	add.s32 	%r11, %r10, 1;
	mul.f32 	%f38, %f37, %f37;
	and.b32  	%r12, %r11, 1;
	setp.eq.b32 	%p4, %r12, 1;
	selp.f32 	%f39, 0f37CCF5CE, 0fB94CA1F9, %p4;
	selp.f32 	%f40, 0fBAB6061A, 0f3C08839E, %p4;
	selp.f32 	%f41, 0f3D2AAAA5, 0fBE2AAAA3, %p4;
	selp.f32 	%f42, 0fBF000000, 0f00000000, %p4;
	selp.f32 	%f43, %f38, %f37, %p4;
	selp.f32 	%f44, 0f3F800000, %f37, %p4;
	fma.rn.f32 	%f45, %f39, %f38, %f40;
	fma.rn.f32 	%f46, %f45, %f38, %f41;
	fma.rn.f32 	%f47, %f46, %f38, %f42;
	fma.rn.f32 	%f48, %f47, %f43, %f44;
	and.b32  	%r13, %r11, 2;
	setp.eq.s32 	%p5, %r13, 0;
	neg.f32 	%f49, %f48;
	selp.f32 	%f8, %f48, %f49, %p5;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs12, %f8;}

	// end inline asm
	// begin inline asm
	{
	  .reg.b16 i,r;        
	  mov.b16 r, %rs12;       
	  mov.b16 i, %rs5;       
	  abs.f16 i, i;        
	{.reg.b16 spc, ulp, p;
  mov.b16 spc,0X2B7CU;
  mov.b16 ulp,0x1000U;
  set.eq.f16.f16 p,i, spc;
  fma.rn.f16 r,p,ulp,r;
}
  mov.b16 %rs12, r;       
}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f9, %rs7;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f10, %rs12;}

	// end inline asm
	// begin inline asm
	{rcp.approx.ftz.f32 %f11, %f10;
}
	// end inline asm
	mul.f32 	%f13, %f9, %f11;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs28, %f13;}

	// end inline asm
	// begin inline asm
	{abs.f16 %rs18,%rs28;
}
	// end inline asm
	mov.u16 	%rs22, 143;
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs18, %rs22;
  selp.u16 %rs20, 1, 0, __$temp3;}
	// end inline asm
	setp.eq.s16 	%p6, %rs20, 0;
	@%p6 bra 	$L__BB37_4;

	mov.f32 	%f50, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs23, %f50;}

	// end inline asm
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs23, %rs18;
  selp.u16 %rs24, 1, 0, __$temp3;}
	// end inline asm
	setp.eq.s16 	%p7, %rs24, 0;
	@%p7 bra 	$L__BB37_4;

	neg.f32 	%f52, %f10;
	fma.rn.f32 	%f53, %f52, %f13, %f9;
	fma.rn.f32 	%f51, %f11, %f53, %f13;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs28, %f51;}

	// end inline asm

$L__BB37_4:
	cvta.to.global.u64 	%rd7, %rd3;
	shl.b64 	%rd8, %rd1, 1;
	add.s64 	%rd9, %rd7, %rd8;
	st.global.u16 	[%rd9], %rs28;

$L__BB37_5:
	ret;

}
	// .globl	unary_tanh_f32
.visible .entry unary_tanh_f32(
	.param .u64 unary_tanh_f32_param_0,
	.param .u64 unary_tanh_f32_param_1,
	.param .u32 unary_tanh_f32_param_2
)
{
	.reg .pred 	%p<4>;
	.reg .f32 	%f<24>;
	.reg .b32 	%r<10>;
	.reg .b64 	%rd<8>;


	ld.param.u64 	%rd1, [unary_tanh_f32_param_0];
	ld.param.u64 	%rd2, [unary_tanh_f32_param_1];
	ld.param.u32 	%r2, [unary_tanh_f32_param_2];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB38_2;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.s32 	%rd4, %r1, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f32 	%f1, [%rd5];
	abs.f32 	%f2, %f1;
	mul.f32 	%f3, %f2, 0f4038AA3B;
	ex2.approx.ftz.f32 	%f4, %f3;
	add.f32 	%f5, %f4, 0f3F800000;
	mov.f32 	%f6, 0f3F800000;
	rcp.approx.ftz.f32 	%f7, %f5;
	mov.f32 	%f8, 0fC0000000;
	fma.rn.f32 	%f9, %f7, %f8, %f6;
	setp.ge.f32 	%p2, %f2, 0f41102CB4;
	selp.f32 	%f10, 0f3F800000, %f9, %p2;
	mov.b32 	%r6, %f10;
	mov.b32 	%r7, %f1;
	and.b32  	%r8, %r7, -2147483648;
	or.b32  	%r9, %r8, %r6;
	mov.b32 	%f11, %r9;
	mul.f32 	%f12, %f1, %f1;
	mov.f32 	%f13, 0fBD563CAE;
	mov.f32 	%f14, 0f3C80F082;
	fma.rn.f32 	%f15, %f14, %f12, %f13;
	mov.f32 	%f16, 0f3E085941;
	fma.rn.f32 	%f17, %f15, %f12, %f16;
	mov.f32 	%f18, 0fBEAAA9ED;
	fma.rn.f32 	%f19, %f17, %f12, %f18;
	mov.f32 	%f20, 0f00000000;
	fma.rn.f32 	%f21, %f19, %f12, %f20;
	fma.rn.f32 	%f22, %f21, %f1, %f1;
	setp.ge.f32 	%p3, %f2, 0f3F19999A;
	selp.f32 	%f23, %f11, %f22, %p3;
	cvta.to.global.u64 	%rd6, %rd2;
	add.s64 	%rd7, %rd6, %rd4;
	st.global.f32 	[%rd7], %f23;

$L__BB38_2:
	ret;

}
	// .globl	unary_tanh_f16
.visible .entry unary_tanh_f16(
	.param .u64 unary_tanh_f16_param_0,
	.param .u64 unary_tanh_f16_param_1,
	.param .u32 unary_tanh_f16_param_2
)
{
	.reg .pred 	%p<4>;
	.reg .b16 	%rs<3>;
	.reg .f32 	%f<24>;
	.reg .b32 	%r<10>;
	.reg .b64 	%rd<8>;


	ld.param.u64 	%rd1, [unary_tanh_f16_param_0];
	ld.param.u64 	%rd2, [unary_tanh_f16_param_1];
	ld.param.u32 	%r2, [unary_tanh_f16_param_2];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB39_2;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.s32 	%rd4, %r1, 2;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u16 	%rs1, [%rd5];
	// begin inline asm
	{  cvt.f32.f16 %f1, %rs1;}

	// end inline asm
	abs.f32 	%f3, %f1;
	mul.f32 	%f4, %f3, 0f4038AA3B;
	ex2.approx.ftz.f32 	%f5, %f4;
	add.f32 	%f6, %f5, 0f3F800000;
	mov.f32 	%f7, 0f3F800000;
	rcp.approx.ftz.f32 	%f8, %f6;
	mov.f32 	%f9, 0fC0000000;
	fma.rn.f32 	%f10, %f8, %f9, %f7;
	setp.ge.f32 	%p2, %f3, 0f41102CB4;
	selp.f32 	%f11, 0f3F800000, %f10, %p2;
	mov.b32 	%r6, %f11;
	mov.b32 	%r7, %f1;
	and.b32  	%r8, %r7, -2147483648;
	or.b32  	%r9, %r8, %r6;
	mov.b32 	%f12, %r9;
	mul.f32 	%f13, %f1, %f1;
	mov.f32 	%f14, 0fBD563CAE;
	mov.f32 	%f15, 0f3C80F082;
	fma.rn.f32 	%f16, %f15, %f13, %f14;
	mov.f32 	%f17, 0f3E085941;
	fma.rn.f32 	%f18, %f16, %f13, %f17;
	mov.f32 	%f19, 0fBEAAA9ED;
	fma.rn.f32 	%f20, %f18, %f13, %f19;
	mov.f32 	%f21, 0f00000000;
	fma.rn.f32 	%f22, %f20, %f13, %f21;
	fma.rn.f32 	%f23, %f22, %f1, %f1;
	setp.ge.f32 	%p3, %f3, 0f3F19999A;
	selp.f32 	%f2, %f12, %f23, %p3;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs2, %f2;}

	// end inline asm
	cvta.to.global.u64 	%rd6, %rd2;
	add.s64 	%rd7, %rd6, %rd4;
	st.global.u16 	[%rd7], %rs2;

$L__BB39_2:
	ret;

}
	// .globl	unary_atan_f32
.visible .entry unary_atan_f32(
	.param .u64 unary_atan_f32_param_0,
	.param .u64 unary_atan_f32_param_1,
	.param .u32 unary_atan_f32_param_2
)
{
	.reg .pred 	%p<5>;
	.reg .f32 	%f<31>;
	.reg .b32 	%r<14>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [unary_atan_f32_param_0];
	ld.param.u64 	%rd2, [unary_atan_f32_param_1];
	ld.param.u32 	%r2, [unary_atan_f32_param_2];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB40_4;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.s32 	%rd4, %r1, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f32 	%f1, [%rd5];
	abs.f32 	%f2, %f1;
	setp.leu.f32 	%p2, %f2, 0f3F800000;
	setp.gt.f32 	%p3, %f2, 0f3F800000;
	rcp.approx.ftz.f32 	%f6, %f2;
	selp.f32 	%f7, %f6, %f2, %p3;
	mul.f32 	%f8, %f7, %f7;
	mov.f32 	%f9, 0fBC6BE14F;
	mov.f32 	%f10, 0f3B2090AA;
	fma.rn.f32 	%f11, %f10, %f8, %f9;
	mov.f32 	%f12, 0f3D23397E;
	fma.rn.f32 	%f13, %f11, %f8, %f12;
	mov.f32 	%f14, 0fBD948A7A;
	fma.rn.f32 	%f15, %f13, %f8, %f14;
	mov.f32 	%f16, 0f3DD76B21;
	fma.rn.f32 	%f17, %f15, %f8, %f16;
	mov.f32 	%f18, 0fBE111E88;
	fma.rn.f32 	%f19, %f17, %f8, %f18;
	mov.f32 	%f20, 0f3E4CAF60;
	fma.rn.f32 	%f21, %f19, %f8, %f20;
	mov.f32 	%f22, 0fBEAAAA27;
	fma.rn.f32 	%f23, %f21, %f8, %f22;
	mul.f32 	%f24, %f8, %f23;
	fma.rn.f32 	%f30, %f24, %f7, %f7;
	@%p2 bra 	$L__BB40_3;

	neg.f32 	%f25, %f30;
	mov.f32 	%f26, 0f3FD774EB;
	mov.f32 	%f27, 0f3F6EE581;
	fma.rn.f32 	%f30, %f27, %f26, %f25;

$L__BB40_3:
	mov.b32 	%r6, %f1;
	and.b32  	%r7, %r6, -2147483648;
	mov.b32 	%r8, %f30;
	or.b32  	%r9, %r7, %r8;
	mov.b32 	%f28, %r9;
	setp.le.f32 	%p4, %f2, 0f7F800000;
	selp.f32 	%f29, %f28, %f30, %p4;
	cvta.to.global.u64 	%rd6, %rd2;
	add.s64 	%rd8, %rd6, %rd4;
	st.global.f32 	[%rd8], %f29;

$L__BB40_4:
	ret;

}
	// .globl	unary_atan_f16
.visible .entry unary_atan_f16(
	.param .u64 unary_atan_f16_param_0,
	.param .u64 unary_atan_f16_param_1,
	.param .u32 unary_atan_f16_param_2
)
{
	.reg .pred 	%p<5>;
	.reg .b16 	%rs<3>;
	.reg .f32 	%f<32>;
	.reg .b32 	%r<14>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [unary_atan_f16_param_0];
	ld.param.u64 	%rd2, [unary_atan_f16_param_1];
	ld.param.u32 	%r2, [unary_atan_f16_param_2];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB41_4;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.s32 	%rd4, %r1, 2;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u16 	%rs1, [%rd5];
	// begin inline asm
	{  cvt.f32.f16 %f6, %rs1;}

	// end inline asm
	abs.f32 	%f2, %f6;
	setp.leu.f32 	%p2, %f2, 0f3F800000;
	setp.gt.f32 	%p3, %f2, 0f3F800000;
	rcp.approx.ftz.f32 	%f7, %f2;
	selp.f32 	%f8, %f7, %f2, %p3;
	mul.f32 	%f9, %f8, %f8;
	mov.f32 	%f10, 0fBC6BE14F;
	mov.f32 	%f11, 0f3B2090AA;
	fma.rn.f32 	%f12, %f11, %f9, %f10;
	mov.f32 	%f13, 0f3D23397E;
	fma.rn.f32 	%f14, %f12, %f9, %f13;
	mov.f32 	%f15, 0fBD948A7A;
	fma.rn.f32 	%f16, %f14, %f9, %f15;
	mov.f32 	%f17, 0f3DD76B21;
	fma.rn.f32 	%f18, %f16, %f9, %f17;
	mov.f32 	%f19, 0fBE111E88;
	fma.rn.f32 	%f20, %f18, %f9, %f19;
	mov.f32 	%f21, 0f3E4CAF60;
	fma.rn.f32 	%f22, %f20, %f9, %f21;
	mov.f32 	%f23, 0fBEAAAA27;
	fma.rn.f32 	%f24, %f22, %f9, %f23;
	mul.f32 	%f25, %f9, %f24;
	fma.rn.f32 	%f31, %f25, %f8, %f8;
	@%p2 bra 	$L__BB41_3;

	neg.f32 	%f26, %f31;
	mov.f32 	%f27, 0f3FD774EB;
	mov.f32 	%f28, 0f3F6EE581;
	fma.rn.f32 	%f31, %f28, %f27, %f26;

$L__BB41_3:
	mov.b32 	%r6, %f6;
	and.b32  	%r7, %r6, -2147483648;
	mov.b32 	%r8, %f31;
	or.b32  	%r9, %r7, %r8;
	mov.b32 	%f30, %r9;
	setp.le.f32 	%p4, %f2, 0f7F800000;
	selp.f32 	%f29, %f30, %f31, %p4;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs2, %f29;}

	// end inline asm
	cvta.to.global.u64 	%rd6, %rd2;
	add.s64 	%rd8, %rd6, %rd4;
	st.global.u16 	[%rd8], %rs2;

$L__BB41_4:
	ret;

}
	// .globl	unary_atanh_f32
.visible .entry unary_atanh_f32(
	.param .u64 unary_atanh_f32_param_0,
	.param .u64 unary_atanh_f32_param_1,
	.param .u32 unary_atanh_f32_param_2
)
{
	.reg .pred 	%p<7>;
	.reg .f32 	%f<50>;
	.reg .b32 	%r<21>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [unary_atanh_f32_param_0];
	ld.param.u64 	%rd2, [unary_atanh_f32_param_1];
	ld.param.u32 	%r3, [unary_atanh_f32_param_2];
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %ntid.x;
	mov.u32 	%r6, %tid.x;
	mad.lo.s32 	%r1, %r4, %r5, %r6;
	setp.ge.s32 	%p1, %r1, %r3;
	@%p1 bra 	$L__BB42_6;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.s32 	%rd4, %r1, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f32 	%f1, [%rd5];
	abs.f32 	%f8, %f1;
	mov.f32 	%f9, 0f3F800000;
	sub.f32 	%f10, %f9, %f8;
	rcp.approx.ftz.f32 	%f11, %f10;
	add.f32 	%f12, %f11, %f11;
	mul.f32 	%f13, %f8, %f12;
	setp.gt.f32 	%p2, %f8, 0f7E800000;
	selp.f32 	%f2, 0fC0000000, %f13, %p2;
	add.rz.f32 	%f14, %f2, %f9;
	mov.b32 	%r7, %f14;
	add.s32 	%r8, %r7, -1061158912;
	and.b32  	%r9, %r8, -8388608;
	mov.b32 	%r2, %f2;
	sub.s32 	%r10, %r2, %r9;
	mov.b32 	%f15, %r10;
	mov.u32 	%r11, 1082130432;
	sub.s32 	%r12, %r11, %r9;
	mov.b32 	%f16, %r12;
	mov.f32 	%f17, 0fBF800000;
	mov.f32 	%f18, 0f3E800000;
	fma.rn.f32 	%f19, %f18, %f16, %f17;
	add.f32 	%f20, %f19, %f15;
	cvt.rn.f32.s32 	%f21, %r9;
	mul.f32 	%f22, %f21, 0f34000000;
	mov.f32 	%f23, 0f3DD80012;
	mov.f32 	%f24, 0fBD39BF78;
	fma.rn.f32 	%f25, %f24, %f20, %f23;
	mov.f32 	%f26, 0fBE0778E0;
	fma.rn.f32 	%f27, %f25, %f20, %f26;
	mov.f32 	%f28, 0f3E146475;
	fma.rn.f32 	%f29, %f27, %f20, %f28;
	mov.f32 	%f30, 0fBE2A68DD;
	fma.rn.f32 	%f31, %f29, %f20, %f30;
	mov.f32 	%f32, 0f3E4CAF9E;
	fma.rn.f32 	%f33, %f31, %f20, %f32;
	mov.f32 	%f34, 0fBE800042;
	fma.rn.f32 	%f35, %f33, %f20, %f34;
	mov.f32 	%f36, 0f3EAAAAE6;
	fma.rn.f32 	%f37, %f35, %f20, %f36;
	mov.f32 	%f38, 0fBF000000;
	fma.rn.f32 	%f39, %f37, %f20, %f38;
	mul.f32 	%f40, %f20, %f39;
	fma.rn.f32 	%f41, %f40, %f20, %f20;
	mov.f32 	%f42, 0f3F317218;
	fma.rn.f32 	%f48, %f22, %f42, %f41;
	setp.lt.u32 	%p3, %r2, 2139095040;
	@%p3 bra 	$L__BB42_5;

	setp.lt.s32 	%p4, %r2, -1082130431;
	@%p4 bra 	$L__BB42_4;

	mov.f32 	%f43, 0f7F800000;
	fma.rn.f32 	%f48, %f2, %f43, %f43;

$L__BB42_4:
	setp.eq.f32 	%p5, %f2, 0f00000000;
	selp.f32 	%f48, 0f80000000, %f48, %p5;

$L__BB42_5:
	mul.f32 	%f44, %f48, 0f3F000000;
	abs.f32 	%f45, %f44;
	setp.le.f32 	%p6, %f45, 0f7F800000;
	mov.b32 	%r13, %f44;
	mov.b32 	%r14, %f1;
	and.b32  	%r15, %r14, -2147483648;
	or.b32  	%r16, %r15, %r13;
	mov.b32 	%f46, %r16;
	selp.f32 	%f47, %f46, %f44, %p6;
	cvta.to.global.u64 	%rd6, %rd2;
	add.s64 	%rd8, %rd6, %rd4;
	st.global.f32 	[%rd8], %f47;

$L__BB42_6:
	ret;

}
	// .globl	unary_atanh_f16
.visible .entry unary_atanh_f16(
	.param .u64 unary_atanh_f16_param_0,
	.param .u64 unary_atanh_f16_param_1,
	.param .u32 unary_atanh_f16_param_2
)
{
	.reg .pred 	%p<7>;
	.reg .b16 	%rs<3>;
	.reg .f32 	%f<51>;
	.reg .b32 	%r<21>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [unary_atanh_f16_param_0];
	ld.param.u64 	%rd2, [unary_atanh_f16_param_1];
	ld.param.u32 	%r3, [unary_atanh_f16_param_2];
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %ntid.x;
	mov.u32 	%r6, %tid.x;
	mad.lo.s32 	%r1, %r4, %r5, %r6;
	setp.ge.s32 	%p1, %r1, %r3;
	@%p1 bra 	$L__BB43_6;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.s32 	%rd4, %r1, 2;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u16 	%rs1, [%rd5];
	// begin inline asm
	{  cvt.f32.f16 %f8, %rs1;}

	// end inline asm
	abs.f32 	%f9, %f8;
	mov.f32 	%f10, 0f3F800000;
	sub.f32 	%f11, %f10, %f9;
	rcp.approx.ftz.f32 	%f12, %f11;
	add.f32 	%f13, %f12, %f12;
	mul.f32 	%f14, %f9, %f13;
	setp.gt.f32 	%p2, %f9, 0f7E800000;
	selp.f32 	%f2, 0fC0000000, %f14, %p2;
	add.rz.f32 	%f15, %f2, %f10;
	mov.b32 	%r7, %f15;
	add.s32 	%r8, %r7, -1061158912;
	and.b32  	%r9, %r8, -8388608;
	mov.b32 	%r2, %f2;
	sub.s32 	%r10, %r2, %r9;
	mov.b32 	%f16, %r10;
	mov.u32 	%r11, 1082130432;
	sub.s32 	%r12, %r11, %r9;
	mov.b32 	%f17, %r12;
	mov.f32 	%f18, 0fBF800000;
	mov.f32 	%f19, 0f3E800000;
	fma.rn.f32 	%f20, %f19, %f17, %f18;
	add.f32 	%f21, %f20, %f16;
	cvt.rn.f32.s32 	%f22, %r9;
	mul.f32 	%f23, %f22, 0f34000000;
	mov.f32 	%f24, 0f3DD80012;
	mov.f32 	%f25, 0fBD39BF78;
	fma.rn.f32 	%f26, %f25, %f21, %f24;
	mov.f32 	%f27, 0fBE0778E0;
	fma.rn.f32 	%f28, %f26, %f21, %f27;
	mov.f32 	%f29, 0f3E146475;
	fma.rn.f32 	%f30, %f28, %f21, %f29;
	mov.f32 	%f31, 0fBE2A68DD;
	fma.rn.f32 	%f32, %f30, %f21, %f31;
	mov.f32 	%f33, 0f3E4CAF9E;
	fma.rn.f32 	%f34, %f32, %f21, %f33;
	mov.f32 	%f35, 0fBE800042;
	fma.rn.f32 	%f36, %f34, %f21, %f35;
	mov.f32 	%f37, 0f3EAAAAE6;
	fma.rn.f32 	%f38, %f36, %f21, %f37;
	mov.f32 	%f39, 0fBF000000;
	fma.rn.f32 	%f40, %f38, %f21, %f39;
	mul.f32 	%f41, %f21, %f40;
	fma.rn.f32 	%f42, %f41, %f21, %f21;
	mov.f32 	%f43, 0f3F317218;
	fma.rn.f32 	%f49, %f23, %f43, %f42;
	setp.lt.u32 	%p3, %r2, 2139095040;
	@%p3 bra 	$L__BB43_5;

	setp.lt.s32 	%p4, %r2, -1082130431;
	@%p4 bra 	$L__BB43_4;

	mov.f32 	%f44, 0f7F800000;
	fma.rn.f32 	%f49, %f2, %f44, %f44;

$L__BB43_4:
	setp.eq.f32 	%p5, %f2, 0f00000000;
	selp.f32 	%f49, 0f80000000, %f49, %p5;

$L__BB43_5:
	mul.f32 	%f46, %f49, 0f3F000000;
	abs.f32 	%f47, %f46;
	setp.le.f32 	%p6, %f47, 0f7F800000;
	mov.b32 	%r13, %f46;
	mov.b32 	%r14, %f8;
	and.b32  	%r15, %r14, -2147483648;
	or.b32  	%r16, %r15, %r13;
	mov.b32 	%f48, %r16;
	selp.f32 	%f45, %f48, %f46, %p6;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs2, %f45;}

	// end inline asm
	cvta.to.global.u64 	%rd6, %rd2;
	add.s64 	%rd8, %rd6, %rd4;
	st.global.u16 	[%rd8], %rs2;

$L__BB43_6:
	ret;

}
	// .globl	unary_exp_f32
.visible .entry unary_exp_f32(
	.param .u64 unary_exp_f32_param_0,
	.param .u64 unary_exp_f32_param_1,
	.param .u32 unary_exp_f32_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<18>;
	.reg .b32 	%r<8>;
	.reg .b64 	%rd<8>;


	ld.param.u64 	%rd1, [unary_exp_f32_param_0];
	ld.param.u64 	%rd2, [unary_exp_f32_param_1];
	ld.param.u32 	%r2, [unary_exp_f32_param_2];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB44_2;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.s32 	%rd4, %r1, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f32 	%f1, [%rd5];
	mov.f32 	%f2, 0f3F000000;
	mov.f32 	%f3, 0f3BBB989D;
	fma.rn.f32 	%f4, %f1, %f3, %f2;
	cvt.sat.f32.f32 	%f5, %f4;
	mov.f32 	%f6, 0f4B400001;
	mov.f32 	%f7, 0f437C0000;
	fma.rm.f32 	%f8, %f5, %f7, %f6;
	add.f32 	%f9, %f8, 0fCB40007F;
	neg.f32 	%f10, %f9;
	mov.f32 	%f11, 0f3FB8AA3B;
	fma.rn.f32 	%f12, %f1, %f11, %f10;
	mov.f32 	%f13, 0f32A57060;
	fma.rn.f32 	%f14, %f1, %f13, %f12;
	mov.b32 	%r6, %f8;
	shl.b32 	%r7, %r6, 23;
	mov.b32 	%f15, %r7;
	ex2.approx.ftz.f32 	%f16, %f14;
	mul.f32 	%f17, %f16, %f15;
	cvta.to.global.u64 	%rd6, %rd2;
	add.s64 	%rd7, %rd6, %rd4;
	st.global.f32 	[%rd7], %f17;

$L__BB44_2:
	ret;

}
	// .globl	unary_exp_f16
.visible .entry unary_exp_f16(
	.param .u64 unary_exp_f16_param_0,
	.param .u64 unary_exp_f16_param_1,
	.param .u32 unary_exp_f16_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .b16 	%rs<3>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<8>;


	ld.param.u64 	%rd1, [unary_exp_f16_param_0];
	ld.param.u64 	%rd2, [unary_exp_f16_param_1];
	ld.param.u32 	%r2, [unary_exp_f16_param_2];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB45_2;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.s32 	%rd4, %r1, 2;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u16 	%rs2, [%rd5];
	// begin inline asm
	{.reg.b32         f, C, nZ;       
 .reg.b16         h,r;            
  mov.b16         h,%rs2;           
  cvt.f32.f16     f,h;            
  mov.b32         C, 0x3fb8aa3bU; 
  mov.b32         nZ, 0x80000000U;
  fma.rn.f32      f,f,C,nZ;       
  ex2.approx.ftz.f32  f,f;        
  cvt.rn.f16.f32      r,f;        
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0X1F79U;
  mov.b16 ulp,0x9400U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0X25CFU;
  mov.b16 ulp,0x9400U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0XC13BU;
  mov.b16 ulp,0x0400U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0XC1EFU;
  mov.b16 ulp,0x0200U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
  mov.b16         %rs1,r;           
}
	// end inline asm
	cvta.to.global.u64 	%rd6, %rd2;
	add.s64 	%rd7, %rd6, %rd4;
	st.global.u16 	[%rd7], %rs1;

$L__BB45_2:
	ret;

}
	// .globl	unary_sigmoid_f32
.visible .entry unary_sigmoid_f32(
	.param .u64 unary_sigmoid_f32_param_0,
	.param .u64 unary_sigmoid_f32_param_1,
	.param .u32 unary_sigmoid_f32_param_2
)
{
	.reg .pred 	%p<3>;
	.reg .f32 	%f<24>;
	.reg .b32 	%r<8>;
	.reg .b64 	%rd<8>;


	ld.param.u64 	%rd1, [unary_sigmoid_f32_param_0];
	ld.param.u64 	%rd2, [unary_sigmoid_f32_param_1];
	ld.param.u32 	%r2, [unary_sigmoid_f32_param_2];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB46_2;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.s32 	%rd4, %r1, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f32 	%f1, [%rd5];
	abs.f32 	%f2, %f1;
	neg.f32 	%f3, %f2;
	mov.f32 	%f4, 0f3F000000;
	mov.f32 	%f5, 0f3BBB989D;
	fma.rn.f32 	%f6, %f3, %f5, %f4;
	cvt.sat.f32.f32 	%f7, %f6;
	mov.f32 	%f8, 0f4B400001;
	mov.f32 	%f9, 0f437C0000;
	fma.rm.f32 	%f10, %f7, %f9, %f8;
	add.f32 	%f11, %f10, 0fCB40007F;
	neg.f32 	%f12, %f11;
	mov.f32 	%f13, 0f3FB8AA3B;
	fma.rn.f32 	%f14, %f3, %f13, %f12;
	mov.f32 	%f15, 0f32A57060;
	fma.rn.f32 	%f16, %f3, %f15, %f14;
	mov.b32 	%r6, %f10;
	shl.b32 	%r7, %r6, 23;
	mov.b32 	%f17, %r7;
	ex2.approx.ftz.f32 	%f18, %f16;
	fma.rn.f32 	%f19, %f18, %f17, 0f3F800000;
	mov.f32 	%f20, 0f3F800000;
	rcp.rn.f32 	%f21, %f19;
	setp.lt.f32 	%p2, %f1, 0f00000000;
	sub.f32 	%f22, %f20, %f21;
	selp.f32 	%f23, %f22, %f21, %p2;
	cvta.to.global.u64 	%rd6, %rd2;
	add.s64 	%rd7, %rd6, %rd4;
	st.global.f32 	[%rd7], %f23;

$L__BB46_2:
	ret;

}
	// .globl	unary_sigmoid_f16
.visible .entry unary_sigmoid_f16(
	.param .u64 unary_sigmoid_f16_param_0,
	.param .u64 unary_sigmoid_f16_param_1,
	.param .u32 unary_sigmoid_f16_param_2
)
{
	.reg .pred 	%p<5>;
	.reg .b16 	%rs<44>;
	.reg .f32 	%f<18>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<10>;


	ld.param.u64 	%rd2, [unary_sigmoid_f16_param_0];
	ld.param.u64 	%rd3, [unary_sigmoid_f16_param_1];
	ld.param.u32 	%r2, [unary_sigmoid_f16_param_2];
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r4, %r3, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB47_7;

	cvta.to.global.u64 	%rd4, %rd2;
	cvt.s64.s32 	%rd1, %r1;
	mul.wide.s32 	%rd5, %r1, 2;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.u16 	%rs1, [%rd6];
	mov.f32 	%f6, 0f3F800000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs10, %f6;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs11, %f6;}

	// end inline asm
	// begin inline asm
	{abs.f16 %rs12,%rs1;
}
	// end inline asm
	// begin inline asm
	{neg.f16 %rs14,%rs12;
}
	// end inline asm
	// begin inline asm
	{.reg.b32         f, C, nZ;       
 .reg.b16         h,r;            
  mov.b16         h,%rs14;           
  cvt.f32.f16     f,h;            
  mov.b32         C, 0x3fb8aa3bU; 
  mov.b32         nZ, 0x80000000U;
  fma.rn.f32      f,f,C,nZ;       
  ex2.approx.ftz.f32  f,f;        
  cvt.rn.f16.f32      r,f;        
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0X1F79U;
  mov.b16 ulp,0x9400U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0X25CFU;
  mov.b16 ulp,0x9400U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0XC13BU;
  mov.b16 ulp,0x0400U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0XC1EFU;
  mov.b16 ulp,0x0200U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
  mov.b16         %rs16,r;           
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs18,%rs11,%rs16;
}
	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f7, %rs10;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f8, %rs18;}

	// end inline asm
	// begin inline asm
	{rcp.approx.ftz.f32 %f9, %f8;
}
	// end inline asm
	mul.f32 	%f11, %f7, %f9;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs42, %f11;}

	// end inline asm
	// begin inline asm
	{abs.f16 %rs24,%rs42;
}
	// end inline asm
	mov.u16 	%rs28, 143;
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs24, %rs28;
  selp.u16 %rs26, 1, 0, __$temp3;}
	// end inline asm
	setp.eq.s16 	%p2, %rs26, 0;
	@%p2 bra 	$L__BB47_4;

	mov.f32 	%f12, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs29, %f12;}

	// end inline asm
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs29, %rs24;
  selp.u16 %rs30, 1, 0, __$temp3;}
	// end inline asm
	setp.eq.s16 	%p3, %rs30, 0;
	@%p3 bra 	$L__BB47_4;

	neg.f32 	%f14, %f8;
	fma.rn.f32 	%f15, %f14, %f11, %f7;
	fma.rn.f32 	%f13, %f9, %f15, %f11;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs42, %f13;}

	// end inline asm

$L__BB47_4:
	mov.f32 	%f16, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs34, %f16;}

	// end inline asm
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs1, %rs34;
  selp.u16 %rs35, 1, 0, __$temp3;}
	// end inline asm
	setp.eq.s16 	%p4, %rs35, 0;
	@%p4 bra 	$L__BB47_6;

	mov.f32 	%f17, 0f3F800000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs38, %f17;}

	// end inline asm
	// begin inline asm
	{sub.f16 %rs42,%rs38,%rs42;
}
	// end inline asm

$L__BB47_6:
	cvta.to.global.u64 	%rd7, %rd3;
	shl.b64 	%rd8, %rd1, 1;
	add.s64 	%rd9, %rd7, %rd8;
	st.global.u16 	[%rd9], %rs42;

$L__BB47_7:
	ret;

}
	// .globl	unary_ln_f32
.visible .entry unary_ln_f32(
	.param .u64 unary_ln_f32_param_0,
	.param .u64 unary_ln_f32_param_1,
	.param .u32 unary_ln_f32_param_2
)
{
	.reg .pred 	%p<5>;
	.reg .f32 	%f<36>;
	.reg .b32 	%r<14>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [unary_ln_f32_param_0];
	ld.param.u64 	%rd2, [unary_ln_f32_param_1];
	ld.param.u32 	%r2, [unary_ln_f32_param_2];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB48_4;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.s32 	%rd4, %r1, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f32 	%f5, [%rd5];
	setp.lt.f32 	%p2, %f5, 0f00800000;
	mul.f32 	%f6, %f5, 0f4B000000;
	selp.f32 	%f1, %f6, %f5, %p2;
	selp.f32 	%f7, 0fC1B80000, 0f00000000, %p2;
	mov.b32 	%r6, %f1;
	add.s32 	%r7, %r6, -1059760811;
	and.b32  	%r8, %r7, -8388608;
	sub.s32 	%r9, %r6, %r8;
	mov.b32 	%f8, %r9;
	cvt.rn.f32.s32 	%f9, %r8;
	mov.f32 	%f10, 0f34000000;
	fma.rn.f32 	%f11, %f9, %f10, %f7;
	add.f32 	%f12, %f8, 0fBF800000;
	mov.f32 	%f13, 0f3E1039F6;
	mov.f32 	%f14, 0fBE055027;
	fma.rn.f32 	%f15, %f14, %f12, %f13;
	mov.f32 	%f16, 0fBDF8CDCC;
	fma.rn.f32 	%f17, %f15, %f12, %f16;
	mov.f32 	%f18, 0f3E0F2955;
	fma.rn.f32 	%f19, %f17, %f12, %f18;
	mov.f32 	%f20, 0fBE2AD8B9;
	fma.rn.f32 	%f21, %f19, %f12, %f20;
	mov.f32 	%f22, 0f3E4CED0B;
	fma.rn.f32 	%f23, %f21, %f12, %f22;
	mov.f32 	%f24, 0fBE7FFF22;
	fma.rn.f32 	%f25, %f23, %f12, %f24;
	mov.f32 	%f26, 0f3EAAAA78;
	fma.rn.f32 	%f27, %f25, %f12, %f26;
	mov.f32 	%f28, 0fBF000000;
	fma.rn.f32 	%f29, %f27, %f12, %f28;
	mul.f32 	%f30, %f12, %f29;
	fma.rn.f32 	%f31, %f30, %f12, %f12;
	mov.f32 	%f32, 0f3F317218;
	fma.rn.f32 	%f35, %f11, %f32, %f31;
	setp.lt.u32 	%p3, %r6, 2139095040;
	@%p3 bra 	$L__BB48_3;

	mov.f32 	%f33, 0f7F800000;
	fma.rn.f32 	%f35, %f1, %f33, %f33;

$L__BB48_3:
	setp.eq.f32 	%p4, %f1, 0f00000000;
	selp.f32 	%f34, 0fFF800000, %f35, %p4;
	cvta.to.global.u64 	%rd6, %rd2;
	add.s64 	%rd8, %rd6, %rd4;
	st.global.f32 	[%rd8], %f34;

$L__BB48_4:
	ret;

}
	// .globl	unary_ln_f16
.visible .entry unary_ln_f16(
	.param .u64 unary_ln_f16_param_0,
	.param .u64 unary_ln_f16_param_1,
	.param .u32 unary_ln_f16_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .b16 	%rs<3>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<8>;


	ld.param.u64 	%rd1, [unary_ln_f16_param_0];
	ld.param.u64 	%rd2, [unary_ln_f16_param_1];
	ld.param.u32 	%r2, [unary_ln_f16_param_2];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB49_2;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.s32 	%rd4, %r1, 2;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u16 	%rs2, [%rd5];
	// begin inline asm
	{.reg.b32         f, C;           
 .reg.b16         r,h;            
  mov.b16         h,%rs2;           
  cvt.f32.f16     f,h;            
  lg2.approx.ftz.f32  f,f;        
  mov.b32         C, 0x3f317218U;  
  mul.f32         f,f,C;          
  cvt.rn.f16.f32      r,f;        
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0X160DU;
  mov.b16 ulp,0x9C00U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0X3BFEU;
  mov.b16 ulp,0x8010U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0X3C0BU;
  mov.b16 ulp,0x8080U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0X6051U;
  mov.b16 ulp,0x1C00U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
  mov.b16         %rs1,r;           
}
	// end inline asm
	cvta.to.global.u64 	%rd6, %rd2;
	add.s64 	%rd7, %rd6, %rd4;
	st.global.u16 	[%rd7], %rs1;

$L__BB49_2:
	ret;

}
	// .globl	unary_erf_f32
.visible .entry unary_erf_f32(
	.param .u64 unary_erf_f32_param_0,
	.param .u64 unary_erf_f32_param_1,
	.param .u32 unary_erf_f32_param_2
)
{
	.reg .pred 	%p<4>;
	.reg .f32 	%f<27>;
	.reg .b32 	%r<10>;
	.reg .b64 	%rd<10>;


	ld.param.u64 	%rd2, [unary_erf_f32_param_0];
	ld.param.u64 	%rd3, [unary_erf_f32_param_1];
	ld.param.u32 	%r2, [unary_erf_f32_param_2];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB50_4;

	cvta.to.global.u64 	%rd4, %rd2;
	cvt.s64.s32 	%rd1, %r1;
	mul.wide.s32 	%rd5, %r1, 4;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.f32 	%f1, [%rd6];
	abs.f32 	%f5, %f1;
	setp.ltu.f32 	%p2, %f5, 0f3F8060FE;
	setp.ge.f32 	%p3, %f5, 0f3F8060FE;
	mul.f32 	%f6, %f1, %f1;
	selp.f32 	%f7, %f5, %f6, %p3;
	selp.f32 	%f8, 0f38EB4C3A, 0f38B1E96A, %p3;
	selp.f32 	%f9, 0fBAAE005B, 0fBA574D20, %p3;
	fma.rn.f32 	%f10, %f8, %f7, %f9;
	selp.f32 	%f11, 0f3C09919F, 0f3BAAD5EA, %p3;
	fma.rn.f32 	%f12, %f10, %f7, %f11;
	selp.f32 	%f13, 0fBD24D99A, 0fBCDC1BE7, %p3;
	fma.rn.f32 	%f14, %f12, %f7, %f13;
	selp.f32 	%f15, 0f3E235519, 0f3DE718AF, %p3;
	fma.rn.f32 	%f16, %f14, %f7, %f15;
	selp.f32 	%f17, 0f3F69B4F9, 0fBEC093AC, %p3;
	fma.rn.f32 	%f18, %f16, %f7, %f17;
	selp.f32 	%f19, 0f3F210A14, 0f3E0375D3, %p3;
	fma.rn.f32 	%f20, %f18, %f7, %f19;
	neg.f32 	%f21, %f5;
	selp.f32 	%f22, %f21, %f1, %p3;
	fma.rn.f32 	%f26, %f20, %f22, %f22;
	@%p2 bra 	$L__BB50_3;

	ex2.approx.ftz.f32 	%f23, %f26;
	mov.f32 	%f24, 0f3F800000;
	sub.f32 	%f25, %f24, %f23;
	mov.b32 	%r6, %f25;
	mov.b32 	%r7, %f1;
	and.b32  	%r8, %r7, -2147483648;
	or.b32  	%r9, %r8, %r6;
	mov.b32 	%f26, %r9;

$L__BB50_3:
	cvta.to.global.u64 	%rd7, %rd3;
	shl.b64 	%rd8, %rd1, 2;
	add.s64 	%rd9, %rd7, %rd8;
	st.global.f32 	[%rd9], %f26;

$L__BB50_4:
	ret;

}
	// .globl	unary_erf_f16
.visible .entry unary_erf_f16(
	.param .u64 unary_erf_f16_param_0,
	.param .u64 unary_erf_f16_param_1,
	.param .u32 unary_erf_f16_param_2
)
{
	.reg .pred 	%p<4>;
	.reg .b16 	%rs<3>;
	.reg .f32 	%f<29>;
	.reg .b32 	%r<10>;
	.reg .b64 	%rd<10>;


	ld.param.u64 	%rd2, [unary_erf_f16_param_0];
	ld.param.u64 	%rd3, [unary_erf_f16_param_1];
	ld.param.u32 	%r2, [unary_erf_f16_param_2];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB51_4;

	cvta.to.global.u64 	%rd4, %rd2;
	cvt.s64.s32 	%rd1, %r1;
	mul.wide.s32 	%rd5, %r1, 2;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.u16 	%rs1, [%rd6];
	// begin inline asm
	{  cvt.f32.f16 %f5, %rs1;}

	// end inline asm
	abs.f32 	%f6, %f5;
	setp.ltu.f32 	%p2, %f6, 0f3F8060FE;
	setp.ge.f32 	%p3, %f6, 0f3F8060FE;
	mul.f32 	%f7, %f5, %f5;
	selp.f32 	%f8, %f6, %f7, %p3;
	selp.f32 	%f9, 0f38EB4C3A, 0f38B1E96A, %p3;
	selp.f32 	%f10, 0fBAAE005B, 0fBA574D20, %p3;
	fma.rn.f32 	%f11, %f9, %f8, %f10;
	selp.f32 	%f12, 0f3C09919F, 0f3BAAD5EA, %p3;
	fma.rn.f32 	%f13, %f11, %f8, %f12;
	selp.f32 	%f14, 0fBD24D99A, 0fBCDC1BE7, %p3;
	fma.rn.f32 	%f15, %f13, %f8, %f14;
	selp.f32 	%f16, 0f3E235519, 0f3DE718AF, %p3;
	fma.rn.f32 	%f17, %f15, %f8, %f16;
	selp.f32 	%f18, 0f3F69B4F9, 0fBEC093AC, %p3;
	fma.rn.f32 	%f19, %f17, %f8, %f18;
	selp.f32 	%f20, 0f3F210A14, 0f3E0375D3, %p3;
	fma.rn.f32 	%f21, %f19, %f8, %f20;
	neg.f32 	%f22, %f6;
	selp.f32 	%f23, %f22, %f5, %p3;
	fma.rn.f32 	%f28, %f21, %f23, %f23;
	@%p2 bra 	$L__BB51_3;

	ex2.approx.ftz.f32 	%f24, %f28;
	mov.f32 	%f25, 0f3F800000;
	sub.f32 	%f26, %f25, %f24;
	mov.b32 	%r6, %f26;
	mov.b32 	%r7, %f5;
	and.b32  	%r8, %r7, -2147483648;
	or.b32  	%r9, %r8, %r6;
	mov.b32 	%f28, %r9;

$L__BB51_3:
	// begin inline asm
	{  cvt.rn.f16.f32 %rs2, %f28;}

	// end inline asm
	cvta.to.global.u64 	%rd7, %rd3;
	shl.b64 	%rd8, %rd1, 1;
	add.s64 	%rd9, %rd7, %rd8;
	st.global.u16 	[%rd9], %rs2;

$L__BB51_4:
	ret;

}
	// .globl	unary_silu_f32
.visible .entry unary_silu_f32(
	.param .u64 unary_silu_f32_param_0,
	.param .u64 unary_silu_f32_param_1,
	.param .u32 unary_silu_f32_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<20>;
	.reg .b32 	%r<8>;
	.reg .b64 	%rd<8>;


	ld.param.u64 	%rd1, [unary_silu_f32_param_0];
	ld.param.u64 	%rd2, [unary_silu_f32_param_1];
	ld.param.u32 	%r2, [unary_silu_f32_param_2];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB52_2;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.s32 	%rd4, %r1, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f32 	%f1, [%rd5];
	neg.f32 	%f2, %f1;
	mov.f32 	%f3, 0f3F000000;
	mov.f32 	%f4, 0f3BBB989D;
	fma.rn.f32 	%f5, %f2, %f4, %f3;
	cvt.sat.f32.f32 	%f6, %f5;
	mov.f32 	%f7, 0f4B400001;
	mov.f32 	%f8, 0f437C0000;
	fma.rm.f32 	%f9, %f6, %f8, %f7;
	add.f32 	%f10, %f9, 0fCB40007F;
	neg.f32 	%f11, %f10;
	mov.f32 	%f12, 0f3FB8AA3B;
	fma.rn.f32 	%f13, %f2, %f12, %f11;
	mov.f32 	%f14, 0f32A57060;
	fma.rn.f32 	%f15, %f2, %f14, %f13;
	mov.b32 	%r6, %f9;
	shl.b32 	%r7, %r6, 23;
	mov.b32 	%f16, %r7;
	ex2.approx.ftz.f32 	%f17, %f15;
	fma.rn.f32 	%f18, %f17, %f16, 0f3F800000;
	div.rn.f32 	%f19, %f1, %f18;
	cvta.to.global.u64 	%rd6, %rd2;
	add.s64 	%rd7, %rd6, %rd4;
	st.global.f32 	[%rd7], %f19;

$L__BB52_2:
	ret;

}
	// .globl	unary_silu_f16
.visible .entry unary_silu_f16(
	.param .u64 unary_silu_f16_param_0,
	.param .u64 unary_silu_f16_param_1,
	.param .u32 unary_silu_f16_param_2
)
{
	.reg .pred 	%p<4>;
	.reg .b16 	%rs<27>;
	.reg .f32 	%f<15>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<10>;


	ld.param.u64 	%rd2, [unary_silu_f16_param_0];
	ld.param.u64 	%rd3, [unary_silu_f16_param_1];
	ld.param.u32 	%r2, [unary_silu_f16_param_2];
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r4, %r3, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB53_5;

	cvta.to.global.u64 	%rd4, %rd2;
	cvt.s64.s32 	%rd1, %r1;
	mul.wide.s32 	%rd5, %r1, 2;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.u16 	%rs7, [%rd6];
	mov.f32 	%f5, 0f3F800000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs5, %f5;}

	// end inline asm
	// begin inline asm
	{neg.f16 %rs6,%rs7;
}
	// end inline asm
	// begin inline asm
	{.reg.b32         f, C, nZ;       
 .reg.b16         h,r;            
  mov.b16         h,%rs6;           
  cvt.f32.f16     f,h;            
  mov.b32         C, 0x3fb8aa3bU; 
  mov.b32         nZ, 0x80000000U;
  fma.rn.f32      f,f,C,nZ;       
  ex2.approx.ftz.f32  f,f;        
  cvt.rn.f16.f32      r,f;        
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0X1F79U;
  mov.b16 ulp,0x9400U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0X25CFU;
  mov.b16 ulp,0x9400U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0XC13BU;
  mov.b16 ulp,0x0400U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
{.reg.b16 spc, ulp, p;
  mov.b16 spc,0XC1EFU;
  mov.b16 ulp,0x0200U;
  set.eq.f16.f16 p,h, spc;
  fma.rn.f16 r,p,ulp,r;
}
  mov.b16         %rs8,r;           
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs10,%rs5,%rs8;
}
	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f6, %rs7;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f7, %rs10;}

	// end inline asm
	// begin inline asm
	{rcp.approx.ftz.f32 %f8, %f7;
}
	// end inline asm
	mul.f32 	%f10, %f6, %f8;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs26, %f10;}

	// end inline asm
	// begin inline asm
	{abs.f16 %rs16,%rs26;
}
	// end inline asm
	mov.u16 	%rs20, 143;
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs16, %rs20;
  selp.u16 %rs18, 1, 0, __$temp3;}
	// end inline asm
	setp.eq.s16 	%p2, %rs18, 0;
	@%p2 bra 	$L__BB53_4;

	mov.f32 	%f11, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs21, %f11;}

	// end inline asm
	// begin inline asm
	{ .reg .pred __$temp3;
  setp.lt.f16  __$temp3, %rs21, %rs16;
  selp.u16 %rs22, 1, 0, __$temp3;}
	// end inline asm
	setp.eq.s16 	%p3, %rs22, 0;
	@%p3 bra 	$L__BB53_4;

	neg.f32 	%f13, %f7;
	fma.rn.f32 	%f14, %f13, %f10, %f6;
	fma.rn.f32 	%f12, %f8, %f14, %f10;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs26, %f12;}

	// end inline asm

$L__BB53_4:
	cvta.to.global.u64 	%rd7, %rd3;
	shl.b64 	%rd8, %rd1, 1;
	add.s64 	%rd9, %rd7, %rd8;
	st.global.u16 	[%rd9], %rs26;

$L__BB53_5:
	ret;

}

