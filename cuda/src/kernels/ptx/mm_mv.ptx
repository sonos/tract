//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-35583870
// Cuda compilation tools, release 12.8, V12.8.93
// Based on NVVM 7.0.1
//

.version 8.7
.target sm_87
.address_size 64

	// .globl	ggml_matvec_f32_acc_f32_bs_32
.extern .shared .align 16 .b8 data_mmv[];

.visible .entry ggml_matvec_f32_acc_f32_bs_32(
	.param .u64 ggml_matvec_f32_acc_f32_bs_32_param_0,
	.param .u64 ggml_matvec_f32_acc_f32_bs_32_param_1,
	.param .u64 ggml_matvec_f32_acc_f32_bs_32_param_2,
	.param .u64 ggml_matvec_f32_acc_f32_bs_32_param_3,
	.param .u64 ggml_matvec_f32_acc_f32_bs_32_param_4,
	.param .u64 ggml_matvec_f32_acc_f32_bs_32_param_5,
	.param .u64 ggml_matvec_f32_acc_f32_bs_32_param_6,
	.param .u64 ggml_matvec_f32_acc_f32_bs_32_param_7,
	.param .u64 ggml_matvec_f32_acc_f32_bs_32_param_8,
	.param .u64 ggml_matvec_f32_acc_f32_bs_32_param_9
)
{
	.reg .pred 	%p<13>;
	.reg .f32 	%f<75>;
	.reg .b32 	%r<25>;
	.reg .b64 	%rd<78>;


	ld.param.u64 	%rd38, [ggml_matvec_f32_acc_f32_bs_32_param_0];
	ld.param.u64 	%rd39, [ggml_matvec_f32_acc_f32_bs_32_param_1];
	ld.param.u64 	%rd31, [ggml_matvec_f32_acc_f32_bs_32_param_2];
	ld.param.u64 	%rd32, [ggml_matvec_f32_acc_f32_bs_32_param_3];
	ld.param.u64 	%rd33, [ggml_matvec_f32_acc_f32_bs_32_param_5];
	ld.param.u64 	%rd34, [ggml_matvec_f32_acc_f32_bs_32_param_6];
	ld.param.u64 	%rd35, [ggml_matvec_f32_acc_f32_bs_32_param_7];
	ld.param.u64 	%rd36, [ggml_matvec_f32_acc_f32_bs_32_param_8];
	ld.param.u64 	%rd37, [ggml_matvec_f32_acc_f32_bs_32_param_9];
	cvta.to.global.u64 	%rd1, %rd39;
	cvta.to.global.u64 	%rd2, %rd38;
	mov.u32 	%r1, %ctaid.x;
	cvt.u64.u32 	%rd3, %r1;
	mov.u32 	%r2, %ctaid.y;
	cvt.u64.u32 	%rd4, %r2;
	and.b64  	%rd40, %rd34, -4294967296;
	setp.eq.s64 	%p1, %rd40, 0;
	@%p1 bra 	$L__BB0_2;

	div.s64 	%rd69, %rd4, %rd34;
	bra.uni 	$L__BB0_3;

$L__BB0_2:
	cvt.u32.u64 	%r3, %rd34;
	cvt.u32.u64 	%r4, %rd4;
	div.u32 	%r5, %r4, %r3;
	cvt.u64.u32 	%rd69, %r5;

$L__BB0_3:
	mov.u32 	%r6, %tid.x;
	cvt.s64.s32 	%rd8, %r6;
	setp.ge.s64 	%p2, %rd8, %rd32;
	mov.f32 	%f74, 0f00000000;
	@%p2 bra 	$L__BB0_10;

	not.b64 	%rd41, %rd8;
	add.s64 	%rd9, %rd41, %rd32;
	shr.u64 	%rd42, %rd9, 5;
	add.s64 	%rd43, %rd42, 1;
	and.b64  	%rd10, %rd43, 3;
	setp.eq.s64 	%p3, %rd10, 0;
	mov.f32 	%f74, 0f00000000;
	mov.u64 	%rd74, %rd8;
	@%p3 bra 	$L__BB0_7;

	shl.b64 	%rd44, %rd8, 1;
	mul.lo.s64 	%rd45, %rd36, %rd4;
	add.s64 	%rd46, %rd44, %rd45;
	shl.b64 	%rd47, %rd46, 2;
	add.s64 	%rd72, %rd1, %rd47;
	mul.lo.s64 	%rd48, %rd69, %rd35;
	add.s64 	%rd49, %rd44, %rd48;
	mul.lo.s64 	%rd50, %rd33, %rd3;
	add.s64 	%rd51, %rd49, %rd50;
	shl.b64 	%rd52, %rd51, 2;
	add.s64 	%rd71, %rd2, %rd52;
	neg.s64 	%rd70, %rd10;
	mov.f32 	%f74, 0f00000000;
	mov.u64 	%rd74, %rd8;

$L__BB0_6:
	.pragma "nounroll";
	ld.global.nc.v2.f32 	{%f13, %f14}, [%rd71];
	ld.global.nc.v2.f32 	{%f17, %f18}, [%rd72];
	fma.rn.f32 	%f21, %f13, %f17, %f74;
	fma.rn.f32 	%f74, %f14, %f18, %f21;
	add.s64 	%rd74, %rd74, 32;
	add.s64 	%rd72, %rd72, 256;
	add.s64 	%rd71, %rd71, 256;
	add.s64 	%rd70, %rd70, 1;
	setp.ne.s64 	%p4, %rd70, 0;
	@%p4 bra 	$L__BB0_6;

$L__BB0_7:
	setp.lt.u64 	%p5, %rd9, 96;
	@%p5 bra 	$L__BB0_10;

	shl.b64 	%rd53, %rd74, 1;
	mul.lo.s64 	%rd54, %rd69, %rd35;
	mul.lo.s64 	%rd55, %rd33, %rd3;
	add.s64 	%rd56, %rd55, %rd54;
	add.s64 	%rd57, %rd56, %rd53;
	shl.b64 	%rd58, %rd57, 2;
	add.s64 	%rd59, %rd2, %rd58;
	add.s64 	%rd76, %rd59, 512;
	mul.lo.s64 	%rd60, %rd36, %rd4;
	add.s64 	%rd61, %rd53, %rd60;
	shl.b64 	%rd62, %rd61, 2;
	add.s64 	%rd63, %rd1, %rd62;
	add.s64 	%rd75, %rd63, 512;

$L__BB0_9:
	ld.global.nc.v2.f32 	{%f22, %f23}, [%rd76+-512];
	ld.global.nc.v2.f32 	{%f26, %f27}, [%rd75+-512];
	fma.rn.f32 	%f30, %f22, %f26, %f74;
	fma.rn.f32 	%f31, %f23, %f27, %f30;
	ld.global.nc.v2.f32 	{%f32, %f33}, [%rd76+-256];
	ld.global.nc.v2.f32 	{%f36, %f37}, [%rd75+-256];
	fma.rn.f32 	%f40, %f32, %f36, %f31;
	fma.rn.f32 	%f41, %f33, %f37, %f40;
	ld.global.nc.v2.f32 	{%f42, %f43}, [%rd76];
	ld.global.nc.v2.f32 	{%f46, %f47}, [%rd75];
	fma.rn.f32 	%f50, %f42, %f46, %f41;
	fma.rn.f32 	%f51, %f43, %f47, %f50;
	ld.global.nc.v2.f32 	{%f52, %f53}, [%rd76+256];
	ld.global.nc.v2.f32 	{%f56, %f57}, [%rd75+256];
	fma.rn.f32 	%f60, %f52, %f56, %f51;
	fma.rn.f32 	%f74, %f53, %f57, %f60;
	add.s64 	%rd76, %rd76, 1024;
	add.s64 	%rd75, %rd75, 1024;
	add.s64 	%rd74, %rd74, 128;
	setp.lt.s64 	%p6, %rd74, %rd32;
	@%p6 bra 	$L__BB0_9;

$L__BB0_10:
	mov.b32 	%r7, %f74;
	mov.u32 	%r8, 31;
	mov.u32 	%r9, 16;
	mov.u32 	%r10, -1;
	shfl.sync.bfly.b32 	%r11|%p7, %r7, %r9, %r8, %r10;
	mov.b32 	%f61, %r11;
	add.f32 	%f62, %f74, %f61;
	mov.b32 	%r12, %f62;
	mov.u32 	%r13, 8;
	shfl.sync.bfly.b32 	%r14|%p8, %r12, %r13, %r8, %r10;
	mov.b32 	%f63, %r14;
	add.f32 	%f64, %f62, %f63;
	mov.b32 	%r15, %f64;
	mov.u32 	%r16, 4;
	shfl.sync.bfly.b32 	%r17|%p9, %r15, %r16, %r8, %r10;
	mov.b32 	%f65, %r17;
	add.f32 	%f66, %f64, %f65;
	mov.b32 	%r18, %f66;
	mov.u32 	%r19, 2;
	shfl.sync.bfly.b32 	%r20|%p10, %r18, %r19, %r8, %r10;
	mov.b32 	%f67, %r20;
	add.f32 	%f68, %f66, %f67;
	mov.b32 	%r21, %f68;
	mov.u32 	%r22, 1;
	shfl.sync.bfly.b32 	%r23|%p11, %r21, %r22, %r8, %r10;
	mov.b32 	%f69, %r23;
	add.f32 	%f8, %f68, %f69;
	cvt.u32.u64 	%r24, %rd8;
	setp.ne.s32 	%p12, %r24, 0;
	@%p12 bra 	$L__BB0_12;

	mul.lo.s64 	%rd64, %rd4, %rd37;
	add.s64 	%rd65, %rd64, %rd3;
	cvta.to.global.u64 	%rd66, %rd31;
	shl.b64 	%rd67, %rd65, 2;
	add.s64 	%rd68, %rd66, %rd67;
	st.global.f32 	[%rd68], %f8;

$L__BB0_12:
	ret;

}
	// .globl	ggml_matvec_f32_acc_f32_bs_64
.visible .entry ggml_matvec_f32_acc_f32_bs_64(
	.param .u64 ggml_matvec_f32_acc_f32_bs_64_param_0,
	.param .u64 ggml_matvec_f32_acc_f32_bs_64_param_1,
	.param .u64 ggml_matvec_f32_acc_f32_bs_64_param_2,
	.param .u64 ggml_matvec_f32_acc_f32_bs_64_param_3,
	.param .u64 ggml_matvec_f32_acc_f32_bs_64_param_4,
	.param .u64 ggml_matvec_f32_acc_f32_bs_64_param_5,
	.param .u64 ggml_matvec_f32_acc_f32_bs_64_param_6,
	.param .u64 ggml_matvec_f32_acc_f32_bs_64_param_7,
	.param .u64 ggml_matvec_f32_acc_f32_bs_64_param_8,
	.param .u64 ggml_matvec_f32_acc_f32_bs_64_param_9
)
{
	.reg .pred 	%p<20>;
	.reg .f32 	%f<86>;
	.reg .b32 	%r<53>;
	.reg .b64 	%rd<79>;


	ld.param.u64 	%rd39, [ggml_matvec_f32_acc_f32_bs_64_param_0];
	ld.param.u64 	%rd40, [ggml_matvec_f32_acc_f32_bs_64_param_1];
	ld.param.u64 	%rd32, [ggml_matvec_f32_acc_f32_bs_64_param_2];
	ld.param.u64 	%rd33, [ggml_matvec_f32_acc_f32_bs_64_param_3];
	ld.param.u64 	%rd34, [ggml_matvec_f32_acc_f32_bs_64_param_5];
	ld.param.u64 	%rd35, [ggml_matvec_f32_acc_f32_bs_64_param_6];
	ld.param.u64 	%rd36, [ggml_matvec_f32_acc_f32_bs_64_param_7];
	ld.param.u64 	%rd37, [ggml_matvec_f32_acc_f32_bs_64_param_8];
	ld.param.u64 	%rd38, [ggml_matvec_f32_acc_f32_bs_64_param_9];
	cvta.to.global.u64 	%rd1, %rd40;
	cvta.to.global.u64 	%rd2, %rd39;
	mov.u32 	%r3, %ctaid.x;
	cvt.u64.u32 	%rd3, %r3;
	mov.u32 	%r4, %ctaid.y;
	cvt.u64.u32 	%rd4, %r4;
	and.b64  	%rd41, %rd35, -4294967296;
	setp.eq.s64 	%p1, %rd41, 0;
	@%p1 bra 	$L__BB1_2;

	div.s64 	%rd70, %rd4, %rd35;
	bra.uni 	$L__BB1_3;

$L__BB1_2:
	cvt.u32.u64 	%r5, %rd35;
	cvt.u32.u64 	%r6, %rd4;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd70, %r7;

$L__BB1_3:
	mov.u32 	%r1, %tid.x;
	setp.gt.s32 	%p2, %r1, 31;
	shl.b32 	%r8, %r1, 2;
	mov.u32 	%r9, data_mmv;
	add.s32 	%r2, %r9, %r8;
	@%p2 bra 	$L__BB1_5;

	mov.u32 	%r10, 0;
	st.shared.u32 	[%r2], %r10;

$L__BB1_5:
	bar.sync 	0;
	cvt.s64.s32 	%rd75, %r1;
	setp.ge.s64 	%p3, %rd75, %rd33;
	mov.f32 	%f85, 0f00000000;
	@%p3 bra 	$L__BB1_12;

	not.b64 	%rd42, %rd75;
	add.s64 	%rd9, %rd42, %rd33;
	shr.u64 	%rd43, %rd9, 6;
	add.s64 	%rd44, %rd43, 1;
	and.b64  	%rd10, %rd44, 3;
	setp.eq.s64 	%p4, %rd10, 0;
	mov.f32 	%f85, 0f00000000;
	@%p4 bra 	$L__BB1_9;

	cvt.s64.s32 	%rd75, %r1;
	mul.wide.s32 	%rd45, %r1, 2;
	mul.lo.s64 	%rd46, %rd37, %rd4;
	add.s64 	%rd47, %rd45, %rd46;
	shl.b64 	%rd48, %rd47, 2;
	add.s64 	%rd73, %rd1, %rd48;
	mul.lo.s64 	%rd49, %rd70, %rd36;
	add.s64 	%rd50, %rd45, %rd49;
	mul.lo.s64 	%rd51, %rd34, %rd3;
	add.s64 	%rd52, %rd50, %rd51;
	shl.b64 	%rd53, %rd52, 2;
	add.s64 	%rd72, %rd2, %rd53;
	neg.s64 	%rd71, %rd10;
	mov.f32 	%f85, 0f00000000;

$L__BB1_8:
	.pragma "nounroll";
	ld.global.nc.v2.f32 	{%f13, %f14}, [%rd72];
	ld.global.nc.v2.f32 	{%f17, %f18}, [%rd73];
	fma.rn.f32 	%f21, %f13, %f17, %f85;
	fma.rn.f32 	%f85, %f14, %f18, %f21;
	add.s64 	%rd75, %rd75, 64;
	add.s64 	%rd73, %rd73, 512;
	add.s64 	%rd72, %rd72, 512;
	add.s64 	%rd71, %rd71, 1;
	setp.ne.s64 	%p5, %rd71, 0;
	@%p5 bra 	$L__BB1_8;

$L__BB1_9:
	setp.lt.u64 	%p6, %rd9, 192;
	@%p6 bra 	$L__BB1_12;

	shl.b64 	%rd54, %rd75, 1;
	mul.lo.s64 	%rd55, %rd70, %rd36;
	mul.lo.s64 	%rd56, %rd34, %rd3;
	add.s64 	%rd57, %rd56, %rd55;
	add.s64 	%rd58, %rd57, %rd54;
	shl.b64 	%rd59, %rd58, 2;
	add.s64 	%rd60, %rd2, %rd59;
	add.s64 	%rd77, %rd60, 1024;
	mul.lo.s64 	%rd61, %rd37, %rd4;
	add.s64 	%rd62, %rd54, %rd61;
	shl.b64 	%rd63, %rd62, 2;
	add.s64 	%rd64, %rd1, %rd63;
	add.s64 	%rd76, %rd64, 1024;

$L__BB1_11:
	ld.global.nc.v2.f32 	{%f22, %f23}, [%rd77+-1024];
	ld.global.nc.v2.f32 	{%f26, %f27}, [%rd76+-1024];
	fma.rn.f32 	%f30, %f22, %f26, %f85;
	fma.rn.f32 	%f31, %f23, %f27, %f30;
	ld.global.nc.v2.f32 	{%f32, %f33}, [%rd77+-512];
	ld.global.nc.v2.f32 	{%f36, %f37}, [%rd76+-512];
	fma.rn.f32 	%f40, %f32, %f36, %f31;
	fma.rn.f32 	%f41, %f33, %f37, %f40;
	ld.global.nc.v2.f32 	{%f42, %f43}, [%rd77];
	ld.global.nc.v2.f32 	{%f46, %f47}, [%rd76];
	fma.rn.f32 	%f50, %f42, %f46, %f41;
	fma.rn.f32 	%f51, %f43, %f47, %f50;
	ld.global.nc.v2.f32 	{%f52, %f53}, [%rd77+512];
	ld.global.nc.v2.f32 	{%f56, %f57}, [%rd76+512];
	fma.rn.f32 	%f60, %f52, %f56, %f51;
	fma.rn.f32 	%f85, %f53, %f57, %f60;
	add.s64 	%rd77, %rd77, 2048;
	add.s64 	%rd76, %rd76, 2048;
	add.s64 	%rd75, %rd75, 256;
	setp.lt.s64 	%p7, %rd75, %rd33;
	@%p7 bra 	$L__BB1_11;

$L__BB1_12:
	mov.b32 	%r12, %f85;
	mov.u32 	%r13, 31;
	mov.u32 	%r14, 16;
	mov.u32 	%r15, -1;
	shfl.sync.bfly.b32 	%r16|%p8, %r12, %r14, %r13, %r15;
	mov.b32 	%f61, %r16;
	add.f32 	%f62, %f85, %f61;
	mov.b32 	%r17, %f62;
	mov.u32 	%r18, 8;
	shfl.sync.bfly.b32 	%r19|%p9, %r17, %r18, %r13, %r15;
	mov.b32 	%f63, %r19;
	add.f32 	%f64, %f62, %f63;
	mov.b32 	%r20, %f64;
	mov.u32 	%r21, 4;
	shfl.sync.bfly.b32 	%r22|%p10, %r20, %r21, %r13, %r15;
	mov.b32 	%f65, %r22;
	add.f32 	%f66, %f64, %f65;
	mov.b32 	%r23, %f66;
	mov.u32 	%r24, 2;
	shfl.sync.bfly.b32 	%r25|%p11, %r23, %r24, %r13, %r15;
	mov.b32 	%f67, %r25;
	add.f32 	%f68, %f66, %f67;
	mov.b32 	%r26, %f68;
	mov.u32 	%r27, 1;
	shfl.sync.bfly.b32 	%r28|%p12, %r26, %r27, %r13, %r15;
	mov.b32 	%f69, %r28;
	add.f32 	%f70, %f68, %f69;
	shr.s32 	%r29, %r1, 31;
	shr.u32 	%r30, %r29, 27;
	add.s32 	%r31, %r1, %r30;
	shr.s32 	%r32, %r31, 5;
	shl.b32 	%r33, %r32, 2;
	add.s32 	%r35, %r9, %r33;
	st.shared.f32 	[%r35], %f70;
	bar.sync 	0;
	@%p2 bra 	$L__BB1_15;

	ld.shared.f32 	%f71, [%r2];
	mov.b32 	%r36, %f71;
	shfl.sync.bfly.b32 	%r40|%p14, %r36, %r14, %r13, %r15;
	mov.b32 	%f72, %r40;
	add.f32 	%f73, %f71, %f72;
	mov.b32 	%r41, %f73;
	shfl.sync.bfly.b32 	%r43|%p15, %r41, %r18, %r13, %r15;
	mov.b32 	%f74, %r43;
	add.f32 	%f75, %f73, %f74;
	mov.b32 	%r44, %f75;
	shfl.sync.bfly.b32 	%r46|%p16, %r44, %r21, %r13, %r15;
	mov.b32 	%f76, %r46;
	add.f32 	%f77, %f75, %f76;
	mov.b32 	%r47, %f77;
	shfl.sync.bfly.b32 	%r49|%p17, %r47, %r24, %r13, %r15;
	mov.b32 	%f78, %r49;
	add.f32 	%f79, %f77, %f78;
	mov.b32 	%r50, %f79;
	shfl.sync.bfly.b32 	%r52|%p18, %r50, %r27, %r13, %r15;
	mov.b32 	%f80, %r52;
	add.f32 	%f8, %f79, %f80;
	setp.ne.s32 	%p19, %r1, 0;
	@%p19 bra 	$L__BB1_15;

	mul.lo.s64 	%rd65, %rd4, %rd38;
	add.s64 	%rd66, %rd65, %rd3;
	cvta.to.global.u64 	%rd67, %rd32;
	shl.b64 	%rd68, %rd66, 2;
	add.s64 	%rd69, %rd67, %rd68;
	st.global.f32 	[%rd69], %f8;

$L__BB1_15:
	ret;

}
	// .globl	ggml_matvec_f32_acc_f32_bs_96
.visible .entry ggml_matvec_f32_acc_f32_bs_96(
	.param .u64 ggml_matvec_f32_acc_f32_bs_96_param_0,
	.param .u64 ggml_matvec_f32_acc_f32_bs_96_param_1,
	.param .u64 ggml_matvec_f32_acc_f32_bs_96_param_2,
	.param .u64 ggml_matvec_f32_acc_f32_bs_96_param_3,
	.param .u64 ggml_matvec_f32_acc_f32_bs_96_param_4,
	.param .u64 ggml_matvec_f32_acc_f32_bs_96_param_5,
	.param .u64 ggml_matvec_f32_acc_f32_bs_96_param_6,
	.param .u64 ggml_matvec_f32_acc_f32_bs_96_param_7,
	.param .u64 ggml_matvec_f32_acc_f32_bs_96_param_8,
	.param .u64 ggml_matvec_f32_acc_f32_bs_96_param_9
)
{
	.reg .pred 	%p<20>;
	.reg .f32 	%f<86>;
	.reg .b32 	%r<53>;
	.reg .b64 	%rd<80>;


	ld.param.u64 	%rd39, [ggml_matvec_f32_acc_f32_bs_96_param_0];
	ld.param.u64 	%rd40, [ggml_matvec_f32_acc_f32_bs_96_param_1];
	ld.param.u64 	%rd32, [ggml_matvec_f32_acc_f32_bs_96_param_2];
	ld.param.u64 	%rd33, [ggml_matvec_f32_acc_f32_bs_96_param_3];
	ld.param.u64 	%rd34, [ggml_matvec_f32_acc_f32_bs_96_param_5];
	ld.param.u64 	%rd35, [ggml_matvec_f32_acc_f32_bs_96_param_6];
	ld.param.u64 	%rd36, [ggml_matvec_f32_acc_f32_bs_96_param_7];
	ld.param.u64 	%rd37, [ggml_matvec_f32_acc_f32_bs_96_param_8];
	ld.param.u64 	%rd38, [ggml_matvec_f32_acc_f32_bs_96_param_9];
	cvta.to.global.u64 	%rd1, %rd40;
	cvta.to.global.u64 	%rd2, %rd39;
	mov.u32 	%r3, %ctaid.x;
	cvt.u64.u32 	%rd3, %r3;
	mov.u32 	%r4, %ctaid.y;
	cvt.u64.u32 	%rd4, %r4;
	and.b64  	%rd41, %rd35, -4294967296;
	setp.eq.s64 	%p1, %rd41, 0;
	@%p1 bra 	$L__BB2_2;

	div.s64 	%rd71, %rd4, %rd35;
	bra.uni 	$L__BB2_3;

$L__BB2_2:
	cvt.u32.u64 	%r5, %rd35;
	cvt.u32.u64 	%r6, %rd4;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd71, %r7;

$L__BB2_3:
	mov.u32 	%r1, %tid.x;
	setp.gt.s32 	%p2, %r1, 31;
	shl.b32 	%r8, %r1, 2;
	mov.u32 	%r9, data_mmv;
	add.s32 	%r2, %r9, %r8;
	@%p2 bra 	$L__BB2_5;

	mov.u32 	%r10, 0;
	st.shared.u32 	[%r2], %r10;

$L__BB2_5:
	bar.sync 	0;
	cvt.s64.s32 	%rd76, %r1;
	setp.ge.s64 	%p3, %rd76, %rd33;
	mov.f32 	%f85, 0f00000000;
	@%p3 bra 	$L__BB2_12;

	not.b64 	%rd42, %rd76;
	add.s64 	%rd9, %rd42, %rd33;
	mul.hi.u64 	%rd43, %rd9, -6148914691236517205;
	shr.u64 	%rd44, %rd43, 6;
	add.s64 	%rd45, %rd44, 1;
	and.b64  	%rd10, %rd45, 3;
	setp.eq.s64 	%p4, %rd10, 0;
	mov.f32 	%f85, 0f00000000;
	@%p4 bra 	$L__BB2_9;

	cvt.s64.s32 	%rd76, %r1;
	mul.wide.s32 	%rd46, %r1, 2;
	mul.lo.s64 	%rd47, %rd37, %rd4;
	add.s64 	%rd48, %rd46, %rd47;
	shl.b64 	%rd49, %rd48, 2;
	add.s64 	%rd74, %rd1, %rd49;
	mul.lo.s64 	%rd50, %rd71, %rd36;
	add.s64 	%rd51, %rd46, %rd50;
	mul.lo.s64 	%rd52, %rd34, %rd3;
	add.s64 	%rd53, %rd51, %rd52;
	shl.b64 	%rd54, %rd53, 2;
	add.s64 	%rd73, %rd2, %rd54;
	neg.s64 	%rd72, %rd10;
	mov.f32 	%f85, 0f00000000;

$L__BB2_8:
	.pragma "nounroll";
	ld.global.nc.v2.f32 	{%f13, %f14}, [%rd73];
	ld.global.nc.v2.f32 	{%f17, %f18}, [%rd74];
	fma.rn.f32 	%f21, %f13, %f17, %f85;
	fma.rn.f32 	%f85, %f14, %f18, %f21;
	add.s64 	%rd76, %rd76, 96;
	add.s64 	%rd74, %rd74, 768;
	add.s64 	%rd73, %rd73, 768;
	add.s64 	%rd72, %rd72, 1;
	setp.ne.s64 	%p5, %rd72, 0;
	@%p5 bra 	$L__BB2_8;

$L__BB2_9:
	setp.lt.u64 	%p6, %rd9, 288;
	@%p6 bra 	$L__BB2_12;

	shl.b64 	%rd55, %rd76, 1;
	mul.lo.s64 	%rd56, %rd71, %rd36;
	mul.lo.s64 	%rd57, %rd34, %rd3;
	add.s64 	%rd58, %rd57, %rd56;
	add.s64 	%rd59, %rd58, %rd55;
	shl.b64 	%rd60, %rd59, 2;
	add.s64 	%rd61, %rd2, %rd60;
	add.s64 	%rd78, %rd61, 1536;
	mul.lo.s64 	%rd62, %rd37, %rd4;
	add.s64 	%rd63, %rd55, %rd62;
	shl.b64 	%rd64, %rd63, 2;
	add.s64 	%rd65, %rd1, %rd64;
	add.s64 	%rd77, %rd65, 1536;

$L__BB2_11:
	ld.global.nc.v2.f32 	{%f22, %f23}, [%rd78+-1536];
	ld.global.nc.v2.f32 	{%f26, %f27}, [%rd77+-1536];
	fma.rn.f32 	%f30, %f22, %f26, %f85;
	fma.rn.f32 	%f31, %f23, %f27, %f30;
	ld.global.nc.v2.f32 	{%f32, %f33}, [%rd78+-768];
	ld.global.nc.v2.f32 	{%f36, %f37}, [%rd77+-768];
	fma.rn.f32 	%f40, %f32, %f36, %f31;
	fma.rn.f32 	%f41, %f33, %f37, %f40;
	ld.global.nc.v2.f32 	{%f42, %f43}, [%rd78];
	ld.global.nc.v2.f32 	{%f46, %f47}, [%rd77];
	fma.rn.f32 	%f50, %f42, %f46, %f41;
	fma.rn.f32 	%f51, %f43, %f47, %f50;
	ld.global.nc.v2.f32 	{%f52, %f53}, [%rd78+768];
	ld.global.nc.v2.f32 	{%f56, %f57}, [%rd77+768];
	fma.rn.f32 	%f60, %f52, %f56, %f51;
	fma.rn.f32 	%f85, %f53, %f57, %f60;
	add.s64 	%rd78, %rd78, 3072;
	add.s64 	%rd77, %rd77, 3072;
	add.s64 	%rd76, %rd76, 384;
	setp.lt.s64 	%p7, %rd76, %rd33;
	@%p7 bra 	$L__BB2_11;

$L__BB2_12:
	mov.b32 	%r12, %f85;
	mov.u32 	%r13, 31;
	mov.u32 	%r14, 16;
	mov.u32 	%r15, -1;
	shfl.sync.bfly.b32 	%r16|%p8, %r12, %r14, %r13, %r15;
	mov.b32 	%f61, %r16;
	add.f32 	%f62, %f85, %f61;
	mov.b32 	%r17, %f62;
	mov.u32 	%r18, 8;
	shfl.sync.bfly.b32 	%r19|%p9, %r17, %r18, %r13, %r15;
	mov.b32 	%f63, %r19;
	add.f32 	%f64, %f62, %f63;
	mov.b32 	%r20, %f64;
	mov.u32 	%r21, 4;
	shfl.sync.bfly.b32 	%r22|%p10, %r20, %r21, %r13, %r15;
	mov.b32 	%f65, %r22;
	add.f32 	%f66, %f64, %f65;
	mov.b32 	%r23, %f66;
	mov.u32 	%r24, 2;
	shfl.sync.bfly.b32 	%r25|%p11, %r23, %r24, %r13, %r15;
	mov.b32 	%f67, %r25;
	add.f32 	%f68, %f66, %f67;
	mov.b32 	%r26, %f68;
	mov.u32 	%r27, 1;
	shfl.sync.bfly.b32 	%r28|%p12, %r26, %r27, %r13, %r15;
	mov.b32 	%f69, %r28;
	add.f32 	%f70, %f68, %f69;
	shr.s32 	%r29, %r1, 31;
	shr.u32 	%r30, %r29, 27;
	add.s32 	%r31, %r1, %r30;
	shr.s32 	%r32, %r31, 5;
	shl.b32 	%r33, %r32, 2;
	add.s32 	%r35, %r9, %r33;
	st.shared.f32 	[%r35], %f70;
	bar.sync 	0;
	@%p2 bra 	$L__BB2_15;

	ld.shared.f32 	%f71, [%r2];
	mov.b32 	%r36, %f71;
	shfl.sync.bfly.b32 	%r40|%p14, %r36, %r14, %r13, %r15;
	mov.b32 	%f72, %r40;
	add.f32 	%f73, %f71, %f72;
	mov.b32 	%r41, %f73;
	shfl.sync.bfly.b32 	%r43|%p15, %r41, %r18, %r13, %r15;
	mov.b32 	%f74, %r43;
	add.f32 	%f75, %f73, %f74;
	mov.b32 	%r44, %f75;
	shfl.sync.bfly.b32 	%r46|%p16, %r44, %r21, %r13, %r15;
	mov.b32 	%f76, %r46;
	add.f32 	%f77, %f75, %f76;
	mov.b32 	%r47, %f77;
	shfl.sync.bfly.b32 	%r49|%p17, %r47, %r24, %r13, %r15;
	mov.b32 	%f78, %r49;
	add.f32 	%f79, %f77, %f78;
	mov.b32 	%r50, %f79;
	shfl.sync.bfly.b32 	%r52|%p18, %r50, %r27, %r13, %r15;
	mov.b32 	%f80, %r52;
	add.f32 	%f8, %f79, %f80;
	setp.ne.s32 	%p19, %r1, 0;
	@%p19 bra 	$L__BB2_15;

	mul.lo.s64 	%rd66, %rd4, %rd38;
	add.s64 	%rd67, %rd66, %rd3;
	cvta.to.global.u64 	%rd68, %rd32;
	shl.b64 	%rd69, %rd67, 2;
	add.s64 	%rd70, %rd68, %rd69;
	st.global.f32 	[%rd70], %f8;

$L__BB2_15:
	ret;

}
	// .globl	ggml_matvec_f32_acc_f32_bs_128
.visible .entry ggml_matvec_f32_acc_f32_bs_128(
	.param .u64 ggml_matvec_f32_acc_f32_bs_128_param_0,
	.param .u64 ggml_matvec_f32_acc_f32_bs_128_param_1,
	.param .u64 ggml_matvec_f32_acc_f32_bs_128_param_2,
	.param .u64 ggml_matvec_f32_acc_f32_bs_128_param_3,
	.param .u64 ggml_matvec_f32_acc_f32_bs_128_param_4,
	.param .u64 ggml_matvec_f32_acc_f32_bs_128_param_5,
	.param .u64 ggml_matvec_f32_acc_f32_bs_128_param_6,
	.param .u64 ggml_matvec_f32_acc_f32_bs_128_param_7,
	.param .u64 ggml_matvec_f32_acc_f32_bs_128_param_8,
	.param .u64 ggml_matvec_f32_acc_f32_bs_128_param_9
)
{
	.reg .pred 	%p<20>;
	.reg .f32 	%f<86>;
	.reg .b32 	%r<53>;
	.reg .b64 	%rd<79>;


	ld.param.u64 	%rd39, [ggml_matvec_f32_acc_f32_bs_128_param_0];
	ld.param.u64 	%rd40, [ggml_matvec_f32_acc_f32_bs_128_param_1];
	ld.param.u64 	%rd32, [ggml_matvec_f32_acc_f32_bs_128_param_2];
	ld.param.u64 	%rd33, [ggml_matvec_f32_acc_f32_bs_128_param_3];
	ld.param.u64 	%rd34, [ggml_matvec_f32_acc_f32_bs_128_param_5];
	ld.param.u64 	%rd35, [ggml_matvec_f32_acc_f32_bs_128_param_6];
	ld.param.u64 	%rd36, [ggml_matvec_f32_acc_f32_bs_128_param_7];
	ld.param.u64 	%rd37, [ggml_matvec_f32_acc_f32_bs_128_param_8];
	ld.param.u64 	%rd38, [ggml_matvec_f32_acc_f32_bs_128_param_9];
	cvta.to.global.u64 	%rd1, %rd40;
	cvta.to.global.u64 	%rd2, %rd39;
	mov.u32 	%r3, %ctaid.x;
	cvt.u64.u32 	%rd3, %r3;
	mov.u32 	%r4, %ctaid.y;
	cvt.u64.u32 	%rd4, %r4;
	and.b64  	%rd41, %rd35, -4294967296;
	setp.eq.s64 	%p1, %rd41, 0;
	@%p1 bra 	$L__BB3_2;

	div.s64 	%rd70, %rd4, %rd35;
	bra.uni 	$L__BB3_3;

$L__BB3_2:
	cvt.u32.u64 	%r5, %rd35;
	cvt.u32.u64 	%r6, %rd4;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd70, %r7;

$L__BB3_3:
	mov.u32 	%r1, %tid.x;
	setp.gt.s32 	%p2, %r1, 31;
	shl.b32 	%r8, %r1, 2;
	mov.u32 	%r9, data_mmv;
	add.s32 	%r2, %r9, %r8;
	@%p2 bra 	$L__BB3_5;

	mov.u32 	%r10, 0;
	st.shared.u32 	[%r2], %r10;

$L__BB3_5:
	bar.sync 	0;
	cvt.s64.s32 	%rd75, %r1;
	setp.ge.s64 	%p3, %rd75, %rd33;
	mov.f32 	%f85, 0f00000000;
	@%p3 bra 	$L__BB3_12;

	not.b64 	%rd42, %rd75;
	add.s64 	%rd9, %rd42, %rd33;
	shr.u64 	%rd43, %rd9, 7;
	add.s64 	%rd44, %rd43, 1;
	and.b64  	%rd10, %rd44, 3;
	setp.eq.s64 	%p4, %rd10, 0;
	mov.f32 	%f85, 0f00000000;
	@%p4 bra 	$L__BB3_9;

	cvt.s64.s32 	%rd75, %r1;
	mul.wide.s32 	%rd45, %r1, 2;
	mul.lo.s64 	%rd46, %rd37, %rd4;
	add.s64 	%rd47, %rd45, %rd46;
	shl.b64 	%rd48, %rd47, 2;
	add.s64 	%rd73, %rd1, %rd48;
	mul.lo.s64 	%rd49, %rd70, %rd36;
	add.s64 	%rd50, %rd45, %rd49;
	mul.lo.s64 	%rd51, %rd34, %rd3;
	add.s64 	%rd52, %rd50, %rd51;
	shl.b64 	%rd53, %rd52, 2;
	add.s64 	%rd72, %rd2, %rd53;
	neg.s64 	%rd71, %rd10;
	mov.f32 	%f85, 0f00000000;

$L__BB3_8:
	.pragma "nounroll";
	ld.global.nc.v2.f32 	{%f13, %f14}, [%rd72];
	ld.global.nc.v2.f32 	{%f17, %f18}, [%rd73];
	fma.rn.f32 	%f21, %f13, %f17, %f85;
	fma.rn.f32 	%f85, %f14, %f18, %f21;
	add.s64 	%rd75, %rd75, 128;
	add.s64 	%rd73, %rd73, 1024;
	add.s64 	%rd72, %rd72, 1024;
	add.s64 	%rd71, %rd71, 1;
	setp.ne.s64 	%p5, %rd71, 0;
	@%p5 bra 	$L__BB3_8;

$L__BB3_9:
	setp.lt.u64 	%p6, %rd9, 384;
	@%p6 bra 	$L__BB3_12;

	shl.b64 	%rd54, %rd75, 1;
	mul.lo.s64 	%rd55, %rd70, %rd36;
	mul.lo.s64 	%rd56, %rd34, %rd3;
	add.s64 	%rd57, %rd56, %rd55;
	add.s64 	%rd58, %rd57, %rd54;
	shl.b64 	%rd59, %rd58, 2;
	add.s64 	%rd60, %rd2, %rd59;
	add.s64 	%rd77, %rd60, 2048;
	mul.lo.s64 	%rd61, %rd37, %rd4;
	add.s64 	%rd62, %rd54, %rd61;
	shl.b64 	%rd63, %rd62, 2;
	add.s64 	%rd64, %rd1, %rd63;
	add.s64 	%rd76, %rd64, 2048;

$L__BB3_11:
	ld.global.nc.v2.f32 	{%f22, %f23}, [%rd77+-2048];
	ld.global.nc.v2.f32 	{%f26, %f27}, [%rd76+-2048];
	fma.rn.f32 	%f30, %f22, %f26, %f85;
	fma.rn.f32 	%f31, %f23, %f27, %f30;
	ld.global.nc.v2.f32 	{%f32, %f33}, [%rd77+-1024];
	ld.global.nc.v2.f32 	{%f36, %f37}, [%rd76+-1024];
	fma.rn.f32 	%f40, %f32, %f36, %f31;
	fma.rn.f32 	%f41, %f33, %f37, %f40;
	ld.global.nc.v2.f32 	{%f42, %f43}, [%rd77];
	ld.global.nc.v2.f32 	{%f46, %f47}, [%rd76];
	fma.rn.f32 	%f50, %f42, %f46, %f41;
	fma.rn.f32 	%f51, %f43, %f47, %f50;
	ld.global.nc.v2.f32 	{%f52, %f53}, [%rd77+1024];
	ld.global.nc.v2.f32 	{%f56, %f57}, [%rd76+1024];
	fma.rn.f32 	%f60, %f52, %f56, %f51;
	fma.rn.f32 	%f85, %f53, %f57, %f60;
	add.s64 	%rd77, %rd77, 4096;
	add.s64 	%rd76, %rd76, 4096;
	add.s64 	%rd75, %rd75, 512;
	setp.lt.s64 	%p7, %rd75, %rd33;
	@%p7 bra 	$L__BB3_11;

$L__BB3_12:
	mov.b32 	%r12, %f85;
	mov.u32 	%r13, 31;
	mov.u32 	%r14, 16;
	mov.u32 	%r15, -1;
	shfl.sync.bfly.b32 	%r16|%p8, %r12, %r14, %r13, %r15;
	mov.b32 	%f61, %r16;
	add.f32 	%f62, %f85, %f61;
	mov.b32 	%r17, %f62;
	mov.u32 	%r18, 8;
	shfl.sync.bfly.b32 	%r19|%p9, %r17, %r18, %r13, %r15;
	mov.b32 	%f63, %r19;
	add.f32 	%f64, %f62, %f63;
	mov.b32 	%r20, %f64;
	mov.u32 	%r21, 4;
	shfl.sync.bfly.b32 	%r22|%p10, %r20, %r21, %r13, %r15;
	mov.b32 	%f65, %r22;
	add.f32 	%f66, %f64, %f65;
	mov.b32 	%r23, %f66;
	mov.u32 	%r24, 2;
	shfl.sync.bfly.b32 	%r25|%p11, %r23, %r24, %r13, %r15;
	mov.b32 	%f67, %r25;
	add.f32 	%f68, %f66, %f67;
	mov.b32 	%r26, %f68;
	mov.u32 	%r27, 1;
	shfl.sync.bfly.b32 	%r28|%p12, %r26, %r27, %r13, %r15;
	mov.b32 	%f69, %r28;
	add.f32 	%f70, %f68, %f69;
	shr.s32 	%r29, %r1, 31;
	shr.u32 	%r30, %r29, 27;
	add.s32 	%r31, %r1, %r30;
	shr.s32 	%r32, %r31, 5;
	shl.b32 	%r33, %r32, 2;
	add.s32 	%r35, %r9, %r33;
	st.shared.f32 	[%r35], %f70;
	bar.sync 	0;
	@%p2 bra 	$L__BB3_15;

	ld.shared.f32 	%f71, [%r2];
	mov.b32 	%r36, %f71;
	shfl.sync.bfly.b32 	%r40|%p14, %r36, %r14, %r13, %r15;
	mov.b32 	%f72, %r40;
	add.f32 	%f73, %f71, %f72;
	mov.b32 	%r41, %f73;
	shfl.sync.bfly.b32 	%r43|%p15, %r41, %r18, %r13, %r15;
	mov.b32 	%f74, %r43;
	add.f32 	%f75, %f73, %f74;
	mov.b32 	%r44, %f75;
	shfl.sync.bfly.b32 	%r46|%p16, %r44, %r21, %r13, %r15;
	mov.b32 	%f76, %r46;
	add.f32 	%f77, %f75, %f76;
	mov.b32 	%r47, %f77;
	shfl.sync.bfly.b32 	%r49|%p17, %r47, %r24, %r13, %r15;
	mov.b32 	%f78, %r49;
	add.f32 	%f79, %f77, %f78;
	mov.b32 	%r50, %f79;
	shfl.sync.bfly.b32 	%r52|%p18, %r50, %r27, %r13, %r15;
	mov.b32 	%f80, %r52;
	add.f32 	%f8, %f79, %f80;
	setp.ne.s32 	%p19, %r1, 0;
	@%p19 bra 	$L__BB3_15;

	mul.lo.s64 	%rd65, %rd4, %rd38;
	add.s64 	%rd66, %rd65, %rd3;
	cvta.to.global.u64 	%rd67, %rd32;
	shl.b64 	%rd68, %rd66, 2;
	add.s64 	%rd69, %rd67, %rd68;
	st.global.f32 	[%rd69], %f8;

$L__BB3_15:
	ret;

}
	// .globl	ggml_matvec_f32_acc_f32_bs_160
.visible .entry ggml_matvec_f32_acc_f32_bs_160(
	.param .u64 ggml_matvec_f32_acc_f32_bs_160_param_0,
	.param .u64 ggml_matvec_f32_acc_f32_bs_160_param_1,
	.param .u64 ggml_matvec_f32_acc_f32_bs_160_param_2,
	.param .u64 ggml_matvec_f32_acc_f32_bs_160_param_3,
	.param .u64 ggml_matvec_f32_acc_f32_bs_160_param_4,
	.param .u64 ggml_matvec_f32_acc_f32_bs_160_param_5,
	.param .u64 ggml_matvec_f32_acc_f32_bs_160_param_6,
	.param .u64 ggml_matvec_f32_acc_f32_bs_160_param_7,
	.param .u64 ggml_matvec_f32_acc_f32_bs_160_param_8,
	.param .u64 ggml_matvec_f32_acc_f32_bs_160_param_9
)
{
	.reg .pred 	%p<20>;
	.reg .f32 	%f<86>;
	.reg .b32 	%r<53>;
	.reg .b64 	%rd<80>;


	ld.param.u64 	%rd39, [ggml_matvec_f32_acc_f32_bs_160_param_0];
	ld.param.u64 	%rd40, [ggml_matvec_f32_acc_f32_bs_160_param_1];
	ld.param.u64 	%rd32, [ggml_matvec_f32_acc_f32_bs_160_param_2];
	ld.param.u64 	%rd33, [ggml_matvec_f32_acc_f32_bs_160_param_3];
	ld.param.u64 	%rd34, [ggml_matvec_f32_acc_f32_bs_160_param_5];
	ld.param.u64 	%rd35, [ggml_matvec_f32_acc_f32_bs_160_param_6];
	ld.param.u64 	%rd36, [ggml_matvec_f32_acc_f32_bs_160_param_7];
	ld.param.u64 	%rd37, [ggml_matvec_f32_acc_f32_bs_160_param_8];
	ld.param.u64 	%rd38, [ggml_matvec_f32_acc_f32_bs_160_param_9];
	cvta.to.global.u64 	%rd1, %rd40;
	cvta.to.global.u64 	%rd2, %rd39;
	mov.u32 	%r3, %ctaid.x;
	cvt.u64.u32 	%rd3, %r3;
	mov.u32 	%r4, %ctaid.y;
	cvt.u64.u32 	%rd4, %r4;
	and.b64  	%rd41, %rd35, -4294967296;
	setp.eq.s64 	%p1, %rd41, 0;
	@%p1 bra 	$L__BB4_2;

	div.s64 	%rd71, %rd4, %rd35;
	bra.uni 	$L__BB4_3;

$L__BB4_2:
	cvt.u32.u64 	%r5, %rd35;
	cvt.u32.u64 	%r6, %rd4;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd71, %r7;

$L__BB4_3:
	mov.u32 	%r1, %tid.x;
	setp.gt.s32 	%p2, %r1, 31;
	shl.b32 	%r8, %r1, 2;
	mov.u32 	%r9, data_mmv;
	add.s32 	%r2, %r9, %r8;
	@%p2 bra 	$L__BB4_5;

	mov.u32 	%r10, 0;
	st.shared.u32 	[%r2], %r10;

$L__BB4_5:
	bar.sync 	0;
	cvt.s64.s32 	%rd76, %r1;
	setp.ge.s64 	%p3, %rd76, %rd33;
	mov.f32 	%f85, 0f00000000;
	@%p3 bra 	$L__BB4_12;

	not.b64 	%rd42, %rd76;
	add.s64 	%rd9, %rd42, %rd33;
	mul.hi.u64 	%rd43, %rd9, -3689348814741910323;
	shr.u64 	%rd44, %rd43, 7;
	add.s64 	%rd45, %rd44, 1;
	and.b64  	%rd10, %rd45, 3;
	setp.eq.s64 	%p4, %rd10, 0;
	mov.f32 	%f85, 0f00000000;
	@%p4 bra 	$L__BB4_9;

	cvt.s64.s32 	%rd76, %r1;
	mul.wide.s32 	%rd46, %r1, 2;
	mul.lo.s64 	%rd47, %rd37, %rd4;
	add.s64 	%rd48, %rd46, %rd47;
	shl.b64 	%rd49, %rd48, 2;
	add.s64 	%rd74, %rd1, %rd49;
	mul.lo.s64 	%rd50, %rd71, %rd36;
	add.s64 	%rd51, %rd46, %rd50;
	mul.lo.s64 	%rd52, %rd34, %rd3;
	add.s64 	%rd53, %rd51, %rd52;
	shl.b64 	%rd54, %rd53, 2;
	add.s64 	%rd73, %rd2, %rd54;
	neg.s64 	%rd72, %rd10;
	mov.f32 	%f85, 0f00000000;

$L__BB4_8:
	.pragma "nounroll";
	ld.global.nc.v2.f32 	{%f13, %f14}, [%rd73];
	ld.global.nc.v2.f32 	{%f17, %f18}, [%rd74];
	fma.rn.f32 	%f21, %f13, %f17, %f85;
	fma.rn.f32 	%f85, %f14, %f18, %f21;
	add.s64 	%rd76, %rd76, 160;
	add.s64 	%rd74, %rd74, 1280;
	add.s64 	%rd73, %rd73, 1280;
	add.s64 	%rd72, %rd72, 1;
	setp.ne.s64 	%p5, %rd72, 0;
	@%p5 bra 	$L__BB4_8;

$L__BB4_9:
	setp.lt.u64 	%p6, %rd9, 480;
	@%p6 bra 	$L__BB4_12;

	shl.b64 	%rd55, %rd76, 1;
	mul.lo.s64 	%rd56, %rd71, %rd36;
	mul.lo.s64 	%rd57, %rd34, %rd3;
	add.s64 	%rd58, %rd57, %rd56;
	add.s64 	%rd59, %rd58, %rd55;
	shl.b64 	%rd60, %rd59, 2;
	add.s64 	%rd61, %rd2, %rd60;
	add.s64 	%rd78, %rd61, 2560;
	mul.lo.s64 	%rd62, %rd37, %rd4;
	add.s64 	%rd63, %rd55, %rd62;
	shl.b64 	%rd64, %rd63, 2;
	add.s64 	%rd65, %rd1, %rd64;
	add.s64 	%rd77, %rd65, 2560;

$L__BB4_11:
	ld.global.nc.v2.f32 	{%f22, %f23}, [%rd78+-2560];
	ld.global.nc.v2.f32 	{%f26, %f27}, [%rd77+-2560];
	fma.rn.f32 	%f30, %f22, %f26, %f85;
	fma.rn.f32 	%f31, %f23, %f27, %f30;
	ld.global.nc.v2.f32 	{%f32, %f33}, [%rd78+-1280];
	ld.global.nc.v2.f32 	{%f36, %f37}, [%rd77+-1280];
	fma.rn.f32 	%f40, %f32, %f36, %f31;
	fma.rn.f32 	%f41, %f33, %f37, %f40;
	ld.global.nc.v2.f32 	{%f42, %f43}, [%rd78];
	ld.global.nc.v2.f32 	{%f46, %f47}, [%rd77];
	fma.rn.f32 	%f50, %f42, %f46, %f41;
	fma.rn.f32 	%f51, %f43, %f47, %f50;
	ld.global.nc.v2.f32 	{%f52, %f53}, [%rd78+1280];
	ld.global.nc.v2.f32 	{%f56, %f57}, [%rd77+1280];
	fma.rn.f32 	%f60, %f52, %f56, %f51;
	fma.rn.f32 	%f85, %f53, %f57, %f60;
	add.s64 	%rd78, %rd78, 5120;
	add.s64 	%rd77, %rd77, 5120;
	add.s64 	%rd76, %rd76, 640;
	setp.lt.s64 	%p7, %rd76, %rd33;
	@%p7 bra 	$L__BB4_11;

$L__BB4_12:
	mov.b32 	%r12, %f85;
	mov.u32 	%r13, 31;
	mov.u32 	%r14, 16;
	mov.u32 	%r15, -1;
	shfl.sync.bfly.b32 	%r16|%p8, %r12, %r14, %r13, %r15;
	mov.b32 	%f61, %r16;
	add.f32 	%f62, %f85, %f61;
	mov.b32 	%r17, %f62;
	mov.u32 	%r18, 8;
	shfl.sync.bfly.b32 	%r19|%p9, %r17, %r18, %r13, %r15;
	mov.b32 	%f63, %r19;
	add.f32 	%f64, %f62, %f63;
	mov.b32 	%r20, %f64;
	mov.u32 	%r21, 4;
	shfl.sync.bfly.b32 	%r22|%p10, %r20, %r21, %r13, %r15;
	mov.b32 	%f65, %r22;
	add.f32 	%f66, %f64, %f65;
	mov.b32 	%r23, %f66;
	mov.u32 	%r24, 2;
	shfl.sync.bfly.b32 	%r25|%p11, %r23, %r24, %r13, %r15;
	mov.b32 	%f67, %r25;
	add.f32 	%f68, %f66, %f67;
	mov.b32 	%r26, %f68;
	mov.u32 	%r27, 1;
	shfl.sync.bfly.b32 	%r28|%p12, %r26, %r27, %r13, %r15;
	mov.b32 	%f69, %r28;
	add.f32 	%f70, %f68, %f69;
	shr.s32 	%r29, %r1, 31;
	shr.u32 	%r30, %r29, 27;
	add.s32 	%r31, %r1, %r30;
	shr.s32 	%r32, %r31, 5;
	shl.b32 	%r33, %r32, 2;
	add.s32 	%r35, %r9, %r33;
	st.shared.f32 	[%r35], %f70;
	bar.sync 	0;
	@%p2 bra 	$L__BB4_15;

	ld.shared.f32 	%f71, [%r2];
	mov.b32 	%r36, %f71;
	shfl.sync.bfly.b32 	%r40|%p14, %r36, %r14, %r13, %r15;
	mov.b32 	%f72, %r40;
	add.f32 	%f73, %f71, %f72;
	mov.b32 	%r41, %f73;
	shfl.sync.bfly.b32 	%r43|%p15, %r41, %r18, %r13, %r15;
	mov.b32 	%f74, %r43;
	add.f32 	%f75, %f73, %f74;
	mov.b32 	%r44, %f75;
	shfl.sync.bfly.b32 	%r46|%p16, %r44, %r21, %r13, %r15;
	mov.b32 	%f76, %r46;
	add.f32 	%f77, %f75, %f76;
	mov.b32 	%r47, %f77;
	shfl.sync.bfly.b32 	%r49|%p17, %r47, %r24, %r13, %r15;
	mov.b32 	%f78, %r49;
	add.f32 	%f79, %f77, %f78;
	mov.b32 	%r50, %f79;
	shfl.sync.bfly.b32 	%r52|%p18, %r50, %r27, %r13, %r15;
	mov.b32 	%f80, %r52;
	add.f32 	%f8, %f79, %f80;
	setp.ne.s32 	%p19, %r1, 0;
	@%p19 bra 	$L__BB4_15;

	mul.lo.s64 	%rd66, %rd4, %rd38;
	add.s64 	%rd67, %rd66, %rd3;
	cvta.to.global.u64 	%rd68, %rd32;
	shl.b64 	%rd69, %rd67, 2;
	add.s64 	%rd70, %rd68, %rd69;
	st.global.f32 	[%rd70], %f8;

$L__BB4_15:
	ret;

}
	// .globl	ggml_matvec_f32_acc_f32_bs_196
.visible .entry ggml_matvec_f32_acc_f32_bs_196(
	.param .u64 ggml_matvec_f32_acc_f32_bs_196_param_0,
	.param .u64 ggml_matvec_f32_acc_f32_bs_196_param_1,
	.param .u64 ggml_matvec_f32_acc_f32_bs_196_param_2,
	.param .u64 ggml_matvec_f32_acc_f32_bs_196_param_3,
	.param .u64 ggml_matvec_f32_acc_f32_bs_196_param_4,
	.param .u64 ggml_matvec_f32_acc_f32_bs_196_param_5,
	.param .u64 ggml_matvec_f32_acc_f32_bs_196_param_6,
	.param .u64 ggml_matvec_f32_acc_f32_bs_196_param_7,
	.param .u64 ggml_matvec_f32_acc_f32_bs_196_param_8,
	.param .u64 ggml_matvec_f32_acc_f32_bs_196_param_9
)
{
	.reg .pred 	%p<20>;
	.reg .f32 	%f<86>;
	.reg .b32 	%r<53>;
	.reg .b64 	%rd<81>;


	ld.param.u64 	%rd39, [ggml_matvec_f32_acc_f32_bs_196_param_0];
	ld.param.u64 	%rd40, [ggml_matvec_f32_acc_f32_bs_196_param_1];
	ld.param.u64 	%rd32, [ggml_matvec_f32_acc_f32_bs_196_param_2];
	ld.param.u64 	%rd33, [ggml_matvec_f32_acc_f32_bs_196_param_3];
	ld.param.u64 	%rd34, [ggml_matvec_f32_acc_f32_bs_196_param_5];
	ld.param.u64 	%rd35, [ggml_matvec_f32_acc_f32_bs_196_param_6];
	ld.param.u64 	%rd36, [ggml_matvec_f32_acc_f32_bs_196_param_7];
	ld.param.u64 	%rd37, [ggml_matvec_f32_acc_f32_bs_196_param_8];
	ld.param.u64 	%rd38, [ggml_matvec_f32_acc_f32_bs_196_param_9];
	cvta.to.global.u64 	%rd1, %rd40;
	cvta.to.global.u64 	%rd2, %rd39;
	mov.u32 	%r3, %ctaid.x;
	cvt.u64.u32 	%rd3, %r3;
	mov.u32 	%r4, %ctaid.y;
	cvt.u64.u32 	%rd4, %r4;
	and.b64  	%rd41, %rd35, -4294967296;
	setp.eq.s64 	%p1, %rd41, 0;
	@%p1 bra 	$L__BB5_2;

	div.s64 	%rd72, %rd4, %rd35;
	bra.uni 	$L__BB5_3;

$L__BB5_2:
	cvt.u32.u64 	%r5, %rd35;
	cvt.u32.u64 	%r6, %rd4;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd72, %r7;

$L__BB5_3:
	mov.u32 	%r1, %tid.x;
	setp.gt.s32 	%p2, %r1, 31;
	shl.b32 	%r8, %r1, 2;
	mov.u32 	%r9, data_mmv;
	add.s32 	%r2, %r9, %r8;
	@%p2 bra 	$L__BB5_5;

	mov.u32 	%r10, 0;
	st.shared.u32 	[%r2], %r10;

$L__BB5_5:
	bar.sync 	0;
	cvt.s64.s32 	%rd77, %r1;
	setp.ge.s64 	%p3, %rd77, %rd33;
	mov.f32 	%f85, 0f00000000;
	@%p3 bra 	$L__BB5_12;

	not.b64 	%rd42, %rd77;
	add.s64 	%rd9, %rd42, %rd33;
	shr.u64 	%rd43, %rd9, 2;
	mul.hi.u64 	%rd44, %rd43, 6023426636313322977;
	shr.u64 	%rd45, %rd44, 4;
	add.s64 	%rd46, %rd45, 1;
	and.b64  	%rd10, %rd46, 3;
	setp.eq.s64 	%p4, %rd10, 0;
	mov.f32 	%f85, 0f00000000;
	@%p4 bra 	$L__BB5_9;

	cvt.s64.s32 	%rd77, %r1;
	mul.wide.s32 	%rd47, %r1, 2;
	mul.lo.s64 	%rd48, %rd37, %rd4;
	add.s64 	%rd49, %rd47, %rd48;
	shl.b64 	%rd50, %rd49, 2;
	add.s64 	%rd75, %rd1, %rd50;
	mul.lo.s64 	%rd51, %rd72, %rd36;
	add.s64 	%rd52, %rd47, %rd51;
	mul.lo.s64 	%rd53, %rd34, %rd3;
	add.s64 	%rd54, %rd52, %rd53;
	shl.b64 	%rd55, %rd54, 2;
	add.s64 	%rd74, %rd2, %rd55;
	neg.s64 	%rd73, %rd10;
	mov.f32 	%f85, 0f00000000;

$L__BB5_8:
	.pragma "nounroll";
	ld.global.nc.v2.f32 	{%f13, %f14}, [%rd74];
	ld.global.nc.v2.f32 	{%f17, %f18}, [%rd75];
	fma.rn.f32 	%f21, %f13, %f17, %f85;
	fma.rn.f32 	%f85, %f14, %f18, %f21;
	add.s64 	%rd77, %rd77, 196;
	add.s64 	%rd75, %rd75, 1568;
	add.s64 	%rd74, %rd74, 1568;
	add.s64 	%rd73, %rd73, 1;
	setp.ne.s64 	%p5, %rd73, 0;
	@%p5 bra 	$L__BB5_8;

$L__BB5_9:
	setp.lt.u64 	%p6, %rd9, 588;
	@%p6 bra 	$L__BB5_12;

	shl.b64 	%rd56, %rd77, 1;
	mul.lo.s64 	%rd57, %rd72, %rd36;
	mul.lo.s64 	%rd58, %rd34, %rd3;
	add.s64 	%rd59, %rd58, %rd57;
	add.s64 	%rd60, %rd59, %rd56;
	shl.b64 	%rd61, %rd60, 2;
	add.s64 	%rd62, %rd2, %rd61;
	add.s64 	%rd79, %rd62, 3136;
	mul.lo.s64 	%rd63, %rd37, %rd4;
	add.s64 	%rd64, %rd56, %rd63;
	shl.b64 	%rd65, %rd64, 2;
	add.s64 	%rd66, %rd1, %rd65;
	add.s64 	%rd78, %rd66, 3136;

$L__BB5_11:
	ld.global.nc.v2.f32 	{%f22, %f23}, [%rd79+-3136];
	ld.global.nc.v2.f32 	{%f26, %f27}, [%rd78+-3136];
	fma.rn.f32 	%f30, %f22, %f26, %f85;
	fma.rn.f32 	%f31, %f23, %f27, %f30;
	ld.global.nc.v2.f32 	{%f32, %f33}, [%rd79+-1568];
	ld.global.nc.v2.f32 	{%f36, %f37}, [%rd78+-1568];
	fma.rn.f32 	%f40, %f32, %f36, %f31;
	fma.rn.f32 	%f41, %f33, %f37, %f40;
	ld.global.nc.v2.f32 	{%f42, %f43}, [%rd79];
	ld.global.nc.v2.f32 	{%f46, %f47}, [%rd78];
	fma.rn.f32 	%f50, %f42, %f46, %f41;
	fma.rn.f32 	%f51, %f43, %f47, %f50;
	ld.global.nc.v2.f32 	{%f52, %f53}, [%rd79+1568];
	ld.global.nc.v2.f32 	{%f56, %f57}, [%rd78+1568];
	fma.rn.f32 	%f60, %f52, %f56, %f51;
	fma.rn.f32 	%f85, %f53, %f57, %f60;
	add.s64 	%rd79, %rd79, 6272;
	add.s64 	%rd78, %rd78, 6272;
	add.s64 	%rd77, %rd77, 784;
	setp.lt.s64 	%p7, %rd77, %rd33;
	@%p7 bra 	$L__BB5_11;

$L__BB5_12:
	mov.b32 	%r12, %f85;
	mov.u32 	%r13, 31;
	mov.u32 	%r14, 16;
	mov.u32 	%r15, -1;
	shfl.sync.bfly.b32 	%r16|%p8, %r12, %r14, %r13, %r15;
	mov.b32 	%f61, %r16;
	add.f32 	%f62, %f85, %f61;
	mov.b32 	%r17, %f62;
	mov.u32 	%r18, 8;
	shfl.sync.bfly.b32 	%r19|%p9, %r17, %r18, %r13, %r15;
	mov.b32 	%f63, %r19;
	add.f32 	%f64, %f62, %f63;
	mov.b32 	%r20, %f64;
	mov.u32 	%r21, 4;
	shfl.sync.bfly.b32 	%r22|%p10, %r20, %r21, %r13, %r15;
	mov.b32 	%f65, %r22;
	add.f32 	%f66, %f64, %f65;
	mov.b32 	%r23, %f66;
	mov.u32 	%r24, 2;
	shfl.sync.bfly.b32 	%r25|%p11, %r23, %r24, %r13, %r15;
	mov.b32 	%f67, %r25;
	add.f32 	%f68, %f66, %f67;
	mov.b32 	%r26, %f68;
	mov.u32 	%r27, 1;
	shfl.sync.bfly.b32 	%r28|%p12, %r26, %r27, %r13, %r15;
	mov.b32 	%f69, %r28;
	add.f32 	%f70, %f68, %f69;
	shr.s32 	%r29, %r1, 31;
	shr.u32 	%r30, %r29, 27;
	add.s32 	%r31, %r1, %r30;
	shr.s32 	%r32, %r31, 5;
	shl.b32 	%r33, %r32, 2;
	add.s32 	%r35, %r9, %r33;
	st.shared.f32 	[%r35], %f70;
	bar.sync 	0;
	@%p2 bra 	$L__BB5_15;

	ld.shared.f32 	%f71, [%r2];
	mov.b32 	%r36, %f71;
	shfl.sync.bfly.b32 	%r40|%p14, %r36, %r14, %r13, %r15;
	mov.b32 	%f72, %r40;
	add.f32 	%f73, %f71, %f72;
	mov.b32 	%r41, %f73;
	shfl.sync.bfly.b32 	%r43|%p15, %r41, %r18, %r13, %r15;
	mov.b32 	%f74, %r43;
	add.f32 	%f75, %f73, %f74;
	mov.b32 	%r44, %f75;
	shfl.sync.bfly.b32 	%r46|%p16, %r44, %r21, %r13, %r15;
	mov.b32 	%f76, %r46;
	add.f32 	%f77, %f75, %f76;
	mov.b32 	%r47, %f77;
	shfl.sync.bfly.b32 	%r49|%p17, %r47, %r24, %r13, %r15;
	mov.b32 	%f78, %r49;
	add.f32 	%f79, %f77, %f78;
	mov.b32 	%r50, %f79;
	shfl.sync.bfly.b32 	%r52|%p18, %r50, %r27, %r13, %r15;
	mov.b32 	%f80, %r52;
	add.f32 	%f8, %f79, %f80;
	setp.ne.s32 	%p19, %r1, 0;
	@%p19 bra 	$L__BB5_15;

	mul.lo.s64 	%rd67, %rd4, %rd38;
	add.s64 	%rd68, %rd67, %rd3;
	cvta.to.global.u64 	%rd69, %rd32;
	shl.b64 	%rd70, %rd68, 2;
	add.s64 	%rd71, %rd69, %rd70;
	st.global.f32 	[%rd71], %f8;

$L__BB5_15:
	ret;

}
	// .globl	ggml_matvec_f32_acc_f32_bs_224
.visible .entry ggml_matvec_f32_acc_f32_bs_224(
	.param .u64 ggml_matvec_f32_acc_f32_bs_224_param_0,
	.param .u64 ggml_matvec_f32_acc_f32_bs_224_param_1,
	.param .u64 ggml_matvec_f32_acc_f32_bs_224_param_2,
	.param .u64 ggml_matvec_f32_acc_f32_bs_224_param_3,
	.param .u64 ggml_matvec_f32_acc_f32_bs_224_param_4,
	.param .u64 ggml_matvec_f32_acc_f32_bs_224_param_5,
	.param .u64 ggml_matvec_f32_acc_f32_bs_224_param_6,
	.param .u64 ggml_matvec_f32_acc_f32_bs_224_param_7,
	.param .u64 ggml_matvec_f32_acc_f32_bs_224_param_8,
	.param .u64 ggml_matvec_f32_acc_f32_bs_224_param_9
)
{
	.reg .pred 	%p<20>;
	.reg .f32 	%f<86>;
	.reg .b32 	%r<53>;
	.reg .b64 	%rd<80>;


	ld.param.u64 	%rd39, [ggml_matvec_f32_acc_f32_bs_224_param_0];
	ld.param.u64 	%rd40, [ggml_matvec_f32_acc_f32_bs_224_param_1];
	ld.param.u64 	%rd32, [ggml_matvec_f32_acc_f32_bs_224_param_2];
	ld.param.u64 	%rd33, [ggml_matvec_f32_acc_f32_bs_224_param_3];
	ld.param.u64 	%rd34, [ggml_matvec_f32_acc_f32_bs_224_param_5];
	ld.param.u64 	%rd35, [ggml_matvec_f32_acc_f32_bs_224_param_6];
	ld.param.u64 	%rd36, [ggml_matvec_f32_acc_f32_bs_224_param_7];
	ld.param.u64 	%rd37, [ggml_matvec_f32_acc_f32_bs_224_param_8];
	ld.param.u64 	%rd38, [ggml_matvec_f32_acc_f32_bs_224_param_9];
	cvta.to.global.u64 	%rd1, %rd40;
	cvta.to.global.u64 	%rd2, %rd39;
	mov.u32 	%r3, %ctaid.x;
	cvt.u64.u32 	%rd3, %r3;
	mov.u32 	%r4, %ctaid.y;
	cvt.u64.u32 	%rd4, %r4;
	and.b64  	%rd41, %rd35, -4294967296;
	setp.eq.s64 	%p1, %rd41, 0;
	@%p1 bra 	$L__BB6_2;

	div.s64 	%rd71, %rd4, %rd35;
	bra.uni 	$L__BB6_3;

$L__BB6_2:
	cvt.u32.u64 	%r5, %rd35;
	cvt.u32.u64 	%r6, %rd4;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd71, %r7;

$L__BB6_3:
	mov.u32 	%r1, %tid.x;
	setp.gt.s32 	%p2, %r1, 31;
	shl.b32 	%r8, %r1, 2;
	mov.u32 	%r9, data_mmv;
	add.s32 	%r2, %r9, %r8;
	@%p2 bra 	$L__BB6_5;

	mov.u32 	%r10, 0;
	st.shared.u32 	[%r2], %r10;

$L__BB6_5:
	bar.sync 	0;
	cvt.s64.s32 	%rd76, %r1;
	setp.ge.s64 	%p3, %rd76, %rd33;
	mov.f32 	%f85, 0f00000000;
	@%p3 bra 	$L__BB6_12;

	not.b64 	%rd42, %rd76;
	add.s64 	%rd9, %rd42, %rd33;
	shr.u64 	%rd43, %rd9, 5;
	mul.hi.u64 	%rd44, %rd43, 2635249153387078803;
	add.s64 	%rd45, %rd44, 1;
	and.b64  	%rd10, %rd45, 3;
	setp.eq.s64 	%p4, %rd10, 0;
	mov.f32 	%f85, 0f00000000;
	@%p4 bra 	$L__BB6_9;

	cvt.s64.s32 	%rd76, %r1;
	mul.wide.s32 	%rd46, %r1, 2;
	mul.lo.s64 	%rd47, %rd37, %rd4;
	add.s64 	%rd48, %rd46, %rd47;
	shl.b64 	%rd49, %rd48, 2;
	add.s64 	%rd74, %rd1, %rd49;
	mul.lo.s64 	%rd50, %rd71, %rd36;
	add.s64 	%rd51, %rd46, %rd50;
	mul.lo.s64 	%rd52, %rd34, %rd3;
	add.s64 	%rd53, %rd51, %rd52;
	shl.b64 	%rd54, %rd53, 2;
	add.s64 	%rd73, %rd2, %rd54;
	neg.s64 	%rd72, %rd10;
	mov.f32 	%f85, 0f00000000;

$L__BB6_8:
	.pragma "nounroll";
	ld.global.nc.v2.f32 	{%f13, %f14}, [%rd73];
	ld.global.nc.v2.f32 	{%f17, %f18}, [%rd74];
	fma.rn.f32 	%f21, %f13, %f17, %f85;
	fma.rn.f32 	%f85, %f14, %f18, %f21;
	add.s64 	%rd76, %rd76, 224;
	add.s64 	%rd74, %rd74, 1792;
	add.s64 	%rd73, %rd73, 1792;
	add.s64 	%rd72, %rd72, 1;
	setp.ne.s64 	%p5, %rd72, 0;
	@%p5 bra 	$L__BB6_8;

$L__BB6_9:
	setp.lt.u64 	%p6, %rd9, 672;
	@%p6 bra 	$L__BB6_12;

	shl.b64 	%rd55, %rd76, 1;
	mul.lo.s64 	%rd56, %rd71, %rd36;
	mul.lo.s64 	%rd57, %rd34, %rd3;
	add.s64 	%rd58, %rd57, %rd56;
	add.s64 	%rd59, %rd58, %rd55;
	shl.b64 	%rd60, %rd59, 2;
	add.s64 	%rd61, %rd2, %rd60;
	add.s64 	%rd78, %rd61, 3584;
	mul.lo.s64 	%rd62, %rd37, %rd4;
	add.s64 	%rd63, %rd55, %rd62;
	shl.b64 	%rd64, %rd63, 2;
	add.s64 	%rd65, %rd1, %rd64;
	add.s64 	%rd77, %rd65, 3584;

$L__BB6_11:
	ld.global.nc.v2.f32 	{%f22, %f23}, [%rd78+-3584];
	ld.global.nc.v2.f32 	{%f26, %f27}, [%rd77+-3584];
	fma.rn.f32 	%f30, %f22, %f26, %f85;
	fma.rn.f32 	%f31, %f23, %f27, %f30;
	ld.global.nc.v2.f32 	{%f32, %f33}, [%rd78+-1792];
	ld.global.nc.v2.f32 	{%f36, %f37}, [%rd77+-1792];
	fma.rn.f32 	%f40, %f32, %f36, %f31;
	fma.rn.f32 	%f41, %f33, %f37, %f40;
	ld.global.nc.v2.f32 	{%f42, %f43}, [%rd78];
	ld.global.nc.v2.f32 	{%f46, %f47}, [%rd77];
	fma.rn.f32 	%f50, %f42, %f46, %f41;
	fma.rn.f32 	%f51, %f43, %f47, %f50;
	ld.global.nc.v2.f32 	{%f52, %f53}, [%rd78+1792];
	ld.global.nc.v2.f32 	{%f56, %f57}, [%rd77+1792];
	fma.rn.f32 	%f60, %f52, %f56, %f51;
	fma.rn.f32 	%f85, %f53, %f57, %f60;
	add.s64 	%rd78, %rd78, 7168;
	add.s64 	%rd77, %rd77, 7168;
	add.s64 	%rd76, %rd76, 896;
	setp.lt.s64 	%p7, %rd76, %rd33;
	@%p7 bra 	$L__BB6_11;

$L__BB6_12:
	mov.b32 	%r12, %f85;
	mov.u32 	%r13, 31;
	mov.u32 	%r14, 16;
	mov.u32 	%r15, -1;
	shfl.sync.bfly.b32 	%r16|%p8, %r12, %r14, %r13, %r15;
	mov.b32 	%f61, %r16;
	add.f32 	%f62, %f85, %f61;
	mov.b32 	%r17, %f62;
	mov.u32 	%r18, 8;
	shfl.sync.bfly.b32 	%r19|%p9, %r17, %r18, %r13, %r15;
	mov.b32 	%f63, %r19;
	add.f32 	%f64, %f62, %f63;
	mov.b32 	%r20, %f64;
	mov.u32 	%r21, 4;
	shfl.sync.bfly.b32 	%r22|%p10, %r20, %r21, %r13, %r15;
	mov.b32 	%f65, %r22;
	add.f32 	%f66, %f64, %f65;
	mov.b32 	%r23, %f66;
	mov.u32 	%r24, 2;
	shfl.sync.bfly.b32 	%r25|%p11, %r23, %r24, %r13, %r15;
	mov.b32 	%f67, %r25;
	add.f32 	%f68, %f66, %f67;
	mov.b32 	%r26, %f68;
	mov.u32 	%r27, 1;
	shfl.sync.bfly.b32 	%r28|%p12, %r26, %r27, %r13, %r15;
	mov.b32 	%f69, %r28;
	add.f32 	%f70, %f68, %f69;
	shr.s32 	%r29, %r1, 31;
	shr.u32 	%r30, %r29, 27;
	add.s32 	%r31, %r1, %r30;
	shr.s32 	%r32, %r31, 5;
	shl.b32 	%r33, %r32, 2;
	add.s32 	%r35, %r9, %r33;
	st.shared.f32 	[%r35], %f70;
	bar.sync 	0;
	@%p2 bra 	$L__BB6_15;

	ld.shared.f32 	%f71, [%r2];
	mov.b32 	%r36, %f71;
	shfl.sync.bfly.b32 	%r40|%p14, %r36, %r14, %r13, %r15;
	mov.b32 	%f72, %r40;
	add.f32 	%f73, %f71, %f72;
	mov.b32 	%r41, %f73;
	shfl.sync.bfly.b32 	%r43|%p15, %r41, %r18, %r13, %r15;
	mov.b32 	%f74, %r43;
	add.f32 	%f75, %f73, %f74;
	mov.b32 	%r44, %f75;
	shfl.sync.bfly.b32 	%r46|%p16, %r44, %r21, %r13, %r15;
	mov.b32 	%f76, %r46;
	add.f32 	%f77, %f75, %f76;
	mov.b32 	%r47, %f77;
	shfl.sync.bfly.b32 	%r49|%p17, %r47, %r24, %r13, %r15;
	mov.b32 	%f78, %r49;
	add.f32 	%f79, %f77, %f78;
	mov.b32 	%r50, %f79;
	shfl.sync.bfly.b32 	%r52|%p18, %r50, %r27, %r13, %r15;
	mov.b32 	%f80, %r52;
	add.f32 	%f8, %f79, %f80;
	setp.ne.s32 	%p19, %r1, 0;
	@%p19 bra 	$L__BB6_15;

	mul.lo.s64 	%rd66, %rd4, %rd38;
	add.s64 	%rd67, %rd66, %rd3;
	cvta.to.global.u64 	%rd68, %rd32;
	shl.b64 	%rd69, %rd67, 2;
	add.s64 	%rd70, %rd68, %rd69;
	st.global.f32 	[%rd70], %f8;

$L__BB6_15:
	ret;

}
	// .globl	ggml_matvec_f32_acc_f32_bs_256
.visible .entry ggml_matvec_f32_acc_f32_bs_256(
	.param .u64 ggml_matvec_f32_acc_f32_bs_256_param_0,
	.param .u64 ggml_matvec_f32_acc_f32_bs_256_param_1,
	.param .u64 ggml_matvec_f32_acc_f32_bs_256_param_2,
	.param .u64 ggml_matvec_f32_acc_f32_bs_256_param_3,
	.param .u64 ggml_matvec_f32_acc_f32_bs_256_param_4,
	.param .u64 ggml_matvec_f32_acc_f32_bs_256_param_5,
	.param .u64 ggml_matvec_f32_acc_f32_bs_256_param_6,
	.param .u64 ggml_matvec_f32_acc_f32_bs_256_param_7,
	.param .u64 ggml_matvec_f32_acc_f32_bs_256_param_8,
	.param .u64 ggml_matvec_f32_acc_f32_bs_256_param_9
)
{
	.reg .pred 	%p<20>;
	.reg .f32 	%f<86>;
	.reg .b32 	%r<53>;
	.reg .b64 	%rd<79>;


	ld.param.u64 	%rd39, [ggml_matvec_f32_acc_f32_bs_256_param_0];
	ld.param.u64 	%rd40, [ggml_matvec_f32_acc_f32_bs_256_param_1];
	ld.param.u64 	%rd32, [ggml_matvec_f32_acc_f32_bs_256_param_2];
	ld.param.u64 	%rd33, [ggml_matvec_f32_acc_f32_bs_256_param_3];
	ld.param.u64 	%rd34, [ggml_matvec_f32_acc_f32_bs_256_param_5];
	ld.param.u64 	%rd35, [ggml_matvec_f32_acc_f32_bs_256_param_6];
	ld.param.u64 	%rd36, [ggml_matvec_f32_acc_f32_bs_256_param_7];
	ld.param.u64 	%rd37, [ggml_matvec_f32_acc_f32_bs_256_param_8];
	ld.param.u64 	%rd38, [ggml_matvec_f32_acc_f32_bs_256_param_9];
	cvta.to.global.u64 	%rd1, %rd40;
	cvta.to.global.u64 	%rd2, %rd39;
	mov.u32 	%r3, %ctaid.x;
	cvt.u64.u32 	%rd3, %r3;
	mov.u32 	%r4, %ctaid.y;
	cvt.u64.u32 	%rd4, %r4;
	and.b64  	%rd41, %rd35, -4294967296;
	setp.eq.s64 	%p1, %rd41, 0;
	@%p1 bra 	$L__BB7_2;

	div.s64 	%rd70, %rd4, %rd35;
	bra.uni 	$L__BB7_3;

$L__BB7_2:
	cvt.u32.u64 	%r5, %rd35;
	cvt.u32.u64 	%r6, %rd4;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd70, %r7;

$L__BB7_3:
	mov.u32 	%r1, %tid.x;
	setp.gt.s32 	%p2, %r1, 31;
	shl.b32 	%r8, %r1, 2;
	mov.u32 	%r9, data_mmv;
	add.s32 	%r2, %r9, %r8;
	@%p2 bra 	$L__BB7_5;

	mov.u32 	%r10, 0;
	st.shared.u32 	[%r2], %r10;

$L__BB7_5:
	bar.sync 	0;
	cvt.s64.s32 	%rd75, %r1;
	setp.ge.s64 	%p3, %rd75, %rd33;
	mov.f32 	%f85, 0f00000000;
	@%p3 bra 	$L__BB7_12;

	not.b64 	%rd42, %rd75;
	add.s64 	%rd9, %rd42, %rd33;
	shr.u64 	%rd43, %rd9, 8;
	add.s64 	%rd44, %rd43, 1;
	and.b64  	%rd10, %rd44, 3;
	setp.eq.s64 	%p4, %rd10, 0;
	mov.f32 	%f85, 0f00000000;
	@%p4 bra 	$L__BB7_9;

	cvt.s64.s32 	%rd75, %r1;
	mul.wide.s32 	%rd45, %r1, 2;
	mul.lo.s64 	%rd46, %rd37, %rd4;
	add.s64 	%rd47, %rd45, %rd46;
	shl.b64 	%rd48, %rd47, 2;
	add.s64 	%rd73, %rd1, %rd48;
	mul.lo.s64 	%rd49, %rd70, %rd36;
	add.s64 	%rd50, %rd45, %rd49;
	mul.lo.s64 	%rd51, %rd34, %rd3;
	add.s64 	%rd52, %rd50, %rd51;
	shl.b64 	%rd53, %rd52, 2;
	add.s64 	%rd72, %rd2, %rd53;
	neg.s64 	%rd71, %rd10;
	mov.f32 	%f85, 0f00000000;

$L__BB7_8:
	.pragma "nounroll";
	ld.global.nc.v2.f32 	{%f13, %f14}, [%rd72];
	ld.global.nc.v2.f32 	{%f17, %f18}, [%rd73];
	fma.rn.f32 	%f21, %f13, %f17, %f85;
	fma.rn.f32 	%f85, %f14, %f18, %f21;
	add.s64 	%rd75, %rd75, 256;
	add.s64 	%rd73, %rd73, 2048;
	add.s64 	%rd72, %rd72, 2048;
	add.s64 	%rd71, %rd71, 1;
	setp.ne.s64 	%p5, %rd71, 0;
	@%p5 bra 	$L__BB7_8;

$L__BB7_9:
	setp.lt.u64 	%p6, %rd9, 768;
	@%p6 bra 	$L__BB7_12;

	shl.b64 	%rd54, %rd75, 1;
	mul.lo.s64 	%rd55, %rd70, %rd36;
	mul.lo.s64 	%rd56, %rd34, %rd3;
	add.s64 	%rd57, %rd56, %rd55;
	add.s64 	%rd58, %rd57, %rd54;
	shl.b64 	%rd59, %rd58, 2;
	add.s64 	%rd60, %rd2, %rd59;
	add.s64 	%rd77, %rd60, 4096;
	mul.lo.s64 	%rd61, %rd37, %rd4;
	add.s64 	%rd62, %rd54, %rd61;
	shl.b64 	%rd63, %rd62, 2;
	add.s64 	%rd64, %rd1, %rd63;
	add.s64 	%rd76, %rd64, 4096;

$L__BB7_11:
	ld.global.nc.v2.f32 	{%f22, %f23}, [%rd77+-4096];
	ld.global.nc.v2.f32 	{%f26, %f27}, [%rd76+-4096];
	fma.rn.f32 	%f30, %f22, %f26, %f85;
	fma.rn.f32 	%f31, %f23, %f27, %f30;
	ld.global.nc.v2.f32 	{%f32, %f33}, [%rd77+-2048];
	ld.global.nc.v2.f32 	{%f36, %f37}, [%rd76+-2048];
	fma.rn.f32 	%f40, %f32, %f36, %f31;
	fma.rn.f32 	%f41, %f33, %f37, %f40;
	ld.global.nc.v2.f32 	{%f42, %f43}, [%rd77];
	ld.global.nc.v2.f32 	{%f46, %f47}, [%rd76];
	fma.rn.f32 	%f50, %f42, %f46, %f41;
	fma.rn.f32 	%f51, %f43, %f47, %f50;
	ld.global.nc.v2.f32 	{%f52, %f53}, [%rd77+2048];
	ld.global.nc.v2.f32 	{%f56, %f57}, [%rd76+2048];
	fma.rn.f32 	%f60, %f52, %f56, %f51;
	fma.rn.f32 	%f85, %f53, %f57, %f60;
	add.s64 	%rd77, %rd77, 8192;
	add.s64 	%rd76, %rd76, 8192;
	add.s64 	%rd75, %rd75, 1024;
	setp.lt.s64 	%p7, %rd75, %rd33;
	@%p7 bra 	$L__BB7_11;

$L__BB7_12:
	mov.b32 	%r12, %f85;
	mov.u32 	%r13, 31;
	mov.u32 	%r14, 16;
	mov.u32 	%r15, -1;
	shfl.sync.bfly.b32 	%r16|%p8, %r12, %r14, %r13, %r15;
	mov.b32 	%f61, %r16;
	add.f32 	%f62, %f85, %f61;
	mov.b32 	%r17, %f62;
	mov.u32 	%r18, 8;
	shfl.sync.bfly.b32 	%r19|%p9, %r17, %r18, %r13, %r15;
	mov.b32 	%f63, %r19;
	add.f32 	%f64, %f62, %f63;
	mov.b32 	%r20, %f64;
	mov.u32 	%r21, 4;
	shfl.sync.bfly.b32 	%r22|%p10, %r20, %r21, %r13, %r15;
	mov.b32 	%f65, %r22;
	add.f32 	%f66, %f64, %f65;
	mov.b32 	%r23, %f66;
	mov.u32 	%r24, 2;
	shfl.sync.bfly.b32 	%r25|%p11, %r23, %r24, %r13, %r15;
	mov.b32 	%f67, %r25;
	add.f32 	%f68, %f66, %f67;
	mov.b32 	%r26, %f68;
	mov.u32 	%r27, 1;
	shfl.sync.bfly.b32 	%r28|%p12, %r26, %r27, %r13, %r15;
	mov.b32 	%f69, %r28;
	add.f32 	%f70, %f68, %f69;
	shr.s32 	%r29, %r1, 31;
	shr.u32 	%r30, %r29, 27;
	add.s32 	%r31, %r1, %r30;
	shr.s32 	%r32, %r31, 5;
	shl.b32 	%r33, %r32, 2;
	add.s32 	%r35, %r9, %r33;
	st.shared.f32 	[%r35], %f70;
	bar.sync 	0;
	@%p2 bra 	$L__BB7_15;

	ld.shared.f32 	%f71, [%r2];
	mov.b32 	%r36, %f71;
	shfl.sync.bfly.b32 	%r40|%p14, %r36, %r14, %r13, %r15;
	mov.b32 	%f72, %r40;
	add.f32 	%f73, %f71, %f72;
	mov.b32 	%r41, %f73;
	shfl.sync.bfly.b32 	%r43|%p15, %r41, %r18, %r13, %r15;
	mov.b32 	%f74, %r43;
	add.f32 	%f75, %f73, %f74;
	mov.b32 	%r44, %f75;
	shfl.sync.bfly.b32 	%r46|%p16, %r44, %r21, %r13, %r15;
	mov.b32 	%f76, %r46;
	add.f32 	%f77, %f75, %f76;
	mov.b32 	%r47, %f77;
	shfl.sync.bfly.b32 	%r49|%p17, %r47, %r24, %r13, %r15;
	mov.b32 	%f78, %r49;
	add.f32 	%f79, %f77, %f78;
	mov.b32 	%r50, %f79;
	shfl.sync.bfly.b32 	%r52|%p18, %r50, %r27, %r13, %r15;
	mov.b32 	%f80, %r52;
	add.f32 	%f8, %f79, %f80;
	setp.ne.s32 	%p19, %r1, 0;
	@%p19 bra 	$L__BB7_15;

	mul.lo.s64 	%rd65, %rd4, %rd38;
	add.s64 	%rd66, %rd65, %rd3;
	cvta.to.global.u64 	%rd67, %rd32;
	shl.b64 	%rd68, %rd66, 2;
	add.s64 	%rd69, %rd67, %rd68;
	st.global.f32 	[%rd69], %f8;

$L__BB7_15:
	ret;

}
	// .globl	ggml_matvec_f16_acc_f32_bs_32
.visible .entry ggml_matvec_f16_acc_f32_bs_32(
	.param .u64 ggml_matvec_f16_acc_f32_bs_32_param_0,
	.param .u64 ggml_matvec_f16_acc_f32_bs_32_param_1,
	.param .u64 ggml_matvec_f16_acc_f32_bs_32_param_2,
	.param .u64 ggml_matvec_f16_acc_f32_bs_32_param_3,
	.param .u64 ggml_matvec_f16_acc_f32_bs_32_param_4,
	.param .u64 ggml_matvec_f16_acc_f32_bs_32_param_5,
	.param .u64 ggml_matvec_f16_acc_f32_bs_32_param_6,
	.param .u64 ggml_matvec_f16_acc_f32_bs_32_param_7,
	.param .u64 ggml_matvec_f16_acc_f32_bs_32_param_8,
	.param .u64 ggml_matvec_f16_acc_f32_bs_32_param_9
)
{
	.reg .pred 	%p<13>;
	.reg .f32 	%f<65>;
	.reg .b32 	%r<35>;
	.reg .b64 	%rd<78>;


	ld.param.u64 	%rd38, [ggml_matvec_f16_acc_f32_bs_32_param_0];
	ld.param.u64 	%rd39, [ggml_matvec_f16_acc_f32_bs_32_param_1];
	ld.param.u64 	%rd31, [ggml_matvec_f16_acc_f32_bs_32_param_2];
	ld.param.u64 	%rd32, [ggml_matvec_f16_acc_f32_bs_32_param_3];
	ld.param.u64 	%rd33, [ggml_matvec_f16_acc_f32_bs_32_param_5];
	ld.param.u64 	%rd34, [ggml_matvec_f16_acc_f32_bs_32_param_6];
	ld.param.u64 	%rd35, [ggml_matvec_f16_acc_f32_bs_32_param_7];
	ld.param.u64 	%rd36, [ggml_matvec_f16_acc_f32_bs_32_param_8];
	ld.param.u64 	%rd37, [ggml_matvec_f16_acc_f32_bs_32_param_9];
	cvta.to.global.u64 	%rd1, %rd39;
	cvta.to.global.u64 	%rd2, %rd38;
	mov.u32 	%r1, %ctaid.x;
	cvt.u64.u32 	%rd3, %r1;
	mov.u32 	%r2, %ctaid.y;
	cvt.u64.u32 	%rd4, %r2;
	and.b64  	%rd40, %rd34, -4294967296;
	setp.eq.s64 	%p1, %rd40, 0;
	@%p1 bra 	$L__BB8_2;

	div.s64 	%rd69, %rd4, %rd34;
	bra.uni 	$L__BB8_3;

$L__BB8_2:
	cvt.u32.u64 	%r3, %rd34;
	cvt.u32.u64 	%r4, %rd4;
	div.u32 	%r5, %r4, %r3;
	cvt.u64.u32 	%rd69, %r5;

$L__BB8_3:
	mov.u32 	%r6, %tid.x;
	cvt.s64.s32 	%rd8, %r6;
	setp.ge.s64 	%p2, %rd8, %rd32;
	mov.f32 	%f64, 0f00000000;
	@%p2 bra 	$L__BB8_10;

	not.b64 	%rd41, %rd8;
	add.s64 	%rd9, %rd41, %rd32;
	shr.u64 	%rd42, %rd9, 5;
	add.s64 	%rd43, %rd42, 1;
	and.b64  	%rd10, %rd43, 3;
	setp.eq.s64 	%p3, %rd10, 0;
	mov.f32 	%f64, 0f00000000;
	mov.u64 	%rd74, %rd8;
	@%p3 bra 	$L__BB8_7;

	shl.b64 	%rd44, %rd8, 1;
	mul.lo.s64 	%rd45, %rd36, %rd4;
	add.s64 	%rd46, %rd44, %rd45;
	shl.b64 	%rd47, %rd46, 2;
	add.s64 	%rd72, %rd1, %rd47;
	mul.lo.s64 	%rd48, %rd69, %rd35;
	add.s64 	%rd49, %rd44, %rd48;
	mul.lo.s64 	%rd50, %rd33, %rd3;
	add.s64 	%rd51, %rd49, %rd50;
	shl.b64 	%rd52, %rd51, 1;
	add.s64 	%rd71, %rd2, %rd52;
	neg.s64 	%rd70, %rd10;
	mov.f32 	%f64, 0f00000000;
	mov.u64 	%rd74, %rd8;

$L__BB8_6:
	.pragma "nounroll";
	ld.global.nc.u32 	%r7, [%rd71];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r7;
  cvt.f32.f16 %f13, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r7;
  cvt.f32.f16 %f14, high;}

	// end inline asm
	ld.global.nc.v2.f32 	{%f15, %f16}, [%rd72];
	fma.rn.f32 	%f19, %f13, %f15, %f64;
	fma.rn.f32 	%f64, %f14, %f16, %f19;
	add.s64 	%rd74, %rd74, 32;
	add.s64 	%rd72, %rd72, 256;
	add.s64 	%rd71, %rd71, 128;
	add.s64 	%rd70, %rd70, 1;
	setp.ne.s64 	%p4, %rd70, 0;
	@%p4 bra 	$L__BB8_6;

$L__BB8_7:
	setp.lt.u64 	%p5, %rd9, 96;
	@%p5 bra 	$L__BB8_10;

	shl.b64 	%rd53, %rd74, 1;
	mul.lo.s64 	%rd54, %rd69, %rd35;
	mul.lo.s64 	%rd55, %rd33, %rd3;
	add.s64 	%rd56, %rd55, %rd54;
	add.s64 	%rd57, %rd56, %rd53;
	shl.b64 	%rd58, %rd57, 1;
	add.s64 	%rd59, %rd2, %rd58;
	add.s64 	%rd76, %rd59, 256;
	mul.lo.s64 	%rd60, %rd36, %rd4;
	add.s64 	%rd61, %rd53, %rd60;
	shl.b64 	%rd62, %rd61, 2;
	add.s64 	%rd63, %rd1, %rd62;
	add.s64 	%rd75, %rd63, 512;

$L__BB8_9:
	ld.global.nc.u32 	%r9, [%rd76+-256];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r9;
  cvt.f32.f16 %f20, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r9;
  cvt.f32.f16 %f21, high;}

	// end inline asm
	ld.global.nc.v2.f32 	{%f28, %f29}, [%rd75+-512];
	fma.rn.f32 	%f32, %f20, %f28, %f64;
	fma.rn.f32 	%f33, %f21, %f29, %f32;
	ld.global.nc.u32 	%r11, [%rd76+-128];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r11;
  cvt.f32.f16 %f22, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r11;
  cvt.f32.f16 %f23, high;}

	// end inline asm
	ld.global.nc.v2.f32 	{%f34, %f35}, [%rd75+-256];
	fma.rn.f32 	%f38, %f22, %f34, %f33;
	fma.rn.f32 	%f39, %f23, %f35, %f38;
	ld.global.nc.u32 	%r13, [%rd76];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r13;
  cvt.f32.f16 %f24, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r13;
  cvt.f32.f16 %f25, high;}

	// end inline asm
	ld.global.nc.v2.f32 	{%f40, %f41}, [%rd75];
	fma.rn.f32 	%f44, %f24, %f40, %f39;
	fma.rn.f32 	%f45, %f25, %f41, %f44;
	ld.global.nc.u32 	%r15, [%rd76+128];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r15;
  cvt.f32.f16 %f26, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r15;
  cvt.f32.f16 %f27, high;}

	// end inline asm
	ld.global.nc.v2.f32 	{%f46, %f47}, [%rd75+256];
	fma.rn.f32 	%f50, %f26, %f46, %f45;
	fma.rn.f32 	%f64, %f27, %f47, %f50;
	add.s64 	%rd76, %rd76, 512;
	add.s64 	%rd75, %rd75, 1024;
	add.s64 	%rd74, %rd74, 128;
	setp.lt.s64 	%p6, %rd74, %rd32;
	@%p6 bra 	$L__BB8_9;

$L__BB8_10:
	mov.b32 	%r17, %f64;
	mov.u32 	%r18, 31;
	mov.u32 	%r19, 16;
	mov.u32 	%r20, -1;
	shfl.sync.bfly.b32 	%r21|%p7, %r17, %r19, %r18, %r20;
	mov.b32 	%f51, %r21;
	add.f32 	%f52, %f64, %f51;
	mov.b32 	%r22, %f52;
	mov.u32 	%r23, 8;
	shfl.sync.bfly.b32 	%r24|%p8, %r22, %r23, %r18, %r20;
	mov.b32 	%f53, %r24;
	add.f32 	%f54, %f52, %f53;
	mov.b32 	%r25, %f54;
	mov.u32 	%r26, 4;
	shfl.sync.bfly.b32 	%r27|%p9, %r25, %r26, %r18, %r20;
	mov.b32 	%f55, %r27;
	add.f32 	%f56, %f54, %f55;
	mov.b32 	%r28, %f56;
	mov.u32 	%r29, 2;
	shfl.sync.bfly.b32 	%r30|%p10, %r28, %r29, %r18, %r20;
	mov.b32 	%f57, %r30;
	add.f32 	%f58, %f56, %f57;
	mov.b32 	%r31, %f58;
	mov.u32 	%r32, 1;
	shfl.sync.bfly.b32 	%r33|%p11, %r31, %r32, %r18, %r20;
	mov.b32 	%f59, %r33;
	add.f32 	%f8, %f58, %f59;
	cvt.u32.u64 	%r34, %rd8;
	setp.ne.s32 	%p12, %r34, 0;
	@%p12 bra 	$L__BB8_12;

	mul.lo.s64 	%rd64, %rd4, %rd37;
	add.s64 	%rd65, %rd64, %rd3;
	cvta.to.global.u64 	%rd66, %rd31;
	shl.b64 	%rd67, %rd65, 2;
	add.s64 	%rd68, %rd66, %rd67;
	st.global.f32 	[%rd68], %f8;

$L__BB8_12:
	ret;

}
	// .globl	ggml_matvec_f16_acc_f32_bs_64
.visible .entry ggml_matvec_f16_acc_f32_bs_64(
	.param .u64 ggml_matvec_f16_acc_f32_bs_64_param_0,
	.param .u64 ggml_matvec_f16_acc_f32_bs_64_param_1,
	.param .u64 ggml_matvec_f16_acc_f32_bs_64_param_2,
	.param .u64 ggml_matvec_f16_acc_f32_bs_64_param_3,
	.param .u64 ggml_matvec_f16_acc_f32_bs_64_param_4,
	.param .u64 ggml_matvec_f16_acc_f32_bs_64_param_5,
	.param .u64 ggml_matvec_f16_acc_f32_bs_64_param_6,
	.param .u64 ggml_matvec_f16_acc_f32_bs_64_param_7,
	.param .u64 ggml_matvec_f16_acc_f32_bs_64_param_8,
	.param .u64 ggml_matvec_f16_acc_f32_bs_64_param_9
)
{
	.reg .pred 	%p<20>;
	.reg .f32 	%f<76>;
	.reg .b32 	%r<63>;
	.reg .b64 	%rd<79>;


	ld.param.u64 	%rd39, [ggml_matvec_f16_acc_f32_bs_64_param_0];
	ld.param.u64 	%rd40, [ggml_matvec_f16_acc_f32_bs_64_param_1];
	ld.param.u64 	%rd32, [ggml_matvec_f16_acc_f32_bs_64_param_2];
	ld.param.u64 	%rd33, [ggml_matvec_f16_acc_f32_bs_64_param_3];
	ld.param.u64 	%rd34, [ggml_matvec_f16_acc_f32_bs_64_param_5];
	ld.param.u64 	%rd35, [ggml_matvec_f16_acc_f32_bs_64_param_6];
	ld.param.u64 	%rd36, [ggml_matvec_f16_acc_f32_bs_64_param_7];
	ld.param.u64 	%rd37, [ggml_matvec_f16_acc_f32_bs_64_param_8];
	ld.param.u64 	%rd38, [ggml_matvec_f16_acc_f32_bs_64_param_9];
	cvta.to.global.u64 	%rd1, %rd40;
	cvta.to.global.u64 	%rd2, %rd39;
	mov.u32 	%r3, %ctaid.x;
	cvt.u64.u32 	%rd3, %r3;
	mov.u32 	%r4, %ctaid.y;
	cvt.u64.u32 	%rd4, %r4;
	and.b64  	%rd41, %rd35, -4294967296;
	setp.eq.s64 	%p1, %rd41, 0;
	@%p1 bra 	$L__BB9_2;

	div.s64 	%rd70, %rd4, %rd35;
	bra.uni 	$L__BB9_3;

$L__BB9_2:
	cvt.u32.u64 	%r5, %rd35;
	cvt.u32.u64 	%r6, %rd4;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd70, %r7;

$L__BB9_3:
	mov.u32 	%r1, %tid.x;
	setp.gt.s32 	%p2, %r1, 31;
	shl.b32 	%r8, %r1, 2;
	mov.u32 	%r9, data_mmv;
	add.s32 	%r2, %r9, %r8;
	@%p2 bra 	$L__BB9_5;

	mov.u32 	%r10, 0;
	st.shared.u32 	[%r2], %r10;

$L__BB9_5:
	bar.sync 	0;
	cvt.s64.s32 	%rd75, %r1;
	setp.ge.s64 	%p3, %rd75, %rd33;
	mov.f32 	%f75, 0f00000000;
	@%p3 bra 	$L__BB9_12;

	not.b64 	%rd42, %rd75;
	add.s64 	%rd9, %rd42, %rd33;
	shr.u64 	%rd43, %rd9, 6;
	add.s64 	%rd44, %rd43, 1;
	and.b64  	%rd10, %rd44, 3;
	setp.eq.s64 	%p4, %rd10, 0;
	mov.f32 	%f75, 0f00000000;
	@%p4 bra 	$L__BB9_9;

	cvt.s64.s32 	%rd75, %r1;
	mul.wide.s32 	%rd45, %r1, 2;
	mul.lo.s64 	%rd46, %rd37, %rd4;
	add.s64 	%rd47, %rd45, %rd46;
	shl.b64 	%rd48, %rd47, 2;
	add.s64 	%rd73, %rd1, %rd48;
	mul.lo.s64 	%rd49, %rd70, %rd36;
	add.s64 	%rd50, %rd45, %rd49;
	mul.lo.s64 	%rd51, %rd34, %rd3;
	add.s64 	%rd52, %rd50, %rd51;
	shl.b64 	%rd53, %rd52, 1;
	add.s64 	%rd72, %rd2, %rd53;
	neg.s64 	%rd71, %rd10;
	mov.f32 	%f75, 0f00000000;

$L__BB9_8:
	.pragma "nounroll";
	ld.global.nc.u32 	%r12, [%rd72];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r12;
  cvt.f32.f16 %f13, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r12;
  cvt.f32.f16 %f14, high;}

	// end inline asm
	ld.global.nc.v2.f32 	{%f15, %f16}, [%rd73];
	fma.rn.f32 	%f19, %f13, %f15, %f75;
	fma.rn.f32 	%f75, %f14, %f16, %f19;
	add.s64 	%rd75, %rd75, 64;
	add.s64 	%rd73, %rd73, 512;
	add.s64 	%rd72, %rd72, 256;
	add.s64 	%rd71, %rd71, 1;
	setp.ne.s64 	%p5, %rd71, 0;
	@%p5 bra 	$L__BB9_8;

$L__BB9_9:
	setp.lt.u64 	%p6, %rd9, 192;
	@%p6 bra 	$L__BB9_12;

	shl.b64 	%rd54, %rd75, 1;
	mul.lo.s64 	%rd55, %rd70, %rd36;
	mul.lo.s64 	%rd56, %rd34, %rd3;
	add.s64 	%rd57, %rd56, %rd55;
	add.s64 	%rd58, %rd57, %rd54;
	shl.b64 	%rd59, %rd58, 1;
	add.s64 	%rd60, %rd2, %rd59;
	add.s64 	%rd77, %rd60, 512;
	mul.lo.s64 	%rd61, %rd37, %rd4;
	add.s64 	%rd62, %rd54, %rd61;
	shl.b64 	%rd63, %rd62, 2;
	add.s64 	%rd64, %rd1, %rd63;
	add.s64 	%rd76, %rd64, 1024;

$L__BB9_11:
	ld.global.nc.u32 	%r14, [%rd77+-512];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r14;
  cvt.f32.f16 %f20, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r14;
  cvt.f32.f16 %f21, high;}

	// end inline asm
	ld.global.nc.v2.f32 	{%f28, %f29}, [%rd76+-1024];
	fma.rn.f32 	%f32, %f20, %f28, %f75;
	fma.rn.f32 	%f33, %f21, %f29, %f32;
	ld.global.nc.u32 	%r16, [%rd77+-256];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r16;
  cvt.f32.f16 %f22, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r16;
  cvt.f32.f16 %f23, high;}

	// end inline asm
	ld.global.nc.v2.f32 	{%f34, %f35}, [%rd76+-512];
	fma.rn.f32 	%f38, %f22, %f34, %f33;
	fma.rn.f32 	%f39, %f23, %f35, %f38;
	ld.global.nc.u32 	%r18, [%rd77];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r18;
  cvt.f32.f16 %f24, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r18;
  cvt.f32.f16 %f25, high;}

	// end inline asm
	ld.global.nc.v2.f32 	{%f40, %f41}, [%rd76];
	fma.rn.f32 	%f44, %f24, %f40, %f39;
	fma.rn.f32 	%f45, %f25, %f41, %f44;
	ld.global.nc.u32 	%r20, [%rd77+256];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r20;
  cvt.f32.f16 %f26, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r20;
  cvt.f32.f16 %f27, high;}

	// end inline asm
	ld.global.nc.v2.f32 	{%f46, %f47}, [%rd76+512];
	fma.rn.f32 	%f50, %f26, %f46, %f45;
	fma.rn.f32 	%f75, %f27, %f47, %f50;
	add.s64 	%rd77, %rd77, 1024;
	add.s64 	%rd76, %rd76, 2048;
	add.s64 	%rd75, %rd75, 256;
	setp.lt.s64 	%p7, %rd75, %rd33;
	@%p7 bra 	$L__BB9_11;

$L__BB9_12:
	mov.b32 	%r22, %f75;
	mov.u32 	%r23, 31;
	mov.u32 	%r24, 16;
	mov.u32 	%r25, -1;
	shfl.sync.bfly.b32 	%r26|%p8, %r22, %r24, %r23, %r25;
	mov.b32 	%f51, %r26;
	add.f32 	%f52, %f75, %f51;
	mov.b32 	%r27, %f52;
	mov.u32 	%r28, 8;
	shfl.sync.bfly.b32 	%r29|%p9, %r27, %r28, %r23, %r25;
	mov.b32 	%f53, %r29;
	add.f32 	%f54, %f52, %f53;
	mov.b32 	%r30, %f54;
	mov.u32 	%r31, 4;
	shfl.sync.bfly.b32 	%r32|%p10, %r30, %r31, %r23, %r25;
	mov.b32 	%f55, %r32;
	add.f32 	%f56, %f54, %f55;
	mov.b32 	%r33, %f56;
	mov.u32 	%r34, 2;
	shfl.sync.bfly.b32 	%r35|%p11, %r33, %r34, %r23, %r25;
	mov.b32 	%f57, %r35;
	add.f32 	%f58, %f56, %f57;
	mov.b32 	%r36, %f58;
	mov.u32 	%r37, 1;
	shfl.sync.bfly.b32 	%r38|%p12, %r36, %r37, %r23, %r25;
	mov.b32 	%f59, %r38;
	add.f32 	%f60, %f58, %f59;
	shr.s32 	%r39, %r1, 31;
	shr.u32 	%r40, %r39, 27;
	add.s32 	%r41, %r1, %r40;
	shr.s32 	%r42, %r41, 5;
	shl.b32 	%r43, %r42, 2;
	add.s32 	%r45, %r9, %r43;
	st.shared.f32 	[%r45], %f60;
	bar.sync 	0;
	@%p2 bra 	$L__BB9_15;

	ld.shared.f32 	%f61, [%r2];
	mov.b32 	%r46, %f61;
	shfl.sync.bfly.b32 	%r50|%p14, %r46, %r24, %r23, %r25;
	mov.b32 	%f62, %r50;
	add.f32 	%f63, %f61, %f62;
	mov.b32 	%r51, %f63;
	shfl.sync.bfly.b32 	%r53|%p15, %r51, %r28, %r23, %r25;
	mov.b32 	%f64, %r53;
	add.f32 	%f65, %f63, %f64;
	mov.b32 	%r54, %f65;
	shfl.sync.bfly.b32 	%r56|%p16, %r54, %r31, %r23, %r25;
	mov.b32 	%f66, %r56;
	add.f32 	%f67, %f65, %f66;
	mov.b32 	%r57, %f67;
	shfl.sync.bfly.b32 	%r59|%p17, %r57, %r34, %r23, %r25;
	mov.b32 	%f68, %r59;
	add.f32 	%f69, %f67, %f68;
	mov.b32 	%r60, %f69;
	shfl.sync.bfly.b32 	%r62|%p18, %r60, %r37, %r23, %r25;
	mov.b32 	%f70, %r62;
	add.f32 	%f8, %f69, %f70;
	setp.ne.s32 	%p19, %r1, 0;
	@%p19 bra 	$L__BB9_15;

	mul.lo.s64 	%rd65, %rd4, %rd38;
	add.s64 	%rd66, %rd65, %rd3;
	cvta.to.global.u64 	%rd67, %rd32;
	shl.b64 	%rd68, %rd66, 2;
	add.s64 	%rd69, %rd67, %rd68;
	st.global.f32 	[%rd69], %f8;

$L__BB9_15:
	ret;

}
	// .globl	ggml_matvec_f16_acc_f32_bs_96
.visible .entry ggml_matvec_f16_acc_f32_bs_96(
	.param .u64 ggml_matvec_f16_acc_f32_bs_96_param_0,
	.param .u64 ggml_matvec_f16_acc_f32_bs_96_param_1,
	.param .u64 ggml_matvec_f16_acc_f32_bs_96_param_2,
	.param .u64 ggml_matvec_f16_acc_f32_bs_96_param_3,
	.param .u64 ggml_matvec_f16_acc_f32_bs_96_param_4,
	.param .u64 ggml_matvec_f16_acc_f32_bs_96_param_5,
	.param .u64 ggml_matvec_f16_acc_f32_bs_96_param_6,
	.param .u64 ggml_matvec_f16_acc_f32_bs_96_param_7,
	.param .u64 ggml_matvec_f16_acc_f32_bs_96_param_8,
	.param .u64 ggml_matvec_f16_acc_f32_bs_96_param_9
)
{
	.reg .pred 	%p<20>;
	.reg .f32 	%f<76>;
	.reg .b32 	%r<63>;
	.reg .b64 	%rd<80>;


	ld.param.u64 	%rd39, [ggml_matvec_f16_acc_f32_bs_96_param_0];
	ld.param.u64 	%rd40, [ggml_matvec_f16_acc_f32_bs_96_param_1];
	ld.param.u64 	%rd32, [ggml_matvec_f16_acc_f32_bs_96_param_2];
	ld.param.u64 	%rd33, [ggml_matvec_f16_acc_f32_bs_96_param_3];
	ld.param.u64 	%rd34, [ggml_matvec_f16_acc_f32_bs_96_param_5];
	ld.param.u64 	%rd35, [ggml_matvec_f16_acc_f32_bs_96_param_6];
	ld.param.u64 	%rd36, [ggml_matvec_f16_acc_f32_bs_96_param_7];
	ld.param.u64 	%rd37, [ggml_matvec_f16_acc_f32_bs_96_param_8];
	ld.param.u64 	%rd38, [ggml_matvec_f16_acc_f32_bs_96_param_9];
	cvta.to.global.u64 	%rd1, %rd40;
	cvta.to.global.u64 	%rd2, %rd39;
	mov.u32 	%r3, %ctaid.x;
	cvt.u64.u32 	%rd3, %r3;
	mov.u32 	%r4, %ctaid.y;
	cvt.u64.u32 	%rd4, %r4;
	and.b64  	%rd41, %rd35, -4294967296;
	setp.eq.s64 	%p1, %rd41, 0;
	@%p1 bra 	$L__BB10_2;

	div.s64 	%rd71, %rd4, %rd35;
	bra.uni 	$L__BB10_3;

$L__BB10_2:
	cvt.u32.u64 	%r5, %rd35;
	cvt.u32.u64 	%r6, %rd4;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd71, %r7;

$L__BB10_3:
	mov.u32 	%r1, %tid.x;
	setp.gt.s32 	%p2, %r1, 31;
	shl.b32 	%r8, %r1, 2;
	mov.u32 	%r9, data_mmv;
	add.s32 	%r2, %r9, %r8;
	@%p2 bra 	$L__BB10_5;

	mov.u32 	%r10, 0;
	st.shared.u32 	[%r2], %r10;

$L__BB10_5:
	bar.sync 	0;
	cvt.s64.s32 	%rd76, %r1;
	setp.ge.s64 	%p3, %rd76, %rd33;
	mov.f32 	%f75, 0f00000000;
	@%p3 bra 	$L__BB10_12;

	not.b64 	%rd42, %rd76;
	add.s64 	%rd9, %rd42, %rd33;
	mul.hi.u64 	%rd43, %rd9, -6148914691236517205;
	shr.u64 	%rd44, %rd43, 6;
	add.s64 	%rd45, %rd44, 1;
	and.b64  	%rd10, %rd45, 3;
	setp.eq.s64 	%p4, %rd10, 0;
	mov.f32 	%f75, 0f00000000;
	@%p4 bra 	$L__BB10_9;

	cvt.s64.s32 	%rd76, %r1;
	mul.wide.s32 	%rd46, %r1, 2;
	mul.lo.s64 	%rd47, %rd37, %rd4;
	add.s64 	%rd48, %rd46, %rd47;
	shl.b64 	%rd49, %rd48, 2;
	add.s64 	%rd74, %rd1, %rd49;
	mul.lo.s64 	%rd50, %rd71, %rd36;
	add.s64 	%rd51, %rd46, %rd50;
	mul.lo.s64 	%rd52, %rd34, %rd3;
	add.s64 	%rd53, %rd51, %rd52;
	shl.b64 	%rd54, %rd53, 1;
	add.s64 	%rd73, %rd2, %rd54;
	neg.s64 	%rd72, %rd10;
	mov.f32 	%f75, 0f00000000;

$L__BB10_8:
	.pragma "nounroll";
	ld.global.nc.u32 	%r12, [%rd73];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r12;
  cvt.f32.f16 %f13, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r12;
  cvt.f32.f16 %f14, high;}

	// end inline asm
	ld.global.nc.v2.f32 	{%f15, %f16}, [%rd74];
	fma.rn.f32 	%f19, %f13, %f15, %f75;
	fma.rn.f32 	%f75, %f14, %f16, %f19;
	add.s64 	%rd76, %rd76, 96;
	add.s64 	%rd74, %rd74, 768;
	add.s64 	%rd73, %rd73, 384;
	add.s64 	%rd72, %rd72, 1;
	setp.ne.s64 	%p5, %rd72, 0;
	@%p5 bra 	$L__BB10_8;

$L__BB10_9:
	setp.lt.u64 	%p6, %rd9, 288;
	@%p6 bra 	$L__BB10_12;

	shl.b64 	%rd55, %rd76, 1;
	mul.lo.s64 	%rd56, %rd71, %rd36;
	mul.lo.s64 	%rd57, %rd34, %rd3;
	add.s64 	%rd58, %rd57, %rd56;
	add.s64 	%rd59, %rd58, %rd55;
	shl.b64 	%rd60, %rd59, 1;
	add.s64 	%rd61, %rd2, %rd60;
	add.s64 	%rd78, %rd61, 768;
	mul.lo.s64 	%rd62, %rd37, %rd4;
	add.s64 	%rd63, %rd55, %rd62;
	shl.b64 	%rd64, %rd63, 2;
	add.s64 	%rd65, %rd1, %rd64;
	add.s64 	%rd77, %rd65, 1536;

$L__BB10_11:
	ld.global.nc.u32 	%r14, [%rd78+-768];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r14;
  cvt.f32.f16 %f20, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r14;
  cvt.f32.f16 %f21, high;}

	// end inline asm
	ld.global.nc.v2.f32 	{%f28, %f29}, [%rd77+-1536];
	fma.rn.f32 	%f32, %f20, %f28, %f75;
	fma.rn.f32 	%f33, %f21, %f29, %f32;
	ld.global.nc.u32 	%r16, [%rd78+-384];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r16;
  cvt.f32.f16 %f22, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r16;
  cvt.f32.f16 %f23, high;}

	// end inline asm
	ld.global.nc.v2.f32 	{%f34, %f35}, [%rd77+-768];
	fma.rn.f32 	%f38, %f22, %f34, %f33;
	fma.rn.f32 	%f39, %f23, %f35, %f38;
	ld.global.nc.u32 	%r18, [%rd78];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r18;
  cvt.f32.f16 %f24, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r18;
  cvt.f32.f16 %f25, high;}

	// end inline asm
	ld.global.nc.v2.f32 	{%f40, %f41}, [%rd77];
	fma.rn.f32 	%f44, %f24, %f40, %f39;
	fma.rn.f32 	%f45, %f25, %f41, %f44;
	ld.global.nc.u32 	%r20, [%rd78+384];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r20;
  cvt.f32.f16 %f26, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r20;
  cvt.f32.f16 %f27, high;}

	// end inline asm
	ld.global.nc.v2.f32 	{%f46, %f47}, [%rd77+768];
	fma.rn.f32 	%f50, %f26, %f46, %f45;
	fma.rn.f32 	%f75, %f27, %f47, %f50;
	add.s64 	%rd78, %rd78, 1536;
	add.s64 	%rd77, %rd77, 3072;
	add.s64 	%rd76, %rd76, 384;
	setp.lt.s64 	%p7, %rd76, %rd33;
	@%p7 bra 	$L__BB10_11;

$L__BB10_12:
	mov.b32 	%r22, %f75;
	mov.u32 	%r23, 31;
	mov.u32 	%r24, 16;
	mov.u32 	%r25, -1;
	shfl.sync.bfly.b32 	%r26|%p8, %r22, %r24, %r23, %r25;
	mov.b32 	%f51, %r26;
	add.f32 	%f52, %f75, %f51;
	mov.b32 	%r27, %f52;
	mov.u32 	%r28, 8;
	shfl.sync.bfly.b32 	%r29|%p9, %r27, %r28, %r23, %r25;
	mov.b32 	%f53, %r29;
	add.f32 	%f54, %f52, %f53;
	mov.b32 	%r30, %f54;
	mov.u32 	%r31, 4;
	shfl.sync.bfly.b32 	%r32|%p10, %r30, %r31, %r23, %r25;
	mov.b32 	%f55, %r32;
	add.f32 	%f56, %f54, %f55;
	mov.b32 	%r33, %f56;
	mov.u32 	%r34, 2;
	shfl.sync.bfly.b32 	%r35|%p11, %r33, %r34, %r23, %r25;
	mov.b32 	%f57, %r35;
	add.f32 	%f58, %f56, %f57;
	mov.b32 	%r36, %f58;
	mov.u32 	%r37, 1;
	shfl.sync.bfly.b32 	%r38|%p12, %r36, %r37, %r23, %r25;
	mov.b32 	%f59, %r38;
	add.f32 	%f60, %f58, %f59;
	shr.s32 	%r39, %r1, 31;
	shr.u32 	%r40, %r39, 27;
	add.s32 	%r41, %r1, %r40;
	shr.s32 	%r42, %r41, 5;
	shl.b32 	%r43, %r42, 2;
	add.s32 	%r45, %r9, %r43;
	st.shared.f32 	[%r45], %f60;
	bar.sync 	0;
	@%p2 bra 	$L__BB10_15;

	ld.shared.f32 	%f61, [%r2];
	mov.b32 	%r46, %f61;
	shfl.sync.bfly.b32 	%r50|%p14, %r46, %r24, %r23, %r25;
	mov.b32 	%f62, %r50;
	add.f32 	%f63, %f61, %f62;
	mov.b32 	%r51, %f63;
	shfl.sync.bfly.b32 	%r53|%p15, %r51, %r28, %r23, %r25;
	mov.b32 	%f64, %r53;
	add.f32 	%f65, %f63, %f64;
	mov.b32 	%r54, %f65;
	shfl.sync.bfly.b32 	%r56|%p16, %r54, %r31, %r23, %r25;
	mov.b32 	%f66, %r56;
	add.f32 	%f67, %f65, %f66;
	mov.b32 	%r57, %f67;
	shfl.sync.bfly.b32 	%r59|%p17, %r57, %r34, %r23, %r25;
	mov.b32 	%f68, %r59;
	add.f32 	%f69, %f67, %f68;
	mov.b32 	%r60, %f69;
	shfl.sync.bfly.b32 	%r62|%p18, %r60, %r37, %r23, %r25;
	mov.b32 	%f70, %r62;
	add.f32 	%f8, %f69, %f70;
	setp.ne.s32 	%p19, %r1, 0;
	@%p19 bra 	$L__BB10_15;

	mul.lo.s64 	%rd66, %rd4, %rd38;
	add.s64 	%rd67, %rd66, %rd3;
	cvta.to.global.u64 	%rd68, %rd32;
	shl.b64 	%rd69, %rd67, 2;
	add.s64 	%rd70, %rd68, %rd69;
	st.global.f32 	[%rd70], %f8;

$L__BB10_15:
	ret;

}
	// .globl	ggml_matvec_f16_acc_f32_bs_128
.visible .entry ggml_matvec_f16_acc_f32_bs_128(
	.param .u64 ggml_matvec_f16_acc_f32_bs_128_param_0,
	.param .u64 ggml_matvec_f16_acc_f32_bs_128_param_1,
	.param .u64 ggml_matvec_f16_acc_f32_bs_128_param_2,
	.param .u64 ggml_matvec_f16_acc_f32_bs_128_param_3,
	.param .u64 ggml_matvec_f16_acc_f32_bs_128_param_4,
	.param .u64 ggml_matvec_f16_acc_f32_bs_128_param_5,
	.param .u64 ggml_matvec_f16_acc_f32_bs_128_param_6,
	.param .u64 ggml_matvec_f16_acc_f32_bs_128_param_7,
	.param .u64 ggml_matvec_f16_acc_f32_bs_128_param_8,
	.param .u64 ggml_matvec_f16_acc_f32_bs_128_param_9
)
{
	.reg .pred 	%p<20>;
	.reg .f32 	%f<76>;
	.reg .b32 	%r<63>;
	.reg .b64 	%rd<79>;


	ld.param.u64 	%rd39, [ggml_matvec_f16_acc_f32_bs_128_param_0];
	ld.param.u64 	%rd40, [ggml_matvec_f16_acc_f32_bs_128_param_1];
	ld.param.u64 	%rd32, [ggml_matvec_f16_acc_f32_bs_128_param_2];
	ld.param.u64 	%rd33, [ggml_matvec_f16_acc_f32_bs_128_param_3];
	ld.param.u64 	%rd34, [ggml_matvec_f16_acc_f32_bs_128_param_5];
	ld.param.u64 	%rd35, [ggml_matvec_f16_acc_f32_bs_128_param_6];
	ld.param.u64 	%rd36, [ggml_matvec_f16_acc_f32_bs_128_param_7];
	ld.param.u64 	%rd37, [ggml_matvec_f16_acc_f32_bs_128_param_8];
	ld.param.u64 	%rd38, [ggml_matvec_f16_acc_f32_bs_128_param_9];
	cvta.to.global.u64 	%rd1, %rd40;
	cvta.to.global.u64 	%rd2, %rd39;
	mov.u32 	%r3, %ctaid.x;
	cvt.u64.u32 	%rd3, %r3;
	mov.u32 	%r4, %ctaid.y;
	cvt.u64.u32 	%rd4, %r4;
	and.b64  	%rd41, %rd35, -4294967296;
	setp.eq.s64 	%p1, %rd41, 0;
	@%p1 bra 	$L__BB11_2;

	div.s64 	%rd70, %rd4, %rd35;
	bra.uni 	$L__BB11_3;

$L__BB11_2:
	cvt.u32.u64 	%r5, %rd35;
	cvt.u32.u64 	%r6, %rd4;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd70, %r7;

$L__BB11_3:
	mov.u32 	%r1, %tid.x;
	setp.gt.s32 	%p2, %r1, 31;
	shl.b32 	%r8, %r1, 2;
	mov.u32 	%r9, data_mmv;
	add.s32 	%r2, %r9, %r8;
	@%p2 bra 	$L__BB11_5;

	mov.u32 	%r10, 0;
	st.shared.u32 	[%r2], %r10;

$L__BB11_5:
	bar.sync 	0;
	cvt.s64.s32 	%rd75, %r1;
	setp.ge.s64 	%p3, %rd75, %rd33;
	mov.f32 	%f75, 0f00000000;
	@%p3 bra 	$L__BB11_12;

	not.b64 	%rd42, %rd75;
	add.s64 	%rd9, %rd42, %rd33;
	shr.u64 	%rd43, %rd9, 7;
	add.s64 	%rd44, %rd43, 1;
	and.b64  	%rd10, %rd44, 3;
	setp.eq.s64 	%p4, %rd10, 0;
	mov.f32 	%f75, 0f00000000;
	@%p4 bra 	$L__BB11_9;

	cvt.s64.s32 	%rd75, %r1;
	mul.wide.s32 	%rd45, %r1, 2;
	mul.lo.s64 	%rd46, %rd37, %rd4;
	add.s64 	%rd47, %rd45, %rd46;
	shl.b64 	%rd48, %rd47, 2;
	add.s64 	%rd73, %rd1, %rd48;
	mul.lo.s64 	%rd49, %rd70, %rd36;
	add.s64 	%rd50, %rd45, %rd49;
	mul.lo.s64 	%rd51, %rd34, %rd3;
	add.s64 	%rd52, %rd50, %rd51;
	shl.b64 	%rd53, %rd52, 1;
	add.s64 	%rd72, %rd2, %rd53;
	neg.s64 	%rd71, %rd10;
	mov.f32 	%f75, 0f00000000;

$L__BB11_8:
	.pragma "nounroll";
	ld.global.nc.u32 	%r12, [%rd72];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r12;
  cvt.f32.f16 %f13, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r12;
  cvt.f32.f16 %f14, high;}

	// end inline asm
	ld.global.nc.v2.f32 	{%f15, %f16}, [%rd73];
	fma.rn.f32 	%f19, %f13, %f15, %f75;
	fma.rn.f32 	%f75, %f14, %f16, %f19;
	add.s64 	%rd75, %rd75, 128;
	add.s64 	%rd73, %rd73, 1024;
	add.s64 	%rd72, %rd72, 512;
	add.s64 	%rd71, %rd71, 1;
	setp.ne.s64 	%p5, %rd71, 0;
	@%p5 bra 	$L__BB11_8;

$L__BB11_9:
	setp.lt.u64 	%p6, %rd9, 384;
	@%p6 bra 	$L__BB11_12;

	shl.b64 	%rd54, %rd75, 1;
	mul.lo.s64 	%rd55, %rd70, %rd36;
	mul.lo.s64 	%rd56, %rd34, %rd3;
	add.s64 	%rd57, %rd56, %rd55;
	add.s64 	%rd58, %rd57, %rd54;
	shl.b64 	%rd59, %rd58, 1;
	add.s64 	%rd60, %rd2, %rd59;
	add.s64 	%rd77, %rd60, 1024;
	mul.lo.s64 	%rd61, %rd37, %rd4;
	add.s64 	%rd62, %rd54, %rd61;
	shl.b64 	%rd63, %rd62, 2;
	add.s64 	%rd64, %rd1, %rd63;
	add.s64 	%rd76, %rd64, 2048;

$L__BB11_11:
	ld.global.nc.u32 	%r14, [%rd77+-1024];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r14;
  cvt.f32.f16 %f20, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r14;
  cvt.f32.f16 %f21, high;}

	// end inline asm
	ld.global.nc.v2.f32 	{%f28, %f29}, [%rd76+-2048];
	fma.rn.f32 	%f32, %f20, %f28, %f75;
	fma.rn.f32 	%f33, %f21, %f29, %f32;
	ld.global.nc.u32 	%r16, [%rd77+-512];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r16;
  cvt.f32.f16 %f22, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r16;
  cvt.f32.f16 %f23, high;}

	// end inline asm
	ld.global.nc.v2.f32 	{%f34, %f35}, [%rd76+-1024];
	fma.rn.f32 	%f38, %f22, %f34, %f33;
	fma.rn.f32 	%f39, %f23, %f35, %f38;
	ld.global.nc.u32 	%r18, [%rd77];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r18;
  cvt.f32.f16 %f24, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r18;
  cvt.f32.f16 %f25, high;}

	// end inline asm
	ld.global.nc.v2.f32 	{%f40, %f41}, [%rd76];
	fma.rn.f32 	%f44, %f24, %f40, %f39;
	fma.rn.f32 	%f45, %f25, %f41, %f44;
	ld.global.nc.u32 	%r20, [%rd77+512];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r20;
  cvt.f32.f16 %f26, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r20;
  cvt.f32.f16 %f27, high;}

	// end inline asm
	ld.global.nc.v2.f32 	{%f46, %f47}, [%rd76+1024];
	fma.rn.f32 	%f50, %f26, %f46, %f45;
	fma.rn.f32 	%f75, %f27, %f47, %f50;
	add.s64 	%rd77, %rd77, 2048;
	add.s64 	%rd76, %rd76, 4096;
	add.s64 	%rd75, %rd75, 512;
	setp.lt.s64 	%p7, %rd75, %rd33;
	@%p7 bra 	$L__BB11_11;

$L__BB11_12:
	mov.b32 	%r22, %f75;
	mov.u32 	%r23, 31;
	mov.u32 	%r24, 16;
	mov.u32 	%r25, -1;
	shfl.sync.bfly.b32 	%r26|%p8, %r22, %r24, %r23, %r25;
	mov.b32 	%f51, %r26;
	add.f32 	%f52, %f75, %f51;
	mov.b32 	%r27, %f52;
	mov.u32 	%r28, 8;
	shfl.sync.bfly.b32 	%r29|%p9, %r27, %r28, %r23, %r25;
	mov.b32 	%f53, %r29;
	add.f32 	%f54, %f52, %f53;
	mov.b32 	%r30, %f54;
	mov.u32 	%r31, 4;
	shfl.sync.bfly.b32 	%r32|%p10, %r30, %r31, %r23, %r25;
	mov.b32 	%f55, %r32;
	add.f32 	%f56, %f54, %f55;
	mov.b32 	%r33, %f56;
	mov.u32 	%r34, 2;
	shfl.sync.bfly.b32 	%r35|%p11, %r33, %r34, %r23, %r25;
	mov.b32 	%f57, %r35;
	add.f32 	%f58, %f56, %f57;
	mov.b32 	%r36, %f58;
	mov.u32 	%r37, 1;
	shfl.sync.bfly.b32 	%r38|%p12, %r36, %r37, %r23, %r25;
	mov.b32 	%f59, %r38;
	add.f32 	%f60, %f58, %f59;
	shr.s32 	%r39, %r1, 31;
	shr.u32 	%r40, %r39, 27;
	add.s32 	%r41, %r1, %r40;
	shr.s32 	%r42, %r41, 5;
	shl.b32 	%r43, %r42, 2;
	add.s32 	%r45, %r9, %r43;
	st.shared.f32 	[%r45], %f60;
	bar.sync 	0;
	@%p2 bra 	$L__BB11_15;

	ld.shared.f32 	%f61, [%r2];
	mov.b32 	%r46, %f61;
	shfl.sync.bfly.b32 	%r50|%p14, %r46, %r24, %r23, %r25;
	mov.b32 	%f62, %r50;
	add.f32 	%f63, %f61, %f62;
	mov.b32 	%r51, %f63;
	shfl.sync.bfly.b32 	%r53|%p15, %r51, %r28, %r23, %r25;
	mov.b32 	%f64, %r53;
	add.f32 	%f65, %f63, %f64;
	mov.b32 	%r54, %f65;
	shfl.sync.bfly.b32 	%r56|%p16, %r54, %r31, %r23, %r25;
	mov.b32 	%f66, %r56;
	add.f32 	%f67, %f65, %f66;
	mov.b32 	%r57, %f67;
	shfl.sync.bfly.b32 	%r59|%p17, %r57, %r34, %r23, %r25;
	mov.b32 	%f68, %r59;
	add.f32 	%f69, %f67, %f68;
	mov.b32 	%r60, %f69;
	shfl.sync.bfly.b32 	%r62|%p18, %r60, %r37, %r23, %r25;
	mov.b32 	%f70, %r62;
	add.f32 	%f8, %f69, %f70;
	setp.ne.s32 	%p19, %r1, 0;
	@%p19 bra 	$L__BB11_15;

	mul.lo.s64 	%rd65, %rd4, %rd38;
	add.s64 	%rd66, %rd65, %rd3;
	cvta.to.global.u64 	%rd67, %rd32;
	shl.b64 	%rd68, %rd66, 2;
	add.s64 	%rd69, %rd67, %rd68;
	st.global.f32 	[%rd69], %f8;

$L__BB11_15:
	ret;

}
	// .globl	ggml_matvec_f16_acc_f32_bs_160
.visible .entry ggml_matvec_f16_acc_f32_bs_160(
	.param .u64 ggml_matvec_f16_acc_f32_bs_160_param_0,
	.param .u64 ggml_matvec_f16_acc_f32_bs_160_param_1,
	.param .u64 ggml_matvec_f16_acc_f32_bs_160_param_2,
	.param .u64 ggml_matvec_f16_acc_f32_bs_160_param_3,
	.param .u64 ggml_matvec_f16_acc_f32_bs_160_param_4,
	.param .u64 ggml_matvec_f16_acc_f32_bs_160_param_5,
	.param .u64 ggml_matvec_f16_acc_f32_bs_160_param_6,
	.param .u64 ggml_matvec_f16_acc_f32_bs_160_param_7,
	.param .u64 ggml_matvec_f16_acc_f32_bs_160_param_8,
	.param .u64 ggml_matvec_f16_acc_f32_bs_160_param_9
)
{
	.reg .pred 	%p<20>;
	.reg .f32 	%f<76>;
	.reg .b32 	%r<63>;
	.reg .b64 	%rd<80>;


	ld.param.u64 	%rd39, [ggml_matvec_f16_acc_f32_bs_160_param_0];
	ld.param.u64 	%rd40, [ggml_matvec_f16_acc_f32_bs_160_param_1];
	ld.param.u64 	%rd32, [ggml_matvec_f16_acc_f32_bs_160_param_2];
	ld.param.u64 	%rd33, [ggml_matvec_f16_acc_f32_bs_160_param_3];
	ld.param.u64 	%rd34, [ggml_matvec_f16_acc_f32_bs_160_param_5];
	ld.param.u64 	%rd35, [ggml_matvec_f16_acc_f32_bs_160_param_6];
	ld.param.u64 	%rd36, [ggml_matvec_f16_acc_f32_bs_160_param_7];
	ld.param.u64 	%rd37, [ggml_matvec_f16_acc_f32_bs_160_param_8];
	ld.param.u64 	%rd38, [ggml_matvec_f16_acc_f32_bs_160_param_9];
	cvta.to.global.u64 	%rd1, %rd40;
	cvta.to.global.u64 	%rd2, %rd39;
	mov.u32 	%r3, %ctaid.x;
	cvt.u64.u32 	%rd3, %r3;
	mov.u32 	%r4, %ctaid.y;
	cvt.u64.u32 	%rd4, %r4;
	and.b64  	%rd41, %rd35, -4294967296;
	setp.eq.s64 	%p1, %rd41, 0;
	@%p1 bra 	$L__BB12_2;

	div.s64 	%rd71, %rd4, %rd35;
	bra.uni 	$L__BB12_3;

$L__BB12_2:
	cvt.u32.u64 	%r5, %rd35;
	cvt.u32.u64 	%r6, %rd4;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd71, %r7;

$L__BB12_3:
	mov.u32 	%r1, %tid.x;
	setp.gt.s32 	%p2, %r1, 31;
	shl.b32 	%r8, %r1, 2;
	mov.u32 	%r9, data_mmv;
	add.s32 	%r2, %r9, %r8;
	@%p2 bra 	$L__BB12_5;

	mov.u32 	%r10, 0;
	st.shared.u32 	[%r2], %r10;

$L__BB12_5:
	bar.sync 	0;
	cvt.s64.s32 	%rd76, %r1;
	setp.ge.s64 	%p3, %rd76, %rd33;
	mov.f32 	%f75, 0f00000000;
	@%p3 bra 	$L__BB12_12;

	not.b64 	%rd42, %rd76;
	add.s64 	%rd9, %rd42, %rd33;
	mul.hi.u64 	%rd43, %rd9, -3689348814741910323;
	shr.u64 	%rd44, %rd43, 7;
	add.s64 	%rd45, %rd44, 1;
	and.b64  	%rd10, %rd45, 3;
	setp.eq.s64 	%p4, %rd10, 0;
	mov.f32 	%f75, 0f00000000;
	@%p4 bra 	$L__BB12_9;

	cvt.s64.s32 	%rd76, %r1;
	mul.wide.s32 	%rd46, %r1, 2;
	mul.lo.s64 	%rd47, %rd37, %rd4;
	add.s64 	%rd48, %rd46, %rd47;
	shl.b64 	%rd49, %rd48, 2;
	add.s64 	%rd74, %rd1, %rd49;
	mul.lo.s64 	%rd50, %rd71, %rd36;
	add.s64 	%rd51, %rd46, %rd50;
	mul.lo.s64 	%rd52, %rd34, %rd3;
	add.s64 	%rd53, %rd51, %rd52;
	shl.b64 	%rd54, %rd53, 1;
	add.s64 	%rd73, %rd2, %rd54;
	neg.s64 	%rd72, %rd10;
	mov.f32 	%f75, 0f00000000;

$L__BB12_8:
	.pragma "nounroll";
	ld.global.nc.u32 	%r12, [%rd73];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r12;
  cvt.f32.f16 %f13, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r12;
  cvt.f32.f16 %f14, high;}

	// end inline asm
	ld.global.nc.v2.f32 	{%f15, %f16}, [%rd74];
	fma.rn.f32 	%f19, %f13, %f15, %f75;
	fma.rn.f32 	%f75, %f14, %f16, %f19;
	add.s64 	%rd76, %rd76, 160;
	add.s64 	%rd74, %rd74, 1280;
	add.s64 	%rd73, %rd73, 640;
	add.s64 	%rd72, %rd72, 1;
	setp.ne.s64 	%p5, %rd72, 0;
	@%p5 bra 	$L__BB12_8;

$L__BB12_9:
	setp.lt.u64 	%p6, %rd9, 480;
	@%p6 bra 	$L__BB12_12;

	shl.b64 	%rd55, %rd76, 1;
	mul.lo.s64 	%rd56, %rd71, %rd36;
	mul.lo.s64 	%rd57, %rd34, %rd3;
	add.s64 	%rd58, %rd57, %rd56;
	add.s64 	%rd59, %rd58, %rd55;
	shl.b64 	%rd60, %rd59, 1;
	add.s64 	%rd61, %rd2, %rd60;
	add.s64 	%rd78, %rd61, 1280;
	mul.lo.s64 	%rd62, %rd37, %rd4;
	add.s64 	%rd63, %rd55, %rd62;
	shl.b64 	%rd64, %rd63, 2;
	add.s64 	%rd65, %rd1, %rd64;
	add.s64 	%rd77, %rd65, 2560;

$L__BB12_11:
	ld.global.nc.u32 	%r14, [%rd78+-1280];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r14;
  cvt.f32.f16 %f20, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r14;
  cvt.f32.f16 %f21, high;}

	// end inline asm
	ld.global.nc.v2.f32 	{%f28, %f29}, [%rd77+-2560];
	fma.rn.f32 	%f32, %f20, %f28, %f75;
	fma.rn.f32 	%f33, %f21, %f29, %f32;
	ld.global.nc.u32 	%r16, [%rd78+-640];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r16;
  cvt.f32.f16 %f22, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r16;
  cvt.f32.f16 %f23, high;}

	// end inline asm
	ld.global.nc.v2.f32 	{%f34, %f35}, [%rd77+-1280];
	fma.rn.f32 	%f38, %f22, %f34, %f33;
	fma.rn.f32 	%f39, %f23, %f35, %f38;
	ld.global.nc.u32 	%r18, [%rd78];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r18;
  cvt.f32.f16 %f24, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r18;
  cvt.f32.f16 %f25, high;}

	// end inline asm
	ld.global.nc.v2.f32 	{%f40, %f41}, [%rd77];
	fma.rn.f32 	%f44, %f24, %f40, %f39;
	fma.rn.f32 	%f45, %f25, %f41, %f44;
	ld.global.nc.u32 	%r20, [%rd78+640];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r20;
  cvt.f32.f16 %f26, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r20;
  cvt.f32.f16 %f27, high;}

	// end inline asm
	ld.global.nc.v2.f32 	{%f46, %f47}, [%rd77+1280];
	fma.rn.f32 	%f50, %f26, %f46, %f45;
	fma.rn.f32 	%f75, %f27, %f47, %f50;
	add.s64 	%rd78, %rd78, 2560;
	add.s64 	%rd77, %rd77, 5120;
	add.s64 	%rd76, %rd76, 640;
	setp.lt.s64 	%p7, %rd76, %rd33;
	@%p7 bra 	$L__BB12_11;

$L__BB12_12:
	mov.b32 	%r22, %f75;
	mov.u32 	%r23, 31;
	mov.u32 	%r24, 16;
	mov.u32 	%r25, -1;
	shfl.sync.bfly.b32 	%r26|%p8, %r22, %r24, %r23, %r25;
	mov.b32 	%f51, %r26;
	add.f32 	%f52, %f75, %f51;
	mov.b32 	%r27, %f52;
	mov.u32 	%r28, 8;
	shfl.sync.bfly.b32 	%r29|%p9, %r27, %r28, %r23, %r25;
	mov.b32 	%f53, %r29;
	add.f32 	%f54, %f52, %f53;
	mov.b32 	%r30, %f54;
	mov.u32 	%r31, 4;
	shfl.sync.bfly.b32 	%r32|%p10, %r30, %r31, %r23, %r25;
	mov.b32 	%f55, %r32;
	add.f32 	%f56, %f54, %f55;
	mov.b32 	%r33, %f56;
	mov.u32 	%r34, 2;
	shfl.sync.bfly.b32 	%r35|%p11, %r33, %r34, %r23, %r25;
	mov.b32 	%f57, %r35;
	add.f32 	%f58, %f56, %f57;
	mov.b32 	%r36, %f58;
	mov.u32 	%r37, 1;
	shfl.sync.bfly.b32 	%r38|%p12, %r36, %r37, %r23, %r25;
	mov.b32 	%f59, %r38;
	add.f32 	%f60, %f58, %f59;
	shr.s32 	%r39, %r1, 31;
	shr.u32 	%r40, %r39, 27;
	add.s32 	%r41, %r1, %r40;
	shr.s32 	%r42, %r41, 5;
	shl.b32 	%r43, %r42, 2;
	add.s32 	%r45, %r9, %r43;
	st.shared.f32 	[%r45], %f60;
	bar.sync 	0;
	@%p2 bra 	$L__BB12_15;

	ld.shared.f32 	%f61, [%r2];
	mov.b32 	%r46, %f61;
	shfl.sync.bfly.b32 	%r50|%p14, %r46, %r24, %r23, %r25;
	mov.b32 	%f62, %r50;
	add.f32 	%f63, %f61, %f62;
	mov.b32 	%r51, %f63;
	shfl.sync.bfly.b32 	%r53|%p15, %r51, %r28, %r23, %r25;
	mov.b32 	%f64, %r53;
	add.f32 	%f65, %f63, %f64;
	mov.b32 	%r54, %f65;
	shfl.sync.bfly.b32 	%r56|%p16, %r54, %r31, %r23, %r25;
	mov.b32 	%f66, %r56;
	add.f32 	%f67, %f65, %f66;
	mov.b32 	%r57, %f67;
	shfl.sync.bfly.b32 	%r59|%p17, %r57, %r34, %r23, %r25;
	mov.b32 	%f68, %r59;
	add.f32 	%f69, %f67, %f68;
	mov.b32 	%r60, %f69;
	shfl.sync.bfly.b32 	%r62|%p18, %r60, %r37, %r23, %r25;
	mov.b32 	%f70, %r62;
	add.f32 	%f8, %f69, %f70;
	setp.ne.s32 	%p19, %r1, 0;
	@%p19 bra 	$L__BB12_15;

	mul.lo.s64 	%rd66, %rd4, %rd38;
	add.s64 	%rd67, %rd66, %rd3;
	cvta.to.global.u64 	%rd68, %rd32;
	shl.b64 	%rd69, %rd67, 2;
	add.s64 	%rd70, %rd68, %rd69;
	st.global.f32 	[%rd70], %f8;

$L__BB12_15:
	ret;

}
	// .globl	ggml_matvec_f16_acc_f32_bs_196
.visible .entry ggml_matvec_f16_acc_f32_bs_196(
	.param .u64 ggml_matvec_f16_acc_f32_bs_196_param_0,
	.param .u64 ggml_matvec_f16_acc_f32_bs_196_param_1,
	.param .u64 ggml_matvec_f16_acc_f32_bs_196_param_2,
	.param .u64 ggml_matvec_f16_acc_f32_bs_196_param_3,
	.param .u64 ggml_matvec_f16_acc_f32_bs_196_param_4,
	.param .u64 ggml_matvec_f16_acc_f32_bs_196_param_5,
	.param .u64 ggml_matvec_f16_acc_f32_bs_196_param_6,
	.param .u64 ggml_matvec_f16_acc_f32_bs_196_param_7,
	.param .u64 ggml_matvec_f16_acc_f32_bs_196_param_8,
	.param .u64 ggml_matvec_f16_acc_f32_bs_196_param_9
)
{
	.reg .pred 	%p<20>;
	.reg .f32 	%f<76>;
	.reg .b32 	%r<63>;
	.reg .b64 	%rd<81>;


	ld.param.u64 	%rd39, [ggml_matvec_f16_acc_f32_bs_196_param_0];
	ld.param.u64 	%rd40, [ggml_matvec_f16_acc_f32_bs_196_param_1];
	ld.param.u64 	%rd32, [ggml_matvec_f16_acc_f32_bs_196_param_2];
	ld.param.u64 	%rd33, [ggml_matvec_f16_acc_f32_bs_196_param_3];
	ld.param.u64 	%rd34, [ggml_matvec_f16_acc_f32_bs_196_param_5];
	ld.param.u64 	%rd35, [ggml_matvec_f16_acc_f32_bs_196_param_6];
	ld.param.u64 	%rd36, [ggml_matvec_f16_acc_f32_bs_196_param_7];
	ld.param.u64 	%rd37, [ggml_matvec_f16_acc_f32_bs_196_param_8];
	ld.param.u64 	%rd38, [ggml_matvec_f16_acc_f32_bs_196_param_9];
	cvta.to.global.u64 	%rd1, %rd40;
	cvta.to.global.u64 	%rd2, %rd39;
	mov.u32 	%r3, %ctaid.x;
	cvt.u64.u32 	%rd3, %r3;
	mov.u32 	%r4, %ctaid.y;
	cvt.u64.u32 	%rd4, %r4;
	and.b64  	%rd41, %rd35, -4294967296;
	setp.eq.s64 	%p1, %rd41, 0;
	@%p1 bra 	$L__BB13_2;

	div.s64 	%rd72, %rd4, %rd35;
	bra.uni 	$L__BB13_3;

$L__BB13_2:
	cvt.u32.u64 	%r5, %rd35;
	cvt.u32.u64 	%r6, %rd4;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd72, %r7;

$L__BB13_3:
	mov.u32 	%r1, %tid.x;
	setp.gt.s32 	%p2, %r1, 31;
	shl.b32 	%r8, %r1, 2;
	mov.u32 	%r9, data_mmv;
	add.s32 	%r2, %r9, %r8;
	@%p2 bra 	$L__BB13_5;

	mov.u32 	%r10, 0;
	st.shared.u32 	[%r2], %r10;

$L__BB13_5:
	bar.sync 	0;
	cvt.s64.s32 	%rd77, %r1;
	setp.ge.s64 	%p3, %rd77, %rd33;
	mov.f32 	%f75, 0f00000000;
	@%p3 bra 	$L__BB13_12;

	not.b64 	%rd42, %rd77;
	add.s64 	%rd9, %rd42, %rd33;
	shr.u64 	%rd43, %rd9, 2;
	mul.hi.u64 	%rd44, %rd43, 6023426636313322977;
	shr.u64 	%rd45, %rd44, 4;
	add.s64 	%rd46, %rd45, 1;
	and.b64  	%rd10, %rd46, 3;
	setp.eq.s64 	%p4, %rd10, 0;
	mov.f32 	%f75, 0f00000000;
	@%p4 bra 	$L__BB13_9;

	cvt.s64.s32 	%rd77, %r1;
	mul.wide.s32 	%rd47, %r1, 2;
	mul.lo.s64 	%rd48, %rd37, %rd4;
	add.s64 	%rd49, %rd47, %rd48;
	shl.b64 	%rd50, %rd49, 2;
	add.s64 	%rd75, %rd1, %rd50;
	mul.lo.s64 	%rd51, %rd72, %rd36;
	add.s64 	%rd52, %rd47, %rd51;
	mul.lo.s64 	%rd53, %rd34, %rd3;
	add.s64 	%rd54, %rd52, %rd53;
	shl.b64 	%rd55, %rd54, 1;
	add.s64 	%rd74, %rd2, %rd55;
	neg.s64 	%rd73, %rd10;
	mov.f32 	%f75, 0f00000000;

$L__BB13_8:
	.pragma "nounroll";
	ld.global.nc.u32 	%r12, [%rd74];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r12;
  cvt.f32.f16 %f13, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r12;
  cvt.f32.f16 %f14, high;}

	// end inline asm
	ld.global.nc.v2.f32 	{%f15, %f16}, [%rd75];
	fma.rn.f32 	%f19, %f13, %f15, %f75;
	fma.rn.f32 	%f75, %f14, %f16, %f19;
	add.s64 	%rd77, %rd77, 196;
	add.s64 	%rd75, %rd75, 1568;
	add.s64 	%rd74, %rd74, 784;
	add.s64 	%rd73, %rd73, 1;
	setp.ne.s64 	%p5, %rd73, 0;
	@%p5 bra 	$L__BB13_8;

$L__BB13_9:
	setp.lt.u64 	%p6, %rd9, 588;
	@%p6 bra 	$L__BB13_12;

	shl.b64 	%rd56, %rd77, 1;
	mul.lo.s64 	%rd57, %rd72, %rd36;
	mul.lo.s64 	%rd58, %rd34, %rd3;
	add.s64 	%rd59, %rd58, %rd57;
	add.s64 	%rd60, %rd59, %rd56;
	shl.b64 	%rd61, %rd60, 1;
	add.s64 	%rd62, %rd2, %rd61;
	add.s64 	%rd79, %rd62, 1568;
	mul.lo.s64 	%rd63, %rd37, %rd4;
	add.s64 	%rd64, %rd56, %rd63;
	shl.b64 	%rd65, %rd64, 2;
	add.s64 	%rd66, %rd1, %rd65;
	add.s64 	%rd78, %rd66, 3136;

$L__BB13_11:
	ld.global.nc.u32 	%r14, [%rd79+-1568];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r14;
  cvt.f32.f16 %f20, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r14;
  cvt.f32.f16 %f21, high;}

	// end inline asm
	ld.global.nc.v2.f32 	{%f28, %f29}, [%rd78+-3136];
	fma.rn.f32 	%f32, %f20, %f28, %f75;
	fma.rn.f32 	%f33, %f21, %f29, %f32;
	ld.global.nc.u32 	%r16, [%rd79+-784];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r16;
  cvt.f32.f16 %f22, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r16;
  cvt.f32.f16 %f23, high;}

	// end inline asm
	ld.global.nc.v2.f32 	{%f34, %f35}, [%rd78+-1568];
	fma.rn.f32 	%f38, %f22, %f34, %f33;
	fma.rn.f32 	%f39, %f23, %f35, %f38;
	ld.global.nc.u32 	%r18, [%rd79];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r18;
  cvt.f32.f16 %f24, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r18;
  cvt.f32.f16 %f25, high;}

	// end inline asm
	ld.global.nc.v2.f32 	{%f40, %f41}, [%rd78];
	fma.rn.f32 	%f44, %f24, %f40, %f39;
	fma.rn.f32 	%f45, %f25, %f41, %f44;
	ld.global.nc.u32 	%r20, [%rd79+784];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r20;
  cvt.f32.f16 %f26, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r20;
  cvt.f32.f16 %f27, high;}

	// end inline asm
	ld.global.nc.v2.f32 	{%f46, %f47}, [%rd78+1568];
	fma.rn.f32 	%f50, %f26, %f46, %f45;
	fma.rn.f32 	%f75, %f27, %f47, %f50;
	add.s64 	%rd79, %rd79, 3136;
	add.s64 	%rd78, %rd78, 6272;
	add.s64 	%rd77, %rd77, 784;
	setp.lt.s64 	%p7, %rd77, %rd33;
	@%p7 bra 	$L__BB13_11;

$L__BB13_12:
	mov.b32 	%r22, %f75;
	mov.u32 	%r23, 31;
	mov.u32 	%r24, 16;
	mov.u32 	%r25, -1;
	shfl.sync.bfly.b32 	%r26|%p8, %r22, %r24, %r23, %r25;
	mov.b32 	%f51, %r26;
	add.f32 	%f52, %f75, %f51;
	mov.b32 	%r27, %f52;
	mov.u32 	%r28, 8;
	shfl.sync.bfly.b32 	%r29|%p9, %r27, %r28, %r23, %r25;
	mov.b32 	%f53, %r29;
	add.f32 	%f54, %f52, %f53;
	mov.b32 	%r30, %f54;
	mov.u32 	%r31, 4;
	shfl.sync.bfly.b32 	%r32|%p10, %r30, %r31, %r23, %r25;
	mov.b32 	%f55, %r32;
	add.f32 	%f56, %f54, %f55;
	mov.b32 	%r33, %f56;
	mov.u32 	%r34, 2;
	shfl.sync.bfly.b32 	%r35|%p11, %r33, %r34, %r23, %r25;
	mov.b32 	%f57, %r35;
	add.f32 	%f58, %f56, %f57;
	mov.b32 	%r36, %f58;
	mov.u32 	%r37, 1;
	shfl.sync.bfly.b32 	%r38|%p12, %r36, %r37, %r23, %r25;
	mov.b32 	%f59, %r38;
	add.f32 	%f60, %f58, %f59;
	shr.s32 	%r39, %r1, 31;
	shr.u32 	%r40, %r39, 27;
	add.s32 	%r41, %r1, %r40;
	shr.s32 	%r42, %r41, 5;
	shl.b32 	%r43, %r42, 2;
	add.s32 	%r45, %r9, %r43;
	st.shared.f32 	[%r45], %f60;
	bar.sync 	0;
	@%p2 bra 	$L__BB13_15;

	ld.shared.f32 	%f61, [%r2];
	mov.b32 	%r46, %f61;
	shfl.sync.bfly.b32 	%r50|%p14, %r46, %r24, %r23, %r25;
	mov.b32 	%f62, %r50;
	add.f32 	%f63, %f61, %f62;
	mov.b32 	%r51, %f63;
	shfl.sync.bfly.b32 	%r53|%p15, %r51, %r28, %r23, %r25;
	mov.b32 	%f64, %r53;
	add.f32 	%f65, %f63, %f64;
	mov.b32 	%r54, %f65;
	shfl.sync.bfly.b32 	%r56|%p16, %r54, %r31, %r23, %r25;
	mov.b32 	%f66, %r56;
	add.f32 	%f67, %f65, %f66;
	mov.b32 	%r57, %f67;
	shfl.sync.bfly.b32 	%r59|%p17, %r57, %r34, %r23, %r25;
	mov.b32 	%f68, %r59;
	add.f32 	%f69, %f67, %f68;
	mov.b32 	%r60, %f69;
	shfl.sync.bfly.b32 	%r62|%p18, %r60, %r37, %r23, %r25;
	mov.b32 	%f70, %r62;
	add.f32 	%f8, %f69, %f70;
	setp.ne.s32 	%p19, %r1, 0;
	@%p19 bra 	$L__BB13_15;

	mul.lo.s64 	%rd67, %rd4, %rd38;
	add.s64 	%rd68, %rd67, %rd3;
	cvta.to.global.u64 	%rd69, %rd32;
	shl.b64 	%rd70, %rd68, 2;
	add.s64 	%rd71, %rd69, %rd70;
	st.global.f32 	[%rd71], %f8;

$L__BB13_15:
	ret;

}
	// .globl	ggml_matvec_f16_acc_f32_bs_224
.visible .entry ggml_matvec_f16_acc_f32_bs_224(
	.param .u64 ggml_matvec_f16_acc_f32_bs_224_param_0,
	.param .u64 ggml_matvec_f16_acc_f32_bs_224_param_1,
	.param .u64 ggml_matvec_f16_acc_f32_bs_224_param_2,
	.param .u64 ggml_matvec_f16_acc_f32_bs_224_param_3,
	.param .u64 ggml_matvec_f16_acc_f32_bs_224_param_4,
	.param .u64 ggml_matvec_f16_acc_f32_bs_224_param_5,
	.param .u64 ggml_matvec_f16_acc_f32_bs_224_param_6,
	.param .u64 ggml_matvec_f16_acc_f32_bs_224_param_7,
	.param .u64 ggml_matvec_f16_acc_f32_bs_224_param_8,
	.param .u64 ggml_matvec_f16_acc_f32_bs_224_param_9
)
{
	.reg .pred 	%p<20>;
	.reg .f32 	%f<76>;
	.reg .b32 	%r<63>;
	.reg .b64 	%rd<80>;


	ld.param.u64 	%rd39, [ggml_matvec_f16_acc_f32_bs_224_param_0];
	ld.param.u64 	%rd40, [ggml_matvec_f16_acc_f32_bs_224_param_1];
	ld.param.u64 	%rd32, [ggml_matvec_f16_acc_f32_bs_224_param_2];
	ld.param.u64 	%rd33, [ggml_matvec_f16_acc_f32_bs_224_param_3];
	ld.param.u64 	%rd34, [ggml_matvec_f16_acc_f32_bs_224_param_5];
	ld.param.u64 	%rd35, [ggml_matvec_f16_acc_f32_bs_224_param_6];
	ld.param.u64 	%rd36, [ggml_matvec_f16_acc_f32_bs_224_param_7];
	ld.param.u64 	%rd37, [ggml_matvec_f16_acc_f32_bs_224_param_8];
	ld.param.u64 	%rd38, [ggml_matvec_f16_acc_f32_bs_224_param_9];
	cvta.to.global.u64 	%rd1, %rd40;
	cvta.to.global.u64 	%rd2, %rd39;
	mov.u32 	%r3, %ctaid.x;
	cvt.u64.u32 	%rd3, %r3;
	mov.u32 	%r4, %ctaid.y;
	cvt.u64.u32 	%rd4, %r4;
	and.b64  	%rd41, %rd35, -4294967296;
	setp.eq.s64 	%p1, %rd41, 0;
	@%p1 bra 	$L__BB14_2;

	div.s64 	%rd71, %rd4, %rd35;
	bra.uni 	$L__BB14_3;

$L__BB14_2:
	cvt.u32.u64 	%r5, %rd35;
	cvt.u32.u64 	%r6, %rd4;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd71, %r7;

$L__BB14_3:
	mov.u32 	%r1, %tid.x;
	setp.gt.s32 	%p2, %r1, 31;
	shl.b32 	%r8, %r1, 2;
	mov.u32 	%r9, data_mmv;
	add.s32 	%r2, %r9, %r8;
	@%p2 bra 	$L__BB14_5;

	mov.u32 	%r10, 0;
	st.shared.u32 	[%r2], %r10;

$L__BB14_5:
	bar.sync 	0;
	cvt.s64.s32 	%rd76, %r1;
	setp.ge.s64 	%p3, %rd76, %rd33;
	mov.f32 	%f75, 0f00000000;
	@%p3 bra 	$L__BB14_12;

	not.b64 	%rd42, %rd76;
	add.s64 	%rd9, %rd42, %rd33;
	shr.u64 	%rd43, %rd9, 5;
	mul.hi.u64 	%rd44, %rd43, 2635249153387078803;
	add.s64 	%rd45, %rd44, 1;
	and.b64  	%rd10, %rd45, 3;
	setp.eq.s64 	%p4, %rd10, 0;
	mov.f32 	%f75, 0f00000000;
	@%p4 bra 	$L__BB14_9;

	cvt.s64.s32 	%rd76, %r1;
	mul.wide.s32 	%rd46, %r1, 2;
	mul.lo.s64 	%rd47, %rd37, %rd4;
	add.s64 	%rd48, %rd46, %rd47;
	shl.b64 	%rd49, %rd48, 2;
	add.s64 	%rd74, %rd1, %rd49;
	mul.lo.s64 	%rd50, %rd71, %rd36;
	add.s64 	%rd51, %rd46, %rd50;
	mul.lo.s64 	%rd52, %rd34, %rd3;
	add.s64 	%rd53, %rd51, %rd52;
	shl.b64 	%rd54, %rd53, 1;
	add.s64 	%rd73, %rd2, %rd54;
	neg.s64 	%rd72, %rd10;
	mov.f32 	%f75, 0f00000000;

$L__BB14_8:
	.pragma "nounroll";
	ld.global.nc.u32 	%r12, [%rd73];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r12;
  cvt.f32.f16 %f13, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r12;
  cvt.f32.f16 %f14, high;}

	// end inline asm
	ld.global.nc.v2.f32 	{%f15, %f16}, [%rd74];
	fma.rn.f32 	%f19, %f13, %f15, %f75;
	fma.rn.f32 	%f75, %f14, %f16, %f19;
	add.s64 	%rd76, %rd76, 224;
	add.s64 	%rd74, %rd74, 1792;
	add.s64 	%rd73, %rd73, 896;
	add.s64 	%rd72, %rd72, 1;
	setp.ne.s64 	%p5, %rd72, 0;
	@%p5 bra 	$L__BB14_8;

$L__BB14_9:
	setp.lt.u64 	%p6, %rd9, 672;
	@%p6 bra 	$L__BB14_12;

	shl.b64 	%rd55, %rd76, 1;
	mul.lo.s64 	%rd56, %rd71, %rd36;
	mul.lo.s64 	%rd57, %rd34, %rd3;
	add.s64 	%rd58, %rd57, %rd56;
	add.s64 	%rd59, %rd58, %rd55;
	shl.b64 	%rd60, %rd59, 1;
	add.s64 	%rd61, %rd2, %rd60;
	add.s64 	%rd78, %rd61, 1792;
	mul.lo.s64 	%rd62, %rd37, %rd4;
	add.s64 	%rd63, %rd55, %rd62;
	shl.b64 	%rd64, %rd63, 2;
	add.s64 	%rd65, %rd1, %rd64;
	add.s64 	%rd77, %rd65, 3584;

$L__BB14_11:
	ld.global.nc.u32 	%r14, [%rd78+-1792];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r14;
  cvt.f32.f16 %f20, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r14;
  cvt.f32.f16 %f21, high;}

	// end inline asm
	ld.global.nc.v2.f32 	{%f28, %f29}, [%rd77+-3584];
	fma.rn.f32 	%f32, %f20, %f28, %f75;
	fma.rn.f32 	%f33, %f21, %f29, %f32;
	ld.global.nc.u32 	%r16, [%rd78+-896];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r16;
  cvt.f32.f16 %f22, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r16;
  cvt.f32.f16 %f23, high;}

	// end inline asm
	ld.global.nc.v2.f32 	{%f34, %f35}, [%rd77+-1792];
	fma.rn.f32 	%f38, %f22, %f34, %f33;
	fma.rn.f32 	%f39, %f23, %f35, %f38;
	ld.global.nc.u32 	%r18, [%rd78];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r18;
  cvt.f32.f16 %f24, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r18;
  cvt.f32.f16 %f25, high;}

	// end inline asm
	ld.global.nc.v2.f32 	{%f40, %f41}, [%rd77];
	fma.rn.f32 	%f44, %f24, %f40, %f39;
	fma.rn.f32 	%f45, %f25, %f41, %f44;
	ld.global.nc.u32 	%r20, [%rd78+896];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r20;
  cvt.f32.f16 %f26, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r20;
  cvt.f32.f16 %f27, high;}

	// end inline asm
	ld.global.nc.v2.f32 	{%f46, %f47}, [%rd77+1792];
	fma.rn.f32 	%f50, %f26, %f46, %f45;
	fma.rn.f32 	%f75, %f27, %f47, %f50;
	add.s64 	%rd78, %rd78, 3584;
	add.s64 	%rd77, %rd77, 7168;
	add.s64 	%rd76, %rd76, 896;
	setp.lt.s64 	%p7, %rd76, %rd33;
	@%p7 bra 	$L__BB14_11;

$L__BB14_12:
	mov.b32 	%r22, %f75;
	mov.u32 	%r23, 31;
	mov.u32 	%r24, 16;
	mov.u32 	%r25, -1;
	shfl.sync.bfly.b32 	%r26|%p8, %r22, %r24, %r23, %r25;
	mov.b32 	%f51, %r26;
	add.f32 	%f52, %f75, %f51;
	mov.b32 	%r27, %f52;
	mov.u32 	%r28, 8;
	shfl.sync.bfly.b32 	%r29|%p9, %r27, %r28, %r23, %r25;
	mov.b32 	%f53, %r29;
	add.f32 	%f54, %f52, %f53;
	mov.b32 	%r30, %f54;
	mov.u32 	%r31, 4;
	shfl.sync.bfly.b32 	%r32|%p10, %r30, %r31, %r23, %r25;
	mov.b32 	%f55, %r32;
	add.f32 	%f56, %f54, %f55;
	mov.b32 	%r33, %f56;
	mov.u32 	%r34, 2;
	shfl.sync.bfly.b32 	%r35|%p11, %r33, %r34, %r23, %r25;
	mov.b32 	%f57, %r35;
	add.f32 	%f58, %f56, %f57;
	mov.b32 	%r36, %f58;
	mov.u32 	%r37, 1;
	shfl.sync.bfly.b32 	%r38|%p12, %r36, %r37, %r23, %r25;
	mov.b32 	%f59, %r38;
	add.f32 	%f60, %f58, %f59;
	shr.s32 	%r39, %r1, 31;
	shr.u32 	%r40, %r39, 27;
	add.s32 	%r41, %r1, %r40;
	shr.s32 	%r42, %r41, 5;
	shl.b32 	%r43, %r42, 2;
	add.s32 	%r45, %r9, %r43;
	st.shared.f32 	[%r45], %f60;
	bar.sync 	0;
	@%p2 bra 	$L__BB14_15;

	ld.shared.f32 	%f61, [%r2];
	mov.b32 	%r46, %f61;
	shfl.sync.bfly.b32 	%r50|%p14, %r46, %r24, %r23, %r25;
	mov.b32 	%f62, %r50;
	add.f32 	%f63, %f61, %f62;
	mov.b32 	%r51, %f63;
	shfl.sync.bfly.b32 	%r53|%p15, %r51, %r28, %r23, %r25;
	mov.b32 	%f64, %r53;
	add.f32 	%f65, %f63, %f64;
	mov.b32 	%r54, %f65;
	shfl.sync.bfly.b32 	%r56|%p16, %r54, %r31, %r23, %r25;
	mov.b32 	%f66, %r56;
	add.f32 	%f67, %f65, %f66;
	mov.b32 	%r57, %f67;
	shfl.sync.bfly.b32 	%r59|%p17, %r57, %r34, %r23, %r25;
	mov.b32 	%f68, %r59;
	add.f32 	%f69, %f67, %f68;
	mov.b32 	%r60, %f69;
	shfl.sync.bfly.b32 	%r62|%p18, %r60, %r37, %r23, %r25;
	mov.b32 	%f70, %r62;
	add.f32 	%f8, %f69, %f70;
	setp.ne.s32 	%p19, %r1, 0;
	@%p19 bra 	$L__BB14_15;

	mul.lo.s64 	%rd66, %rd4, %rd38;
	add.s64 	%rd67, %rd66, %rd3;
	cvta.to.global.u64 	%rd68, %rd32;
	shl.b64 	%rd69, %rd67, 2;
	add.s64 	%rd70, %rd68, %rd69;
	st.global.f32 	[%rd70], %f8;

$L__BB14_15:
	ret;

}
	// .globl	ggml_matvec_f16_acc_f32_bs_256
.visible .entry ggml_matvec_f16_acc_f32_bs_256(
	.param .u64 ggml_matvec_f16_acc_f32_bs_256_param_0,
	.param .u64 ggml_matvec_f16_acc_f32_bs_256_param_1,
	.param .u64 ggml_matvec_f16_acc_f32_bs_256_param_2,
	.param .u64 ggml_matvec_f16_acc_f32_bs_256_param_3,
	.param .u64 ggml_matvec_f16_acc_f32_bs_256_param_4,
	.param .u64 ggml_matvec_f16_acc_f32_bs_256_param_5,
	.param .u64 ggml_matvec_f16_acc_f32_bs_256_param_6,
	.param .u64 ggml_matvec_f16_acc_f32_bs_256_param_7,
	.param .u64 ggml_matvec_f16_acc_f32_bs_256_param_8,
	.param .u64 ggml_matvec_f16_acc_f32_bs_256_param_9
)
{
	.reg .pred 	%p<20>;
	.reg .f32 	%f<76>;
	.reg .b32 	%r<63>;
	.reg .b64 	%rd<79>;


	ld.param.u64 	%rd39, [ggml_matvec_f16_acc_f32_bs_256_param_0];
	ld.param.u64 	%rd40, [ggml_matvec_f16_acc_f32_bs_256_param_1];
	ld.param.u64 	%rd32, [ggml_matvec_f16_acc_f32_bs_256_param_2];
	ld.param.u64 	%rd33, [ggml_matvec_f16_acc_f32_bs_256_param_3];
	ld.param.u64 	%rd34, [ggml_matvec_f16_acc_f32_bs_256_param_5];
	ld.param.u64 	%rd35, [ggml_matvec_f16_acc_f32_bs_256_param_6];
	ld.param.u64 	%rd36, [ggml_matvec_f16_acc_f32_bs_256_param_7];
	ld.param.u64 	%rd37, [ggml_matvec_f16_acc_f32_bs_256_param_8];
	ld.param.u64 	%rd38, [ggml_matvec_f16_acc_f32_bs_256_param_9];
	cvta.to.global.u64 	%rd1, %rd40;
	cvta.to.global.u64 	%rd2, %rd39;
	mov.u32 	%r3, %ctaid.x;
	cvt.u64.u32 	%rd3, %r3;
	mov.u32 	%r4, %ctaid.y;
	cvt.u64.u32 	%rd4, %r4;
	and.b64  	%rd41, %rd35, -4294967296;
	setp.eq.s64 	%p1, %rd41, 0;
	@%p1 bra 	$L__BB15_2;

	div.s64 	%rd70, %rd4, %rd35;
	bra.uni 	$L__BB15_3;

$L__BB15_2:
	cvt.u32.u64 	%r5, %rd35;
	cvt.u32.u64 	%r6, %rd4;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd70, %r7;

$L__BB15_3:
	mov.u32 	%r1, %tid.x;
	setp.gt.s32 	%p2, %r1, 31;
	shl.b32 	%r8, %r1, 2;
	mov.u32 	%r9, data_mmv;
	add.s32 	%r2, %r9, %r8;
	@%p2 bra 	$L__BB15_5;

	mov.u32 	%r10, 0;
	st.shared.u32 	[%r2], %r10;

$L__BB15_5:
	bar.sync 	0;
	cvt.s64.s32 	%rd75, %r1;
	setp.ge.s64 	%p3, %rd75, %rd33;
	mov.f32 	%f75, 0f00000000;
	@%p3 bra 	$L__BB15_12;

	not.b64 	%rd42, %rd75;
	add.s64 	%rd9, %rd42, %rd33;
	shr.u64 	%rd43, %rd9, 8;
	add.s64 	%rd44, %rd43, 1;
	and.b64  	%rd10, %rd44, 3;
	setp.eq.s64 	%p4, %rd10, 0;
	mov.f32 	%f75, 0f00000000;
	@%p4 bra 	$L__BB15_9;

	cvt.s64.s32 	%rd75, %r1;
	mul.wide.s32 	%rd45, %r1, 2;
	mul.lo.s64 	%rd46, %rd37, %rd4;
	add.s64 	%rd47, %rd45, %rd46;
	shl.b64 	%rd48, %rd47, 2;
	add.s64 	%rd73, %rd1, %rd48;
	mul.lo.s64 	%rd49, %rd70, %rd36;
	add.s64 	%rd50, %rd45, %rd49;
	mul.lo.s64 	%rd51, %rd34, %rd3;
	add.s64 	%rd52, %rd50, %rd51;
	shl.b64 	%rd53, %rd52, 1;
	add.s64 	%rd72, %rd2, %rd53;
	neg.s64 	%rd71, %rd10;
	mov.f32 	%f75, 0f00000000;

$L__BB15_8:
	.pragma "nounroll";
	ld.global.nc.u32 	%r12, [%rd72];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r12;
  cvt.f32.f16 %f13, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r12;
  cvt.f32.f16 %f14, high;}

	// end inline asm
	ld.global.nc.v2.f32 	{%f15, %f16}, [%rd73];
	fma.rn.f32 	%f19, %f13, %f15, %f75;
	fma.rn.f32 	%f75, %f14, %f16, %f19;
	add.s64 	%rd75, %rd75, 256;
	add.s64 	%rd73, %rd73, 2048;
	add.s64 	%rd72, %rd72, 1024;
	add.s64 	%rd71, %rd71, 1;
	setp.ne.s64 	%p5, %rd71, 0;
	@%p5 bra 	$L__BB15_8;

$L__BB15_9:
	setp.lt.u64 	%p6, %rd9, 768;
	@%p6 bra 	$L__BB15_12;

	shl.b64 	%rd54, %rd75, 1;
	mul.lo.s64 	%rd55, %rd70, %rd36;
	mul.lo.s64 	%rd56, %rd34, %rd3;
	add.s64 	%rd57, %rd56, %rd55;
	add.s64 	%rd58, %rd57, %rd54;
	shl.b64 	%rd59, %rd58, 1;
	add.s64 	%rd60, %rd2, %rd59;
	add.s64 	%rd77, %rd60, 2048;
	mul.lo.s64 	%rd61, %rd37, %rd4;
	add.s64 	%rd62, %rd54, %rd61;
	shl.b64 	%rd63, %rd62, 2;
	add.s64 	%rd64, %rd1, %rd63;
	add.s64 	%rd76, %rd64, 4096;

$L__BB15_11:
	ld.global.nc.u32 	%r14, [%rd77+-2048];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r14;
  cvt.f32.f16 %f20, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r14;
  cvt.f32.f16 %f21, high;}

	// end inline asm
	ld.global.nc.v2.f32 	{%f28, %f29}, [%rd76+-4096];
	fma.rn.f32 	%f32, %f20, %f28, %f75;
	fma.rn.f32 	%f33, %f21, %f29, %f32;
	ld.global.nc.u32 	%r16, [%rd77+-1024];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r16;
  cvt.f32.f16 %f22, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r16;
  cvt.f32.f16 %f23, high;}

	// end inline asm
	ld.global.nc.v2.f32 	{%f34, %f35}, [%rd76+-2048];
	fma.rn.f32 	%f38, %f22, %f34, %f33;
	fma.rn.f32 	%f39, %f23, %f35, %f38;
	ld.global.nc.u32 	%r18, [%rd77];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r18;
  cvt.f32.f16 %f24, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r18;
  cvt.f32.f16 %f25, high;}

	// end inline asm
	ld.global.nc.v2.f32 	{%f40, %f41}, [%rd76];
	fma.rn.f32 	%f44, %f24, %f40, %f39;
	fma.rn.f32 	%f45, %f25, %f41, %f44;
	ld.global.nc.u32 	%r20, [%rd77+1024];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r20;
  cvt.f32.f16 %f26, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r20;
  cvt.f32.f16 %f27, high;}

	// end inline asm
	ld.global.nc.v2.f32 	{%f46, %f47}, [%rd76+2048];
	fma.rn.f32 	%f50, %f26, %f46, %f45;
	fma.rn.f32 	%f75, %f27, %f47, %f50;
	add.s64 	%rd77, %rd77, 4096;
	add.s64 	%rd76, %rd76, 8192;
	add.s64 	%rd75, %rd75, 1024;
	setp.lt.s64 	%p7, %rd75, %rd33;
	@%p7 bra 	$L__BB15_11;

$L__BB15_12:
	mov.b32 	%r22, %f75;
	mov.u32 	%r23, 31;
	mov.u32 	%r24, 16;
	mov.u32 	%r25, -1;
	shfl.sync.bfly.b32 	%r26|%p8, %r22, %r24, %r23, %r25;
	mov.b32 	%f51, %r26;
	add.f32 	%f52, %f75, %f51;
	mov.b32 	%r27, %f52;
	mov.u32 	%r28, 8;
	shfl.sync.bfly.b32 	%r29|%p9, %r27, %r28, %r23, %r25;
	mov.b32 	%f53, %r29;
	add.f32 	%f54, %f52, %f53;
	mov.b32 	%r30, %f54;
	mov.u32 	%r31, 4;
	shfl.sync.bfly.b32 	%r32|%p10, %r30, %r31, %r23, %r25;
	mov.b32 	%f55, %r32;
	add.f32 	%f56, %f54, %f55;
	mov.b32 	%r33, %f56;
	mov.u32 	%r34, 2;
	shfl.sync.bfly.b32 	%r35|%p11, %r33, %r34, %r23, %r25;
	mov.b32 	%f57, %r35;
	add.f32 	%f58, %f56, %f57;
	mov.b32 	%r36, %f58;
	mov.u32 	%r37, 1;
	shfl.sync.bfly.b32 	%r38|%p12, %r36, %r37, %r23, %r25;
	mov.b32 	%f59, %r38;
	add.f32 	%f60, %f58, %f59;
	shr.s32 	%r39, %r1, 31;
	shr.u32 	%r40, %r39, 27;
	add.s32 	%r41, %r1, %r40;
	shr.s32 	%r42, %r41, 5;
	shl.b32 	%r43, %r42, 2;
	add.s32 	%r45, %r9, %r43;
	st.shared.f32 	[%r45], %f60;
	bar.sync 	0;
	@%p2 bra 	$L__BB15_15;

	ld.shared.f32 	%f61, [%r2];
	mov.b32 	%r46, %f61;
	shfl.sync.bfly.b32 	%r50|%p14, %r46, %r24, %r23, %r25;
	mov.b32 	%f62, %r50;
	add.f32 	%f63, %f61, %f62;
	mov.b32 	%r51, %f63;
	shfl.sync.bfly.b32 	%r53|%p15, %r51, %r28, %r23, %r25;
	mov.b32 	%f64, %r53;
	add.f32 	%f65, %f63, %f64;
	mov.b32 	%r54, %f65;
	shfl.sync.bfly.b32 	%r56|%p16, %r54, %r31, %r23, %r25;
	mov.b32 	%f66, %r56;
	add.f32 	%f67, %f65, %f66;
	mov.b32 	%r57, %f67;
	shfl.sync.bfly.b32 	%r59|%p17, %r57, %r34, %r23, %r25;
	mov.b32 	%f68, %r59;
	add.f32 	%f69, %f67, %f68;
	mov.b32 	%r60, %f69;
	shfl.sync.bfly.b32 	%r62|%p18, %r60, %r37, %r23, %r25;
	mov.b32 	%f70, %r62;
	add.f32 	%f8, %f69, %f70;
	setp.ne.s32 	%p19, %r1, 0;
	@%p19 bra 	$L__BB15_15;

	mul.lo.s64 	%rd65, %rd4, %rd38;
	add.s64 	%rd66, %rd65, %rd3;
	cvta.to.global.u64 	%rd67, %rd32;
	shl.b64 	%rd68, %rd66, 2;
	add.s64 	%rd69, %rd67, %rd68;
	st.global.f32 	[%rd69], %f8;

$L__BB15_15:
	ret;

}

