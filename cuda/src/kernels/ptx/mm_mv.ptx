//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-35583870
// Cuda compilation tools, release 12.8, V12.8.93
// Based on NVVM 7.0.1
//

.version 8.7
.target sm_87
.address_size 64

	// .globl	ggml_matvec_f32_ncols_1_bs_32
.extern .shared .align 16 .b8 data_mmv[];

.visible .entry ggml_matvec_f32_ncols_1_bs_32(
	.param .u64 ggml_matvec_f32_ncols_1_bs_32_param_0,
	.param .u64 ggml_matvec_f32_ncols_1_bs_32_param_1,
	.param .u64 ggml_matvec_f32_ncols_1_bs_32_param_2,
	.param .u32 ggml_matvec_f32_ncols_1_bs_32_param_3,
	.param .u32 ggml_matvec_f32_ncols_1_bs_32_param_4,
	.param .u32 ggml_matvec_f32_ncols_1_bs_32_param_5,
	.param .u32 ggml_matvec_f32_ncols_1_bs_32_param_6,
	.param .u32 ggml_matvec_f32_ncols_1_bs_32_param_7,
	.param .u32 ggml_matvec_f32_ncols_1_bs_32_param_8,
	.param .u32 ggml_matvec_f32_ncols_1_bs_32_param_9,
	.param .u32 ggml_matvec_f32_ncols_1_bs_32_param_10,
	.param .u32 ggml_matvec_f32_ncols_1_bs_32_param_11
)
{
	.reg .pred 	%p<12>;
	.reg .f32 	%f<75>;
	.reg .b32 	%r<51>;
	.reg .b64 	%rd<42>;


	ld.param.u64 	%rd18, [ggml_matvec_f32_ncols_1_bs_32_param_0];
	ld.param.u64 	%rd19, [ggml_matvec_f32_ncols_1_bs_32_param_1];
	ld.param.u64 	%rd17, [ggml_matvec_f32_ncols_1_bs_32_param_2];
	ld.param.u32 	%r12, [ggml_matvec_f32_ncols_1_bs_32_param_3];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_1_bs_32_param_5];
	ld.param.u32 	%r13, [ggml_matvec_f32_ncols_1_bs_32_param_7];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_1_bs_32_param_8];
	ld.param.u32 	%r17, [ggml_matvec_f32_ncols_1_bs_32_param_9];
	ld.param.u32 	%r18, [ggml_matvec_f32_ncols_1_bs_32_param_10];
	ld.param.u32 	%r14, [ggml_matvec_f32_ncols_1_bs_32_param_11];
	cvta.to.global.u64 	%rd1, %rd19;
	cvta.to.global.u64 	%rd2, %rd18;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r19, %r1, %r16;
	mov.u32 	%r20, %ctaid.x;
	mul.lo.s32 	%r21, %r20, %r15;
	mad.lo.s32 	%r22, %r19, %r17, %r21;
	cvt.s64.s32 	%rd3, %r22;
	mul.lo.s32 	%r23, %r1, %r18;
	cvt.s64.s32 	%rd4, %r23;
	mov.u32 	%r2, %tid.x;
	setp.ge.s32 	%p1, %r2, %r12;
	mov.f32 	%f74, 0f00000000;
	@%p1 bra 	$L__BB0_7;

	not.b32 	%r24, %r2;
	add.s32 	%r3, %r24, %r12;
	shr.u32 	%r25, %r3, 5;
	add.s32 	%r26, %r25, 1;
	and.b32  	%r48, %r26, 3;
	setp.eq.s32 	%p2, %r48, 0;
	mov.f32 	%f74, 0f00000000;
	mov.u32 	%r49, %r2;
	@%p2 bra 	$L__BB0_4;

	mul.wide.s32 	%rd20, %r2, 2;
	add.s64 	%rd21, %rd20, %rd4;
	shl.b64 	%rd22, %rd21, 2;
	add.s64 	%rd39, %rd1, %rd22;
	add.s64 	%rd23, %rd20, %rd3;
	shl.b64 	%rd24, %rd23, 2;
	add.s64 	%rd38, %rd2, %rd24;
	mov.f32 	%f74, 0f00000000;
	mov.u32 	%r49, %r2;

$L__BB0_3:
	.pragma "nounroll";
	ld.global.nc.v2.f32 	{%f13, %f14}, [%rd38];
	ld.global.nc.v2.f32 	{%f17, %f18}, [%rd39];
	fma.rn.f32 	%f21, %f13, %f17, %f74;
	fma.rn.f32 	%f74, %f14, %f18, %f21;
	add.s32 	%r49, %r49, 32;
	add.s64 	%rd39, %rd39, 256;
	add.s64 	%rd38, %rd38, 256;
	add.s32 	%r48, %r48, -1;
	setp.ne.s32 	%p3, %r48, 0;
	@%p3 bra 	$L__BB0_3;

$L__BB0_4:
	setp.lt.u32 	%p4, %r3, 96;
	@%p4 bra 	$L__BB0_7;

	mul.wide.s32 	%rd25, %r49, 2;
	add.s64 	%rd26, %rd25, %rd3;
	shl.b64 	%rd27, %rd26, 2;
	add.s64 	%rd28, %rd2, %rd27;
	add.s64 	%rd41, %rd28, 512;
	add.s64 	%rd29, %rd25, %rd4;
	shl.b64 	%rd30, %rd29, 2;
	add.s64 	%rd31, %rd1, %rd30;
	add.s64 	%rd40, %rd31, 512;

$L__BB0_6:
	ld.global.nc.v2.f32 	{%f22, %f23}, [%rd41+-512];
	ld.global.nc.v2.f32 	{%f26, %f27}, [%rd40+-512];
	fma.rn.f32 	%f30, %f22, %f26, %f74;
	fma.rn.f32 	%f31, %f23, %f27, %f30;
	ld.global.nc.v2.f32 	{%f32, %f33}, [%rd41+-256];
	ld.global.nc.v2.f32 	{%f36, %f37}, [%rd40+-256];
	fma.rn.f32 	%f40, %f32, %f36, %f31;
	fma.rn.f32 	%f41, %f33, %f37, %f40;
	ld.global.nc.v2.f32 	{%f42, %f43}, [%rd41];
	ld.global.nc.v2.f32 	{%f46, %f47}, [%rd40];
	fma.rn.f32 	%f50, %f42, %f46, %f41;
	fma.rn.f32 	%f51, %f43, %f47, %f50;
	ld.global.nc.v2.f32 	{%f52, %f53}, [%rd41+256];
	ld.global.nc.v2.f32 	{%f56, %f57}, [%rd40+256];
	fma.rn.f32 	%f60, %f52, %f56, %f51;
	fma.rn.f32 	%f74, %f53, %f57, %f60;
	add.s64 	%rd41, %rd41, 1024;
	add.s64 	%rd40, %rd40, 1024;
	add.s32 	%r49, %r49, 128;
	setp.lt.s32 	%p5, %r49, %r12;
	@%p5 bra 	$L__BB0_6;

$L__BB0_7:
	mov.b32 	%r27, %f74;
	mov.u32 	%r28, 31;
	mov.u32 	%r29, 16;
	mov.u32 	%r30, -1;
	shfl.sync.bfly.b32 	%r31|%p6, %r27, %r29, %r28, %r30;
	mov.b32 	%f61, %r31;
	add.f32 	%f62, %f74, %f61;
	mov.b32 	%r32, %f62;
	mov.u32 	%r33, 8;
	shfl.sync.bfly.b32 	%r34|%p7, %r32, %r33, %r28, %r30;
	mov.b32 	%f63, %r34;
	add.f32 	%f64, %f62, %f63;
	mov.b32 	%r35, %f64;
	mov.u32 	%r36, 4;
	shfl.sync.bfly.b32 	%r37|%p8, %r35, %r36, %r28, %r30;
	mov.b32 	%f65, %r37;
	add.f32 	%f66, %f64, %f65;
	mov.b32 	%r38, %f66;
	mov.u32 	%r39, 2;
	shfl.sync.bfly.b32 	%r40|%p9, %r38, %r39, %r28, %r30;
	mov.b32 	%f67, %r40;
	add.f32 	%f68, %f66, %f67;
	mov.b32 	%r41, %f68;
	mov.u32 	%r42, 1;
	shfl.sync.bfly.b32 	%r43|%p10, %r41, %r42, %r28, %r30;
	mov.b32 	%f69, %r43;
	add.f32 	%f8, %f68, %f69;
	setp.gt.s32 	%p11, %r2, 0;
	@%p11 bra 	$L__BB0_9;

	mad.lo.s32 	%r45, %r2, %r13, %r20;
	cvt.s64.s32 	%rd32, %r45;
	mul.lo.s32 	%r46, %r1, %r14;
	cvt.s64.s32 	%rd33, %r46;
	add.s64 	%rd34, %rd33, %rd32;
	cvta.to.global.u64 	%rd35, %rd17;
	shl.b64 	%rd36, %rd34, 2;
	add.s64 	%rd37, %rd35, %rd36;
	st.global.f32 	[%rd37], %f8;

$L__BB0_9:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_2_bs_32
.visible .entry ggml_matvec_f32_ncols_2_bs_32(
	.param .u64 ggml_matvec_f32_ncols_2_bs_32_param_0,
	.param .u64 ggml_matvec_f32_ncols_2_bs_32_param_1,
	.param .u64 ggml_matvec_f32_ncols_2_bs_32_param_2,
	.param .u32 ggml_matvec_f32_ncols_2_bs_32_param_3,
	.param .u32 ggml_matvec_f32_ncols_2_bs_32_param_4,
	.param .u32 ggml_matvec_f32_ncols_2_bs_32_param_5,
	.param .u32 ggml_matvec_f32_ncols_2_bs_32_param_6,
	.param .u32 ggml_matvec_f32_ncols_2_bs_32_param_7,
	.param .u32 ggml_matvec_f32_ncols_2_bs_32_param_8,
	.param .u32 ggml_matvec_f32_ncols_2_bs_32_param_9,
	.param .u32 ggml_matvec_f32_ncols_2_bs_32_param_10,
	.param .u32 ggml_matvec_f32_ncols_2_bs_32_param_11
)
{
	.local .align 8 .b8 	__local_depot1[8];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<17>;
	.reg .f32 	%f<124>;
	.reg .b32 	%r<61>;
	.reg .b64 	%rd<63>;


	mov.u64 	%SPL, __local_depot1;
	ld.param.u64 	%rd26, [ggml_matvec_f32_ncols_2_bs_32_param_0];
	ld.param.u64 	%rd27, [ggml_matvec_f32_ncols_2_bs_32_param_1];
	ld.param.u64 	%rd25, [ggml_matvec_f32_ncols_2_bs_32_param_2];
	ld.param.u32 	%r13, [ggml_matvec_f32_ncols_2_bs_32_param_3];
	ld.param.u32 	%r17, [ggml_matvec_f32_ncols_2_bs_32_param_5];
	ld.param.u32 	%r14, [ggml_matvec_f32_ncols_2_bs_32_param_6];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_2_bs_32_param_7];
	ld.param.u32 	%r18, [ggml_matvec_f32_ncols_2_bs_32_param_8];
	ld.param.u32 	%r19, [ggml_matvec_f32_ncols_2_bs_32_param_9];
	ld.param.u32 	%r20, [ggml_matvec_f32_ncols_2_bs_32_param_10];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_2_bs_32_param_11];
	cvta.to.global.u64 	%rd1, %rd27;
	cvta.to.global.u64 	%rd2, %rd26;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r21, %r1, %r18;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r22, %r2, %r17;
	mad.lo.s32 	%r23, %r21, %r19, %r22;
	cvt.s64.s32 	%rd4, %r23;
	mul.lo.s32 	%r24, %r1, %r20;
	cvt.s64.s32 	%rd5, %r24;
	mov.f32 	%f122, 0f00000000;
	st.local.v2.f32 	[%rd3], {%f122, %f122};
	mov.u32 	%r3, %tid.x;
	setp.ge.s32 	%p1, %r3, %r13;
	mov.f32 	%f123, %f122;
	@%p1 bra 	$L__BB1_9;

	not.b32 	%r25, %r3;
	add.s32 	%r4, %r25, %r13;
	shr.u32 	%r26, %r4, 5;
	add.s32 	%r27, %r26, 1;
	and.b32  	%r58, %r27, 3;
	setp.eq.s32 	%p2, %r58, 0;
	mov.f32 	%f122, 0f00000000;
	mov.u32 	%r59, %r3;
	@%p2 bra 	$L__BB1_5;

	mul.wide.s32 	%rd29, %r14, 2;
	mul.wide.s32 	%rd30, %r3, 2;
	add.s64 	%rd31, %rd29, %rd30;
	add.s64 	%rd32, %rd31, %rd5;
	shl.b64 	%rd33, %rd32, 2;
	add.s64 	%rd59, %rd1, %rd33;
	add.s64 	%rd34, %rd30, %rd5;
	shl.b64 	%rd35, %rd34, 2;
	add.s64 	%rd58, %rd1, %rd35;
	add.s64 	%rd36, %rd30, %rd4;
	shl.b64 	%rd37, %rd36, 2;
	add.s64 	%rd57, %rd2, %rd37;
	mov.f32 	%f122, 0f00000000;
	mov.f32 	%f123, %f122;
	mov.u32 	%r59, %r3;

$L__BB1_3:
	.pragma "nounroll";
	ld.global.nc.v2.f32 	{%f19, %f20}, [%rd57];
	ld.global.nc.v2.f32 	{%f23, %f24}, [%rd58];
	fma.rn.f32 	%f27, %f19, %f23, %f123;
	fma.rn.f32 	%f123, %f20, %f24, %f27;
	ld.global.nc.v2.f32 	{%f28, %f29}, [%rd59];
	fma.rn.f32 	%f32, %f19, %f28, %f122;
	fma.rn.f32 	%f122, %f20, %f29, %f32;
	add.s32 	%r59, %r59, 32;
	add.s64 	%rd59, %rd59, 256;
	add.s64 	%rd58, %rd58, 256;
	add.s64 	%rd57, %rd57, 256;
	add.s32 	%r58, %r58, -1;
	setp.ne.s32 	%p3, %r58, 0;
	@%p3 bra 	$L__BB1_3;

	st.local.v2.f32 	[%rd3], {%f123, %f122};

$L__BB1_5:
	setp.lt.u32 	%p4, %r4, 96;
	@%p4 bra 	$L__BB1_9;

	mul.wide.s32 	%rd38, %r59, 2;
	add.s64 	%rd39, %rd38, %rd4;
	shl.b64 	%rd40, %rd39, 2;
	add.s64 	%rd41, %rd2, %rd40;
	add.s64 	%rd62, %rd41, 512;
	add.s64 	%rd42, %rd38, %rd5;
	shl.b64 	%rd43, %rd42, 2;
	add.s64 	%rd44, %rd1, %rd43;
	add.s64 	%rd61, %rd44, 768;
	mul.wide.s32 	%rd45, %r14, 2;
	add.s64 	%rd46, %rd42, %rd45;
	shl.b64 	%rd47, %rd46, 2;
	add.s64 	%rd48, %rd1, %rd47;
	add.s64 	%rd60, %rd48, 512;

$L__BB1_7:
	ld.global.nc.v2.f32 	{%f33, %f34}, [%rd62+-512];
	ld.global.nc.v2.f32 	{%f37, %f38}, [%rd61+-768];
	fma.rn.f32 	%f41, %f33, %f37, %f123;
	fma.rn.f32 	%f42, %f34, %f38, %f41;
	ld.global.nc.v2.f32 	{%f43, %f44}, [%rd60+-512];
	fma.rn.f32 	%f47, %f33, %f43, %f122;
	fma.rn.f32 	%f48, %f34, %f44, %f47;
	ld.global.nc.v2.f32 	{%f49, %f50}, [%rd62+-256];
	ld.global.nc.v2.f32 	{%f53, %f54}, [%rd61+-512];
	fma.rn.f32 	%f57, %f49, %f53, %f42;
	fma.rn.f32 	%f58, %f50, %f54, %f57;
	ld.global.nc.v2.f32 	{%f59, %f60}, [%rd60+-256];
	fma.rn.f32 	%f63, %f49, %f59, %f48;
	fma.rn.f32 	%f64, %f50, %f60, %f63;
	ld.global.nc.v2.f32 	{%f65, %f66}, [%rd62];
	ld.global.nc.v2.f32 	{%f69, %f70}, [%rd61+-256];
	fma.rn.f32 	%f73, %f65, %f69, %f58;
	fma.rn.f32 	%f74, %f66, %f70, %f73;
	ld.global.nc.v2.f32 	{%f75, %f76}, [%rd60];
	fma.rn.f32 	%f79, %f65, %f75, %f64;
	fma.rn.f32 	%f80, %f66, %f76, %f79;
	ld.global.nc.v2.f32 	{%f81, %f82}, [%rd62+256];
	ld.global.nc.v2.f32 	{%f85, %f86}, [%rd61];
	fma.rn.f32 	%f89, %f81, %f85, %f74;
	fma.rn.f32 	%f123, %f82, %f86, %f89;
	ld.global.nc.v2.f32 	{%f90, %f91}, [%rd60+256];
	fma.rn.f32 	%f94, %f81, %f90, %f80;
	fma.rn.f32 	%f122, %f82, %f91, %f94;
	add.s64 	%rd62, %rd62, 1024;
	add.s64 	%rd61, %rd61, 1024;
	add.s64 	%rd60, %rd60, 1024;
	add.s32 	%r59, %r59, 128;
	setp.lt.s32 	%p5, %r59, %r13;
	@%p5 bra 	$L__BB1_7;

	st.local.v2.f32 	[%rd3], {%f123, %f122};

$L__BB1_9:
	mov.b32 	%r28, %f123;
	mov.u32 	%r29, 31;
	mov.u32 	%r30, 16;
	mov.u32 	%r31, -1;
	shfl.sync.bfly.b32 	%r32|%p6, %r28, %r30, %r29, %r31;
	mov.b32 	%f95, %r32;
	add.f32 	%f96, %f123, %f95;
	mov.b32 	%r33, %f96;
	mov.u32 	%r34, 8;
	shfl.sync.bfly.b32 	%r35|%p7, %r33, %r34, %r29, %r31;
	mov.b32 	%f97, %r35;
	add.f32 	%f98, %f96, %f97;
	mov.b32 	%r36, %f98;
	mov.u32 	%r37, 4;
	shfl.sync.bfly.b32 	%r38|%p8, %r36, %r37, %r29, %r31;
	mov.b32 	%f99, %r38;
	add.f32 	%f100, %f98, %f99;
	mov.b32 	%r39, %f100;
	mov.u32 	%r40, 2;
	shfl.sync.bfly.b32 	%r41|%p9, %r39, %r40, %r29, %r31;
	mov.b32 	%f101, %r41;
	add.f32 	%f102, %f100, %f101;
	mov.b32 	%r42, %f102;
	mov.u32 	%r43, 1;
	shfl.sync.bfly.b32 	%r44|%p10, %r42, %r43, %r29, %r31;
	mov.b32 	%f103, %r44;
	add.f32 	%f104, %f102, %f103;
	st.local.f32 	[%rd3], %f104;
	mov.b32 	%r45, %f122;
	shfl.sync.bfly.b32 	%r46|%p11, %r45, %r30, %r29, %r31;
	mov.b32 	%f105, %r46;
	add.f32 	%f106, %f122, %f105;
	mov.b32 	%r47, %f106;
	shfl.sync.bfly.b32 	%r48|%p12, %r47, %r34, %r29, %r31;
	mov.b32 	%f107, %r48;
	add.f32 	%f108, %f106, %f107;
	mov.b32 	%r49, %f108;
	shfl.sync.bfly.b32 	%r50|%p13, %r49, %r37, %r29, %r31;
	mov.b32 	%f109, %r50;
	add.f32 	%f110, %f108, %f109;
	mov.b32 	%r51, %f110;
	shfl.sync.bfly.b32 	%r52|%p14, %r51, %r40, %r29, %r31;
	mov.b32 	%f111, %r52;
	add.f32 	%f112, %f110, %f111;
	mov.b32 	%r53, %f112;
	shfl.sync.bfly.b32 	%r54|%p15, %r53, %r43, %r29, %r31;
	mov.b32 	%f113, %r54;
	add.f32 	%f114, %f112, %f113;
	st.local.f32 	[%rd3+4], %f114;
	setp.gt.s32 	%p16, %r3, 1;
	@%p16 bra 	$L__BB1_11;

	mul.wide.s32 	%rd49, %r3, 4;
	add.s64 	%rd50, %rd3, %rd49;
	ld.local.f32 	%f115, [%rd50];
	mad.lo.s32 	%r55, %r3, %r15, %r2;
	cvt.s64.s32 	%rd51, %r55;
	mul.lo.s32 	%r56, %r1, %r16;
	cvt.s64.s32 	%rd52, %r56;
	add.s64 	%rd53, %rd52, %rd51;
	cvta.to.global.u64 	%rd54, %rd25;
	shl.b64 	%rd55, %rd53, 2;
	add.s64 	%rd56, %rd54, %rd55;
	st.global.f32 	[%rd56], %f115;

$L__BB1_11:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_3_bs_32
.visible .entry ggml_matvec_f32_ncols_3_bs_32(
	.param .u64 ggml_matvec_f32_ncols_3_bs_32_param_0,
	.param .u64 ggml_matvec_f32_ncols_3_bs_32_param_1,
	.param .u64 ggml_matvec_f32_ncols_3_bs_32_param_2,
	.param .u32 ggml_matvec_f32_ncols_3_bs_32_param_3,
	.param .u32 ggml_matvec_f32_ncols_3_bs_32_param_4,
	.param .u32 ggml_matvec_f32_ncols_3_bs_32_param_5,
	.param .u32 ggml_matvec_f32_ncols_3_bs_32_param_6,
	.param .u32 ggml_matvec_f32_ncols_3_bs_32_param_7,
	.param .u32 ggml_matvec_f32_ncols_3_bs_32_param_8,
	.param .u32 ggml_matvec_f32_ncols_3_bs_32_param_9,
	.param .u32 ggml_matvec_f32_ncols_3_bs_32_param_10,
	.param .u32 ggml_matvec_f32_ncols_3_bs_32_param_11
)
{
	.local .align 4 .b8 	__local_depot2[12];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<22>;
	.reg .f32 	%f<175>;
	.reg .b32 	%r<78>;
	.reg .b64 	%rd<72>;


	mov.u64 	%SPL, __local_depot2;
	ld.param.u64 	%rd29, [ggml_matvec_f32_ncols_3_bs_32_param_0];
	ld.param.u64 	%rd30, [ggml_matvec_f32_ncols_3_bs_32_param_1];
	ld.param.u64 	%rd28, [ggml_matvec_f32_ncols_3_bs_32_param_2];
	ld.param.u32 	%r13, [ggml_matvec_f32_ncols_3_bs_32_param_3];
	ld.param.u32 	%r17, [ggml_matvec_f32_ncols_3_bs_32_param_5];
	ld.param.u32 	%r14, [ggml_matvec_f32_ncols_3_bs_32_param_6];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_3_bs_32_param_7];
	ld.param.u32 	%r18, [ggml_matvec_f32_ncols_3_bs_32_param_8];
	ld.param.u32 	%r19, [ggml_matvec_f32_ncols_3_bs_32_param_9];
	ld.param.u32 	%r20, [ggml_matvec_f32_ncols_3_bs_32_param_10];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_3_bs_32_param_11];
	cvta.to.global.u64 	%rd71, %rd30;
	cvta.to.global.u64 	%rd2, %rd29;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r21, %r1, %r18;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r22, %r2, %r17;
	mad.lo.s32 	%r23, %r21, %r19, %r22;
	cvt.s64.s32 	%rd4, %r23;
	mul.lo.s32 	%r24, %r1, %r20;
	cvt.s64.s32 	%rd5, %r24;
	mov.f32 	%f172, 0f00000000;
	mov.u32 	%r25, 0;
	st.local.u32 	[%rd3], %r25;
	st.local.u32 	[%rd3+4], %r25;
	st.local.u32 	[%rd3+8], %r25;
	mov.u32 	%r3, %tid.x;
	setp.ge.s32 	%p1, %r3, %r13;
	mov.f32 	%f173, %f172;
	mov.f32 	%f174, %f172;
	@%p1 bra 	$L__BB2_9;

	not.b32 	%r26, %r3;
	add.s32 	%r4, %r26, %r13;
	shr.u32 	%r27, %r4, 5;
	add.s32 	%r28, %r27, 1;
	and.b32  	%r75, %r28, 3;
	setp.eq.s32 	%p2, %r75, 0;
	mov.f32 	%f172, 0f00000000;
	mov.u32 	%r76, %r3;
	@%p2 bra 	$L__BB2_5;

	shl.b32 	%r29, %r14, 1;
	add.s32 	%r30, %r3, %r29;
	mul.wide.s32 	%rd32, %r30, 2;
	add.s64 	%rd33, %rd32, %rd5;
	shl.b64 	%rd34, %rd33, 2;
	add.s64 	%rd69, %rd71, %rd34;
	mul.wide.s32 	%rd35, %r14, 2;
	mul.wide.s32 	%rd36, %r3, 2;
	add.s64 	%rd37, %rd35, %rd36;
	add.s64 	%rd38, %rd37, %rd5;
	shl.b64 	%rd39, %rd38, 2;
	add.s64 	%rd68, %rd71, %rd39;
	add.s64 	%rd40, %rd36, %rd5;
	shl.b64 	%rd41, %rd40, 2;
	add.s64 	%rd67, %rd71, %rd41;
	add.s64 	%rd42, %rd36, %rd4;
	shl.b64 	%rd43, %rd42, 2;
	add.s64 	%rd66, %rd2, %rd43;
	mov.f32 	%f172, 0f00000000;
	mov.f32 	%f173, %f172;
	mov.f32 	%f174, %f172;
	mov.u32 	%r76, %r3;

$L__BB2_3:
	.pragma "nounroll";
	ld.global.nc.v2.f32 	{%f28, %f29}, [%rd66];
	ld.global.nc.v2.f32 	{%f32, %f33}, [%rd67];
	fma.rn.f32 	%f36, %f28, %f32, %f174;
	fma.rn.f32 	%f174, %f29, %f33, %f36;
	ld.global.nc.v2.f32 	{%f37, %f38}, [%rd68];
	fma.rn.f32 	%f41, %f28, %f37, %f173;
	fma.rn.f32 	%f173, %f29, %f38, %f41;
	ld.global.nc.v2.f32 	{%f42, %f43}, [%rd69];
	fma.rn.f32 	%f46, %f28, %f42, %f172;
	fma.rn.f32 	%f172, %f29, %f43, %f46;
	add.s32 	%r76, %r76, 32;
	add.s64 	%rd69, %rd69, 256;
	add.s64 	%rd68, %rd68, 256;
	add.s64 	%rd67, %rd67, 256;
	add.s64 	%rd66, %rd66, 256;
	add.s32 	%r75, %r75, -1;
	setp.ne.s32 	%p3, %r75, 0;
	@%p3 bra 	$L__BB2_3;

	st.local.f32 	[%rd3], %f174;
	st.local.f32 	[%rd3+4], %f173;
	st.local.f32 	[%rd3+8], %f172;

$L__BB2_5:
	setp.lt.u32 	%p4, %r4, 96;
	@%p4 bra 	$L__BB2_9;

	add.s32 	%r31, %r76, %r14;
	shl.b32 	%r32, %r14, 1;
	add.s32 	%r33, %r76, %r32;
	add.s32 	%r34, %r31, 32;
	mul.wide.s32 	%rd44, %r34, 8;
	shl.b64 	%rd45, %rd5, 2;
	add.s64 	%rd19, %rd44, %rd45;
	mul.wide.s32 	%rd46, %r33, 8;
	add.s64 	%rd20, %rd46, %rd45;
	mul.wide.s32 	%rd47, %r76, 2;
	add.s64 	%rd48, %rd47, %rd4;
	shl.b64 	%rd49, %rd48, 2;
	add.s64 	%rd50, %rd2, %rd49;
	add.s64 	%rd70, %rd50, 512;
	mul.wide.s32 	%rd51, %r76, 8;
	add.s64 	%rd22, %rd51, %rd45;
	mul.wide.s32 	%rd52, %r14, 8;
	add.s64 	%rd53, %rd51, %rd52;
	add.s64 	%rd23, %rd53, %rd45;

$L__BB2_7:
	ld.global.nc.v2.f32 	{%f47, %f48}, [%rd70+-512];
	add.s64 	%rd54, %rd71, %rd22;
	ld.global.nc.v2.f32 	{%f51, %f52}, [%rd54];
	fma.rn.f32 	%f55, %f47, %f51, %f174;
	fma.rn.f32 	%f56, %f48, %f52, %f55;
	add.s64 	%rd55, %rd71, %rd23;
	ld.global.nc.v2.f32 	{%f57, %f58}, [%rd55];
	fma.rn.f32 	%f61, %f47, %f57, %f173;
	fma.rn.f32 	%f62, %f48, %f58, %f61;
	add.s64 	%rd56, %rd71, %rd20;
	ld.global.nc.v2.f32 	{%f63, %f64}, [%rd56];
	fma.rn.f32 	%f67, %f47, %f63, %f172;
	fma.rn.f32 	%f68, %f48, %f64, %f67;
	ld.global.nc.v2.f32 	{%f69, %f70}, [%rd70+-256];
	ld.global.nc.v2.f32 	{%f73, %f74}, [%rd54+256];
	fma.rn.f32 	%f77, %f69, %f73, %f56;
	fma.rn.f32 	%f78, %f70, %f74, %f77;
	add.s64 	%rd57, %rd71, %rd19;
	ld.global.nc.v2.f32 	{%f79, %f80}, [%rd57];
	fma.rn.f32 	%f83, %f69, %f79, %f62;
	fma.rn.f32 	%f84, %f70, %f80, %f83;
	ld.global.nc.v2.f32 	{%f85, %f86}, [%rd56+256];
	fma.rn.f32 	%f89, %f69, %f85, %f68;
	fma.rn.f32 	%f90, %f70, %f86, %f89;
	ld.global.nc.v2.f32 	{%f91, %f92}, [%rd70];
	ld.global.nc.v2.f32 	{%f95, %f96}, [%rd54+512];
	fma.rn.f32 	%f99, %f91, %f95, %f78;
	fma.rn.f32 	%f100, %f92, %f96, %f99;
	ld.global.nc.v2.f32 	{%f101, %f102}, [%rd57+256];
	fma.rn.f32 	%f105, %f91, %f101, %f84;
	fma.rn.f32 	%f106, %f92, %f102, %f105;
	ld.global.nc.v2.f32 	{%f107, %f108}, [%rd56+512];
	fma.rn.f32 	%f111, %f91, %f107, %f90;
	fma.rn.f32 	%f112, %f92, %f108, %f111;
	ld.global.nc.v2.f32 	{%f113, %f114}, [%rd70+256];
	ld.global.nc.v2.f32 	{%f117, %f118}, [%rd54+768];
	fma.rn.f32 	%f121, %f113, %f117, %f100;
	fma.rn.f32 	%f174, %f114, %f118, %f121;
	ld.global.nc.v2.f32 	{%f122, %f123}, [%rd57+512];
	fma.rn.f32 	%f126, %f113, %f122, %f106;
	fma.rn.f32 	%f173, %f114, %f123, %f126;
	ld.global.nc.v2.f32 	{%f127, %f128}, [%rd56+768];
	fma.rn.f32 	%f131, %f113, %f127, %f112;
	fma.rn.f32 	%f172, %f114, %f128, %f131;
	add.s64 	%rd71, %rd71, 1024;
	add.s64 	%rd70, %rd70, 1024;
	add.s32 	%r76, %r76, 128;
	setp.lt.s32 	%p5, %r76, %r13;
	@%p5 bra 	$L__BB2_7;

	st.local.f32 	[%rd3], %f174;
	st.local.f32 	[%rd3+4], %f173;
	st.local.f32 	[%rd3+8], %f172;

$L__BB2_9:
	mov.b32 	%r35, %f174;
	mov.u32 	%r36, 31;
	mov.u32 	%r37, 16;
	mov.u32 	%r38, -1;
	shfl.sync.bfly.b32 	%r39|%p6, %r35, %r37, %r36, %r38;
	mov.b32 	%f132, %r39;
	add.f32 	%f133, %f174, %f132;
	mov.b32 	%r40, %f133;
	mov.u32 	%r41, 8;
	shfl.sync.bfly.b32 	%r42|%p7, %r40, %r41, %r36, %r38;
	mov.b32 	%f134, %r42;
	add.f32 	%f135, %f133, %f134;
	mov.b32 	%r43, %f135;
	mov.u32 	%r44, 4;
	shfl.sync.bfly.b32 	%r45|%p8, %r43, %r44, %r36, %r38;
	mov.b32 	%f136, %r45;
	add.f32 	%f137, %f135, %f136;
	mov.b32 	%r46, %f137;
	mov.u32 	%r47, 2;
	shfl.sync.bfly.b32 	%r48|%p9, %r46, %r47, %r36, %r38;
	mov.b32 	%f138, %r48;
	add.f32 	%f139, %f137, %f138;
	mov.b32 	%r49, %f139;
	mov.u32 	%r50, 1;
	shfl.sync.bfly.b32 	%r51|%p10, %r49, %r50, %r36, %r38;
	mov.b32 	%f140, %r51;
	add.f32 	%f141, %f139, %f140;
	st.local.f32 	[%rd3], %f141;
	mov.b32 	%r52, %f173;
	shfl.sync.bfly.b32 	%r53|%p11, %r52, %r37, %r36, %r38;
	mov.b32 	%f142, %r53;
	add.f32 	%f143, %f173, %f142;
	mov.b32 	%r54, %f143;
	shfl.sync.bfly.b32 	%r55|%p12, %r54, %r41, %r36, %r38;
	mov.b32 	%f144, %r55;
	add.f32 	%f145, %f143, %f144;
	mov.b32 	%r56, %f145;
	shfl.sync.bfly.b32 	%r57|%p13, %r56, %r44, %r36, %r38;
	mov.b32 	%f146, %r57;
	add.f32 	%f147, %f145, %f146;
	mov.b32 	%r58, %f147;
	shfl.sync.bfly.b32 	%r59|%p14, %r58, %r47, %r36, %r38;
	mov.b32 	%f148, %r59;
	add.f32 	%f149, %f147, %f148;
	mov.b32 	%r60, %f149;
	shfl.sync.bfly.b32 	%r61|%p15, %r60, %r50, %r36, %r38;
	mov.b32 	%f150, %r61;
	add.f32 	%f151, %f149, %f150;
	st.local.f32 	[%rd3+4], %f151;
	mov.b32 	%r62, %f172;
	shfl.sync.bfly.b32 	%r63|%p16, %r62, %r37, %r36, %r38;
	mov.b32 	%f152, %r63;
	add.f32 	%f153, %f172, %f152;
	mov.b32 	%r64, %f153;
	shfl.sync.bfly.b32 	%r65|%p17, %r64, %r41, %r36, %r38;
	mov.b32 	%f154, %r65;
	add.f32 	%f155, %f153, %f154;
	mov.b32 	%r66, %f155;
	shfl.sync.bfly.b32 	%r67|%p18, %r66, %r44, %r36, %r38;
	mov.b32 	%f156, %r67;
	add.f32 	%f157, %f155, %f156;
	mov.b32 	%r68, %f157;
	shfl.sync.bfly.b32 	%r69|%p19, %r68, %r47, %r36, %r38;
	mov.b32 	%f158, %r69;
	add.f32 	%f159, %f157, %f158;
	mov.b32 	%r70, %f159;
	shfl.sync.bfly.b32 	%r71|%p20, %r70, %r50, %r36, %r38;
	mov.b32 	%f160, %r71;
	add.f32 	%f161, %f159, %f160;
	st.local.f32 	[%rd3+8], %f161;
	setp.gt.s32 	%p21, %r3, 2;
	@%p21 bra 	$L__BB2_11;

	mul.wide.s32 	%rd58, %r3, 4;
	add.s64 	%rd59, %rd3, %rd58;
	ld.local.f32 	%f162, [%rd59];
	mad.lo.s32 	%r72, %r3, %r15, %r2;
	cvt.s64.s32 	%rd60, %r72;
	mul.lo.s32 	%r73, %r1, %r16;
	cvt.s64.s32 	%rd61, %r73;
	add.s64 	%rd62, %rd61, %rd60;
	cvta.to.global.u64 	%rd63, %rd28;
	shl.b64 	%rd64, %rd62, 2;
	add.s64 	%rd65, %rd63, %rd64;
	st.global.f32 	[%rd65], %f162;

$L__BB2_11:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_4_bs_32
.visible .entry ggml_matvec_f32_ncols_4_bs_32(
	.param .u64 ggml_matvec_f32_ncols_4_bs_32_param_0,
	.param .u64 ggml_matvec_f32_ncols_4_bs_32_param_1,
	.param .u64 ggml_matvec_f32_ncols_4_bs_32_param_2,
	.param .u32 ggml_matvec_f32_ncols_4_bs_32_param_3,
	.param .u32 ggml_matvec_f32_ncols_4_bs_32_param_4,
	.param .u32 ggml_matvec_f32_ncols_4_bs_32_param_5,
	.param .u32 ggml_matvec_f32_ncols_4_bs_32_param_6,
	.param .u32 ggml_matvec_f32_ncols_4_bs_32_param_7,
	.param .u32 ggml_matvec_f32_ncols_4_bs_32_param_8,
	.param .u32 ggml_matvec_f32_ncols_4_bs_32_param_9,
	.param .u32 ggml_matvec_f32_ncols_4_bs_32_param_10,
	.param .u32 ggml_matvec_f32_ncols_4_bs_32_param_11
)
{
	.local .align 16 .b8 	__local_depot3[16];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<27>;
	.reg .f32 	%f<226>;
	.reg .b32 	%r<89>;
	.reg .b64 	%rd<82>;


	mov.u64 	%SPL, __local_depot3;
	ld.param.u64 	%rd33, [ggml_matvec_f32_ncols_4_bs_32_param_0];
	ld.param.u64 	%rd34, [ggml_matvec_f32_ncols_4_bs_32_param_1];
	ld.param.u64 	%rd32, [ggml_matvec_f32_ncols_4_bs_32_param_2];
	ld.param.u32 	%r13, [ggml_matvec_f32_ncols_4_bs_32_param_3];
	ld.param.u32 	%r17, [ggml_matvec_f32_ncols_4_bs_32_param_5];
	ld.param.u32 	%r14, [ggml_matvec_f32_ncols_4_bs_32_param_6];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_4_bs_32_param_7];
	ld.param.u32 	%r18, [ggml_matvec_f32_ncols_4_bs_32_param_8];
	ld.param.u32 	%r19, [ggml_matvec_f32_ncols_4_bs_32_param_9];
	ld.param.u32 	%r20, [ggml_matvec_f32_ncols_4_bs_32_param_10];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_4_bs_32_param_11];
	cvta.to.global.u64 	%rd81, %rd34;
	cvta.to.global.u64 	%rd2, %rd33;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r21, %r1, %r18;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r22, %r2, %r17;
	mad.lo.s32 	%r23, %r21, %r19, %r22;
	cvt.s64.s32 	%rd4, %r23;
	mul.lo.s32 	%r24, %r1, %r20;
	cvt.s64.s32 	%rd5, %r24;
	mov.f32 	%f222, 0f00000000;
	st.local.v4.f32 	[%rd3], {%f222, %f222, %f222, %f222};
	mov.u32 	%r3, %tid.x;
	setp.ge.s32 	%p1, %r3, %r13;
	mov.f32 	%f223, %f222;
	mov.f32 	%f224, %f222;
	mov.f32 	%f225, %f222;
	@%p1 bra 	$L__BB3_9;

	not.b32 	%r25, %r3;
	add.s32 	%r4, %r25, %r13;
	shr.u32 	%r26, %r4, 5;
	add.s32 	%r27, %r26, 1;
	and.b32  	%r86, %r27, 3;
	setp.eq.s32 	%p2, %r86, 0;
	mov.f32 	%f222, 0f00000000;
	mov.u32 	%r87, %r3;
	@%p2 bra 	$L__BB3_5;

	shl.b32 	%r28, %r14, 1;
	add.s32 	%r29, %r3, %r28;
	mul.wide.s32 	%rd36, %r29, 2;
	add.s64 	%rd37, %rd36, %rd5;
	shl.b64 	%rd38, %rd37, 2;
	add.s64 	%rd79, %rd81, %rd38;
	mad.lo.s32 	%r30, %r14, 3, %r3;
	mul.wide.s32 	%rd39, %r30, 2;
	add.s64 	%rd40, %rd39, %rd5;
	shl.b64 	%rd41, %rd40, 2;
	add.s64 	%rd78, %rd81, %rd41;
	mul.wide.s32 	%rd42, %r14, 2;
	mul.wide.s32 	%rd43, %r3, 2;
	add.s64 	%rd44, %rd42, %rd43;
	add.s64 	%rd45, %rd44, %rd5;
	shl.b64 	%rd46, %rd45, 2;
	add.s64 	%rd77, %rd81, %rd46;
	add.s64 	%rd47, %rd43, %rd5;
	shl.b64 	%rd48, %rd47, 2;
	add.s64 	%rd76, %rd81, %rd48;
	add.s64 	%rd49, %rd43, %rd4;
	shl.b64 	%rd50, %rd49, 2;
	add.s64 	%rd75, %rd2, %rd50;
	mov.f32 	%f222, 0f00000000;
	mov.f32 	%f223, %f222;
	mov.f32 	%f224, %f222;
	mov.f32 	%f225, %f222;
	mov.u32 	%r87, %r3;

$L__BB3_3:
	.pragma "nounroll";
	ld.global.nc.v2.f32 	{%f37, %f38}, [%rd75];
	ld.global.nc.v2.f32 	{%f41, %f42}, [%rd76];
	fma.rn.f32 	%f45, %f37, %f41, %f225;
	fma.rn.f32 	%f225, %f38, %f42, %f45;
	ld.global.nc.v2.f32 	{%f46, %f47}, [%rd77];
	fma.rn.f32 	%f50, %f37, %f46, %f224;
	fma.rn.f32 	%f224, %f38, %f47, %f50;
	ld.global.nc.v2.f32 	{%f51, %f52}, [%rd79];
	fma.rn.f32 	%f55, %f37, %f51, %f223;
	fma.rn.f32 	%f223, %f38, %f52, %f55;
	ld.global.nc.v2.f32 	{%f56, %f57}, [%rd78];
	fma.rn.f32 	%f60, %f37, %f56, %f222;
	fma.rn.f32 	%f222, %f38, %f57, %f60;
	add.s32 	%r87, %r87, 32;
	add.s64 	%rd79, %rd79, 256;
	add.s64 	%rd78, %rd78, 256;
	add.s64 	%rd77, %rd77, 256;
	add.s64 	%rd76, %rd76, 256;
	add.s64 	%rd75, %rd75, 256;
	add.s32 	%r86, %r86, -1;
	setp.ne.s32 	%p3, %r86, 0;
	@%p3 bra 	$L__BB3_3;

	st.local.v4.f32 	[%rd3], {%f225, %f224, %f223, %f222};

$L__BB3_5:
	setp.lt.u32 	%p4, %r4, 96;
	@%p4 bra 	$L__BB3_9;

	add.s32 	%r31, %r87, %r14;
	shl.b32 	%r32, %r14, 1;
	add.s32 	%r33, %r87, %r32;
	mad.lo.s32 	%r34, %r14, 3, %r87;
	add.s32 	%r35, %r31, 32;
	mul.wide.s32 	%rd51, %r35, 8;
	shl.b64 	%rd52, %rd5, 2;
	add.s64 	%rd22, %rd51, %rd52;
	mul.wide.s32 	%rd53, %r33, 8;
	add.s64 	%rd23, %rd53, %rd52;
	mul.wide.s32 	%rd54, %r34, 8;
	add.s64 	%rd24, %rd54, %rd52;
	mul.wide.s32 	%rd55, %r87, 2;
	add.s64 	%rd56, %rd55, %rd4;
	shl.b64 	%rd57, %rd56, 2;
	add.s64 	%rd58, %rd2, %rd57;
	add.s64 	%rd80, %rd58, 512;
	mul.wide.s32 	%rd59, %r87, 8;
	add.s64 	%rd26, %rd59, %rd52;
	mul.wide.s32 	%rd60, %r14, 8;
	add.s64 	%rd61, %rd59, %rd60;
	add.s64 	%rd27, %rd61, %rd52;

$L__BB3_7:
	ld.global.nc.v2.f32 	{%f61, %f62}, [%rd80+-512];
	add.s64 	%rd62, %rd81, %rd26;
	ld.global.nc.v2.f32 	{%f65, %f66}, [%rd62];
	fma.rn.f32 	%f69, %f61, %f65, %f225;
	fma.rn.f32 	%f70, %f62, %f66, %f69;
	add.s64 	%rd63, %rd81, %rd27;
	ld.global.nc.v2.f32 	{%f71, %f72}, [%rd63];
	fma.rn.f32 	%f75, %f61, %f71, %f224;
	fma.rn.f32 	%f76, %f62, %f72, %f75;
	add.s64 	%rd64, %rd81, %rd23;
	ld.global.nc.v2.f32 	{%f77, %f78}, [%rd64];
	fma.rn.f32 	%f81, %f61, %f77, %f223;
	fma.rn.f32 	%f82, %f62, %f78, %f81;
	add.s64 	%rd65, %rd81, %rd24;
	ld.global.nc.v2.f32 	{%f83, %f84}, [%rd65];
	fma.rn.f32 	%f87, %f61, %f83, %f222;
	fma.rn.f32 	%f88, %f62, %f84, %f87;
	ld.global.nc.v2.f32 	{%f89, %f90}, [%rd80+-256];
	ld.global.nc.v2.f32 	{%f93, %f94}, [%rd62+256];
	fma.rn.f32 	%f97, %f89, %f93, %f70;
	fma.rn.f32 	%f98, %f90, %f94, %f97;
	add.s64 	%rd66, %rd81, %rd22;
	ld.global.nc.v2.f32 	{%f99, %f100}, [%rd66];
	fma.rn.f32 	%f103, %f89, %f99, %f76;
	fma.rn.f32 	%f104, %f90, %f100, %f103;
	ld.global.nc.v2.f32 	{%f105, %f106}, [%rd64+256];
	fma.rn.f32 	%f109, %f89, %f105, %f82;
	fma.rn.f32 	%f110, %f90, %f106, %f109;
	ld.global.nc.v2.f32 	{%f111, %f112}, [%rd65+256];
	fma.rn.f32 	%f115, %f89, %f111, %f88;
	fma.rn.f32 	%f116, %f90, %f112, %f115;
	ld.global.nc.v2.f32 	{%f117, %f118}, [%rd80];
	ld.global.nc.v2.f32 	{%f121, %f122}, [%rd62+512];
	fma.rn.f32 	%f125, %f117, %f121, %f98;
	fma.rn.f32 	%f126, %f118, %f122, %f125;
	ld.global.nc.v2.f32 	{%f127, %f128}, [%rd66+256];
	fma.rn.f32 	%f131, %f117, %f127, %f104;
	fma.rn.f32 	%f132, %f118, %f128, %f131;
	ld.global.nc.v2.f32 	{%f133, %f134}, [%rd64+512];
	fma.rn.f32 	%f137, %f117, %f133, %f110;
	fma.rn.f32 	%f138, %f118, %f134, %f137;
	ld.global.nc.v2.f32 	{%f139, %f140}, [%rd65+512];
	fma.rn.f32 	%f143, %f117, %f139, %f116;
	fma.rn.f32 	%f144, %f118, %f140, %f143;
	ld.global.nc.v2.f32 	{%f145, %f146}, [%rd80+256];
	ld.global.nc.v2.f32 	{%f149, %f150}, [%rd62+768];
	fma.rn.f32 	%f153, %f145, %f149, %f126;
	fma.rn.f32 	%f225, %f146, %f150, %f153;
	ld.global.nc.v2.f32 	{%f154, %f155}, [%rd66+512];
	fma.rn.f32 	%f158, %f145, %f154, %f132;
	fma.rn.f32 	%f224, %f146, %f155, %f158;
	ld.global.nc.v2.f32 	{%f159, %f160}, [%rd64+768];
	fma.rn.f32 	%f163, %f145, %f159, %f138;
	fma.rn.f32 	%f223, %f146, %f160, %f163;
	ld.global.nc.v2.f32 	{%f164, %f165}, [%rd65+768];
	fma.rn.f32 	%f168, %f145, %f164, %f144;
	fma.rn.f32 	%f222, %f146, %f165, %f168;
	add.s64 	%rd81, %rd81, 1024;
	add.s64 	%rd80, %rd80, 1024;
	add.s32 	%r87, %r87, 128;
	setp.lt.s32 	%p5, %r87, %r13;
	@%p5 bra 	$L__BB3_7;

	st.local.v4.f32 	[%rd3], {%f225, %f224, %f223, %f222};

$L__BB3_9:
	mov.b32 	%r36, %f225;
	mov.u32 	%r37, 31;
	mov.u32 	%r38, 16;
	mov.u32 	%r39, -1;
	shfl.sync.bfly.b32 	%r40|%p6, %r36, %r38, %r37, %r39;
	mov.b32 	%f169, %r40;
	add.f32 	%f170, %f225, %f169;
	mov.b32 	%r41, %f170;
	mov.u32 	%r42, 8;
	shfl.sync.bfly.b32 	%r43|%p7, %r41, %r42, %r37, %r39;
	mov.b32 	%f171, %r43;
	add.f32 	%f172, %f170, %f171;
	mov.b32 	%r44, %f172;
	mov.u32 	%r45, 4;
	shfl.sync.bfly.b32 	%r46|%p8, %r44, %r45, %r37, %r39;
	mov.b32 	%f173, %r46;
	add.f32 	%f174, %f172, %f173;
	mov.b32 	%r47, %f174;
	mov.u32 	%r48, 2;
	shfl.sync.bfly.b32 	%r49|%p9, %r47, %r48, %r37, %r39;
	mov.b32 	%f175, %r49;
	add.f32 	%f176, %f174, %f175;
	mov.b32 	%r50, %f176;
	mov.u32 	%r51, 1;
	shfl.sync.bfly.b32 	%r52|%p10, %r50, %r51, %r37, %r39;
	mov.b32 	%f177, %r52;
	add.f32 	%f178, %f176, %f177;
	st.local.f32 	[%rd3], %f178;
	mov.b32 	%r53, %f224;
	shfl.sync.bfly.b32 	%r54|%p11, %r53, %r38, %r37, %r39;
	mov.b32 	%f179, %r54;
	add.f32 	%f180, %f224, %f179;
	mov.b32 	%r55, %f180;
	shfl.sync.bfly.b32 	%r56|%p12, %r55, %r42, %r37, %r39;
	mov.b32 	%f181, %r56;
	add.f32 	%f182, %f180, %f181;
	mov.b32 	%r57, %f182;
	shfl.sync.bfly.b32 	%r58|%p13, %r57, %r45, %r37, %r39;
	mov.b32 	%f183, %r58;
	add.f32 	%f184, %f182, %f183;
	mov.b32 	%r59, %f184;
	shfl.sync.bfly.b32 	%r60|%p14, %r59, %r48, %r37, %r39;
	mov.b32 	%f185, %r60;
	add.f32 	%f186, %f184, %f185;
	mov.b32 	%r61, %f186;
	shfl.sync.bfly.b32 	%r62|%p15, %r61, %r51, %r37, %r39;
	mov.b32 	%f187, %r62;
	add.f32 	%f188, %f186, %f187;
	st.local.f32 	[%rd3+4], %f188;
	mov.b32 	%r63, %f223;
	shfl.sync.bfly.b32 	%r64|%p16, %r63, %r38, %r37, %r39;
	mov.b32 	%f189, %r64;
	add.f32 	%f190, %f223, %f189;
	mov.b32 	%r65, %f190;
	shfl.sync.bfly.b32 	%r66|%p17, %r65, %r42, %r37, %r39;
	mov.b32 	%f191, %r66;
	add.f32 	%f192, %f190, %f191;
	mov.b32 	%r67, %f192;
	shfl.sync.bfly.b32 	%r68|%p18, %r67, %r45, %r37, %r39;
	mov.b32 	%f193, %r68;
	add.f32 	%f194, %f192, %f193;
	mov.b32 	%r69, %f194;
	shfl.sync.bfly.b32 	%r70|%p19, %r69, %r48, %r37, %r39;
	mov.b32 	%f195, %r70;
	add.f32 	%f196, %f194, %f195;
	mov.b32 	%r71, %f196;
	shfl.sync.bfly.b32 	%r72|%p20, %r71, %r51, %r37, %r39;
	mov.b32 	%f197, %r72;
	add.f32 	%f198, %f196, %f197;
	st.local.f32 	[%rd3+8], %f198;
	mov.b32 	%r73, %f222;
	shfl.sync.bfly.b32 	%r74|%p21, %r73, %r38, %r37, %r39;
	mov.b32 	%f199, %r74;
	add.f32 	%f200, %f222, %f199;
	mov.b32 	%r75, %f200;
	shfl.sync.bfly.b32 	%r76|%p22, %r75, %r42, %r37, %r39;
	mov.b32 	%f201, %r76;
	add.f32 	%f202, %f200, %f201;
	mov.b32 	%r77, %f202;
	shfl.sync.bfly.b32 	%r78|%p23, %r77, %r45, %r37, %r39;
	mov.b32 	%f203, %r78;
	add.f32 	%f204, %f202, %f203;
	mov.b32 	%r79, %f204;
	shfl.sync.bfly.b32 	%r80|%p24, %r79, %r48, %r37, %r39;
	mov.b32 	%f205, %r80;
	add.f32 	%f206, %f204, %f205;
	mov.b32 	%r81, %f206;
	shfl.sync.bfly.b32 	%r82|%p25, %r81, %r51, %r37, %r39;
	mov.b32 	%f207, %r82;
	add.f32 	%f208, %f206, %f207;
	st.local.f32 	[%rd3+12], %f208;
	setp.gt.s32 	%p26, %r3, 3;
	@%p26 bra 	$L__BB3_11;

	mul.wide.s32 	%rd67, %r3, 4;
	add.s64 	%rd68, %rd3, %rd67;
	ld.local.f32 	%f209, [%rd68];
	mad.lo.s32 	%r83, %r3, %r15, %r2;
	cvt.s64.s32 	%rd69, %r83;
	mul.lo.s32 	%r84, %r1, %r16;
	cvt.s64.s32 	%rd70, %r84;
	add.s64 	%rd71, %rd70, %rd69;
	cvta.to.global.u64 	%rd72, %rd32;
	shl.b64 	%rd73, %rd71, 2;
	add.s64 	%rd74, %rd72, %rd73;
	st.global.f32 	[%rd74], %f209;

$L__BB3_11:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_5_bs_32
.visible .entry ggml_matvec_f32_ncols_5_bs_32(
	.param .u64 ggml_matvec_f32_ncols_5_bs_32_param_0,
	.param .u64 ggml_matvec_f32_ncols_5_bs_32_param_1,
	.param .u64 ggml_matvec_f32_ncols_5_bs_32_param_2,
	.param .u32 ggml_matvec_f32_ncols_5_bs_32_param_3,
	.param .u32 ggml_matvec_f32_ncols_5_bs_32_param_4,
	.param .u32 ggml_matvec_f32_ncols_5_bs_32_param_5,
	.param .u32 ggml_matvec_f32_ncols_5_bs_32_param_6,
	.param .u32 ggml_matvec_f32_ncols_5_bs_32_param_7,
	.param .u32 ggml_matvec_f32_ncols_5_bs_32_param_8,
	.param .u32 ggml_matvec_f32_ncols_5_bs_32_param_9,
	.param .u32 ggml_matvec_f32_ncols_5_bs_32_param_10,
	.param .u32 ggml_matvec_f32_ncols_5_bs_32_param_11
)
{
	.local .align 4 .b8 	__local_depot4[20];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<32>;
	.reg .f32 	%f<277>;
	.reg .b32 	%r<101>;
	.reg .b64 	%rd<74>;


	mov.u64 	%SPL, __local_depot4;
	ld.param.u64 	%rd28, [ggml_matvec_f32_ncols_5_bs_32_param_0];
	ld.param.u64 	%rd29, [ggml_matvec_f32_ncols_5_bs_32_param_1];
	ld.param.u64 	%rd27, [ggml_matvec_f32_ncols_5_bs_32_param_2];
	ld.param.u32 	%r13, [ggml_matvec_f32_ncols_5_bs_32_param_3];
	ld.param.u32 	%r17, [ggml_matvec_f32_ncols_5_bs_32_param_5];
	ld.param.u32 	%r14, [ggml_matvec_f32_ncols_5_bs_32_param_6];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_5_bs_32_param_7];
	ld.param.u32 	%r18, [ggml_matvec_f32_ncols_5_bs_32_param_8];
	ld.param.u32 	%r19, [ggml_matvec_f32_ncols_5_bs_32_param_9];
	ld.param.u32 	%r20, [ggml_matvec_f32_ncols_5_bs_32_param_10];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_5_bs_32_param_11];
	cvta.to.global.u64 	%rd73, %rd29;
	cvta.to.global.u64 	%rd2, %rd28;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r21, %r1, %r18;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r22, %r2, %r17;
	mad.lo.s32 	%r23, %r21, %r19, %r22;
	cvt.s64.s32 	%rd4, %r23;
	mul.lo.s32 	%r24, %r1, %r20;
	cvt.s64.s32 	%rd5, %r24;
	mov.f32 	%f272, 0f00000000;
	mov.u32 	%r25, 0;
	st.local.u32 	[%rd3], %r25;
	st.local.u32 	[%rd3+4], %r25;
	st.local.u32 	[%rd3+8], %r25;
	st.local.u32 	[%rd3+12], %r25;
	st.local.u32 	[%rd3+16], %r25;
	mov.u32 	%r3, %tid.x;
	setp.ge.s32 	%p1, %r3, %r13;
	mov.f32 	%f273, %f272;
	mov.f32 	%f274, %f272;
	mov.f32 	%f275, %f272;
	mov.f32 	%f276, %f272;
	@%p1 bra 	$L__BB4_9;

	not.b32 	%r26, %r3;
	add.s32 	%r4, %r26, %r13;
	shr.u32 	%r27, %r4, 5;
	add.s32 	%r28, %r27, 1;
	and.b32  	%r98, %r28, 3;
	setp.eq.s32 	%p2, %r98, 0;
	mov.f32 	%f272, 0f00000000;
	mov.u32 	%r99, %r3;
	@%p2 bra 	$L__BB4_5;

	shl.b32 	%r29, %r14, 1;
	mad.lo.s32 	%r30, %r14, 3, %r3;
	mul.wide.s32 	%rd31, %r30, 8;
	shl.b64 	%rd32, %rd5, 2;
	add.s64 	%rd7, %rd31, %rd32;
	mul.wide.s32 	%rd33, %r3, 8;
	mul.wide.s32 	%rd34, %r14, 8;
	add.s64 	%rd35, %rd33, %rd34;
	add.s64 	%rd8, %rd35, %rd32;
	add.s64 	%rd9, %rd33, %rd32;
	mul.wide.s32 	%rd36, %r3, 2;
	add.s64 	%rd37, %rd36, %rd4;
	shl.b64 	%rd38, %rd37, 2;
	add.s64 	%rd70, %rd2, %rd38;
	mul.wide.s32 	%rd11, %r29, 8;
	mov.f32 	%f272, 0f00000000;
	mov.u64 	%rd71, %rd73;
	mov.f32 	%f273, %f272;
	mov.f32 	%f274, %f272;
	mov.f32 	%f275, %f272;
	mov.f32 	%f276, %f272;
	mov.u32 	%r99, %r3;

$L__BB4_3:
	.pragma "nounroll";
	ld.global.nc.v2.f32 	{%f46, %f47}, [%rd70];
	add.s64 	%rd39, %rd71, %rd9;
	ld.global.nc.v2.f32 	{%f50, %f51}, [%rd39];
	fma.rn.f32 	%f54, %f46, %f50, %f276;
	fma.rn.f32 	%f276, %f47, %f51, %f54;
	add.s64 	%rd40, %rd71, %rd8;
	ld.global.nc.v2.f32 	{%f55, %f56}, [%rd40];
	fma.rn.f32 	%f59, %f46, %f55, %f275;
	fma.rn.f32 	%f275, %f47, %f56, %f59;
	add.s64 	%rd41, %rd39, %rd11;
	ld.global.nc.v2.f32 	{%f60, %f61}, [%rd41];
	fma.rn.f32 	%f64, %f46, %f60, %f274;
	fma.rn.f32 	%f274, %f47, %f61, %f64;
	add.s64 	%rd42, %rd71, %rd7;
	ld.global.nc.v2.f32 	{%f65, %f66}, [%rd42];
	fma.rn.f32 	%f69, %f46, %f65, %f273;
	fma.rn.f32 	%f273, %f47, %f66, %f69;
	add.s64 	%rd43, %rd41, %rd11;
	ld.global.nc.v2.f32 	{%f70, %f71}, [%rd43];
	fma.rn.f32 	%f74, %f46, %f70, %f272;
	fma.rn.f32 	%f272, %f47, %f71, %f74;
	add.s32 	%r99, %r99, 32;
	add.s64 	%rd71, %rd71, 256;
	add.s64 	%rd70, %rd70, 256;
	add.s32 	%r98, %r98, -1;
	setp.ne.s32 	%p3, %r98, 0;
	@%p3 bra 	$L__BB4_3;

	st.local.f32 	[%rd3], %f276;
	st.local.f32 	[%rd3+4], %f275;
	st.local.f32 	[%rd3+8], %f274;
	st.local.f32 	[%rd3+12], %f273;
	st.local.f32 	[%rd3+16], %f272;

$L__BB4_5:
	setp.lt.u32 	%p4, %r4, 96;
	@%p4 bra 	$L__BB4_9;

	add.s32 	%r31, %r99, %r14;
	shl.b32 	%r32, %r14, 1;
	add.s32 	%r33, %r99, %r32;
	mad.lo.s32 	%r34, %r14, 3, %r99;
	shl.b32 	%r35, %r14, 2;
	add.s32 	%r36, %r99, %r35;
	add.s32 	%r37, %r31, 32;
	mul.wide.s32 	%rd44, %r37, 8;
	shl.b64 	%rd45, %rd5, 2;
	add.s64 	%rd16, %rd44, %rd45;
	mul.wide.s32 	%rd46, %r33, 8;
	add.s64 	%rd17, %rd46, %rd45;
	mul.wide.s32 	%rd47, %r34, 8;
	add.s64 	%rd18, %rd47, %rd45;
	mul.wide.s32 	%rd48, %r36, 8;
	add.s64 	%rd19, %rd48, %rd45;
	mul.wide.s32 	%rd49, %r99, 2;
	add.s64 	%rd50, %rd49, %rd4;
	shl.b64 	%rd51, %rd50, 2;
	add.s64 	%rd52, %rd2, %rd51;
	add.s64 	%rd72, %rd52, 512;
	mul.wide.s32 	%rd53, %r99, 8;
	add.s64 	%rd21, %rd53, %rd45;
	mul.wide.s32 	%rd54, %r14, 8;
	add.s64 	%rd55, %rd53, %rd54;
	add.s64 	%rd22, %rd55, %rd45;

$L__BB4_7:
	ld.global.nc.v2.f32 	{%f75, %f76}, [%rd72+-512];
	add.s64 	%rd56, %rd73, %rd21;
	ld.global.nc.v2.f32 	{%f79, %f80}, [%rd56];
	fma.rn.f32 	%f83, %f75, %f79, %f276;
	fma.rn.f32 	%f84, %f76, %f80, %f83;
	add.s64 	%rd57, %rd73, %rd22;
	ld.global.nc.v2.f32 	{%f85, %f86}, [%rd57];
	fma.rn.f32 	%f89, %f75, %f85, %f275;
	fma.rn.f32 	%f90, %f76, %f86, %f89;
	add.s64 	%rd58, %rd73, %rd17;
	ld.global.nc.v2.f32 	{%f91, %f92}, [%rd58];
	fma.rn.f32 	%f95, %f75, %f91, %f274;
	fma.rn.f32 	%f96, %f76, %f92, %f95;
	add.s64 	%rd59, %rd73, %rd18;
	ld.global.nc.v2.f32 	{%f97, %f98}, [%rd59];
	fma.rn.f32 	%f101, %f75, %f97, %f273;
	fma.rn.f32 	%f102, %f76, %f98, %f101;
	add.s64 	%rd60, %rd73, %rd19;
	ld.global.nc.v2.f32 	{%f103, %f104}, [%rd60];
	fma.rn.f32 	%f107, %f75, %f103, %f272;
	fma.rn.f32 	%f108, %f76, %f104, %f107;
	ld.global.nc.v2.f32 	{%f109, %f110}, [%rd72+-256];
	ld.global.nc.v2.f32 	{%f113, %f114}, [%rd56+256];
	fma.rn.f32 	%f117, %f109, %f113, %f84;
	fma.rn.f32 	%f118, %f110, %f114, %f117;
	add.s64 	%rd61, %rd73, %rd16;
	ld.global.nc.v2.f32 	{%f119, %f120}, [%rd61];
	fma.rn.f32 	%f123, %f109, %f119, %f90;
	fma.rn.f32 	%f124, %f110, %f120, %f123;
	ld.global.nc.v2.f32 	{%f125, %f126}, [%rd58+256];
	fma.rn.f32 	%f129, %f109, %f125, %f96;
	fma.rn.f32 	%f130, %f110, %f126, %f129;
	ld.global.nc.v2.f32 	{%f131, %f132}, [%rd59+256];
	fma.rn.f32 	%f135, %f109, %f131, %f102;
	fma.rn.f32 	%f136, %f110, %f132, %f135;
	ld.global.nc.v2.f32 	{%f137, %f138}, [%rd60+256];
	fma.rn.f32 	%f141, %f109, %f137, %f108;
	fma.rn.f32 	%f142, %f110, %f138, %f141;
	ld.global.nc.v2.f32 	{%f143, %f144}, [%rd72];
	ld.global.nc.v2.f32 	{%f147, %f148}, [%rd56+512];
	fma.rn.f32 	%f151, %f143, %f147, %f118;
	fma.rn.f32 	%f152, %f144, %f148, %f151;
	ld.global.nc.v2.f32 	{%f153, %f154}, [%rd61+256];
	fma.rn.f32 	%f157, %f143, %f153, %f124;
	fma.rn.f32 	%f158, %f144, %f154, %f157;
	ld.global.nc.v2.f32 	{%f159, %f160}, [%rd58+512];
	fma.rn.f32 	%f163, %f143, %f159, %f130;
	fma.rn.f32 	%f164, %f144, %f160, %f163;
	ld.global.nc.v2.f32 	{%f165, %f166}, [%rd59+512];
	fma.rn.f32 	%f169, %f143, %f165, %f136;
	fma.rn.f32 	%f170, %f144, %f166, %f169;
	ld.global.nc.v2.f32 	{%f171, %f172}, [%rd60+512];
	fma.rn.f32 	%f175, %f143, %f171, %f142;
	fma.rn.f32 	%f176, %f144, %f172, %f175;
	ld.global.nc.v2.f32 	{%f177, %f178}, [%rd72+256];
	ld.global.nc.v2.f32 	{%f181, %f182}, [%rd56+768];
	fma.rn.f32 	%f185, %f177, %f181, %f152;
	fma.rn.f32 	%f276, %f178, %f182, %f185;
	ld.global.nc.v2.f32 	{%f186, %f187}, [%rd61+512];
	fma.rn.f32 	%f190, %f177, %f186, %f158;
	fma.rn.f32 	%f275, %f178, %f187, %f190;
	ld.global.nc.v2.f32 	{%f191, %f192}, [%rd58+768];
	fma.rn.f32 	%f195, %f177, %f191, %f164;
	fma.rn.f32 	%f274, %f178, %f192, %f195;
	ld.global.nc.v2.f32 	{%f196, %f197}, [%rd59+768];
	fma.rn.f32 	%f200, %f177, %f196, %f170;
	fma.rn.f32 	%f273, %f178, %f197, %f200;
	ld.global.nc.v2.f32 	{%f201, %f202}, [%rd60+768];
	fma.rn.f32 	%f205, %f177, %f201, %f176;
	fma.rn.f32 	%f272, %f178, %f202, %f205;
	add.s64 	%rd73, %rd73, 1024;
	add.s64 	%rd72, %rd72, 1024;
	add.s32 	%r99, %r99, 128;
	setp.lt.s32 	%p5, %r99, %r13;
	@%p5 bra 	$L__BB4_7;

	st.local.f32 	[%rd3], %f276;
	st.local.f32 	[%rd3+4], %f275;
	st.local.f32 	[%rd3+8], %f274;
	st.local.f32 	[%rd3+12], %f273;
	st.local.f32 	[%rd3+16], %f272;

$L__BB4_9:
	mov.b32 	%r38, %f276;
	mov.u32 	%r39, 31;
	mov.u32 	%r40, 16;
	mov.u32 	%r41, -1;
	shfl.sync.bfly.b32 	%r42|%p6, %r38, %r40, %r39, %r41;
	mov.b32 	%f206, %r42;
	add.f32 	%f207, %f276, %f206;
	mov.b32 	%r43, %f207;
	mov.u32 	%r44, 8;
	shfl.sync.bfly.b32 	%r45|%p7, %r43, %r44, %r39, %r41;
	mov.b32 	%f208, %r45;
	add.f32 	%f209, %f207, %f208;
	mov.b32 	%r46, %f209;
	mov.u32 	%r47, 4;
	shfl.sync.bfly.b32 	%r48|%p8, %r46, %r47, %r39, %r41;
	mov.b32 	%f210, %r48;
	add.f32 	%f211, %f209, %f210;
	mov.b32 	%r49, %f211;
	mov.u32 	%r50, 2;
	shfl.sync.bfly.b32 	%r51|%p9, %r49, %r50, %r39, %r41;
	mov.b32 	%f212, %r51;
	add.f32 	%f213, %f211, %f212;
	mov.b32 	%r52, %f213;
	mov.u32 	%r53, 1;
	shfl.sync.bfly.b32 	%r54|%p10, %r52, %r53, %r39, %r41;
	mov.b32 	%f214, %r54;
	add.f32 	%f215, %f213, %f214;
	st.local.f32 	[%rd3], %f215;
	mov.b32 	%r55, %f275;
	shfl.sync.bfly.b32 	%r56|%p11, %r55, %r40, %r39, %r41;
	mov.b32 	%f216, %r56;
	add.f32 	%f217, %f275, %f216;
	mov.b32 	%r57, %f217;
	shfl.sync.bfly.b32 	%r58|%p12, %r57, %r44, %r39, %r41;
	mov.b32 	%f218, %r58;
	add.f32 	%f219, %f217, %f218;
	mov.b32 	%r59, %f219;
	shfl.sync.bfly.b32 	%r60|%p13, %r59, %r47, %r39, %r41;
	mov.b32 	%f220, %r60;
	add.f32 	%f221, %f219, %f220;
	mov.b32 	%r61, %f221;
	shfl.sync.bfly.b32 	%r62|%p14, %r61, %r50, %r39, %r41;
	mov.b32 	%f222, %r62;
	add.f32 	%f223, %f221, %f222;
	mov.b32 	%r63, %f223;
	shfl.sync.bfly.b32 	%r64|%p15, %r63, %r53, %r39, %r41;
	mov.b32 	%f224, %r64;
	add.f32 	%f225, %f223, %f224;
	st.local.f32 	[%rd3+4], %f225;
	mov.b32 	%r65, %f274;
	shfl.sync.bfly.b32 	%r66|%p16, %r65, %r40, %r39, %r41;
	mov.b32 	%f226, %r66;
	add.f32 	%f227, %f274, %f226;
	mov.b32 	%r67, %f227;
	shfl.sync.bfly.b32 	%r68|%p17, %r67, %r44, %r39, %r41;
	mov.b32 	%f228, %r68;
	add.f32 	%f229, %f227, %f228;
	mov.b32 	%r69, %f229;
	shfl.sync.bfly.b32 	%r70|%p18, %r69, %r47, %r39, %r41;
	mov.b32 	%f230, %r70;
	add.f32 	%f231, %f229, %f230;
	mov.b32 	%r71, %f231;
	shfl.sync.bfly.b32 	%r72|%p19, %r71, %r50, %r39, %r41;
	mov.b32 	%f232, %r72;
	add.f32 	%f233, %f231, %f232;
	mov.b32 	%r73, %f233;
	shfl.sync.bfly.b32 	%r74|%p20, %r73, %r53, %r39, %r41;
	mov.b32 	%f234, %r74;
	add.f32 	%f235, %f233, %f234;
	st.local.f32 	[%rd3+8], %f235;
	mov.b32 	%r75, %f273;
	shfl.sync.bfly.b32 	%r76|%p21, %r75, %r40, %r39, %r41;
	mov.b32 	%f236, %r76;
	add.f32 	%f237, %f273, %f236;
	mov.b32 	%r77, %f237;
	shfl.sync.bfly.b32 	%r78|%p22, %r77, %r44, %r39, %r41;
	mov.b32 	%f238, %r78;
	add.f32 	%f239, %f237, %f238;
	mov.b32 	%r79, %f239;
	shfl.sync.bfly.b32 	%r80|%p23, %r79, %r47, %r39, %r41;
	mov.b32 	%f240, %r80;
	add.f32 	%f241, %f239, %f240;
	mov.b32 	%r81, %f241;
	shfl.sync.bfly.b32 	%r82|%p24, %r81, %r50, %r39, %r41;
	mov.b32 	%f242, %r82;
	add.f32 	%f243, %f241, %f242;
	mov.b32 	%r83, %f243;
	shfl.sync.bfly.b32 	%r84|%p25, %r83, %r53, %r39, %r41;
	mov.b32 	%f244, %r84;
	add.f32 	%f245, %f243, %f244;
	st.local.f32 	[%rd3+12], %f245;
	mov.b32 	%r85, %f272;
	shfl.sync.bfly.b32 	%r86|%p26, %r85, %r40, %r39, %r41;
	mov.b32 	%f246, %r86;
	add.f32 	%f247, %f272, %f246;
	mov.b32 	%r87, %f247;
	shfl.sync.bfly.b32 	%r88|%p27, %r87, %r44, %r39, %r41;
	mov.b32 	%f248, %r88;
	add.f32 	%f249, %f247, %f248;
	mov.b32 	%r89, %f249;
	shfl.sync.bfly.b32 	%r90|%p28, %r89, %r47, %r39, %r41;
	mov.b32 	%f250, %r90;
	add.f32 	%f251, %f249, %f250;
	mov.b32 	%r91, %f251;
	shfl.sync.bfly.b32 	%r92|%p29, %r91, %r50, %r39, %r41;
	mov.b32 	%f252, %r92;
	add.f32 	%f253, %f251, %f252;
	mov.b32 	%r93, %f253;
	shfl.sync.bfly.b32 	%r94|%p30, %r93, %r53, %r39, %r41;
	mov.b32 	%f254, %r94;
	add.f32 	%f255, %f253, %f254;
	st.local.f32 	[%rd3+16], %f255;
	setp.gt.s32 	%p31, %r3, 4;
	@%p31 bra 	$L__BB4_11;

	mul.wide.s32 	%rd62, %r3, 4;
	add.s64 	%rd63, %rd3, %rd62;
	ld.local.f32 	%f256, [%rd63];
	mad.lo.s32 	%r95, %r3, %r15, %r2;
	cvt.s64.s32 	%rd64, %r95;
	mul.lo.s32 	%r96, %r1, %r16;
	cvt.s64.s32 	%rd65, %r96;
	add.s64 	%rd66, %rd65, %rd64;
	cvta.to.global.u64 	%rd67, %rd27;
	shl.b64 	%rd68, %rd66, 2;
	add.s64 	%rd69, %rd67, %rd68;
	st.global.f32 	[%rd69], %f256;

$L__BB4_11:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_6_bs_32
.visible .entry ggml_matvec_f32_ncols_6_bs_32(
	.param .u64 ggml_matvec_f32_ncols_6_bs_32_param_0,
	.param .u64 ggml_matvec_f32_ncols_6_bs_32_param_1,
	.param .u64 ggml_matvec_f32_ncols_6_bs_32_param_2,
	.param .u32 ggml_matvec_f32_ncols_6_bs_32_param_3,
	.param .u32 ggml_matvec_f32_ncols_6_bs_32_param_4,
	.param .u32 ggml_matvec_f32_ncols_6_bs_32_param_5,
	.param .u32 ggml_matvec_f32_ncols_6_bs_32_param_6,
	.param .u32 ggml_matvec_f32_ncols_6_bs_32_param_7,
	.param .u32 ggml_matvec_f32_ncols_6_bs_32_param_8,
	.param .u32 ggml_matvec_f32_ncols_6_bs_32_param_9,
	.param .u32 ggml_matvec_f32_ncols_6_bs_32_param_10,
	.param .u32 ggml_matvec_f32_ncols_6_bs_32_param_11
)
{
	.local .align 8 .b8 	__local_depot5[24];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<36>;
	.reg .f32 	%f<230>;
	.reg .b32 	%r<105>;
	.reg .b64 	%rd<67>;


	mov.u64 	%SPL, __local_depot5;
	ld.param.u64 	%rd20, [ggml_matvec_f32_ncols_6_bs_32_param_0];
	ld.param.u64 	%rd21, [ggml_matvec_f32_ncols_6_bs_32_param_1];
	ld.param.u64 	%rd19, [ggml_matvec_f32_ncols_6_bs_32_param_2];
	ld.param.u32 	%r9, [ggml_matvec_f32_ncols_6_bs_32_param_3];
	ld.param.u32 	%r13, [ggml_matvec_f32_ncols_6_bs_32_param_5];
	ld.param.u32 	%r10, [ggml_matvec_f32_ncols_6_bs_32_param_6];
	ld.param.u32 	%r11, [ggml_matvec_f32_ncols_6_bs_32_param_7];
	ld.param.u32 	%r14, [ggml_matvec_f32_ncols_6_bs_32_param_8];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_6_bs_32_param_9];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_6_bs_32_param_10];
	ld.param.u32 	%r12, [ggml_matvec_f32_ncols_6_bs_32_param_11];
	cvta.to.global.u64 	%rd66, %rd21;
	add.u64 	%rd2, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r17, %r1, %r14;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r18, %r2, %r13;
	mad.lo.s32 	%r19, %r17, %r15, %r18;
	cvt.s64.s32 	%rd3, %r19;
	cvta.to.global.u64 	%rd4, %rd20;
	mul.lo.s32 	%r20, %r1, %r16;
	cvt.s64.s32 	%rd5, %r20;
	mov.f32 	%f224, 0f00000000;
	st.local.v2.f32 	[%rd2], {%f224, %f224};
	st.local.v2.f32 	[%rd2+8], {%f224, %f224};
	st.local.v2.f32 	[%rd2+16], {%f224, %f224};
	mov.u32 	%r3, %tid.x;
	setp.ge.s32 	%p1, %r3, %r9;
	mov.f32 	%f225, %f224;
	mov.f32 	%f226, %f224;
	mov.f32 	%f227, %f224;
	mov.f32 	%f228, %f224;
	mov.f32 	%f229, %f224;
	@%p1 bra 	$L__BB5_7;

	not.b32 	%r21, %r3;
	add.s32 	%r4, %r21, %r9;
	and.b32  	%r22, %r4, 32;
	setp.ne.s32 	%p2, %r22, 0;
	mov.f32 	%f224, 0f00000000;
	mov.u32 	%r104, %r3;
	@%p2 bra 	$L__BB5_3;

	shl.b64 	%rd23, %rd5, 2;
	add.s64 	%rd24, %rd66, %rd23;
	shl.b64 	%rd25, %rd3, 2;
	add.s64 	%rd26, %rd4, %rd25;
	mul.wide.s32 	%rd27, %r3, 8;
	add.s64 	%rd28, %rd26, %rd27;
	ld.global.nc.v2.f32 	{%f43, %f44}, [%rd28];
	add.s64 	%rd29, %rd24, %rd27;
	ld.global.nc.v2.f32 	{%f47, %f48}, [%rd29];
	fma.rn.f32 	%f51, %f43, %f47, 0f00000000;
	fma.rn.f32 	%f229, %f44, %f48, %f51;
	mul.wide.s32 	%rd30, %r10, 8;
	add.s64 	%rd31, %rd29, %rd30;
	ld.global.nc.v2.f32 	{%f52, %f53}, [%rd31];
	fma.rn.f32 	%f56, %f43, %f52, 0f00000000;
	fma.rn.f32 	%f228, %f44, %f53, %f56;
	st.local.v2.f32 	[%rd2], {%f229, %f228};
	add.s32 	%r23, %r3, %r10;
	add.s32 	%r24, %r23, %r10;
	mul.wide.s32 	%rd32, %r24, 8;
	add.s64 	%rd33, %rd24, %rd32;
	ld.global.nc.v2.f32 	{%f57, %f58}, [%rd33];
	fma.rn.f32 	%f61, %f43, %f57, 0f00000000;
	fma.rn.f32 	%f227, %f44, %f58, %f61;
	add.s64 	%rd34, %rd33, %rd30;
	ld.global.nc.v2.f32 	{%f62, %f63}, [%rd34];
	fma.rn.f32 	%f66, %f43, %f62, 0f00000000;
	fma.rn.f32 	%f226, %f44, %f63, %f66;
	st.local.v2.f32 	[%rd2+8], {%f227, %f226};
	add.s64 	%rd35, %rd34, %rd30;
	ld.global.nc.v2.f32 	{%f67, %f68}, [%rd35];
	fma.rn.f32 	%f71, %f43, %f67, 0f00000000;
	fma.rn.f32 	%f225, %f44, %f68, %f71;
	add.s64 	%rd36, %rd35, %rd30;
	ld.global.nc.v2.f32 	{%f72, %f73}, [%rd36];
	fma.rn.f32 	%f76, %f43, %f72, 0f00000000;
	fma.rn.f32 	%f224, %f44, %f73, %f76;
	st.local.v2.f32 	[%rd2+16], {%f225, %f224};
	add.s32 	%r104, %r3, 32;

$L__BB5_3:
	and.b32  	%r25, %r4, -32;
	setp.eq.s32 	%p3, %r25, 0;
	@%p3 bra 	$L__BB5_7;

	add.s32 	%r26, %r104, %r10;
	add.s32 	%r27, %r26, 32;
	mul.wide.s32 	%rd37, %r27, 8;
	shl.b64 	%rd38, %rd5, 2;
	add.s64 	%rd7, %rd37, %rd38;
	shl.b32 	%r28, %r10, 1;
	add.s32 	%r29, %r104, %r28;
	mad.lo.s32 	%r30, %r10, 3, %r104;
	shl.b32 	%r31, %r10, 2;
	add.s32 	%r32, %r104, %r31;
	mad.lo.s32 	%r33, %r10, 5, %r104;
	mul.wide.s32 	%rd39, %r29, 8;
	add.s64 	%rd8, %rd39, %rd38;
	mul.wide.s32 	%rd40, %r30, 8;
	add.s64 	%rd9, %rd40, %rd38;
	mul.wide.s32 	%rd41, %r32, 8;
	add.s64 	%rd10, %rd41, %rd38;
	mul.wide.s32 	%rd42, %r33, 8;
	add.s64 	%rd11, %rd42, %rd38;
	mul.wide.s32 	%rd43, %r104, 2;
	add.s64 	%rd44, %rd43, %rd3;
	shl.b64 	%rd45, %rd44, 2;
	add.s64 	%rd46, %rd4, %rd45;
	add.s64 	%rd65, %rd46, 256;
	mul.wide.s32 	%rd47, %r104, 8;
	mul.wide.s32 	%rd48, %r10, 8;
	add.s64 	%rd49, %rd47, %rd48;
	add.s64 	%rd13, %rd49, %rd38;
	add.s64 	%rd14, %rd47, %rd38;

$L__BB5_5:
	ld.global.nc.v2.f32 	{%f77, %f78}, [%rd65+-256];
	add.s64 	%rd50, %rd66, %rd14;
	ld.global.nc.v2.f32 	{%f81, %f82}, [%rd50];
	fma.rn.f32 	%f85, %f77, %f81, %f229;
	fma.rn.f32 	%f86, %f78, %f82, %f85;
	add.s64 	%rd51, %rd66, %rd13;
	ld.global.nc.v2.f32 	{%f87, %f88}, [%rd51];
	fma.rn.f32 	%f91, %f77, %f87, %f228;
	fma.rn.f32 	%f92, %f78, %f88, %f91;
	add.s64 	%rd52, %rd66, %rd8;
	ld.global.nc.v2.f32 	{%f93, %f94}, [%rd52];
	fma.rn.f32 	%f97, %f77, %f93, %f227;
	fma.rn.f32 	%f98, %f78, %f94, %f97;
	add.s64 	%rd53, %rd66, %rd9;
	ld.global.nc.v2.f32 	{%f99, %f100}, [%rd53];
	fma.rn.f32 	%f103, %f77, %f99, %f226;
	fma.rn.f32 	%f104, %f78, %f100, %f103;
	add.s64 	%rd54, %rd66, %rd10;
	ld.global.nc.v2.f32 	{%f105, %f106}, [%rd54];
	fma.rn.f32 	%f109, %f77, %f105, %f225;
	fma.rn.f32 	%f110, %f78, %f106, %f109;
	add.s64 	%rd55, %rd66, %rd11;
	ld.global.nc.v2.f32 	{%f111, %f112}, [%rd55];
	fma.rn.f32 	%f115, %f77, %f111, %f224;
	fma.rn.f32 	%f116, %f78, %f112, %f115;
	ld.global.nc.v2.f32 	{%f117, %f118}, [%rd65];
	ld.global.nc.v2.f32 	{%f121, %f122}, [%rd50+256];
	fma.rn.f32 	%f125, %f117, %f121, %f86;
	fma.rn.f32 	%f229, %f118, %f122, %f125;
	add.s64 	%rd56, %rd66, %rd7;
	ld.global.nc.v2.f32 	{%f126, %f127}, [%rd56];
	fma.rn.f32 	%f130, %f117, %f126, %f92;
	fma.rn.f32 	%f228, %f118, %f127, %f130;
	ld.global.nc.v2.f32 	{%f131, %f132}, [%rd52+256];
	fma.rn.f32 	%f135, %f117, %f131, %f98;
	fma.rn.f32 	%f227, %f118, %f132, %f135;
	ld.global.nc.v2.f32 	{%f136, %f137}, [%rd53+256];
	fma.rn.f32 	%f140, %f117, %f136, %f104;
	fma.rn.f32 	%f226, %f118, %f137, %f140;
	ld.global.nc.v2.f32 	{%f141, %f142}, [%rd54+256];
	fma.rn.f32 	%f145, %f117, %f141, %f110;
	fma.rn.f32 	%f225, %f118, %f142, %f145;
	ld.global.nc.v2.f32 	{%f146, %f147}, [%rd55+256];
	fma.rn.f32 	%f150, %f117, %f146, %f116;
	fma.rn.f32 	%f224, %f118, %f147, %f150;
	add.s64 	%rd66, %rd66, 512;
	add.s64 	%rd65, %rd65, 512;
	add.s32 	%r104, %r104, 64;
	setp.lt.s32 	%p4, %r104, %r9;
	@%p4 bra 	$L__BB5_5;

	st.local.v2.f32 	[%rd2], {%f229, %f228};
	st.local.v2.f32 	[%rd2+8], {%f227, %f226};
	st.local.v2.f32 	[%rd2+16], {%f225, %f224};

$L__BB5_7:
	mov.b32 	%r34, %f229;
	mov.u32 	%r35, 31;
	mov.u32 	%r36, 16;
	mov.u32 	%r37, -1;
	shfl.sync.bfly.b32 	%r38|%p5, %r34, %r36, %r35, %r37;
	mov.b32 	%f151, %r38;
	add.f32 	%f152, %f229, %f151;
	mov.b32 	%r39, %f152;
	mov.u32 	%r40, 8;
	shfl.sync.bfly.b32 	%r41|%p6, %r39, %r40, %r35, %r37;
	mov.b32 	%f153, %r41;
	add.f32 	%f154, %f152, %f153;
	mov.b32 	%r42, %f154;
	mov.u32 	%r43, 4;
	shfl.sync.bfly.b32 	%r44|%p7, %r42, %r43, %r35, %r37;
	mov.b32 	%f155, %r44;
	add.f32 	%f156, %f154, %f155;
	mov.b32 	%r45, %f156;
	mov.u32 	%r46, 2;
	shfl.sync.bfly.b32 	%r47|%p8, %r45, %r46, %r35, %r37;
	mov.b32 	%f157, %r47;
	add.f32 	%f158, %f156, %f157;
	mov.b32 	%r48, %f158;
	mov.u32 	%r49, 1;
	shfl.sync.bfly.b32 	%r50|%p9, %r48, %r49, %r35, %r37;
	mov.b32 	%f159, %r50;
	add.f32 	%f160, %f158, %f159;
	st.local.f32 	[%rd2], %f160;
	mov.b32 	%r51, %f228;
	shfl.sync.bfly.b32 	%r52|%p10, %r51, %r36, %r35, %r37;
	mov.b32 	%f161, %r52;
	add.f32 	%f162, %f228, %f161;
	mov.b32 	%r53, %f162;
	shfl.sync.bfly.b32 	%r54|%p11, %r53, %r40, %r35, %r37;
	mov.b32 	%f163, %r54;
	add.f32 	%f164, %f162, %f163;
	mov.b32 	%r55, %f164;
	shfl.sync.bfly.b32 	%r56|%p12, %r55, %r43, %r35, %r37;
	mov.b32 	%f165, %r56;
	add.f32 	%f166, %f164, %f165;
	mov.b32 	%r57, %f166;
	shfl.sync.bfly.b32 	%r58|%p13, %r57, %r46, %r35, %r37;
	mov.b32 	%f167, %r58;
	add.f32 	%f168, %f166, %f167;
	mov.b32 	%r59, %f168;
	shfl.sync.bfly.b32 	%r60|%p14, %r59, %r49, %r35, %r37;
	mov.b32 	%f169, %r60;
	add.f32 	%f170, %f168, %f169;
	st.local.f32 	[%rd2+4], %f170;
	mov.b32 	%r61, %f227;
	shfl.sync.bfly.b32 	%r62|%p15, %r61, %r36, %r35, %r37;
	mov.b32 	%f171, %r62;
	add.f32 	%f172, %f227, %f171;
	mov.b32 	%r63, %f172;
	shfl.sync.bfly.b32 	%r64|%p16, %r63, %r40, %r35, %r37;
	mov.b32 	%f173, %r64;
	add.f32 	%f174, %f172, %f173;
	mov.b32 	%r65, %f174;
	shfl.sync.bfly.b32 	%r66|%p17, %r65, %r43, %r35, %r37;
	mov.b32 	%f175, %r66;
	add.f32 	%f176, %f174, %f175;
	mov.b32 	%r67, %f176;
	shfl.sync.bfly.b32 	%r68|%p18, %r67, %r46, %r35, %r37;
	mov.b32 	%f177, %r68;
	add.f32 	%f178, %f176, %f177;
	mov.b32 	%r69, %f178;
	shfl.sync.bfly.b32 	%r70|%p19, %r69, %r49, %r35, %r37;
	mov.b32 	%f179, %r70;
	add.f32 	%f180, %f178, %f179;
	st.local.f32 	[%rd2+8], %f180;
	mov.b32 	%r71, %f226;
	shfl.sync.bfly.b32 	%r72|%p20, %r71, %r36, %r35, %r37;
	mov.b32 	%f181, %r72;
	add.f32 	%f182, %f226, %f181;
	mov.b32 	%r73, %f182;
	shfl.sync.bfly.b32 	%r74|%p21, %r73, %r40, %r35, %r37;
	mov.b32 	%f183, %r74;
	add.f32 	%f184, %f182, %f183;
	mov.b32 	%r75, %f184;
	shfl.sync.bfly.b32 	%r76|%p22, %r75, %r43, %r35, %r37;
	mov.b32 	%f185, %r76;
	add.f32 	%f186, %f184, %f185;
	mov.b32 	%r77, %f186;
	shfl.sync.bfly.b32 	%r78|%p23, %r77, %r46, %r35, %r37;
	mov.b32 	%f187, %r78;
	add.f32 	%f188, %f186, %f187;
	mov.b32 	%r79, %f188;
	shfl.sync.bfly.b32 	%r80|%p24, %r79, %r49, %r35, %r37;
	mov.b32 	%f189, %r80;
	add.f32 	%f190, %f188, %f189;
	st.local.f32 	[%rd2+12], %f190;
	mov.b32 	%r81, %f225;
	shfl.sync.bfly.b32 	%r82|%p25, %r81, %r36, %r35, %r37;
	mov.b32 	%f191, %r82;
	add.f32 	%f192, %f225, %f191;
	mov.b32 	%r83, %f192;
	shfl.sync.bfly.b32 	%r84|%p26, %r83, %r40, %r35, %r37;
	mov.b32 	%f193, %r84;
	add.f32 	%f194, %f192, %f193;
	mov.b32 	%r85, %f194;
	shfl.sync.bfly.b32 	%r86|%p27, %r85, %r43, %r35, %r37;
	mov.b32 	%f195, %r86;
	add.f32 	%f196, %f194, %f195;
	mov.b32 	%r87, %f196;
	shfl.sync.bfly.b32 	%r88|%p28, %r87, %r46, %r35, %r37;
	mov.b32 	%f197, %r88;
	add.f32 	%f198, %f196, %f197;
	mov.b32 	%r89, %f198;
	shfl.sync.bfly.b32 	%r90|%p29, %r89, %r49, %r35, %r37;
	mov.b32 	%f199, %r90;
	add.f32 	%f200, %f198, %f199;
	st.local.f32 	[%rd2+16], %f200;
	mov.b32 	%r91, %f224;
	shfl.sync.bfly.b32 	%r92|%p30, %r91, %r36, %r35, %r37;
	mov.b32 	%f201, %r92;
	add.f32 	%f202, %f224, %f201;
	mov.b32 	%r93, %f202;
	shfl.sync.bfly.b32 	%r94|%p31, %r93, %r40, %r35, %r37;
	mov.b32 	%f203, %r94;
	add.f32 	%f204, %f202, %f203;
	mov.b32 	%r95, %f204;
	shfl.sync.bfly.b32 	%r96|%p32, %r95, %r43, %r35, %r37;
	mov.b32 	%f205, %r96;
	add.f32 	%f206, %f204, %f205;
	mov.b32 	%r97, %f206;
	shfl.sync.bfly.b32 	%r98|%p33, %r97, %r46, %r35, %r37;
	mov.b32 	%f207, %r98;
	add.f32 	%f208, %f206, %f207;
	mov.b32 	%r99, %f208;
	shfl.sync.bfly.b32 	%r100|%p34, %r99, %r49, %r35, %r37;
	mov.b32 	%f209, %r100;
	add.f32 	%f210, %f208, %f209;
	st.local.f32 	[%rd2+20], %f210;
	setp.gt.s32 	%p35, %r3, 5;
	@%p35 bra 	$L__BB5_9;

	mul.wide.s32 	%rd57, %r3, 4;
	add.s64 	%rd58, %rd2, %rd57;
	ld.local.f32 	%f211, [%rd58];
	mad.lo.s32 	%r101, %r3, %r11, %r2;
	cvt.s64.s32 	%rd59, %r101;
	mul.lo.s32 	%r102, %r1, %r12;
	cvt.s64.s32 	%rd60, %r102;
	add.s64 	%rd61, %rd60, %rd59;
	cvta.to.global.u64 	%rd62, %rd19;
	shl.b64 	%rd63, %rd61, 2;
	add.s64 	%rd64, %rd62, %rd63;
	st.global.f32 	[%rd64], %f211;

$L__BB5_9:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_7_bs_32
.visible .entry ggml_matvec_f32_ncols_7_bs_32(
	.param .u64 ggml_matvec_f32_ncols_7_bs_32_param_0,
	.param .u64 ggml_matvec_f32_ncols_7_bs_32_param_1,
	.param .u64 ggml_matvec_f32_ncols_7_bs_32_param_2,
	.param .u32 ggml_matvec_f32_ncols_7_bs_32_param_3,
	.param .u32 ggml_matvec_f32_ncols_7_bs_32_param_4,
	.param .u32 ggml_matvec_f32_ncols_7_bs_32_param_5,
	.param .u32 ggml_matvec_f32_ncols_7_bs_32_param_6,
	.param .u32 ggml_matvec_f32_ncols_7_bs_32_param_7,
	.param .u32 ggml_matvec_f32_ncols_7_bs_32_param_8,
	.param .u32 ggml_matvec_f32_ncols_7_bs_32_param_9,
	.param .u32 ggml_matvec_f32_ncols_7_bs_32_param_10,
	.param .u32 ggml_matvec_f32_ncols_7_bs_32_param_11
)
{
	.local .align 4 .b8 	__local_depot6[28];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<41>;
	.reg .f32 	%f<266>;
	.reg .b32 	%r<117>;
	.reg .b64 	%rd<70>;


	mov.u64 	%SPL, __local_depot6;
	ld.param.u64 	%rd20, [ggml_matvec_f32_ncols_7_bs_32_param_0];
	ld.param.u64 	%rd21, [ggml_matvec_f32_ncols_7_bs_32_param_1];
	ld.param.u64 	%rd19, [ggml_matvec_f32_ncols_7_bs_32_param_2];
	ld.param.u32 	%r9, [ggml_matvec_f32_ncols_7_bs_32_param_3];
	ld.param.u32 	%r13, [ggml_matvec_f32_ncols_7_bs_32_param_5];
	ld.param.u32 	%r10, [ggml_matvec_f32_ncols_7_bs_32_param_6];
	ld.param.u32 	%r11, [ggml_matvec_f32_ncols_7_bs_32_param_7];
	ld.param.u32 	%r14, [ggml_matvec_f32_ncols_7_bs_32_param_8];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_7_bs_32_param_9];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_7_bs_32_param_10];
	ld.param.u32 	%r12, [ggml_matvec_f32_ncols_7_bs_32_param_11];
	cvta.to.global.u64 	%rd69, %rd21;
	add.u64 	%rd2, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r17, %r1, %r14;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r18, %r2, %r13;
	mad.lo.s32 	%r19, %r17, %r15, %r18;
	cvt.s64.s32 	%rd3, %r19;
	cvta.to.global.u64 	%rd4, %rd20;
	mul.lo.s32 	%r20, %r1, %r16;
	cvt.s64.s32 	%rd5, %r20;
	mov.f32 	%f259, 0f00000000;
	mov.u32 	%r21, 0;
	st.local.u32 	[%rd2], %r21;
	st.local.u32 	[%rd2+4], %r21;
	st.local.u32 	[%rd2+8], %r21;
	st.local.u32 	[%rd2+12], %r21;
	st.local.u32 	[%rd2+16], %r21;
	st.local.u32 	[%rd2+20], %r21;
	st.local.u32 	[%rd2+24], %r21;
	mov.u32 	%r3, %tid.x;
	setp.ge.s32 	%p1, %r3, %r9;
	mov.f32 	%f260, %f259;
	mov.f32 	%f261, %f259;
	mov.f32 	%f262, %f259;
	mov.f32 	%f263, %f259;
	mov.f32 	%f264, %f259;
	mov.f32 	%f265, %f259;
	@%p1 bra 	$L__BB6_7;

	not.b32 	%r22, %r3;
	add.s32 	%r4, %r22, %r9;
	and.b32  	%r23, %r4, 32;
	setp.ne.s32 	%p2, %r23, 0;
	mov.f32 	%f259, 0f00000000;
	mov.u32 	%r116, %r3;
	@%p2 bra 	$L__BB6_3;

	shl.b64 	%rd23, %rd5, 2;
	add.s64 	%rd24, %rd69, %rd23;
	shl.b64 	%rd25, %rd3, 2;
	add.s64 	%rd26, %rd4, %rd25;
	mul.wide.s32 	%rd27, %r3, 8;
	add.s64 	%rd28, %rd26, %rd27;
	ld.global.nc.v2.f32 	{%f50, %f51}, [%rd28];
	add.s64 	%rd29, %rd24, %rd27;
	ld.global.nc.v2.f32 	{%f54, %f55}, [%rd29];
	fma.rn.f32 	%f58, %f50, %f54, 0f00000000;
	fma.rn.f32 	%f265, %f51, %f55, %f58;
	st.local.f32 	[%rd2], %f265;
	mul.wide.s32 	%rd30, %r10, 8;
	add.s64 	%rd31, %rd29, %rd30;
	ld.global.nc.v2.f32 	{%f59, %f60}, [%rd31];
	fma.rn.f32 	%f63, %f50, %f59, 0f00000000;
	fma.rn.f32 	%f264, %f51, %f60, %f63;
	st.local.f32 	[%rd2+4], %f264;
	add.s32 	%r24, %r3, %r10;
	add.s32 	%r25, %r24, %r10;
	mul.wide.s32 	%rd32, %r25, 8;
	add.s64 	%rd33, %rd24, %rd32;
	ld.global.nc.v2.f32 	{%f64, %f65}, [%rd33];
	fma.rn.f32 	%f68, %f50, %f64, 0f00000000;
	fma.rn.f32 	%f263, %f51, %f65, %f68;
	st.local.f32 	[%rd2+8], %f263;
	add.s64 	%rd34, %rd33, %rd30;
	ld.global.nc.v2.f32 	{%f69, %f70}, [%rd34];
	fma.rn.f32 	%f73, %f50, %f69, 0f00000000;
	fma.rn.f32 	%f262, %f51, %f70, %f73;
	st.local.f32 	[%rd2+12], %f262;
	add.s64 	%rd35, %rd34, %rd30;
	ld.global.nc.v2.f32 	{%f74, %f75}, [%rd35];
	fma.rn.f32 	%f78, %f50, %f74, 0f00000000;
	fma.rn.f32 	%f261, %f51, %f75, %f78;
	st.local.f32 	[%rd2+16], %f261;
	add.s64 	%rd36, %rd35, %rd30;
	ld.global.nc.v2.f32 	{%f79, %f80}, [%rd36];
	fma.rn.f32 	%f83, %f50, %f79, 0f00000000;
	fma.rn.f32 	%f260, %f51, %f80, %f83;
	st.local.f32 	[%rd2+20], %f260;
	add.s64 	%rd37, %rd36, %rd30;
	ld.global.nc.v2.f32 	{%f84, %f85}, [%rd37];
	fma.rn.f32 	%f88, %f50, %f84, 0f00000000;
	fma.rn.f32 	%f259, %f51, %f85, %f88;
	st.local.f32 	[%rd2+24], %f259;
	add.s32 	%r116, %r3, 32;

$L__BB6_3:
	and.b32  	%r26, %r4, -32;
	setp.eq.s32 	%p3, %r26, 0;
	@%p3 bra 	$L__BB6_7;

	add.s32 	%r27, %r116, %r10;
	add.s32 	%r28, %r27, 32;
	mul.wide.s32 	%rd38, %r28, 8;
	shl.b64 	%rd39, %rd5, 2;
	add.s64 	%rd6, %rd38, %rd39;
	shl.b32 	%r29, %r10, 1;
	add.s32 	%r30, %r116, %r29;
	mad.lo.s32 	%r31, %r10, 3, %r116;
	shl.b32 	%r32, %r10, 2;
	add.s32 	%r33, %r116, %r32;
	mad.lo.s32 	%r34, %r10, 5, %r116;
	mad.lo.s32 	%r35, %r10, 6, %r116;
	mul.wide.s32 	%rd40, %r30, 8;
	add.s64 	%rd7, %rd40, %rd39;
	mul.wide.s32 	%rd41, %r31, 8;
	add.s64 	%rd8, %rd41, %rd39;
	mul.wide.s32 	%rd42, %r33, 8;
	add.s64 	%rd9, %rd42, %rd39;
	mul.wide.s32 	%rd43, %r34, 8;
	add.s64 	%rd10, %rd43, %rd39;
	mul.wide.s32 	%rd44, %r35, 8;
	add.s64 	%rd11, %rd44, %rd39;
	mul.wide.s32 	%rd45, %r116, 2;
	add.s64 	%rd46, %rd45, %rd3;
	shl.b64 	%rd47, %rd46, 2;
	add.s64 	%rd48, %rd4, %rd47;
	add.s64 	%rd68, %rd48, 256;
	mul.wide.s32 	%rd49, %r116, 8;
	mul.wide.s32 	%rd50, %r10, 8;
	add.s64 	%rd51, %rd49, %rd50;
	add.s64 	%rd13, %rd51, %rd39;
	add.s64 	%rd14, %rd49, %rd39;

$L__BB6_5:
	ld.global.nc.v2.f32 	{%f89, %f90}, [%rd68+-256];
	add.s64 	%rd52, %rd69, %rd14;
	ld.global.nc.v2.f32 	{%f93, %f94}, [%rd52];
	fma.rn.f32 	%f97, %f89, %f93, %f265;
	fma.rn.f32 	%f98, %f90, %f94, %f97;
	add.s64 	%rd53, %rd69, %rd13;
	ld.global.nc.v2.f32 	{%f99, %f100}, [%rd53];
	fma.rn.f32 	%f103, %f89, %f99, %f264;
	fma.rn.f32 	%f104, %f90, %f100, %f103;
	add.s64 	%rd54, %rd69, %rd7;
	ld.global.nc.v2.f32 	{%f105, %f106}, [%rd54];
	fma.rn.f32 	%f109, %f89, %f105, %f263;
	fma.rn.f32 	%f110, %f90, %f106, %f109;
	add.s64 	%rd55, %rd69, %rd8;
	ld.global.nc.v2.f32 	{%f111, %f112}, [%rd55];
	fma.rn.f32 	%f115, %f89, %f111, %f262;
	fma.rn.f32 	%f116, %f90, %f112, %f115;
	add.s64 	%rd56, %rd69, %rd9;
	ld.global.nc.v2.f32 	{%f117, %f118}, [%rd56];
	fma.rn.f32 	%f121, %f89, %f117, %f261;
	fma.rn.f32 	%f122, %f90, %f118, %f121;
	add.s64 	%rd57, %rd69, %rd10;
	ld.global.nc.v2.f32 	{%f123, %f124}, [%rd57];
	fma.rn.f32 	%f127, %f89, %f123, %f260;
	fma.rn.f32 	%f128, %f90, %f124, %f127;
	add.s64 	%rd58, %rd69, %rd11;
	ld.global.nc.v2.f32 	{%f129, %f130}, [%rd58];
	fma.rn.f32 	%f133, %f89, %f129, %f259;
	fma.rn.f32 	%f134, %f90, %f130, %f133;
	ld.global.nc.v2.f32 	{%f135, %f136}, [%rd68];
	ld.global.nc.v2.f32 	{%f139, %f140}, [%rd52+256];
	fma.rn.f32 	%f143, %f135, %f139, %f98;
	fma.rn.f32 	%f265, %f136, %f140, %f143;
	add.s64 	%rd59, %rd69, %rd6;
	ld.global.nc.v2.f32 	{%f144, %f145}, [%rd59];
	fma.rn.f32 	%f148, %f135, %f144, %f104;
	fma.rn.f32 	%f264, %f136, %f145, %f148;
	ld.global.nc.v2.f32 	{%f149, %f150}, [%rd54+256];
	fma.rn.f32 	%f153, %f135, %f149, %f110;
	fma.rn.f32 	%f263, %f136, %f150, %f153;
	ld.global.nc.v2.f32 	{%f154, %f155}, [%rd55+256];
	fma.rn.f32 	%f158, %f135, %f154, %f116;
	fma.rn.f32 	%f262, %f136, %f155, %f158;
	ld.global.nc.v2.f32 	{%f159, %f160}, [%rd56+256];
	fma.rn.f32 	%f163, %f135, %f159, %f122;
	fma.rn.f32 	%f261, %f136, %f160, %f163;
	ld.global.nc.v2.f32 	{%f164, %f165}, [%rd57+256];
	fma.rn.f32 	%f168, %f135, %f164, %f128;
	fma.rn.f32 	%f260, %f136, %f165, %f168;
	ld.global.nc.v2.f32 	{%f169, %f170}, [%rd58+256];
	fma.rn.f32 	%f173, %f135, %f169, %f134;
	fma.rn.f32 	%f259, %f136, %f170, %f173;
	add.s64 	%rd69, %rd69, 512;
	add.s64 	%rd68, %rd68, 512;
	add.s32 	%r116, %r116, 64;
	setp.lt.s32 	%p4, %r116, %r9;
	@%p4 bra 	$L__BB6_5;

	st.local.f32 	[%rd2], %f265;
	st.local.f32 	[%rd2+4], %f264;
	st.local.f32 	[%rd2+8], %f263;
	st.local.f32 	[%rd2+12], %f262;
	st.local.f32 	[%rd2+16], %f261;
	st.local.f32 	[%rd2+20], %f260;
	st.local.f32 	[%rd2+24], %f259;

$L__BB6_7:
	mov.b32 	%r36, %f265;
	mov.u32 	%r37, 31;
	mov.u32 	%r38, 16;
	mov.u32 	%r39, -1;
	shfl.sync.bfly.b32 	%r40|%p5, %r36, %r38, %r37, %r39;
	mov.b32 	%f174, %r40;
	add.f32 	%f175, %f265, %f174;
	mov.b32 	%r41, %f175;
	mov.u32 	%r42, 8;
	shfl.sync.bfly.b32 	%r43|%p6, %r41, %r42, %r37, %r39;
	mov.b32 	%f176, %r43;
	add.f32 	%f177, %f175, %f176;
	mov.b32 	%r44, %f177;
	mov.u32 	%r45, 4;
	shfl.sync.bfly.b32 	%r46|%p7, %r44, %r45, %r37, %r39;
	mov.b32 	%f178, %r46;
	add.f32 	%f179, %f177, %f178;
	mov.b32 	%r47, %f179;
	mov.u32 	%r48, 2;
	shfl.sync.bfly.b32 	%r49|%p8, %r47, %r48, %r37, %r39;
	mov.b32 	%f180, %r49;
	add.f32 	%f181, %f179, %f180;
	mov.b32 	%r50, %f181;
	mov.u32 	%r51, 1;
	shfl.sync.bfly.b32 	%r52|%p9, %r50, %r51, %r37, %r39;
	mov.b32 	%f182, %r52;
	add.f32 	%f183, %f181, %f182;
	st.local.f32 	[%rd2], %f183;
	mov.b32 	%r53, %f264;
	shfl.sync.bfly.b32 	%r54|%p10, %r53, %r38, %r37, %r39;
	mov.b32 	%f184, %r54;
	add.f32 	%f185, %f264, %f184;
	mov.b32 	%r55, %f185;
	shfl.sync.bfly.b32 	%r56|%p11, %r55, %r42, %r37, %r39;
	mov.b32 	%f186, %r56;
	add.f32 	%f187, %f185, %f186;
	mov.b32 	%r57, %f187;
	shfl.sync.bfly.b32 	%r58|%p12, %r57, %r45, %r37, %r39;
	mov.b32 	%f188, %r58;
	add.f32 	%f189, %f187, %f188;
	mov.b32 	%r59, %f189;
	shfl.sync.bfly.b32 	%r60|%p13, %r59, %r48, %r37, %r39;
	mov.b32 	%f190, %r60;
	add.f32 	%f191, %f189, %f190;
	mov.b32 	%r61, %f191;
	shfl.sync.bfly.b32 	%r62|%p14, %r61, %r51, %r37, %r39;
	mov.b32 	%f192, %r62;
	add.f32 	%f193, %f191, %f192;
	st.local.f32 	[%rd2+4], %f193;
	mov.b32 	%r63, %f263;
	shfl.sync.bfly.b32 	%r64|%p15, %r63, %r38, %r37, %r39;
	mov.b32 	%f194, %r64;
	add.f32 	%f195, %f263, %f194;
	mov.b32 	%r65, %f195;
	shfl.sync.bfly.b32 	%r66|%p16, %r65, %r42, %r37, %r39;
	mov.b32 	%f196, %r66;
	add.f32 	%f197, %f195, %f196;
	mov.b32 	%r67, %f197;
	shfl.sync.bfly.b32 	%r68|%p17, %r67, %r45, %r37, %r39;
	mov.b32 	%f198, %r68;
	add.f32 	%f199, %f197, %f198;
	mov.b32 	%r69, %f199;
	shfl.sync.bfly.b32 	%r70|%p18, %r69, %r48, %r37, %r39;
	mov.b32 	%f200, %r70;
	add.f32 	%f201, %f199, %f200;
	mov.b32 	%r71, %f201;
	shfl.sync.bfly.b32 	%r72|%p19, %r71, %r51, %r37, %r39;
	mov.b32 	%f202, %r72;
	add.f32 	%f203, %f201, %f202;
	st.local.f32 	[%rd2+8], %f203;
	mov.b32 	%r73, %f262;
	shfl.sync.bfly.b32 	%r74|%p20, %r73, %r38, %r37, %r39;
	mov.b32 	%f204, %r74;
	add.f32 	%f205, %f262, %f204;
	mov.b32 	%r75, %f205;
	shfl.sync.bfly.b32 	%r76|%p21, %r75, %r42, %r37, %r39;
	mov.b32 	%f206, %r76;
	add.f32 	%f207, %f205, %f206;
	mov.b32 	%r77, %f207;
	shfl.sync.bfly.b32 	%r78|%p22, %r77, %r45, %r37, %r39;
	mov.b32 	%f208, %r78;
	add.f32 	%f209, %f207, %f208;
	mov.b32 	%r79, %f209;
	shfl.sync.bfly.b32 	%r80|%p23, %r79, %r48, %r37, %r39;
	mov.b32 	%f210, %r80;
	add.f32 	%f211, %f209, %f210;
	mov.b32 	%r81, %f211;
	shfl.sync.bfly.b32 	%r82|%p24, %r81, %r51, %r37, %r39;
	mov.b32 	%f212, %r82;
	add.f32 	%f213, %f211, %f212;
	st.local.f32 	[%rd2+12], %f213;
	mov.b32 	%r83, %f261;
	shfl.sync.bfly.b32 	%r84|%p25, %r83, %r38, %r37, %r39;
	mov.b32 	%f214, %r84;
	add.f32 	%f215, %f261, %f214;
	mov.b32 	%r85, %f215;
	shfl.sync.bfly.b32 	%r86|%p26, %r85, %r42, %r37, %r39;
	mov.b32 	%f216, %r86;
	add.f32 	%f217, %f215, %f216;
	mov.b32 	%r87, %f217;
	shfl.sync.bfly.b32 	%r88|%p27, %r87, %r45, %r37, %r39;
	mov.b32 	%f218, %r88;
	add.f32 	%f219, %f217, %f218;
	mov.b32 	%r89, %f219;
	shfl.sync.bfly.b32 	%r90|%p28, %r89, %r48, %r37, %r39;
	mov.b32 	%f220, %r90;
	add.f32 	%f221, %f219, %f220;
	mov.b32 	%r91, %f221;
	shfl.sync.bfly.b32 	%r92|%p29, %r91, %r51, %r37, %r39;
	mov.b32 	%f222, %r92;
	add.f32 	%f223, %f221, %f222;
	st.local.f32 	[%rd2+16], %f223;
	mov.b32 	%r93, %f260;
	shfl.sync.bfly.b32 	%r94|%p30, %r93, %r38, %r37, %r39;
	mov.b32 	%f224, %r94;
	add.f32 	%f225, %f260, %f224;
	mov.b32 	%r95, %f225;
	shfl.sync.bfly.b32 	%r96|%p31, %r95, %r42, %r37, %r39;
	mov.b32 	%f226, %r96;
	add.f32 	%f227, %f225, %f226;
	mov.b32 	%r97, %f227;
	shfl.sync.bfly.b32 	%r98|%p32, %r97, %r45, %r37, %r39;
	mov.b32 	%f228, %r98;
	add.f32 	%f229, %f227, %f228;
	mov.b32 	%r99, %f229;
	shfl.sync.bfly.b32 	%r100|%p33, %r99, %r48, %r37, %r39;
	mov.b32 	%f230, %r100;
	add.f32 	%f231, %f229, %f230;
	mov.b32 	%r101, %f231;
	shfl.sync.bfly.b32 	%r102|%p34, %r101, %r51, %r37, %r39;
	mov.b32 	%f232, %r102;
	add.f32 	%f233, %f231, %f232;
	st.local.f32 	[%rd2+20], %f233;
	mov.b32 	%r103, %f259;
	shfl.sync.bfly.b32 	%r104|%p35, %r103, %r38, %r37, %r39;
	mov.b32 	%f234, %r104;
	add.f32 	%f235, %f259, %f234;
	mov.b32 	%r105, %f235;
	shfl.sync.bfly.b32 	%r106|%p36, %r105, %r42, %r37, %r39;
	mov.b32 	%f236, %r106;
	add.f32 	%f237, %f235, %f236;
	mov.b32 	%r107, %f237;
	shfl.sync.bfly.b32 	%r108|%p37, %r107, %r45, %r37, %r39;
	mov.b32 	%f238, %r108;
	add.f32 	%f239, %f237, %f238;
	mov.b32 	%r109, %f239;
	shfl.sync.bfly.b32 	%r110|%p38, %r109, %r48, %r37, %r39;
	mov.b32 	%f240, %r110;
	add.f32 	%f241, %f239, %f240;
	mov.b32 	%r111, %f241;
	shfl.sync.bfly.b32 	%r112|%p39, %r111, %r51, %r37, %r39;
	mov.b32 	%f242, %r112;
	add.f32 	%f243, %f241, %f242;
	st.local.f32 	[%rd2+24], %f243;
	setp.gt.s32 	%p40, %r3, 6;
	@%p40 bra 	$L__BB6_9;

	mul.wide.s32 	%rd60, %r3, 4;
	add.s64 	%rd61, %rd2, %rd60;
	ld.local.f32 	%f244, [%rd61];
	mad.lo.s32 	%r113, %r3, %r11, %r2;
	cvt.s64.s32 	%rd62, %r113;
	mul.lo.s32 	%r114, %r1, %r12;
	cvt.s64.s32 	%rd63, %r114;
	add.s64 	%rd64, %rd63, %rd62;
	cvta.to.global.u64 	%rd65, %rd19;
	shl.b64 	%rd66, %rd64, 2;
	add.s64 	%rd67, %rd65, %rd66;
	st.global.f32 	[%rd67], %f244;

$L__BB6_9:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_8_bs_32
.visible .entry ggml_matvec_f32_ncols_8_bs_32(
	.param .u64 ggml_matvec_f32_ncols_8_bs_32_param_0,
	.param .u64 ggml_matvec_f32_ncols_8_bs_32_param_1,
	.param .u64 ggml_matvec_f32_ncols_8_bs_32_param_2,
	.param .u32 ggml_matvec_f32_ncols_8_bs_32_param_3,
	.param .u32 ggml_matvec_f32_ncols_8_bs_32_param_4,
	.param .u32 ggml_matvec_f32_ncols_8_bs_32_param_5,
	.param .u32 ggml_matvec_f32_ncols_8_bs_32_param_6,
	.param .u32 ggml_matvec_f32_ncols_8_bs_32_param_7,
	.param .u32 ggml_matvec_f32_ncols_8_bs_32_param_8,
	.param .u32 ggml_matvec_f32_ncols_8_bs_32_param_9,
	.param .u32 ggml_matvec_f32_ncols_8_bs_32_param_10,
	.param .u32 ggml_matvec_f32_ncols_8_bs_32_param_11
)
{
	.local .align 16 .b8 	__local_depot7[32];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<46>;
	.reg .f32 	%f<302>;
	.reg .b32 	%r<127>;
	.reg .b64 	%rd<74>;


	mov.u64 	%SPL, __local_depot7;
	ld.param.u64 	%rd21, [ggml_matvec_f32_ncols_8_bs_32_param_0];
	ld.param.u64 	%rd22, [ggml_matvec_f32_ncols_8_bs_32_param_1];
	ld.param.u64 	%rd20, [ggml_matvec_f32_ncols_8_bs_32_param_2];
	ld.param.u32 	%r9, [ggml_matvec_f32_ncols_8_bs_32_param_3];
	ld.param.u32 	%r13, [ggml_matvec_f32_ncols_8_bs_32_param_5];
	ld.param.u32 	%r10, [ggml_matvec_f32_ncols_8_bs_32_param_6];
	ld.param.u32 	%r11, [ggml_matvec_f32_ncols_8_bs_32_param_7];
	ld.param.u32 	%r14, [ggml_matvec_f32_ncols_8_bs_32_param_8];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_8_bs_32_param_9];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_8_bs_32_param_10];
	ld.param.u32 	%r12, [ggml_matvec_f32_ncols_8_bs_32_param_11];
	cvta.to.global.u64 	%rd73, %rd22;
	add.u64 	%rd2, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r17, %r1, %r14;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r18, %r2, %r13;
	mad.lo.s32 	%r19, %r17, %r15, %r18;
	cvt.s64.s32 	%rd3, %r19;
	cvta.to.global.u64 	%rd4, %rd21;
	mul.lo.s32 	%r20, %r1, %r16;
	cvt.s64.s32 	%rd5, %r20;
	mov.f32 	%f294, 0f00000000;
	st.local.v4.f32 	[%rd2], {%f294, %f294, %f294, %f294};
	st.local.v4.f32 	[%rd2+16], {%f294, %f294, %f294, %f294};
	mov.u32 	%r3, %tid.x;
	setp.ge.s32 	%p1, %r3, %r9;
	mov.f32 	%f295, %f294;
	mov.f32 	%f296, %f294;
	mov.f32 	%f297, %f294;
	mov.f32 	%f298, %f294;
	mov.f32 	%f299, %f294;
	mov.f32 	%f300, %f294;
	mov.f32 	%f301, %f294;
	@%p1 bra 	$L__BB7_7;

	not.b32 	%r21, %r3;
	add.s32 	%r4, %r21, %r9;
	and.b32  	%r22, %r4, 32;
	setp.ne.s32 	%p2, %r22, 0;
	mov.f32 	%f294, 0f00000000;
	mov.u32 	%r126, %r3;
	@%p2 bra 	$L__BB7_3;

	shl.b64 	%rd24, %rd5, 2;
	add.s64 	%rd25, %rd73, %rd24;
	shl.b64 	%rd26, %rd3, 2;
	add.s64 	%rd27, %rd4, %rd26;
	mul.wide.s32 	%rd28, %r3, 8;
	add.s64 	%rd29, %rd27, %rd28;
	ld.global.nc.v2.f32 	{%f57, %f58}, [%rd29];
	add.s64 	%rd30, %rd25, %rd28;
	ld.global.nc.v2.f32 	{%f61, %f62}, [%rd30];
	fma.rn.f32 	%f65, %f57, %f61, 0f00000000;
	fma.rn.f32 	%f301, %f58, %f62, %f65;
	mul.wide.s32 	%rd31, %r10, 8;
	add.s64 	%rd32, %rd30, %rd31;
	ld.global.nc.v2.f32 	{%f66, %f67}, [%rd32];
	fma.rn.f32 	%f70, %f57, %f66, 0f00000000;
	fma.rn.f32 	%f300, %f58, %f67, %f70;
	add.s32 	%r23, %r3, %r10;
	add.s32 	%r24, %r23, %r10;
	mul.wide.s32 	%rd33, %r24, 8;
	add.s64 	%rd34, %rd25, %rd33;
	ld.global.nc.v2.f32 	{%f71, %f72}, [%rd34];
	fma.rn.f32 	%f75, %f57, %f71, 0f00000000;
	fma.rn.f32 	%f299, %f58, %f72, %f75;
	add.s64 	%rd35, %rd34, %rd31;
	ld.global.nc.v2.f32 	{%f76, %f77}, [%rd35];
	fma.rn.f32 	%f80, %f57, %f76, 0f00000000;
	fma.rn.f32 	%f298, %f58, %f77, %f80;
	st.local.v4.f32 	[%rd2], {%f301, %f300, %f299, %f298};
	add.s64 	%rd36, %rd35, %rd31;
	ld.global.nc.v2.f32 	{%f81, %f82}, [%rd36];
	fma.rn.f32 	%f85, %f57, %f81, 0f00000000;
	fma.rn.f32 	%f297, %f58, %f82, %f85;
	add.s64 	%rd37, %rd36, %rd31;
	ld.global.nc.v2.f32 	{%f86, %f87}, [%rd37];
	fma.rn.f32 	%f90, %f57, %f86, 0f00000000;
	fma.rn.f32 	%f296, %f58, %f87, %f90;
	add.s64 	%rd38, %rd37, %rd31;
	ld.global.nc.v2.f32 	{%f91, %f92}, [%rd38];
	fma.rn.f32 	%f95, %f57, %f91, 0f00000000;
	fma.rn.f32 	%f295, %f58, %f92, %f95;
	add.s64 	%rd39, %rd38, %rd31;
	ld.global.nc.v2.f32 	{%f96, %f97}, [%rd39];
	fma.rn.f32 	%f100, %f57, %f96, 0f00000000;
	fma.rn.f32 	%f294, %f58, %f97, %f100;
	st.local.v4.f32 	[%rd2+16], {%f297, %f296, %f295, %f294};
	add.s32 	%r126, %r3, 32;

$L__BB7_3:
	and.b32  	%r25, %r4, -32;
	setp.eq.s32 	%p3, %r25, 0;
	@%p3 bra 	$L__BB7_7;

	add.s32 	%r26, %r126, %r10;
	add.s32 	%r27, %r26, 32;
	mul.wide.s32 	%rd40, %r27, 8;
	shl.b64 	%rd41, %rd5, 2;
	add.s64 	%rd6, %rd40, %rd41;
	shl.b32 	%r28, %r10, 1;
	add.s32 	%r29, %r126, %r28;
	mad.lo.s32 	%r30, %r10, 3, %r126;
	shl.b32 	%r31, %r10, 2;
	add.s32 	%r32, %r126, %r31;
	mad.lo.s32 	%r33, %r10, 5, %r126;
	mad.lo.s32 	%r34, %r10, 6, %r126;
	mad.lo.s32 	%r35, %r10, 7, %r126;
	mul.wide.s32 	%rd42, %r29, 8;
	add.s64 	%rd7, %rd42, %rd41;
	mul.wide.s32 	%rd43, %r30, 8;
	add.s64 	%rd8, %rd43, %rd41;
	mul.wide.s32 	%rd44, %r32, 8;
	add.s64 	%rd9, %rd44, %rd41;
	mul.wide.s32 	%rd45, %r33, 8;
	add.s64 	%rd10, %rd45, %rd41;
	mul.wide.s32 	%rd46, %r34, 8;
	add.s64 	%rd11, %rd46, %rd41;
	mul.wide.s32 	%rd47, %r35, 8;
	add.s64 	%rd12, %rd47, %rd41;
	mul.wide.s32 	%rd48, %r126, 2;
	add.s64 	%rd49, %rd48, %rd3;
	shl.b64 	%rd50, %rd49, 2;
	add.s64 	%rd51, %rd4, %rd50;
	add.s64 	%rd72, %rd51, 256;
	mul.wide.s32 	%rd52, %r126, 8;
	mul.wide.s32 	%rd53, %r10, 8;
	add.s64 	%rd54, %rd52, %rd53;
	add.s64 	%rd14, %rd54, %rd41;
	add.s64 	%rd15, %rd52, %rd41;

$L__BB7_5:
	ld.global.nc.v2.f32 	{%f101, %f102}, [%rd72+-256];
	add.s64 	%rd55, %rd73, %rd15;
	ld.global.nc.v2.f32 	{%f105, %f106}, [%rd55];
	fma.rn.f32 	%f109, %f101, %f105, %f301;
	fma.rn.f32 	%f110, %f102, %f106, %f109;
	add.s64 	%rd56, %rd73, %rd14;
	ld.global.nc.v2.f32 	{%f111, %f112}, [%rd56];
	fma.rn.f32 	%f115, %f101, %f111, %f300;
	fma.rn.f32 	%f116, %f102, %f112, %f115;
	add.s64 	%rd57, %rd73, %rd7;
	ld.global.nc.v2.f32 	{%f117, %f118}, [%rd57];
	fma.rn.f32 	%f121, %f101, %f117, %f299;
	fma.rn.f32 	%f122, %f102, %f118, %f121;
	add.s64 	%rd58, %rd73, %rd8;
	ld.global.nc.v2.f32 	{%f123, %f124}, [%rd58];
	fma.rn.f32 	%f127, %f101, %f123, %f298;
	fma.rn.f32 	%f128, %f102, %f124, %f127;
	add.s64 	%rd59, %rd73, %rd9;
	ld.global.nc.v2.f32 	{%f129, %f130}, [%rd59];
	fma.rn.f32 	%f133, %f101, %f129, %f297;
	fma.rn.f32 	%f134, %f102, %f130, %f133;
	add.s64 	%rd60, %rd73, %rd10;
	ld.global.nc.v2.f32 	{%f135, %f136}, [%rd60];
	fma.rn.f32 	%f139, %f101, %f135, %f296;
	fma.rn.f32 	%f140, %f102, %f136, %f139;
	add.s64 	%rd61, %rd73, %rd11;
	ld.global.nc.v2.f32 	{%f141, %f142}, [%rd61];
	fma.rn.f32 	%f145, %f101, %f141, %f295;
	fma.rn.f32 	%f146, %f102, %f142, %f145;
	add.s64 	%rd62, %rd73, %rd12;
	ld.global.nc.v2.f32 	{%f147, %f148}, [%rd62];
	fma.rn.f32 	%f151, %f101, %f147, %f294;
	fma.rn.f32 	%f152, %f102, %f148, %f151;
	ld.global.nc.v2.f32 	{%f153, %f154}, [%rd72];
	ld.global.nc.v2.f32 	{%f157, %f158}, [%rd55+256];
	fma.rn.f32 	%f161, %f153, %f157, %f110;
	fma.rn.f32 	%f301, %f154, %f158, %f161;
	add.s64 	%rd63, %rd73, %rd6;
	ld.global.nc.v2.f32 	{%f162, %f163}, [%rd63];
	fma.rn.f32 	%f166, %f153, %f162, %f116;
	fma.rn.f32 	%f300, %f154, %f163, %f166;
	ld.global.nc.v2.f32 	{%f167, %f168}, [%rd57+256];
	fma.rn.f32 	%f171, %f153, %f167, %f122;
	fma.rn.f32 	%f299, %f154, %f168, %f171;
	ld.global.nc.v2.f32 	{%f172, %f173}, [%rd58+256];
	fma.rn.f32 	%f176, %f153, %f172, %f128;
	fma.rn.f32 	%f298, %f154, %f173, %f176;
	ld.global.nc.v2.f32 	{%f177, %f178}, [%rd59+256];
	fma.rn.f32 	%f181, %f153, %f177, %f134;
	fma.rn.f32 	%f297, %f154, %f178, %f181;
	ld.global.nc.v2.f32 	{%f182, %f183}, [%rd60+256];
	fma.rn.f32 	%f186, %f153, %f182, %f140;
	fma.rn.f32 	%f296, %f154, %f183, %f186;
	ld.global.nc.v2.f32 	{%f187, %f188}, [%rd61+256];
	fma.rn.f32 	%f191, %f153, %f187, %f146;
	fma.rn.f32 	%f295, %f154, %f188, %f191;
	ld.global.nc.v2.f32 	{%f192, %f193}, [%rd62+256];
	fma.rn.f32 	%f196, %f153, %f192, %f152;
	fma.rn.f32 	%f294, %f154, %f193, %f196;
	add.s64 	%rd73, %rd73, 512;
	add.s64 	%rd72, %rd72, 512;
	add.s32 	%r126, %r126, 64;
	setp.lt.s32 	%p4, %r126, %r9;
	@%p4 bra 	$L__BB7_5;

	st.local.v4.f32 	[%rd2], {%f301, %f300, %f299, %f298};
	st.local.v4.f32 	[%rd2+16], {%f297, %f296, %f295, %f294};

$L__BB7_7:
	mov.b32 	%r36, %f301;
	mov.u32 	%r37, 31;
	mov.u32 	%r38, 16;
	mov.u32 	%r39, -1;
	shfl.sync.bfly.b32 	%r40|%p5, %r36, %r38, %r37, %r39;
	mov.b32 	%f197, %r40;
	add.f32 	%f198, %f301, %f197;
	mov.b32 	%r41, %f198;
	mov.u32 	%r42, 8;
	shfl.sync.bfly.b32 	%r43|%p6, %r41, %r42, %r37, %r39;
	mov.b32 	%f199, %r43;
	add.f32 	%f200, %f198, %f199;
	mov.b32 	%r44, %f200;
	mov.u32 	%r45, 4;
	shfl.sync.bfly.b32 	%r46|%p7, %r44, %r45, %r37, %r39;
	mov.b32 	%f201, %r46;
	add.f32 	%f202, %f200, %f201;
	mov.b32 	%r47, %f202;
	mov.u32 	%r48, 2;
	shfl.sync.bfly.b32 	%r49|%p8, %r47, %r48, %r37, %r39;
	mov.b32 	%f203, %r49;
	add.f32 	%f204, %f202, %f203;
	mov.b32 	%r50, %f204;
	mov.u32 	%r51, 1;
	shfl.sync.bfly.b32 	%r52|%p9, %r50, %r51, %r37, %r39;
	mov.b32 	%f205, %r52;
	add.f32 	%f206, %f204, %f205;
	st.local.f32 	[%rd2], %f206;
	mov.b32 	%r53, %f300;
	shfl.sync.bfly.b32 	%r54|%p10, %r53, %r38, %r37, %r39;
	mov.b32 	%f207, %r54;
	add.f32 	%f208, %f300, %f207;
	mov.b32 	%r55, %f208;
	shfl.sync.bfly.b32 	%r56|%p11, %r55, %r42, %r37, %r39;
	mov.b32 	%f209, %r56;
	add.f32 	%f210, %f208, %f209;
	mov.b32 	%r57, %f210;
	shfl.sync.bfly.b32 	%r58|%p12, %r57, %r45, %r37, %r39;
	mov.b32 	%f211, %r58;
	add.f32 	%f212, %f210, %f211;
	mov.b32 	%r59, %f212;
	shfl.sync.bfly.b32 	%r60|%p13, %r59, %r48, %r37, %r39;
	mov.b32 	%f213, %r60;
	add.f32 	%f214, %f212, %f213;
	mov.b32 	%r61, %f214;
	shfl.sync.bfly.b32 	%r62|%p14, %r61, %r51, %r37, %r39;
	mov.b32 	%f215, %r62;
	add.f32 	%f216, %f214, %f215;
	st.local.f32 	[%rd2+4], %f216;
	mov.b32 	%r63, %f299;
	shfl.sync.bfly.b32 	%r64|%p15, %r63, %r38, %r37, %r39;
	mov.b32 	%f217, %r64;
	add.f32 	%f218, %f299, %f217;
	mov.b32 	%r65, %f218;
	shfl.sync.bfly.b32 	%r66|%p16, %r65, %r42, %r37, %r39;
	mov.b32 	%f219, %r66;
	add.f32 	%f220, %f218, %f219;
	mov.b32 	%r67, %f220;
	shfl.sync.bfly.b32 	%r68|%p17, %r67, %r45, %r37, %r39;
	mov.b32 	%f221, %r68;
	add.f32 	%f222, %f220, %f221;
	mov.b32 	%r69, %f222;
	shfl.sync.bfly.b32 	%r70|%p18, %r69, %r48, %r37, %r39;
	mov.b32 	%f223, %r70;
	add.f32 	%f224, %f222, %f223;
	mov.b32 	%r71, %f224;
	shfl.sync.bfly.b32 	%r72|%p19, %r71, %r51, %r37, %r39;
	mov.b32 	%f225, %r72;
	add.f32 	%f226, %f224, %f225;
	st.local.f32 	[%rd2+8], %f226;
	mov.b32 	%r73, %f298;
	shfl.sync.bfly.b32 	%r74|%p20, %r73, %r38, %r37, %r39;
	mov.b32 	%f227, %r74;
	add.f32 	%f228, %f298, %f227;
	mov.b32 	%r75, %f228;
	shfl.sync.bfly.b32 	%r76|%p21, %r75, %r42, %r37, %r39;
	mov.b32 	%f229, %r76;
	add.f32 	%f230, %f228, %f229;
	mov.b32 	%r77, %f230;
	shfl.sync.bfly.b32 	%r78|%p22, %r77, %r45, %r37, %r39;
	mov.b32 	%f231, %r78;
	add.f32 	%f232, %f230, %f231;
	mov.b32 	%r79, %f232;
	shfl.sync.bfly.b32 	%r80|%p23, %r79, %r48, %r37, %r39;
	mov.b32 	%f233, %r80;
	add.f32 	%f234, %f232, %f233;
	mov.b32 	%r81, %f234;
	shfl.sync.bfly.b32 	%r82|%p24, %r81, %r51, %r37, %r39;
	mov.b32 	%f235, %r82;
	add.f32 	%f236, %f234, %f235;
	st.local.f32 	[%rd2+12], %f236;
	mov.b32 	%r83, %f297;
	shfl.sync.bfly.b32 	%r84|%p25, %r83, %r38, %r37, %r39;
	mov.b32 	%f237, %r84;
	add.f32 	%f238, %f297, %f237;
	mov.b32 	%r85, %f238;
	shfl.sync.bfly.b32 	%r86|%p26, %r85, %r42, %r37, %r39;
	mov.b32 	%f239, %r86;
	add.f32 	%f240, %f238, %f239;
	mov.b32 	%r87, %f240;
	shfl.sync.bfly.b32 	%r88|%p27, %r87, %r45, %r37, %r39;
	mov.b32 	%f241, %r88;
	add.f32 	%f242, %f240, %f241;
	mov.b32 	%r89, %f242;
	shfl.sync.bfly.b32 	%r90|%p28, %r89, %r48, %r37, %r39;
	mov.b32 	%f243, %r90;
	add.f32 	%f244, %f242, %f243;
	mov.b32 	%r91, %f244;
	shfl.sync.bfly.b32 	%r92|%p29, %r91, %r51, %r37, %r39;
	mov.b32 	%f245, %r92;
	add.f32 	%f246, %f244, %f245;
	st.local.f32 	[%rd2+16], %f246;
	mov.b32 	%r93, %f296;
	shfl.sync.bfly.b32 	%r94|%p30, %r93, %r38, %r37, %r39;
	mov.b32 	%f247, %r94;
	add.f32 	%f248, %f296, %f247;
	mov.b32 	%r95, %f248;
	shfl.sync.bfly.b32 	%r96|%p31, %r95, %r42, %r37, %r39;
	mov.b32 	%f249, %r96;
	add.f32 	%f250, %f248, %f249;
	mov.b32 	%r97, %f250;
	shfl.sync.bfly.b32 	%r98|%p32, %r97, %r45, %r37, %r39;
	mov.b32 	%f251, %r98;
	add.f32 	%f252, %f250, %f251;
	mov.b32 	%r99, %f252;
	shfl.sync.bfly.b32 	%r100|%p33, %r99, %r48, %r37, %r39;
	mov.b32 	%f253, %r100;
	add.f32 	%f254, %f252, %f253;
	mov.b32 	%r101, %f254;
	shfl.sync.bfly.b32 	%r102|%p34, %r101, %r51, %r37, %r39;
	mov.b32 	%f255, %r102;
	add.f32 	%f256, %f254, %f255;
	st.local.f32 	[%rd2+20], %f256;
	mov.b32 	%r103, %f295;
	shfl.sync.bfly.b32 	%r104|%p35, %r103, %r38, %r37, %r39;
	mov.b32 	%f257, %r104;
	add.f32 	%f258, %f295, %f257;
	mov.b32 	%r105, %f258;
	shfl.sync.bfly.b32 	%r106|%p36, %r105, %r42, %r37, %r39;
	mov.b32 	%f259, %r106;
	add.f32 	%f260, %f258, %f259;
	mov.b32 	%r107, %f260;
	shfl.sync.bfly.b32 	%r108|%p37, %r107, %r45, %r37, %r39;
	mov.b32 	%f261, %r108;
	add.f32 	%f262, %f260, %f261;
	mov.b32 	%r109, %f262;
	shfl.sync.bfly.b32 	%r110|%p38, %r109, %r48, %r37, %r39;
	mov.b32 	%f263, %r110;
	add.f32 	%f264, %f262, %f263;
	mov.b32 	%r111, %f264;
	shfl.sync.bfly.b32 	%r112|%p39, %r111, %r51, %r37, %r39;
	mov.b32 	%f265, %r112;
	add.f32 	%f266, %f264, %f265;
	st.local.f32 	[%rd2+24], %f266;
	mov.b32 	%r113, %f294;
	shfl.sync.bfly.b32 	%r114|%p40, %r113, %r38, %r37, %r39;
	mov.b32 	%f267, %r114;
	add.f32 	%f268, %f294, %f267;
	mov.b32 	%r115, %f268;
	shfl.sync.bfly.b32 	%r116|%p41, %r115, %r42, %r37, %r39;
	mov.b32 	%f269, %r116;
	add.f32 	%f270, %f268, %f269;
	mov.b32 	%r117, %f270;
	shfl.sync.bfly.b32 	%r118|%p42, %r117, %r45, %r37, %r39;
	mov.b32 	%f271, %r118;
	add.f32 	%f272, %f270, %f271;
	mov.b32 	%r119, %f272;
	shfl.sync.bfly.b32 	%r120|%p43, %r119, %r48, %r37, %r39;
	mov.b32 	%f273, %r120;
	add.f32 	%f274, %f272, %f273;
	mov.b32 	%r121, %f274;
	shfl.sync.bfly.b32 	%r122|%p44, %r121, %r51, %r37, %r39;
	mov.b32 	%f275, %r122;
	add.f32 	%f276, %f274, %f275;
	st.local.f32 	[%rd2+28], %f276;
	setp.gt.s32 	%p45, %r3, 7;
	@%p45 bra 	$L__BB7_9;

	mul.wide.s32 	%rd64, %r3, 4;
	add.s64 	%rd65, %rd2, %rd64;
	ld.local.f32 	%f277, [%rd65];
	mad.lo.s32 	%r123, %r3, %r11, %r2;
	cvt.s64.s32 	%rd66, %r123;
	mul.lo.s32 	%r124, %r1, %r12;
	cvt.s64.s32 	%rd67, %r124;
	add.s64 	%rd68, %rd67, %rd66;
	cvta.to.global.u64 	%rd69, %rd20;
	shl.b64 	%rd70, %rd68, 2;
	add.s64 	%rd71, %rd69, %rd70;
	st.global.f32 	[%rd71], %f277;

$L__BB7_9:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_1_bs_64
.visible .entry ggml_matvec_f32_ncols_1_bs_64(
	.param .u64 ggml_matvec_f32_ncols_1_bs_64_param_0,
	.param .u64 ggml_matvec_f32_ncols_1_bs_64_param_1,
	.param .u64 ggml_matvec_f32_ncols_1_bs_64_param_2,
	.param .u32 ggml_matvec_f32_ncols_1_bs_64_param_3,
	.param .u32 ggml_matvec_f32_ncols_1_bs_64_param_4,
	.param .u32 ggml_matvec_f32_ncols_1_bs_64_param_5,
	.param .u32 ggml_matvec_f32_ncols_1_bs_64_param_6,
	.param .u32 ggml_matvec_f32_ncols_1_bs_64_param_7,
	.param .u32 ggml_matvec_f32_ncols_1_bs_64_param_8,
	.param .u32 ggml_matvec_f32_ncols_1_bs_64_param_9,
	.param .u32 ggml_matvec_f32_ncols_1_bs_64_param_10,
	.param .u32 ggml_matvec_f32_ncols_1_bs_64_param_11
)
{
	.reg .pred 	%p<19>;
	.reg .f32 	%f<88>;
	.reg .b32 	%r<79>;
	.reg .b64 	%rd<42>;


	ld.param.u64 	%rd18, [ggml_matvec_f32_ncols_1_bs_64_param_0];
	ld.param.u64 	%rd19, [ggml_matvec_f32_ncols_1_bs_64_param_1];
	ld.param.u64 	%rd17, [ggml_matvec_f32_ncols_1_bs_64_param_2];
	ld.param.u32 	%r13, [ggml_matvec_f32_ncols_1_bs_64_param_3];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_1_bs_64_param_5];
	ld.param.u32 	%r14, [ggml_matvec_f32_ncols_1_bs_64_param_7];
	ld.param.u32 	%r17, [ggml_matvec_f32_ncols_1_bs_64_param_8];
	ld.param.u32 	%r18, [ggml_matvec_f32_ncols_1_bs_64_param_9];
	ld.param.u32 	%r19, [ggml_matvec_f32_ncols_1_bs_64_param_10];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_1_bs_64_param_11];
	cvta.to.global.u64 	%rd1, %rd19;
	cvta.to.global.u64 	%rd2, %rd18;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r20, %r1, %r17;
	mov.u32 	%r21, %ctaid.x;
	mul.lo.s32 	%r22, %r21, %r16;
	mad.lo.s32 	%r23, %r20, %r18, %r22;
	cvt.s64.s32 	%rd3, %r23;
	mul.lo.s32 	%r24, %r1, %r19;
	cvt.s64.s32 	%rd4, %r24;
	mov.u32 	%r2, %tid.x;
	setp.gt.s32 	%p1, %r2, 31;
	shl.b32 	%r25, %r2, 2;
	mov.u32 	%r26, data_mmv;
	add.s32 	%r3, %r26, %r25;
	@%p1 bra 	$L__BB8_2;

	mov.u32 	%r27, 0;
	st.shared.u32 	[%r3], %r27;

$L__BB8_2:
	bar.sync 	0;
	setp.ge.s32 	%p2, %r2, %r13;
	mov.f32 	%f86, 0f00000000;
	@%p2 bra 	$L__BB8_9;

	not.b32 	%r28, %r2;
	add.s32 	%r4, %r28, %r13;
	shr.u32 	%r29, %r4, 6;
	add.s32 	%r30, %r29, 1;
	and.b32  	%r76, %r30, 3;
	setp.eq.s32 	%p3, %r76, 0;
	mov.f32 	%f86, 0f00000000;
	mov.u32 	%r77, %r2;
	@%p3 bra 	$L__BB8_6;

	mul.wide.s32 	%rd20, %r2, 2;
	add.s64 	%rd21, %rd20, %rd4;
	shl.b64 	%rd22, %rd21, 2;
	add.s64 	%rd39, %rd1, %rd22;
	add.s64 	%rd23, %rd20, %rd3;
	shl.b64 	%rd24, %rd23, 2;
	add.s64 	%rd38, %rd2, %rd24;
	mov.f32 	%f86, 0f00000000;
	mov.u32 	%r77, %r2;

$L__BB8_5:
	.pragma "nounroll";
	ld.global.nc.v2.f32 	{%f15, %f16}, [%rd38];
	ld.global.nc.v2.f32 	{%f19, %f20}, [%rd39];
	fma.rn.f32 	%f23, %f15, %f19, %f86;
	fma.rn.f32 	%f86, %f16, %f20, %f23;
	add.s32 	%r77, %r77, 64;
	add.s64 	%rd39, %rd39, 512;
	add.s64 	%rd38, %rd38, 512;
	add.s32 	%r76, %r76, -1;
	setp.ne.s32 	%p4, %r76, 0;
	@%p4 bra 	$L__BB8_5;

$L__BB8_6:
	setp.lt.u32 	%p5, %r4, 192;
	@%p5 bra 	$L__BB8_9;

	mul.wide.s32 	%rd25, %r77, 2;
	add.s64 	%rd26, %rd25, %rd3;
	shl.b64 	%rd27, %rd26, 2;
	add.s64 	%rd28, %rd2, %rd27;
	add.s64 	%rd41, %rd28, 1024;
	add.s64 	%rd29, %rd25, %rd4;
	shl.b64 	%rd30, %rd29, 2;
	add.s64 	%rd31, %rd1, %rd30;
	add.s64 	%rd40, %rd31, 1024;

$L__BB8_8:
	ld.global.nc.v2.f32 	{%f24, %f25}, [%rd41+-1024];
	ld.global.nc.v2.f32 	{%f28, %f29}, [%rd40+-1024];
	fma.rn.f32 	%f32, %f24, %f28, %f86;
	fma.rn.f32 	%f33, %f25, %f29, %f32;
	ld.global.nc.v2.f32 	{%f34, %f35}, [%rd41+-512];
	ld.global.nc.v2.f32 	{%f38, %f39}, [%rd40+-512];
	fma.rn.f32 	%f42, %f34, %f38, %f33;
	fma.rn.f32 	%f43, %f35, %f39, %f42;
	ld.global.nc.v2.f32 	{%f44, %f45}, [%rd41];
	ld.global.nc.v2.f32 	{%f48, %f49}, [%rd40];
	fma.rn.f32 	%f52, %f44, %f48, %f43;
	fma.rn.f32 	%f53, %f45, %f49, %f52;
	ld.global.nc.v2.f32 	{%f54, %f55}, [%rd41+512];
	ld.global.nc.v2.f32 	{%f58, %f59}, [%rd40+512];
	fma.rn.f32 	%f62, %f54, %f58, %f53;
	fma.rn.f32 	%f86, %f55, %f59, %f62;
	add.s64 	%rd41, %rd41, 2048;
	add.s64 	%rd40, %rd40, 2048;
	add.s32 	%r77, %r77, 256;
	setp.lt.s32 	%p6, %r77, %r13;
	@%p6 bra 	$L__BB8_8;

$L__BB8_9:
	mov.b32 	%r31, %f86;
	mov.u32 	%r32, 31;
	mov.u32 	%r33, 16;
	mov.u32 	%r34, -1;
	shfl.sync.bfly.b32 	%r35|%p7, %r31, %r33, %r32, %r34;
	mov.b32 	%f63, %r35;
	add.f32 	%f64, %f86, %f63;
	mov.b32 	%r36, %f64;
	mov.u32 	%r37, 8;
	shfl.sync.bfly.b32 	%r38|%p8, %r36, %r37, %r32, %r34;
	mov.b32 	%f65, %r38;
	add.f32 	%f66, %f64, %f65;
	mov.b32 	%r39, %f66;
	mov.u32 	%r40, 4;
	shfl.sync.bfly.b32 	%r41|%p9, %r39, %r40, %r32, %r34;
	mov.b32 	%f67, %r41;
	add.f32 	%f68, %f66, %f67;
	mov.b32 	%r42, %f68;
	mov.u32 	%r43, 2;
	shfl.sync.bfly.b32 	%r44|%p10, %r42, %r43, %r32, %r34;
	mov.b32 	%f69, %r44;
	add.f32 	%f70, %f68, %f69;
	mov.b32 	%r45, %f70;
	mov.u32 	%r46, 1;
	shfl.sync.bfly.b32 	%r47|%p11, %r45, %r46, %r32, %r34;
	mov.b32 	%f71, %r47;
	add.f32 	%f87, %f70, %f71;
	shr.s32 	%r48, %r2, 31;
	shr.u32 	%r49, %r48, 27;
	add.s32 	%r50, %r2, %r49;
	shr.s32 	%r51, %r50, 5;
	shl.b32 	%r52, %r51, 2;
	add.s32 	%r54, %r26, %r52;
	st.shared.f32 	[%r54], %f87;
	bar.sync 	0;
	@%p1 bra 	$L__BB8_11;

	ld.shared.f32 	%f72, [%r3];
	mov.b32 	%r55, %f72;
	shfl.sync.bfly.b32 	%r59|%p13, %r55, %r33, %r32, %r34;
	mov.b32 	%f73, %r59;
	add.f32 	%f74, %f72, %f73;
	mov.b32 	%r60, %f74;
	shfl.sync.bfly.b32 	%r62|%p14, %r60, %r37, %r32, %r34;
	mov.b32 	%f75, %r62;
	add.f32 	%f76, %f74, %f75;
	mov.b32 	%r63, %f76;
	shfl.sync.bfly.b32 	%r65|%p15, %r63, %r40, %r32, %r34;
	mov.b32 	%f77, %r65;
	add.f32 	%f78, %f76, %f77;
	mov.b32 	%r66, %f78;
	shfl.sync.bfly.b32 	%r68|%p16, %r66, %r43, %r32, %r34;
	mov.b32 	%f79, %r68;
	add.f32 	%f80, %f78, %f79;
	mov.b32 	%r69, %f80;
	shfl.sync.bfly.b32 	%r71|%p17, %r69, %r46, %r32, %r34;
	mov.b32 	%f81, %r71;
	add.f32 	%f87, %f80, %f81;

$L__BB8_11:
	bar.sync 	0;
	setp.gt.s32 	%p18, %r2, 0;
	@%p18 bra 	$L__BB8_13;

	mad.lo.s32 	%r73, %r2, %r14, %r21;
	cvt.s64.s32 	%rd32, %r73;
	mul.lo.s32 	%r74, %r1, %r15;
	cvt.s64.s32 	%rd33, %r74;
	add.s64 	%rd34, %rd33, %rd32;
	cvta.to.global.u64 	%rd35, %rd17;
	shl.b64 	%rd36, %rd34, 2;
	add.s64 	%rd37, %rd35, %rd36;
	st.global.f32 	[%rd37], %f87;

$L__BB8_13:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_2_bs_64
.visible .entry ggml_matvec_f32_ncols_2_bs_64(
	.param .u64 ggml_matvec_f32_ncols_2_bs_64_param_0,
	.param .u64 ggml_matvec_f32_ncols_2_bs_64_param_1,
	.param .u64 ggml_matvec_f32_ncols_2_bs_64_param_2,
	.param .u32 ggml_matvec_f32_ncols_2_bs_64_param_3,
	.param .u32 ggml_matvec_f32_ncols_2_bs_64_param_4,
	.param .u32 ggml_matvec_f32_ncols_2_bs_64_param_5,
	.param .u32 ggml_matvec_f32_ncols_2_bs_64_param_6,
	.param .u32 ggml_matvec_f32_ncols_2_bs_64_param_7,
	.param .u32 ggml_matvec_f32_ncols_2_bs_64_param_8,
	.param .u32 ggml_matvec_f32_ncols_2_bs_64_param_9,
	.param .u32 ggml_matvec_f32_ncols_2_bs_64_param_10,
	.param .u32 ggml_matvec_f32_ncols_2_bs_64_param_11
)
{
	.local .align 8 .b8 	__local_depot9[8];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<30>;
	.reg .f32 	%f<146>;
	.reg .b32 	%r<113>;
	.reg .b64 	%rd<64>;


	mov.u64 	%SPL, __local_depot9;
	ld.param.u64 	%rd27, [ggml_matvec_f32_ncols_2_bs_64_param_0];
	ld.param.u64 	%rd28, [ggml_matvec_f32_ncols_2_bs_64_param_1];
	ld.param.u64 	%rd26, [ggml_matvec_f32_ncols_2_bs_64_param_2];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_2_bs_64_param_3];
	ld.param.u32 	%r19, [ggml_matvec_f32_ncols_2_bs_64_param_5];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_2_bs_64_param_6];
	ld.param.u32 	%r17, [ggml_matvec_f32_ncols_2_bs_64_param_7];
	ld.param.u32 	%r20, [ggml_matvec_f32_ncols_2_bs_64_param_8];
	ld.param.u32 	%r21, [ggml_matvec_f32_ncols_2_bs_64_param_9];
	ld.param.u32 	%r22, [ggml_matvec_f32_ncols_2_bs_64_param_10];
	ld.param.u32 	%r18, [ggml_matvec_f32_ncols_2_bs_64_param_11];
	cvta.to.global.u64 	%rd1, %rd28;
	cvta.to.global.u64 	%rd2, %rd27;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r23, %r1, %r20;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r24, %r2, %r19;
	mad.lo.s32 	%r25, %r23, %r21, %r24;
	cvt.s64.s32 	%rd4, %r25;
	mul.lo.s32 	%r26, %r1, %r22;
	cvt.s64.s32 	%rd5, %r26;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r27, %r3, 2;
	mov.u32 	%r28, data_mmv;
	add.s32 	%r4, %r28, %r27;
	@%p1 bra 	$L__BB9_2;

	mov.u32 	%r29, 0;
	st.shared.u32 	[%r4], %r29;

$L__BB9_2:
	bar.sync 	0;
	mov.f32 	%f144, 0f00000000;
	st.local.v2.f32 	[%rd3], {%f144, %f144};
	setp.ge.s32 	%p2, %r3, %r15;
	mov.f32 	%f145, %f144;
	@%p2 bra 	$L__BB9_11;

	not.b32 	%r30, %r3;
	add.s32 	%r5, %r30, %r15;
	shr.u32 	%r31, %r5, 6;
	add.s32 	%r32, %r31, 1;
	and.b32  	%r110, %r32, 3;
	setp.eq.s32 	%p3, %r110, 0;
	mov.f32 	%f144, 0f00000000;
	mov.u32 	%r111, %r3;
	@%p3 bra 	$L__BB9_7;

	mul.wide.s32 	%rd30, %r16, 2;
	mul.wide.s32 	%rd31, %r3, 2;
	add.s64 	%rd32, %rd30, %rd31;
	add.s64 	%rd33, %rd32, %rd5;
	shl.b64 	%rd34, %rd33, 2;
	add.s64 	%rd60, %rd1, %rd34;
	add.s64 	%rd35, %rd31, %rd5;
	shl.b64 	%rd36, %rd35, 2;
	add.s64 	%rd59, %rd1, %rd36;
	add.s64 	%rd37, %rd31, %rd4;
	shl.b64 	%rd38, %rd37, 2;
	add.s64 	%rd58, %rd2, %rd38;
	mov.f32 	%f144, 0f00000000;
	mov.f32 	%f145, %f144;
	mov.u32 	%r111, %r3;

$L__BB9_5:
	.pragma "nounroll";
	ld.global.nc.v2.f32 	{%f19, %f20}, [%rd58];
	ld.global.nc.v2.f32 	{%f23, %f24}, [%rd59];
	fma.rn.f32 	%f27, %f19, %f23, %f145;
	fma.rn.f32 	%f145, %f20, %f24, %f27;
	ld.global.nc.v2.f32 	{%f28, %f29}, [%rd60];
	fma.rn.f32 	%f32, %f19, %f28, %f144;
	fma.rn.f32 	%f144, %f20, %f29, %f32;
	add.s32 	%r111, %r111, 64;
	add.s64 	%rd60, %rd60, 512;
	add.s64 	%rd59, %rd59, 512;
	add.s64 	%rd58, %rd58, 512;
	add.s32 	%r110, %r110, -1;
	setp.ne.s32 	%p4, %r110, 0;
	@%p4 bra 	$L__BB9_5;

	st.local.v2.f32 	[%rd3], {%f145, %f144};

$L__BB9_7:
	setp.lt.u32 	%p5, %r5, 192;
	@%p5 bra 	$L__BB9_11;

	mul.wide.s32 	%rd39, %r111, 2;
	add.s64 	%rd40, %rd39, %rd4;
	shl.b64 	%rd41, %rd40, 2;
	add.s64 	%rd42, %rd2, %rd41;
	add.s64 	%rd63, %rd42, 1024;
	add.s64 	%rd43, %rd39, %rd5;
	shl.b64 	%rd44, %rd43, 2;
	add.s64 	%rd45, %rd1, %rd44;
	add.s64 	%rd62, %rd45, 1536;
	mul.wide.s32 	%rd46, %r16, 2;
	add.s64 	%rd47, %rd43, %rd46;
	shl.b64 	%rd48, %rd47, 2;
	add.s64 	%rd49, %rd1, %rd48;
	add.s64 	%rd61, %rd49, 1024;

$L__BB9_9:
	ld.global.nc.v2.f32 	{%f33, %f34}, [%rd63+-1024];
	ld.global.nc.v2.f32 	{%f37, %f38}, [%rd62+-1536];
	fma.rn.f32 	%f41, %f33, %f37, %f145;
	fma.rn.f32 	%f42, %f34, %f38, %f41;
	ld.global.nc.v2.f32 	{%f43, %f44}, [%rd61+-1024];
	fma.rn.f32 	%f47, %f33, %f43, %f144;
	fma.rn.f32 	%f48, %f34, %f44, %f47;
	ld.global.nc.v2.f32 	{%f49, %f50}, [%rd63+-512];
	ld.global.nc.v2.f32 	{%f53, %f54}, [%rd62+-1024];
	fma.rn.f32 	%f57, %f49, %f53, %f42;
	fma.rn.f32 	%f58, %f50, %f54, %f57;
	ld.global.nc.v2.f32 	{%f59, %f60}, [%rd61+-512];
	fma.rn.f32 	%f63, %f49, %f59, %f48;
	fma.rn.f32 	%f64, %f50, %f60, %f63;
	ld.global.nc.v2.f32 	{%f65, %f66}, [%rd63];
	ld.global.nc.v2.f32 	{%f69, %f70}, [%rd62+-512];
	fma.rn.f32 	%f73, %f65, %f69, %f58;
	fma.rn.f32 	%f74, %f66, %f70, %f73;
	ld.global.nc.v2.f32 	{%f75, %f76}, [%rd61];
	fma.rn.f32 	%f79, %f65, %f75, %f64;
	fma.rn.f32 	%f80, %f66, %f76, %f79;
	ld.global.nc.v2.f32 	{%f81, %f82}, [%rd63+512];
	ld.global.nc.v2.f32 	{%f85, %f86}, [%rd62];
	fma.rn.f32 	%f89, %f81, %f85, %f74;
	fma.rn.f32 	%f145, %f82, %f86, %f89;
	ld.global.nc.v2.f32 	{%f90, %f91}, [%rd61+512];
	fma.rn.f32 	%f94, %f81, %f90, %f80;
	fma.rn.f32 	%f144, %f82, %f91, %f94;
	add.s64 	%rd63, %rd63, 2048;
	add.s64 	%rd62, %rd62, 2048;
	add.s64 	%rd61, %rd61, 2048;
	add.s32 	%r111, %r111, 256;
	setp.lt.s32 	%p6, %r111, %r15;
	@%p6 bra 	$L__BB9_9;

	st.local.v2.f32 	[%rd3], {%f145, %f144};

$L__BB9_11:
	shr.s32 	%r33, %r3, 31;
	shr.u32 	%r34, %r33, 27;
	add.s32 	%r35, %r3, %r34;
	shr.s32 	%r36, %r35, 5;
	shl.b32 	%r37, %r36, 2;
	add.s32 	%r14, %r28, %r37;
	mov.u32 	%r39, 2;
	mov.b32 	%r40, %f145;
	mov.u32 	%r41, 31;
	mov.u32 	%r42, 16;
	mov.u32 	%r43, -1;
	shfl.sync.bfly.b32 	%r44|%p7, %r40, %r42, %r41, %r43;
	mov.b32 	%f95, %r44;
	add.f32 	%f96, %f145, %f95;
	mov.b32 	%r45, %f96;
	mov.u32 	%r46, 8;
	shfl.sync.bfly.b32 	%r47|%p8, %r45, %r46, %r41, %r43;
	mov.b32 	%f97, %r47;
	add.f32 	%f98, %f96, %f97;
	mov.b32 	%r48, %f98;
	mov.u32 	%r49, 4;
	shfl.sync.bfly.b32 	%r50|%p9, %r48, %r49, %r41, %r43;
	mov.b32 	%f99, %r50;
	add.f32 	%f100, %f98, %f99;
	mov.b32 	%r51, %f100;
	shfl.sync.bfly.b32 	%r52|%p10, %r51, %r39, %r41, %r43;
	mov.b32 	%f101, %r52;
	add.f32 	%f102, %f100, %f101;
	mov.b32 	%r53, %f102;
	mov.u32 	%r54, 1;
	shfl.sync.bfly.b32 	%r55|%p11, %r53, %r54, %r41, %r43;
	mov.b32 	%f103, %r55;
	add.f32 	%f104, %f102, %f103;
	st.local.f32 	[%rd3], %f104;
	st.shared.f32 	[%r14], %f104;
	bar.sync 	0;
	@%p1 bra 	$L__BB9_13;

	ld.shared.f32 	%f105, [%r4];
	mov.b32 	%r56, %f105;
	shfl.sync.bfly.b32 	%r60|%p13, %r56, %r42, %r41, %r43;
	mov.b32 	%f106, %r60;
	add.f32 	%f107, %f105, %f106;
	mov.b32 	%r61, %f107;
	shfl.sync.bfly.b32 	%r63|%p14, %r61, %r46, %r41, %r43;
	mov.b32 	%f108, %r63;
	add.f32 	%f109, %f107, %f108;
	mov.b32 	%r64, %f109;
	shfl.sync.bfly.b32 	%r66|%p15, %r64, %r49, %r41, %r43;
	mov.b32 	%f110, %r66;
	add.f32 	%f111, %f109, %f110;
	mov.b32 	%r67, %f111;
	shfl.sync.bfly.b32 	%r69|%p16, %r67, %r39, %r41, %r43;
	mov.b32 	%f112, %r69;
	add.f32 	%f113, %f111, %f112;
	mov.b32 	%r70, %f113;
	shfl.sync.bfly.b32 	%r72|%p17, %r70, %r54, %r41, %r43;
	mov.b32 	%f114, %r72;
	add.f32 	%f115, %f113, %f114;
	st.local.f32 	[%rd3], %f115;

$L__BB9_13:
	bar.sync 	0;
	mov.b32 	%r73, %f144;
	shfl.sync.bfly.b32 	%r77|%p19, %r73, %r42, %r41, %r43;
	mov.b32 	%f116, %r77;
	add.f32 	%f117, %f144, %f116;
	mov.b32 	%r78, %f117;
	shfl.sync.bfly.b32 	%r80|%p20, %r78, %r46, %r41, %r43;
	mov.b32 	%f118, %r80;
	add.f32 	%f119, %f117, %f118;
	mov.b32 	%r81, %f119;
	shfl.sync.bfly.b32 	%r83|%p21, %r81, %r49, %r41, %r43;
	mov.b32 	%f120, %r83;
	add.f32 	%f121, %f119, %f120;
	mov.b32 	%r84, %f121;
	shfl.sync.bfly.b32 	%r86|%p22, %r84, %r39, %r41, %r43;
	mov.b32 	%f122, %r86;
	add.f32 	%f123, %f121, %f122;
	mov.b32 	%r87, %f123;
	shfl.sync.bfly.b32 	%r89|%p23, %r87, %r54, %r41, %r43;
	mov.b32 	%f124, %r89;
	add.f32 	%f125, %f123, %f124;
	st.local.f32 	[%rd3+4], %f125;
	st.shared.f32 	[%r14], %f125;
	bar.sync 	0;
	@%p1 bra 	$L__BB9_15;

	ld.shared.f32 	%f126, [%r4];
	mov.b32 	%r90, %f126;
	mov.u32 	%r91, 31;
	mov.u32 	%r92, 16;
	mov.u32 	%r93, -1;
	shfl.sync.bfly.b32 	%r94|%p24, %r90, %r92, %r91, %r93;
	mov.b32 	%f127, %r94;
	add.f32 	%f128, %f126, %f127;
	mov.b32 	%r95, %f128;
	mov.u32 	%r96, 8;
	shfl.sync.bfly.b32 	%r97|%p25, %r95, %r96, %r91, %r93;
	mov.b32 	%f129, %r97;
	add.f32 	%f130, %f128, %f129;
	mov.b32 	%r98, %f130;
	mov.u32 	%r99, 4;
	shfl.sync.bfly.b32 	%r100|%p26, %r98, %r99, %r91, %r93;
	mov.b32 	%f131, %r100;
	add.f32 	%f132, %f130, %f131;
	mov.b32 	%r101, %f132;
	mov.u32 	%r102, 2;
	shfl.sync.bfly.b32 	%r103|%p27, %r101, %r102, %r91, %r93;
	mov.b32 	%f133, %r103;
	add.f32 	%f134, %f132, %f133;
	mov.b32 	%r104, %f134;
	mov.u32 	%r105, 1;
	shfl.sync.bfly.b32 	%r106|%p28, %r104, %r105, %r91, %r93;
	mov.b32 	%f135, %r106;
	add.f32 	%f136, %f134, %f135;
	st.local.f32 	[%rd3+4], %f136;

$L__BB9_15:
	bar.sync 	0;
	setp.gt.s32 	%p29, %r3, 1;
	@%p29 bra 	$L__BB9_17;

	mul.wide.s32 	%rd50, %r3, 4;
	add.s64 	%rd51, %rd3, %rd50;
	ld.local.f32 	%f137, [%rd51];
	mad.lo.s32 	%r107, %r3, %r17, %r2;
	cvt.s64.s32 	%rd52, %r107;
	mul.lo.s32 	%r108, %r1, %r18;
	cvt.s64.s32 	%rd53, %r108;
	add.s64 	%rd54, %rd53, %rd52;
	cvta.to.global.u64 	%rd55, %rd26;
	shl.b64 	%rd56, %rd54, 2;
	add.s64 	%rd57, %rd55, %rd56;
	st.global.f32 	[%rd57], %f137;

$L__BB9_17:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_3_bs_64
.visible .entry ggml_matvec_f32_ncols_3_bs_64(
	.param .u64 ggml_matvec_f32_ncols_3_bs_64_param_0,
	.param .u64 ggml_matvec_f32_ncols_3_bs_64_param_1,
	.param .u64 ggml_matvec_f32_ncols_3_bs_64_param_2,
	.param .u32 ggml_matvec_f32_ncols_3_bs_64_param_3,
	.param .u32 ggml_matvec_f32_ncols_3_bs_64_param_4,
	.param .u32 ggml_matvec_f32_ncols_3_bs_64_param_5,
	.param .u32 ggml_matvec_f32_ncols_3_bs_64_param_6,
	.param .u32 ggml_matvec_f32_ncols_3_bs_64_param_7,
	.param .u32 ggml_matvec_f32_ncols_3_bs_64_param_8,
	.param .u32 ggml_matvec_f32_ncols_3_bs_64_param_9,
	.param .u32 ggml_matvec_f32_ncols_3_bs_64_param_10,
	.param .u32 ggml_matvec_f32_ncols_3_bs_64_param_11
)
{
	.local .align 4 .b8 	__local_depot10[12];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<41>;
	.reg .f32 	%f<208>;
	.reg .b32 	%r<154>;
	.reg .b64 	%rd<72>;


	mov.u64 	%SPL, __local_depot10;
	ld.param.u64 	%rd29, [ggml_matvec_f32_ncols_3_bs_64_param_0];
	ld.param.u64 	%rd30, [ggml_matvec_f32_ncols_3_bs_64_param_1];
	ld.param.u64 	%rd28, [ggml_matvec_f32_ncols_3_bs_64_param_2];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_3_bs_64_param_3];
	ld.param.u32 	%r19, [ggml_matvec_f32_ncols_3_bs_64_param_5];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_3_bs_64_param_6];
	ld.param.u32 	%r17, [ggml_matvec_f32_ncols_3_bs_64_param_7];
	ld.param.u32 	%r20, [ggml_matvec_f32_ncols_3_bs_64_param_8];
	ld.param.u32 	%r21, [ggml_matvec_f32_ncols_3_bs_64_param_9];
	ld.param.u32 	%r22, [ggml_matvec_f32_ncols_3_bs_64_param_10];
	ld.param.u32 	%r18, [ggml_matvec_f32_ncols_3_bs_64_param_11];
	cvta.to.global.u64 	%rd71, %rd30;
	cvta.to.global.u64 	%rd2, %rd29;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r23, %r1, %r20;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r24, %r2, %r19;
	mad.lo.s32 	%r25, %r23, %r21, %r24;
	cvt.s64.s32 	%rd4, %r25;
	mul.lo.s32 	%r26, %r1, %r22;
	cvt.s64.s32 	%rd5, %r26;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r27, %r3, 2;
	mov.u32 	%r28, data_mmv;
	add.s32 	%r4, %r28, %r27;
	@%p1 bra 	$L__BB10_2;

	mov.u32 	%r29, 0;
	st.shared.u32 	[%r4], %r29;

$L__BB10_2:
	bar.sync 	0;
	mov.f32 	%f205, 0f00000000;
	mov.u32 	%r30, 0;
	st.local.u32 	[%rd3], %r30;
	st.local.u32 	[%rd3+4], %r30;
	st.local.u32 	[%rd3+8], %r30;
	setp.ge.s32 	%p2, %r3, %r15;
	mov.f32 	%f206, %f205;
	mov.f32 	%f207, %f205;
	@%p2 bra 	$L__BB10_11;

	not.b32 	%r31, %r3;
	add.s32 	%r5, %r31, %r15;
	shr.u32 	%r32, %r5, 6;
	add.s32 	%r33, %r32, 1;
	and.b32  	%r151, %r33, 3;
	setp.eq.s32 	%p3, %r151, 0;
	mov.f32 	%f205, 0f00000000;
	mov.u32 	%r152, %r3;
	@%p3 bra 	$L__BB10_7;

	shl.b32 	%r34, %r16, 1;
	add.s32 	%r35, %r3, %r34;
	mul.wide.s32 	%rd32, %r35, 2;
	add.s64 	%rd33, %rd32, %rd5;
	shl.b64 	%rd34, %rd33, 2;
	add.s64 	%rd69, %rd71, %rd34;
	mul.wide.s32 	%rd35, %r16, 2;
	mul.wide.s32 	%rd36, %r3, 2;
	add.s64 	%rd37, %rd35, %rd36;
	add.s64 	%rd38, %rd37, %rd5;
	shl.b64 	%rd39, %rd38, 2;
	add.s64 	%rd68, %rd71, %rd39;
	add.s64 	%rd40, %rd36, %rd5;
	shl.b64 	%rd41, %rd40, 2;
	add.s64 	%rd67, %rd71, %rd41;
	add.s64 	%rd42, %rd36, %rd4;
	shl.b64 	%rd43, %rd42, 2;
	add.s64 	%rd66, %rd2, %rd43;
	mov.f32 	%f205, 0f00000000;
	mov.f32 	%f206, %f205;
	mov.f32 	%f207, %f205;
	mov.u32 	%r152, %r3;

$L__BB10_5:
	.pragma "nounroll";
	ld.global.nc.v2.f32 	{%f28, %f29}, [%rd66];
	ld.global.nc.v2.f32 	{%f32, %f33}, [%rd67];
	fma.rn.f32 	%f36, %f28, %f32, %f207;
	fma.rn.f32 	%f207, %f29, %f33, %f36;
	ld.global.nc.v2.f32 	{%f37, %f38}, [%rd68];
	fma.rn.f32 	%f41, %f28, %f37, %f206;
	fma.rn.f32 	%f206, %f29, %f38, %f41;
	ld.global.nc.v2.f32 	{%f42, %f43}, [%rd69];
	fma.rn.f32 	%f46, %f28, %f42, %f205;
	fma.rn.f32 	%f205, %f29, %f43, %f46;
	add.s32 	%r152, %r152, 64;
	add.s64 	%rd69, %rd69, 512;
	add.s64 	%rd68, %rd68, 512;
	add.s64 	%rd67, %rd67, 512;
	add.s64 	%rd66, %rd66, 512;
	add.s32 	%r151, %r151, -1;
	setp.ne.s32 	%p4, %r151, 0;
	@%p4 bra 	$L__BB10_5;

	st.local.f32 	[%rd3], %f207;
	st.local.f32 	[%rd3+4], %f206;
	st.local.f32 	[%rd3+8], %f205;

$L__BB10_7:
	setp.lt.u32 	%p5, %r5, 192;
	@%p5 bra 	$L__BB10_11;

	add.s32 	%r36, %r152, %r16;
	shl.b32 	%r37, %r16, 1;
	add.s32 	%r38, %r152, %r37;
	add.s32 	%r39, %r36, 64;
	mul.wide.s32 	%rd44, %r39, 8;
	shl.b64 	%rd45, %rd5, 2;
	add.s64 	%rd19, %rd44, %rd45;
	mul.wide.s32 	%rd46, %r38, 8;
	add.s64 	%rd20, %rd46, %rd45;
	mul.wide.s32 	%rd47, %r152, 2;
	add.s64 	%rd48, %rd47, %rd4;
	shl.b64 	%rd49, %rd48, 2;
	add.s64 	%rd50, %rd2, %rd49;
	add.s64 	%rd70, %rd50, 1024;
	mul.wide.s32 	%rd51, %r152, 8;
	add.s64 	%rd22, %rd51, %rd45;
	mul.wide.s32 	%rd52, %r16, 8;
	add.s64 	%rd53, %rd51, %rd52;
	add.s64 	%rd23, %rd53, %rd45;

$L__BB10_9:
	ld.global.nc.v2.f32 	{%f47, %f48}, [%rd70+-1024];
	add.s64 	%rd54, %rd71, %rd22;
	ld.global.nc.v2.f32 	{%f51, %f52}, [%rd54];
	fma.rn.f32 	%f55, %f47, %f51, %f207;
	fma.rn.f32 	%f56, %f48, %f52, %f55;
	add.s64 	%rd55, %rd71, %rd23;
	ld.global.nc.v2.f32 	{%f57, %f58}, [%rd55];
	fma.rn.f32 	%f61, %f47, %f57, %f206;
	fma.rn.f32 	%f62, %f48, %f58, %f61;
	add.s64 	%rd56, %rd71, %rd20;
	ld.global.nc.v2.f32 	{%f63, %f64}, [%rd56];
	fma.rn.f32 	%f67, %f47, %f63, %f205;
	fma.rn.f32 	%f68, %f48, %f64, %f67;
	ld.global.nc.v2.f32 	{%f69, %f70}, [%rd70+-512];
	ld.global.nc.v2.f32 	{%f73, %f74}, [%rd54+512];
	fma.rn.f32 	%f77, %f69, %f73, %f56;
	fma.rn.f32 	%f78, %f70, %f74, %f77;
	add.s64 	%rd57, %rd71, %rd19;
	ld.global.nc.v2.f32 	{%f79, %f80}, [%rd57];
	fma.rn.f32 	%f83, %f69, %f79, %f62;
	fma.rn.f32 	%f84, %f70, %f80, %f83;
	ld.global.nc.v2.f32 	{%f85, %f86}, [%rd56+512];
	fma.rn.f32 	%f89, %f69, %f85, %f68;
	fma.rn.f32 	%f90, %f70, %f86, %f89;
	ld.global.nc.v2.f32 	{%f91, %f92}, [%rd70];
	ld.global.nc.v2.f32 	{%f95, %f96}, [%rd54+1024];
	fma.rn.f32 	%f99, %f91, %f95, %f78;
	fma.rn.f32 	%f100, %f92, %f96, %f99;
	ld.global.nc.v2.f32 	{%f101, %f102}, [%rd57+512];
	fma.rn.f32 	%f105, %f91, %f101, %f84;
	fma.rn.f32 	%f106, %f92, %f102, %f105;
	ld.global.nc.v2.f32 	{%f107, %f108}, [%rd56+1024];
	fma.rn.f32 	%f111, %f91, %f107, %f90;
	fma.rn.f32 	%f112, %f92, %f108, %f111;
	ld.global.nc.v2.f32 	{%f113, %f114}, [%rd70+512];
	ld.global.nc.v2.f32 	{%f117, %f118}, [%rd54+1536];
	fma.rn.f32 	%f121, %f113, %f117, %f100;
	fma.rn.f32 	%f207, %f114, %f118, %f121;
	ld.global.nc.v2.f32 	{%f122, %f123}, [%rd57+1024];
	fma.rn.f32 	%f126, %f113, %f122, %f106;
	fma.rn.f32 	%f206, %f114, %f123, %f126;
	ld.global.nc.v2.f32 	{%f127, %f128}, [%rd56+1536];
	fma.rn.f32 	%f131, %f113, %f127, %f112;
	fma.rn.f32 	%f205, %f114, %f128, %f131;
	add.s64 	%rd71, %rd71, 2048;
	add.s64 	%rd70, %rd70, 2048;
	add.s32 	%r152, %r152, 256;
	setp.lt.s32 	%p6, %r152, %r15;
	@%p6 bra 	$L__BB10_9;

	st.local.f32 	[%rd3], %f207;
	st.local.f32 	[%rd3+4], %f206;
	st.local.f32 	[%rd3+8], %f205;

$L__BB10_11:
	shr.s32 	%r40, %r3, 31;
	shr.u32 	%r41, %r40, 27;
	add.s32 	%r42, %r3, %r41;
	shr.s32 	%r43, %r42, 5;
	shl.b32 	%r44, %r43, 2;
	add.s32 	%r14, %r28, %r44;
	mov.u32 	%r46, 2;
	mov.b32 	%r47, %f207;
	mov.u32 	%r48, 31;
	mov.u32 	%r49, 16;
	mov.u32 	%r50, -1;
	shfl.sync.bfly.b32 	%r51|%p7, %r47, %r49, %r48, %r50;
	mov.b32 	%f132, %r51;
	add.f32 	%f133, %f207, %f132;
	mov.b32 	%r52, %f133;
	mov.u32 	%r53, 8;
	shfl.sync.bfly.b32 	%r54|%p8, %r52, %r53, %r48, %r50;
	mov.b32 	%f134, %r54;
	add.f32 	%f135, %f133, %f134;
	mov.b32 	%r55, %f135;
	mov.u32 	%r56, 4;
	shfl.sync.bfly.b32 	%r57|%p9, %r55, %r56, %r48, %r50;
	mov.b32 	%f136, %r57;
	add.f32 	%f137, %f135, %f136;
	mov.b32 	%r58, %f137;
	shfl.sync.bfly.b32 	%r59|%p10, %r58, %r46, %r48, %r50;
	mov.b32 	%f138, %r59;
	add.f32 	%f139, %f137, %f138;
	mov.b32 	%r60, %f139;
	mov.u32 	%r61, 1;
	shfl.sync.bfly.b32 	%r62|%p11, %r60, %r61, %r48, %r50;
	mov.b32 	%f140, %r62;
	add.f32 	%f141, %f139, %f140;
	st.local.f32 	[%rd3], %f141;
	st.shared.f32 	[%r14], %f141;
	bar.sync 	0;
	@%p1 bra 	$L__BB10_13;

	ld.shared.f32 	%f142, [%r4];
	mov.b32 	%r63, %f142;
	shfl.sync.bfly.b32 	%r67|%p13, %r63, %r49, %r48, %r50;
	mov.b32 	%f143, %r67;
	add.f32 	%f144, %f142, %f143;
	mov.b32 	%r68, %f144;
	shfl.sync.bfly.b32 	%r70|%p14, %r68, %r53, %r48, %r50;
	mov.b32 	%f145, %r70;
	add.f32 	%f146, %f144, %f145;
	mov.b32 	%r71, %f146;
	shfl.sync.bfly.b32 	%r73|%p15, %r71, %r56, %r48, %r50;
	mov.b32 	%f147, %r73;
	add.f32 	%f148, %f146, %f147;
	mov.b32 	%r74, %f148;
	shfl.sync.bfly.b32 	%r76|%p16, %r74, %r46, %r48, %r50;
	mov.b32 	%f149, %r76;
	add.f32 	%f150, %f148, %f149;
	mov.b32 	%r77, %f150;
	shfl.sync.bfly.b32 	%r79|%p17, %r77, %r61, %r48, %r50;
	mov.b32 	%f151, %r79;
	add.f32 	%f152, %f150, %f151;
	st.local.f32 	[%rd3], %f152;

$L__BB10_13:
	bar.sync 	0;
	mov.b32 	%r80, %f206;
	shfl.sync.bfly.b32 	%r84|%p19, %r80, %r49, %r48, %r50;
	mov.b32 	%f153, %r84;
	add.f32 	%f154, %f206, %f153;
	mov.b32 	%r85, %f154;
	shfl.sync.bfly.b32 	%r87|%p20, %r85, %r53, %r48, %r50;
	mov.b32 	%f155, %r87;
	add.f32 	%f156, %f154, %f155;
	mov.b32 	%r88, %f156;
	shfl.sync.bfly.b32 	%r90|%p21, %r88, %r56, %r48, %r50;
	mov.b32 	%f157, %r90;
	add.f32 	%f158, %f156, %f157;
	mov.b32 	%r91, %f158;
	shfl.sync.bfly.b32 	%r93|%p22, %r91, %r46, %r48, %r50;
	mov.b32 	%f159, %r93;
	add.f32 	%f160, %f158, %f159;
	mov.b32 	%r94, %f160;
	shfl.sync.bfly.b32 	%r96|%p23, %r94, %r61, %r48, %r50;
	mov.b32 	%f161, %r96;
	add.f32 	%f162, %f160, %f161;
	st.local.f32 	[%rd3+4], %f162;
	st.shared.f32 	[%r14], %f162;
	bar.sync 	0;
	@%p1 bra 	$L__BB10_15;

	ld.shared.f32 	%f163, [%r4];
	mov.b32 	%r97, %f163;
	mov.u32 	%r98, 31;
	mov.u32 	%r99, 16;
	mov.u32 	%r100, -1;
	shfl.sync.bfly.b32 	%r101|%p24, %r97, %r99, %r98, %r100;
	mov.b32 	%f164, %r101;
	add.f32 	%f165, %f163, %f164;
	mov.b32 	%r102, %f165;
	mov.u32 	%r103, 8;
	shfl.sync.bfly.b32 	%r104|%p25, %r102, %r103, %r98, %r100;
	mov.b32 	%f166, %r104;
	add.f32 	%f167, %f165, %f166;
	mov.b32 	%r105, %f167;
	mov.u32 	%r106, 4;
	shfl.sync.bfly.b32 	%r107|%p26, %r105, %r106, %r98, %r100;
	mov.b32 	%f168, %r107;
	add.f32 	%f169, %f167, %f168;
	mov.b32 	%r108, %f169;
	mov.u32 	%r109, 2;
	shfl.sync.bfly.b32 	%r110|%p27, %r108, %r109, %r98, %r100;
	mov.b32 	%f170, %r110;
	add.f32 	%f171, %f169, %f170;
	mov.b32 	%r111, %f171;
	mov.u32 	%r112, 1;
	shfl.sync.bfly.b32 	%r113|%p28, %r111, %r112, %r98, %r100;
	mov.b32 	%f172, %r113;
	add.f32 	%f173, %f171, %f172;
	st.local.f32 	[%rd3+4], %f173;

$L__BB10_15:
	bar.sync 	0;
	mov.b32 	%r114, %f205;
	mov.u32 	%r115, 31;
	mov.u32 	%r116, 16;
	mov.u32 	%r117, -1;
	shfl.sync.bfly.b32 	%r118|%p30, %r114, %r116, %r115, %r117;
	mov.b32 	%f174, %r118;
	add.f32 	%f175, %f205, %f174;
	mov.b32 	%r119, %f175;
	mov.u32 	%r120, 8;
	shfl.sync.bfly.b32 	%r121|%p31, %r119, %r120, %r115, %r117;
	mov.b32 	%f176, %r121;
	add.f32 	%f177, %f175, %f176;
	mov.b32 	%r122, %f177;
	mov.u32 	%r123, 4;
	shfl.sync.bfly.b32 	%r124|%p32, %r122, %r123, %r115, %r117;
	mov.b32 	%f178, %r124;
	add.f32 	%f179, %f177, %f178;
	mov.b32 	%r125, %f179;
	mov.u32 	%r126, 2;
	shfl.sync.bfly.b32 	%r127|%p33, %r125, %r126, %r115, %r117;
	mov.b32 	%f180, %r127;
	add.f32 	%f181, %f179, %f180;
	mov.b32 	%r128, %f181;
	mov.u32 	%r129, 1;
	shfl.sync.bfly.b32 	%r130|%p34, %r128, %r129, %r115, %r117;
	mov.b32 	%f182, %r130;
	add.f32 	%f183, %f181, %f182;
	st.local.f32 	[%rd3+8], %f183;
	st.shared.f32 	[%r14], %f183;
	bar.sync 	0;
	@%p1 bra 	$L__BB10_17;

	ld.shared.f32 	%f184, [%r4];
	mov.b32 	%r131, %f184;
	shfl.sync.bfly.b32 	%r135|%p35, %r131, %r116, %r115, %r117;
	mov.b32 	%f185, %r135;
	add.f32 	%f186, %f184, %f185;
	mov.b32 	%r136, %f186;
	shfl.sync.bfly.b32 	%r138|%p36, %r136, %r120, %r115, %r117;
	mov.b32 	%f187, %r138;
	add.f32 	%f188, %f186, %f187;
	mov.b32 	%r139, %f188;
	shfl.sync.bfly.b32 	%r141|%p37, %r139, %r123, %r115, %r117;
	mov.b32 	%f189, %r141;
	add.f32 	%f190, %f188, %f189;
	mov.b32 	%r142, %f190;
	shfl.sync.bfly.b32 	%r144|%p38, %r142, %r126, %r115, %r117;
	mov.b32 	%f191, %r144;
	add.f32 	%f192, %f190, %f191;
	mov.b32 	%r145, %f192;
	shfl.sync.bfly.b32 	%r147|%p39, %r145, %r129, %r115, %r117;
	mov.b32 	%f193, %r147;
	add.f32 	%f194, %f192, %f193;
	st.local.f32 	[%rd3+8], %f194;

$L__BB10_17:
	bar.sync 	0;
	setp.gt.s32 	%p40, %r3, 2;
	@%p40 bra 	$L__BB10_19;

	mul.wide.s32 	%rd58, %r3, 4;
	add.s64 	%rd59, %rd3, %rd58;
	ld.local.f32 	%f195, [%rd59];
	mad.lo.s32 	%r148, %r3, %r17, %r2;
	cvt.s64.s32 	%rd60, %r148;
	mul.lo.s32 	%r149, %r1, %r18;
	cvt.s64.s32 	%rd61, %r149;
	add.s64 	%rd62, %rd61, %rd60;
	cvta.to.global.u64 	%rd63, %rd28;
	shl.b64 	%rd64, %rd62, 2;
	add.s64 	%rd65, %rd63, %rd64;
	st.global.f32 	[%rd65], %f195;

$L__BB10_19:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_4_bs_64
.visible .entry ggml_matvec_f32_ncols_4_bs_64(
	.param .u64 ggml_matvec_f32_ncols_4_bs_64_param_0,
	.param .u64 ggml_matvec_f32_ncols_4_bs_64_param_1,
	.param .u64 ggml_matvec_f32_ncols_4_bs_64_param_2,
	.param .u32 ggml_matvec_f32_ncols_4_bs_64_param_3,
	.param .u32 ggml_matvec_f32_ncols_4_bs_64_param_4,
	.param .u32 ggml_matvec_f32_ncols_4_bs_64_param_5,
	.param .u32 ggml_matvec_f32_ncols_4_bs_64_param_6,
	.param .u32 ggml_matvec_f32_ncols_4_bs_64_param_7,
	.param .u32 ggml_matvec_f32_ncols_4_bs_64_param_8,
	.param .u32 ggml_matvec_f32_ncols_4_bs_64_param_9,
	.param .u32 ggml_matvec_f32_ncols_4_bs_64_param_10,
	.param .u32 ggml_matvec_f32_ncols_4_bs_64_param_11
)
{
	.local .align 16 .b8 	__local_depot11[16];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<52>;
	.reg .f32 	%f<270>;
	.reg .b32 	%r<189>;
	.reg .b64 	%rd<83>;


	mov.u64 	%SPL, __local_depot11;
	ld.param.u64 	%rd34, [ggml_matvec_f32_ncols_4_bs_64_param_0];
	ld.param.u64 	%rd35, [ggml_matvec_f32_ncols_4_bs_64_param_1];
	ld.param.u64 	%rd33, [ggml_matvec_f32_ncols_4_bs_64_param_2];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_4_bs_64_param_3];
	ld.param.u32 	%r19, [ggml_matvec_f32_ncols_4_bs_64_param_5];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_4_bs_64_param_6];
	ld.param.u32 	%r17, [ggml_matvec_f32_ncols_4_bs_64_param_7];
	ld.param.u32 	%r20, [ggml_matvec_f32_ncols_4_bs_64_param_8];
	ld.param.u32 	%r21, [ggml_matvec_f32_ncols_4_bs_64_param_9];
	ld.param.u32 	%r22, [ggml_matvec_f32_ncols_4_bs_64_param_10];
	ld.param.u32 	%r18, [ggml_matvec_f32_ncols_4_bs_64_param_11];
	cvta.to.global.u64 	%rd82, %rd35;
	cvta.to.global.u64 	%rd2, %rd34;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r23, %r1, %r20;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r24, %r2, %r19;
	mad.lo.s32 	%r25, %r23, %r21, %r24;
	cvt.s64.s32 	%rd4, %r25;
	mul.lo.s32 	%r26, %r1, %r22;
	cvt.s64.s32 	%rd5, %r26;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r27, %r3, 2;
	mov.u32 	%r28, data_mmv;
	add.s32 	%r4, %r28, %r27;
	@%p1 bra 	$L__BB11_2;

	mov.u32 	%r29, 0;
	st.shared.u32 	[%r4], %r29;

$L__BB11_2:
	bar.sync 	0;
	mov.f32 	%f266, 0f00000000;
	st.local.v4.f32 	[%rd3], {%f266, %f266, %f266, %f266};
	setp.ge.s32 	%p2, %r3, %r15;
	mov.f32 	%f267, %f266;
	mov.f32 	%f268, %f266;
	mov.f32 	%f269, %f266;
	@%p2 bra 	$L__BB11_11;

	not.b32 	%r30, %r3;
	add.s32 	%r5, %r30, %r15;
	shr.u32 	%r31, %r5, 6;
	add.s32 	%r32, %r31, 1;
	and.b32  	%r186, %r32, 3;
	setp.eq.s32 	%p3, %r186, 0;
	mov.f32 	%f266, 0f00000000;
	mov.u32 	%r187, %r3;
	@%p3 bra 	$L__BB11_7;

	shl.b32 	%r33, %r16, 1;
	add.s32 	%r34, %r3, %r33;
	mul.wide.s32 	%rd37, %r34, 2;
	add.s64 	%rd38, %rd37, %rd5;
	shl.b64 	%rd39, %rd38, 2;
	add.s64 	%rd80, %rd82, %rd39;
	mad.lo.s32 	%r35, %r16, 3, %r3;
	mul.wide.s32 	%rd40, %r35, 2;
	add.s64 	%rd41, %rd40, %rd5;
	shl.b64 	%rd42, %rd41, 2;
	add.s64 	%rd79, %rd82, %rd42;
	mul.wide.s32 	%rd43, %r16, 2;
	mul.wide.s32 	%rd44, %r3, 2;
	add.s64 	%rd45, %rd43, %rd44;
	add.s64 	%rd46, %rd45, %rd5;
	shl.b64 	%rd47, %rd46, 2;
	add.s64 	%rd78, %rd82, %rd47;
	add.s64 	%rd48, %rd44, %rd5;
	shl.b64 	%rd49, %rd48, 2;
	add.s64 	%rd77, %rd82, %rd49;
	add.s64 	%rd50, %rd44, %rd4;
	shl.b64 	%rd51, %rd50, 2;
	add.s64 	%rd76, %rd2, %rd51;
	mov.f32 	%f266, 0f00000000;
	mov.f32 	%f267, %f266;
	mov.f32 	%f268, %f266;
	mov.f32 	%f269, %f266;
	mov.u32 	%r187, %r3;

$L__BB11_5:
	.pragma "nounroll";
	ld.global.nc.v2.f32 	{%f37, %f38}, [%rd76];
	ld.global.nc.v2.f32 	{%f41, %f42}, [%rd77];
	fma.rn.f32 	%f45, %f37, %f41, %f269;
	fma.rn.f32 	%f269, %f38, %f42, %f45;
	ld.global.nc.v2.f32 	{%f46, %f47}, [%rd78];
	fma.rn.f32 	%f50, %f37, %f46, %f268;
	fma.rn.f32 	%f268, %f38, %f47, %f50;
	ld.global.nc.v2.f32 	{%f51, %f52}, [%rd80];
	fma.rn.f32 	%f55, %f37, %f51, %f267;
	fma.rn.f32 	%f267, %f38, %f52, %f55;
	ld.global.nc.v2.f32 	{%f56, %f57}, [%rd79];
	fma.rn.f32 	%f60, %f37, %f56, %f266;
	fma.rn.f32 	%f266, %f38, %f57, %f60;
	add.s32 	%r187, %r187, 64;
	add.s64 	%rd80, %rd80, 512;
	add.s64 	%rd79, %rd79, 512;
	add.s64 	%rd78, %rd78, 512;
	add.s64 	%rd77, %rd77, 512;
	add.s64 	%rd76, %rd76, 512;
	add.s32 	%r186, %r186, -1;
	setp.ne.s32 	%p4, %r186, 0;
	@%p4 bra 	$L__BB11_5;

	st.local.v4.f32 	[%rd3], {%f269, %f268, %f267, %f266};

$L__BB11_7:
	setp.lt.u32 	%p5, %r5, 192;
	@%p5 bra 	$L__BB11_11;

	add.s32 	%r36, %r187, %r16;
	shl.b32 	%r37, %r16, 1;
	add.s32 	%r38, %r187, %r37;
	mad.lo.s32 	%r39, %r16, 3, %r187;
	add.s32 	%r40, %r36, 64;
	mul.wide.s32 	%rd52, %r40, 8;
	shl.b64 	%rd53, %rd5, 2;
	add.s64 	%rd23, %rd52, %rd53;
	mul.wide.s32 	%rd54, %r38, 8;
	add.s64 	%rd24, %rd54, %rd53;
	mul.wide.s32 	%rd55, %r39, 8;
	add.s64 	%rd25, %rd55, %rd53;
	mul.wide.s32 	%rd56, %r187, 2;
	add.s64 	%rd57, %rd56, %rd4;
	shl.b64 	%rd58, %rd57, 2;
	add.s64 	%rd59, %rd2, %rd58;
	add.s64 	%rd81, %rd59, 1024;
	mul.wide.s32 	%rd60, %r187, 8;
	add.s64 	%rd27, %rd60, %rd53;
	mul.wide.s32 	%rd61, %r16, 8;
	add.s64 	%rd62, %rd60, %rd61;
	add.s64 	%rd28, %rd62, %rd53;

$L__BB11_9:
	ld.global.nc.v2.f32 	{%f61, %f62}, [%rd81+-1024];
	add.s64 	%rd63, %rd82, %rd27;
	ld.global.nc.v2.f32 	{%f65, %f66}, [%rd63];
	fma.rn.f32 	%f69, %f61, %f65, %f269;
	fma.rn.f32 	%f70, %f62, %f66, %f69;
	add.s64 	%rd64, %rd82, %rd28;
	ld.global.nc.v2.f32 	{%f71, %f72}, [%rd64];
	fma.rn.f32 	%f75, %f61, %f71, %f268;
	fma.rn.f32 	%f76, %f62, %f72, %f75;
	add.s64 	%rd65, %rd82, %rd24;
	ld.global.nc.v2.f32 	{%f77, %f78}, [%rd65];
	fma.rn.f32 	%f81, %f61, %f77, %f267;
	fma.rn.f32 	%f82, %f62, %f78, %f81;
	add.s64 	%rd66, %rd82, %rd25;
	ld.global.nc.v2.f32 	{%f83, %f84}, [%rd66];
	fma.rn.f32 	%f87, %f61, %f83, %f266;
	fma.rn.f32 	%f88, %f62, %f84, %f87;
	ld.global.nc.v2.f32 	{%f89, %f90}, [%rd81+-512];
	ld.global.nc.v2.f32 	{%f93, %f94}, [%rd63+512];
	fma.rn.f32 	%f97, %f89, %f93, %f70;
	fma.rn.f32 	%f98, %f90, %f94, %f97;
	add.s64 	%rd67, %rd82, %rd23;
	ld.global.nc.v2.f32 	{%f99, %f100}, [%rd67];
	fma.rn.f32 	%f103, %f89, %f99, %f76;
	fma.rn.f32 	%f104, %f90, %f100, %f103;
	ld.global.nc.v2.f32 	{%f105, %f106}, [%rd65+512];
	fma.rn.f32 	%f109, %f89, %f105, %f82;
	fma.rn.f32 	%f110, %f90, %f106, %f109;
	ld.global.nc.v2.f32 	{%f111, %f112}, [%rd66+512];
	fma.rn.f32 	%f115, %f89, %f111, %f88;
	fma.rn.f32 	%f116, %f90, %f112, %f115;
	ld.global.nc.v2.f32 	{%f117, %f118}, [%rd81];
	ld.global.nc.v2.f32 	{%f121, %f122}, [%rd63+1024];
	fma.rn.f32 	%f125, %f117, %f121, %f98;
	fma.rn.f32 	%f126, %f118, %f122, %f125;
	ld.global.nc.v2.f32 	{%f127, %f128}, [%rd67+512];
	fma.rn.f32 	%f131, %f117, %f127, %f104;
	fma.rn.f32 	%f132, %f118, %f128, %f131;
	ld.global.nc.v2.f32 	{%f133, %f134}, [%rd65+1024];
	fma.rn.f32 	%f137, %f117, %f133, %f110;
	fma.rn.f32 	%f138, %f118, %f134, %f137;
	ld.global.nc.v2.f32 	{%f139, %f140}, [%rd66+1024];
	fma.rn.f32 	%f143, %f117, %f139, %f116;
	fma.rn.f32 	%f144, %f118, %f140, %f143;
	ld.global.nc.v2.f32 	{%f145, %f146}, [%rd81+512];
	ld.global.nc.v2.f32 	{%f149, %f150}, [%rd63+1536];
	fma.rn.f32 	%f153, %f145, %f149, %f126;
	fma.rn.f32 	%f269, %f146, %f150, %f153;
	ld.global.nc.v2.f32 	{%f154, %f155}, [%rd67+1024];
	fma.rn.f32 	%f158, %f145, %f154, %f132;
	fma.rn.f32 	%f268, %f146, %f155, %f158;
	ld.global.nc.v2.f32 	{%f159, %f160}, [%rd65+1536];
	fma.rn.f32 	%f163, %f145, %f159, %f138;
	fma.rn.f32 	%f267, %f146, %f160, %f163;
	ld.global.nc.v2.f32 	{%f164, %f165}, [%rd66+1536];
	fma.rn.f32 	%f168, %f145, %f164, %f144;
	fma.rn.f32 	%f266, %f146, %f165, %f168;
	add.s64 	%rd82, %rd82, 2048;
	add.s64 	%rd81, %rd81, 2048;
	add.s32 	%r187, %r187, 256;
	setp.lt.s32 	%p6, %r187, %r15;
	@%p6 bra 	$L__BB11_9;

	st.local.v4.f32 	[%rd3], {%f269, %f268, %f267, %f266};

$L__BB11_11:
	shr.s32 	%r41, %r3, 31;
	shr.u32 	%r42, %r41, 27;
	add.s32 	%r43, %r3, %r42;
	shr.s32 	%r44, %r43, 5;
	shl.b32 	%r45, %r44, 2;
	add.s32 	%r14, %r28, %r45;
	mov.u32 	%r47, 2;
	mov.b32 	%r48, %f269;
	mov.u32 	%r49, 31;
	mov.u32 	%r50, 16;
	mov.u32 	%r51, -1;
	shfl.sync.bfly.b32 	%r52|%p7, %r48, %r50, %r49, %r51;
	mov.b32 	%f169, %r52;
	add.f32 	%f170, %f269, %f169;
	mov.b32 	%r53, %f170;
	mov.u32 	%r54, 8;
	shfl.sync.bfly.b32 	%r55|%p8, %r53, %r54, %r49, %r51;
	mov.b32 	%f171, %r55;
	add.f32 	%f172, %f170, %f171;
	mov.b32 	%r56, %f172;
	mov.u32 	%r57, 4;
	shfl.sync.bfly.b32 	%r58|%p9, %r56, %r57, %r49, %r51;
	mov.b32 	%f173, %r58;
	add.f32 	%f174, %f172, %f173;
	mov.b32 	%r59, %f174;
	shfl.sync.bfly.b32 	%r60|%p10, %r59, %r47, %r49, %r51;
	mov.b32 	%f175, %r60;
	add.f32 	%f176, %f174, %f175;
	mov.b32 	%r61, %f176;
	mov.u32 	%r62, 1;
	shfl.sync.bfly.b32 	%r63|%p11, %r61, %r62, %r49, %r51;
	mov.b32 	%f177, %r63;
	add.f32 	%f178, %f176, %f177;
	st.local.f32 	[%rd3], %f178;
	st.shared.f32 	[%r14], %f178;
	bar.sync 	0;
	@%p1 bra 	$L__BB11_13;

	ld.shared.f32 	%f179, [%r4];
	mov.b32 	%r64, %f179;
	shfl.sync.bfly.b32 	%r68|%p13, %r64, %r50, %r49, %r51;
	mov.b32 	%f180, %r68;
	add.f32 	%f181, %f179, %f180;
	mov.b32 	%r69, %f181;
	shfl.sync.bfly.b32 	%r71|%p14, %r69, %r54, %r49, %r51;
	mov.b32 	%f182, %r71;
	add.f32 	%f183, %f181, %f182;
	mov.b32 	%r72, %f183;
	shfl.sync.bfly.b32 	%r74|%p15, %r72, %r57, %r49, %r51;
	mov.b32 	%f184, %r74;
	add.f32 	%f185, %f183, %f184;
	mov.b32 	%r75, %f185;
	shfl.sync.bfly.b32 	%r77|%p16, %r75, %r47, %r49, %r51;
	mov.b32 	%f186, %r77;
	add.f32 	%f187, %f185, %f186;
	mov.b32 	%r78, %f187;
	shfl.sync.bfly.b32 	%r80|%p17, %r78, %r62, %r49, %r51;
	mov.b32 	%f188, %r80;
	add.f32 	%f189, %f187, %f188;
	st.local.f32 	[%rd3], %f189;

$L__BB11_13:
	bar.sync 	0;
	mov.b32 	%r81, %f268;
	shfl.sync.bfly.b32 	%r85|%p19, %r81, %r50, %r49, %r51;
	mov.b32 	%f190, %r85;
	add.f32 	%f191, %f268, %f190;
	mov.b32 	%r86, %f191;
	shfl.sync.bfly.b32 	%r88|%p20, %r86, %r54, %r49, %r51;
	mov.b32 	%f192, %r88;
	add.f32 	%f193, %f191, %f192;
	mov.b32 	%r89, %f193;
	shfl.sync.bfly.b32 	%r91|%p21, %r89, %r57, %r49, %r51;
	mov.b32 	%f194, %r91;
	add.f32 	%f195, %f193, %f194;
	mov.b32 	%r92, %f195;
	shfl.sync.bfly.b32 	%r94|%p22, %r92, %r47, %r49, %r51;
	mov.b32 	%f196, %r94;
	add.f32 	%f197, %f195, %f196;
	mov.b32 	%r95, %f197;
	shfl.sync.bfly.b32 	%r97|%p23, %r95, %r62, %r49, %r51;
	mov.b32 	%f198, %r97;
	add.f32 	%f199, %f197, %f198;
	st.local.f32 	[%rd3+4], %f199;
	st.shared.f32 	[%r14], %f199;
	bar.sync 	0;
	@%p1 bra 	$L__BB11_15;

	ld.shared.f32 	%f200, [%r4];
	mov.b32 	%r98, %f200;
	mov.u32 	%r99, 31;
	mov.u32 	%r100, 16;
	mov.u32 	%r101, -1;
	shfl.sync.bfly.b32 	%r102|%p24, %r98, %r100, %r99, %r101;
	mov.b32 	%f201, %r102;
	add.f32 	%f202, %f200, %f201;
	mov.b32 	%r103, %f202;
	mov.u32 	%r104, 8;
	shfl.sync.bfly.b32 	%r105|%p25, %r103, %r104, %r99, %r101;
	mov.b32 	%f203, %r105;
	add.f32 	%f204, %f202, %f203;
	mov.b32 	%r106, %f204;
	mov.u32 	%r107, 4;
	shfl.sync.bfly.b32 	%r108|%p26, %r106, %r107, %r99, %r101;
	mov.b32 	%f205, %r108;
	add.f32 	%f206, %f204, %f205;
	mov.b32 	%r109, %f206;
	mov.u32 	%r110, 2;
	shfl.sync.bfly.b32 	%r111|%p27, %r109, %r110, %r99, %r101;
	mov.b32 	%f207, %r111;
	add.f32 	%f208, %f206, %f207;
	mov.b32 	%r112, %f208;
	mov.u32 	%r113, 1;
	shfl.sync.bfly.b32 	%r114|%p28, %r112, %r113, %r99, %r101;
	mov.b32 	%f209, %r114;
	add.f32 	%f210, %f208, %f209;
	st.local.f32 	[%rd3+4], %f210;

$L__BB11_15:
	bar.sync 	0;
	mov.b32 	%r115, %f267;
	mov.u32 	%r116, 31;
	mov.u32 	%r117, 16;
	mov.u32 	%r118, -1;
	shfl.sync.bfly.b32 	%r119|%p30, %r115, %r117, %r116, %r118;
	mov.b32 	%f211, %r119;
	add.f32 	%f212, %f267, %f211;
	mov.b32 	%r120, %f212;
	mov.u32 	%r121, 8;
	shfl.sync.bfly.b32 	%r122|%p31, %r120, %r121, %r116, %r118;
	mov.b32 	%f213, %r122;
	add.f32 	%f214, %f212, %f213;
	mov.b32 	%r123, %f214;
	mov.u32 	%r124, 4;
	shfl.sync.bfly.b32 	%r125|%p32, %r123, %r124, %r116, %r118;
	mov.b32 	%f215, %r125;
	add.f32 	%f216, %f214, %f215;
	mov.b32 	%r126, %f216;
	mov.u32 	%r127, 2;
	shfl.sync.bfly.b32 	%r128|%p33, %r126, %r127, %r116, %r118;
	mov.b32 	%f217, %r128;
	add.f32 	%f218, %f216, %f217;
	mov.b32 	%r129, %f218;
	mov.u32 	%r130, 1;
	shfl.sync.bfly.b32 	%r131|%p34, %r129, %r130, %r116, %r118;
	mov.b32 	%f219, %r131;
	add.f32 	%f220, %f218, %f219;
	st.local.f32 	[%rd3+8], %f220;
	st.shared.f32 	[%r14], %f220;
	bar.sync 	0;
	@%p1 bra 	$L__BB11_17;

	ld.shared.f32 	%f221, [%r4];
	mov.b32 	%r132, %f221;
	shfl.sync.bfly.b32 	%r136|%p35, %r132, %r117, %r116, %r118;
	mov.b32 	%f222, %r136;
	add.f32 	%f223, %f221, %f222;
	mov.b32 	%r137, %f223;
	shfl.sync.bfly.b32 	%r139|%p36, %r137, %r121, %r116, %r118;
	mov.b32 	%f224, %r139;
	add.f32 	%f225, %f223, %f224;
	mov.b32 	%r140, %f225;
	shfl.sync.bfly.b32 	%r142|%p37, %r140, %r124, %r116, %r118;
	mov.b32 	%f226, %r142;
	add.f32 	%f227, %f225, %f226;
	mov.b32 	%r143, %f227;
	shfl.sync.bfly.b32 	%r145|%p38, %r143, %r127, %r116, %r118;
	mov.b32 	%f228, %r145;
	add.f32 	%f229, %f227, %f228;
	mov.b32 	%r146, %f229;
	shfl.sync.bfly.b32 	%r148|%p39, %r146, %r130, %r116, %r118;
	mov.b32 	%f230, %r148;
	add.f32 	%f231, %f229, %f230;
	st.local.f32 	[%rd3+8], %f231;

$L__BB11_17:
	bar.sync 	0;
	mov.b32 	%r149, %f266;
	shfl.sync.bfly.b32 	%r153|%p41, %r149, %r117, %r116, %r118;
	mov.b32 	%f232, %r153;
	add.f32 	%f233, %f266, %f232;
	mov.b32 	%r154, %f233;
	shfl.sync.bfly.b32 	%r156|%p42, %r154, %r121, %r116, %r118;
	mov.b32 	%f234, %r156;
	add.f32 	%f235, %f233, %f234;
	mov.b32 	%r157, %f235;
	shfl.sync.bfly.b32 	%r159|%p43, %r157, %r124, %r116, %r118;
	mov.b32 	%f236, %r159;
	add.f32 	%f237, %f235, %f236;
	mov.b32 	%r160, %f237;
	shfl.sync.bfly.b32 	%r162|%p44, %r160, %r127, %r116, %r118;
	mov.b32 	%f238, %r162;
	add.f32 	%f239, %f237, %f238;
	mov.b32 	%r163, %f239;
	shfl.sync.bfly.b32 	%r165|%p45, %r163, %r130, %r116, %r118;
	mov.b32 	%f240, %r165;
	add.f32 	%f241, %f239, %f240;
	st.local.f32 	[%rd3+12], %f241;
	st.shared.f32 	[%r14], %f241;
	bar.sync 	0;
	@%p1 bra 	$L__BB11_19;

	ld.shared.f32 	%f242, [%r4];
	mov.b32 	%r166, %f242;
	mov.u32 	%r167, 31;
	mov.u32 	%r168, 16;
	mov.u32 	%r169, -1;
	shfl.sync.bfly.b32 	%r170|%p46, %r166, %r168, %r167, %r169;
	mov.b32 	%f243, %r170;
	add.f32 	%f244, %f242, %f243;
	mov.b32 	%r171, %f244;
	mov.u32 	%r172, 8;
	shfl.sync.bfly.b32 	%r173|%p47, %r171, %r172, %r167, %r169;
	mov.b32 	%f245, %r173;
	add.f32 	%f246, %f244, %f245;
	mov.b32 	%r174, %f246;
	mov.u32 	%r175, 4;
	shfl.sync.bfly.b32 	%r176|%p48, %r174, %r175, %r167, %r169;
	mov.b32 	%f247, %r176;
	add.f32 	%f248, %f246, %f247;
	mov.b32 	%r177, %f248;
	mov.u32 	%r178, 2;
	shfl.sync.bfly.b32 	%r179|%p49, %r177, %r178, %r167, %r169;
	mov.b32 	%f249, %r179;
	add.f32 	%f250, %f248, %f249;
	mov.b32 	%r180, %f250;
	mov.u32 	%r181, 1;
	shfl.sync.bfly.b32 	%r182|%p50, %r180, %r181, %r167, %r169;
	mov.b32 	%f251, %r182;
	add.f32 	%f252, %f250, %f251;
	st.local.f32 	[%rd3+12], %f252;

$L__BB11_19:
	bar.sync 	0;
	setp.gt.s32 	%p51, %r3, 3;
	@%p51 bra 	$L__BB11_21;

	mul.wide.s32 	%rd68, %r3, 4;
	add.s64 	%rd69, %rd3, %rd68;
	ld.local.f32 	%f253, [%rd69];
	mad.lo.s32 	%r183, %r3, %r17, %r2;
	cvt.s64.s32 	%rd70, %r183;
	mul.lo.s32 	%r184, %r1, %r18;
	cvt.s64.s32 	%rd71, %r184;
	add.s64 	%rd72, %rd71, %rd70;
	cvta.to.global.u64 	%rd73, %rd33;
	shl.b64 	%rd74, %rd72, 2;
	add.s64 	%rd75, %rd73, %rd74;
	st.global.f32 	[%rd75], %f253;

$L__BB11_21:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_5_bs_64
.visible .entry ggml_matvec_f32_ncols_5_bs_64(
	.param .u64 ggml_matvec_f32_ncols_5_bs_64_param_0,
	.param .u64 ggml_matvec_f32_ncols_5_bs_64_param_1,
	.param .u64 ggml_matvec_f32_ncols_5_bs_64_param_2,
	.param .u32 ggml_matvec_f32_ncols_5_bs_64_param_3,
	.param .u32 ggml_matvec_f32_ncols_5_bs_64_param_4,
	.param .u32 ggml_matvec_f32_ncols_5_bs_64_param_5,
	.param .u32 ggml_matvec_f32_ncols_5_bs_64_param_6,
	.param .u32 ggml_matvec_f32_ncols_5_bs_64_param_7,
	.param .u32 ggml_matvec_f32_ncols_5_bs_64_param_8,
	.param .u32 ggml_matvec_f32_ncols_5_bs_64_param_9,
	.param .u32 ggml_matvec_f32_ncols_5_bs_64_param_10,
	.param .u32 ggml_matvec_f32_ncols_5_bs_64_param_11
)
{
	.local .align 4 .b8 	__local_depot12[20];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<63>;
	.reg .f32 	%f<332>;
	.reg .b32 	%r<225>;
	.reg .b64 	%rd<74>;


	mov.u64 	%SPL, __local_depot12;
	ld.param.u64 	%rd28, [ggml_matvec_f32_ncols_5_bs_64_param_0];
	ld.param.u64 	%rd29, [ggml_matvec_f32_ncols_5_bs_64_param_1];
	ld.param.u64 	%rd27, [ggml_matvec_f32_ncols_5_bs_64_param_2];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_5_bs_64_param_3];
	ld.param.u32 	%r19, [ggml_matvec_f32_ncols_5_bs_64_param_5];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_5_bs_64_param_6];
	ld.param.u32 	%r17, [ggml_matvec_f32_ncols_5_bs_64_param_7];
	ld.param.u32 	%r20, [ggml_matvec_f32_ncols_5_bs_64_param_8];
	ld.param.u32 	%r21, [ggml_matvec_f32_ncols_5_bs_64_param_9];
	ld.param.u32 	%r22, [ggml_matvec_f32_ncols_5_bs_64_param_10];
	ld.param.u32 	%r18, [ggml_matvec_f32_ncols_5_bs_64_param_11];
	cvta.to.global.u64 	%rd73, %rd29;
	cvta.to.global.u64 	%rd2, %rd28;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r23, %r1, %r20;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r24, %r2, %r19;
	mad.lo.s32 	%r25, %r23, %r21, %r24;
	cvt.s64.s32 	%rd4, %r25;
	mul.lo.s32 	%r26, %r1, %r22;
	cvt.s64.s32 	%rd5, %r26;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r27, %r3, 2;
	mov.u32 	%r28, data_mmv;
	add.s32 	%r4, %r28, %r27;
	@%p1 bra 	$L__BB12_2;

	mov.u32 	%r29, 0;
	st.shared.u32 	[%r4], %r29;

$L__BB12_2:
	bar.sync 	0;
	mov.f32 	%f327, 0f00000000;
	mov.u32 	%r30, 0;
	st.local.u32 	[%rd3], %r30;
	st.local.u32 	[%rd3+4], %r30;
	st.local.u32 	[%rd3+8], %r30;
	st.local.u32 	[%rd3+12], %r30;
	st.local.u32 	[%rd3+16], %r30;
	setp.ge.s32 	%p2, %r3, %r15;
	mov.f32 	%f328, %f327;
	mov.f32 	%f329, %f327;
	mov.f32 	%f330, %f327;
	mov.f32 	%f331, %f327;
	@%p2 bra 	$L__BB12_11;

	not.b32 	%r31, %r3;
	add.s32 	%r5, %r31, %r15;
	shr.u32 	%r32, %r5, 6;
	add.s32 	%r33, %r32, 1;
	and.b32  	%r222, %r33, 3;
	setp.eq.s32 	%p3, %r222, 0;
	mov.f32 	%f327, 0f00000000;
	mov.u32 	%r223, %r3;
	@%p3 bra 	$L__BB12_7;

	shl.b32 	%r34, %r16, 1;
	mad.lo.s32 	%r35, %r16, 3, %r3;
	mul.wide.s32 	%rd31, %r35, 8;
	shl.b64 	%rd32, %rd5, 2;
	add.s64 	%rd7, %rd31, %rd32;
	mul.wide.s32 	%rd33, %r3, 8;
	mul.wide.s32 	%rd34, %r16, 8;
	add.s64 	%rd35, %rd33, %rd34;
	add.s64 	%rd8, %rd35, %rd32;
	add.s64 	%rd9, %rd33, %rd32;
	mul.wide.s32 	%rd36, %r3, 2;
	add.s64 	%rd37, %rd36, %rd4;
	shl.b64 	%rd38, %rd37, 2;
	add.s64 	%rd70, %rd2, %rd38;
	mul.wide.s32 	%rd11, %r34, 8;
	mov.f32 	%f327, 0f00000000;
	mov.u64 	%rd71, %rd73;
	mov.f32 	%f328, %f327;
	mov.f32 	%f329, %f327;
	mov.f32 	%f330, %f327;
	mov.f32 	%f331, %f327;
	mov.u32 	%r223, %r3;

$L__BB12_5:
	.pragma "nounroll";
	ld.global.nc.v2.f32 	{%f46, %f47}, [%rd70];
	add.s64 	%rd39, %rd71, %rd9;
	ld.global.nc.v2.f32 	{%f50, %f51}, [%rd39];
	fma.rn.f32 	%f54, %f46, %f50, %f331;
	fma.rn.f32 	%f331, %f47, %f51, %f54;
	add.s64 	%rd40, %rd71, %rd8;
	ld.global.nc.v2.f32 	{%f55, %f56}, [%rd40];
	fma.rn.f32 	%f59, %f46, %f55, %f330;
	fma.rn.f32 	%f330, %f47, %f56, %f59;
	add.s64 	%rd41, %rd39, %rd11;
	ld.global.nc.v2.f32 	{%f60, %f61}, [%rd41];
	fma.rn.f32 	%f64, %f46, %f60, %f329;
	fma.rn.f32 	%f329, %f47, %f61, %f64;
	add.s64 	%rd42, %rd71, %rd7;
	ld.global.nc.v2.f32 	{%f65, %f66}, [%rd42];
	fma.rn.f32 	%f69, %f46, %f65, %f328;
	fma.rn.f32 	%f328, %f47, %f66, %f69;
	add.s64 	%rd43, %rd41, %rd11;
	ld.global.nc.v2.f32 	{%f70, %f71}, [%rd43];
	fma.rn.f32 	%f74, %f46, %f70, %f327;
	fma.rn.f32 	%f327, %f47, %f71, %f74;
	add.s32 	%r223, %r223, 64;
	add.s64 	%rd71, %rd71, 512;
	add.s64 	%rd70, %rd70, 512;
	add.s32 	%r222, %r222, -1;
	setp.ne.s32 	%p4, %r222, 0;
	@%p4 bra 	$L__BB12_5;

	st.local.f32 	[%rd3], %f331;
	st.local.f32 	[%rd3+4], %f330;
	st.local.f32 	[%rd3+8], %f329;
	st.local.f32 	[%rd3+12], %f328;
	st.local.f32 	[%rd3+16], %f327;

$L__BB12_7:
	setp.lt.u32 	%p5, %r5, 192;
	@%p5 bra 	$L__BB12_11;

	add.s32 	%r36, %r223, %r16;
	shl.b32 	%r37, %r16, 1;
	add.s32 	%r38, %r223, %r37;
	mad.lo.s32 	%r39, %r16, 3, %r223;
	shl.b32 	%r40, %r16, 2;
	add.s32 	%r41, %r223, %r40;
	add.s32 	%r42, %r36, 64;
	mul.wide.s32 	%rd44, %r42, 8;
	shl.b64 	%rd45, %rd5, 2;
	add.s64 	%rd16, %rd44, %rd45;
	mul.wide.s32 	%rd46, %r38, 8;
	add.s64 	%rd17, %rd46, %rd45;
	mul.wide.s32 	%rd47, %r39, 8;
	add.s64 	%rd18, %rd47, %rd45;
	mul.wide.s32 	%rd48, %r41, 8;
	add.s64 	%rd19, %rd48, %rd45;
	mul.wide.s32 	%rd49, %r223, 2;
	add.s64 	%rd50, %rd49, %rd4;
	shl.b64 	%rd51, %rd50, 2;
	add.s64 	%rd52, %rd2, %rd51;
	add.s64 	%rd72, %rd52, 1024;
	mul.wide.s32 	%rd53, %r223, 8;
	add.s64 	%rd21, %rd53, %rd45;
	mul.wide.s32 	%rd54, %r16, 8;
	add.s64 	%rd55, %rd53, %rd54;
	add.s64 	%rd22, %rd55, %rd45;

$L__BB12_9:
	ld.global.nc.v2.f32 	{%f75, %f76}, [%rd72+-1024];
	add.s64 	%rd56, %rd73, %rd21;
	ld.global.nc.v2.f32 	{%f79, %f80}, [%rd56];
	fma.rn.f32 	%f83, %f75, %f79, %f331;
	fma.rn.f32 	%f84, %f76, %f80, %f83;
	add.s64 	%rd57, %rd73, %rd22;
	ld.global.nc.v2.f32 	{%f85, %f86}, [%rd57];
	fma.rn.f32 	%f89, %f75, %f85, %f330;
	fma.rn.f32 	%f90, %f76, %f86, %f89;
	add.s64 	%rd58, %rd73, %rd17;
	ld.global.nc.v2.f32 	{%f91, %f92}, [%rd58];
	fma.rn.f32 	%f95, %f75, %f91, %f329;
	fma.rn.f32 	%f96, %f76, %f92, %f95;
	add.s64 	%rd59, %rd73, %rd18;
	ld.global.nc.v2.f32 	{%f97, %f98}, [%rd59];
	fma.rn.f32 	%f101, %f75, %f97, %f328;
	fma.rn.f32 	%f102, %f76, %f98, %f101;
	add.s64 	%rd60, %rd73, %rd19;
	ld.global.nc.v2.f32 	{%f103, %f104}, [%rd60];
	fma.rn.f32 	%f107, %f75, %f103, %f327;
	fma.rn.f32 	%f108, %f76, %f104, %f107;
	ld.global.nc.v2.f32 	{%f109, %f110}, [%rd72+-512];
	ld.global.nc.v2.f32 	{%f113, %f114}, [%rd56+512];
	fma.rn.f32 	%f117, %f109, %f113, %f84;
	fma.rn.f32 	%f118, %f110, %f114, %f117;
	add.s64 	%rd61, %rd73, %rd16;
	ld.global.nc.v2.f32 	{%f119, %f120}, [%rd61];
	fma.rn.f32 	%f123, %f109, %f119, %f90;
	fma.rn.f32 	%f124, %f110, %f120, %f123;
	ld.global.nc.v2.f32 	{%f125, %f126}, [%rd58+512];
	fma.rn.f32 	%f129, %f109, %f125, %f96;
	fma.rn.f32 	%f130, %f110, %f126, %f129;
	ld.global.nc.v2.f32 	{%f131, %f132}, [%rd59+512];
	fma.rn.f32 	%f135, %f109, %f131, %f102;
	fma.rn.f32 	%f136, %f110, %f132, %f135;
	ld.global.nc.v2.f32 	{%f137, %f138}, [%rd60+512];
	fma.rn.f32 	%f141, %f109, %f137, %f108;
	fma.rn.f32 	%f142, %f110, %f138, %f141;
	ld.global.nc.v2.f32 	{%f143, %f144}, [%rd72];
	ld.global.nc.v2.f32 	{%f147, %f148}, [%rd56+1024];
	fma.rn.f32 	%f151, %f143, %f147, %f118;
	fma.rn.f32 	%f152, %f144, %f148, %f151;
	ld.global.nc.v2.f32 	{%f153, %f154}, [%rd61+512];
	fma.rn.f32 	%f157, %f143, %f153, %f124;
	fma.rn.f32 	%f158, %f144, %f154, %f157;
	ld.global.nc.v2.f32 	{%f159, %f160}, [%rd58+1024];
	fma.rn.f32 	%f163, %f143, %f159, %f130;
	fma.rn.f32 	%f164, %f144, %f160, %f163;
	ld.global.nc.v2.f32 	{%f165, %f166}, [%rd59+1024];
	fma.rn.f32 	%f169, %f143, %f165, %f136;
	fma.rn.f32 	%f170, %f144, %f166, %f169;
	ld.global.nc.v2.f32 	{%f171, %f172}, [%rd60+1024];
	fma.rn.f32 	%f175, %f143, %f171, %f142;
	fma.rn.f32 	%f176, %f144, %f172, %f175;
	ld.global.nc.v2.f32 	{%f177, %f178}, [%rd72+512];
	ld.global.nc.v2.f32 	{%f181, %f182}, [%rd56+1536];
	fma.rn.f32 	%f185, %f177, %f181, %f152;
	fma.rn.f32 	%f331, %f178, %f182, %f185;
	ld.global.nc.v2.f32 	{%f186, %f187}, [%rd61+1024];
	fma.rn.f32 	%f190, %f177, %f186, %f158;
	fma.rn.f32 	%f330, %f178, %f187, %f190;
	ld.global.nc.v2.f32 	{%f191, %f192}, [%rd58+1536];
	fma.rn.f32 	%f195, %f177, %f191, %f164;
	fma.rn.f32 	%f329, %f178, %f192, %f195;
	ld.global.nc.v2.f32 	{%f196, %f197}, [%rd59+1536];
	fma.rn.f32 	%f200, %f177, %f196, %f170;
	fma.rn.f32 	%f328, %f178, %f197, %f200;
	ld.global.nc.v2.f32 	{%f201, %f202}, [%rd60+1536];
	fma.rn.f32 	%f205, %f177, %f201, %f176;
	fma.rn.f32 	%f327, %f178, %f202, %f205;
	add.s64 	%rd73, %rd73, 2048;
	add.s64 	%rd72, %rd72, 2048;
	add.s32 	%r223, %r223, 256;
	setp.lt.s32 	%p6, %r223, %r15;
	@%p6 bra 	$L__BB12_9;

	st.local.f32 	[%rd3], %f331;
	st.local.f32 	[%rd3+4], %f330;
	st.local.f32 	[%rd3+8], %f329;
	st.local.f32 	[%rd3+12], %f328;
	st.local.f32 	[%rd3+16], %f327;

$L__BB12_11:
	shr.s32 	%r43, %r3, 31;
	shr.u32 	%r44, %r43, 27;
	add.s32 	%r45, %r3, %r44;
	shr.s32 	%r46, %r45, 5;
	shl.b32 	%r47, %r46, 2;
	add.s32 	%r14, %r28, %r47;
	mov.u32 	%r49, 2;
	mov.b32 	%r50, %f331;
	mov.u32 	%r51, 31;
	mov.u32 	%r52, 16;
	mov.u32 	%r53, -1;
	shfl.sync.bfly.b32 	%r54|%p7, %r50, %r52, %r51, %r53;
	mov.b32 	%f206, %r54;
	add.f32 	%f207, %f331, %f206;
	mov.b32 	%r55, %f207;
	mov.u32 	%r56, 8;
	shfl.sync.bfly.b32 	%r57|%p8, %r55, %r56, %r51, %r53;
	mov.b32 	%f208, %r57;
	add.f32 	%f209, %f207, %f208;
	mov.b32 	%r58, %f209;
	mov.u32 	%r59, 4;
	shfl.sync.bfly.b32 	%r60|%p9, %r58, %r59, %r51, %r53;
	mov.b32 	%f210, %r60;
	add.f32 	%f211, %f209, %f210;
	mov.b32 	%r61, %f211;
	shfl.sync.bfly.b32 	%r62|%p10, %r61, %r49, %r51, %r53;
	mov.b32 	%f212, %r62;
	add.f32 	%f213, %f211, %f212;
	mov.b32 	%r63, %f213;
	mov.u32 	%r64, 1;
	shfl.sync.bfly.b32 	%r65|%p11, %r63, %r64, %r51, %r53;
	mov.b32 	%f214, %r65;
	add.f32 	%f215, %f213, %f214;
	st.local.f32 	[%rd3], %f215;
	st.shared.f32 	[%r14], %f215;
	bar.sync 	0;
	@%p1 bra 	$L__BB12_13;

	ld.shared.f32 	%f216, [%r4];
	mov.b32 	%r66, %f216;
	shfl.sync.bfly.b32 	%r70|%p13, %r66, %r52, %r51, %r53;
	mov.b32 	%f217, %r70;
	add.f32 	%f218, %f216, %f217;
	mov.b32 	%r71, %f218;
	shfl.sync.bfly.b32 	%r73|%p14, %r71, %r56, %r51, %r53;
	mov.b32 	%f219, %r73;
	add.f32 	%f220, %f218, %f219;
	mov.b32 	%r74, %f220;
	shfl.sync.bfly.b32 	%r76|%p15, %r74, %r59, %r51, %r53;
	mov.b32 	%f221, %r76;
	add.f32 	%f222, %f220, %f221;
	mov.b32 	%r77, %f222;
	shfl.sync.bfly.b32 	%r79|%p16, %r77, %r49, %r51, %r53;
	mov.b32 	%f223, %r79;
	add.f32 	%f224, %f222, %f223;
	mov.b32 	%r80, %f224;
	shfl.sync.bfly.b32 	%r82|%p17, %r80, %r64, %r51, %r53;
	mov.b32 	%f225, %r82;
	add.f32 	%f226, %f224, %f225;
	st.local.f32 	[%rd3], %f226;

$L__BB12_13:
	bar.sync 	0;
	mov.b32 	%r83, %f330;
	shfl.sync.bfly.b32 	%r87|%p19, %r83, %r52, %r51, %r53;
	mov.b32 	%f227, %r87;
	add.f32 	%f228, %f330, %f227;
	mov.b32 	%r88, %f228;
	shfl.sync.bfly.b32 	%r90|%p20, %r88, %r56, %r51, %r53;
	mov.b32 	%f229, %r90;
	add.f32 	%f230, %f228, %f229;
	mov.b32 	%r91, %f230;
	shfl.sync.bfly.b32 	%r93|%p21, %r91, %r59, %r51, %r53;
	mov.b32 	%f231, %r93;
	add.f32 	%f232, %f230, %f231;
	mov.b32 	%r94, %f232;
	shfl.sync.bfly.b32 	%r96|%p22, %r94, %r49, %r51, %r53;
	mov.b32 	%f233, %r96;
	add.f32 	%f234, %f232, %f233;
	mov.b32 	%r97, %f234;
	shfl.sync.bfly.b32 	%r99|%p23, %r97, %r64, %r51, %r53;
	mov.b32 	%f235, %r99;
	add.f32 	%f236, %f234, %f235;
	st.local.f32 	[%rd3+4], %f236;
	st.shared.f32 	[%r14], %f236;
	bar.sync 	0;
	@%p1 bra 	$L__BB12_15;

	ld.shared.f32 	%f237, [%r4];
	mov.b32 	%r100, %f237;
	mov.u32 	%r101, 31;
	mov.u32 	%r102, 16;
	mov.u32 	%r103, -1;
	shfl.sync.bfly.b32 	%r104|%p24, %r100, %r102, %r101, %r103;
	mov.b32 	%f238, %r104;
	add.f32 	%f239, %f237, %f238;
	mov.b32 	%r105, %f239;
	mov.u32 	%r106, 8;
	shfl.sync.bfly.b32 	%r107|%p25, %r105, %r106, %r101, %r103;
	mov.b32 	%f240, %r107;
	add.f32 	%f241, %f239, %f240;
	mov.b32 	%r108, %f241;
	mov.u32 	%r109, 4;
	shfl.sync.bfly.b32 	%r110|%p26, %r108, %r109, %r101, %r103;
	mov.b32 	%f242, %r110;
	add.f32 	%f243, %f241, %f242;
	mov.b32 	%r111, %f243;
	mov.u32 	%r112, 2;
	shfl.sync.bfly.b32 	%r113|%p27, %r111, %r112, %r101, %r103;
	mov.b32 	%f244, %r113;
	add.f32 	%f245, %f243, %f244;
	mov.b32 	%r114, %f245;
	mov.u32 	%r115, 1;
	shfl.sync.bfly.b32 	%r116|%p28, %r114, %r115, %r101, %r103;
	mov.b32 	%f246, %r116;
	add.f32 	%f247, %f245, %f246;
	st.local.f32 	[%rd3+4], %f247;

$L__BB12_15:
	bar.sync 	0;
	mov.b32 	%r117, %f329;
	mov.u32 	%r118, 31;
	mov.u32 	%r119, 16;
	mov.u32 	%r120, -1;
	shfl.sync.bfly.b32 	%r121|%p30, %r117, %r119, %r118, %r120;
	mov.b32 	%f248, %r121;
	add.f32 	%f249, %f329, %f248;
	mov.b32 	%r122, %f249;
	mov.u32 	%r123, 8;
	shfl.sync.bfly.b32 	%r124|%p31, %r122, %r123, %r118, %r120;
	mov.b32 	%f250, %r124;
	add.f32 	%f251, %f249, %f250;
	mov.b32 	%r125, %f251;
	mov.u32 	%r126, 4;
	shfl.sync.bfly.b32 	%r127|%p32, %r125, %r126, %r118, %r120;
	mov.b32 	%f252, %r127;
	add.f32 	%f253, %f251, %f252;
	mov.b32 	%r128, %f253;
	mov.u32 	%r129, 2;
	shfl.sync.bfly.b32 	%r130|%p33, %r128, %r129, %r118, %r120;
	mov.b32 	%f254, %r130;
	add.f32 	%f255, %f253, %f254;
	mov.b32 	%r131, %f255;
	mov.u32 	%r132, 1;
	shfl.sync.bfly.b32 	%r133|%p34, %r131, %r132, %r118, %r120;
	mov.b32 	%f256, %r133;
	add.f32 	%f257, %f255, %f256;
	st.local.f32 	[%rd3+8], %f257;
	st.shared.f32 	[%r14], %f257;
	bar.sync 	0;
	@%p1 bra 	$L__BB12_17;

	ld.shared.f32 	%f258, [%r4];
	mov.b32 	%r134, %f258;
	shfl.sync.bfly.b32 	%r138|%p35, %r134, %r119, %r118, %r120;
	mov.b32 	%f259, %r138;
	add.f32 	%f260, %f258, %f259;
	mov.b32 	%r139, %f260;
	shfl.sync.bfly.b32 	%r141|%p36, %r139, %r123, %r118, %r120;
	mov.b32 	%f261, %r141;
	add.f32 	%f262, %f260, %f261;
	mov.b32 	%r142, %f262;
	shfl.sync.bfly.b32 	%r144|%p37, %r142, %r126, %r118, %r120;
	mov.b32 	%f263, %r144;
	add.f32 	%f264, %f262, %f263;
	mov.b32 	%r145, %f264;
	shfl.sync.bfly.b32 	%r147|%p38, %r145, %r129, %r118, %r120;
	mov.b32 	%f265, %r147;
	add.f32 	%f266, %f264, %f265;
	mov.b32 	%r148, %f266;
	shfl.sync.bfly.b32 	%r150|%p39, %r148, %r132, %r118, %r120;
	mov.b32 	%f267, %r150;
	add.f32 	%f268, %f266, %f267;
	st.local.f32 	[%rd3+8], %f268;

$L__BB12_17:
	bar.sync 	0;
	mov.b32 	%r151, %f328;
	shfl.sync.bfly.b32 	%r155|%p41, %r151, %r119, %r118, %r120;
	mov.b32 	%f269, %r155;
	add.f32 	%f270, %f328, %f269;
	mov.b32 	%r156, %f270;
	shfl.sync.bfly.b32 	%r158|%p42, %r156, %r123, %r118, %r120;
	mov.b32 	%f271, %r158;
	add.f32 	%f272, %f270, %f271;
	mov.b32 	%r159, %f272;
	shfl.sync.bfly.b32 	%r161|%p43, %r159, %r126, %r118, %r120;
	mov.b32 	%f273, %r161;
	add.f32 	%f274, %f272, %f273;
	mov.b32 	%r162, %f274;
	shfl.sync.bfly.b32 	%r164|%p44, %r162, %r129, %r118, %r120;
	mov.b32 	%f275, %r164;
	add.f32 	%f276, %f274, %f275;
	mov.b32 	%r165, %f276;
	shfl.sync.bfly.b32 	%r167|%p45, %r165, %r132, %r118, %r120;
	mov.b32 	%f277, %r167;
	add.f32 	%f278, %f276, %f277;
	st.local.f32 	[%rd3+12], %f278;
	st.shared.f32 	[%r14], %f278;
	bar.sync 	0;
	@%p1 bra 	$L__BB12_19;

	ld.shared.f32 	%f279, [%r4];
	mov.b32 	%r168, %f279;
	mov.u32 	%r169, 31;
	mov.u32 	%r170, 16;
	mov.u32 	%r171, -1;
	shfl.sync.bfly.b32 	%r172|%p46, %r168, %r170, %r169, %r171;
	mov.b32 	%f280, %r172;
	add.f32 	%f281, %f279, %f280;
	mov.b32 	%r173, %f281;
	mov.u32 	%r174, 8;
	shfl.sync.bfly.b32 	%r175|%p47, %r173, %r174, %r169, %r171;
	mov.b32 	%f282, %r175;
	add.f32 	%f283, %f281, %f282;
	mov.b32 	%r176, %f283;
	mov.u32 	%r177, 4;
	shfl.sync.bfly.b32 	%r178|%p48, %r176, %r177, %r169, %r171;
	mov.b32 	%f284, %r178;
	add.f32 	%f285, %f283, %f284;
	mov.b32 	%r179, %f285;
	mov.u32 	%r180, 2;
	shfl.sync.bfly.b32 	%r181|%p49, %r179, %r180, %r169, %r171;
	mov.b32 	%f286, %r181;
	add.f32 	%f287, %f285, %f286;
	mov.b32 	%r182, %f287;
	mov.u32 	%r183, 1;
	shfl.sync.bfly.b32 	%r184|%p50, %r182, %r183, %r169, %r171;
	mov.b32 	%f288, %r184;
	add.f32 	%f289, %f287, %f288;
	st.local.f32 	[%rd3+12], %f289;

$L__BB12_19:
	bar.sync 	0;
	mov.b32 	%r185, %f327;
	mov.u32 	%r186, 31;
	mov.u32 	%r187, 16;
	mov.u32 	%r188, -1;
	shfl.sync.bfly.b32 	%r189|%p52, %r185, %r187, %r186, %r188;
	mov.b32 	%f290, %r189;
	add.f32 	%f291, %f327, %f290;
	mov.b32 	%r190, %f291;
	mov.u32 	%r191, 8;
	shfl.sync.bfly.b32 	%r192|%p53, %r190, %r191, %r186, %r188;
	mov.b32 	%f292, %r192;
	add.f32 	%f293, %f291, %f292;
	mov.b32 	%r193, %f293;
	mov.u32 	%r194, 4;
	shfl.sync.bfly.b32 	%r195|%p54, %r193, %r194, %r186, %r188;
	mov.b32 	%f294, %r195;
	add.f32 	%f295, %f293, %f294;
	mov.b32 	%r196, %f295;
	mov.u32 	%r197, 2;
	shfl.sync.bfly.b32 	%r198|%p55, %r196, %r197, %r186, %r188;
	mov.b32 	%f296, %r198;
	add.f32 	%f297, %f295, %f296;
	mov.b32 	%r199, %f297;
	mov.u32 	%r200, 1;
	shfl.sync.bfly.b32 	%r201|%p56, %r199, %r200, %r186, %r188;
	mov.b32 	%f298, %r201;
	add.f32 	%f299, %f297, %f298;
	st.local.f32 	[%rd3+16], %f299;
	st.shared.f32 	[%r14], %f299;
	bar.sync 	0;
	@%p1 bra 	$L__BB12_21;

	ld.shared.f32 	%f300, [%r4];
	mov.b32 	%r202, %f300;
	shfl.sync.bfly.b32 	%r206|%p57, %r202, %r187, %r186, %r188;
	mov.b32 	%f301, %r206;
	add.f32 	%f302, %f300, %f301;
	mov.b32 	%r207, %f302;
	shfl.sync.bfly.b32 	%r209|%p58, %r207, %r191, %r186, %r188;
	mov.b32 	%f303, %r209;
	add.f32 	%f304, %f302, %f303;
	mov.b32 	%r210, %f304;
	shfl.sync.bfly.b32 	%r212|%p59, %r210, %r194, %r186, %r188;
	mov.b32 	%f305, %r212;
	add.f32 	%f306, %f304, %f305;
	mov.b32 	%r213, %f306;
	shfl.sync.bfly.b32 	%r215|%p60, %r213, %r197, %r186, %r188;
	mov.b32 	%f307, %r215;
	add.f32 	%f308, %f306, %f307;
	mov.b32 	%r216, %f308;
	shfl.sync.bfly.b32 	%r218|%p61, %r216, %r200, %r186, %r188;
	mov.b32 	%f309, %r218;
	add.f32 	%f310, %f308, %f309;
	st.local.f32 	[%rd3+16], %f310;

$L__BB12_21:
	bar.sync 	0;
	setp.gt.s32 	%p62, %r3, 4;
	@%p62 bra 	$L__BB12_23;

	mul.wide.s32 	%rd62, %r3, 4;
	add.s64 	%rd63, %rd3, %rd62;
	ld.local.f32 	%f311, [%rd63];
	mad.lo.s32 	%r219, %r3, %r17, %r2;
	cvt.s64.s32 	%rd64, %r219;
	mul.lo.s32 	%r220, %r1, %r18;
	cvt.s64.s32 	%rd65, %r220;
	add.s64 	%rd66, %rd65, %rd64;
	cvta.to.global.u64 	%rd67, %rd27;
	shl.b64 	%rd68, %rd66, 2;
	add.s64 	%rd69, %rd67, %rd68;
	st.global.f32 	[%rd69], %f311;

$L__BB12_23:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_6_bs_64
.visible .entry ggml_matvec_f32_ncols_6_bs_64(
	.param .u64 ggml_matvec_f32_ncols_6_bs_64_param_0,
	.param .u64 ggml_matvec_f32_ncols_6_bs_64_param_1,
	.param .u64 ggml_matvec_f32_ncols_6_bs_64_param_2,
	.param .u32 ggml_matvec_f32_ncols_6_bs_64_param_3,
	.param .u32 ggml_matvec_f32_ncols_6_bs_64_param_4,
	.param .u32 ggml_matvec_f32_ncols_6_bs_64_param_5,
	.param .u32 ggml_matvec_f32_ncols_6_bs_64_param_6,
	.param .u32 ggml_matvec_f32_ncols_6_bs_64_param_7,
	.param .u32 ggml_matvec_f32_ncols_6_bs_64_param_8,
	.param .u32 ggml_matvec_f32_ncols_6_bs_64_param_9,
	.param .u32 ggml_matvec_f32_ncols_6_bs_64_param_10,
	.param .u32 ggml_matvec_f32_ncols_6_bs_64_param_11
)
{
	.local .align 8 .b8 	__local_depot13[24];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<73>;
	.reg .f32 	%f<296>;
	.reg .b32 	%r<253>;
	.reg .b64 	%rd<67>;


	mov.u64 	%SPL, __local_depot13;
	ld.param.u64 	%rd20, [ggml_matvec_f32_ncols_6_bs_64_param_0];
	ld.param.u64 	%rd21, [ggml_matvec_f32_ncols_6_bs_64_param_1];
	ld.param.u64 	%rd19, [ggml_matvec_f32_ncols_6_bs_64_param_2];
	ld.param.u32 	%r11, [ggml_matvec_f32_ncols_6_bs_64_param_3];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_6_bs_64_param_5];
	ld.param.u32 	%r12, [ggml_matvec_f32_ncols_6_bs_64_param_6];
	ld.param.u32 	%r13, [ggml_matvec_f32_ncols_6_bs_64_param_7];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_6_bs_64_param_8];
	ld.param.u32 	%r17, [ggml_matvec_f32_ncols_6_bs_64_param_9];
	ld.param.u32 	%r18, [ggml_matvec_f32_ncols_6_bs_64_param_10];
	ld.param.u32 	%r14, [ggml_matvec_f32_ncols_6_bs_64_param_11];
	cvta.to.global.u64 	%rd66, %rd21;
	add.u64 	%rd2, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r19, %r1, %r16;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r20, %r2, %r15;
	mad.lo.s32 	%r21, %r19, %r17, %r20;
	cvt.s64.s32 	%rd3, %r21;
	cvta.to.global.u64 	%rd4, %rd20;
	mul.lo.s32 	%r22, %r1, %r18;
	cvt.s64.s32 	%rd5, %r22;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r23, %r3, 2;
	mov.u32 	%r24, data_mmv;
	add.s32 	%r4, %r24, %r23;
	@%p1 bra 	$L__BB13_2;

	mov.u32 	%r25, 0;
	st.shared.u32 	[%r4], %r25;

$L__BB13_2:
	bar.sync 	0;
	mov.f32 	%f290, 0f00000000;
	st.local.v2.f32 	[%rd2], {%f290, %f290};
	st.local.v2.f32 	[%rd2+8], {%f290, %f290};
	st.local.v2.f32 	[%rd2+16], {%f290, %f290};
	setp.ge.s32 	%p2, %r3, %r11;
	mov.f32 	%f291, %f290;
	mov.f32 	%f292, %f290;
	mov.f32 	%f293, %f290;
	mov.f32 	%f294, %f290;
	mov.f32 	%f295, %f290;
	@%p2 bra 	$L__BB13_9;

	not.b32 	%r26, %r3;
	add.s32 	%r5, %r26, %r11;
	and.b32  	%r27, %r5, 64;
	setp.ne.s32 	%p3, %r27, 0;
	mov.f32 	%f290, 0f00000000;
	mov.u32 	%r252, %r3;
	@%p3 bra 	$L__BB13_5;

	shl.b64 	%rd23, %rd5, 2;
	add.s64 	%rd24, %rd66, %rd23;
	shl.b64 	%rd25, %rd3, 2;
	add.s64 	%rd26, %rd4, %rd25;
	mul.wide.s32 	%rd27, %r3, 8;
	add.s64 	%rd28, %rd26, %rd27;
	ld.global.nc.v2.f32 	{%f43, %f44}, [%rd28];
	add.s64 	%rd29, %rd24, %rd27;
	ld.global.nc.v2.f32 	{%f47, %f48}, [%rd29];
	fma.rn.f32 	%f51, %f43, %f47, 0f00000000;
	fma.rn.f32 	%f295, %f44, %f48, %f51;
	mul.wide.s32 	%rd30, %r12, 8;
	add.s64 	%rd31, %rd29, %rd30;
	ld.global.nc.v2.f32 	{%f52, %f53}, [%rd31];
	fma.rn.f32 	%f56, %f43, %f52, 0f00000000;
	fma.rn.f32 	%f294, %f44, %f53, %f56;
	st.local.v2.f32 	[%rd2], {%f295, %f294};
	add.s32 	%r28, %r3, %r12;
	add.s32 	%r29, %r28, %r12;
	mul.wide.s32 	%rd32, %r29, 8;
	add.s64 	%rd33, %rd24, %rd32;
	ld.global.nc.v2.f32 	{%f57, %f58}, [%rd33];
	fma.rn.f32 	%f61, %f43, %f57, 0f00000000;
	fma.rn.f32 	%f293, %f44, %f58, %f61;
	add.s64 	%rd34, %rd33, %rd30;
	ld.global.nc.v2.f32 	{%f62, %f63}, [%rd34];
	fma.rn.f32 	%f66, %f43, %f62, 0f00000000;
	fma.rn.f32 	%f292, %f44, %f63, %f66;
	st.local.v2.f32 	[%rd2+8], {%f293, %f292};
	add.s64 	%rd35, %rd34, %rd30;
	ld.global.nc.v2.f32 	{%f67, %f68}, [%rd35];
	fma.rn.f32 	%f71, %f43, %f67, 0f00000000;
	fma.rn.f32 	%f291, %f44, %f68, %f71;
	add.s64 	%rd36, %rd35, %rd30;
	ld.global.nc.v2.f32 	{%f72, %f73}, [%rd36];
	fma.rn.f32 	%f76, %f43, %f72, 0f00000000;
	fma.rn.f32 	%f290, %f44, %f73, %f76;
	st.local.v2.f32 	[%rd2+16], {%f291, %f290};
	add.s32 	%r252, %r3, 64;

$L__BB13_5:
	and.b32  	%r30, %r5, -64;
	setp.eq.s32 	%p4, %r30, 0;
	@%p4 bra 	$L__BB13_9;

	add.s32 	%r31, %r252, %r12;
	add.s32 	%r32, %r31, 64;
	mul.wide.s32 	%rd37, %r32, 8;
	shl.b64 	%rd38, %rd5, 2;
	add.s64 	%rd7, %rd37, %rd38;
	shl.b32 	%r33, %r12, 1;
	add.s32 	%r34, %r252, %r33;
	mad.lo.s32 	%r35, %r12, 3, %r252;
	shl.b32 	%r36, %r12, 2;
	add.s32 	%r37, %r252, %r36;
	mad.lo.s32 	%r38, %r12, 5, %r252;
	mul.wide.s32 	%rd39, %r34, 8;
	add.s64 	%rd8, %rd39, %rd38;
	mul.wide.s32 	%rd40, %r35, 8;
	add.s64 	%rd9, %rd40, %rd38;
	mul.wide.s32 	%rd41, %r37, 8;
	add.s64 	%rd10, %rd41, %rd38;
	mul.wide.s32 	%rd42, %r38, 8;
	add.s64 	%rd11, %rd42, %rd38;
	mul.wide.s32 	%rd43, %r252, 2;
	add.s64 	%rd44, %rd43, %rd3;
	shl.b64 	%rd45, %rd44, 2;
	add.s64 	%rd46, %rd4, %rd45;
	add.s64 	%rd65, %rd46, 512;
	mul.wide.s32 	%rd47, %r252, 8;
	mul.wide.s32 	%rd48, %r12, 8;
	add.s64 	%rd49, %rd47, %rd48;
	add.s64 	%rd13, %rd49, %rd38;
	add.s64 	%rd14, %rd47, %rd38;

$L__BB13_7:
	ld.global.nc.v2.f32 	{%f77, %f78}, [%rd65+-512];
	add.s64 	%rd50, %rd66, %rd14;
	ld.global.nc.v2.f32 	{%f81, %f82}, [%rd50];
	fma.rn.f32 	%f85, %f77, %f81, %f295;
	fma.rn.f32 	%f86, %f78, %f82, %f85;
	add.s64 	%rd51, %rd66, %rd13;
	ld.global.nc.v2.f32 	{%f87, %f88}, [%rd51];
	fma.rn.f32 	%f91, %f77, %f87, %f294;
	fma.rn.f32 	%f92, %f78, %f88, %f91;
	add.s64 	%rd52, %rd66, %rd8;
	ld.global.nc.v2.f32 	{%f93, %f94}, [%rd52];
	fma.rn.f32 	%f97, %f77, %f93, %f293;
	fma.rn.f32 	%f98, %f78, %f94, %f97;
	add.s64 	%rd53, %rd66, %rd9;
	ld.global.nc.v2.f32 	{%f99, %f100}, [%rd53];
	fma.rn.f32 	%f103, %f77, %f99, %f292;
	fma.rn.f32 	%f104, %f78, %f100, %f103;
	add.s64 	%rd54, %rd66, %rd10;
	ld.global.nc.v2.f32 	{%f105, %f106}, [%rd54];
	fma.rn.f32 	%f109, %f77, %f105, %f291;
	fma.rn.f32 	%f110, %f78, %f106, %f109;
	add.s64 	%rd55, %rd66, %rd11;
	ld.global.nc.v2.f32 	{%f111, %f112}, [%rd55];
	fma.rn.f32 	%f115, %f77, %f111, %f290;
	fma.rn.f32 	%f116, %f78, %f112, %f115;
	ld.global.nc.v2.f32 	{%f117, %f118}, [%rd65];
	ld.global.nc.v2.f32 	{%f121, %f122}, [%rd50+512];
	fma.rn.f32 	%f125, %f117, %f121, %f86;
	fma.rn.f32 	%f295, %f118, %f122, %f125;
	add.s64 	%rd56, %rd66, %rd7;
	ld.global.nc.v2.f32 	{%f126, %f127}, [%rd56];
	fma.rn.f32 	%f130, %f117, %f126, %f92;
	fma.rn.f32 	%f294, %f118, %f127, %f130;
	ld.global.nc.v2.f32 	{%f131, %f132}, [%rd52+512];
	fma.rn.f32 	%f135, %f117, %f131, %f98;
	fma.rn.f32 	%f293, %f118, %f132, %f135;
	ld.global.nc.v2.f32 	{%f136, %f137}, [%rd53+512];
	fma.rn.f32 	%f140, %f117, %f136, %f104;
	fma.rn.f32 	%f292, %f118, %f137, %f140;
	ld.global.nc.v2.f32 	{%f141, %f142}, [%rd54+512];
	fma.rn.f32 	%f145, %f117, %f141, %f110;
	fma.rn.f32 	%f291, %f118, %f142, %f145;
	ld.global.nc.v2.f32 	{%f146, %f147}, [%rd55+512];
	fma.rn.f32 	%f150, %f117, %f146, %f116;
	fma.rn.f32 	%f290, %f118, %f147, %f150;
	add.s64 	%rd66, %rd66, 1024;
	add.s64 	%rd65, %rd65, 1024;
	add.s32 	%r252, %r252, 128;
	setp.lt.s32 	%p5, %r252, %r11;
	@%p5 bra 	$L__BB13_7;

	st.local.v2.f32 	[%rd2], {%f295, %f294};
	st.local.v2.f32 	[%rd2+8], {%f293, %f292};
	st.local.v2.f32 	[%rd2+16], {%f291, %f290};

$L__BB13_9:
	shr.s32 	%r39, %r3, 31;
	shr.u32 	%r40, %r39, 27;
	add.s32 	%r41, %r3, %r40;
	shr.s32 	%r42, %r41, 5;
	shl.b32 	%r43, %r42, 2;
	add.s32 	%r10, %r24, %r43;
	mov.u32 	%r45, 2;
	mov.b32 	%r46, %f295;
	mov.u32 	%r47, 31;
	mov.u32 	%r48, 16;
	mov.u32 	%r49, -1;
	shfl.sync.bfly.b32 	%r50|%p6, %r46, %r48, %r47, %r49;
	mov.b32 	%f151, %r50;
	add.f32 	%f152, %f295, %f151;
	mov.b32 	%r51, %f152;
	mov.u32 	%r52, 8;
	shfl.sync.bfly.b32 	%r53|%p7, %r51, %r52, %r47, %r49;
	mov.b32 	%f153, %r53;
	add.f32 	%f154, %f152, %f153;
	mov.b32 	%r54, %f154;
	mov.u32 	%r55, 4;
	shfl.sync.bfly.b32 	%r56|%p8, %r54, %r55, %r47, %r49;
	mov.b32 	%f155, %r56;
	add.f32 	%f156, %f154, %f155;
	mov.b32 	%r57, %f156;
	shfl.sync.bfly.b32 	%r58|%p9, %r57, %r45, %r47, %r49;
	mov.b32 	%f157, %r58;
	add.f32 	%f158, %f156, %f157;
	mov.b32 	%r59, %f158;
	mov.u32 	%r60, 1;
	shfl.sync.bfly.b32 	%r61|%p10, %r59, %r60, %r47, %r49;
	mov.b32 	%f159, %r61;
	add.f32 	%f160, %f158, %f159;
	st.local.f32 	[%rd2], %f160;
	st.shared.f32 	[%r10], %f160;
	bar.sync 	0;
	@%p1 bra 	$L__BB13_11;

	ld.shared.f32 	%f161, [%r4];
	mov.b32 	%r62, %f161;
	shfl.sync.bfly.b32 	%r66|%p12, %r62, %r48, %r47, %r49;
	mov.b32 	%f162, %r66;
	add.f32 	%f163, %f161, %f162;
	mov.b32 	%r67, %f163;
	shfl.sync.bfly.b32 	%r69|%p13, %r67, %r52, %r47, %r49;
	mov.b32 	%f164, %r69;
	add.f32 	%f165, %f163, %f164;
	mov.b32 	%r70, %f165;
	shfl.sync.bfly.b32 	%r72|%p14, %r70, %r55, %r47, %r49;
	mov.b32 	%f166, %r72;
	add.f32 	%f167, %f165, %f166;
	mov.b32 	%r73, %f167;
	shfl.sync.bfly.b32 	%r75|%p15, %r73, %r45, %r47, %r49;
	mov.b32 	%f168, %r75;
	add.f32 	%f169, %f167, %f168;
	mov.b32 	%r76, %f169;
	shfl.sync.bfly.b32 	%r78|%p16, %r76, %r60, %r47, %r49;
	mov.b32 	%f170, %r78;
	add.f32 	%f171, %f169, %f170;
	st.local.f32 	[%rd2], %f171;

$L__BB13_11:
	bar.sync 	0;
	mov.b32 	%r79, %f294;
	shfl.sync.bfly.b32 	%r83|%p18, %r79, %r48, %r47, %r49;
	mov.b32 	%f172, %r83;
	add.f32 	%f173, %f294, %f172;
	mov.b32 	%r84, %f173;
	shfl.sync.bfly.b32 	%r86|%p19, %r84, %r52, %r47, %r49;
	mov.b32 	%f174, %r86;
	add.f32 	%f175, %f173, %f174;
	mov.b32 	%r87, %f175;
	shfl.sync.bfly.b32 	%r89|%p20, %r87, %r55, %r47, %r49;
	mov.b32 	%f176, %r89;
	add.f32 	%f177, %f175, %f176;
	mov.b32 	%r90, %f177;
	shfl.sync.bfly.b32 	%r92|%p21, %r90, %r45, %r47, %r49;
	mov.b32 	%f178, %r92;
	add.f32 	%f179, %f177, %f178;
	mov.b32 	%r93, %f179;
	shfl.sync.bfly.b32 	%r95|%p22, %r93, %r60, %r47, %r49;
	mov.b32 	%f180, %r95;
	add.f32 	%f181, %f179, %f180;
	st.local.f32 	[%rd2+4], %f181;
	st.shared.f32 	[%r10], %f181;
	bar.sync 	0;
	@%p1 bra 	$L__BB13_13;

	ld.shared.f32 	%f182, [%r4];
	mov.b32 	%r96, %f182;
	mov.u32 	%r97, 31;
	mov.u32 	%r98, 16;
	mov.u32 	%r99, -1;
	shfl.sync.bfly.b32 	%r100|%p23, %r96, %r98, %r97, %r99;
	mov.b32 	%f183, %r100;
	add.f32 	%f184, %f182, %f183;
	mov.b32 	%r101, %f184;
	mov.u32 	%r102, 8;
	shfl.sync.bfly.b32 	%r103|%p24, %r101, %r102, %r97, %r99;
	mov.b32 	%f185, %r103;
	add.f32 	%f186, %f184, %f185;
	mov.b32 	%r104, %f186;
	mov.u32 	%r105, 4;
	shfl.sync.bfly.b32 	%r106|%p25, %r104, %r105, %r97, %r99;
	mov.b32 	%f187, %r106;
	add.f32 	%f188, %f186, %f187;
	mov.b32 	%r107, %f188;
	mov.u32 	%r108, 2;
	shfl.sync.bfly.b32 	%r109|%p26, %r107, %r108, %r97, %r99;
	mov.b32 	%f189, %r109;
	add.f32 	%f190, %f188, %f189;
	mov.b32 	%r110, %f190;
	mov.u32 	%r111, 1;
	shfl.sync.bfly.b32 	%r112|%p27, %r110, %r111, %r97, %r99;
	mov.b32 	%f191, %r112;
	add.f32 	%f192, %f190, %f191;
	st.local.f32 	[%rd2+4], %f192;

$L__BB13_13:
	bar.sync 	0;
	mov.b32 	%r113, %f293;
	mov.u32 	%r114, 31;
	mov.u32 	%r115, 16;
	mov.u32 	%r116, -1;
	shfl.sync.bfly.b32 	%r117|%p29, %r113, %r115, %r114, %r116;
	mov.b32 	%f193, %r117;
	add.f32 	%f194, %f293, %f193;
	mov.b32 	%r118, %f194;
	mov.u32 	%r119, 8;
	shfl.sync.bfly.b32 	%r120|%p30, %r118, %r119, %r114, %r116;
	mov.b32 	%f195, %r120;
	add.f32 	%f196, %f194, %f195;
	mov.b32 	%r121, %f196;
	mov.u32 	%r122, 4;
	shfl.sync.bfly.b32 	%r123|%p31, %r121, %r122, %r114, %r116;
	mov.b32 	%f197, %r123;
	add.f32 	%f198, %f196, %f197;
	mov.b32 	%r124, %f198;
	mov.u32 	%r125, 2;
	shfl.sync.bfly.b32 	%r126|%p32, %r124, %r125, %r114, %r116;
	mov.b32 	%f199, %r126;
	add.f32 	%f200, %f198, %f199;
	mov.b32 	%r127, %f200;
	mov.u32 	%r128, 1;
	shfl.sync.bfly.b32 	%r129|%p33, %r127, %r128, %r114, %r116;
	mov.b32 	%f201, %r129;
	add.f32 	%f202, %f200, %f201;
	st.local.f32 	[%rd2+8], %f202;
	st.shared.f32 	[%r10], %f202;
	bar.sync 	0;
	@%p1 bra 	$L__BB13_15;

	ld.shared.f32 	%f203, [%r4];
	mov.b32 	%r130, %f203;
	shfl.sync.bfly.b32 	%r134|%p34, %r130, %r115, %r114, %r116;
	mov.b32 	%f204, %r134;
	add.f32 	%f205, %f203, %f204;
	mov.b32 	%r135, %f205;
	shfl.sync.bfly.b32 	%r137|%p35, %r135, %r119, %r114, %r116;
	mov.b32 	%f206, %r137;
	add.f32 	%f207, %f205, %f206;
	mov.b32 	%r138, %f207;
	shfl.sync.bfly.b32 	%r140|%p36, %r138, %r122, %r114, %r116;
	mov.b32 	%f208, %r140;
	add.f32 	%f209, %f207, %f208;
	mov.b32 	%r141, %f209;
	shfl.sync.bfly.b32 	%r143|%p37, %r141, %r125, %r114, %r116;
	mov.b32 	%f210, %r143;
	add.f32 	%f211, %f209, %f210;
	mov.b32 	%r144, %f211;
	shfl.sync.bfly.b32 	%r146|%p38, %r144, %r128, %r114, %r116;
	mov.b32 	%f212, %r146;
	add.f32 	%f213, %f211, %f212;
	st.local.f32 	[%rd2+8], %f213;

$L__BB13_15:
	bar.sync 	0;
	mov.b32 	%r147, %f292;
	shfl.sync.bfly.b32 	%r151|%p40, %r147, %r115, %r114, %r116;
	mov.b32 	%f214, %r151;
	add.f32 	%f215, %f292, %f214;
	mov.b32 	%r152, %f215;
	shfl.sync.bfly.b32 	%r154|%p41, %r152, %r119, %r114, %r116;
	mov.b32 	%f216, %r154;
	add.f32 	%f217, %f215, %f216;
	mov.b32 	%r155, %f217;
	shfl.sync.bfly.b32 	%r157|%p42, %r155, %r122, %r114, %r116;
	mov.b32 	%f218, %r157;
	add.f32 	%f219, %f217, %f218;
	mov.b32 	%r158, %f219;
	shfl.sync.bfly.b32 	%r160|%p43, %r158, %r125, %r114, %r116;
	mov.b32 	%f220, %r160;
	add.f32 	%f221, %f219, %f220;
	mov.b32 	%r161, %f221;
	shfl.sync.bfly.b32 	%r163|%p44, %r161, %r128, %r114, %r116;
	mov.b32 	%f222, %r163;
	add.f32 	%f223, %f221, %f222;
	st.local.f32 	[%rd2+12], %f223;
	st.shared.f32 	[%r10], %f223;
	bar.sync 	0;
	@%p1 bra 	$L__BB13_17;

	ld.shared.f32 	%f224, [%r4];
	mov.b32 	%r164, %f224;
	mov.u32 	%r165, 31;
	mov.u32 	%r166, 16;
	mov.u32 	%r167, -1;
	shfl.sync.bfly.b32 	%r168|%p45, %r164, %r166, %r165, %r167;
	mov.b32 	%f225, %r168;
	add.f32 	%f226, %f224, %f225;
	mov.b32 	%r169, %f226;
	mov.u32 	%r170, 8;
	shfl.sync.bfly.b32 	%r171|%p46, %r169, %r170, %r165, %r167;
	mov.b32 	%f227, %r171;
	add.f32 	%f228, %f226, %f227;
	mov.b32 	%r172, %f228;
	mov.u32 	%r173, 4;
	shfl.sync.bfly.b32 	%r174|%p47, %r172, %r173, %r165, %r167;
	mov.b32 	%f229, %r174;
	add.f32 	%f230, %f228, %f229;
	mov.b32 	%r175, %f230;
	mov.u32 	%r176, 2;
	shfl.sync.bfly.b32 	%r177|%p48, %r175, %r176, %r165, %r167;
	mov.b32 	%f231, %r177;
	add.f32 	%f232, %f230, %f231;
	mov.b32 	%r178, %f232;
	mov.u32 	%r179, 1;
	shfl.sync.bfly.b32 	%r180|%p49, %r178, %r179, %r165, %r167;
	mov.b32 	%f233, %r180;
	add.f32 	%f234, %f232, %f233;
	st.local.f32 	[%rd2+12], %f234;

$L__BB13_17:
	bar.sync 	0;
	mov.b32 	%r181, %f291;
	mov.u32 	%r182, 31;
	mov.u32 	%r183, 16;
	mov.u32 	%r184, -1;
	shfl.sync.bfly.b32 	%r185|%p51, %r181, %r183, %r182, %r184;
	mov.b32 	%f235, %r185;
	add.f32 	%f236, %f291, %f235;
	mov.b32 	%r186, %f236;
	mov.u32 	%r187, 8;
	shfl.sync.bfly.b32 	%r188|%p52, %r186, %r187, %r182, %r184;
	mov.b32 	%f237, %r188;
	add.f32 	%f238, %f236, %f237;
	mov.b32 	%r189, %f238;
	mov.u32 	%r190, 4;
	shfl.sync.bfly.b32 	%r191|%p53, %r189, %r190, %r182, %r184;
	mov.b32 	%f239, %r191;
	add.f32 	%f240, %f238, %f239;
	mov.b32 	%r192, %f240;
	mov.u32 	%r193, 2;
	shfl.sync.bfly.b32 	%r194|%p54, %r192, %r193, %r182, %r184;
	mov.b32 	%f241, %r194;
	add.f32 	%f242, %f240, %f241;
	mov.b32 	%r195, %f242;
	mov.u32 	%r196, 1;
	shfl.sync.bfly.b32 	%r197|%p55, %r195, %r196, %r182, %r184;
	mov.b32 	%f243, %r197;
	add.f32 	%f244, %f242, %f243;
	st.local.f32 	[%rd2+16], %f244;
	st.shared.f32 	[%r10], %f244;
	bar.sync 	0;
	@%p1 bra 	$L__BB13_19;

	ld.shared.f32 	%f245, [%r4];
	mov.b32 	%r198, %f245;
	shfl.sync.bfly.b32 	%r202|%p56, %r198, %r183, %r182, %r184;
	mov.b32 	%f246, %r202;
	add.f32 	%f247, %f245, %f246;
	mov.b32 	%r203, %f247;
	shfl.sync.bfly.b32 	%r205|%p57, %r203, %r187, %r182, %r184;
	mov.b32 	%f248, %r205;
	add.f32 	%f249, %f247, %f248;
	mov.b32 	%r206, %f249;
	shfl.sync.bfly.b32 	%r208|%p58, %r206, %r190, %r182, %r184;
	mov.b32 	%f250, %r208;
	add.f32 	%f251, %f249, %f250;
	mov.b32 	%r209, %f251;
	shfl.sync.bfly.b32 	%r211|%p59, %r209, %r193, %r182, %r184;
	mov.b32 	%f252, %r211;
	add.f32 	%f253, %f251, %f252;
	mov.b32 	%r212, %f253;
	shfl.sync.bfly.b32 	%r214|%p60, %r212, %r196, %r182, %r184;
	mov.b32 	%f254, %r214;
	add.f32 	%f255, %f253, %f254;
	st.local.f32 	[%rd2+16], %f255;

$L__BB13_19:
	bar.sync 	0;
	mov.b32 	%r215, %f290;
	shfl.sync.bfly.b32 	%r219|%p62, %r215, %r183, %r182, %r184;
	mov.b32 	%f256, %r219;
	add.f32 	%f257, %f290, %f256;
	mov.b32 	%r220, %f257;
	shfl.sync.bfly.b32 	%r222|%p63, %r220, %r187, %r182, %r184;
	mov.b32 	%f258, %r222;
	add.f32 	%f259, %f257, %f258;
	mov.b32 	%r223, %f259;
	shfl.sync.bfly.b32 	%r225|%p64, %r223, %r190, %r182, %r184;
	mov.b32 	%f260, %r225;
	add.f32 	%f261, %f259, %f260;
	mov.b32 	%r226, %f261;
	shfl.sync.bfly.b32 	%r228|%p65, %r226, %r193, %r182, %r184;
	mov.b32 	%f262, %r228;
	add.f32 	%f263, %f261, %f262;
	mov.b32 	%r229, %f263;
	shfl.sync.bfly.b32 	%r231|%p66, %r229, %r196, %r182, %r184;
	mov.b32 	%f264, %r231;
	add.f32 	%f265, %f263, %f264;
	st.local.f32 	[%rd2+20], %f265;
	st.shared.f32 	[%r10], %f265;
	bar.sync 	0;
	@%p1 bra 	$L__BB13_21;

	ld.shared.f32 	%f266, [%r4];
	mov.b32 	%r232, %f266;
	mov.u32 	%r233, 31;
	mov.u32 	%r234, 16;
	mov.u32 	%r235, -1;
	shfl.sync.bfly.b32 	%r236|%p67, %r232, %r234, %r233, %r235;
	mov.b32 	%f267, %r236;
	add.f32 	%f268, %f266, %f267;
	mov.b32 	%r237, %f268;
	mov.u32 	%r238, 8;
	shfl.sync.bfly.b32 	%r239|%p68, %r237, %r238, %r233, %r235;
	mov.b32 	%f269, %r239;
	add.f32 	%f270, %f268, %f269;
	mov.b32 	%r240, %f270;
	mov.u32 	%r241, 4;
	shfl.sync.bfly.b32 	%r242|%p69, %r240, %r241, %r233, %r235;
	mov.b32 	%f271, %r242;
	add.f32 	%f272, %f270, %f271;
	mov.b32 	%r243, %f272;
	mov.u32 	%r244, 2;
	shfl.sync.bfly.b32 	%r245|%p70, %r243, %r244, %r233, %r235;
	mov.b32 	%f273, %r245;
	add.f32 	%f274, %f272, %f273;
	mov.b32 	%r246, %f274;
	mov.u32 	%r247, 1;
	shfl.sync.bfly.b32 	%r248|%p71, %r246, %r247, %r233, %r235;
	mov.b32 	%f275, %r248;
	add.f32 	%f276, %f274, %f275;
	st.local.f32 	[%rd2+20], %f276;

$L__BB13_21:
	bar.sync 	0;
	setp.gt.s32 	%p72, %r3, 5;
	@%p72 bra 	$L__BB13_23;

	mul.wide.s32 	%rd57, %r3, 4;
	add.s64 	%rd58, %rd2, %rd57;
	ld.local.f32 	%f277, [%rd58];
	mad.lo.s32 	%r249, %r3, %r13, %r2;
	cvt.s64.s32 	%rd59, %r249;
	mul.lo.s32 	%r250, %r1, %r14;
	cvt.s64.s32 	%rd60, %r250;
	add.s64 	%rd61, %rd60, %rd59;
	cvta.to.global.u64 	%rd62, %rd19;
	shl.b64 	%rd63, %rd61, 2;
	add.s64 	%rd64, %rd62, %rd63;
	st.global.f32 	[%rd64], %f277;

$L__BB13_23:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_7_bs_64
.visible .entry ggml_matvec_f32_ncols_7_bs_64(
	.param .u64 ggml_matvec_f32_ncols_7_bs_64_param_0,
	.param .u64 ggml_matvec_f32_ncols_7_bs_64_param_1,
	.param .u64 ggml_matvec_f32_ncols_7_bs_64_param_2,
	.param .u32 ggml_matvec_f32_ncols_7_bs_64_param_3,
	.param .u32 ggml_matvec_f32_ncols_7_bs_64_param_4,
	.param .u32 ggml_matvec_f32_ncols_7_bs_64_param_5,
	.param .u32 ggml_matvec_f32_ncols_7_bs_64_param_6,
	.param .u32 ggml_matvec_f32_ncols_7_bs_64_param_7,
	.param .u32 ggml_matvec_f32_ncols_7_bs_64_param_8,
	.param .u32 ggml_matvec_f32_ncols_7_bs_64_param_9,
	.param .u32 ggml_matvec_f32_ncols_7_bs_64_param_10,
	.param .u32 ggml_matvec_f32_ncols_7_bs_64_param_11
)
{
	.local .align 4 .b8 	__local_depot14[28];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<84>;
	.reg .f32 	%f<343>;
	.reg .b32 	%r<289>;
	.reg .b64 	%rd<71>;


	mov.u64 	%SPL, __local_depot14;
	ld.param.u64 	%rd21, [ggml_matvec_f32_ncols_7_bs_64_param_0];
	ld.param.u64 	%rd22, [ggml_matvec_f32_ncols_7_bs_64_param_1];
	ld.param.u64 	%rd20, [ggml_matvec_f32_ncols_7_bs_64_param_2];
	ld.param.u32 	%r11, [ggml_matvec_f32_ncols_7_bs_64_param_3];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_7_bs_64_param_5];
	ld.param.u32 	%r12, [ggml_matvec_f32_ncols_7_bs_64_param_6];
	ld.param.u32 	%r13, [ggml_matvec_f32_ncols_7_bs_64_param_7];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_7_bs_64_param_8];
	ld.param.u32 	%r17, [ggml_matvec_f32_ncols_7_bs_64_param_9];
	ld.param.u32 	%r18, [ggml_matvec_f32_ncols_7_bs_64_param_10];
	ld.param.u32 	%r14, [ggml_matvec_f32_ncols_7_bs_64_param_11];
	cvta.to.global.u64 	%rd70, %rd22;
	add.u64 	%rd2, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r19, %r1, %r16;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r20, %r2, %r15;
	mad.lo.s32 	%r21, %r19, %r17, %r20;
	cvt.s64.s32 	%rd3, %r21;
	cvta.to.global.u64 	%rd4, %rd21;
	mul.lo.s32 	%r22, %r1, %r18;
	cvt.s64.s32 	%rd5, %r22;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r23, %r3, 2;
	mov.u32 	%r24, data_mmv;
	add.s32 	%r4, %r24, %r23;
	@%p1 bra 	$L__BB14_2;

	mov.u32 	%r25, 0;
	st.shared.u32 	[%r4], %r25;

$L__BB14_2:
	bar.sync 	0;
	mov.f32 	%f336, 0f00000000;
	mov.u32 	%r26, 0;
	st.local.u32 	[%rd2], %r26;
	st.local.u32 	[%rd2+4], %r26;
	st.local.u32 	[%rd2+8], %r26;
	st.local.u32 	[%rd2+12], %r26;
	st.local.u32 	[%rd2+16], %r26;
	st.local.u32 	[%rd2+20], %r26;
	st.local.u32 	[%rd2+24], %r26;
	setp.ge.s32 	%p2, %r3, %r11;
	mov.f32 	%f337, %f336;
	mov.f32 	%f338, %f336;
	mov.f32 	%f339, %f336;
	mov.f32 	%f340, %f336;
	mov.f32 	%f341, %f336;
	mov.f32 	%f342, %f336;
	@%p2 bra 	$L__BB14_9;

	not.b32 	%r27, %r3;
	add.s32 	%r5, %r27, %r11;
	and.b32  	%r28, %r5, 64;
	setp.ne.s32 	%p3, %r28, 0;
	mov.f32 	%f336, 0f00000000;
	mov.u32 	%r288, %r3;
	@%p3 bra 	$L__BB14_5;

	shl.b64 	%rd24, %rd5, 2;
	add.s64 	%rd25, %rd70, %rd24;
	shl.b64 	%rd26, %rd3, 2;
	add.s64 	%rd27, %rd4, %rd26;
	mul.wide.s32 	%rd28, %r3, 8;
	add.s64 	%rd29, %rd27, %rd28;
	ld.global.nc.v2.f32 	{%f50, %f51}, [%rd29];
	add.s64 	%rd30, %rd25, %rd28;
	ld.global.nc.v2.f32 	{%f54, %f55}, [%rd30];
	fma.rn.f32 	%f58, %f50, %f54, 0f00000000;
	fma.rn.f32 	%f342, %f51, %f55, %f58;
	st.local.f32 	[%rd2], %f342;
	mul.wide.s32 	%rd31, %r12, 8;
	add.s64 	%rd32, %rd30, %rd31;
	ld.global.nc.v2.f32 	{%f59, %f60}, [%rd32];
	fma.rn.f32 	%f63, %f50, %f59, 0f00000000;
	fma.rn.f32 	%f341, %f51, %f60, %f63;
	st.local.f32 	[%rd2+4], %f341;
	add.s32 	%r29, %r3, %r12;
	add.s32 	%r30, %r29, %r12;
	mul.wide.s32 	%rd33, %r30, 8;
	add.s64 	%rd34, %rd25, %rd33;
	ld.global.nc.v2.f32 	{%f64, %f65}, [%rd34];
	fma.rn.f32 	%f68, %f50, %f64, 0f00000000;
	fma.rn.f32 	%f340, %f51, %f65, %f68;
	st.local.f32 	[%rd2+8], %f340;
	add.s64 	%rd35, %rd34, %rd31;
	ld.global.nc.v2.f32 	{%f69, %f70}, [%rd35];
	fma.rn.f32 	%f73, %f50, %f69, 0f00000000;
	fma.rn.f32 	%f339, %f51, %f70, %f73;
	st.local.f32 	[%rd2+12], %f339;
	add.s64 	%rd36, %rd35, %rd31;
	ld.global.nc.v2.f32 	{%f74, %f75}, [%rd36];
	fma.rn.f32 	%f78, %f50, %f74, 0f00000000;
	fma.rn.f32 	%f338, %f51, %f75, %f78;
	st.local.f32 	[%rd2+16], %f338;
	add.s64 	%rd37, %rd36, %rd31;
	ld.global.nc.v2.f32 	{%f79, %f80}, [%rd37];
	fma.rn.f32 	%f83, %f50, %f79, 0f00000000;
	fma.rn.f32 	%f337, %f51, %f80, %f83;
	st.local.f32 	[%rd2+20], %f337;
	add.s64 	%rd38, %rd37, %rd31;
	ld.global.nc.v2.f32 	{%f84, %f85}, [%rd38];
	fma.rn.f32 	%f88, %f50, %f84, 0f00000000;
	fma.rn.f32 	%f336, %f51, %f85, %f88;
	st.local.f32 	[%rd2+24], %f336;
	add.s32 	%r288, %r3, 64;

$L__BB14_5:
	and.b32  	%r31, %r5, -64;
	setp.eq.s32 	%p4, %r31, 0;
	@%p4 bra 	$L__BB14_9;

	add.s32 	%r32, %r288, %r12;
	add.s32 	%r33, %r32, 64;
	mul.wide.s32 	%rd39, %r33, 8;
	shl.b64 	%rd40, %rd5, 2;
	add.s64 	%rd7, %rd39, %rd40;
	shl.b32 	%r34, %r12, 1;
	add.s32 	%r35, %r288, %r34;
	mad.lo.s32 	%r36, %r12, 3, %r288;
	shl.b32 	%r37, %r12, 2;
	add.s32 	%r38, %r288, %r37;
	mad.lo.s32 	%r39, %r12, 5, %r288;
	mad.lo.s32 	%r40, %r12, 6, %r288;
	mul.wide.s32 	%rd41, %r35, 8;
	add.s64 	%rd8, %rd41, %rd40;
	mul.wide.s32 	%rd42, %r36, 8;
	add.s64 	%rd9, %rd42, %rd40;
	mul.wide.s32 	%rd43, %r38, 8;
	add.s64 	%rd10, %rd43, %rd40;
	mul.wide.s32 	%rd44, %r39, 8;
	add.s64 	%rd11, %rd44, %rd40;
	mul.wide.s32 	%rd45, %r40, 8;
	add.s64 	%rd12, %rd45, %rd40;
	mul.wide.s32 	%rd46, %r288, 2;
	add.s64 	%rd47, %rd46, %rd3;
	shl.b64 	%rd48, %rd47, 2;
	add.s64 	%rd49, %rd4, %rd48;
	add.s64 	%rd69, %rd49, 512;
	mul.wide.s32 	%rd50, %r288, 8;
	mul.wide.s32 	%rd51, %r12, 8;
	add.s64 	%rd52, %rd50, %rd51;
	add.s64 	%rd14, %rd52, %rd40;
	add.s64 	%rd15, %rd50, %rd40;

$L__BB14_7:
	ld.global.nc.v2.f32 	{%f89, %f90}, [%rd69+-512];
	add.s64 	%rd53, %rd70, %rd15;
	ld.global.nc.v2.f32 	{%f93, %f94}, [%rd53];
	fma.rn.f32 	%f97, %f89, %f93, %f342;
	fma.rn.f32 	%f98, %f90, %f94, %f97;
	add.s64 	%rd54, %rd70, %rd14;
	ld.global.nc.v2.f32 	{%f99, %f100}, [%rd54];
	fma.rn.f32 	%f103, %f89, %f99, %f341;
	fma.rn.f32 	%f104, %f90, %f100, %f103;
	add.s64 	%rd55, %rd70, %rd8;
	ld.global.nc.v2.f32 	{%f105, %f106}, [%rd55];
	fma.rn.f32 	%f109, %f89, %f105, %f340;
	fma.rn.f32 	%f110, %f90, %f106, %f109;
	add.s64 	%rd56, %rd70, %rd9;
	ld.global.nc.v2.f32 	{%f111, %f112}, [%rd56];
	fma.rn.f32 	%f115, %f89, %f111, %f339;
	fma.rn.f32 	%f116, %f90, %f112, %f115;
	add.s64 	%rd57, %rd70, %rd10;
	ld.global.nc.v2.f32 	{%f117, %f118}, [%rd57];
	fma.rn.f32 	%f121, %f89, %f117, %f338;
	fma.rn.f32 	%f122, %f90, %f118, %f121;
	add.s64 	%rd58, %rd70, %rd11;
	ld.global.nc.v2.f32 	{%f123, %f124}, [%rd58];
	fma.rn.f32 	%f127, %f89, %f123, %f337;
	fma.rn.f32 	%f128, %f90, %f124, %f127;
	add.s64 	%rd59, %rd70, %rd12;
	ld.global.nc.v2.f32 	{%f129, %f130}, [%rd59];
	fma.rn.f32 	%f133, %f89, %f129, %f336;
	fma.rn.f32 	%f134, %f90, %f130, %f133;
	ld.global.nc.v2.f32 	{%f135, %f136}, [%rd69];
	ld.global.nc.v2.f32 	{%f139, %f140}, [%rd53+512];
	fma.rn.f32 	%f143, %f135, %f139, %f98;
	fma.rn.f32 	%f342, %f136, %f140, %f143;
	add.s64 	%rd60, %rd70, %rd7;
	ld.global.nc.v2.f32 	{%f144, %f145}, [%rd60];
	fma.rn.f32 	%f148, %f135, %f144, %f104;
	fma.rn.f32 	%f341, %f136, %f145, %f148;
	ld.global.nc.v2.f32 	{%f149, %f150}, [%rd55+512];
	fma.rn.f32 	%f153, %f135, %f149, %f110;
	fma.rn.f32 	%f340, %f136, %f150, %f153;
	ld.global.nc.v2.f32 	{%f154, %f155}, [%rd56+512];
	fma.rn.f32 	%f158, %f135, %f154, %f116;
	fma.rn.f32 	%f339, %f136, %f155, %f158;
	ld.global.nc.v2.f32 	{%f159, %f160}, [%rd57+512];
	fma.rn.f32 	%f163, %f135, %f159, %f122;
	fma.rn.f32 	%f338, %f136, %f160, %f163;
	ld.global.nc.v2.f32 	{%f164, %f165}, [%rd58+512];
	fma.rn.f32 	%f168, %f135, %f164, %f128;
	fma.rn.f32 	%f337, %f136, %f165, %f168;
	ld.global.nc.v2.f32 	{%f169, %f170}, [%rd59+512];
	fma.rn.f32 	%f173, %f135, %f169, %f134;
	fma.rn.f32 	%f336, %f136, %f170, %f173;
	add.s64 	%rd70, %rd70, 1024;
	add.s64 	%rd69, %rd69, 1024;
	add.s32 	%r288, %r288, 128;
	setp.lt.s32 	%p5, %r288, %r11;
	@%p5 bra 	$L__BB14_7;

	st.local.f32 	[%rd2], %f342;
	st.local.f32 	[%rd2+4], %f341;
	st.local.f32 	[%rd2+8], %f340;
	st.local.f32 	[%rd2+12], %f339;
	st.local.f32 	[%rd2+16], %f338;
	st.local.f32 	[%rd2+20], %f337;
	st.local.f32 	[%rd2+24], %f336;

$L__BB14_9:
	shr.s32 	%r41, %r3, 31;
	shr.u32 	%r42, %r41, 27;
	add.s32 	%r43, %r3, %r42;
	shr.s32 	%r44, %r43, 5;
	shl.b32 	%r45, %r44, 2;
	add.s32 	%r10, %r24, %r45;
	mov.u32 	%r47, 2;
	mov.b32 	%r48, %f342;
	mov.u32 	%r49, 31;
	mov.u32 	%r50, 16;
	mov.u32 	%r51, -1;
	shfl.sync.bfly.b32 	%r52|%p6, %r48, %r50, %r49, %r51;
	mov.b32 	%f174, %r52;
	add.f32 	%f175, %f342, %f174;
	mov.b32 	%r53, %f175;
	mov.u32 	%r54, 8;
	shfl.sync.bfly.b32 	%r55|%p7, %r53, %r54, %r49, %r51;
	mov.b32 	%f176, %r55;
	add.f32 	%f177, %f175, %f176;
	mov.b32 	%r56, %f177;
	mov.u32 	%r57, 4;
	shfl.sync.bfly.b32 	%r58|%p8, %r56, %r57, %r49, %r51;
	mov.b32 	%f178, %r58;
	add.f32 	%f179, %f177, %f178;
	mov.b32 	%r59, %f179;
	shfl.sync.bfly.b32 	%r60|%p9, %r59, %r47, %r49, %r51;
	mov.b32 	%f180, %r60;
	add.f32 	%f181, %f179, %f180;
	mov.b32 	%r61, %f181;
	mov.u32 	%r62, 1;
	shfl.sync.bfly.b32 	%r63|%p10, %r61, %r62, %r49, %r51;
	mov.b32 	%f182, %r63;
	add.f32 	%f183, %f181, %f182;
	st.local.f32 	[%rd2], %f183;
	st.shared.f32 	[%r10], %f183;
	bar.sync 	0;
	@%p1 bra 	$L__BB14_11;

	ld.shared.f32 	%f184, [%r4];
	mov.b32 	%r64, %f184;
	shfl.sync.bfly.b32 	%r68|%p12, %r64, %r50, %r49, %r51;
	mov.b32 	%f185, %r68;
	add.f32 	%f186, %f184, %f185;
	mov.b32 	%r69, %f186;
	shfl.sync.bfly.b32 	%r71|%p13, %r69, %r54, %r49, %r51;
	mov.b32 	%f187, %r71;
	add.f32 	%f188, %f186, %f187;
	mov.b32 	%r72, %f188;
	shfl.sync.bfly.b32 	%r74|%p14, %r72, %r57, %r49, %r51;
	mov.b32 	%f189, %r74;
	add.f32 	%f190, %f188, %f189;
	mov.b32 	%r75, %f190;
	shfl.sync.bfly.b32 	%r77|%p15, %r75, %r47, %r49, %r51;
	mov.b32 	%f191, %r77;
	add.f32 	%f192, %f190, %f191;
	mov.b32 	%r78, %f192;
	shfl.sync.bfly.b32 	%r80|%p16, %r78, %r62, %r49, %r51;
	mov.b32 	%f193, %r80;
	add.f32 	%f194, %f192, %f193;
	st.local.f32 	[%rd2], %f194;

$L__BB14_11:
	bar.sync 	0;
	mov.b32 	%r81, %f341;
	shfl.sync.bfly.b32 	%r85|%p18, %r81, %r50, %r49, %r51;
	mov.b32 	%f195, %r85;
	add.f32 	%f196, %f341, %f195;
	mov.b32 	%r86, %f196;
	shfl.sync.bfly.b32 	%r88|%p19, %r86, %r54, %r49, %r51;
	mov.b32 	%f197, %r88;
	add.f32 	%f198, %f196, %f197;
	mov.b32 	%r89, %f198;
	shfl.sync.bfly.b32 	%r91|%p20, %r89, %r57, %r49, %r51;
	mov.b32 	%f199, %r91;
	add.f32 	%f200, %f198, %f199;
	mov.b32 	%r92, %f200;
	shfl.sync.bfly.b32 	%r94|%p21, %r92, %r47, %r49, %r51;
	mov.b32 	%f201, %r94;
	add.f32 	%f202, %f200, %f201;
	mov.b32 	%r95, %f202;
	shfl.sync.bfly.b32 	%r97|%p22, %r95, %r62, %r49, %r51;
	mov.b32 	%f203, %r97;
	add.f32 	%f204, %f202, %f203;
	st.local.f32 	[%rd2+4], %f204;
	st.shared.f32 	[%r10], %f204;
	bar.sync 	0;
	@%p1 bra 	$L__BB14_13;

	ld.shared.f32 	%f205, [%r4];
	mov.b32 	%r98, %f205;
	mov.u32 	%r99, 31;
	mov.u32 	%r100, 16;
	mov.u32 	%r101, -1;
	shfl.sync.bfly.b32 	%r102|%p23, %r98, %r100, %r99, %r101;
	mov.b32 	%f206, %r102;
	add.f32 	%f207, %f205, %f206;
	mov.b32 	%r103, %f207;
	mov.u32 	%r104, 8;
	shfl.sync.bfly.b32 	%r105|%p24, %r103, %r104, %r99, %r101;
	mov.b32 	%f208, %r105;
	add.f32 	%f209, %f207, %f208;
	mov.b32 	%r106, %f209;
	mov.u32 	%r107, 4;
	shfl.sync.bfly.b32 	%r108|%p25, %r106, %r107, %r99, %r101;
	mov.b32 	%f210, %r108;
	add.f32 	%f211, %f209, %f210;
	mov.b32 	%r109, %f211;
	mov.u32 	%r110, 2;
	shfl.sync.bfly.b32 	%r111|%p26, %r109, %r110, %r99, %r101;
	mov.b32 	%f212, %r111;
	add.f32 	%f213, %f211, %f212;
	mov.b32 	%r112, %f213;
	mov.u32 	%r113, 1;
	shfl.sync.bfly.b32 	%r114|%p27, %r112, %r113, %r99, %r101;
	mov.b32 	%f214, %r114;
	add.f32 	%f215, %f213, %f214;
	st.local.f32 	[%rd2+4], %f215;

$L__BB14_13:
	bar.sync 	0;
	mov.b32 	%r115, %f340;
	mov.u32 	%r116, 31;
	mov.u32 	%r117, 16;
	mov.u32 	%r118, -1;
	shfl.sync.bfly.b32 	%r119|%p29, %r115, %r117, %r116, %r118;
	mov.b32 	%f216, %r119;
	add.f32 	%f217, %f340, %f216;
	mov.b32 	%r120, %f217;
	mov.u32 	%r121, 8;
	shfl.sync.bfly.b32 	%r122|%p30, %r120, %r121, %r116, %r118;
	mov.b32 	%f218, %r122;
	add.f32 	%f219, %f217, %f218;
	mov.b32 	%r123, %f219;
	mov.u32 	%r124, 4;
	shfl.sync.bfly.b32 	%r125|%p31, %r123, %r124, %r116, %r118;
	mov.b32 	%f220, %r125;
	add.f32 	%f221, %f219, %f220;
	mov.b32 	%r126, %f221;
	mov.u32 	%r127, 2;
	shfl.sync.bfly.b32 	%r128|%p32, %r126, %r127, %r116, %r118;
	mov.b32 	%f222, %r128;
	add.f32 	%f223, %f221, %f222;
	mov.b32 	%r129, %f223;
	mov.u32 	%r130, 1;
	shfl.sync.bfly.b32 	%r131|%p33, %r129, %r130, %r116, %r118;
	mov.b32 	%f224, %r131;
	add.f32 	%f225, %f223, %f224;
	st.local.f32 	[%rd2+8], %f225;
	st.shared.f32 	[%r10], %f225;
	bar.sync 	0;
	@%p1 bra 	$L__BB14_15;

	ld.shared.f32 	%f226, [%r4];
	mov.b32 	%r132, %f226;
	shfl.sync.bfly.b32 	%r136|%p34, %r132, %r117, %r116, %r118;
	mov.b32 	%f227, %r136;
	add.f32 	%f228, %f226, %f227;
	mov.b32 	%r137, %f228;
	shfl.sync.bfly.b32 	%r139|%p35, %r137, %r121, %r116, %r118;
	mov.b32 	%f229, %r139;
	add.f32 	%f230, %f228, %f229;
	mov.b32 	%r140, %f230;
	shfl.sync.bfly.b32 	%r142|%p36, %r140, %r124, %r116, %r118;
	mov.b32 	%f231, %r142;
	add.f32 	%f232, %f230, %f231;
	mov.b32 	%r143, %f232;
	shfl.sync.bfly.b32 	%r145|%p37, %r143, %r127, %r116, %r118;
	mov.b32 	%f233, %r145;
	add.f32 	%f234, %f232, %f233;
	mov.b32 	%r146, %f234;
	shfl.sync.bfly.b32 	%r148|%p38, %r146, %r130, %r116, %r118;
	mov.b32 	%f235, %r148;
	add.f32 	%f236, %f234, %f235;
	st.local.f32 	[%rd2+8], %f236;

$L__BB14_15:
	bar.sync 	0;
	mov.b32 	%r149, %f339;
	shfl.sync.bfly.b32 	%r153|%p40, %r149, %r117, %r116, %r118;
	mov.b32 	%f237, %r153;
	add.f32 	%f238, %f339, %f237;
	mov.b32 	%r154, %f238;
	shfl.sync.bfly.b32 	%r156|%p41, %r154, %r121, %r116, %r118;
	mov.b32 	%f239, %r156;
	add.f32 	%f240, %f238, %f239;
	mov.b32 	%r157, %f240;
	shfl.sync.bfly.b32 	%r159|%p42, %r157, %r124, %r116, %r118;
	mov.b32 	%f241, %r159;
	add.f32 	%f242, %f240, %f241;
	mov.b32 	%r160, %f242;
	shfl.sync.bfly.b32 	%r162|%p43, %r160, %r127, %r116, %r118;
	mov.b32 	%f243, %r162;
	add.f32 	%f244, %f242, %f243;
	mov.b32 	%r163, %f244;
	shfl.sync.bfly.b32 	%r165|%p44, %r163, %r130, %r116, %r118;
	mov.b32 	%f245, %r165;
	add.f32 	%f246, %f244, %f245;
	st.local.f32 	[%rd2+12], %f246;
	st.shared.f32 	[%r10], %f246;
	bar.sync 	0;
	@%p1 bra 	$L__BB14_17;

	ld.shared.f32 	%f247, [%r4];
	mov.b32 	%r166, %f247;
	mov.u32 	%r167, 31;
	mov.u32 	%r168, 16;
	mov.u32 	%r169, -1;
	shfl.sync.bfly.b32 	%r170|%p45, %r166, %r168, %r167, %r169;
	mov.b32 	%f248, %r170;
	add.f32 	%f249, %f247, %f248;
	mov.b32 	%r171, %f249;
	mov.u32 	%r172, 8;
	shfl.sync.bfly.b32 	%r173|%p46, %r171, %r172, %r167, %r169;
	mov.b32 	%f250, %r173;
	add.f32 	%f251, %f249, %f250;
	mov.b32 	%r174, %f251;
	mov.u32 	%r175, 4;
	shfl.sync.bfly.b32 	%r176|%p47, %r174, %r175, %r167, %r169;
	mov.b32 	%f252, %r176;
	add.f32 	%f253, %f251, %f252;
	mov.b32 	%r177, %f253;
	mov.u32 	%r178, 2;
	shfl.sync.bfly.b32 	%r179|%p48, %r177, %r178, %r167, %r169;
	mov.b32 	%f254, %r179;
	add.f32 	%f255, %f253, %f254;
	mov.b32 	%r180, %f255;
	mov.u32 	%r181, 1;
	shfl.sync.bfly.b32 	%r182|%p49, %r180, %r181, %r167, %r169;
	mov.b32 	%f256, %r182;
	add.f32 	%f257, %f255, %f256;
	st.local.f32 	[%rd2+12], %f257;

$L__BB14_17:
	bar.sync 	0;
	mov.b32 	%r183, %f338;
	mov.u32 	%r184, 31;
	mov.u32 	%r185, 16;
	mov.u32 	%r186, -1;
	shfl.sync.bfly.b32 	%r187|%p51, %r183, %r185, %r184, %r186;
	mov.b32 	%f258, %r187;
	add.f32 	%f259, %f338, %f258;
	mov.b32 	%r188, %f259;
	mov.u32 	%r189, 8;
	shfl.sync.bfly.b32 	%r190|%p52, %r188, %r189, %r184, %r186;
	mov.b32 	%f260, %r190;
	add.f32 	%f261, %f259, %f260;
	mov.b32 	%r191, %f261;
	mov.u32 	%r192, 4;
	shfl.sync.bfly.b32 	%r193|%p53, %r191, %r192, %r184, %r186;
	mov.b32 	%f262, %r193;
	add.f32 	%f263, %f261, %f262;
	mov.b32 	%r194, %f263;
	mov.u32 	%r195, 2;
	shfl.sync.bfly.b32 	%r196|%p54, %r194, %r195, %r184, %r186;
	mov.b32 	%f264, %r196;
	add.f32 	%f265, %f263, %f264;
	mov.b32 	%r197, %f265;
	mov.u32 	%r198, 1;
	shfl.sync.bfly.b32 	%r199|%p55, %r197, %r198, %r184, %r186;
	mov.b32 	%f266, %r199;
	add.f32 	%f267, %f265, %f266;
	st.local.f32 	[%rd2+16], %f267;
	st.shared.f32 	[%r10], %f267;
	bar.sync 	0;
	@%p1 bra 	$L__BB14_19;

	ld.shared.f32 	%f268, [%r4];
	mov.b32 	%r200, %f268;
	shfl.sync.bfly.b32 	%r204|%p56, %r200, %r185, %r184, %r186;
	mov.b32 	%f269, %r204;
	add.f32 	%f270, %f268, %f269;
	mov.b32 	%r205, %f270;
	shfl.sync.bfly.b32 	%r207|%p57, %r205, %r189, %r184, %r186;
	mov.b32 	%f271, %r207;
	add.f32 	%f272, %f270, %f271;
	mov.b32 	%r208, %f272;
	shfl.sync.bfly.b32 	%r210|%p58, %r208, %r192, %r184, %r186;
	mov.b32 	%f273, %r210;
	add.f32 	%f274, %f272, %f273;
	mov.b32 	%r211, %f274;
	shfl.sync.bfly.b32 	%r213|%p59, %r211, %r195, %r184, %r186;
	mov.b32 	%f275, %r213;
	add.f32 	%f276, %f274, %f275;
	mov.b32 	%r214, %f276;
	shfl.sync.bfly.b32 	%r216|%p60, %r214, %r198, %r184, %r186;
	mov.b32 	%f277, %r216;
	add.f32 	%f278, %f276, %f277;
	st.local.f32 	[%rd2+16], %f278;

$L__BB14_19:
	bar.sync 	0;
	mov.b32 	%r217, %f337;
	shfl.sync.bfly.b32 	%r221|%p62, %r217, %r185, %r184, %r186;
	mov.b32 	%f279, %r221;
	add.f32 	%f280, %f337, %f279;
	mov.b32 	%r222, %f280;
	shfl.sync.bfly.b32 	%r224|%p63, %r222, %r189, %r184, %r186;
	mov.b32 	%f281, %r224;
	add.f32 	%f282, %f280, %f281;
	mov.b32 	%r225, %f282;
	shfl.sync.bfly.b32 	%r227|%p64, %r225, %r192, %r184, %r186;
	mov.b32 	%f283, %r227;
	add.f32 	%f284, %f282, %f283;
	mov.b32 	%r228, %f284;
	shfl.sync.bfly.b32 	%r230|%p65, %r228, %r195, %r184, %r186;
	mov.b32 	%f285, %r230;
	add.f32 	%f286, %f284, %f285;
	mov.b32 	%r231, %f286;
	shfl.sync.bfly.b32 	%r233|%p66, %r231, %r198, %r184, %r186;
	mov.b32 	%f287, %r233;
	add.f32 	%f288, %f286, %f287;
	st.local.f32 	[%rd2+20], %f288;
	st.shared.f32 	[%r10], %f288;
	bar.sync 	0;
	@%p1 bra 	$L__BB14_21;

	ld.shared.f32 	%f289, [%r4];
	mov.b32 	%r234, %f289;
	mov.u32 	%r235, 31;
	mov.u32 	%r236, 16;
	mov.u32 	%r237, -1;
	shfl.sync.bfly.b32 	%r238|%p67, %r234, %r236, %r235, %r237;
	mov.b32 	%f290, %r238;
	add.f32 	%f291, %f289, %f290;
	mov.b32 	%r239, %f291;
	mov.u32 	%r240, 8;
	shfl.sync.bfly.b32 	%r241|%p68, %r239, %r240, %r235, %r237;
	mov.b32 	%f292, %r241;
	add.f32 	%f293, %f291, %f292;
	mov.b32 	%r242, %f293;
	mov.u32 	%r243, 4;
	shfl.sync.bfly.b32 	%r244|%p69, %r242, %r243, %r235, %r237;
	mov.b32 	%f294, %r244;
	add.f32 	%f295, %f293, %f294;
	mov.b32 	%r245, %f295;
	mov.u32 	%r246, 2;
	shfl.sync.bfly.b32 	%r247|%p70, %r245, %r246, %r235, %r237;
	mov.b32 	%f296, %r247;
	add.f32 	%f297, %f295, %f296;
	mov.b32 	%r248, %f297;
	mov.u32 	%r249, 1;
	shfl.sync.bfly.b32 	%r250|%p71, %r248, %r249, %r235, %r237;
	mov.b32 	%f298, %r250;
	add.f32 	%f299, %f297, %f298;
	st.local.f32 	[%rd2+20], %f299;

$L__BB14_21:
	bar.sync 	0;
	mov.b32 	%r251, %f336;
	mov.u32 	%r252, 31;
	mov.u32 	%r253, 16;
	mov.u32 	%r254, -1;
	shfl.sync.bfly.b32 	%r255|%p73, %r251, %r253, %r252, %r254;
	mov.b32 	%f300, %r255;
	add.f32 	%f301, %f336, %f300;
	mov.b32 	%r256, %f301;
	mov.u32 	%r257, 8;
	shfl.sync.bfly.b32 	%r258|%p74, %r256, %r257, %r252, %r254;
	mov.b32 	%f302, %r258;
	add.f32 	%f303, %f301, %f302;
	mov.b32 	%r259, %f303;
	mov.u32 	%r260, 4;
	shfl.sync.bfly.b32 	%r261|%p75, %r259, %r260, %r252, %r254;
	mov.b32 	%f304, %r261;
	add.f32 	%f305, %f303, %f304;
	mov.b32 	%r262, %f305;
	mov.u32 	%r263, 2;
	shfl.sync.bfly.b32 	%r264|%p76, %r262, %r263, %r252, %r254;
	mov.b32 	%f306, %r264;
	add.f32 	%f307, %f305, %f306;
	mov.b32 	%r265, %f307;
	mov.u32 	%r266, 1;
	shfl.sync.bfly.b32 	%r267|%p77, %r265, %r266, %r252, %r254;
	mov.b32 	%f308, %r267;
	add.f32 	%f309, %f307, %f308;
	st.local.f32 	[%rd2+24], %f309;
	st.shared.f32 	[%r10], %f309;
	bar.sync 	0;
	@%p1 bra 	$L__BB14_23;

	ld.shared.f32 	%f310, [%r4];
	mov.b32 	%r268, %f310;
	shfl.sync.bfly.b32 	%r272|%p78, %r268, %r253, %r252, %r254;
	mov.b32 	%f311, %r272;
	add.f32 	%f312, %f310, %f311;
	mov.b32 	%r273, %f312;
	shfl.sync.bfly.b32 	%r275|%p79, %r273, %r257, %r252, %r254;
	mov.b32 	%f313, %r275;
	add.f32 	%f314, %f312, %f313;
	mov.b32 	%r276, %f314;
	shfl.sync.bfly.b32 	%r278|%p80, %r276, %r260, %r252, %r254;
	mov.b32 	%f315, %r278;
	add.f32 	%f316, %f314, %f315;
	mov.b32 	%r279, %f316;
	shfl.sync.bfly.b32 	%r281|%p81, %r279, %r263, %r252, %r254;
	mov.b32 	%f317, %r281;
	add.f32 	%f318, %f316, %f317;
	mov.b32 	%r282, %f318;
	shfl.sync.bfly.b32 	%r284|%p82, %r282, %r266, %r252, %r254;
	mov.b32 	%f319, %r284;
	add.f32 	%f320, %f318, %f319;
	st.local.f32 	[%rd2+24], %f320;

$L__BB14_23:
	bar.sync 	0;
	setp.gt.s32 	%p83, %r3, 6;
	@%p83 bra 	$L__BB14_25;

	mul.wide.s32 	%rd61, %r3, 4;
	add.s64 	%rd62, %rd2, %rd61;
	ld.local.f32 	%f321, [%rd62];
	mad.lo.s32 	%r285, %r3, %r13, %r2;
	cvt.s64.s32 	%rd63, %r285;
	mul.lo.s32 	%r286, %r1, %r14;
	cvt.s64.s32 	%rd64, %r286;
	add.s64 	%rd65, %rd64, %rd63;
	cvta.to.global.u64 	%rd66, %rd20;
	shl.b64 	%rd67, %rd65, 2;
	add.s64 	%rd68, %rd66, %rd67;
	st.global.f32 	[%rd68], %f321;

$L__BB14_25:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_8_bs_64
.visible .entry ggml_matvec_f32_ncols_8_bs_64(
	.param .u64 ggml_matvec_f32_ncols_8_bs_64_param_0,
	.param .u64 ggml_matvec_f32_ncols_8_bs_64_param_1,
	.param .u64 ggml_matvec_f32_ncols_8_bs_64_param_2,
	.param .u32 ggml_matvec_f32_ncols_8_bs_64_param_3,
	.param .u32 ggml_matvec_f32_ncols_8_bs_64_param_4,
	.param .u32 ggml_matvec_f32_ncols_8_bs_64_param_5,
	.param .u32 ggml_matvec_f32_ncols_8_bs_64_param_6,
	.param .u32 ggml_matvec_f32_ncols_8_bs_64_param_7,
	.param .u32 ggml_matvec_f32_ncols_8_bs_64_param_8,
	.param .u32 ggml_matvec_f32_ncols_8_bs_64_param_9,
	.param .u32 ggml_matvec_f32_ncols_8_bs_64_param_10,
	.param .u32 ggml_matvec_f32_ncols_8_bs_64_param_11
)
{
	.local .align 16 .b8 	__local_depot15[32];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<95>;
	.reg .f32 	%f<390>;
	.reg .b32 	%r<323>;
	.reg .b64 	%rd<75>;


	mov.u64 	%SPL, __local_depot15;
	ld.param.u64 	%rd22, [ggml_matvec_f32_ncols_8_bs_64_param_0];
	ld.param.u64 	%rd23, [ggml_matvec_f32_ncols_8_bs_64_param_1];
	ld.param.u64 	%rd21, [ggml_matvec_f32_ncols_8_bs_64_param_2];
	ld.param.u32 	%r11, [ggml_matvec_f32_ncols_8_bs_64_param_3];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_8_bs_64_param_5];
	ld.param.u32 	%r12, [ggml_matvec_f32_ncols_8_bs_64_param_6];
	ld.param.u32 	%r13, [ggml_matvec_f32_ncols_8_bs_64_param_7];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_8_bs_64_param_8];
	ld.param.u32 	%r17, [ggml_matvec_f32_ncols_8_bs_64_param_9];
	ld.param.u32 	%r18, [ggml_matvec_f32_ncols_8_bs_64_param_10];
	ld.param.u32 	%r14, [ggml_matvec_f32_ncols_8_bs_64_param_11];
	cvta.to.global.u64 	%rd74, %rd23;
	add.u64 	%rd2, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r19, %r1, %r16;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r20, %r2, %r15;
	mad.lo.s32 	%r21, %r19, %r17, %r20;
	cvt.s64.s32 	%rd3, %r21;
	cvta.to.global.u64 	%rd4, %rd22;
	mul.lo.s32 	%r22, %r1, %r18;
	cvt.s64.s32 	%rd5, %r22;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r23, %r3, 2;
	mov.u32 	%r24, data_mmv;
	add.s32 	%r4, %r24, %r23;
	@%p1 bra 	$L__BB15_2;

	mov.u32 	%r25, 0;
	st.shared.u32 	[%r4], %r25;

$L__BB15_2:
	bar.sync 	0;
	mov.f32 	%f382, 0f00000000;
	st.local.v4.f32 	[%rd2], {%f382, %f382, %f382, %f382};
	st.local.v4.f32 	[%rd2+16], {%f382, %f382, %f382, %f382};
	setp.ge.s32 	%p2, %r3, %r11;
	mov.f32 	%f383, %f382;
	mov.f32 	%f384, %f382;
	mov.f32 	%f385, %f382;
	mov.f32 	%f386, %f382;
	mov.f32 	%f387, %f382;
	mov.f32 	%f388, %f382;
	mov.f32 	%f389, %f382;
	@%p2 bra 	$L__BB15_9;

	not.b32 	%r26, %r3;
	add.s32 	%r5, %r26, %r11;
	and.b32  	%r27, %r5, 64;
	setp.ne.s32 	%p3, %r27, 0;
	mov.f32 	%f382, 0f00000000;
	mov.u32 	%r322, %r3;
	@%p3 bra 	$L__BB15_5;

	shl.b64 	%rd25, %rd5, 2;
	add.s64 	%rd26, %rd74, %rd25;
	shl.b64 	%rd27, %rd3, 2;
	add.s64 	%rd28, %rd4, %rd27;
	mul.wide.s32 	%rd29, %r3, 8;
	add.s64 	%rd30, %rd28, %rd29;
	ld.global.nc.v2.f32 	{%f57, %f58}, [%rd30];
	add.s64 	%rd31, %rd26, %rd29;
	ld.global.nc.v2.f32 	{%f61, %f62}, [%rd31];
	fma.rn.f32 	%f65, %f57, %f61, 0f00000000;
	fma.rn.f32 	%f389, %f58, %f62, %f65;
	mul.wide.s32 	%rd32, %r12, 8;
	add.s64 	%rd33, %rd31, %rd32;
	ld.global.nc.v2.f32 	{%f66, %f67}, [%rd33];
	fma.rn.f32 	%f70, %f57, %f66, 0f00000000;
	fma.rn.f32 	%f388, %f58, %f67, %f70;
	add.s32 	%r28, %r3, %r12;
	add.s32 	%r29, %r28, %r12;
	mul.wide.s32 	%rd34, %r29, 8;
	add.s64 	%rd35, %rd26, %rd34;
	ld.global.nc.v2.f32 	{%f71, %f72}, [%rd35];
	fma.rn.f32 	%f75, %f57, %f71, 0f00000000;
	fma.rn.f32 	%f387, %f58, %f72, %f75;
	add.s64 	%rd36, %rd35, %rd32;
	ld.global.nc.v2.f32 	{%f76, %f77}, [%rd36];
	fma.rn.f32 	%f80, %f57, %f76, 0f00000000;
	fma.rn.f32 	%f386, %f58, %f77, %f80;
	st.local.v4.f32 	[%rd2], {%f389, %f388, %f387, %f386};
	add.s64 	%rd37, %rd36, %rd32;
	ld.global.nc.v2.f32 	{%f81, %f82}, [%rd37];
	fma.rn.f32 	%f85, %f57, %f81, 0f00000000;
	fma.rn.f32 	%f385, %f58, %f82, %f85;
	add.s64 	%rd38, %rd37, %rd32;
	ld.global.nc.v2.f32 	{%f86, %f87}, [%rd38];
	fma.rn.f32 	%f90, %f57, %f86, 0f00000000;
	fma.rn.f32 	%f384, %f58, %f87, %f90;
	add.s64 	%rd39, %rd38, %rd32;
	ld.global.nc.v2.f32 	{%f91, %f92}, [%rd39];
	fma.rn.f32 	%f95, %f57, %f91, 0f00000000;
	fma.rn.f32 	%f383, %f58, %f92, %f95;
	add.s64 	%rd40, %rd39, %rd32;
	ld.global.nc.v2.f32 	{%f96, %f97}, [%rd40];
	fma.rn.f32 	%f100, %f57, %f96, 0f00000000;
	fma.rn.f32 	%f382, %f58, %f97, %f100;
	st.local.v4.f32 	[%rd2+16], {%f385, %f384, %f383, %f382};
	add.s32 	%r322, %r3, 64;

$L__BB15_5:
	and.b32  	%r30, %r5, -64;
	setp.eq.s32 	%p4, %r30, 0;
	@%p4 bra 	$L__BB15_9;

	add.s32 	%r31, %r322, %r12;
	add.s32 	%r32, %r31, 64;
	mul.wide.s32 	%rd41, %r32, 8;
	shl.b64 	%rd42, %rd5, 2;
	add.s64 	%rd7, %rd41, %rd42;
	shl.b32 	%r33, %r12, 1;
	add.s32 	%r34, %r322, %r33;
	mad.lo.s32 	%r35, %r12, 3, %r322;
	shl.b32 	%r36, %r12, 2;
	add.s32 	%r37, %r322, %r36;
	mad.lo.s32 	%r38, %r12, 5, %r322;
	mad.lo.s32 	%r39, %r12, 6, %r322;
	mad.lo.s32 	%r40, %r12, 7, %r322;
	mul.wide.s32 	%rd43, %r34, 8;
	add.s64 	%rd8, %rd43, %rd42;
	mul.wide.s32 	%rd44, %r35, 8;
	add.s64 	%rd9, %rd44, %rd42;
	mul.wide.s32 	%rd45, %r37, 8;
	add.s64 	%rd10, %rd45, %rd42;
	mul.wide.s32 	%rd46, %r38, 8;
	add.s64 	%rd11, %rd46, %rd42;
	mul.wide.s32 	%rd47, %r39, 8;
	add.s64 	%rd12, %rd47, %rd42;
	mul.wide.s32 	%rd48, %r40, 8;
	add.s64 	%rd13, %rd48, %rd42;
	mul.wide.s32 	%rd49, %r322, 2;
	add.s64 	%rd50, %rd49, %rd3;
	shl.b64 	%rd51, %rd50, 2;
	add.s64 	%rd52, %rd4, %rd51;
	add.s64 	%rd73, %rd52, 512;
	mul.wide.s32 	%rd53, %r322, 8;
	mul.wide.s32 	%rd54, %r12, 8;
	add.s64 	%rd55, %rd53, %rd54;
	add.s64 	%rd15, %rd55, %rd42;
	add.s64 	%rd16, %rd53, %rd42;

$L__BB15_7:
	ld.global.nc.v2.f32 	{%f101, %f102}, [%rd73+-512];
	add.s64 	%rd56, %rd74, %rd16;
	ld.global.nc.v2.f32 	{%f105, %f106}, [%rd56];
	fma.rn.f32 	%f109, %f101, %f105, %f389;
	fma.rn.f32 	%f110, %f102, %f106, %f109;
	add.s64 	%rd57, %rd74, %rd15;
	ld.global.nc.v2.f32 	{%f111, %f112}, [%rd57];
	fma.rn.f32 	%f115, %f101, %f111, %f388;
	fma.rn.f32 	%f116, %f102, %f112, %f115;
	add.s64 	%rd58, %rd74, %rd8;
	ld.global.nc.v2.f32 	{%f117, %f118}, [%rd58];
	fma.rn.f32 	%f121, %f101, %f117, %f387;
	fma.rn.f32 	%f122, %f102, %f118, %f121;
	add.s64 	%rd59, %rd74, %rd9;
	ld.global.nc.v2.f32 	{%f123, %f124}, [%rd59];
	fma.rn.f32 	%f127, %f101, %f123, %f386;
	fma.rn.f32 	%f128, %f102, %f124, %f127;
	add.s64 	%rd60, %rd74, %rd10;
	ld.global.nc.v2.f32 	{%f129, %f130}, [%rd60];
	fma.rn.f32 	%f133, %f101, %f129, %f385;
	fma.rn.f32 	%f134, %f102, %f130, %f133;
	add.s64 	%rd61, %rd74, %rd11;
	ld.global.nc.v2.f32 	{%f135, %f136}, [%rd61];
	fma.rn.f32 	%f139, %f101, %f135, %f384;
	fma.rn.f32 	%f140, %f102, %f136, %f139;
	add.s64 	%rd62, %rd74, %rd12;
	ld.global.nc.v2.f32 	{%f141, %f142}, [%rd62];
	fma.rn.f32 	%f145, %f101, %f141, %f383;
	fma.rn.f32 	%f146, %f102, %f142, %f145;
	add.s64 	%rd63, %rd74, %rd13;
	ld.global.nc.v2.f32 	{%f147, %f148}, [%rd63];
	fma.rn.f32 	%f151, %f101, %f147, %f382;
	fma.rn.f32 	%f152, %f102, %f148, %f151;
	ld.global.nc.v2.f32 	{%f153, %f154}, [%rd73];
	ld.global.nc.v2.f32 	{%f157, %f158}, [%rd56+512];
	fma.rn.f32 	%f161, %f153, %f157, %f110;
	fma.rn.f32 	%f389, %f154, %f158, %f161;
	add.s64 	%rd64, %rd74, %rd7;
	ld.global.nc.v2.f32 	{%f162, %f163}, [%rd64];
	fma.rn.f32 	%f166, %f153, %f162, %f116;
	fma.rn.f32 	%f388, %f154, %f163, %f166;
	ld.global.nc.v2.f32 	{%f167, %f168}, [%rd58+512];
	fma.rn.f32 	%f171, %f153, %f167, %f122;
	fma.rn.f32 	%f387, %f154, %f168, %f171;
	ld.global.nc.v2.f32 	{%f172, %f173}, [%rd59+512];
	fma.rn.f32 	%f176, %f153, %f172, %f128;
	fma.rn.f32 	%f386, %f154, %f173, %f176;
	ld.global.nc.v2.f32 	{%f177, %f178}, [%rd60+512];
	fma.rn.f32 	%f181, %f153, %f177, %f134;
	fma.rn.f32 	%f385, %f154, %f178, %f181;
	ld.global.nc.v2.f32 	{%f182, %f183}, [%rd61+512];
	fma.rn.f32 	%f186, %f153, %f182, %f140;
	fma.rn.f32 	%f384, %f154, %f183, %f186;
	ld.global.nc.v2.f32 	{%f187, %f188}, [%rd62+512];
	fma.rn.f32 	%f191, %f153, %f187, %f146;
	fma.rn.f32 	%f383, %f154, %f188, %f191;
	ld.global.nc.v2.f32 	{%f192, %f193}, [%rd63+512];
	fma.rn.f32 	%f196, %f153, %f192, %f152;
	fma.rn.f32 	%f382, %f154, %f193, %f196;
	add.s64 	%rd74, %rd74, 1024;
	add.s64 	%rd73, %rd73, 1024;
	add.s32 	%r322, %r322, 128;
	setp.lt.s32 	%p5, %r322, %r11;
	@%p5 bra 	$L__BB15_7;

	st.local.v4.f32 	[%rd2], {%f389, %f388, %f387, %f386};
	st.local.v4.f32 	[%rd2+16], {%f385, %f384, %f383, %f382};

$L__BB15_9:
	shr.s32 	%r41, %r3, 31;
	shr.u32 	%r42, %r41, 27;
	add.s32 	%r43, %r3, %r42;
	shr.s32 	%r44, %r43, 5;
	shl.b32 	%r45, %r44, 2;
	add.s32 	%r10, %r24, %r45;
	mov.u32 	%r47, 2;
	mov.b32 	%r48, %f389;
	mov.u32 	%r49, 31;
	mov.u32 	%r50, 16;
	mov.u32 	%r51, -1;
	shfl.sync.bfly.b32 	%r52|%p6, %r48, %r50, %r49, %r51;
	mov.b32 	%f197, %r52;
	add.f32 	%f198, %f389, %f197;
	mov.b32 	%r53, %f198;
	mov.u32 	%r54, 8;
	shfl.sync.bfly.b32 	%r55|%p7, %r53, %r54, %r49, %r51;
	mov.b32 	%f199, %r55;
	add.f32 	%f200, %f198, %f199;
	mov.b32 	%r56, %f200;
	mov.u32 	%r57, 4;
	shfl.sync.bfly.b32 	%r58|%p8, %r56, %r57, %r49, %r51;
	mov.b32 	%f201, %r58;
	add.f32 	%f202, %f200, %f201;
	mov.b32 	%r59, %f202;
	shfl.sync.bfly.b32 	%r60|%p9, %r59, %r47, %r49, %r51;
	mov.b32 	%f203, %r60;
	add.f32 	%f204, %f202, %f203;
	mov.b32 	%r61, %f204;
	mov.u32 	%r62, 1;
	shfl.sync.bfly.b32 	%r63|%p10, %r61, %r62, %r49, %r51;
	mov.b32 	%f205, %r63;
	add.f32 	%f206, %f204, %f205;
	st.local.f32 	[%rd2], %f206;
	st.shared.f32 	[%r10], %f206;
	bar.sync 	0;
	@%p1 bra 	$L__BB15_11;

	ld.shared.f32 	%f207, [%r4];
	mov.b32 	%r64, %f207;
	shfl.sync.bfly.b32 	%r68|%p12, %r64, %r50, %r49, %r51;
	mov.b32 	%f208, %r68;
	add.f32 	%f209, %f207, %f208;
	mov.b32 	%r69, %f209;
	shfl.sync.bfly.b32 	%r71|%p13, %r69, %r54, %r49, %r51;
	mov.b32 	%f210, %r71;
	add.f32 	%f211, %f209, %f210;
	mov.b32 	%r72, %f211;
	shfl.sync.bfly.b32 	%r74|%p14, %r72, %r57, %r49, %r51;
	mov.b32 	%f212, %r74;
	add.f32 	%f213, %f211, %f212;
	mov.b32 	%r75, %f213;
	shfl.sync.bfly.b32 	%r77|%p15, %r75, %r47, %r49, %r51;
	mov.b32 	%f214, %r77;
	add.f32 	%f215, %f213, %f214;
	mov.b32 	%r78, %f215;
	shfl.sync.bfly.b32 	%r80|%p16, %r78, %r62, %r49, %r51;
	mov.b32 	%f216, %r80;
	add.f32 	%f217, %f215, %f216;
	st.local.f32 	[%rd2], %f217;

$L__BB15_11:
	bar.sync 	0;
	mov.b32 	%r81, %f388;
	shfl.sync.bfly.b32 	%r85|%p18, %r81, %r50, %r49, %r51;
	mov.b32 	%f218, %r85;
	add.f32 	%f219, %f388, %f218;
	mov.b32 	%r86, %f219;
	shfl.sync.bfly.b32 	%r88|%p19, %r86, %r54, %r49, %r51;
	mov.b32 	%f220, %r88;
	add.f32 	%f221, %f219, %f220;
	mov.b32 	%r89, %f221;
	shfl.sync.bfly.b32 	%r91|%p20, %r89, %r57, %r49, %r51;
	mov.b32 	%f222, %r91;
	add.f32 	%f223, %f221, %f222;
	mov.b32 	%r92, %f223;
	shfl.sync.bfly.b32 	%r94|%p21, %r92, %r47, %r49, %r51;
	mov.b32 	%f224, %r94;
	add.f32 	%f225, %f223, %f224;
	mov.b32 	%r95, %f225;
	shfl.sync.bfly.b32 	%r97|%p22, %r95, %r62, %r49, %r51;
	mov.b32 	%f226, %r97;
	add.f32 	%f227, %f225, %f226;
	st.local.f32 	[%rd2+4], %f227;
	st.shared.f32 	[%r10], %f227;
	bar.sync 	0;
	@%p1 bra 	$L__BB15_13;

	ld.shared.f32 	%f228, [%r4];
	mov.b32 	%r98, %f228;
	mov.u32 	%r99, 31;
	mov.u32 	%r100, 16;
	mov.u32 	%r101, -1;
	shfl.sync.bfly.b32 	%r102|%p23, %r98, %r100, %r99, %r101;
	mov.b32 	%f229, %r102;
	add.f32 	%f230, %f228, %f229;
	mov.b32 	%r103, %f230;
	mov.u32 	%r104, 8;
	shfl.sync.bfly.b32 	%r105|%p24, %r103, %r104, %r99, %r101;
	mov.b32 	%f231, %r105;
	add.f32 	%f232, %f230, %f231;
	mov.b32 	%r106, %f232;
	mov.u32 	%r107, 4;
	shfl.sync.bfly.b32 	%r108|%p25, %r106, %r107, %r99, %r101;
	mov.b32 	%f233, %r108;
	add.f32 	%f234, %f232, %f233;
	mov.b32 	%r109, %f234;
	mov.u32 	%r110, 2;
	shfl.sync.bfly.b32 	%r111|%p26, %r109, %r110, %r99, %r101;
	mov.b32 	%f235, %r111;
	add.f32 	%f236, %f234, %f235;
	mov.b32 	%r112, %f236;
	mov.u32 	%r113, 1;
	shfl.sync.bfly.b32 	%r114|%p27, %r112, %r113, %r99, %r101;
	mov.b32 	%f237, %r114;
	add.f32 	%f238, %f236, %f237;
	st.local.f32 	[%rd2+4], %f238;

$L__BB15_13:
	bar.sync 	0;
	mov.b32 	%r115, %f387;
	mov.u32 	%r116, 31;
	mov.u32 	%r117, 16;
	mov.u32 	%r118, -1;
	shfl.sync.bfly.b32 	%r119|%p29, %r115, %r117, %r116, %r118;
	mov.b32 	%f239, %r119;
	add.f32 	%f240, %f387, %f239;
	mov.b32 	%r120, %f240;
	mov.u32 	%r121, 8;
	shfl.sync.bfly.b32 	%r122|%p30, %r120, %r121, %r116, %r118;
	mov.b32 	%f241, %r122;
	add.f32 	%f242, %f240, %f241;
	mov.b32 	%r123, %f242;
	mov.u32 	%r124, 4;
	shfl.sync.bfly.b32 	%r125|%p31, %r123, %r124, %r116, %r118;
	mov.b32 	%f243, %r125;
	add.f32 	%f244, %f242, %f243;
	mov.b32 	%r126, %f244;
	mov.u32 	%r127, 2;
	shfl.sync.bfly.b32 	%r128|%p32, %r126, %r127, %r116, %r118;
	mov.b32 	%f245, %r128;
	add.f32 	%f246, %f244, %f245;
	mov.b32 	%r129, %f246;
	mov.u32 	%r130, 1;
	shfl.sync.bfly.b32 	%r131|%p33, %r129, %r130, %r116, %r118;
	mov.b32 	%f247, %r131;
	add.f32 	%f248, %f246, %f247;
	st.local.f32 	[%rd2+8], %f248;
	st.shared.f32 	[%r10], %f248;
	bar.sync 	0;
	@%p1 bra 	$L__BB15_15;

	ld.shared.f32 	%f249, [%r4];
	mov.b32 	%r132, %f249;
	shfl.sync.bfly.b32 	%r136|%p34, %r132, %r117, %r116, %r118;
	mov.b32 	%f250, %r136;
	add.f32 	%f251, %f249, %f250;
	mov.b32 	%r137, %f251;
	shfl.sync.bfly.b32 	%r139|%p35, %r137, %r121, %r116, %r118;
	mov.b32 	%f252, %r139;
	add.f32 	%f253, %f251, %f252;
	mov.b32 	%r140, %f253;
	shfl.sync.bfly.b32 	%r142|%p36, %r140, %r124, %r116, %r118;
	mov.b32 	%f254, %r142;
	add.f32 	%f255, %f253, %f254;
	mov.b32 	%r143, %f255;
	shfl.sync.bfly.b32 	%r145|%p37, %r143, %r127, %r116, %r118;
	mov.b32 	%f256, %r145;
	add.f32 	%f257, %f255, %f256;
	mov.b32 	%r146, %f257;
	shfl.sync.bfly.b32 	%r148|%p38, %r146, %r130, %r116, %r118;
	mov.b32 	%f258, %r148;
	add.f32 	%f259, %f257, %f258;
	st.local.f32 	[%rd2+8], %f259;

$L__BB15_15:
	bar.sync 	0;
	mov.b32 	%r149, %f386;
	shfl.sync.bfly.b32 	%r153|%p40, %r149, %r117, %r116, %r118;
	mov.b32 	%f260, %r153;
	add.f32 	%f261, %f386, %f260;
	mov.b32 	%r154, %f261;
	shfl.sync.bfly.b32 	%r156|%p41, %r154, %r121, %r116, %r118;
	mov.b32 	%f262, %r156;
	add.f32 	%f263, %f261, %f262;
	mov.b32 	%r157, %f263;
	shfl.sync.bfly.b32 	%r159|%p42, %r157, %r124, %r116, %r118;
	mov.b32 	%f264, %r159;
	add.f32 	%f265, %f263, %f264;
	mov.b32 	%r160, %f265;
	shfl.sync.bfly.b32 	%r162|%p43, %r160, %r127, %r116, %r118;
	mov.b32 	%f266, %r162;
	add.f32 	%f267, %f265, %f266;
	mov.b32 	%r163, %f267;
	shfl.sync.bfly.b32 	%r165|%p44, %r163, %r130, %r116, %r118;
	mov.b32 	%f268, %r165;
	add.f32 	%f269, %f267, %f268;
	st.local.f32 	[%rd2+12], %f269;
	st.shared.f32 	[%r10], %f269;
	bar.sync 	0;
	@%p1 bra 	$L__BB15_17;

	ld.shared.f32 	%f270, [%r4];
	mov.b32 	%r166, %f270;
	mov.u32 	%r167, 31;
	mov.u32 	%r168, 16;
	mov.u32 	%r169, -1;
	shfl.sync.bfly.b32 	%r170|%p45, %r166, %r168, %r167, %r169;
	mov.b32 	%f271, %r170;
	add.f32 	%f272, %f270, %f271;
	mov.b32 	%r171, %f272;
	mov.u32 	%r172, 8;
	shfl.sync.bfly.b32 	%r173|%p46, %r171, %r172, %r167, %r169;
	mov.b32 	%f273, %r173;
	add.f32 	%f274, %f272, %f273;
	mov.b32 	%r174, %f274;
	mov.u32 	%r175, 4;
	shfl.sync.bfly.b32 	%r176|%p47, %r174, %r175, %r167, %r169;
	mov.b32 	%f275, %r176;
	add.f32 	%f276, %f274, %f275;
	mov.b32 	%r177, %f276;
	mov.u32 	%r178, 2;
	shfl.sync.bfly.b32 	%r179|%p48, %r177, %r178, %r167, %r169;
	mov.b32 	%f277, %r179;
	add.f32 	%f278, %f276, %f277;
	mov.b32 	%r180, %f278;
	mov.u32 	%r181, 1;
	shfl.sync.bfly.b32 	%r182|%p49, %r180, %r181, %r167, %r169;
	mov.b32 	%f279, %r182;
	add.f32 	%f280, %f278, %f279;
	st.local.f32 	[%rd2+12], %f280;

$L__BB15_17:
	bar.sync 	0;
	mov.b32 	%r183, %f385;
	mov.u32 	%r184, 31;
	mov.u32 	%r185, 16;
	mov.u32 	%r186, -1;
	shfl.sync.bfly.b32 	%r187|%p51, %r183, %r185, %r184, %r186;
	mov.b32 	%f281, %r187;
	add.f32 	%f282, %f385, %f281;
	mov.b32 	%r188, %f282;
	mov.u32 	%r189, 8;
	shfl.sync.bfly.b32 	%r190|%p52, %r188, %r189, %r184, %r186;
	mov.b32 	%f283, %r190;
	add.f32 	%f284, %f282, %f283;
	mov.b32 	%r191, %f284;
	mov.u32 	%r192, 4;
	shfl.sync.bfly.b32 	%r193|%p53, %r191, %r192, %r184, %r186;
	mov.b32 	%f285, %r193;
	add.f32 	%f286, %f284, %f285;
	mov.b32 	%r194, %f286;
	mov.u32 	%r195, 2;
	shfl.sync.bfly.b32 	%r196|%p54, %r194, %r195, %r184, %r186;
	mov.b32 	%f287, %r196;
	add.f32 	%f288, %f286, %f287;
	mov.b32 	%r197, %f288;
	mov.u32 	%r198, 1;
	shfl.sync.bfly.b32 	%r199|%p55, %r197, %r198, %r184, %r186;
	mov.b32 	%f289, %r199;
	add.f32 	%f290, %f288, %f289;
	st.local.f32 	[%rd2+16], %f290;
	st.shared.f32 	[%r10], %f290;
	bar.sync 	0;
	@%p1 bra 	$L__BB15_19;

	ld.shared.f32 	%f291, [%r4];
	mov.b32 	%r200, %f291;
	shfl.sync.bfly.b32 	%r204|%p56, %r200, %r185, %r184, %r186;
	mov.b32 	%f292, %r204;
	add.f32 	%f293, %f291, %f292;
	mov.b32 	%r205, %f293;
	shfl.sync.bfly.b32 	%r207|%p57, %r205, %r189, %r184, %r186;
	mov.b32 	%f294, %r207;
	add.f32 	%f295, %f293, %f294;
	mov.b32 	%r208, %f295;
	shfl.sync.bfly.b32 	%r210|%p58, %r208, %r192, %r184, %r186;
	mov.b32 	%f296, %r210;
	add.f32 	%f297, %f295, %f296;
	mov.b32 	%r211, %f297;
	shfl.sync.bfly.b32 	%r213|%p59, %r211, %r195, %r184, %r186;
	mov.b32 	%f298, %r213;
	add.f32 	%f299, %f297, %f298;
	mov.b32 	%r214, %f299;
	shfl.sync.bfly.b32 	%r216|%p60, %r214, %r198, %r184, %r186;
	mov.b32 	%f300, %r216;
	add.f32 	%f301, %f299, %f300;
	st.local.f32 	[%rd2+16], %f301;

$L__BB15_19:
	bar.sync 	0;
	mov.b32 	%r217, %f384;
	shfl.sync.bfly.b32 	%r221|%p62, %r217, %r185, %r184, %r186;
	mov.b32 	%f302, %r221;
	add.f32 	%f303, %f384, %f302;
	mov.b32 	%r222, %f303;
	shfl.sync.bfly.b32 	%r224|%p63, %r222, %r189, %r184, %r186;
	mov.b32 	%f304, %r224;
	add.f32 	%f305, %f303, %f304;
	mov.b32 	%r225, %f305;
	shfl.sync.bfly.b32 	%r227|%p64, %r225, %r192, %r184, %r186;
	mov.b32 	%f306, %r227;
	add.f32 	%f307, %f305, %f306;
	mov.b32 	%r228, %f307;
	shfl.sync.bfly.b32 	%r230|%p65, %r228, %r195, %r184, %r186;
	mov.b32 	%f308, %r230;
	add.f32 	%f309, %f307, %f308;
	mov.b32 	%r231, %f309;
	shfl.sync.bfly.b32 	%r233|%p66, %r231, %r198, %r184, %r186;
	mov.b32 	%f310, %r233;
	add.f32 	%f311, %f309, %f310;
	st.local.f32 	[%rd2+20], %f311;
	st.shared.f32 	[%r10], %f311;
	bar.sync 	0;
	@%p1 bra 	$L__BB15_21;

	ld.shared.f32 	%f312, [%r4];
	mov.b32 	%r234, %f312;
	mov.u32 	%r235, 31;
	mov.u32 	%r236, 16;
	mov.u32 	%r237, -1;
	shfl.sync.bfly.b32 	%r238|%p67, %r234, %r236, %r235, %r237;
	mov.b32 	%f313, %r238;
	add.f32 	%f314, %f312, %f313;
	mov.b32 	%r239, %f314;
	mov.u32 	%r240, 8;
	shfl.sync.bfly.b32 	%r241|%p68, %r239, %r240, %r235, %r237;
	mov.b32 	%f315, %r241;
	add.f32 	%f316, %f314, %f315;
	mov.b32 	%r242, %f316;
	mov.u32 	%r243, 4;
	shfl.sync.bfly.b32 	%r244|%p69, %r242, %r243, %r235, %r237;
	mov.b32 	%f317, %r244;
	add.f32 	%f318, %f316, %f317;
	mov.b32 	%r245, %f318;
	mov.u32 	%r246, 2;
	shfl.sync.bfly.b32 	%r247|%p70, %r245, %r246, %r235, %r237;
	mov.b32 	%f319, %r247;
	add.f32 	%f320, %f318, %f319;
	mov.b32 	%r248, %f320;
	mov.u32 	%r249, 1;
	shfl.sync.bfly.b32 	%r250|%p71, %r248, %r249, %r235, %r237;
	mov.b32 	%f321, %r250;
	add.f32 	%f322, %f320, %f321;
	st.local.f32 	[%rd2+20], %f322;

$L__BB15_21:
	bar.sync 	0;
	mov.b32 	%r251, %f383;
	mov.u32 	%r252, 31;
	mov.u32 	%r253, 16;
	mov.u32 	%r254, -1;
	shfl.sync.bfly.b32 	%r255|%p73, %r251, %r253, %r252, %r254;
	mov.b32 	%f323, %r255;
	add.f32 	%f324, %f383, %f323;
	mov.b32 	%r256, %f324;
	mov.u32 	%r257, 8;
	shfl.sync.bfly.b32 	%r258|%p74, %r256, %r257, %r252, %r254;
	mov.b32 	%f325, %r258;
	add.f32 	%f326, %f324, %f325;
	mov.b32 	%r259, %f326;
	mov.u32 	%r260, 4;
	shfl.sync.bfly.b32 	%r261|%p75, %r259, %r260, %r252, %r254;
	mov.b32 	%f327, %r261;
	add.f32 	%f328, %f326, %f327;
	mov.b32 	%r262, %f328;
	mov.u32 	%r263, 2;
	shfl.sync.bfly.b32 	%r264|%p76, %r262, %r263, %r252, %r254;
	mov.b32 	%f329, %r264;
	add.f32 	%f330, %f328, %f329;
	mov.b32 	%r265, %f330;
	mov.u32 	%r266, 1;
	shfl.sync.bfly.b32 	%r267|%p77, %r265, %r266, %r252, %r254;
	mov.b32 	%f331, %r267;
	add.f32 	%f332, %f330, %f331;
	st.local.f32 	[%rd2+24], %f332;
	st.shared.f32 	[%r10], %f332;
	bar.sync 	0;
	@%p1 bra 	$L__BB15_23;

	ld.shared.f32 	%f333, [%r4];
	mov.b32 	%r268, %f333;
	shfl.sync.bfly.b32 	%r272|%p78, %r268, %r253, %r252, %r254;
	mov.b32 	%f334, %r272;
	add.f32 	%f335, %f333, %f334;
	mov.b32 	%r273, %f335;
	shfl.sync.bfly.b32 	%r275|%p79, %r273, %r257, %r252, %r254;
	mov.b32 	%f336, %r275;
	add.f32 	%f337, %f335, %f336;
	mov.b32 	%r276, %f337;
	shfl.sync.bfly.b32 	%r278|%p80, %r276, %r260, %r252, %r254;
	mov.b32 	%f338, %r278;
	add.f32 	%f339, %f337, %f338;
	mov.b32 	%r279, %f339;
	shfl.sync.bfly.b32 	%r281|%p81, %r279, %r263, %r252, %r254;
	mov.b32 	%f340, %r281;
	add.f32 	%f341, %f339, %f340;
	mov.b32 	%r282, %f341;
	shfl.sync.bfly.b32 	%r284|%p82, %r282, %r266, %r252, %r254;
	mov.b32 	%f342, %r284;
	add.f32 	%f343, %f341, %f342;
	st.local.f32 	[%rd2+24], %f343;

$L__BB15_23:
	bar.sync 	0;
	mov.b32 	%r285, %f382;
	shfl.sync.bfly.b32 	%r289|%p84, %r285, %r253, %r252, %r254;
	mov.b32 	%f344, %r289;
	add.f32 	%f345, %f382, %f344;
	mov.b32 	%r290, %f345;
	shfl.sync.bfly.b32 	%r292|%p85, %r290, %r257, %r252, %r254;
	mov.b32 	%f346, %r292;
	add.f32 	%f347, %f345, %f346;
	mov.b32 	%r293, %f347;
	shfl.sync.bfly.b32 	%r295|%p86, %r293, %r260, %r252, %r254;
	mov.b32 	%f348, %r295;
	add.f32 	%f349, %f347, %f348;
	mov.b32 	%r296, %f349;
	shfl.sync.bfly.b32 	%r298|%p87, %r296, %r263, %r252, %r254;
	mov.b32 	%f350, %r298;
	add.f32 	%f351, %f349, %f350;
	mov.b32 	%r299, %f351;
	shfl.sync.bfly.b32 	%r301|%p88, %r299, %r266, %r252, %r254;
	mov.b32 	%f352, %r301;
	add.f32 	%f353, %f351, %f352;
	st.local.f32 	[%rd2+28], %f353;
	st.shared.f32 	[%r10], %f353;
	bar.sync 	0;
	@%p1 bra 	$L__BB15_25;

	ld.shared.f32 	%f354, [%r4];
	mov.b32 	%r302, %f354;
	mov.u32 	%r303, 31;
	mov.u32 	%r304, 16;
	mov.u32 	%r305, -1;
	shfl.sync.bfly.b32 	%r306|%p89, %r302, %r304, %r303, %r305;
	mov.b32 	%f355, %r306;
	add.f32 	%f356, %f354, %f355;
	mov.b32 	%r307, %f356;
	mov.u32 	%r308, 8;
	shfl.sync.bfly.b32 	%r309|%p90, %r307, %r308, %r303, %r305;
	mov.b32 	%f357, %r309;
	add.f32 	%f358, %f356, %f357;
	mov.b32 	%r310, %f358;
	mov.u32 	%r311, 4;
	shfl.sync.bfly.b32 	%r312|%p91, %r310, %r311, %r303, %r305;
	mov.b32 	%f359, %r312;
	add.f32 	%f360, %f358, %f359;
	mov.b32 	%r313, %f360;
	mov.u32 	%r314, 2;
	shfl.sync.bfly.b32 	%r315|%p92, %r313, %r314, %r303, %r305;
	mov.b32 	%f361, %r315;
	add.f32 	%f362, %f360, %f361;
	mov.b32 	%r316, %f362;
	mov.u32 	%r317, 1;
	shfl.sync.bfly.b32 	%r318|%p93, %r316, %r317, %r303, %r305;
	mov.b32 	%f363, %r318;
	add.f32 	%f364, %f362, %f363;
	st.local.f32 	[%rd2+28], %f364;

$L__BB15_25:
	bar.sync 	0;
	setp.gt.s32 	%p94, %r3, 7;
	@%p94 bra 	$L__BB15_27;

	mul.wide.s32 	%rd65, %r3, 4;
	add.s64 	%rd66, %rd2, %rd65;
	ld.local.f32 	%f365, [%rd66];
	mad.lo.s32 	%r319, %r3, %r13, %r2;
	cvt.s64.s32 	%rd67, %r319;
	mul.lo.s32 	%r320, %r1, %r14;
	cvt.s64.s32 	%rd68, %r320;
	add.s64 	%rd69, %rd68, %rd67;
	cvta.to.global.u64 	%rd70, %rd21;
	shl.b64 	%rd71, %rd69, 2;
	add.s64 	%rd72, %rd70, %rd71;
	st.global.f32 	[%rd72], %f365;

$L__BB15_27:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_1_bs_96
.visible .entry ggml_matvec_f32_ncols_1_bs_96(
	.param .u64 ggml_matvec_f32_ncols_1_bs_96_param_0,
	.param .u64 ggml_matvec_f32_ncols_1_bs_96_param_1,
	.param .u64 ggml_matvec_f32_ncols_1_bs_96_param_2,
	.param .u32 ggml_matvec_f32_ncols_1_bs_96_param_3,
	.param .u32 ggml_matvec_f32_ncols_1_bs_96_param_4,
	.param .u32 ggml_matvec_f32_ncols_1_bs_96_param_5,
	.param .u32 ggml_matvec_f32_ncols_1_bs_96_param_6,
	.param .u32 ggml_matvec_f32_ncols_1_bs_96_param_7,
	.param .u32 ggml_matvec_f32_ncols_1_bs_96_param_8,
	.param .u32 ggml_matvec_f32_ncols_1_bs_96_param_9,
	.param .u32 ggml_matvec_f32_ncols_1_bs_96_param_10,
	.param .u32 ggml_matvec_f32_ncols_1_bs_96_param_11
)
{
	.reg .pred 	%p<19>;
	.reg .f32 	%f<88>;
	.reg .b32 	%r<79>;
	.reg .b64 	%rd<44>;


	ld.param.u64 	%rd18, [ggml_matvec_f32_ncols_1_bs_96_param_0];
	ld.param.u64 	%rd19, [ggml_matvec_f32_ncols_1_bs_96_param_1];
	ld.param.u64 	%rd17, [ggml_matvec_f32_ncols_1_bs_96_param_2];
	ld.param.u32 	%r13, [ggml_matvec_f32_ncols_1_bs_96_param_3];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_1_bs_96_param_5];
	ld.param.u32 	%r14, [ggml_matvec_f32_ncols_1_bs_96_param_7];
	ld.param.u32 	%r17, [ggml_matvec_f32_ncols_1_bs_96_param_8];
	ld.param.u32 	%r18, [ggml_matvec_f32_ncols_1_bs_96_param_9];
	ld.param.u32 	%r19, [ggml_matvec_f32_ncols_1_bs_96_param_10];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_1_bs_96_param_11];
	cvta.to.global.u64 	%rd1, %rd19;
	cvta.to.global.u64 	%rd2, %rd18;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r20, %r1, %r17;
	mov.u32 	%r21, %ctaid.x;
	mul.lo.s32 	%r22, %r21, %r16;
	mad.lo.s32 	%r23, %r20, %r18, %r22;
	cvt.s64.s32 	%rd3, %r23;
	mul.lo.s32 	%r24, %r1, %r19;
	cvt.s64.s32 	%rd4, %r24;
	mov.u32 	%r2, %tid.x;
	setp.gt.s32 	%p1, %r2, 31;
	shl.b32 	%r25, %r2, 2;
	mov.u32 	%r26, data_mmv;
	add.s32 	%r3, %r26, %r25;
	@%p1 bra 	$L__BB16_2;

	mov.u32 	%r27, 0;
	st.shared.u32 	[%r3], %r27;

$L__BB16_2:
	bar.sync 	0;
	setp.ge.s32 	%p2, %r2, %r13;
	mov.f32 	%f86, 0f00000000;
	@%p2 bra 	$L__BB16_9;

	not.b32 	%r28, %r2;
	add.s32 	%r4, %r28, %r13;
	mul.wide.u32 	%rd20, %r4, -1431655765;
	shr.u64 	%rd21, %rd20, 38;
	cvt.u32.u64 	%r29, %rd21;
	add.s32 	%r30, %r29, 1;
	and.b32  	%r76, %r30, 3;
	setp.eq.s32 	%p3, %r76, 0;
	mov.f32 	%f86, 0f00000000;
	mov.u32 	%r77, %r2;
	@%p3 bra 	$L__BB16_6;

	mul.wide.s32 	%rd22, %r2, 2;
	add.s64 	%rd23, %rd22, %rd4;
	shl.b64 	%rd24, %rd23, 2;
	add.s64 	%rd41, %rd1, %rd24;
	add.s64 	%rd25, %rd22, %rd3;
	shl.b64 	%rd26, %rd25, 2;
	add.s64 	%rd40, %rd2, %rd26;
	mov.f32 	%f86, 0f00000000;
	mov.u32 	%r77, %r2;

$L__BB16_5:
	.pragma "nounroll";
	ld.global.nc.v2.f32 	{%f15, %f16}, [%rd40];
	ld.global.nc.v2.f32 	{%f19, %f20}, [%rd41];
	fma.rn.f32 	%f23, %f15, %f19, %f86;
	fma.rn.f32 	%f86, %f16, %f20, %f23;
	add.s32 	%r77, %r77, 96;
	add.s64 	%rd41, %rd41, 768;
	add.s64 	%rd40, %rd40, 768;
	add.s32 	%r76, %r76, -1;
	setp.ne.s32 	%p4, %r76, 0;
	@%p4 bra 	$L__BB16_5;

$L__BB16_6:
	setp.lt.u32 	%p5, %r4, 288;
	@%p5 bra 	$L__BB16_9;

	mul.wide.s32 	%rd27, %r77, 2;
	add.s64 	%rd28, %rd27, %rd3;
	shl.b64 	%rd29, %rd28, 2;
	add.s64 	%rd30, %rd2, %rd29;
	add.s64 	%rd43, %rd30, 1536;
	add.s64 	%rd31, %rd27, %rd4;
	shl.b64 	%rd32, %rd31, 2;
	add.s64 	%rd33, %rd1, %rd32;
	add.s64 	%rd42, %rd33, 1536;

$L__BB16_8:
	ld.global.nc.v2.f32 	{%f24, %f25}, [%rd43+-1536];
	ld.global.nc.v2.f32 	{%f28, %f29}, [%rd42+-1536];
	fma.rn.f32 	%f32, %f24, %f28, %f86;
	fma.rn.f32 	%f33, %f25, %f29, %f32;
	ld.global.nc.v2.f32 	{%f34, %f35}, [%rd43+-768];
	ld.global.nc.v2.f32 	{%f38, %f39}, [%rd42+-768];
	fma.rn.f32 	%f42, %f34, %f38, %f33;
	fma.rn.f32 	%f43, %f35, %f39, %f42;
	ld.global.nc.v2.f32 	{%f44, %f45}, [%rd43];
	ld.global.nc.v2.f32 	{%f48, %f49}, [%rd42];
	fma.rn.f32 	%f52, %f44, %f48, %f43;
	fma.rn.f32 	%f53, %f45, %f49, %f52;
	ld.global.nc.v2.f32 	{%f54, %f55}, [%rd43+768];
	ld.global.nc.v2.f32 	{%f58, %f59}, [%rd42+768];
	fma.rn.f32 	%f62, %f54, %f58, %f53;
	fma.rn.f32 	%f86, %f55, %f59, %f62;
	add.s64 	%rd43, %rd43, 3072;
	add.s64 	%rd42, %rd42, 3072;
	add.s32 	%r77, %r77, 384;
	setp.lt.s32 	%p6, %r77, %r13;
	@%p6 bra 	$L__BB16_8;

$L__BB16_9:
	mov.b32 	%r31, %f86;
	mov.u32 	%r32, 31;
	mov.u32 	%r33, 16;
	mov.u32 	%r34, -1;
	shfl.sync.bfly.b32 	%r35|%p7, %r31, %r33, %r32, %r34;
	mov.b32 	%f63, %r35;
	add.f32 	%f64, %f86, %f63;
	mov.b32 	%r36, %f64;
	mov.u32 	%r37, 8;
	shfl.sync.bfly.b32 	%r38|%p8, %r36, %r37, %r32, %r34;
	mov.b32 	%f65, %r38;
	add.f32 	%f66, %f64, %f65;
	mov.b32 	%r39, %f66;
	mov.u32 	%r40, 4;
	shfl.sync.bfly.b32 	%r41|%p9, %r39, %r40, %r32, %r34;
	mov.b32 	%f67, %r41;
	add.f32 	%f68, %f66, %f67;
	mov.b32 	%r42, %f68;
	mov.u32 	%r43, 2;
	shfl.sync.bfly.b32 	%r44|%p10, %r42, %r43, %r32, %r34;
	mov.b32 	%f69, %r44;
	add.f32 	%f70, %f68, %f69;
	mov.b32 	%r45, %f70;
	mov.u32 	%r46, 1;
	shfl.sync.bfly.b32 	%r47|%p11, %r45, %r46, %r32, %r34;
	mov.b32 	%f71, %r47;
	add.f32 	%f87, %f70, %f71;
	shr.s32 	%r48, %r2, 31;
	shr.u32 	%r49, %r48, 27;
	add.s32 	%r50, %r2, %r49;
	shr.s32 	%r51, %r50, 5;
	shl.b32 	%r52, %r51, 2;
	add.s32 	%r54, %r26, %r52;
	st.shared.f32 	[%r54], %f87;
	bar.sync 	0;
	@%p1 bra 	$L__BB16_11;

	ld.shared.f32 	%f72, [%r3];
	mov.b32 	%r55, %f72;
	shfl.sync.bfly.b32 	%r59|%p13, %r55, %r33, %r32, %r34;
	mov.b32 	%f73, %r59;
	add.f32 	%f74, %f72, %f73;
	mov.b32 	%r60, %f74;
	shfl.sync.bfly.b32 	%r62|%p14, %r60, %r37, %r32, %r34;
	mov.b32 	%f75, %r62;
	add.f32 	%f76, %f74, %f75;
	mov.b32 	%r63, %f76;
	shfl.sync.bfly.b32 	%r65|%p15, %r63, %r40, %r32, %r34;
	mov.b32 	%f77, %r65;
	add.f32 	%f78, %f76, %f77;
	mov.b32 	%r66, %f78;
	shfl.sync.bfly.b32 	%r68|%p16, %r66, %r43, %r32, %r34;
	mov.b32 	%f79, %r68;
	add.f32 	%f80, %f78, %f79;
	mov.b32 	%r69, %f80;
	shfl.sync.bfly.b32 	%r71|%p17, %r69, %r46, %r32, %r34;
	mov.b32 	%f81, %r71;
	add.f32 	%f87, %f80, %f81;

$L__BB16_11:
	bar.sync 	0;
	setp.gt.s32 	%p18, %r2, 0;
	@%p18 bra 	$L__BB16_13;

	mad.lo.s32 	%r73, %r2, %r14, %r21;
	cvt.s64.s32 	%rd34, %r73;
	mul.lo.s32 	%r74, %r1, %r15;
	cvt.s64.s32 	%rd35, %r74;
	add.s64 	%rd36, %rd35, %rd34;
	cvta.to.global.u64 	%rd37, %rd17;
	shl.b64 	%rd38, %rd36, 2;
	add.s64 	%rd39, %rd37, %rd38;
	st.global.f32 	[%rd39], %f87;

$L__BB16_13:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_2_bs_96
.visible .entry ggml_matvec_f32_ncols_2_bs_96(
	.param .u64 ggml_matvec_f32_ncols_2_bs_96_param_0,
	.param .u64 ggml_matvec_f32_ncols_2_bs_96_param_1,
	.param .u64 ggml_matvec_f32_ncols_2_bs_96_param_2,
	.param .u32 ggml_matvec_f32_ncols_2_bs_96_param_3,
	.param .u32 ggml_matvec_f32_ncols_2_bs_96_param_4,
	.param .u32 ggml_matvec_f32_ncols_2_bs_96_param_5,
	.param .u32 ggml_matvec_f32_ncols_2_bs_96_param_6,
	.param .u32 ggml_matvec_f32_ncols_2_bs_96_param_7,
	.param .u32 ggml_matvec_f32_ncols_2_bs_96_param_8,
	.param .u32 ggml_matvec_f32_ncols_2_bs_96_param_9,
	.param .u32 ggml_matvec_f32_ncols_2_bs_96_param_10,
	.param .u32 ggml_matvec_f32_ncols_2_bs_96_param_11
)
{
	.local .align 8 .b8 	__local_depot17[8];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<30>;
	.reg .f32 	%f<146>;
	.reg .b32 	%r<113>;
	.reg .b64 	%rd<66>;


	mov.u64 	%SPL, __local_depot17;
	ld.param.u64 	%rd27, [ggml_matvec_f32_ncols_2_bs_96_param_0];
	ld.param.u64 	%rd28, [ggml_matvec_f32_ncols_2_bs_96_param_1];
	ld.param.u64 	%rd26, [ggml_matvec_f32_ncols_2_bs_96_param_2];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_2_bs_96_param_3];
	ld.param.u32 	%r19, [ggml_matvec_f32_ncols_2_bs_96_param_5];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_2_bs_96_param_6];
	ld.param.u32 	%r17, [ggml_matvec_f32_ncols_2_bs_96_param_7];
	ld.param.u32 	%r20, [ggml_matvec_f32_ncols_2_bs_96_param_8];
	ld.param.u32 	%r21, [ggml_matvec_f32_ncols_2_bs_96_param_9];
	ld.param.u32 	%r22, [ggml_matvec_f32_ncols_2_bs_96_param_10];
	ld.param.u32 	%r18, [ggml_matvec_f32_ncols_2_bs_96_param_11];
	cvta.to.global.u64 	%rd1, %rd28;
	cvta.to.global.u64 	%rd2, %rd27;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r23, %r1, %r20;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r24, %r2, %r19;
	mad.lo.s32 	%r25, %r23, %r21, %r24;
	cvt.s64.s32 	%rd4, %r25;
	mul.lo.s32 	%r26, %r1, %r22;
	cvt.s64.s32 	%rd5, %r26;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r27, %r3, 2;
	mov.u32 	%r28, data_mmv;
	add.s32 	%r4, %r28, %r27;
	@%p1 bra 	$L__BB17_2;

	mov.u32 	%r29, 0;
	st.shared.u32 	[%r4], %r29;

$L__BB17_2:
	bar.sync 	0;
	mov.f32 	%f144, 0f00000000;
	st.local.v2.f32 	[%rd3], {%f144, %f144};
	setp.ge.s32 	%p2, %r3, %r15;
	mov.f32 	%f145, %f144;
	@%p2 bra 	$L__BB17_11;

	not.b32 	%r30, %r3;
	add.s32 	%r5, %r30, %r15;
	mul.wide.u32 	%rd30, %r5, -1431655765;
	shr.u64 	%rd31, %rd30, 38;
	cvt.u32.u64 	%r31, %rd31;
	add.s32 	%r32, %r31, 1;
	and.b32  	%r110, %r32, 3;
	setp.eq.s32 	%p3, %r110, 0;
	mov.f32 	%f144, 0f00000000;
	mov.u32 	%r111, %r3;
	@%p3 bra 	$L__BB17_7;

	mul.wide.s32 	%rd32, %r16, 2;
	mul.wide.s32 	%rd33, %r3, 2;
	add.s64 	%rd34, %rd32, %rd33;
	add.s64 	%rd35, %rd34, %rd5;
	shl.b64 	%rd36, %rd35, 2;
	add.s64 	%rd62, %rd1, %rd36;
	add.s64 	%rd37, %rd33, %rd5;
	shl.b64 	%rd38, %rd37, 2;
	add.s64 	%rd61, %rd1, %rd38;
	add.s64 	%rd39, %rd33, %rd4;
	shl.b64 	%rd40, %rd39, 2;
	add.s64 	%rd60, %rd2, %rd40;
	mov.f32 	%f144, 0f00000000;
	mov.f32 	%f145, %f144;
	mov.u32 	%r111, %r3;

$L__BB17_5:
	.pragma "nounroll";
	ld.global.nc.v2.f32 	{%f19, %f20}, [%rd60];
	ld.global.nc.v2.f32 	{%f23, %f24}, [%rd61];
	fma.rn.f32 	%f27, %f19, %f23, %f145;
	fma.rn.f32 	%f145, %f20, %f24, %f27;
	ld.global.nc.v2.f32 	{%f28, %f29}, [%rd62];
	fma.rn.f32 	%f32, %f19, %f28, %f144;
	fma.rn.f32 	%f144, %f20, %f29, %f32;
	add.s32 	%r111, %r111, 96;
	add.s64 	%rd62, %rd62, 768;
	add.s64 	%rd61, %rd61, 768;
	add.s64 	%rd60, %rd60, 768;
	add.s32 	%r110, %r110, -1;
	setp.ne.s32 	%p4, %r110, 0;
	@%p4 bra 	$L__BB17_5;

	st.local.v2.f32 	[%rd3], {%f145, %f144};

$L__BB17_7:
	setp.lt.u32 	%p5, %r5, 288;
	@%p5 bra 	$L__BB17_11;

	mul.wide.s32 	%rd41, %r111, 2;
	add.s64 	%rd42, %rd41, %rd4;
	shl.b64 	%rd43, %rd42, 2;
	add.s64 	%rd44, %rd2, %rd43;
	add.s64 	%rd65, %rd44, 1536;
	add.s64 	%rd45, %rd41, %rd5;
	shl.b64 	%rd46, %rd45, 2;
	add.s64 	%rd47, %rd1, %rd46;
	add.s64 	%rd64, %rd47, 2304;
	mul.wide.s32 	%rd48, %r16, 2;
	add.s64 	%rd49, %rd45, %rd48;
	shl.b64 	%rd50, %rd49, 2;
	add.s64 	%rd51, %rd1, %rd50;
	add.s64 	%rd63, %rd51, 1536;

$L__BB17_9:
	ld.global.nc.v2.f32 	{%f33, %f34}, [%rd65+-1536];
	ld.global.nc.v2.f32 	{%f37, %f38}, [%rd64+-2304];
	fma.rn.f32 	%f41, %f33, %f37, %f145;
	fma.rn.f32 	%f42, %f34, %f38, %f41;
	ld.global.nc.v2.f32 	{%f43, %f44}, [%rd63+-1536];
	fma.rn.f32 	%f47, %f33, %f43, %f144;
	fma.rn.f32 	%f48, %f34, %f44, %f47;
	ld.global.nc.v2.f32 	{%f49, %f50}, [%rd65+-768];
	ld.global.nc.v2.f32 	{%f53, %f54}, [%rd64+-1536];
	fma.rn.f32 	%f57, %f49, %f53, %f42;
	fma.rn.f32 	%f58, %f50, %f54, %f57;
	ld.global.nc.v2.f32 	{%f59, %f60}, [%rd63+-768];
	fma.rn.f32 	%f63, %f49, %f59, %f48;
	fma.rn.f32 	%f64, %f50, %f60, %f63;
	ld.global.nc.v2.f32 	{%f65, %f66}, [%rd65];
	ld.global.nc.v2.f32 	{%f69, %f70}, [%rd64+-768];
	fma.rn.f32 	%f73, %f65, %f69, %f58;
	fma.rn.f32 	%f74, %f66, %f70, %f73;
	ld.global.nc.v2.f32 	{%f75, %f76}, [%rd63];
	fma.rn.f32 	%f79, %f65, %f75, %f64;
	fma.rn.f32 	%f80, %f66, %f76, %f79;
	ld.global.nc.v2.f32 	{%f81, %f82}, [%rd65+768];
	ld.global.nc.v2.f32 	{%f85, %f86}, [%rd64];
	fma.rn.f32 	%f89, %f81, %f85, %f74;
	fma.rn.f32 	%f145, %f82, %f86, %f89;
	ld.global.nc.v2.f32 	{%f90, %f91}, [%rd63+768];
	fma.rn.f32 	%f94, %f81, %f90, %f80;
	fma.rn.f32 	%f144, %f82, %f91, %f94;
	add.s64 	%rd65, %rd65, 3072;
	add.s64 	%rd64, %rd64, 3072;
	add.s64 	%rd63, %rd63, 3072;
	add.s32 	%r111, %r111, 384;
	setp.lt.s32 	%p6, %r111, %r15;
	@%p6 bra 	$L__BB17_9;

	st.local.v2.f32 	[%rd3], {%f145, %f144};

$L__BB17_11:
	shr.s32 	%r33, %r3, 31;
	shr.u32 	%r34, %r33, 27;
	add.s32 	%r35, %r3, %r34;
	shr.s32 	%r36, %r35, 5;
	shl.b32 	%r37, %r36, 2;
	add.s32 	%r14, %r28, %r37;
	mov.u32 	%r39, 2;
	mov.b32 	%r40, %f145;
	mov.u32 	%r41, 31;
	mov.u32 	%r42, 16;
	mov.u32 	%r43, -1;
	shfl.sync.bfly.b32 	%r44|%p7, %r40, %r42, %r41, %r43;
	mov.b32 	%f95, %r44;
	add.f32 	%f96, %f145, %f95;
	mov.b32 	%r45, %f96;
	mov.u32 	%r46, 8;
	shfl.sync.bfly.b32 	%r47|%p8, %r45, %r46, %r41, %r43;
	mov.b32 	%f97, %r47;
	add.f32 	%f98, %f96, %f97;
	mov.b32 	%r48, %f98;
	mov.u32 	%r49, 4;
	shfl.sync.bfly.b32 	%r50|%p9, %r48, %r49, %r41, %r43;
	mov.b32 	%f99, %r50;
	add.f32 	%f100, %f98, %f99;
	mov.b32 	%r51, %f100;
	shfl.sync.bfly.b32 	%r52|%p10, %r51, %r39, %r41, %r43;
	mov.b32 	%f101, %r52;
	add.f32 	%f102, %f100, %f101;
	mov.b32 	%r53, %f102;
	mov.u32 	%r54, 1;
	shfl.sync.bfly.b32 	%r55|%p11, %r53, %r54, %r41, %r43;
	mov.b32 	%f103, %r55;
	add.f32 	%f104, %f102, %f103;
	st.local.f32 	[%rd3], %f104;
	st.shared.f32 	[%r14], %f104;
	bar.sync 	0;
	@%p1 bra 	$L__BB17_13;

	ld.shared.f32 	%f105, [%r4];
	mov.b32 	%r56, %f105;
	shfl.sync.bfly.b32 	%r60|%p13, %r56, %r42, %r41, %r43;
	mov.b32 	%f106, %r60;
	add.f32 	%f107, %f105, %f106;
	mov.b32 	%r61, %f107;
	shfl.sync.bfly.b32 	%r63|%p14, %r61, %r46, %r41, %r43;
	mov.b32 	%f108, %r63;
	add.f32 	%f109, %f107, %f108;
	mov.b32 	%r64, %f109;
	shfl.sync.bfly.b32 	%r66|%p15, %r64, %r49, %r41, %r43;
	mov.b32 	%f110, %r66;
	add.f32 	%f111, %f109, %f110;
	mov.b32 	%r67, %f111;
	shfl.sync.bfly.b32 	%r69|%p16, %r67, %r39, %r41, %r43;
	mov.b32 	%f112, %r69;
	add.f32 	%f113, %f111, %f112;
	mov.b32 	%r70, %f113;
	shfl.sync.bfly.b32 	%r72|%p17, %r70, %r54, %r41, %r43;
	mov.b32 	%f114, %r72;
	add.f32 	%f115, %f113, %f114;
	st.local.f32 	[%rd3], %f115;

$L__BB17_13:
	bar.sync 	0;
	mov.b32 	%r73, %f144;
	shfl.sync.bfly.b32 	%r77|%p19, %r73, %r42, %r41, %r43;
	mov.b32 	%f116, %r77;
	add.f32 	%f117, %f144, %f116;
	mov.b32 	%r78, %f117;
	shfl.sync.bfly.b32 	%r80|%p20, %r78, %r46, %r41, %r43;
	mov.b32 	%f118, %r80;
	add.f32 	%f119, %f117, %f118;
	mov.b32 	%r81, %f119;
	shfl.sync.bfly.b32 	%r83|%p21, %r81, %r49, %r41, %r43;
	mov.b32 	%f120, %r83;
	add.f32 	%f121, %f119, %f120;
	mov.b32 	%r84, %f121;
	shfl.sync.bfly.b32 	%r86|%p22, %r84, %r39, %r41, %r43;
	mov.b32 	%f122, %r86;
	add.f32 	%f123, %f121, %f122;
	mov.b32 	%r87, %f123;
	shfl.sync.bfly.b32 	%r89|%p23, %r87, %r54, %r41, %r43;
	mov.b32 	%f124, %r89;
	add.f32 	%f125, %f123, %f124;
	st.local.f32 	[%rd3+4], %f125;
	st.shared.f32 	[%r14], %f125;
	bar.sync 	0;
	@%p1 bra 	$L__BB17_15;

	ld.shared.f32 	%f126, [%r4];
	mov.b32 	%r90, %f126;
	mov.u32 	%r91, 31;
	mov.u32 	%r92, 16;
	mov.u32 	%r93, -1;
	shfl.sync.bfly.b32 	%r94|%p24, %r90, %r92, %r91, %r93;
	mov.b32 	%f127, %r94;
	add.f32 	%f128, %f126, %f127;
	mov.b32 	%r95, %f128;
	mov.u32 	%r96, 8;
	shfl.sync.bfly.b32 	%r97|%p25, %r95, %r96, %r91, %r93;
	mov.b32 	%f129, %r97;
	add.f32 	%f130, %f128, %f129;
	mov.b32 	%r98, %f130;
	mov.u32 	%r99, 4;
	shfl.sync.bfly.b32 	%r100|%p26, %r98, %r99, %r91, %r93;
	mov.b32 	%f131, %r100;
	add.f32 	%f132, %f130, %f131;
	mov.b32 	%r101, %f132;
	mov.u32 	%r102, 2;
	shfl.sync.bfly.b32 	%r103|%p27, %r101, %r102, %r91, %r93;
	mov.b32 	%f133, %r103;
	add.f32 	%f134, %f132, %f133;
	mov.b32 	%r104, %f134;
	mov.u32 	%r105, 1;
	shfl.sync.bfly.b32 	%r106|%p28, %r104, %r105, %r91, %r93;
	mov.b32 	%f135, %r106;
	add.f32 	%f136, %f134, %f135;
	st.local.f32 	[%rd3+4], %f136;

$L__BB17_15:
	bar.sync 	0;
	setp.gt.s32 	%p29, %r3, 1;
	@%p29 bra 	$L__BB17_17;

	mul.wide.s32 	%rd52, %r3, 4;
	add.s64 	%rd53, %rd3, %rd52;
	ld.local.f32 	%f137, [%rd53];
	mad.lo.s32 	%r107, %r3, %r17, %r2;
	cvt.s64.s32 	%rd54, %r107;
	mul.lo.s32 	%r108, %r1, %r18;
	cvt.s64.s32 	%rd55, %r108;
	add.s64 	%rd56, %rd55, %rd54;
	cvta.to.global.u64 	%rd57, %rd26;
	shl.b64 	%rd58, %rd56, 2;
	add.s64 	%rd59, %rd57, %rd58;
	st.global.f32 	[%rd59], %f137;

$L__BB17_17:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_3_bs_96
.visible .entry ggml_matvec_f32_ncols_3_bs_96(
	.param .u64 ggml_matvec_f32_ncols_3_bs_96_param_0,
	.param .u64 ggml_matvec_f32_ncols_3_bs_96_param_1,
	.param .u64 ggml_matvec_f32_ncols_3_bs_96_param_2,
	.param .u32 ggml_matvec_f32_ncols_3_bs_96_param_3,
	.param .u32 ggml_matvec_f32_ncols_3_bs_96_param_4,
	.param .u32 ggml_matvec_f32_ncols_3_bs_96_param_5,
	.param .u32 ggml_matvec_f32_ncols_3_bs_96_param_6,
	.param .u32 ggml_matvec_f32_ncols_3_bs_96_param_7,
	.param .u32 ggml_matvec_f32_ncols_3_bs_96_param_8,
	.param .u32 ggml_matvec_f32_ncols_3_bs_96_param_9,
	.param .u32 ggml_matvec_f32_ncols_3_bs_96_param_10,
	.param .u32 ggml_matvec_f32_ncols_3_bs_96_param_11
)
{
	.local .align 4 .b8 	__local_depot18[12];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<41>;
	.reg .f32 	%f<208>;
	.reg .b32 	%r<154>;
	.reg .b64 	%rd<74>;


	mov.u64 	%SPL, __local_depot18;
	ld.param.u64 	%rd29, [ggml_matvec_f32_ncols_3_bs_96_param_0];
	ld.param.u64 	%rd30, [ggml_matvec_f32_ncols_3_bs_96_param_1];
	ld.param.u64 	%rd28, [ggml_matvec_f32_ncols_3_bs_96_param_2];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_3_bs_96_param_3];
	ld.param.u32 	%r19, [ggml_matvec_f32_ncols_3_bs_96_param_5];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_3_bs_96_param_6];
	ld.param.u32 	%r17, [ggml_matvec_f32_ncols_3_bs_96_param_7];
	ld.param.u32 	%r20, [ggml_matvec_f32_ncols_3_bs_96_param_8];
	ld.param.u32 	%r21, [ggml_matvec_f32_ncols_3_bs_96_param_9];
	ld.param.u32 	%r22, [ggml_matvec_f32_ncols_3_bs_96_param_10];
	ld.param.u32 	%r18, [ggml_matvec_f32_ncols_3_bs_96_param_11];
	cvta.to.global.u64 	%rd73, %rd30;
	cvta.to.global.u64 	%rd2, %rd29;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r23, %r1, %r20;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r24, %r2, %r19;
	mad.lo.s32 	%r25, %r23, %r21, %r24;
	cvt.s64.s32 	%rd4, %r25;
	mul.lo.s32 	%r26, %r1, %r22;
	cvt.s64.s32 	%rd5, %r26;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r27, %r3, 2;
	mov.u32 	%r28, data_mmv;
	add.s32 	%r4, %r28, %r27;
	@%p1 bra 	$L__BB18_2;

	mov.u32 	%r29, 0;
	st.shared.u32 	[%r4], %r29;

$L__BB18_2:
	bar.sync 	0;
	mov.f32 	%f205, 0f00000000;
	mov.u32 	%r30, 0;
	st.local.u32 	[%rd3], %r30;
	st.local.u32 	[%rd3+4], %r30;
	st.local.u32 	[%rd3+8], %r30;
	setp.ge.s32 	%p2, %r3, %r15;
	mov.f32 	%f206, %f205;
	mov.f32 	%f207, %f205;
	@%p2 bra 	$L__BB18_11;

	not.b32 	%r31, %r3;
	add.s32 	%r5, %r31, %r15;
	mul.wide.u32 	%rd32, %r5, -1431655765;
	shr.u64 	%rd33, %rd32, 38;
	cvt.u32.u64 	%r32, %rd33;
	add.s32 	%r33, %r32, 1;
	and.b32  	%r151, %r33, 3;
	setp.eq.s32 	%p3, %r151, 0;
	mov.f32 	%f205, 0f00000000;
	mov.u32 	%r152, %r3;
	@%p3 bra 	$L__BB18_7;

	shl.b32 	%r34, %r16, 1;
	add.s32 	%r35, %r3, %r34;
	mul.wide.s32 	%rd34, %r35, 2;
	add.s64 	%rd35, %rd34, %rd5;
	shl.b64 	%rd36, %rd35, 2;
	add.s64 	%rd71, %rd73, %rd36;
	mul.wide.s32 	%rd37, %r16, 2;
	mul.wide.s32 	%rd38, %r3, 2;
	add.s64 	%rd39, %rd37, %rd38;
	add.s64 	%rd40, %rd39, %rd5;
	shl.b64 	%rd41, %rd40, 2;
	add.s64 	%rd70, %rd73, %rd41;
	add.s64 	%rd42, %rd38, %rd5;
	shl.b64 	%rd43, %rd42, 2;
	add.s64 	%rd69, %rd73, %rd43;
	add.s64 	%rd44, %rd38, %rd4;
	shl.b64 	%rd45, %rd44, 2;
	add.s64 	%rd68, %rd2, %rd45;
	mov.f32 	%f205, 0f00000000;
	mov.f32 	%f206, %f205;
	mov.f32 	%f207, %f205;
	mov.u32 	%r152, %r3;

$L__BB18_5:
	.pragma "nounroll";
	ld.global.nc.v2.f32 	{%f28, %f29}, [%rd68];
	ld.global.nc.v2.f32 	{%f32, %f33}, [%rd69];
	fma.rn.f32 	%f36, %f28, %f32, %f207;
	fma.rn.f32 	%f207, %f29, %f33, %f36;
	ld.global.nc.v2.f32 	{%f37, %f38}, [%rd70];
	fma.rn.f32 	%f41, %f28, %f37, %f206;
	fma.rn.f32 	%f206, %f29, %f38, %f41;
	ld.global.nc.v2.f32 	{%f42, %f43}, [%rd71];
	fma.rn.f32 	%f46, %f28, %f42, %f205;
	fma.rn.f32 	%f205, %f29, %f43, %f46;
	add.s32 	%r152, %r152, 96;
	add.s64 	%rd71, %rd71, 768;
	add.s64 	%rd70, %rd70, 768;
	add.s64 	%rd69, %rd69, 768;
	add.s64 	%rd68, %rd68, 768;
	add.s32 	%r151, %r151, -1;
	setp.ne.s32 	%p4, %r151, 0;
	@%p4 bra 	$L__BB18_5;

	st.local.f32 	[%rd3], %f207;
	st.local.f32 	[%rd3+4], %f206;
	st.local.f32 	[%rd3+8], %f205;

$L__BB18_7:
	setp.lt.u32 	%p5, %r5, 288;
	@%p5 bra 	$L__BB18_11;

	add.s32 	%r36, %r152, %r16;
	shl.b32 	%r37, %r16, 1;
	add.s32 	%r38, %r152, %r37;
	add.s32 	%r39, %r36, 96;
	mul.wide.s32 	%rd46, %r39, 8;
	shl.b64 	%rd47, %rd5, 2;
	add.s64 	%rd19, %rd46, %rd47;
	mul.wide.s32 	%rd48, %r38, 8;
	add.s64 	%rd20, %rd48, %rd47;
	mul.wide.s32 	%rd49, %r152, 2;
	add.s64 	%rd50, %rd49, %rd4;
	shl.b64 	%rd51, %rd50, 2;
	add.s64 	%rd52, %rd2, %rd51;
	add.s64 	%rd72, %rd52, 1536;
	mul.wide.s32 	%rd53, %r152, 8;
	add.s64 	%rd22, %rd53, %rd47;
	mul.wide.s32 	%rd54, %r16, 8;
	add.s64 	%rd55, %rd53, %rd54;
	add.s64 	%rd23, %rd55, %rd47;

$L__BB18_9:
	ld.global.nc.v2.f32 	{%f47, %f48}, [%rd72+-1536];
	add.s64 	%rd56, %rd73, %rd22;
	ld.global.nc.v2.f32 	{%f51, %f52}, [%rd56];
	fma.rn.f32 	%f55, %f47, %f51, %f207;
	fma.rn.f32 	%f56, %f48, %f52, %f55;
	add.s64 	%rd57, %rd73, %rd23;
	ld.global.nc.v2.f32 	{%f57, %f58}, [%rd57];
	fma.rn.f32 	%f61, %f47, %f57, %f206;
	fma.rn.f32 	%f62, %f48, %f58, %f61;
	add.s64 	%rd58, %rd73, %rd20;
	ld.global.nc.v2.f32 	{%f63, %f64}, [%rd58];
	fma.rn.f32 	%f67, %f47, %f63, %f205;
	fma.rn.f32 	%f68, %f48, %f64, %f67;
	ld.global.nc.v2.f32 	{%f69, %f70}, [%rd72+-768];
	ld.global.nc.v2.f32 	{%f73, %f74}, [%rd56+768];
	fma.rn.f32 	%f77, %f69, %f73, %f56;
	fma.rn.f32 	%f78, %f70, %f74, %f77;
	add.s64 	%rd59, %rd73, %rd19;
	ld.global.nc.v2.f32 	{%f79, %f80}, [%rd59];
	fma.rn.f32 	%f83, %f69, %f79, %f62;
	fma.rn.f32 	%f84, %f70, %f80, %f83;
	ld.global.nc.v2.f32 	{%f85, %f86}, [%rd58+768];
	fma.rn.f32 	%f89, %f69, %f85, %f68;
	fma.rn.f32 	%f90, %f70, %f86, %f89;
	ld.global.nc.v2.f32 	{%f91, %f92}, [%rd72];
	ld.global.nc.v2.f32 	{%f95, %f96}, [%rd56+1536];
	fma.rn.f32 	%f99, %f91, %f95, %f78;
	fma.rn.f32 	%f100, %f92, %f96, %f99;
	ld.global.nc.v2.f32 	{%f101, %f102}, [%rd59+768];
	fma.rn.f32 	%f105, %f91, %f101, %f84;
	fma.rn.f32 	%f106, %f92, %f102, %f105;
	ld.global.nc.v2.f32 	{%f107, %f108}, [%rd58+1536];
	fma.rn.f32 	%f111, %f91, %f107, %f90;
	fma.rn.f32 	%f112, %f92, %f108, %f111;
	ld.global.nc.v2.f32 	{%f113, %f114}, [%rd72+768];
	ld.global.nc.v2.f32 	{%f117, %f118}, [%rd56+2304];
	fma.rn.f32 	%f121, %f113, %f117, %f100;
	fma.rn.f32 	%f207, %f114, %f118, %f121;
	ld.global.nc.v2.f32 	{%f122, %f123}, [%rd59+1536];
	fma.rn.f32 	%f126, %f113, %f122, %f106;
	fma.rn.f32 	%f206, %f114, %f123, %f126;
	ld.global.nc.v2.f32 	{%f127, %f128}, [%rd58+2304];
	fma.rn.f32 	%f131, %f113, %f127, %f112;
	fma.rn.f32 	%f205, %f114, %f128, %f131;
	add.s64 	%rd73, %rd73, 3072;
	add.s64 	%rd72, %rd72, 3072;
	add.s32 	%r152, %r152, 384;
	setp.lt.s32 	%p6, %r152, %r15;
	@%p6 bra 	$L__BB18_9;

	st.local.f32 	[%rd3], %f207;
	st.local.f32 	[%rd3+4], %f206;
	st.local.f32 	[%rd3+8], %f205;

$L__BB18_11:
	shr.s32 	%r40, %r3, 31;
	shr.u32 	%r41, %r40, 27;
	add.s32 	%r42, %r3, %r41;
	shr.s32 	%r43, %r42, 5;
	shl.b32 	%r44, %r43, 2;
	add.s32 	%r14, %r28, %r44;
	mov.u32 	%r46, 2;
	mov.b32 	%r47, %f207;
	mov.u32 	%r48, 31;
	mov.u32 	%r49, 16;
	mov.u32 	%r50, -1;
	shfl.sync.bfly.b32 	%r51|%p7, %r47, %r49, %r48, %r50;
	mov.b32 	%f132, %r51;
	add.f32 	%f133, %f207, %f132;
	mov.b32 	%r52, %f133;
	mov.u32 	%r53, 8;
	shfl.sync.bfly.b32 	%r54|%p8, %r52, %r53, %r48, %r50;
	mov.b32 	%f134, %r54;
	add.f32 	%f135, %f133, %f134;
	mov.b32 	%r55, %f135;
	mov.u32 	%r56, 4;
	shfl.sync.bfly.b32 	%r57|%p9, %r55, %r56, %r48, %r50;
	mov.b32 	%f136, %r57;
	add.f32 	%f137, %f135, %f136;
	mov.b32 	%r58, %f137;
	shfl.sync.bfly.b32 	%r59|%p10, %r58, %r46, %r48, %r50;
	mov.b32 	%f138, %r59;
	add.f32 	%f139, %f137, %f138;
	mov.b32 	%r60, %f139;
	mov.u32 	%r61, 1;
	shfl.sync.bfly.b32 	%r62|%p11, %r60, %r61, %r48, %r50;
	mov.b32 	%f140, %r62;
	add.f32 	%f141, %f139, %f140;
	st.local.f32 	[%rd3], %f141;
	st.shared.f32 	[%r14], %f141;
	bar.sync 	0;
	@%p1 bra 	$L__BB18_13;

	ld.shared.f32 	%f142, [%r4];
	mov.b32 	%r63, %f142;
	shfl.sync.bfly.b32 	%r67|%p13, %r63, %r49, %r48, %r50;
	mov.b32 	%f143, %r67;
	add.f32 	%f144, %f142, %f143;
	mov.b32 	%r68, %f144;
	shfl.sync.bfly.b32 	%r70|%p14, %r68, %r53, %r48, %r50;
	mov.b32 	%f145, %r70;
	add.f32 	%f146, %f144, %f145;
	mov.b32 	%r71, %f146;
	shfl.sync.bfly.b32 	%r73|%p15, %r71, %r56, %r48, %r50;
	mov.b32 	%f147, %r73;
	add.f32 	%f148, %f146, %f147;
	mov.b32 	%r74, %f148;
	shfl.sync.bfly.b32 	%r76|%p16, %r74, %r46, %r48, %r50;
	mov.b32 	%f149, %r76;
	add.f32 	%f150, %f148, %f149;
	mov.b32 	%r77, %f150;
	shfl.sync.bfly.b32 	%r79|%p17, %r77, %r61, %r48, %r50;
	mov.b32 	%f151, %r79;
	add.f32 	%f152, %f150, %f151;
	st.local.f32 	[%rd3], %f152;

$L__BB18_13:
	bar.sync 	0;
	mov.b32 	%r80, %f206;
	shfl.sync.bfly.b32 	%r84|%p19, %r80, %r49, %r48, %r50;
	mov.b32 	%f153, %r84;
	add.f32 	%f154, %f206, %f153;
	mov.b32 	%r85, %f154;
	shfl.sync.bfly.b32 	%r87|%p20, %r85, %r53, %r48, %r50;
	mov.b32 	%f155, %r87;
	add.f32 	%f156, %f154, %f155;
	mov.b32 	%r88, %f156;
	shfl.sync.bfly.b32 	%r90|%p21, %r88, %r56, %r48, %r50;
	mov.b32 	%f157, %r90;
	add.f32 	%f158, %f156, %f157;
	mov.b32 	%r91, %f158;
	shfl.sync.bfly.b32 	%r93|%p22, %r91, %r46, %r48, %r50;
	mov.b32 	%f159, %r93;
	add.f32 	%f160, %f158, %f159;
	mov.b32 	%r94, %f160;
	shfl.sync.bfly.b32 	%r96|%p23, %r94, %r61, %r48, %r50;
	mov.b32 	%f161, %r96;
	add.f32 	%f162, %f160, %f161;
	st.local.f32 	[%rd3+4], %f162;
	st.shared.f32 	[%r14], %f162;
	bar.sync 	0;
	@%p1 bra 	$L__BB18_15;

	ld.shared.f32 	%f163, [%r4];
	mov.b32 	%r97, %f163;
	mov.u32 	%r98, 31;
	mov.u32 	%r99, 16;
	mov.u32 	%r100, -1;
	shfl.sync.bfly.b32 	%r101|%p24, %r97, %r99, %r98, %r100;
	mov.b32 	%f164, %r101;
	add.f32 	%f165, %f163, %f164;
	mov.b32 	%r102, %f165;
	mov.u32 	%r103, 8;
	shfl.sync.bfly.b32 	%r104|%p25, %r102, %r103, %r98, %r100;
	mov.b32 	%f166, %r104;
	add.f32 	%f167, %f165, %f166;
	mov.b32 	%r105, %f167;
	mov.u32 	%r106, 4;
	shfl.sync.bfly.b32 	%r107|%p26, %r105, %r106, %r98, %r100;
	mov.b32 	%f168, %r107;
	add.f32 	%f169, %f167, %f168;
	mov.b32 	%r108, %f169;
	mov.u32 	%r109, 2;
	shfl.sync.bfly.b32 	%r110|%p27, %r108, %r109, %r98, %r100;
	mov.b32 	%f170, %r110;
	add.f32 	%f171, %f169, %f170;
	mov.b32 	%r111, %f171;
	mov.u32 	%r112, 1;
	shfl.sync.bfly.b32 	%r113|%p28, %r111, %r112, %r98, %r100;
	mov.b32 	%f172, %r113;
	add.f32 	%f173, %f171, %f172;
	st.local.f32 	[%rd3+4], %f173;

$L__BB18_15:
	bar.sync 	0;
	mov.b32 	%r114, %f205;
	mov.u32 	%r115, 31;
	mov.u32 	%r116, 16;
	mov.u32 	%r117, -1;
	shfl.sync.bfly.b32 	%r118|%p30, %r114, %r116, %r115, %r117;
	mov.b32 	%f174, %r118;
	add.f32 	%f175, %f205, %f174;
	mov.b32 	%r119, %f175;
	mov.u32 	%r120, 8;
	shfl.sync.bfly.b32 	%r121|%p31, %r119, %r120, %r115, %r117;
	mov.b32 	%f176, %r121;
	add.f32 	%f177, %f175, %f176;
	mov.b32 	%r122, %f177;
	mov.u32 	%r123, 4;
	shfl.sync.bfly.b32 	%r124|%p32, %r122, %r123, %r115, %r117;
	mov.b32 	%f178, %r124;
	add.f32 	%f179, %f177, %f178;
	mov.b32 	%r125, %f179;
	mov.u32 	%r126, 2;
	shfl.sync.bfly.b32 	%r127|%p33, %r125, %r126, %r115, %r117;
	mov.b32 	%f180, %r127;
	add.f32 	%f181, %f179, %f180;
	mov.b32 	%r128, %f181;
	mov.u32 	%r129, 1;
	shfl.sync.bfly.b32 	%r130|%p34, %r128, %r129, %r115, %r117;
	mov.b32 	%f182, %r130;
	add.f32 	%f183, %f181, %f182;
	st.local.f32 	[%rd3+8], %f183;
	st.shared.f32 	[%r14], %f183;
	bar.sync 	0;
	@%p1 bra 	$L__BB18_17;

	ld.shared.f32 	%f184, [%r4];
	mov.b32 	%r131, %f184;
	shfl.sync.bfly.b32 	%r135|%p35, %r131, %r116, %r115, %r117;
	mov.b32 	%f185, %r135;
	add.f32 	%f186, %f184, %f185;
	mov.b32 	%r136, %f186;
	shfl.sync.bfly.b32 	%r138|%p36, %r136, %r120, %r115, %r117;
	mov.b32 	%f187, %r138;
	add.f32 	%f188, %f186, %f187;
	mov.b32 	%r139, %f188;
	shfl.sync.bfly.b32 	%r141|%p37, %r139, %r123, %r115, %r117;
	mov.b32 	%f189, %r141;
	add.f32 	%f190, %f188, %f189;
	mov.b32 	%r142, %f190;
	shfl.sync.bfly.b32 	%r144|%p38, %r142, %r126, %r115, %r117;
	mov.b32 	%f191, %r144;
	add.f32 	%f192, %f190, %f191;
	mov.b32 	%r145, %f192;
	shfl.sync.bfly.b32 	%r147|%p39, %r145, %r129, %r115, %r117;
	mov.b32 	%f193, %r147;
	add.f32 	%f194, %f192, %f193;
	st.local.f32 	[%rd3+8], %f194;

$L__BB18_17:
	bar.sync 	0;
	setp.gt.s32 	%p40, %r3, 2;
	@%p40 bra 	$L__BB18_19;

	mul.wide.s32 	%rd60, %r3, 4;
	add.s64 	%rd61, %rd3, %rd60;
	ld.local.f32 	%f195, [%rd61];
	mad.lo.s32 	%r148, %r3, %r17, %r2;
	cvt.s64.s32 	%rd62, %r148;
	mul.lo.s32 	%r149, %r1, %r18;
	cvt.s64.s32 	%rd63, %r149;
	add.s64 	%rd64, %rd63, %rd62;
	cvta.to.global.u64 	%rd65, %rd28;
	shl.b64 	%rd66, %rd64, 2;
	add.s64 	%rd67, %rd65, %rd66;
	st.global.f32 	[%rd67], %f195;

$L__BB18_19:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_4_bs_96
.visible .entry ggml_matvec_f32_ncols_4_bs_96(
	.param .u64 ggml_matvec_f32_ncols_4_bs_96_param_0,
	.param .u64 ggml_matvec_f32_ncols_4_bs_96_param_1,
	.param .u64 ggml_matvec_f32_ncols_4_bs_96_param_2,
	.param .u32 ggml_matvec_f32_ncols_4_bs_96_param_3,
	.param .u32 ggml_matvec_f32_ncols_4_bs_96_param_4,
	.param .u32 ggml_matvec_f32_ncols_4_bs_96_param_5,
	.param .u32 ggml_matvec_f32_ncols_4_bs_96_param_6,
	.param .u32 ggml_matvec_f32_ncols_4_bs_96_param_7,
	.param .u32 ggml_matvec_f32_ncols_4_bs_96_param_8,
	.param .u32 ggml_matvec_f32_ncols_4_bs_96_param_9,
	.param .u32 ggml_matvec_f32_ncols_4_bs_96_param_10,
	.param .u32 ggml_matvec_f32_ncols_4_bs_96_param_11
)
{
	.local .align 16 .b8 	__local_depot19[16];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<52>;
	.reg .f32 	%f<270>;
	.reg .b32 	%r<189>;
	.reg .b64 	%rd<85>;


	mov.u64 	%SPL, __local_depot19;
	ld.param.u64 	%rd34, [ggml_matvec_f32_ncols_4_bs_96_param_0];
	ld.param.u64 	%rd35, [ggml_matvec_f32_ncols_4_bs_96_param_1];
	ld.param.u64 	%rd33, [ggml_matvec_f32_ncols_4_bs_96_param_2];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_4_bs_96_param_3];
	ld.param.u32 	%r19, [ggml_matvec_f32_ncols_4_bs_96_param_5];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_4_bs_96_param_6];
	ld.param.u32 	%r17, [ggml_matvec_f32_ncols_4_bs_96_param_7];
	ld.param.u32 	%r20, [ggml_matvec_f32_ncols_4_bs_96_param_8];
	ld.param.u32 	%r21, [ggml_matvec_f32_ncols_4_bs_96_param_9];
	ld.param.u32 	%r22, [ggml_matvec_f32_ncols_4_bs_96_param_10];
	ld.param.u32 	%r18, [ggml_matvec_f32_ncols_4_bs_96_param_11];
	cvta.to.global.u64 	%rd84, %rd35;
	cvta.to.global.u64 	%rd2, %rd34;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r23, %r1, %r20;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r24, %r2, %r19;
	mad.lo.s32 	%r25, %r23, %r21, %r24;
	cvt.s64.s32 	%rd4, %r25;
	mul.lo.s32 	%r26, %r1, %r22;
	cvt.s64.s32 	%rd5, %r26;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r27, %r3, 2;
	mov.u32 	%r28, data_mmv;
	add.s32 	%r4, %r28, %r27;
	@%p1 bra 	$L__BB19_2;

	mov.u32 	%r29, 0;
	st.shared.u32 	[%r4], %r29;

$L__BB19_2:
	bar.sync 	0;
	mov.f32 	%f266, 0f00000000;
	st.local.v4.f32 	[%rd3], {%f266, %f266, %f266, %f266};
	setp.ge.s32 	%p2, %r3, %r15;
	mov.f32 	%f267, %f266;
	mov.f32 	%f268, %f266;
	mov.f32 	%f269, %f266;
	@%p2 bra 	$L__BB19_11;

	not.b32 	%r30, %r3;
	add.s32 	%r5, %r30, %r15;
	mul.wide.u32 	%rd37, %r5, -1431655765;
	shr.u64 	%rd38, %rd37, 38;
	cvt.u32.u64 	%r31, %rd38;
	add.s32 	%r32, %r31, 1;
	and.b32  	%r186, %r32, 3;
	setp.eq.s32 	%p3, %r186, 0;
	mov.f32 	%f266, 0f00000000;
	mov.u32 	%r187, %r3;
	@%p3 bra 	$L__BB19_7;

	shl.b32 	%r33, %r16, 1;
	add.s32 	%r34, %r3, %r33;
	mul.wide.s32 	%rd39, %r34, 2;
	add.s64 	%rd40, %rd39, %rd5;
	shl.b64 	%rd41, %rd40, 2;
	add.s64 	%rd82, %rd84, %rd41;
	mad.lo.s32 	%r35, %r16, 3, %r3;
	mul.wide.s32 	%rd42, %r35, 2;
	add.s64 	%rd43, %rd42, %rd5;
	shl.b64 	%rd44, %rd43, 2;
	add.s64 	%rd81, %rd84, %rd44;
	mul.wide.s32 	%rd45, %r16, 2;
	mul.wide.s32 	%rd46, %r3, 2;
	add.s64 	%rd47, %rd45, %rd46;
	add.s64 	%rd48, %rd47, %rd5;
	shl.b64 	%rd49, %rd48, 2;
	add.s64 	%rd80, %rd84, %rd49;
	add.s64 	%rd50, %rd46, %rd5;
	shl.b64 	%rd51, %rd50, 2;
	add.s64 	%rd79, %rd84, %rd51;
	add.s64 	%rd52, %rd46, %rd4;
	shl.b64 	%rd53, %rd52, 2;
	add.s64 	%rd78, %rd2, %rd53;
	mov.f32 	%f266, 0f00000000;
	mov.f32 	%f267, %f266;
	mov.f32 	%f268, %f266;
	mov.f32 	%f269, %f266;
	mov.u32 	%r187, %r3;

$L__BB19_5:
	.pragma "nounroll";
	ld.global.nc.v2.f32 	{%f37, %f38}, [%rd78];
	ld.global.nc.v2.f32 	{%f41, %f42}, [%rd79];
	fma.rn.f32 	%f45, %f37, %f41, %f269;
	fma.rn.f32 	%f269, %f38, %f42, %f45;
	ld.global.nc.v2.f32 	{%f46, %f47}, [%rd80];
	fma.rn.f32 	%f50, %f37, %f46, %f268;
	fma.rn.f32 	%f268, %f38, %f47, %f50;
	ld.global.nc.v2.f32 	{%f51, %f52}, [%rd82];
	fma.rn.f32 	%f55, %f37, %f51, %f267;
	fma.rn.f32 	%f267, %f38, %f52, %f55;
	ld.global.nc.v2.f32 	{%f56, %f57}, [%rd81];
	fma.rn.f32 	%f60, %f37, %f56, %f266;
	fma.rn.f32 	%f266, %f38, %f57, %f60;
	add.s32 	%r187, %r187, 96;
	add.s64 	%rd82, %rd82, 768;
	add.s64 	%rd81, %rd81, 768;
	add.s64 	%rd80, %rd80, 768;
	add.s64 	%rd79, %rd79, 768;
	add.s64 	%rd78, %rd78, 768;
	add.s32 	%r186, %r186, -1;
	setp.ne.s32 	%p4, %r186, 0;
	@%p4 bra 	$L__BB19_5;

	st.local.v4.f32 	[%rd3], {%f269, %f268, %f267, %f266};

$L__BB19_7:
	setp.lt.u32 	%p5, %r5, 288;
	@%p5 bra 	$L__BB19_11;

	add.s32 	%r36, %r187, %r16;
	shl.b32 	%r37, %r16, 1;
	add.s32 	%r38, %r187, %r37;
	mad.lo.s32 	%r39, %r16, 3, %r187;
	add.s32 	%r40, %r36, 96;
	mul.wide.s32 	%rd54, %r40, 8;
	shl.b64 	%rd55, %rd5, 2;
	add.s64 	%rd23, %rd54, %rd55;
	mul.wide.s32 	%rd56, %r38, 8;
	add.s64 	%rd24, %rd56, %rd55;
	mul.wide.s32 	%rd57, %r39, 8;
	add.s64 	%rd25, %rd57, %rd55;
	mul.wide.s32 	%rd58, %r187, 2;
	add.s64 	%rd59, %rd58, %rd4;
	shl.b64 	%rd60, %rd59, 2;
	add.s64 	%rd61, %rd2, %rd60;
	add.s64 	%rd83, %rd61, 1536;
	mul.wide.s32 	%rd62, %r187, 8;
	add.s64 	%rd27, %rd62, %rd55;
	mul.wide.s32 	%rd63, %r16, 8;
	add.s64 	%rd64, %rd62, %rd63;
	add.s64 	%rd28, %rd64, %rd55;

$L__BB19_9:
	ld.global.nc.v2.f32 	{%f61, %f62}, [%rd83+-1536];
	add.s64 	%rd65, %rd84, %rd27;
	ld.global.nc.v2.f32 	{%f65, %f66}, [%rd65];
	fma.rn.f32 	%f69, %f61, %f65, %f269;
	fma.rn.f32 	%f70, %f62, %f66, %f69;
	add.s64 	%rd66, %rd84, %rd28;
	ld.global.nc.v2.f32 	{%f71, %f72}, [%rd66];
	fma.rn.f32 	%f75, %f61, %f71, %f268;
	fma.rn.f32 	%f76, %f62, %f72, %f75;
	add.s64 	%rd67, %rd84, %rd24;
	ld.global.nc.v2.f32 	{%f77, %f78}, [%rd67];
	fma.rn.f32 	%f81, %f61, %f77, %f267;
	fma.rn.f32 	%f82, %f62, %f78, %f81;
	add.s64 	%rd68, %rd84, %rd25;
	ld.global.nc.v2.f32 	{%f83, %f84}, [%rd68];
	fma.rn.f32 	%f87, %f61, %f83, %f266;
	fma.rn.f32 	%f88, %f62, %f84, %f87;
	ld.global.nc.v2.f32 	{%f89, %f90}, [%rd83+-768];
	ld.global.nc.v2.f32 	{%f93, %f94}, [%rd65+768];
	fma.rn.f32 	%f97, %f89, %f93, %f70;
	fma.rn.f32 	%f98, %f90, %f94, %f97;
	add.s64 	%rd69, %rd84, %rd23;
	ld.global.nc.v2.f32 	{%f99, %f100}, [%rd69];
	fma.rn.f32 	%f103, %f89, %f99, %f76;
	fma.rn.f32 	%f104, %f90, %f100, %f103;
	ld.global.nc.v2.f32 	{%f105, %f106}, [%rd67+768];
	fma.rn.f32 	%f109, %f89, %f105, %f82;
	fma.rn.f32 	%f110, %f90, %f106, %f109;
	ld.global.nc.v2.f32 	{%f111, %f112}, [%rd68+768];
	fma.rn.f32 	%f115, %f89, %f111, %f88;
	fma.rn.f32 	%f116, %f90, %f112, %f115;
	ld.global.nc.v2.f32 	{%f117, %f118}, [%rd83];
	ld.global.nc.v2.f32 	{%f121, %f122}, [%rd65+1536];
	fma.rn.f32 	%f125, %f117, %f121, %f98;
	fma.rn.f32 	%f126, %f118, %f122, %f125;
	ld.global.nc.v2.f32 	{%f127, %f128}, [%rd69+768];
	fma.rn.f32 	%f131, %f117, %f127, %f104;
	fma.rn.f32 	%f132, %f118, %f128, %f131;
	ld.global.nc.v2.f32 	{%f133, %f134}, [%rd67+1536];
	fma.rn.f32 	%f137, %f117, %f133, %f110;
	fma.rn.f32 	%f138, %f118, %f134, %f137;
	ld.global.nc.v2.f32 	{%f139, %f140}, [%rd68+1536];
	fma.rn.f32 	%f143, %f117, %f139, %f116;
	fma.rn.f32 	%f144, %f118, %f140, %f143;
	ld.global.nc.v2.f32 	{%f145, %f146}, [%rd83+768];
	ld.global.nc.v2.f32 	{%f149, %f150}, [%rd65+2304];
	fma.rn.f32 	%f153, %f145, %f149, %f126;
	fma.rn.f32 	%f269, %f146, %f150, %f153;
	ld.global.nc.v2.f32 	{%f154, %f155}, [%rd69+1536];
	fma.rn.f32 	%f158, %f145, %f154, %f132;
	fma.rn.f32 	%f268, %f146, %f155, %f158;
	ld.global.nc.v2.f32 	{%f159, %f160}, [%rd67+2304];
	fma.rn.f32 	%f163, %f145, %f159, %f138;
	fma.rn.f32 	%f267, %f146, %f160, %f163;
	ld.global.nc.v2.f32 	{%f164, %f165}, [%rd68+2304];
	fma.rn.f32 	%f168, %f145, %f164, %f144;
	fma.rn.f32 	%f266, %f146, %f165, %f168;
	add.s64 	%rd84, %rd84, 3072;
	add.s64 	%rd83, %rd83, 3072;
	add.s32 	%r187, %r187, 384;
	setp.lt.s32 	%p6, %r187, %r15;
	@%p6 bra 	$L__BB19_9;

	st.local.v4.f32 	[%rd3], {%f269, %f268, %f267, %f266};

$L__BB19_11:
	shr.s32 	%r41, %r3, 31;
	shr.u32 	%r42, %r41, 27;
	add.s32 	%r43, %r3, %r42;
	shr.s32 	%r44, %r43, 5;
	shl.b32 	%r45, %r44, 2;
	add.s32 	%r14, %r28, %r45;
	mov.u32 	%r47, 2;
	mov.b32 	%r48, %f269;
	mov.u32 	%r49, 31;
	mov.u32 	%r50, 16;
	mov.u32 	%r51, -1;
	shfl.sync.bfly.b32 	%r52|%p7, %r48, %r50, %r49, %r51;
	mov.b32 	%f169, %r52;
	add.f32 	%f170, %f269, %f169;
	mov.b32 	%r53, %f170;
	mov.u32 	%r54, 8;
	shfl.sync.bfly.b32 	%r55|%p8, %r53, %r54, %r49, %r51;
	mov.b32 	%f171, %r55;
	add.f32 	%f172, %f170, %f171;
	mov.b32 	%r56, %f172;
	mov.u32 	%r57, 4;
	shfl.sync.bfly.b32 	%r58|%p9, %r56, %r57, %r49, %r51;
	mov.b32 	%f173, %r58;
	add.f32 	%f174, %f172, %f173;
	mov.b32 	%r59, %f174;
	shfl.sync.bfly.b32 	%r60|%p10, %r59, %r47, %r49, %r51;
	mov.b32 	%f175, %r60;
	add.f32 	%f176, %f174, %f175;
	mov.b32 	%r61, %f176;
	mov.u32 	%r62, 1;
	shfl.sync.bfly.b32 	%r63|%p11, %r61, %r62, %r49, %r51;
	mov.b32 	%f177, %r63;
	add.f32 	%f178, %f176, %f177;
	st.local.f32 	[%rd3], %f178;
	st.shared.f32 	[%r14], %f178;
	bar.sync 	0;
	@%p1 bra 	$L__BB19_13;

	ld.shared.f32 	%f179, [%r4];
	mov.b32 	%r64, %f179;
	shfl.sync.bfly.b32 	%r68|%p13, %r64, %r50, %r49, %r51;
	mov.b32 	%f180, %r68;
	add.f32 	%f181, %f179, %f180;
	mov.b32 	%r69, %f181;
	shfl.sync.bfly.b32 	%r71|%p14, %r69, %r54, %r49, %r51;
	mov.b32 	%f182, %r71;
	add.f32 	%f183, %f181, %f182;
	mov.b32 	%r72, %f183;
	shfl.sync.bfly.b32 	%r74|%p15, %r72, %r57, %r49, %r51;
	mov.b32 	%f184, %r74;
	add.f32 	%f185, %f183, %f184;
	mov.b32 	%r75, %f185;
	shfl.sync.bfly.b32 	%r77|%p16, %r75, %r47, %r49, %r51;
	mov.b32 	%f186, %r77;
	add.f32 	%f187, %f185, %f186;
	mov.b32 	%r78, %f187;
	shfl.sync.bfly.b32 	%r80|%p17, %r78, %r62, %r49, %r51;
	mov.b32 	%f188, %r80;
	add.f32 	%f189, %f187, %f188;
	st.local.f32 	[%rd3], %f189;

$L__BB19_13:
	bar.sync 	0;
	mov.b32 	%r81, %f268;
	shfl.sync.bfly.b32 	%r85|%p19, %r81, %r50, %r49, %r51;
	mov.b32 	%f190, %r85;
	add.f32 	%f191, %f268, %f190;
	mov.b32 	%r86, %f191;
	shfl.sync.bfly.b32 	%r88|%p20, %r86, %r54, %r49, %r51;
	mov.b32 	%f192, %r88;
	add.f32 	%f193, %f191, %f192;
	mov.b32 	%r89, %f193;
	shfl.sync.bfly.b32 	%r91|%p21, %r89, %r57, %r49, %r51;
	mov.b32 	%f194, %r91;
	add.f32 	%f195, %f193, %f194;
	mov.b32 	%r92, %f195;
	shfl.sync.bfly.b32 	%r94|%p22, %r92, %r47, %r49, %r51;
	mov.b32 	%f196, %r94;
	add.f32 	%f197, %f195, %f196;
	mov.b32 	%r95, %f197;
	shfl.sync.bfly.b32 	%r97|%p23, %r95, %r62, %r49, %r51;
	mov.b32 	%f198, %r97;
	add.f32 	%f199, %f197, %f198;
	st.local.f32 	[%rd3+4], %f199;
	st.shared.f32 	[%r14], %f199;
	bar.sync 	0;
	@%p1 bra 	$L__BB19_15;

	ld.shared.f32 	%f200, [%r4];
	mov.b32 	%r98, %f200;
	mov.u32 	%r99, 31;
	mov.u32 	%r100, 16;
	mov.u32 	%r101, -1;
	shfl.sync.bfly.b32 	%r102|%p24, %r98, %r100, %r99, %r101;
	mov.b32 	%f201, %r102;
	add.f32 	%f202, %f200, %f201;
	mov.b32 	%r103, %f202;
	mov.u32 	%r104, 8;
	shfl.sync.bfly.b32 	%r105|%p25, %r103, %r104, %r99, %r101;
	mov.b32 	%f203, %r105;
	add.f32 	%f204, %f202, %f203;
	mov.b32 	%r106, %f204;
	mov.u32 	%r107, 4;
	shfl.sync.bfly.b32 	%r108|%p26, %r106, %r107, %r99, %r101;
	mov.b32 	%f205, %r108;
	add.f32 	%f206, %f204, %f205;
	mov.b32 	%r109, %f206;
	mov.u32 	%r110, 2;
	shfl.sync.bfly.b32 	%r111|%p27, %r109, %r110, %r99, %r101;
	mov.b32 	%f207, %r111;
	add.f32 	%f208, %f206, %f207;
	mov.b32 	%r112, %f208;
	mov.u32 	%r113, 1;
	shfl.sync.bfly.b32 	%r114|%p28, %r112, %r113, %r99, %r101;
	mov.b32 	%f209, %r114;
	add.f32 	%f210, %f208, %f209;
	st.local.f32 	[%rd3+4], %f210;

$L__BB19_15:
	bar.sync 	0;
	mov.b32 	%r115, %f267;
	mov.u32 	%r116, 31;
	mov.u32 	%r117, 16;
	mov.u32 	%r118, -1;
	shfl.sync.bfly.b32 	%r119|%p30, %r115, %r117, %r116, %r118;
	mov.b32 	%f211, %r119;
	add.f32 	%f212, %f267, %f211;
	mov.b32 	%r120, %f212;
	mov.u32 	%r121, 8;
	shfl.sync.bfly.b32 	%r122|%p31, %r120, %r121, %r116, %r118;
	mov.b32 	%f213, %r122;
	add.f32 	%f214, %f212, %f213;
	mov.b32 	%r123, %f214;
	mov.u32 	%r124, 4;
	shfl.sync.bfly.b32 	%r125|%p32, %r123, %r124, %r116, %r118;
	mov.b32 	%f215, %r125;
	add.f32 	%f216, %f214, %f215;
	mov.b32 	%r126, %f216;
	mov.u32 	%r127, 2;
	shfl.sync.bfly.b32 	%r128|%p33, %r126, %r127, %r116, %r118;
	mov.b32 	%f217, %r128;
	add.f32 	%f218, %f216, %f217;
	mov.b32 	%r129, %f218;
	mov.u32 	%r130, 1;
	shfl.sync.bfly.b32 	%r131|%p34, %r129, %r130, %r116, %r118;
	mov.b32 	%f219, %r131;
	add.f32 	%f220, %f218, %f219;
	st.local.f32 	[%rd3+8], %f220;
	st.shared.f32 	[%r14], %f220;
	bar.sync 	0;
	@%p1 bra 	$L__BB19_17;

	ld.shared.f32 	%f221, [%r4];
	mov.b32 	%r132, %f221;
	shfl.sync.bfly.b32 	%r136|%p35, %r132, %r117, %r116, %r118;
	mov.b32 	%f222, %r136;
	add.f32 	%f223, %f221, %f222;
	mov.b32 	%r137, %f223;
	shfl.sync.bfly.b32 	%r139|%p36, %r137, %r121, %r116, %r118;
	mov.b32 	%f224, %r139;
	add.f32 	%f225, %f223, %f224;
	mov.b32 	%r140, %f225;
	shfl.sync.bfly.b32 	%r142|%p37, %r140, %r124, %r116, %r118;
	mov.b32 	%f226, %r142;
	add.f32 	%f227, %f225, %f226;
	mov.b32 	%r143, %f227;
	shfl.sync.bfly.b32 	%r145|%p38, %r143, %r127, %r116, %r118;
	mov.b32 	%f228, %r145;
	add.f32 	%f229, %f227, %f228;
	mov.b32 	%r146, %f229;
	shfl.sync.bfly.b32 	%r148|%p39, %r146, %r130, %r116, %r118;
	mov.b32 	%f230, %r148;
	add.f32 	%f231, %f229, %f230;
	st.local.f32 	[%rd3+8], %f231;

$L__BB19_17:
	bar.sync 	0;
	mov.b32 	%r149, %f266;
	shfl.sync.bfly.b32 	%r153|%p41, %r149, %r117, %r116, %r118;
	mov.b32 	%f232, %r153;
	add.f32 	%f233, %f266, %f232;
	mov.b32 	%r154, %f233;
	shfl.sync.bfly.b32 	%r156|%p42, %r154, %r121, %r116, %r118;
	mov.b32 	%f234, %r156;
	add.f32 	%f235, %f233, %f234;
	mov.b32 	%r157, %f235;
	shfl.sync.bfly.b32 	%r159|%p43, %r157, %r124, %r116, %r118;
	mov.b32 	%f236, %r159;
	add.f32 	%f237, %f235, %f236;
	mov.b32 	%r160, %f237;
	shfl.sync.bfly.b32 	%r162|%p44, %r160, %r127, %r116, %r118;
	mov.b32 	%f238, %r162;
	add.f32 	%f239, %f237, %f238;
	mov.b32 	%r163, %f239;
	shfl.sync.bfly.b32 	%r165|%p45, %r163, %r130, %r116, %r118;
	mov.b32 	%f240, %r165;
	add.f32 	%f241, %f239, %f240;
	st.local.f32 	[%rd3+12], %f241;
	st.shared.f32 	[%r14], %f241;
	bar.sync 	0;
	@%p1 bra 	$L__BB19_19;

	ld.shared.f32 	%f242, [%r4];
	mov.b32 	%r166, %f242;
	mov.u32 	%r167, 31;
	mov.u32 	%r168, 16;
	mov.u32 	%r169, -1;
	shfl.sync.bfly.b32 	%r170|%p46, %r166, %r168, %r167, %r169;
	mov.b32 	%f243, %r170;
	add.f32 	%f244, %f242, %f243;
	mov.b32 	%r171, %f244;
	mov.u32 	%r172, 8;
	shfl.sync.bfly.b32 	%r173|%p47, %r171, %r172, %r167, %r169;
	mov.b32 	%f245, %r173;
	add.f32 	%f246, %f244, %f245;
	mov.b32 	%r174, %f246;
	mov.u32 	%r175, 4;
	shfl.sync.bfly.b32 	%r176|%p48, %r174, %r175, %r167, %r169;
	mov.b32 	%f247, %r176;
	add.f32 	%f248, %f246, %f247;
	mov.b32 	%r177, %f248;
	mov.u32 	%r178, 2;
	shfl.sync.bfly.b32 	%r179|%p49, %r177, %r178, %r167, %r169;
	mov.b32 	%f249, %r179;
	add.f32 	%f250, %f248, %f249;
	mov.b32 	%r180, %f250;
	mov.u32 	%r181, 1;
	shfl.sync.bfly.b32 	%r182|%p50, %r180, %r181, %r167, %r169;
	mov.b32 	%f251, %r182;
	add.f32 	%f252, %f250, %f251;
	st.local.f32 	[%rd3+12], %f252;

$L__BB19_19:
	bar.sync 	0;
	setp.gt.s32 	%p51, %r3, 3;
	@%p51 bra 	$L__BB19_21;

	mul.wide.s32 	%rd70, %r3, 4;
	add.s64 	%rd71, %rd3, %rd70;
	ld.local.f32 	%f253, [%rd71];
	mad.lo.s32 	%r183, %r3, %r17, %r2;
	cvt.s64.s32 	%rd72, %r183;
	mul.lo.s32 	%r184, %r1, %r18;
	cvt.s64.s32 	%rd73, %r184;
	add.s64 	%rd74, %rd73, %rd72;
	cvta.to.global.u64 	%rd75, %rd33;
	shl.b64 	%rd76, %rd74, 2;
	add.s64 	%rd77, %rd75, %rd76;
	st.global.f32 	[%rd77], %f253;

$L__BB19_21:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_5_bs_96
.visible .entry ggml_matvec_f32_ncols_5_bs_96(
	.param .u64 ggml_matvec_f32_ncols_5_bs_96_param_0,
	.param .u64 ggml_matvec_f32_ncols_5_bs_96_param_1,
	.param .u64 ggml_matvec_f32_ncols_5_bs_96_param_2,
	.param .u32 ggml_matvec_f32_ncols_5_bs_96_param_3,
	.param .u32 ggml_matvec_f32_ncols_5_bs_96_param_4,
	.param .u32 ggml_matvec_f32_ncols_5_bs_96_param_5,
	.param .u32 ggml_matvec_f32_ncols_5_bs_96_param_6,
	.param .u32 ggml_matvec_f32_ncols_5_bs_96_param_7,
	.param .u32 ggml_matvec_f32_ncols_5_bs_96_param_8,
	.param .u32 ggml_matvec_f32_ncols_5_bs_96_param_9,
	.param .u32 ggml_matvec_f32_ncols_5_bs_96_param_10,
	.param .u32 ggml_matvec_f32_ncols_5_bs_96_param_11
)
{
	.local .align 4 .b8 	__local_depot20[20];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<63>;
	.reg .f32 	%f<332>;
	.reg .b32 	%r<225>;
	.reg .b64 	%rd<76>;


	mov.u64 	%SPL, __local_depot20;
	ld.param.u64 	%rd28, [ggml_matvec_f32_ncols_5_bs_96_param_0];
	ld.param.u64 	%rd29, [ggml_matvec_f32_ncols_5_bs_96_param_1];
	ld.param.u64 	%rd27, [ggml_matvec_f32_ncols_5_bs_96_param_2];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_5_bs_96_param_3];
	ld.param.u32 	%r19, [ggml_matvec_f32_ncols_5_bs_96_param_5];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_5_bs_96_param_6];
	ld.param.u32 	%r17, [ggml_matvec_f32_ncols_5_bs_96_param_7];
	ld.param.u32 	%r20, [ggml_matvec_f32_ncols_5_bs_96_param_8];
	ld.param.u32 	%r21, [ggml_matvec_f32_ncols_5_bs_96_param_9];
	ld.param.u32 	%r22, [ggml_matvec_f32_ncols_5_bs_96_param_10];
	ld.param.u32 	%r18, [ggml_matvec_f32_ncols_5_bs_96_param_11];
	cvta.to.global.u64 	%rd75, %rd29;
	cvta.to.global.u64 	%rd2, %rd28;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r23, %r1, %r20;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r24, %r2, %r19;
	mad.lo.s32 	%r25, %r23, %r21, %r24;
	cvt.s64.s32 	%rd4, %r25;
	mul.lo.s32 	%r26, %r1, %r22;
	cvt.s64.s32 	%rd5, %r26;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r27, %r3, 2;
	mov.u32 	%r28, data_mmv;
	add.s32 	%r4, %r28, %r27;
	@%p1 bra 	$L__BB20_2;

	mov.u32 	%r29, 0;
	st.shared.u32 	[%r4], %r29;

$L__BB20_2:
	bar.sync 	0;
	mov.f32 	%f327, 0f00000000;
	mov.u32 	%r30, 0;
	st.local.u32 	[%rd3], %r30;
	st.local.u32 	[%rd3+4], %r30;
	st.local.u32 	[%rd3+8], %r30;
	st.local.u32 	[%rd3+12], %r30;
	st.local.u32 	[%rd3+16], %r30;
	setp.ge.s32 	%p2, %r3, %r15;
	mov.f32 	%f328, %f327;
	mov.f32 	%f329, %f327;
	mov.f32 	%f330, %f327;
	mov.f32 	%f331, %f327;
	@%p2 bra 	$L__BB20_11;

	not.b32 	%r31, %r3;
	add.s32 	%r5, %r31, %r15;
	mul.wide.u32 	%rd31, %r5, -1431655765;
	shr.u64 	%rd32, %rd31, 38;
	cvt.u32.u64 	%r32, %rd32;
	add.s32 	%r33, %r32, 1;
	and.b32  	%r222, %r33, 3;
	setp.eq.s32 	%p3, %r222, 0;
	mov.f32 	%f327, 0f00000000;
	mov.u32 	%r223, %r3;
	@%p3 bra 	$L__BB20_7;

	shl.b32 	%r34, %r16, 1;
	mad.lo.s32 	%r35, %r16, 3, %r3;
	mul.wide.s32 	%rd33, %r35, 8;
	shl.b64 	%rd34, %rd5, 2;
	add.s64 	%rd7, %rd33, %rd34;
	mul.wide.s32 	%rd35, %r3, 8;
	mul.wide.s32 	%rd36, %r16, 8;
	add.s64 	%rd37, %rd35, %rd36;
	add.s64 	%rd8, %rd37, %rd34;
	add.s64 	%rd9, %rd35, %rd34;
	mul.wide.s32 	%rd38, %r3, 2;
	add.s64 	%rd39, %rd38, %rd4;
	shl.b64 	%rd40, %rd39, 2;
	add.s64 	%rd72, %rd2, %rd40;
	mul.wide.s32 	%rd11, %r34, 8;
	mov.f32 	%f327, 0f00000000;
	mov.u64 	%rd73, %rd75;
	mov.f32 	%f328, %f327;
	mov.f32 	%f329, %f327;
	mov.f32 	%f330, %f327;
	mov.f32 	%f331, %f327;
	mov.u32 	%r223, %r3;

$L__BB20_5:
	.pragma "nounroll";
	ld.global.nc.v2.f32 	{%f46, %f47}, [%rd72];
	add.s64 	%rd41, %rd73, %rd9;
	ld.global.nc.v2.f32 	{%f50, %f51}, [%rd41];
	fma.rn.f32 	%f54, %f46, %f50, %f331;
	fma.rn.f32 	%f331, %f47, %f51, %f54;
	add.s64 	%rd42, %rd73, %rd8;
	ld.global.nc.v2.f32 	{%f55, %f56}, [%rd42];
	fma.rn.f32 	%f59, %f46, %f55, %f330;
	fma.rn.f32 	%f330, %f47, %f56, %f59;
	add.s64 	%rd43, %rd41, %rd11;
	ld.global.nc.v2.f32 	{%f60, %f61}, [%rd43];
	fma.rn.f32 	%f64, %f46, %f60, %f329;
	fma.rn.f32 	%f329, %f47, %f61, %f64;
	add.s64 	%rd44, %rd73, %rd7;
	ld.global.nc.v2.f32 	{%f65, %f66}, [%rd44];
	fma.rn.f32 	%f69, %f46, %f65, %f328;
	fma.rn.f32 	%f328, %f47, %f66, %f69;
	add.s64 	%rd45, %rd43, %rd11;
	ld.global.nc.v2.f32 	{%f70, %f71}, [%rd45];
	fma.rn.f32 	%f74, %f46, %f70, %f327;
	fma.rn.f32 	%f327, %f47, %f71, %f74;
	add.s32 	%r223, %r223, 96;
	add.s64 	%rd73, %rd73, 768;
	add.s64 	%rd72, %rd72, 768;
	add.s32 	%r222, %r222, -1;
	setp.ne.s32 	%p4, %r222, 0;
	@%p4 bra 	$L__BB20_5;

	st.local.f32 	[%rd3], %f331;
	st.local.f32 	[%rd3+4], %f330;
	st.local.f32 	[%rd3+8], %f329;
	st.local.f32 	[%rd3+12], %f328;
	st.local.f32 	[%rd3+16], %f327;

$L__BB20_7:
	setp.lt.u32 	%p5, %r5, 288;
	@%p5 bra 	$L__BB20_11;

	add.s32 	%r36, %r223, %r16;
	shl.b32 	%r37, %r16, 1;
	add.s32 	%r38, %r223, %r37;
	mad.lo.s32 	%r39, %r16, 3, %r223;
	shl.b32 	%r40, %r16, 2;
	add.s32 	%r41, %r223, %r40;
	add.s32 	%r42, %r36, 96;
	mul.wide.s32 	%rd46, %r42, 8;
	shl.b64 	%rd47, %rd5, 2;
	add.s64 	%rd16, %rd46, %rd47;
	mul.wide.s32 	%rd48, %r38, 8;
	add.s64 	%rd17, %rd48, %rd47;
	mul.wide.s32 	%rd49, %r39, 8;
	add.s64 	%rd18, %rd49, %rd47;
	mul.wide.s32 	%rd50, %r41, 8;
	add.s64 	%rd19, %rd50, %rd47;
	mul.wide.s32 	%rd51, %r223, 2;
	add.s64 	%rd52, %rd51, %rd4;
	shl.b64 	%rd53, %rd52, 2;
	add.s64 	%rd54, %rd2, %rd53;
	add.s64 	%rd74, %rd54, 1536;
	mul.wide.s32 	%rd55, %r223, 8;
	add.s64 	%rd21, %rd55, %rd47;
	mul.wide.s32 	%rd56, %r16, 8;
	add.s64 	%rd57, %rd55, %rd56;
	add.s64 	%rd22, %rd57, %rd47;

$L__BB20_9:
	ld.global.nc.v2.f32 	{%f75, %f76}, [%rd74+-1536];
	add.s64 	%rd58, %rd75, %rd21;
	ld.global.nc.v2.f32 	{%f79, %f80}, [%rd58];
	fma.rn.f32 	%f83, %f75, %f79, %f331;
	fma.rn.f32 	%f84, %f76, %f80, %f83;
	add.s64 	%rd59, %rd75, %rd22;
	ld.global.nc.v2.f32 	{%f85, %f86}, [%rd59];
	fma.rn.f32 	%f89, %f75, %f85, %f330;
	fma.rn.f32 	%f90, %f76, %f86, %f89;
	add.s64 	%rd60, %rd75, %rd17;
	ld.global.nc.v2.f32 	{%f91, %f92}, [%rd60];
	fma.rn.f32 	%f95, %f75, %f91, %f329;
	fma.rn.f32 	%f96, %f76, %f92, %f95;
	add.s64 	%rd61, %rd75, %rd18;
	ld.global.nc.v2.f32 	{%f97, %f98}, [%rd61];
	fma.rn.f32 	%f101, %f75, %f97, %f328;
	fma.rn.f32 	%f102, %f76, %f98, %f101;
	add.s64 	%rd62, %rd75, %rd19;
	ld.global.nc.v2.f32 	{%f103, %f104}, [%rd62];
	fma.rn.f32 	%f107, %f75, %f103, %f327;
	fma.rn.f32 	%f108, %f76, %f104, %f107;
	ld.global.nc.v2.f32 	{%f109, %f110}, [%rd74+-768];
	ld.global.nc.v2.f32 	{%f113, %f114}, [%rd58+768];
	fma.rn.f32 	%f117, %f109, %f113, %f84;
	fma.rn.f32 	%f118, %f110, %f114, %f117;
	add.s64 	%rd63, %rd75, %rd16;
	ld.global.nc.v2.f32 	{%f119, %f120}, [%rd63];
	fma.rn.f32 	%f123, %f109, %f119, %f90;
	fma.rn.f32 	%f124, %f110, %f120, %f123;
	ld.global.nc.v2.f32 	{%f125, %f126}, [%rd60+768];
	fma.rn.f32 	%f129, %f109, %f125, %f96;
	fma.rn.f32 	%f130, %f110, %f126, %f129;
	ld.global.nc.v2.f32 	{%f131, %f132}, [%rd61+768];
	fma.rn.f32 	%f135, %f109, %f131, %f102;
	fma.rn.f32 	%f136, %f110, %f132, %f135;
	ld.global.nc.v2.f32 	{%f137, %f138}, [%rd62+768];
	fma.rn.f32 	%f141, %f109, %f137, %f108;
	fma.rn.f32 	%f142, %f110, %f138, %f141;
	ld.global.nc.v2.f32 	{%f143, %f144}, [%rd74];
	ld.global.nc.v2.f32 	{%f147, %f148}, [%rd58+1536];
	fma.rn.f32 	%f151, %f143, %f147, %f118;
	fma.rn.f32 	%f152, %f144, %f148, %f151;
	ld.global.nc.v2.f32 	{%f153, %f154}, [%rd63+768];
	fma.rn.f32 	%f157, %f143, %f153, %f124;
	fma.rn.f32 	%f158, %f144, %f154, %f157;
	ld.global.nc.v2.f32 	{%f159, %f160}, [%rd60+1536];
	fma.rn.f32 	%f163, %f143, %f159, %f130;
	fma.rn.f32 	%f164, %f144, %f160, %f163;
	ld.global.nc.v2.f32 	{%f165, %f166}, [%rd61+1536];
	fma.rn.f32 	%f169, %f143, %f165, %f136;
	fma.rn.f32 	%f170, %f144, %f166, %f169;
	ld.global.nc.v2.f32 	{%f171, %f172}, [%rd62+1536];
	fma.rn.f32 	%f175, %f143, %f171, %f142;
	fma.rn.f32 	%f176, %f144, %f172, %f175;
	ld.global.nc.v2.f32 	{%f177, %f178}, [%rd74+768];
	ld.global.nc.v2.f32 	{%f181, %f182}, [%rd58+2304];
	fma.rn.f32 	%f185, %f177, %f181, %f152;
	fma.rn.f32 	%f331, %f178, %f182, %f185;
	ld.global.nc.v2.f32 	{%f186, %f187}, [%rd63+1536];
	fma.rn.f32 	%f190, %f177, %f186, %f158;
	fma.rn.f32 	%f330, %f178, %f187, %f190;
	ld.global.nc.v2.f32 	{%f191, %f192}, [%rd60+2304];
	fma.rn.f32 	%f195, %f177, %f191, %f164;
	fma.rn.f32 	%f329, %f178, %f192, %f195;
	ld.global.nc.v2.f32 	{%f196, %f197}, [%rd61+2304];
	fma.rn.f32 	%f200, %f177, %f196, %f170;
	fma.rn.f32 	%f328, %f178, %f197, %f200;
	ld.global.nc.v2.f32 	{%f201, %f202}, [%rd62+2304];
	fma.rn.f32 	%f205, %f177, %f201, %f176;
	fma.rn.f32 	%f327, %f178, %f202, %f205;
	add.s64 	%rd75, %rd75, 3072;
	add.s64 	%rd74, %rd74, 3072;
	add.s32 	%r223, %r223, 384;
	setp.lt.s32 	%p6, %r223, %r15;
	@%p6 bra 	$L__BB20_9;

	st.local.f32 	[%rd3], %f331;
	st.local.f32 	[%rd3+4], %f330;
	st.local.f32 	[%rd3+8], %f329;
	st.local.f32 	[%rd3+12], %f328;
	st.local.f32 	[%rd3+16], %f327;

$L__BB20_11:
	shr.s32 	%r43, %r3, 31;
	shr.u32 	%r44, %r43, 27;
	add.s32 	%r45, %r3, %r44;
	shr.s32 	%r46, %r45, 5;
	shl.b32 	%r47, %r46, 2;
	add.s32 	%r14, %r28, %r47;
	mov.u32 	%r49, 2;
	mov.b32 	%r50, %f331;
	mov.u32 	%r51, 31;
	mov.u32 	%r52, 16;
	mov.u32 	%r53, -1;
	shfl.sync.bfly.b32 	%r54|%p7, %r50, %r52, %r51, %r53;
	mov.b32 	%f206, %r54;
	add.f32 	%f207, %f331, %f206;
	mov.b32 	%r55, %f207;
	mov.u32 	%r56, 8;
	shfl.sync.bfly.b32 	%r57|%p8, %r55, %r56, %r51, %r53;
	mov.b32 	%f208, %r57;
	add.f32 	%f209, %f207, %f208;
	mov.b32 	%r58, %f209;
	mov.u32 	%r59, 4;
	shfl.sync.bfly.b32 	%r60|%p9, %r58, %r59, %r51, %r53;
	mov.b32 	%f210, %r60;
	add.f32 	%f211, %f209, %f210;
	mov.b32 	%r61, %f211;
	shfl.sync.bfly.b32 	%r62|%p10, %r61, %r49, %r51, %r53;
	mov.b32 	%f212, %r62;
	add.f32 	%f213, %f211, %f212;
	mov.b32 	%r63, %f213;
	mov.u32 	%r64, 1;
	shfl.sync.bfly.b32 	%r65|%p11, %r63, %r64, %r51, %r53;
	mov.b32 	%f214, %r65;
	add.f32 	%f215, %f213, %f214;
	st.local.f32 	[%rd3], %f215;
	st.shared.f32 	[%r14], %f215;
	bar.sync 	0;
	@%p1 bra 	$L__BB20_13;

	ld.shared.f32 	%f216, [%r4];
	mov.b32 	%r66, %f216;
	shfl.sync.bfly.b32 	%r70|%p13, %r66, %r52, %r51, %r53;
	mov.b32 	%f217, %r70;
	add.f32 	%f218, %f216, %f217;
	mov.b32 	%r71, %f218;
	shfl.sync.bfly.b32 	%r73|%p14, %r71, %r56, %r51, %r53;
	mov.b32 	%f219, %r73;
	add.f32 	%f220, %f218, %f219;
	mov.b32 	%r74, %f220;
	shfl.sync.bfly.b32 	%r76|%p15, %r74, %r59, %r51, %r53;
	mov.b32 	%f221, %r76;
	add.f32 	%f222, %f220, %f221;
	mov.b32 	%r77, %f222;
	shfl.sync.bfly.b32 	%r79|%p16, %r77, %r49, %r51, %r53;
	mov.b32 	%f223, %r79;
	add.f32 	%f224, %f222, %f223;
	mov.b32 	%r80, %f224;
	shfl.sync.bfly.b32 	%r82|%p17, %r80, %r64, %r51, %r53;
	mov.b32 	%f225, %r82;
	add.f32 	%f226, %f224, %f225;
	st.local.f32 	[%rd3], %f226;

$L__BB20_13:
	bar.sync 	0;
	mov.b32 	%r83, %f330;
	shfl.sync.bfly.b32 	%r87|%p19, %r83, %r52, %r51, %r53;
	mov.b32 	%f227, %r87;
	add.f32 	%f228, %f330, %f227;
	mov.b32 	%r88, %f228;
	shfl.sync.bfly.b32 	%r90|%p20, %r88, %r56, %r51, %r53;
	mov.b32 	%f229, %r90;
	add.f32 	%f230, %f228, %f229;
	mov.b32 	%r91, %f230;
	shfl.sync.bfly.b32 	%r93|%p21, %r91, %r59, %r51, %r53;
	mov.b32 	%f231, %r93;
	add.f32 	%f232, %f230, %f231;
	mov.b32 	%r94, %f232;
	shfl.sync.bfly.b32 	%r96|%p22, %r94, %r49, %r51, %r53;
	mov.b32 	%f233, %r96;
	add.f32 	%f234, %f232, %f233;
	mov.b32 	%r97, %f234;
	shfl.sync.bfly.b32 	%r99|%p23, %r97, %r64, %r51, %r53;
	mov.b32 	%f235, %r99;
	add.f32 	%f236, %f234, %f235;
	st.local.f32 	[%rd3+4], %f236;
	st.shared.f32 	[%r14], %f236;
	bar.sync 	0;
	@%p1 bra 	$L__BB20_15;

	ld.shared.f32 	%f237, [%r4];
	mov.b32 	%r100, %f237;
	mov.u32 	%r101, 31;
	mov.u32 	%r102, 16;
	mov.u32 	%r103, -1;
	shfl.sync.bfly.b32 	%r104|%p24, %r100, %r102, %r101, %r103;
	mov.b32 	%f238, %r104;
	add.f32 	%f239, %f237, %f238;
	mov.b32 	%r105, %f239;
	mov.u32 	%r106, 8;
	shfl.sync.bfly.b32 	%r107|%p25, %r105, %r106, %r101, %r103;
	mov.b32 	%f240, %r107;
	add.f32 	%f241, %f239, %f240;
	mov.b32 	%r108, %f241;
	mov.u32 	%r109, 4;
	shfl.sync.bfly.b32 	%r110|%p26, %r108, %r109, %r101, %r103;
	mov.b32 	%f242, %r110;
	add.f32 	%f243, %f241, %f242;
	mov.b32 	%r111, %f243;
	mov.u32 	%r112, 2;
	shfl.sync.bfly.b32 	%r113|%p27, %r111, %r112, %r101, %r103;
	mov.b32 	%f244, %r113;
	add.f32 	%f245, %f243, %f244;
	mov.b32 	%r114, %f245;
	mov.u32 	%r115, 1;
	shfl.sync.bfly.b32 	%r116|%p28, %r114, %r115, %r101, %r103;
	mov.b32 	%f246, %r116;
	add.f32 	%f247, %f245, %f246;
	st.local.f32 	[%rd3+4], %f247;

$L__BB20_15:
	bar.sync 	0;
	mov.b32 	%r117, %f329;
	mov.u32 	%r118, 31;
	mov.u32 	%r119, 16;
	mov.u32 	%r120, -1;
	shfl.sync.bfly.b32 	%r121|%p30, %r117, %r119, %r118, %r120;
	mov.b32 	%f248, %r121;
	add.f32 	%f249, %f329, %f248;
	mov.b32 	%r122, %f249;
	mov.u32 	%r123, 8;
	shfl.sync.bfly.b32 	%r124|%p31, %r122, %r123, %r118, %r120;
	mov.b32 	%f250, %r124;
	add.f32 	%f251, %f249, %f250;
	mov.b32 	%r125, %f251;
	mov.u32 	%r126, 4;
	shfl.sync.bfly.b32 	%r127|%p32, %r125, %r126, %r118, %r120;
	mov.b32 	%f252, %r127;
	add.f32 	%f253, %f251, %f252;
	mov.b32 	%r128, %f253;
	mov.u32 	%r129, 2;
	shfl.sync.bfly.b32 	%r130|%p33, %r128, %r129, %r118, %r120;
	mov.b32 	%f254, %r130;
	add.f32 	%f255, %f253, %f254;
	mov.b32 	%r131, %f255;
	mov.u32 	%r132, 1;
	shfl.sync.bfly.b32 	%r133|%p34, %r131, %r132, %r118, %r120;
	mov.b32 	%f256, %r133;
	add.f32 	%f257, %f255, %f256;
	st.local.f32 	[%rd3+8], %f257;
	st.shared.f32 	[%r14], %f257;
	bar.sync 	0;
	@%p1 bra 	$L__BB20_17;

	ld.shared.f32 	%f258, [%r4];
	mov.b32 	%r134, %f258;
	shfl.sync.bfly.b32 	%r138|%p35, %r134, %r119, %r118, %r120;
	mov.b32 	%f259, %r138;
	add.f32 	%f260, %f258, %f259;
	mov.b32 	%r139, %f260;
	shfl.sync.bfly.b32 	%r141|%p36, %r139, %r123, %r118, %r120;
	mov.b32 	%f261, %r141;
	add.f32 	%f262, %f260, %f261;
	mov.b32 	%r142, %f262;
	shfl.sync.bfly.b32 	%r144|%p37, %r142, %r126, %r118, %r120;
	mov.b32 	%f263, %r144;
	add.f32 	%f264, %f262, %f263;
	mov.b32 	%r145, %f264;
	shfl.sync.bfly.b32 	%r147|%p38, %r145, %r129, %r118, %r120;
	mov.b32 	%f265, %r147;
	add.f32 	%f266, %f264, %f265;
	mov.b32 	%r148, %f266;
	shfl.sync.bfly.b32 	%r150|%p39, %r148, %r132, %r118, %r120;
	mov.b32 	%f267, %r150;
	add.f32 	%f268, %f266, %f267;
	st.local.f32 	[%rd3+8], %f268;

$L__BB20_17:
	bar.sync 	0;
	mov.b32 	%r151, %f328;
	shfl.sync.bfly.b32 	%r155|%p41, %r151, %r119, %r118, %r120;
	mov.b32 	%f269, %r155;
	add.f32 	%f270, %f328, %f269;
	mov.b32 	%r156, %f270;
	shfl.sync.bfly.b32 	%r158|%p42, %r156, %r123, %r118, %r120;
	mov.b32 	%f271, %r158;
	add.f32 	%f272, %f270, %f271;
	mov.b32 	%r159, %f272;
	shfl.sync.bfly.b32 	%r161|%p43, %r159, %r126, %r118, %r120;
	mov.b32 	%f273, %r161;
	add.f32 	%f274, %f272, %f273;
	mov.b32 	%r162, %f274;
	shfl.sync.bfly.b32 	%r164|%p44, %r162, %r129, %r118, %r120;
	mov.b32 	%f275, %r164;
	add.f32 	%f276, %f274, %f275;
	mov.b32 	%r165, %f276;
	shfl.sync.bfly.b32 	%r167|%p45, %r165, %r132, %r118, %r120;
	mov.b32 	%f277, %r167;
	add.f32 	%f278, %f276, %f277;
	st.local.f32 	[%rd3+12], %f278;
	st.shared.f32 	[%r14], %f278;
	bar.sync 	0;
	@%p1 bra 	$L__BB20_19;

	ld.shared.f32 	%f279, [%r4];
	mov.b32 	%r168, %f279;
	mov.u32 	%r169, 31;
	mov.u32 	%r170, 16;
	mov.u32 	%r171, -1;
	shfl.sync.bfly.b32 	%r172|%p46, %r168, %r170, %r169, %r171;
	mov.b32 	%f280, %r172;
	add.f32 	%f281, %f279, %f280;
	mov.b32 	%r173, %f281;
	mov.u32 	%r174, 8;
	shfl.sync.bfly.b32 	%r175|%p47, %r173, %r174, %r169, %r171;
	mov.b32 	%f282, %r175;
	add.f32 	%f283, %f281, %f282;
	mov.b32 	%r176, %f283;
	mov.u32 	%r177, 4;
	shfl.sync.bfly.b32 	%r178|%p48, %r176, %r177, %r169, %r171;
	mov.b32 	%f284, %r178;
	add.f32 	%f285, %f283, %f284;
	mov.b32 	%r179, %f285;
	mov.u32 	%r180, 2;
	shfl.sync.bfly.b32 	%r181|%p49, %r179, %r180, %r169, %r171;
	mov.b32 	%f286, %r181;
	add.f32 	%f287, %f285, %f286;
	mov.b32 	%r182, %f287;
	mov.u32 	%r183, 1;
	shfl.sync.bfly.b32 	%r184|%p50, %r182, %r183, %r169, %r171;
	mov.b32 	%f288, %r184;
	add.f32 	%f289, %f287, %f288;
	st.local.f32 	[%rd3+12], %f289;

$L__BB20_19:
	bar.sync 	0;
	mov.b32 	%r185, %f327;
	mov.u32 	%r186, 31;
	mov.u32 	%r187, 16;
	mov.u32 	%r188, -1;
	shfl.sync.bfly.b32 	%r189|%p52, %r185, %r187, %r186, %r188;
	mov.b32 	%f290, %r189;
	add.f32 	%f291, %f327, %f290;
	mov.b32 	%r190, %f291;
	mov.u32 	%r191, 8;
	shfl.sync.bfly.b32 	%r192|%p53, %r190, %r191, %r186, %r188;
	mov.b32 	%f292, %r192;
	add.f32 	%f293, %f291, %f292;
	mov.b32 	%r193, %f293;
	mov.u32 	%r194, 4;
	shfl.sync.bfly.b32 	%r195|%p54, %r193, %r194, %r186, %r188;
	mov.b32 	%f294, %r195;
	add.f32 	%f295, %f293, %f294;
	mov.b32 	%r196, %f295;
	mov.u32 	%r197, 2;
	shfl.sync.bfly.b32 	%r198|%p55, %r196, %r197, %r186, %r188;
	mov.b32 	%f296, %r198;
	add.f32 	%f297, %f295, %f296;
	mov.b32 	%r199, %f297;
	mov.u32 	%r200, 1;
	shfl.sync.bfly.b32 	%r201|%p56, %r199, %r200, %r186, %r188;
	mov.b32 	%f298, %r201;
	add.f32 	%f299, %f297, %f298;
	st.local.f32 	[%rd3+16], %f299;
	st.shared.f32 	[%r14], %f299;
	bar.sync 	0;
	@%p1 bra 	$L__BB20_21;

	ld.shared.f32 	%f300, [%r4];
	mov.b32 	%r202, %f300;
	shfl.sync.bfly.b32 	%r206|%p57, %r202, %r187, %r186, %r188;
	mov.b32 	%f301, %r206;
	add.f32 	%f302, %f300, %f301;
	mov.b32 	%r207, %f302;
	shfl.sync.bfly.b32 	%r209|%p58, %r207, %r191, %r186, %r188;
	mov.b32 	%f303, %r209;
	add.f32 	%f304, %f302, %f303;
	mov.b32 	%r210, %f304;
	shfl.sync.bfly.b32 	%r212|%p59, %r210, %r194, %r186, %r188;
	mov.b32 	%f305, %r212;
	add.f32 	%f306, %f304, %f305;
	mov.b32 	%r213, %f306;
	shfl.sync.bfly.b32 	%r215|%p60, %r213, %r197, %r186, %r188;
	mov.b32 	%f307, %r215;
	add.f32 	%f308, %f306, %f307;
	mov.b32 	%r216, %f308;
	shfl.sync.bfly.b32 	%r218|%p61, %r216, %r200, %r186, %r188;
	mov.b32 	%f309, %r218;
	add.f32 	%f310, %f308, %f309;
	st.local.f32 	[%rd3+16], %f310;

$L__BB20_21:
	bar.sync 	0;
	setp.gt.s32 	%p62, %r3, 4;
	@%p62 bra 	$L__BB20_23;

	mul.wide.s32 	%rd64, %r3, 4;
	add.s64 	%rd65, %rd3, %rd64;
	ld.local.f32 	%f311, [%rd65];
	mad.lo.s32 	%r219, %r3, %r17, %r2;
	cvt.s64.s32 	%rd66, %r219;
	mul.lo.s32 	%r220, %r1, %r18;
	cvt.s64.s32 	%rd67, %r220;
	add.s64 	%rd68, %rd67, %rd66;
	cvta.to.global.u64 	%rd69, %rd27;
	shl.b64 	%rd70, %rd68, 2;
	add.s64 	%rd71, %rd69, %rd70;
	st.global.f32 	[%rd71], %f311;

$L__BB20_23:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_6_bs_96
.visible .entry ggml_matvec_f32_ncols_6_bs_96(
	.param .u64 ggml_matvec_f32_ncols_6_bs_96_param_0,
	.param .u64 ggml_matvec_f32_ncols_6_bs_96_param_1,
	.param .u64 ggml_matvec_f32_ncols_6_bs_96_param_2,
	.param .u32 ggml_matvec_f32_ncols_6_bs_96_param_3,
	.param .u32 ggml_matvec_f32_ncols_6_bs_96_param_4,
	.param .u32 ggml_matvec_f32_ncols_6_bs_96_param_5,
	.param .u32 ggml_matvec_f32_ncols_6_bs_96_param_6,
	.param .u32 ggml_matvec_f32_ncols_6_bs_96_param_7,
	.param .u32 ggml_matvec_f32_ncols_6_bs_96_param_8,
	.param .u32 ggml_matvec_f32_ncols_6_bs_96_param_9,
	.param .u32 ggml_matvec_f32_ncols_6_bs_96_param_10,
	.param .u32 ggml_matvec_f32_ncols_6_bs_96_param_11
)
{
	.local .align 8 .b8 	__local_depot21[24];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<75>;
	.reg .f32 	%f<296>;
	.reg .b32 	%r<251>;
	.reg .b64 	%rd<70>;


	mov.u64 	%SPL, __local_depot21;
	ld.param.u64 	%rd20, [ggml_matvec_f32_ncols_6_bs_96_param_0];
	ld.param.u64 	%rd21, [ggml_matvec_f32_ncols_6_bs_96_param_1];
	ld.param.u64 	%rd19, [ggml_matvec_f32_ncols_6_bs_96_param_2];
	ld.param.u32 	%r11, [ggml_matvec_f32_ncols_6_bs_96_param_3];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_6_bs_96_param_5];
	ld.param.u32 	%r12, [ggml_matvec_f32_ncols_6_bs_96_param_6];
	ld.param.u32 	%r13, [ggml_matvec_f32_ncols_6_bs_96_param_7];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_6_bs_96_param_8];
	ld.param.u32 	%r17, [ggml_matvec_f32_ncols_6_bs_96_param_9];
	ld.param.u32 	%r18, [ggml_matvec_f32_ncols_6_bs_96_param_10];
	ld.param.u32 	%r14, [ggml_matvec_f32_ncols_6_bs_96_param_11];
	cvta.to.global.u64 	%rd69, %rd21;
	add.u64 	%rd2, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r19, %r1, %r16;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r20, %r2, %r15;
	mad.lo.s32 	%r21, %r19, %r17, %r20;
	cvt.s64.s32 	%rd3, %r21;
	cvta.to.global.u64 	%rd4, %rd20;
	mul.lo.s32 	%r22, %r1, %r18;
	cvt.s64.s32 	%rd5, %r22;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r23, %r3, 2;
	mov.u32 	%r24, data_mmv;
	add.s32 	%r4, %r24, %r23;
	@%p1 bra 	$L__BB21_2;

	mov.u32 	%r25, 0;
	st.shared.u32 	[%r4], %r25;

$L__BB21_2:
	bar.sync 	0;
	mov.f32 	%f290, 0f00000000;
	st.local.v2.f32 	[%rd2], {%f290, %f290};
	st.local.v2.f32 	[%rd2+8], {%f290, %f290};
	st.local.v2.f32 	[%rd2+16], {%f290, %f290};
	setp.ge.s32 	%p2, %r3, %r11;
	mov.f32 	%f291, %f290;
	mov.f32 	%f292, %f290;
	mov.f32 	%f293, %f290;
	mov.f32 	%f294, %f290;
	mov.f32 	%f295, %f290;
	@%p2 bra 	$L__BB21_9;

	not.b32 	%r26, %r3;
	add.s32 	%r5, %r26, %r11;
	mul.wide.u32 	%rd23, %r5, -1431655765;
	shr.u64 	%rd24, %rd23, 38;
	and.b64  	%rd25, %rd24, 1;
	setp.eq.b64 	%p3, %rd25, 1;
	mov.pred 	%p4, 0;
	xor.pred  	%p5, %p3, %p4;
	mov.f32 	%f290, 0f00000000;
	mov.u32 	%r250, %r3;
	@%p5 bra 	$L__BB21_5;

	shl.b64 	%rd26, %rd5, 2;
	add.s64 	%rd27, %rd69, %rd26;
	shl.b64 	%rd28, %rd3, 2;
	add.s64 	%rd29, %rd4, %rd28;
	mul.wide.s32 	%rd30, %r3, 8;
	add.s64 	%rd31, %rd29, %rd30;
	ld.global.nc.v2.f32 	{%f43, %f44}, [%rd31];
	add.s64 	%rd32, %rd27, %rd30;
	ld.global.nc.v2.f32 	{%f47, %f48}, [%rd32];
	fma.rn.f32 	%f51, %f43, %f47, 0f00000000;
	fma.rn.f32 	%f295, %f44, %f48, %f51;
	mul.wide.s32 	%rd33, %r12, 8;
	add.s64 	%rd34, %rd32, %rd33;
	ld.global.nc.v2.f32 	{%f52, %f53}, [%rd34];
	fma.rn.f32 	%f56, %f43, %f52, 0f00000000;
	fma.rn.f32 	%f294, %f44, %f53, %f56;
	st.local.v2.f32 	[%rd2], {%f295, %f294};
	add.s32 	%r27, %r3, %r12;
	add.s32 	%r28, %r27, %r12;
	mul.wide.s32 	%rd35, %r28, 8;
	add.s64 	%rd36, %rd27, %rd35;
	ld.global.nc.v2.f32 	{%f57, %f58}, [%rd36];
	fma.rn.f32 	%f61, %f43, %f57, 0f00000000;
	fma.rn.f32 	%f293, %f44, %f58, %f61;
	add.s64 	%rd37, %rd36, %rd33;
	ld.global.nc.v2.f32 	{%f62, %f63}, [%rd37];
	fma.rn.f32 	%f66, %f43, %f62, 0f00000000;
	fma.rn.f32 	%f292, %f44, %f63, %f66;
	st.local.v2.f32 	[%rd2+8], {%f293, %f292};
	add.s64 	%rd38, %rd37, %rd33;
	ld.global.nc.v2.f32 	{%f67, %f68}, [%rd38];
	fma.rn.f32 	%f71, %f43, %f67, 0f00000000;
	fma.rn.f32 	%f291, %f44, %f68, %f71;
	add.s64 	%rd39, %rd38, %rd33;
	ld.global.nc.v2.f32 	{%f72, %f73}, [%rd39];
	fma.rn.f32 	%f76, %f43, %f72, 0f00000000;
	fma.rn.f32 	%f290, %f44, %f73, %f76;
	st.local.v2.f32 	[%rd2+16], {%f291, %f290};
	add.s32 	%r250, %r3, 96;

$L__BB21_5:
	setp.lt.u32 	%p6, %r5, 96;
	@%p6 bra 	$L__BB21_9;

	add.s32 	%r29, %r250, %r12;
	add.s32 	%r30, %r29, 96;
	mul.wide.s32 	%rd40, %r30, 8;
	shl.b64 	%rd41, %rd5, 2;
	add.s64 	%rd7, %rd40, %rd41;
	shl.b32 	%r31, %r12, 1;
	add.s32 	%r32, %r250, %r31;
	mad.lo.s32 	%r33, %r12, 3, %r250;
	shl.b32 	%r34, %r12, 2;
	add.s32 	%r35, %r250, %r34;
	mad.lo.s32 	%r36, %r12, 5, %r250;
	mul.wide.s32 	%rd42, %r32, 8;
	add.s64 	%rd8, %rd42, %rd41;
	mul.wide.s32 	%rd43, %r33, 8;
	add.s64 	%rd9, %rd43, %rd41;
	mul.wide.s32 	%rd44, %r35, 8;
	add.s64 	%rd10, %rd44, %rd41;
	mul.wide.s32 	%rd45, %r36, 8;
	add.s64 	%rd11, %rd45, %rd41;
	mul.wide.s32 	%rd46, %r250, 2;
	add.s64 	%rd47, %rd46, %rd3;
	shl.b64 	%rd48, %rd47, 2;
	add.s64 	%rd49, %rd4, %rd48;
	add.s64 	%rd68, %rd49, 768;
	mul.wide.s32 	%rd50, %r250, 8;
	mul.wide.s32 	%rd51, %r12, 8;
	add.s64 	%rd52, %rd50, %rd51;
	add.s64 	%rd13, %rd52, %rd41;
	add.s64 	%rd14, %rd50, %rd41;

$L__BB21_7:
	ld.global.nc.v2.f32 	{%f77, %f78}, [%rd68+-768];
	add.s64 	%rd53, %rd69, %rd14;
	ld.global.nc.v2.f32 	{%f81, %f82}, [%rd53];
	fma.rn.f32 	%f85, %f77, %f81, %f295;
	fma.rn.f32 	%f86, %f78, %f82, %f85;
	add.s64 	%rd54, %rd69, %rd13;
	ld.global.nc.v2.f32 	{%f87, %f88}, [%rd54];
	fma.rn.f32 	%f91, %f77, %f87, %f294;
	fma.rn.f32 	%f92, %f78, %f88, %f91;
	add.s64 	%rd55, %rd69, %rd8;
	ld.global.nc.v2.f32 	{%f93, %f94}, [%rd55];
	fma.rn.f32 	%f97, %f77, %f93, %f293;
	fma.rn.f32 	%f98, %f78, %f94, %f97;
	add.s64 	%rd56, %rd69, %rd9;
	ld.global.nc.v2.f32 	{%f99, %f100}, [%rd56];
	fma.rn.f32 	%f103, %f77, %f99, %f292;
	fma.rn.f32 	%f104, %f78, %f100, %f103;
	add.s64 	%rd57, %rd69, %rd10;
	ld.global.nc.v2.f32 	{%f105, %f106}, [%rd57];
	fma.rn.f32 	%f109, %f77, %f105, %f291;
	fma.rn.f32 	%f110, %f78, %f106, %f109;
	add.s64 	%rd58, %rd69, %rd11;
	ld.global.nc.v2.f32 	{%f111, %f112}, [%rd58];
	fma.rn.f32 	%f115, %f77, %f111, %f290;
	fma.rn.f32 	%f116, %f78, %f112, %f115;
	ld.global.nc.v2.f32 	{%f117, %f118}, [%rd68];
	ld.global.nc.v2.f32 	{%f121, %f122}, [%rd53+768];
	fma.rn.f32 	%f125, %f117, %f121, %f86;
	fma.rn.f32 	%f295, %f118, %f122, %f125;
	add.s64 	%rd59, %rd69, %rd7;
	ld.global.nc.v2.f32 	{%f126, %f127}, [%rd59];
	fma.rn.f32 	%f130, %f117, %f126, %f92;
	fma.rn.f32 	%f294, %f118, %f127, %f130;
	ld.global.nc.v2.f32 	{%f131, %f132}, [%rd55+768];
	fma.rn.f32 	%f135, %f117, %f131, %f98;
	fma.rn.f32 	%f293, %f118, %f132, %f135;
	ld.global.nc.v2.f32 	{%f136, %f137}, [%rd56+768];
	fma.rn.f32 	%f140, %f117, %f136, %f104;
	fma.rn.f32 	%f292, %f118, %f137, %f140;
	ld.global.nc.v2.f32 	{%f141, %f142}, [%rd57+768];
	fma.rn.f32 	%f145, %f117, %f141, %f110;
	fma.rn.f32 	%f291, %f118, %f142, %f145;
	ld.global.nc.v2.f32 	{%f146, %f147}, [%rd58+768];
	fma.rn.f32 	%f150, %f117, %f146, %f116;
	fma.rn.f32 	%f290, %f118, %f147, %f150;
	add.s64 	%rd69, %rd69, 1536;
	add.s64 	%rd68, %rd68, 1536;
	add.s32 	%r250, %r250, 192;
	setp.lt.s32 	%p7, %r250, %r11;
	@%p7 bra 	$L__BB21_7;

	st.local.v2.f32 	[%rd2], {%f295, %f294};
	st.local.v2.f32 	[%rd2+8], {%f293, %f292};
	st.local.v2.f32 	[%rd2+16], {%f291, %f290};

$L__BB21_9:
	shr.s32 	%r37, %r3, 31;
	shr.u32 	%r38, %r37, 27;
	add.s32 	%r39, %r3, %r38;
	shr.s32 	%r40, %r39, 5;
	shl.b32 	%r41, %r40, 2;
	add.s32 	%r10, %r24, %r41;
	mov.u32 	%r43, 2;
	mov.b32 	%r44, %f295;
	mov.u32 	%r45, 31;
	mov.u32 	%r46, 16;
	mov.u32 	%r47, -1;
	shfl.sync.bfly.b32 	%r48|%p8, %r44, %r46, %r45, %r47;
	mov.b32 	%f151, %r48;
	add.f32 	%f152, %f295, %f151;
	mov.b32 	%r49, %f152;
	mov.u32 	%r50, 8;
	shfl.sync.bfly.b32 	%r51|%p9, %r49, %r50, %r45, %r47;
	mov.b32 	%f153, %r51;
	add.f32 	%f154, %f152, %f153;
	mov.b32 	%r52, %f154;
	mov.u32 	%r53, 4;
	shfl.sync.bfly.b32 	%r54|%p10, %r52, %r53, %r45, %r47;
	mov.b32 	%f155, %r54;
	add.f32 	%f156, %f154, %f155;
	mov.b32 	%r55, %f156;
	shfl.sync.bfly.b32 	%r56|%p11, %r55, %r43, %r45, %r47;
	mov.b32 	%f157, %r56;
	add.f32 	%f158, %f156, %f157;
	mov.b32 	%r57, %f158;
	mov.u32 	%r58, 1;
	shfl.sync.bfly.b32 	%r59|%p12, %r57, %r58, %r45, %r47;
	mov.b32 	%f159, %r59;
	add.f32 	%f160, %f158, %f159;
	st.local.f32 	[%rd2], %f160;
	st.shared.f32 	[%r10], %f160;
	bar.sync 	0;
	@%p1 bra 	$L__BB21_11;

	ld.shared.f32 	%f161, [%r4];
	mov.b32 	%r60, %f161;
	shfl.sync.bfly.b32 	%r64|%p14, %r60, %r46, %r45, %r47;
	mov.b32 	%f162, %r64;
	add.f32 	%f163, %f161, %f162;
	mov.b32 	%r65, %f163;
	shfl.sync.bfly.b32 	%r67|%p15, %r65, %r50, %r45, %r47;
	mov.b32 	%f164, %r67;
	add.f32 	%f165, %f163, %f164;
	mov.b32 	%r68, %f165;
	shfl.sync.bfly.b32 	%r70|%p16, %r68, %r53, %r45, %r47;
	mov.b32 	%f166, %r70;
	add.f32 	%f167, %f165, %f166;
	mov.b32 	%r71, %f167;
	shfl.sync.bfly.b32 	%r73|%p17, %r71, %r43, %r45, %r47;
	mov.b32 	%f168, %r73;
	add.f32 	%f169, %f167, %f168;
	mov.b32 	%r74, %f169;
	shfl.sync.bfly.b32 	%r76|%p18, %r74, %r58, %r45, %r47;
	mov.b32 	%f170, %r76;
	add.f32 	%f171, %f169, %f170;
	st.local.f32 	[%rd2], %f171;

$L__BB21_11:
	bar.sync 	0;
	mov.b32 	%r77, %f294;
	shfl.sync.bfly.b32 	%r81|%p20, %r77, %r46, %r45, %r47;
	mov.b32 	%f172, %r81;
	add.f32 	%f173, %f294, %f172;
	mov.b32 	%r82, %f173;
	shfl.sync.bfly.b32 	%r84|%p21, %r82, %r50, %r45, %r47;
	mov.b32 	%f174, %r84;
	add.f32 	%f175, %f173, %f174;
	mov.b32 	%r85, %f175;
	shfl.sync.bfly.b32 	%r87|%p22, %r85, %r53, %r45, %r47;
	mov.b32 	%f176, %r87;
	add.f32 	%f177, %f175, %f176;
	mov.b32 	%r88, %f177;
	shfl.sync.bfly.b32 	%r90|%p23, %r88, %r43, %r45, %r47;
	mov.b32 	%f178, %r90;
	add.f32 	%f179, %f177, %f178;
	mov.b32 	%r91, %f179;
	shfl.sync.bfly.b32 	%r93|%p24, %r91, %r58, %r45, %r47;
	mov.b32 	%f180, %r93;
	add.f32 	%f181, %f179, %f180;
	st.local.f32 	[%rd2+4], %f181;
	st.shared.f32 	[%r10], %f181;
	bar.sync 	0;
	@%p1 bra 	$L__BB21_13;

	ld.shared.f32 	%f182, [%r4];
	mov.b32 	%r94, %f182;
	mov.u32 	%r95, 31;
	mov.u32 	%r96, 16;
	mov.u32 	%r97, -1;
	shfl.sync.bfly.b32 	%r98|%p25, %r94, %r96, %r95, %r97;
	mov.b32 	%f183, %r98;
	add.f32 	%f184, %f182, %f183;
	mov.b32 	%r99, %f184;
	mov.u32 	%r100, 8;
	shfl.sync.bfly.b32 	%r101|%p26, %r99, %r100, %r95, %r97;
	mov.b32 	%f185, %r101;
	add.f32 	%f186, %f184, %f185;
	mov.b32 	%r102, %f186;
	mov.u32 	%r103, 4;
	shfl.sync.bfly.b32 	%r104|%p27, %r102, %r103, %r95, %r97;
	mov.b32 	%f187, %r104;
	add.f32 	%f188, %f186, %f187;
	mov.b32 	%r105, %f188;
	mov.u32 	%r106, 2;
	shfl.sync.bfly.b32 	%r107|%p28, %r105, %r106, %r95, %r97;
	mov.b32 	%f189, %r107;
	add.f32 	%f190, %f188, %f189;
	mov.b32 	%r108, %f190;
	mov.u32 	%r109, 1;
	shfl.sync.bfly.b32 	%r110|%p29, %r108, %r109, %r95, %r97;
	mov.b32 	%f191, %r110;
	add.f32 	%f192, %f190, %f191;
	st.local.f32 	[%rd2+4], %f192;

$L__BB21_13:
	bar.sync 	0;
	mov.b32 	%r111, %f293;
	mov.u32 	%r112, 31;
	mov.u32 	%r113, 16;
	mov.u32 	%r114, -1;
	shfl.sync.bfly.b32 	%r115|%p31, %r111, %r113, %r112, %r114;
	mov.b32 	%f193, %r115;
	add.f32 	%f194, %f293, %f193;
	mov.b32 	%r116, %f194;
	mov.u32 	%r117, 8;
	shfl.sync.bfly.b32 	%r118|%p32, %r116, %r117, %r112, %r114;
	mov.b32 	%f195, %r118;
	add.f32 	%f196, %f194, %f195;
	mov.b32 	%r119, %f196;
	mov.u32 	%r120, 4;
	shfl.sync.bfly.b32 	%r121|%p33, %r119, %r120, %r112, %r114;
	mov.b32 	%f197, %r121;
	add.f32 	%f198, %f196, %f197;
	mov.b32 	%r122, %f198;
	mov.u32 	%r123, 2;
	shfl.sync.bfly.b32 	%r124|%p34, %r122, %r123, %r112, %r114;
	mov.b32 	%f199, %r124;
	add.f32 	%f200, %f198, %f199;
	mov.b32 	%r125, %f200;
	mov.u32 	%r126, 1;
	shfl.sync.bfly.b32 	%r127|%p35, %r125, %r126, %r112, %r114;
	mov.b32 	%f201, %r127;
	add.f32 	%f202, %f200, %f201;
	st.local.f32 	[%rd2+8], %f202;
	st.shared.f32 	[%r10], %f202;
	bar.sync 	0;
	@%p1 bra 	$L__BB21_15;

	ld.shared.f32 	%f203, [%r4];
	mov.b32 	%r128, %f203;
	shfl.sync.bfly.b32 	%r132|%p36, %r128, %r113, %r112, %r114;
	mov.b32 	%f204, %r132;
	add.f32 	%f205, %f203, %f204;
	mov.b32 	%r133, %f205;
	shfl.sync.bfly.b32 	%r135|%p37, %r133, %r117, %r112, %r114;
	mov.b32 	%f206, %r135;
	add.f32 	%f207, %f205, %f206;
	mov.b32 	%r136, %f207;
	shfl.sync.bfly.b32 	%r138|%p38, %r136, %r120, %r112, %r114;
	mov.b32 	%f208, %r138;
	add.f32 	%f209, %f207, %f208;
	mov.b32 	%r139, %f209;
	shfl.sync.bfly.b32 	%r141|%p39, %r139, %r123, %r112, %r114;
	mov.b32 	%f210, %r141;
	add.f32 	%f211, %f209, %f210;
	mov.b32 	%r142, %f211;
	shfl.sync.bfly.b32 	%r144|%p40, %r142, %r126, %r112, %r114;
	mov.b32 	%f212, %r144;
	add.f32 	%f213, %f211, %f212;
	st.local.f32 	[%rd2+8], %f213;

$L__BB21_15:
	bar.sync 	0;
	mov.b32 	%r145, %f292;
	shfl.sync.bfly.b32 	%r149|%p42, %r145, %r113, %r112, %r114;
	mov.b32 	%f214, %r149;
	add.f32 	%f215, %f292, %f214;
	mov.b32 	%r150, %f215;
	shfl.sync.bfly.b32 	%r152|%p43, %r150, %r117, %r112, %r114;
	mov.b32 	%f216, %r152;
	add.f32 	%f217, %f215, %f216;
	mov.b32 	%r153, %f217;
	shfl.sync.bfly.b32 	%r155|%p44, %r153, %r120, %r112, %r114;
	mov.b32 	%f218, %r155;
	add.f32 	%f219, %f217, %f218;
	mov.b32 	%r156, %f219;
	shfl.sync.bfly.b32 	%r158|%p45, %r156, %r123, %r112, %r114;
	mov.b32 	%f220, %r158;
	add.f32 	%f221, %f219, %f220;
	mov.b32 	%r159, %f221;
	shfl.sync.bfly.b32 	%r161|%p46, %r159, %r126, %r112, %r114;
	mov.b32 	%f222, %r161;
	add.f32 	%f223, %f221, %f222;
	st.local.f32 	[%rd2+12], %f223;
	st.shared.f32 	[%r10], %f223;
	bar.sync 	0;
	@%p1 bra 	$L__BB21_17;

	ld.shared.f32 	%f224, [%r4];
	mov.b32 	%r162, %f224;
	mov.u32 	%r163, 31;
	mov.u32 	%r164, 16;
	mov.u32 	%r165, -1;
	shfl.sync.bfly.b32 	%r166|%p47, %r162, %r164, %r163, %r165;
	mov.b32 	%f225, %r166;
	add.f32 	%f226, %f224, %f225;
	mov.b32 	%r167, %f226;
	mov.u32 	%r168, 8;
	shfl.sync.bfly.b32 	%r169|%p48, %r167, %r168, %r163, %r165;
	mov.b32 	%f227, %r169;
	add.f32 	%f228, %f226, %f227;
	mov.b32 	%r170, %f228;
	mov.u32 	%r171, 4;
	shfl.sync.bfly.b32 	%r172|%p49, %r170, %r171, %r163, %r165;
	mov.b32 	%f229, %r172;
	add.f32 	%f230, %f228, %f229;
	mov.b32 	%r173, %f230;
	mov.u32 	%r174, 2;
	shfl.sync.bfly.b32 	%r175|%p50, %r173, %r174, %r163, %r165;
	mov.b32 	%f231, %r175;
	add.f32 	%f232, %f230, %f231;
	mov.b32 	%r176, %f232;
	mov.u32 	%r177, 1;
	shfl.sync.bfly.b32 	%r178|%p51, %r176, %r177, %r163, %r165;
	mov.b32 	%f233, %r178;
	add.f32 	%f234, %f232, %f233;
	st.local.f32 	[%rd2+12], %f234;

$L__BB21_17:
	bar.sync 	0;
	mov.b32 	%r179, %f291;
	mov.u32 	%r180, 31;
	mov.u32 	%r181, 16;
	mov.u32 	%r182, -1;
	shfl.sync.bfly.b32 	%r183|%p53, %r179, %r181, %r180, %r182;
	mov.b32 	%f235, %r183;
	add.f32 	%f236, %f291, %f235;
	mov.b32 	%r184, %f236;
	mov.u32 	%r185, 8;
	shfl.sync.bfly.b32 	%r186|%p54, %r184, %r185, %r180, %r182;
	mov.b32 	%f237, %r186;
	add.f32 	%f238, %f236, %f237;
	mov.b32 	%r187, %f238;
	mov.u32 	%r188, 4;
	shfl.sync.bfly.b32 	%r189|%p55, %r187, %r188, %r180, %r182;
	mov.b32 	%f239, %r189;
	add.f32 	%f240, %f238, %f239;
	mov.b32 	%r190, %f240;
	mov.u32 	%r191, 2;
	shfl.sync.bfly.b32 	%r192|%p56, %r190, %r191, %r180, %r182;
	mov.b32 	%f241, %r192;
	add.f32 	%f242, %f240, %f241;
	mov.b32 	%r193, %f242;
	mov.u32 	%r194, 1;
	shfl.sync.bfly.b32 	%r195|%p57, %r193, %r194, %r180, %r182;
	mov.b32 	%f243, %r195;
	add.f32 	%f244, %f242, %f243;
	st.local.f32 	[%rd2+16], %f244;
	st.shared.f32 	[%r10], %f244;
	bar.sync 	0;
	@%p1 bra 	$L__BB21_19;

	ld.shared.f32 	%f245, [%r4];
	mov.b32 	%r196, %f245;
	shfl.sync.bfly.b32 	%r200|%p58, %r196, %r181, %r180, %r182;
	mov.b32 	%f246, %r200;
	add.f32 	%f247, %f245, %f246;
	mov.b32 	%r201, %f247;
	shfl.sync.bfly.b32 	%r203|%p59, %r201, %r185, %r180, %r182;
	mov.b32 	%f248, %r203;
	add.f32 	%f249, %f247, %f248;
	mov.b32 	%r204, %f249;
	shfl.sync.bfly.b32 	%r206|%p60, %r204, %r188, %r180, %r182;
	mov.b32 	%f250, %r206;
	add.f32 	%f251, %f249, %f250;
	mov.b32 	%r207, %f251;
	shfl.sync.bfly.b32 	%r209|%p61, %r207, %r191, %r180, %r182;
	mov.b32 	%f252, %r209;
	add.f32 	%f253, %f251, %f252;
	mov.b32 	%r210, %f253;
	shfl.sync.bfly.b32 	%r212|%p62, %r210, %r194, %r180, %r182;
	mov.b32 	%f254, %r212;
	add.f32 	%f255, %f253, %f254;
	st.local.f32 	[%rd2+16], %f255;

$L__BB21_19:
	bar.sync 	0;
	mov.b32 	%r213, %f290;
	shfl.sync.bfly.b32 	%r217|%p64, %r213, %r181, %r180, %r182;
	mov.b32 	%f256, %r217;
	add.f32 	%f257, %f290, %f256;
	mov.b32 	%r218, %f257;
	shfl.sync.bfly.b32 	%r220|%p65, %r218, %r185, %r180, %r182;
	mov.b32 	%f258, %r220;
	add.f32 	%f259, %f257, %f258;
	mov.b32 	%r221, %f259;
	shfl.sync.bfly.b32 	%r223|%p66, %r221, %r188, %r180, %r182;
	mov.b32 	%f260, %r223;
	add.f32 	%f261, %f259, %f260;
	mov.b32 	%r224, %f261;
	shfl.sync.bfly.b32 	%r226|%p67, %r224, %r191, %r180, %r182;
	mov.b32 	%f262, %r226;
	add.f32 	%f263, %f261, %f262;
	mov.b32 	%r227, %f263;
	shfl.sync.bfly.b32 	%r229|%p68, %r227, %r194, %r180, %r182;
	mov.b32 	%f264, %r229;
	add.f32 	%f265, %f263, %f264;
	st.local.f32 	[%rd2+20], %f265;
	st.shared.f32 	[%r10], %f265;
	bar.sync 	0;
	@%p1 bra 	$L__BB21_21;

	ld.shared.f32 	%f266, [%r4];
	mov.b32 	%r230, %f266;
	mov.u32 	%r231, 31;
	mov.u32 	%r232, 16;
	mov.u32 	%r233, -1;
	shfl.sync.bfly.b32 	%r234|%p69, %r230, %r232, %r231, %r233;
	mov.b32 	%f267, %r234;
	add.f32 	%f268, %f266, %f267;
	mov.b32 	%r235, %f268;
	mov.u32 	%r236, 8;
	shfl.sync.bfly.b32 	%r237|%p70, %r235, %r236, %r231, %r233;
	mov.b32 	%f269, %r237;
	add.f32 	%f270, %f268, %f269;
	mov.b32 	%r238, %f270;
	mov.u32 	%r239, 4;
	shfl.sync.bfly.b32 	%r240|%p71, %r238, %r239, %r231, %r233;
	mov.b32 	%f271, %r240;
	add.f32 	%f272, %f270, %f271;
	mov.b32 	%r241, %f272;
	mov.u32 	%r242, 2;
	shfl.sync.bfly.b32 	%r243|%p72, %r241, %r242, %r231, %r233;
	mov.b32 	%f273, %r243;
	add.f32 	%f274, %f272, %f273;
	mov.b32 	%r244, %f274;
	mov.u32 	%r245, 1;
	shfl.sync.bfly.b32 	%r246|%p73, %r244, %r245, %r231, %r233;
	mov.b32 	%f275, %r246;
	add.f32 	%f276, %f274, %f275;
	st.local.f32 	[%rd2+20], %f276;

$L__BB21_21:
	bar.sync 	0;
	setp.gt.s32 	%p74, %r3, 5;
	@%p74 bra 	$L__BB21_23;

	mul.wide.s32 	%rd60, %r3, 4;
	add.s64 	%rd61, %rd2, %rd60;
	ld.local.f32 	%f277, [%rd61];
	mad.lo.s32 	%r247, %r3, %r13, %r2;
	cvt.s64.s32 	%rd62, %r247;
	mul.lo.s32 	%r248, %r1, %r14;
	cvt.s64.s32 	%rd63, %r248;
	add.s64 	%rd64, %rd63, %rd62;
	cvta.to.global.u64 	%rd65, %rd19;
	shl.b64 	%rd66, %rd64, 2;
	add.s64 	%rd67, %rd65, %rd66;
	st.global.f32 	[%rd67], %f277;

$L__BB21_23:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_7_bs_96
.visible .entry ggml_matvec_f32_ncols_7_bs_96(
	.param .u64 ggml_matvec_f32_ncols_7_bs_96_param_0,
	.param .u64 ggml_matvec_f32_ncols_7_bs_96_param_1,
	.param .u64 ggml_matvec_f32_ncols_7_bs_96_param_2,
	.param .u32 ggml_matvec_f32_ncols_7_bs_96_param_3,
	.param .u32 ggml_matvec_f32_ncols_7_bs_96_param_4,
	.param .u32 ggml_matvec_f32_ncols_7_bs_96_param_5,
	.param .u32 ggml_matvec_f32_ncols_7_bs_96_param_6,
	.param .u32 ggml_matvec_f32_ncols_7_bs_96_param_7,
	.param .u32 ggml_matvec_f32_ncols_7_bs_96_param_8,
	.param .u32 ggml_matvec_f32_ncols_7_bs_96_param_9,
	.param .u32 ggml_matvec_f32_ncols_7_bs_96_param_10,
	.param .u32 ggml_matvec_f32_ncols_7_bs_96_param_11
)
{
	.local .align 4 .b8 	__local_depot22[28];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<86>;
	.reg .f32 	%f<343>;
	.reg .b32 	%r<287>;
	.reg .b64 	%rd<74>;


	mov.u64 	%SPL, __local_depot22;
	ld.param.u64 	%rd21, [ggml_matvec_f32_ncols_7_bs_96_param_0];
	ld.param.u64 	%rd22, [ggml_matvec_f32_ncols_7_bs_96_param_1];
	ld.param.u64 	%rd20, [ggml_matvec_f32_ncols_7_bs_96_param_2];
	ld.param.u32 	%r11, [ggml_matvec_f32_ncols_7_bs_96_param_3];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_7_bs_96_param_5];
	ld.param.u32 	%r12, [ggml_matvec_f32_ncols_7_bs_96_param_6];
	ld.param.u32 	%r13, [ggml_matvec_f32_ncols_7_bs_96_param_7];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_7_bs_96_param_8];
	ld.param.u32 	%r17, [ggml_matvec_f32_ncols_7_bs_96_param_9];
	ld.param.u32 	%r18, [ggml_matvec_f32_ncols_7_bs_96_param_10];
	ld.param.u32 	%r14, [ggml_matvec_f32_ncols_7_bs_96_param_11];
	cvta.to.global.u64 	%rd73, %rd22;
	add.u64 	%rd2, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r19, %r1, %r16;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r20, %r2, %r15;
	mad.lo.s32 	%r21, %r19, %r17, %r20;
	cvt.s64.s32 	%rd3, %r21;
	cvta.to.global.u64 	%rd4, %rd21;
	mul.lo.s32 	%r22, %r1, %r18;
	cvt.s64.s32 	%rd5, %r22;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r23, %r3, 2;
	mov.u32 	%r24, data_mmv;
	add.s32 	%r4, %r24, %r23;
	@%p1 bra 	$L__BB22_2;

	mov.u32 	%r25, 0;
	st.shared.u32 	[%r4], %r25;

$L__BB22_2:
	bar.sync 	0;
	mov.f32 	%f336, 0f00000000;
	mov.u32 	%r26, 0;
	st.local.u32 	[%rd2], %r26;
	st.local.u32 	[%rd2+4], %r26;
	st.local.u32 	[%rd2+8], %r26;
	st.local.u32 	[%rd2+12], %r26;
	st.local.u32 	[%rd2+16], %r26;
	st.local.u32 	[%rd2+20], %r26;
	st.local.u32 	[%rd2+24], %r26;
	setp.ge.s32 	%p2, %r3, %r11;
	mov.f32 	%f337, %f336;
	mov.f32 	%f338, %f336;
	mov.f32 	%f339, %f336;
	mov.f32 	%f340, %f336;
	mov.f32 	%f341, %f336;
	mov.f32 	%f342, %f336;
	@%p2 bra 	$L__BB22_9;

	not.b32 	%r27, %r3;
	add.s32 	%r5, %r27, %r11;
	mul.wide.u32 	%rd24, %r5, -1431655765;
	shr.u64 	%rd25, %rd24, 38;
	and.b64  	%rd26, %rd25, 1;
	setp.eq.b64 	%p3, %rd26, 1;
	mov.pred 	%p4, 0;
	xor.pred  	%p5, %p3, %p4;
	mov.f32 	%f336, 0f00000000;
	mov.u32 	%r286, %r3;
	@%p5 bra 	$L__BB22_5;

	shl.b64 	%rd27, %rd5, 2;
	add.s64 	%rd28, %rd73, %rd27;
	shl.b64 	%rd29, %rd3, 2;
	add.s64 	%rd30, %rd4, %rd29;
	mul.wide.s32 	%rd31, %r3, 8;
	add.s64 	%rd32, %rd30, %rd31;
	ld.global.nc.v2.f32 	{%f50, %f51}, [%rd32];
	add.s64 	%rd33, %rd28, %rd31;
	ld.global.nc.v2.f32 	{%f54, %f55}, [%rd33];
	fma.rn.f32 	%f58, %f50, %f54, 0f00000000;
	fma.rn.f32 	%f342, %f51, %f55, %f58;
	st.local.f32 	[%rd2], %f342;
	mul.wide.s32 	%rd34, %r12, 8;
	add.s64 	%rd35, %rd33, %rd34;
	ld.global.nc.v2.f32 	{%f59, %f60}, [%rd35];
	fma.rn.f32 	%f63, %f50, %f59, 0f00000000;
	fma.rn.f32 	%f341, %f51, %f60, %f63;
	st.local.f32 	[%rd2+4], %f341;
	add.s32 	%r28, %r3, %r12;
	add.s32 	%r29, %r28, %r12;
	mul.wide.s32 	%rd36, %r29, 8;
	add.s64 	%rd37, %rd28, %rd36;
	ld.global.nc.v2.f32 	{%f64, %f65}, [%rd37];
	fma.rn.f32 	%f68, %f50, %f64, 0f00000000;
	fma.rn.f32 	%f340, %f51, %f65, %f68;
	st.local.f32 	[%rd2+8], %f340;
	add.s64 	%rd38, %rd37, %rd34;
	ld.global.nc.v2.f32 	{%f69, %f70}, [%rd38];
	fma.rn.f32 	%f73, %f50, %f69, 0f00000000;
	fma.rn.f32 	%f339, %f51, %f70, %f73;
	st.local.f32 	[%rd2+12], %f339;
	add.s64 	%rd39, %rd38, %rd34;
	ld.global.nc.v2.f32 	{%f74, %f75}, [%rd39];
	fma.rn.f32 	%f78, %f50, %f74, 0f00000000;
	fma.rn.f32 	%f338, %f51, %f75, %f78;
	st.local.f32 	[%rd2+16], %f338;
	add.s64 	%rd40, %rd39, %rd34;
	ld.global.nc.v2.f32 	{%f79, %f80}, [%rd40];
	fma.rn.f32 	%f83, %f50, %f79, 0f00000000;
	fma.rn.f32 	%f337, %f51, %f80, %f83;
	st.local.f32 	[%rd2+20], %f337;
	add.s64 	%rd41, %rd40, %rd34;
	ld.global.nc.v2.f32 	{%f84, %f85}, [%rd41];
	fma.rn.f32 	%f88, %f50, %f84, 0f00000000;
	fma.rn.f32 	%f336, %f51, %f85, %f88;
	st.local.f32 	[%rd2+24], %f336;
	add.s32 	%r286, %r3, 96;

$L__BB22_5:
	setp.lt.u32 	%p6, %r5, 96;
	@%p6 bra 	$L__BB22_9;

	add.s32 	%r30, %r286, %r12;
	add.s32 	%r31, %r30, 96;
	mul.wide.s32 	%rd42, %r31, 8;
	shl.b64 	%rd43, %rd5, 2;
	add.s64 	%rd7, %rd42, %rd43;
	shl.b32 	%r32, %r12, 1;
	add.s32 	%r33, %r286, %r32;
	mad.lo.s32 	%r34, %r12, 3, %r286;
	shl.b32 	%r35, %r12, 2;
	add.s32 	%r36, %r286, %r35;
	mad.lo.s32 	%r37, %r12, 5, %r286;
	mad.lo.s32 	%r38, %r12, 6, %r286;
	mul.wide.s32 	%rd44, %r33, 8;
	add.s64 	%rd8, %rd44, %rd43;
	mul.wide.s32 	%rd45, %r34, 8;
	add.s64 	%rd9, %rd45, %rd43;
	mul.wide.s32 	%rd46, %r36, 8;
	add.s64 	%rd10, %rd46, %rd43;
	mul.wide.s32 	%rd47, %r37, 8;
	add.s64 	%rd11, %rd47, %rd43;
	mul.wide.s32 	%rd48, %r38, 8;
	add.s64 	%rd12, %rd48, %rd43;
	mul.wide.s32 	%rd49, %r286, 2;
	add.s64 	%rd50, %rd49, %rd3;
	shl.b64 	%rd51, %rd50, 2;
	add.s64 	%rd52, %rd4, %rd51;
	add.s64 	%rd72, %rd52, 768;
	mul.wide.s32 	%rd53, %r286, 8;
	mul.wide.s32 	%rd54, %r12, 8;
	add.s64 	%rd55, %rd53, %rd54;
	add.s64 	%rd14, %rd55, %rd43;
	add.s64 	%rd15, %rd53, %rd43;

$L__BB22_7:
	ld.global.nc.v2.f32 	{%f89, %f90}, [%rd72+-768];
	add.s64 	%rd56, %rd73, %rd15;
	ld.global.nc.v2.f32 	{%f93, %f94}, [%rd56];
	fma.rn.f32 	%f97, %f89, %f93, %f342;
	fma.rn.f32 	%f98, %f90, %f94, %f97;
	add.s64 	%rd57, %rd73, %rd14;
	ld.global.nc.v2.f32 	{%f99, %f100}, [%rd57];
	fma.rn.f32 	%f103, %f89, %f99, %f341;
	fma.rn.f32 	%f104, %f90, %f100, %f103;
	add.s64 	%rd58, %rd73, %rd8;
	ld.global.nc.v2.f32 	{%f105, %f106}, [%rd58];
	fma.rn.f32 	%f109, %f89, %f105, %f340;
	fma.rn.f32 	%f110, %f90, %f106, %f109;
	add.s64 	%rd59, %rd73, %rd9;
	ld.global.nc.v2.f32 	{%f111, %f112}, [%rd59];
	fma.rn.f32 	%f115, %f89, %f111, %f339;
	fma.rn.f32 	%f116, %f90, %f112, %f115;
	add.s64 	%rd60, %rd73, %rd10;
	ld.global.nc.v2.f32 	{%f117, %f118}, [%rd60];
	fma.rn.f32 	%f121, %f89, %f117, %f338;
	fma.rn.f32 	%f122, %f90, %f118, %f121;
	add.s64 	%rd61, %rd73, %rd11;
	ld.global.nc.v2.f32 	{%f123, %f124}, [%rd61];
	fma.rn.f32 	%f127, %f89, %f123, %f337;
	fma.rn.f32 	%f128, %f90, %f124, %f127;
	add.s64 	%rd62, %rd73, %rd12;
	ld.global.nc.v2.f32 	{%f129, %f130}, [%rd62];
	fma.rn.f32 	%f133, %f89, %f129, %f336;
	fma.rn.f32 	%f134, %f90, %f130, %f133;
	ld.global.nc.v2.f32 	{%f135, %f136}, [%rd72];
	ld.global.nc.v2.f32 	{%f139, %f140}, [%rd56+768];
	fma.rn.f32 	%f143, %f135, %f139, %f98;
	fma.rn.f32 	%f342, %f136, %f140, %f143;
	add.s64 	%rd63, %rd73, %rd7;
	ld.global.nc.v2.f32 	{%f144, %f145}, [%rd63];
	fma.rn.f32 	%f148, %f135, %f144, %f104;
	fma.rn.f32 	%f341, %f136, %f145, %f148;
	ld.global.nc.v2.f32 	{%f149, %f150}, [%rd58+768];
	fma.rn.f32 	%f153, %f135, %f149, %f110;
	fma.rn.f32 	%f340, %f136, %f150, %f153;
	ld.global.nc.v2.f32 	{%f154, %f155}, [%rd59+768];
	fma.rn.f32 	%f158, %f135, %f154, %f116;
	fma.rn.f32 	%f339, %f136, %f155, %f158;
	ld.global.nc.v2.f32 	{%f159, %f160}, [%rd60+768];
	fma.rn.f32 	%f163, %f135, %f159, %f122;
	fma.rn.f32 	%f338, %f136, %f160, %f163;
	ld.global.nc.v2.f32 	{%f164, %f165}, [%rd61+768];
	fma.rn.f32 	%f168, %f135, %f164, %f128;
	fma.rn.f32 	%f337, %f136, %f165, %f168;
	ld.global.nc.v2.f32 	{%f169, %f170}, [%rd62+768];
	fma.rn.f32 	%f173, %f135, %f169, %f134;
	fma.rn.f32 	%f336, %f136, %f170, %f173;
	add.s64 	%rd73, %rd73, 1536;
	add.s64 	%rd72, %rd72, 1536;
	add.s32 	%r286, %r286, 192;
	setp.lt.s32 	%p7, %r286, %r11;
	@%p7 bra 	$L__BB22_7;

	st.local.f32 	[%rd2], %f342;
	st.local.f32 	[%rd2+4], %f341;
	st.local.f32 	[%rd2+8], %f340;
	st.local.f32 	[%rd2+12], %f339;
	st.local.f32 	[%rd2+16], %f338;
	st.local.f32 	[%rd2+20], %f337;
	st.local.f32 	[%rd2+24], %f336;

$L__BB22_9:
	shr.s32 	%r39, %r3, 31;
	shr.u32 	%r40, %r39, 27;
	add.s32 	%r41, %r3, %r40;
	shr.s32 	%r42, %r41, 5;
	shl.b32 	%r43, %r42, 2;
	add.s32 	%r10, %r24, %r43;
	mov.u32 	%r45, 2;
	mov.b32 	%r46, %f342;
	mov.u32 	%r47, 31;
	mov.u32 	%r48, 16;
	mov.u32 	%r49, -1;
	shfl.sync.bfly.b32 	%r50|%p8, %r46, %r48, %r47, %r49;
	mov.b32 	%f174, %r50;
	add.f32 	%f175, %f342, %f174;
	mov.b32 	%r51, %f175;
	mov.u32 	%r52, 8;
	shfl.sync.bfly.b32 	%r53|%p9, %r51, %r52, %r47, %r49;
	mov.b32 	%f176, %r53;
	add.f32 	%f177, %f175, %f176;
	mov.b32 	%r54, %f177;
	mov.u32 	%r55, 4;
	shfl.sync.bfly.b32 	%r56|%p10, %r54, %r55, %r47, %r49;
	mov.b32 	%f178, %r56;
	add.f32 	%f179, %f177, %f178;
	mov.b32 	%r57, %f179;
	shfl.sync.bfly.b32 	%r58|%p11, %r57, %r45, %r47, %r49;
	mov.b32 	%f180, %r58;
	add.f32 	%f181, %f179, %f180;
	mov.b32 	%r59, %f181;
	mov.u32 	%r60, 1;
	shfl.sync.bfly.b32 	%r61|%p12, %r59, %r60, %r47, %r49;
	mov.b32 	%f182, %r61;
	add.f32 	%f183, %f181, %f182;
	st.local.f32 	[%rd2], %f183;
	st.shared.f32 	[%r10], %f183;
	bar.sync 	0;
	@%p1 bra 	$L__BB22_11;

	ld.shared.f32 	%f184, [%r4];
	mov.b32 	%r62, %f184;
	shfl.sync.bfly.b32 	%r66|%p14, %r62, %r48, %r47, %r49;
	mov.b32 	%f185, %r66;
	add.f32 	%f186, %f184, %f185;
	mov.b32 	%r67, %f186;
	shfl.sync.bfly.b32 	%r69|%p15, %r67, %r52, %r47, %r49;
	mov.b32 	%f187, %r69;
	add.f32 	%f188, %f186, %f187;
	mov.b32 	%r70, %f188;
	shfl.sync.bfly.b32 	%r72|%p16, %r70, %r55, %r47, %r49;
	mov.b32 	%f189, %r72;
	add.f32 	%f190, %f188, %f189;
	mov.b32 	%r73, %f190;
	shfl.sync.bfly.b32 	%r75|%p17, %r73, %r45, %r47, %r49;
	mov.b32 	%f191, %r75;
	add.f32 	%f192, %f190, %f191;
	mov.b32 	%r76, %f192;
	shfl.sync.bfly.b32 	%r78|%p18, %r76, %r60, %r47, %r49;
	mov.b32 	%f193, %r78;
	add.f32 	%f194, %f192, %f193;
	st.local.f32 	[%rd2], %f194;

$L__BB22_11:
	bar.sync 	0;
	mov.b32 	%r79, %f341;
	shfl.sync.bfly.b32 	%r83|%p20, %r79, %r48, %r47, %r49;
	mov.b32 	%f195, %r83;
	add.f32 	%f196, %f341, %f195;
	mov.b32 	%r84, %f196;
	shfl.sync.bfly.b32 	%r86|%p21, %r84, %r52, %r47, %r49;
	mov.b32 	%f197, %r86;
	add.f32 	%f198, %f196, %f197;
	mov.b32 	%r87, %f198;
	shfl.sync.bfly.b32 	%r89|%p22, %r87, %r55, %r47, %r49;
	mov.b32 	%f199, %r89;
	add.f32 	%f200, %f198, %f199;
	mov.b32 	%r90, %f200;
	shfl.sync.bfly.b32 	%r92|%p23, %r90, %r45, %r47, %r49;
	mov.b32 	%f201, %r92;
	add.f32 	%f202, %f200, %f201;
	mov.b32 	%r93, %f202;
	shfl.sync.bfly.b32 	%r95|%p24, %r93, %r60, %r47, %r49;
	mov.b32 	%f203, %r95;
	add.f32 	%f204, %f202, %f203;
	st.local.f32 	[%rd2+4], %f204;
	st.shared.f32 	[%r10], %f204;
	bar.sync 	0;
	@%p1 bra 	$L__BB22_13;

	ld.shared.f32 	%f205, [%r4];
	mov.b32 	%r96, %f205;
	mov.u32 	%r97, 31;
	mov.u32 	%r98, 16;
	mov.u32 	%r99, -1;
	shfl.sync.bfly.b32 	%r100|%p25, %r96, %r98, %r97, %r99;
	mov.b32 	%f206, %r100;
	add.f32 	%f207, %f205, %f206;
	mov.b32 	%r101, %f207;
	mov.u32 	%r102, 8;
	shfl.sync.bfly.b32 	%r103|%p26, %r101, %r102, %r97, %r99;
	mov.b32 	%f208, %r103;
	add.f32 	%f209, %f207, %f208;
	mov.b32 	%r104, %f209;
	mov.u32 	%r105, 4;
	shfl.sync.bfly.b32 	%r106|%p27, %r104, %r105, %r97, %r99;
	mov.b32 	%f210, %r106;
	add.f32 	%f211, %f209, %f210;
	mov.b32 	%r107, %f211;
	mov.u32 	%r108, 2;
	shfl.sync.bfly.b32 	%r109|%p28, %r107, %r108, %r97, %r99;
	mov.b32 	%f212, %r109;
	add.f32 	%f213, %f211, %f212;
	mov.b32 	%r110, %f213;
	mov.u32 	%r111, 1;
	shfl.sync.bfly.b32 	%r112|%p29, %r110, %r111, %r97, %r99;
	mov.b32 	%f214, %r112;
	add.f32 	%f215, %f213, %f214;
	st.local.f32 	[%rd2+4], %f215;

$L__BB22_13:
	bar.sync 	0;
	mov.b32 	%r113, %f340;
	mov.u32 	%r114, 31;
	mov.u32 	%r115, 16;
	mov.u32 	%r116, -1;
	shfl.sync.bfly.b32 	%r117|%p31, %r113, %r115, %r114, %r116;
	mov.b32 	%f216, %r117;
	add.f32 	%f217, %f340, %f216;
	mov.b32 	%r118, %f217;
	mov.u32 	%r119, 8;
	shfl.sync.bfly.b32 	%r120|%p32, %r118, %r119, %r114, %r116;
	mov.b32 	%f218, %r120;
	add.f32 	%f219, %f217, %f218;
	mov.b32 	%r121, %f219;
	mov.u32 	%r122, 4;
	shfl.sync.bfly.b32 	%r123|%p33, %r121, %r122, %r114, %r116;
	mov.b32 	%f220, %r123;
	add.f32 	%f221, %f219, %f220;
	mov.b32 	%r124, %f221;
	mov.u32 	%r125, 2;
	shfl.sync.bfly.b32 	%r126|%p34, %r124, %r125, %r114, %r116;
	mov.b32 	%f222, %r126;
	add.f32 	%f223, %f221, %f222;
	mov.b32 	%r127, %f223;
	mov.u32 	%r128, 1;
	shfl.sync.bfly.b32 	%r129|%p35, %r127, %r128, %r114, %r116;
	mov.b32 	%f224, %r129;
	add.f32 	%f225, %f223, %f224;
	st.local.f32 	[%rd2+8], %f225;
	st.shared.f32 	[%r10], %f225;
	bar.sync 	0;
	@%p1 bra 	$L__BB22_15;

	ld.shared.f32 	%f226, [%r4];
	mov.b32 	%r130, %f226;
	shfl.sync.bfly.b32 	%r134|%p36, %r130, %r115, %r114, %r116;
	mov.b32 	%f227, %r134;
	add.f32 	%f228, %f226, %f227;
	mov.b32 	%r135, %f228;
	shfl.sync.bfly.b32 	%r137|%p37, %r135, %r119, %r114, %r116;
	mov.b32 	%f229, %r137;
	add.f32 	%f230, %f228, %f229;
	mov.b32 	%r138, %f230;
	shfl.sync.bfly.b32 	%r140|%p38, %r138, %r122, %r114, %r116;
	mov.b32 	%f231, %r140;
	add.f32 	%f232, %f230, %f231;
	mov.b32 	%r141, %f232;
	shfl.sync.bfly.b32 	%r143|%p39, %r141, %r125, %r114, %r116;
	mov.b32 	%f233, %r143;
	add.f32 	%f234, %f232, %f233;
	mov.b32 	%r144, %f234;
	shfl.sync.bfly.b32 	%r146|%p40, %r144, %r128, %r114, %r116;
	mov.b32 	%f235, %r146;
	add.f32 	%f236, %f234, %f235;
	st.local.f32 	[%rd2+8], %f236;

$L__BB22_15:
	bar.sync 	0;
	mov.b32 	%r147, %f339;
	shfl.sync.bfly.b32 	%r151|%p42, %r147, %r115, %r114, %r116;
	mov.b32 	%f237, %r151;
	add.f32 	%f238, %f339, %f237;
	mov.b32 	%r152, %f238;
	shfl.sync.bfly.b32 	%r154|%p43, %r152, %r119, %r114, %r116;
	mov.b32 	%f239, %r154;
	add.f32 	%f240, %f238, %f239;
	mov.b32 	%r155, %f240;
	shfl.sync.bfly.b32 	%r157|%p44, %r155, %r122, %r114, %r116;
	mov.b32 	%f241, %r157;
	add.f32 	%f242, %f240, %f241;
	mov.b32 	%r158, %f242;
	shfl.sync.bfly.b32 	%r160|%p45, %r158, %r125, %r114, %r116;
	mov.b32 	%f243, %r160;
	add.f32 	%f244, %f242, %f243;
	mov.b32 	%r161, %f244;
	shfl.sync.bfly.b32 	%r163|%p46, %r161, %r128, %r114, %r116;
	mov.b32 	%f245, %r163;
	add.f32 	%f246, %f244, %f245;
	st.local.f32 	[%rd2+12], %f246;
	st.shared.f32 	[%r10], %f246;
	bar.sync 	0;
	@%p1 bra 	$L__BB22_17;

	ld.shared.f32 	%f247, [%r4];
	mov.b32 	%r164, %f247;
	mov.u32 	%r165, 31;
	mov.u32 	%r166, 16;
	mov.u32 	%r167, -1;
	shfl.sync.bfly.b32 	%r168|%p47, %r164, %r166, %r165, %r167;
	mov.b32 	%f248, %r168;
	add.f32 	%f249, %f247, %f248;
	mov.b32 	%r169, %f249;
	mov.u32 	%r170, 8;
	shfl.sync.bfly.b32 	%r171|%p48, %r169, %r170, %r165, %r167;
	mov.b32 	%f250, %r171;
	add.f32 	%f251, %f249, %f250;
	mov.b32 	%r172, %f251;
	mov.u32 	%r173, 4;
	shfl.sync.bfly.b32 	%r174|%p49, %r172, %r173, %r165, %r167;
	mov.b32 	%f252, %r174;
	add.f32 	%f253, %f251, %f252;
	mov.b32 	%r175, %f253;
	mov.u32 	%r176, 2;
	shfl.sync.bfly.b32 	%r177|%p50, %r175, %r176, %r165, %r167;
	mov.b32 	%f254, %r177;
	add.f32 	%f255, %f253, %f254;
	mov.b32 	%r178, %f255;
	mov.u32 	%r179, 1;
	shfl.sync.bfly.b32 	%r180|%p51, %r178, %r179, %r165, %r167;
	mov.b32 	%f256, %r180;
	add.f32 	%f257, %f255, %f256;
	st.local.f32 	[%rd2+12], %f257;

$L__BB22_17:
	bar.sync 	0;
	mov.b32 	%r181, %f338;
	mov.u32 	%r182, 31;
	mov.u32 	%r183, 16;
	mov.u32 	%r184, -1;
	shfl.sync.bfly.b32 	%r185|%p53, %r181, %r183, %r182, %r184;
	mov.b32 	%f258, %r185;
	add.f32 	%f259, %f338, %f258;
	mov.b32 	%r186, %f259;
	mov.u32 	%r187, 8;
	shfl.sync.bfly.b32 	%r188|%p54, %r186, %r187, %r182, %r184;
	mov.b32 	%f260, %r188;
	add.f32 	%f261, %f259, %f260;
	mov.b32 	%r189, %f261;
	mov.u32 	%r190, 4;
	shfl.sync.bfly.b32 	%r191|%p55, %r189, %r190, %r182, %r184;
	mov.b32 	%f262, %r191;
	add.f32 	%f263, %f261, %f262;
	mov.b32 	%r192, %f263;
	mov.u32 	%r193, 2;
	shfl.sync.bfly.b32 	%r194|%p56, %r192, %r193, %r182, %r184;
	mov.b32 	%f264, %r194;
	add.f32 	%f265, %f263, %f264;
	mov.b32 	%r195, %f265;
	mov.u32 	%r196, 1;
	shfl.sync.bfly.b32 	%r197|%p57, %r195, %r196, %r182, %r184;
	mov.b32 	%f266, %r197;
	add.f32 	%f267, %f265, %f266;
	st.local.f32 	[%rd2+16], %f267;
	st.shared.f32 	[%r10], %f267;
	bar.sync 	0;
	@%p1 bra 	$L__BB22_19;

	ld.shared.f32 	%f268, [%r4];
	mov.b32 	%r198, %f268;
	shfl.sync.bfly.b32 	%r202|%p58, %r198, %r183, %r182, %r184;
	mov.b32 	%f269, %r202;
	add.f32 	%f270, %f268, %f269;
	mov.b32 	%r203, %f270;
	shfl.sync.bfly.b32 	%r205|%p59, %r203, %r187, %r182, %r184;
	mov.b32 	%f271, %r205;
	add.f32 	%f272, %f270, %f271;
	mov.b32 	%r206, %f272;
	shfl.sync.bfly.b32 	%r208|%p60, %r206, %r190, %r182, %r184;
	mov.b32 	%f273, %r208;
	add.f32 	%f274, %f272, %f273;
	mov.b32 	%r209, %f274;
	shfl.sync.bfly.b32 	%r211|%p61, %r209, %r193, %r182, %r184;
	mov.b32 	%f275, %r211;
	add.f32 	%f276, %f274, %f275;
	mov.b32 	%r212, %f276;
	shfl.sync.bfly.b32 	%r214|%p62, %r212, %r196, %r182, %r184;
	mov.b32 	%f277, %r214;
	add.f32 	%f278, %f276, %f277;
	st.local.f32 	[%rd2+16], %f278;

$L__BB22_19:
	bar.sync 	0;
	mov.b32 	%r215, %f337;
	shfl.sync.bfly.b32 	%r219|%p64, %r215, %r183, %r182, %r184;
	mov.b32 	%f279, %r219;
	add.f32 	%f280, %f337, %f279;
	mov.b32 	%r220, %f280;
	shfl.sync.bfly.b32 	%r222|%p65, %r220, %r187, %r182, %r184;
	mov.b32 	%f281, %r222;
	add.f32 	%f282, %f280, %f281;
	mov.b32 	%r223, %f282;
	shfl.sync.bfly.b32 	%r225|%p66, %r223, %r190, %r182, %r184;
	mov.b32 	%f283, %r225;
	add.f32 	%f284, %f282, %f283;
	mov.b32 	%r226, %f284;
	shfl.sync.bfly.b32 	%r228|%p67, %r226, %r193, %r182, %r184;
	mov.b32 	%f285, %r228;
	add.f32 	%f286, %f284, %f285;
	mov.b32 	%r229, %f286;
	shfl.sync.bfly.b32 	%r231|%p68, %r229, %r196, %r182, %r184;
	mov.b32 	%f287, %r231;
	add.f32 	%f288, %f286, %f287;
	st.local.f32 	[%rd2+20], %f288;
	st.shared.f32 	[%r10], %f288;
	bar.sync 	0;
	@%p1 bra 	$L__BB22_21;

	ld.shared.f32 	%f289, [%r4];
	mov.b32 	%r232, %f289;
	mov.u32 	%r233, 31;
	mov.u32 	%r234, 16;
	mov.u32 	%r235, -1;
	shfl.sync.bfly.b32 	%r236|%p69, %r232, %r234, %r233, %r235;
	mov.b32 	%f290, %r236;
	add.f32 	%f291, %f289, %f290;
	mov.b32 	%r237, %f291;
	mov.u32 	%r238, 8;
	shfl.sync.bfly.b32 	%r239|%p70, %r237, %r238, %r233, %r235;
	mov.b32 	%f292, %r239;
	add.f32 	%f293, %f291, %f292;
	mov.b32 	%r240, %f293;
	mov.u32 	%r241, 4;
	shfl.sync.bfly.b32 	%r242|%p71, %r240, %r241, %r233, %r235;
	mov.b32 	%f294, %r242;
	add.f32 	%f295, %f293, %f294;
	mov.b32 	%r243, %f295;
	mov.u32 	%r244, 2;
	shfl.sync.bfly.b32 	%r245|%p72, %r243, %r244, %r233, %r235;
	mov.b32 	%f296, %r245;
	add.f32 	%f297, %f295, %f296;
	mov.b32 	%r246, %f297;
	mov.u32 	%r247, 1;
	shfl.sync.bfly.b32 	%r248|%p73, %r246, %r247, %r233, %r235;
	mov.b32 	%f298, %r248;
	add.f32 	%f299, %f297, %f298;
	st.local.f32 	[%rd2+20], %f299;

$L__BB22_21:
	bar.sync 	0;
	mov.b32 	%r249, %f336;
	mov.u32 	%r250, 31;
	mov.u32 	%r251, 16;
	mov.u32 	%r252, -1;
	shfl.sync.bfly.b32 	%r253|%p75, %r249, %r251, %r250, %r252;
	mov.b32 	%f300, %r253;
	add.f32 	%f301, %f336, %f300;
	mov.b32 	%r254, %f301;
	mov.u32 	%r255, 8;
	shfl.sync.bfly.b32 	%r256|%p76, %r254, %r255, %r250, %r252;
	mov.b32 	%f302, %r256;
	add.f32 	%f303, %f301, %f302;
	mov.b32 	%r257, %f303;
	mov.u32 	%r258, 4;
	shfl.sync.bfly.b32 	%r259|%p77, %r257, %r258, %r250, %r252;
	mov.b32 	%f304, %r259;
	add.f32 	%f305, %f303, %f304;
	mov.b32 	%r260, %f305;
	mov.u32 	%r261, 2;
	shfl.sync.bfly.b32 	%r262|%p78, %r260, %r261, %r250, %r252;
	mov.b32 	%f306, %r262;
	add.f32 	%f307, %f305, %f306;
	mov.b32 	%r263, %f307;
	mov.u32 	%r264, 1;
	shfl.sync.bfly.b32 	%r265|%p79, %r263, %r264, %r250, %r252;
	mov.b32 	%f308, %r265;
	add.f32 	%f309, %f307, %f308;
	st.local.f32 	[%rd2+24], %f309;
	st.shared.f32 	[%r10], %f309;
	bar.sync 	0;
	@%p1 bra 	$L__BB22_23;

	ld.shared.f32 	%f310, [%r4];
	mov.b32 	%r266, %f310;
	shfl.sync.bfly.b32 	%r270|%p80, %r266, %r251, %r250, %r252;
	mov.b32 	%f311, %r270;
	add.f32 	%f312, %f310, %f311;
	mov.b32 	%r271, %f312;
	shfl.sync.bfly.b32 	%r273|%p81, %r271, %r255, %r250, %r252;
	mov.b32 	%f313, %r273;
	add.f32 	%f314, %f312, %f313;
	mov.b32 	%r274, %f314;
	shfl.sync.bfly.b32 	%r276|%p82, %r274, %r258, %r250, %r252;
	mov.b32 	%f315, %r276;
	add.f32 	%f316, %f314, %f315;
	mov.b32 	%r277, %f316;
	shfl.sync.bfly.b32 	%r279|%p83, %r277, %r261, %r250, %r252;
	mov.b32 	%f317, %r279;
	add.f32 	%f318, %f316, %f317;
	mov.b32 	%r280, %f318;
	shfl.sync.bfly.b32 	%r282|%p84, %r280, %r264, %r250, %r252;
	mov.b32 	%f319, %r282;
	add.f32 	%f320, %f318, %f319;
	st.local.f32 	[%rd2+24], %f320;

$L__BB22_23:
	bar.sync 	0;
	setp.gt.s32 	%p85, %r3, 6;
	@%p85 bra 	$L__BB22_25;

	mul.wide.s32 	%rd64, %r3, 4;
	add.s64 	%rd65, %rd2, %rd64;
	ld.local.f32 	%f321, [%rd65];
	mad.lo.s32 	%r283, %r3, %r13, %r2;
	cvt.s64.s32 	%rd66, %r283;
	mul.lo.s32 	%r284, %r1, %r14;
	cvt.s64.s32 	%rd67, %r284;
	add.s64 	%rd68, %rd67, %rd66;
	cvta.to.global.u64 	%rd69, %rd20;
	shl.b64 	%rd70, %rd68, 2;
	add.s64 	%rd71, %rd69, %rd70;
	st.global.f32 	[%rd71], %f321;

$L__BB22_25:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_8_bs_96
.visible .entry ggml_matvec_f32_ncols_8_bs_96(
	.param .u64 ggml_matvec_f32_ncols_8_bs_96_param_0,
	.param .u64 ggml_matvec_f32_ncols_8_bs_96_param_1,
	.param .u64 ggml_matvec_f32_ncols_8_bs_96_param_2,
	.param .u32 ggml_matvec_f32_ncols_8_bs_96_param_3,
	.param .u32 ggml_matvec_f32_ncols_8_bs_96_param_4,
	.param .u32 ggml_matvec_f32_ncols_8_bs_96_param_5,
	.param .u32 ggml_matvec_f32_ncols_8_bs_96_param_6,
	.param .u32 ggml_matvec_f32_ncols_8_bs_96_param_7,
	.param .u32 ggml_matvec_f32_ncols_8_bs_96_param_8,
	.param .u32 ggml_matvec_f32_ncols_8_bs_96_param_9,
	.param .u32 ggml_matvec_f32_ncols_8_bs_96_param_10,
	.param .u32 ggml_matvec_f32_ncols_8_bs_96_param_11
)
{
	.local .align 16 .b8 	__local_depot23[32];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<97>;
	.reg .f32 	%f<390>;
	.reg .b32 	%r<321>;
	.reg .b64 	%rd<78>;


	mov.u64 	%SPL, __local_depot23;
	ld.param.u64 	%rd22, [ggml_matvec_f32_ncols_8_bs_96_param_0];
	ld.param.u64 	%rd23, [ggml_matvec_f32_ncols_8_bs_96_param_1];
	ld.param.u64 	%rd21, [ggml_matvec_f32_ncols_8_bs_96_param_2];
	ld.param.u32 	%r11, [ggml_matvec_f32_ncols_8_bs_96_param_3];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_8_bs_96_param_5];
	ld.param.u32 	%r12, [ggml_matvec_f32_ncols_8_bs_96_param_6];
	ld.param.u32 	%r13, [ggml_matvec_f32_ncols_8_bs_96_param_7];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_8_bs_96_param_8];
	ld.param.u32 	%r17, [ggml_matvec_f32_ncols_8_bs_96_param_9];
	ld.param.u32 	%r18, [ggml_matvec_f32_ncols_8_bs_96_param_10];
	ld.param.u32 	%r14, [ggml_matvec_f32_ncols_8_bs_96_param_11];
	cvta.to.global.u64 	%rd77, %rd23;
	add.u64 	%rd2, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r19, %r1, %r16;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r20, %r2, %r15;
	mad.lo.s32 	%r21, %r19, %r17, %r20;
	cvt.s64.s32 	%rd3, %r21;
	cvta.to.global.u64 	%rd4, %rd22;
	mul.lo.s32 	%r22, %r1, %r18;
	cvt.s64.s32 	%rd5, %r22;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r23, %r3, 2;
	mov.u32 	%r24, data_mmv;
	add.s32 	%r4, %r24, %r23;
	@%p1 bra 	$L__BB23_2;

	mov.u32 	%r25, 0;
	st.shared.u32 	[%r4], %r25;

$L__BB23_2:
	bar.sync 	0;
	mov.f32 	%f382, 0f00000000;
	st.local.v4.f32 	[%rd2], {%f382, %f382, %f382, %f382};
	st.local.v4.f32 	[%rd2+16], {%f382, %f382, %f382, %f382};
	setp.ge.s32 	%p2, %r3, %r11;
	mov.f32 	%f383, %f382;
	mov.f32 	%f384, %f382;
	mov.f32 	%f385, %f382;
	mov.f32 	%f386, %f382;
	mov.f32 	%f387, %f382;
	mov.f32 	%f388, %f382;
	mov.f32 	%f389, %f382;
	@%p2 bra 	$L__BB23_9;

	not.b32 	%r26, %r3;
	add.s32 	%r5, %r26, %r11;
	mul.wide.u32 	%rd25, %r5, -1431655765;
	shr.u64 	%rd26, %rd25, 38;
	and.b64  	%rd27, %rd26, 1;
	setp.eq.b64 	%p3, %rd27, 1;
	mov.pred 	%p4, 0;
	xor.pred  	%p5, %p3, %p4;
	mov.f32 	%f382, 0f00000000;
	mov.u32 	%r320, %r3;
	@%p5 bra 	$L__BB23_5;

	shl.b64 	%rd28, %rd5, 2;
	add.s64 	%rd29, %rd77, %rd28;
	shl.b64 	%rd30, %rd3, 2;
	add.s64 	%rd31, %rd4, %rd30;
	mul.wide.s32 	%rd32, %r3, 8;
	add.s64 	%rd33, %rd31, %rd32;
	ld.global.nc.v2.f32 	{%f57, %f58}, [%rd33];
	add.s64 	%rd34, %rd29, %rd32;
	ld.global.nc.v2.f32 	{%f61, %f62}, [%rd34];
	fma.rn.f32 	%f65, %f57, %f61, 0f00000000;
	fma.rn.f32 	%f389, %f58, %f62, %f65;
	mul.wide.s32 	%rd35, %r12, 8;
	add.s64 	%rd36, %rd34, %rd35;
	ld.global.nc.v2.f32 	{%f66, %f67}, [%rd36];
	fma.rn.f32 	%f70, %f57, %f66, 0f00000000;
	fma.rn.f32 	%f388, %f58, %f67, %f70;
	add.s32 	%r27, %r3, %r12;
	add.s32 	%r28, %r27, %r12;
	mul.wide.s32 	%rd37, %r28, 8;
	add.s64 	%rd38, %rd29, %rd37;
	ld.global.nc.v2.f32 	{%f71, %f72}, [%rd38];
	fma.rn.f32 	%f75, %f57, %f71, 0f00000000;
	fma.rn.f32 	%f387, %f58, %f72, %f75;
	add.s64 	%rd39, %rd38, %rd35;
	ld.global.nc.v2.f32 	{%f76, %f77}, [%rd39];
	fma.rn.f32 	%f80, %f57, %f76, 0f00000000;
	fma.rn.f32 	%f386, %f58, %f77, %f80;
	st.local.v4.f32 	[%rd2], {%f389, %f388, %f387, %f386};
	add.s64 	%rd40, %rd39, %rd35;
	ld.global.nc.v2.f32 	{%f81, %f82}, [%rd40];
	fma.rn.f32 	%f85, %f57, %f81, 0f00000000;
	fma.rn.f32 	%f385, %f58, %f82, %f85;
	add.s64 	%rd41, %rd40, %rd35;
	ld.global.nc.v2.f32 	{%f86, %f87}, [%rd41];
	fma.rn.f32 	%f90, %f57, %f86, 0f00000000;
	fma.rn.f32 	%f384, %f58, %f87, %f90;
	add.s64 	%rd42, %rd41, %rd35;
	ld.global.nc.v2.f32 	{%f91, %f92}, [%rd42];
	fma.rn.f32 	%f95, %f57, %f91, 0f00000000;
	fma.rn.f32 	%f383, %f58, %f92, %f95;
	add.s64 	%rd43, %rd42, %rd35;
	ld.global.nc.v2.f32 	{%f96, %f97}, [%rd43];
	fma.rn.f32 	%f100, %f57, %f96, 0f00000000;
	fma.rn.f32 	%f382, %f58, %f97, %f100;
	st.local.v4.f32 	[%rd2+16], {%f385, %f384, %f383, %f382};
	add.s32 	%r320, %r3, 96;

$L__BB23_5:
	setp.lt.u32 	%p6, %r5, 96;
	@%p6 bra 	$L__BB23_9;

	add.s32 	%r29, %r320, %r12;
	add.s32 	%r30, %r29, 96;
	mul.wide.s32 	%rd44, %r30, 8;
	shl.b64 	%rd45, %rd5, 2;
	add.s64 	%rd7, %rd44, %rd45;
	shl.b32 	%r31, %r12, 1;
	add.s32 	%r32, %r320, %r31;
	mad.lo.s32 	%r33, %r12, 3, %r320;
	shl.b32 	%r34, %r12, 2;
	add.s32 	%r35, %r320, %r34;
	mad.lo.s32 	%r36, %r12, 5, %r320;
	mad.lo.s32 	%r37, %r12, 6, %r320;
	mad.lo.s32 	%r38, %r12, 7, %r320;
	mul.wide.s32 	%rd46, %r32, 8;
	add.s64 	%rd8, %rd46, %rd45;
	mul.wide.s32 	%rd47, %r33, 8;
	add.s64 	%rd9, %rd47, %rd45;
	mul.wide.s32 	%rd48, %r35, 8;
	add.s64 	%rd10, %rd48, %rd45;
	mul.wide.s32 	%rd49, %r36, 8;
	add.s64 	%rd11, %rd49, %rd45;
	mul.wide.s32 	%rd50, %r37, 8;
	add.s64 	%rd12, %rd50, %rd45;
	mul.wide.s32 	%rd51, %r38, 8;
	add.s64 	%rd13, %rd51, %rd45;
	mul.wide.s32 	%rd52, %r320, 2;
	add.s64 	%rd53, %rd52, %rd3;
	shl.b64 	%rd54, %rd53, 2;
	add.s64 	%rd55, %rd4, %rd54;
	add.s64 	%rd76, %rd55, 768;
	mul.wide.s32 	%rd56, %r320, 8;
	mul.wide.s32 	%rd57, %r12, 8;
	add.s64 	%rd58, %rd56, %rd57;
	add.s64 	%rd15, %rd58, %rd45;
	add.s64 	%rd16, %rd56, %rd45;

$L__BB23_7:
	ld.global.nc.v2.f32 	{%f101, %f102}, [%rd76+-768];
	add.s64 	%rd59, %rd77, %rd16;
	ld.global.nc.v2.f32 	{%f105, %f106}, [%rd59];
	fma.rn.f32 	%f109, %f101, %f105, %f389;
	fma.rn.f32 	%f110, %f102, %f106, %f109;
	add.s64 	%rd60, %rd77, %rd15;
	ld.global.nc.v2.f32 	{%f111, %f112}, [%rd60];
	fma.rn.f32 	%f115, %f101, %f111, %f388;
	fma.rn.f32 	%f116, %f102, %f112, %f115;
	add.s64 	%rd61, %rd77, %rd8;
	ld.global.nc.v2.f32 	{%f117, %f118}, [%rd61];
	fma.rn.f32 	%f121, %f101, %f117, %f387;
	fma.rn.f32 	%f122, %f102, %f118, %f121;
	add.s64 	%rd62, %rd77, %rd9;
	ld.global.nc.v2.f32 	{%f123, %f124}, [%rd62];
	fma.rn.f32 	%f127, %f101, %f123, %f386;
	fma.rn.f32 	%f128, %f102, %f124, %f127;
	add.s64 	%rd63, %rd77, %rd10;
	ld.global.nc.v2.f32 	{%f129, %f130}, [%rd63];
	fma.rn.f32 	%f133, %f101, %f129, %f385;
	fma.rn.f32 	%f134, %f102, %f130, %f133;
	add.s64 	%rd64, %rd77, %rd11;
	ld.global.nc.v2.f32 	{%f135, %f136}, [%rd64];
	fma.rn.f32 	%f139, %f101, %f135, %f384;
	fma.rn.f32 	%f140, %f102, %f136, %f139;
	add.s64 	%rd65, %rd77, %rd12;
	ld.global.nc.v2.f32 	{%f141, %f142}, [%rd65];
	fma.rn.f32 	%f145, %f101, %f141, %f383;
	fma.rn.f32 	%f146, %f102, %f142, %f145;
	add.s64 	%rd66, %rd77, %rd13;
	ld.global.nc.v2.f32 	{%f147, %f148}, [%rd66];
	fma.rn.f32 	%f151, %f101, %f147, %f382;
	fma.rn.f32 	%f152, %f102, %f148, %f151;
	ld.global.nc.v2.f32 	{%f153, %f154}, [%rd76];
	ld.global.nc.v2.f32 	{%f157, %f158}, [%rd59+768];
	fma.rn.f32 	%f161, %f153, %f157, %f110;
	fma.rn.f32 	%f389, %f154, %f158, %f161;
	add.s64 	%rd67, %rd77, %rd7;
	ld.global.nc.v2.f32 	{%f162, %f163}, [%rd67];
	fma.rn.f32 	%f166, %f153, %f162, %f116;
	fma.rn.f32 	%f388, %f154, %f163, %f166;
	ld.global.nc.v2.f32 	{%f167, %f168}, [%rd61+768];
	fma.rn.f32 	%f171, %f153, %f167, %f122;
	fma.rn.f32 	%f387, %f154, %f168, %f171;
	ld.global.nc.v2.f32 	{%f172, %f173}, [%rd62+768];
	fma.rn.f32 	%f176, %f153, %f172, %f128;
	fma.rn.f32 	%f386, %f154, %f173, %f176;
	ld.global.nc.v2.f32 	{%f177, %f178}, [%rd63+768];
	fma.rn.f32 	%f181, %f153, %f177, %f134;
	fma.rn.f32 	%f385, %f154, %f178, %f181;
	ld.global.nc.v2.f32 	{%f182, %f183}, [%rd64+768];
	fma.rn.f32 	%f186, %f153, %f182, %f140;
	fma.rn.f32 	%f384, %f154, %f183, %f186;
	ld.global.nc.v2.f32 	{%f187, %f188}, [%rd65+768];
	fma.rn.f32 	%f191, %f153, %f187, %f146;
	fma.rn.f32 	%f383, %f154, %f188, %f191;
	ld.global.nc.v2.f32 	{%f192, %f193}, [%rd66+768];
	fma.rn.f32 	%f196, %f153, %f192, %f152;
	fma.rn.f32 	%f382, %f154, %f193, %f196;
	add.s64 	%rd77, %rd77, 1536;
	add.s64 	%rd76, %rd76, 1536;
	add.s32 	%r320, %r320, 192;
	setp.lt.s32 	%p7, %r320, %r11;
	@%p7 bra 	$L__BB23_7;

	st.local.v4.f32 	[%rd2], {%f389, %f388, %f387, %f386};
	st.local.v4.f32 	[%rd2+16], {%f385, %f384, %f383, %f382};

$L__BB23_9:
	shr.s32 	%r39, %r3, 31;
	shr.u32 	%r40, %r39, 27;
	add.s32 	%r41, %r3, %r40;
	shr.s32 	%r42, %r41, 5;
	shl.b32 	%r43, %r42, 2;
	add.s32 	%r10, %r24, %r43;
	mov.u32 	%r45, 2;
	mov.b32 	%r46, %f389;
	mov.u32 	%r47, 31;
	mov.u32 	%r48, 16;
	mov.u32 	%r49, -1;
	shfl.sync.bfly.b32 	%r50|%p8, %r46, %r48, %r47, %r49;
	mov.b32 	%f197, %r50;
	add.f32 	%f198, %f389, %f197;
	mov.b32 	%r51, %f198;
	mov.u32 	%r52, 8;
	shfl.sync.bfly.b32 	%r53|%p9, %r51, %r52, %r47, %r49;
	mov.b32 	%f199, %r53;
	add.f32 	%f200, %f198, %f199;
	mov.b32 	%r54, %f200;
	mov.u32 	%r55, 4;
	shfl.sync.bfly.b32 	%r56|%p10, %r54, %r55, %r47, %r49;
	mov.b32 	%f201, %r56;
	add.f32 	%f202, %f200, %f201;
	mov.b32 	%r57, %f202;
	shfl.sync.bfly.b32 	%r58|%p11, %r57, %r45, %r47, %r49;
	mov.b32 	%f203, %r58;
	add.f32 	%f204, %f202, %f203;
	mov.b32 	%r59, %f204;
	mov.u32 	%r60, 1;
	shfl.sync.bfly.b32 	%r61|%p12, %r59, %r60, %r47, %r49;
	mov.b32 	%f205, %r61;
	add.f32 	%f206, %f204, %f205;
	st.local.f32 	[%rd2], %f206;
	st.shared.f32 	[%r10], %f206;
	bar.sync 	0;
	@%p1 bra 	$L__BB23_11;

	ld.shared.f32 	%f207, [%r4];
	mov.b32 	%r62, %f207;
	shfl.sync.bfly.b32 	%r66|%p14, %r62, %r48, %r47, %r49;
	mov.b32 	%f208, %r66;
	add.f32 	%f209, %f207, %f208;
	mov.b32 	%r67, %f209;
	shfl.sync.bfly.b32 	%r69|%p15, %r67, %r52, %r47, %r49;
	mov.b32 	%f210, %r69;
	add.f32 	%f211, %f209, %f210;
	mov.b32 	%r70, %f211;
	shfl.sync.bfly.b32 	%r72|%p16, %r70, %r55, %r47, %r49;
	mov.b32 	%f212, %r72;
	add.f32 	%f213, %f211, %f212;
	mov.b32 	%r73, %f213;
	shfl.sync.bfly.b32 	%r75|%p17, %r73, %r45, %r47, %r49;
	mov.b32 	%f214, %r75;
	add.f32 	%f215, %f213, %f214;
	mov.b32 	%r76, %f215;
	shfl.sync.bfly.b32 	%r78|%p18, %r76, %r60, %r47, %r49;
	mov.b32 	%f216, %r78;
	add.f32 	%f217, %f215, %f216;
	st.local.f32 	[%rd2], %f217;

$L__BB23_11:
	bar.sync 	0;
	mov.b32 	%r79, %f388;
	shfl.sync.bfly.b32 	%r83|%p20, %r79, %r48, %r47, %r49;
	mov.b32 	%f218, %r83;
	add.f32 	%f219, %f388, %f218;
	mov.b32 	%r84, %f219;
	shfl.sync.bfly.b32 	%r86|%p21, %r84, %r52, %r47, %r49;
	mov.b32 	%f220, %r86;
	add.f32 	%f221, %f219, %f220;
	mov.b32 	%r87, %f221;
	shfl.sync.bfly.b32 	%r89|%p22, %r87, %r55, %r47, %r49;
	mov.b32 	%f222, %r89;
	add.f32 	%f223, %f221, %f222;
	mov.b32 	%r90, %f223;
	shfl.sync.bfly.b32 	%r92|%p23, %r90, %r45, %r47, %r49;
	mov.b32 	%f224, %r92;
	add.f32 	%f225, %f223, %f224;
	mov.b32 	%r93, %f225;
	shfl.sync.bfly.b32 	%r95|%p24, %r93, %r60, %r47, %r49;
	mov.b32 	%f226, %r95;
	add.f32 	%f227, %f225, %f226;
	st.local.f32 	[%rd2+4], %f227;
	st.shared.f32 	[%r10], %f227;
	bar.sync 	0;
	@%p1 bra 	$L__BB23_13;

	ld.shared.f32 	%f228, [%r4];
	mov.b32 	%r96, %f228;
	mov.u32 	%r97, 31;
	mov.u32 	%r98, 16;
	mov.u32 	%r99, -1;
	shfl.sync.bfly.b32 	%r100|%p25, %r96, %r98, %r97, %r99;
	mov.b32 	%f229, %r100;
	add.f32 	%f230, %f228, %f229;
	mov.b32 	%r101, %f230;
	mov.u32 	%r102, 8;
	shfl.sync.bfly.b32 	%r103|%p26, %r101, %r102, %r97, %r99;
	mov.b32 	%f231, %r103;
	add.f32 	%f232, %f230, %f231;
	mov.b32 	%r104, %f232;
	mov.u32 	%r105, 4;
	shfl.sync.bfly.b32 	%r106|%p27, %r104, %r105, %r97, %r99;
	mov.b32 	%f233, %r106;
	add.f32 	%f234, %f232, %f233;
	mov.b32 	%r107, %f234;
	mov.u32 	%r108, 2;
	shfl.sync.bfly.b32 	%r109|%p28, %r107, %r108, %r97, %r99;
	mov.b32 	%f235, %r109;
	add.f32 	%f236, %f234, %f235;
	mov.b32 	%r110, %f236;
	mov.u32 	%r111, 1;
	shfl.sync.bfly.b32 	%r112|%p29, %r110, %r111, %r97, %r99;
	mov.b32 	%f237, %r112;
	add.f32 	%f238, %f236, %f237;
	st.local.f32 	[%rd2+4], %f238;

$L__BB23_13:
	bar.sync 	0;
	mov.b32 	%r113, %f387;
	mov.u32 	%r114, 31;
	mov.u32 	%r115, 16;
	mov.u32 	%r116, -1;
	shfl.sync.bfly.b32 	%r117|%p31, %r113, %r115, %r114, %r116;
	mov.b32 	%f239, %r117;
	add.f32 	%f240, %f387, %f239;
	mov.b32 	%r118, %f240;
	mov.u32 	%r119, 8;
	shfl.sync.bfly.b32 	%r120|%p32, %r118, %r119, %r114, %r116;
	mov.b32 	%f241, %r120;
	add.f32 	%f242, %f240, %f241;
	mov.b32 	%r121, %f242;
	mov.u32 	%r122, 4;
	shfl.sync.bfly.b32 	%r123|%p33, %r121, %r122, %r114, %r116;
	mov.b32 	%f243, %r123;
	add.f32 	%f244, %f242, %f243;
	mov.b32 	%r124, %f244;
	mov.u32 	%r125, 2;
	shfl.sync.bfly.b32 	%r126|%p34, %r124, %r125, %r114, %r116;
	mov.b32 	%f245, %r126;
	add.f32 	%f246, %f244, %f245;
	mov.b32 	%r127, %f246;
	mov.u32 	%r128, 1;
	shfl.sync.bfly.b32 	%r129|%p35, %r127, %r128, %r114, %r116;
	mov.b32 	%f247, %r129;
	add.f32 	%f248, %f246, %f247;
	st.local.f32 	[%rd2+8], %f248;
	st.shared.f32 	[%r10], %f248;
	bar.sync 	0;
	@%p1 bra 	$L__BB23_15;

	ld.shared.f32 	%f249, [%r4];
	mov.b32 	%r130, %f249;
	shfl.sync.bfly.b32 	%r134|%p36, %r130, %r115, %r114, %r116;
	mov.b32 	%f250, %r134;
	add.f32 	%f251, %f249, %f250;
	mov.b32 	%r135, %f251;
	shfl.sync.bfly.b32 	%r137|%p37, %r135, %r119, %r114, %r116;
	mov.b32 	%f252, %r137;
	add.f32 	%f253, %f251, %f252;
	mov.b32 	%r138, %f253;
	shfl.sync.bfly.b32 	%r140|%p38, %r138, %r122, %r114, %r116;
	mov.b32 	%f254, %r140;
	add.f32 	%f255, %f253, %f254;
	mov.b32 	%r141, %f255;
	shfl.sync.bfly.b32 	%r143|%p39, %r141, %r125, %r114, %r116;
	mov.b32 	%f256, %r143;
	add.f32 	%f257, %f255, %f256;
	mov.b32 	%r144, %f257;
	shfl.sync.bfly.b32 	%r146|%p40, %r144, %r128, %r114, %r116;
	mov.b32 	%f258, %r146;
	add.f32 	%f259, %f257, %f258;
	st.local.f32 	[%rd2+8], %f259;

$L__BB23_15:
	bar.sync 	0;
	mov.b32 	%r147, %f386;
	shfl.sync.bfly.b32 	%r151|%p42, %r147, %r115, %r114, %r116;
	mov.b32 	%f260, %r151;
	add.f32 	%f261, %f386, %f260;
	mov.b32 	%r152, %f261;
	shfl.sync.bfly.b32 	%r154|%p43, %r152, %r119, %r114, %r116;
	mov.b32 	%f262, %r154;
	add.f32 	%f263, %f261, %f262;
	mov.b32 	%r155, %f263;
	shfl.sync.bfly.b32 	%r157|%p44, %r155, %r122, %r114, %r116;
	mov.b32 	%f264, %r157;
	add.f32 	%f265, %f263, %f264;
	mov.b32 	%r158, %f265;
	shfl.sync.bfly.b32 	%r160|%p45, %r158, %r125, %r114, %r116;
	mov.b32 	%f266, %r160;
	add.f32 	%f267, %f265, %f266;
	mov.b32 	%r161, %f267;
	shfl.sync.bfly.b32 	%r163|%p46, %r161, %r128, %r114, %r116;
	mov.b32 	%f268, %r163;
	add.f32 	%f269, %f267, %f268;
	st.local.f32 	[%rd2+12], %f269;
	st.shared.f32 	[%r10], %f269;
	bar.sync 	0;
	@%p1 bra 	$L__BB23_17;

	ld.shared.f32 	%f270, [%r4];
	mov.b32 	%r164, %f270;
	mov.u32 	%r165, 31;
	mov.u32 	%r166, 16;
	mov.u32 	%r167, -1;
	shfl.sync.bfly.b32 	%r168|%p47, %r164, %r166, %r165, %r167;
	mov.b32 	%f271, %r168;
	add.f32 	%f272, %f270, %f271;
	mov.b32 	%r169, %f272;
	mov.u32 	%r170, 8;
	shfl.sync.bfly.b32 	%r171|%p48, %r169, %r170, %r165, %r167;
	mov.b32 	%f273, %r171;
	add.f32 	%f274, %f272, %f273;
	mov.b32 	%r172, %f274;
	mov.u32 	%r173, 4;
	shfl.sync.bfly.b32 	%r174|%p49, %r172, %r173, %r165, %r167;
	mov.b32 	%f275, %r174;
	add.f32 	%f276, %f274, %f275;
	mov.b32 	%r175, %f276;
	mov.u32 	%r176, 2;
	shfl.sync.bfly.b32 	%r177|%p50, %r175, %r176, %r165, %r167;
	mov.b32 	%f277, %r177;
	add.f32 	%f278, %f276, %f277;
	mov.b32 	%r178, %f278;
	mov.u32 	%r179, 1;
	shfl.sync.bfly.b32 	%r180|%p51, %r178, %r179, %r165, %r167;
	mov.b32 	%f279, %r180;
	add.f32 	%f280, %f278, %f279;
	st.local.f32 	[%rd2+12], %f280;

$L__BB23_17:
	bar.sync 	0;
	mov.b32 	%r181, %f385;
	mov.u32 	%r182, 31;
	mov.u32 	%r183, 16;
	mov.u32 	%r184, -1;
	shfl.sync.bfly.b32 	%r185|%p53, %r181, %r183, %r182, %r184;
	mov.b32 	%f281, %r185;
	add.f32 	%f282, %f385, %f281;
	mov.b32 	%r186, %f282;
	mov.u32 	%r187, 8;
	shfl.sync.bfly.b32 	%r188|%p54, %r186, %r187, %r182, %r184;
	mov.b32 	%f283, %r188;
	add.f32 	%f284, %f282, %f283;
	mov.b32 	%r189, %f284;
	mov.u32 	%r190, 4;
	shfl.sync.bfly.b32 	%r191|%p55, %r189, %r190, %r182, %r184;
	mov.b32 	%f285, %r191;
	add.f32 	%f286, %f284, %f285;
	mov.b32 	%r192, %f286;
	mov.u32 	%r193, 2;
	shfl.sync.bfly.b32 	%r194|%p56, %r192, %r193, %r182, %r184;
	mov.b32 	%f287, %r194;
	add.f32 	%f288, %f286, %f287;
	mov.b32 	%r195, %f288;
	mov.u32 	%r196, 1;
	shfl.sync.bfly.b32 	%r197|%p57, %r195, %r196, %r182, %r184;
	mov.b32 	%f289, %r197;
	add.f32 	%f290, %f288, %f289;
	st.local.f32 	[%rd2+16], %f290;
	st.shared.f32 	[%r10], %f290;
	bar.sync 	0;
	@%p1 bra 	$L__BB23_19;

	ld.shared.f32 	%f291, [%r4];
	mov.b32 	%r198, %f291;
	shfl.sync.bfly.b32 	%r202|%p58, %r198, %r183, %r182, %r184;
	mov.b32 	%f292, %r202;
	add.f32 	%f293, %f291, %f292;
	mov.b32 	%r203, %f293;
	shfl.sync.bfly.b32 	%r205|%p59, %r203, %r187, %r182, %r184;
	mov.b32 	%f294, %r205;
	add.f32 	%f295, %f293, %f294;
	mov.b32 	%r206, %f295;
	shfl.sync.bfly.b32 	%r208|%p60, %r206, %r190, %r182, %r184;
	mov.b32 	%f296, %r208;
	add.f32 	%f297, %f295, %f296;
	mov.b32 	%r209, %f297;
	shfl.sync.bfly.b32 	%r211|%p61, %r209, %r193, %r182, %r184;
	mov.b32 	%f298, %r211;
	add.f32 	%f299, %f297, %f298;
	mov.b32 	%r212, %f299;
	shfl.sync.bfly.b32 	%r214|%p62, %r212, %r196, %r182, %r184;
	mov.b32 	%f300, %r214;
	add.f32 	%f301, %f299, %f300;
	st.local.f32 	[%rd2+16], %f301;

$L__BB23_19:
	bar.sync 	0;
	mov.b32 	%r215, %f384;
	shfl.sync.bfly.b32 	%r219|%p64, %r215, %r183, %r182, %r184;
	mov.b32 	%f302, %r219;
	add.f32 	%f303, %f384, %f302;
	mov.b32 	%r220, %f303;
	shfl.sync.bfly.b32 	%r222|%p65, %r220, %r187, %r182, %r184;
	mov.b32 	%f304, %r222;
	add.f32 	%f305, %f303, %f304;
	mov.b32 	%r223, %f305;
	shfl.sync.bfly.b32 	%r225|%p66, %r223, %r190, %r182, %r184;
	mov.b32 	%f306, %r225;
	add.f32 	%f307, %f305, %f306;
	mov.b32 	%r226, %f307;
	shfl.sync.bfly.b32 	%r228|%p67, %r226, %r193, %r182, %r184;
	mov.b32 	%f308, %r228;
	add.f32 	%f309, %f307, %f308;
	mov.b32 	%r229, %f309;
	shfl.sync.bfly.b32 	%r231|%p68, %r229, %r196, %r182, %r184;
	mov.b32 	%f310, %r231;
	add.f32 	%f311, %f309, %f310;
	st.local.f32 	[%rd2+20], %f311;
	st.shared.f32 	[%r10], %f311;
	bar.sync 	0;
	@%p1 bra 	$L__BB23_21;

	ld.shared.f32 	%f312, [%r4];
	mov.b32 	%r232, %f312;
	mov.u32 	%r233, 31;
	mov.u32 	%r234, 16;
	mov.u32 	%r235, -1;
	shfl.sync.bfly.b32 	%r236|%p69, %r232, %r234, %r233, %r235;
	mov.b32 	%f313, %r236;
	add.f32 	%f314, %f312, %f313;
	mov.b32 	%r237, %f314;
	mov.u32 	%r238, 8;
	shfl.sync.bfly.b32 	%r239|%p70, %r237, %r238, %r233, %r235;
	mov.b32 	%f315, %r239;
	add.f32 	%f316, %f314, %f315;
	mov.b32 	%r240, %f316;
	mov.u32 	%r241, 4;
	shfl.sync.bfly.b32 	%r242|%p71, %r240, %r241, %r233, %r235;
	mov.b32 	%f317, %r242;
	add.f32 	%f318, %f316, %f317;
	mov.b32 	%r243, %f318;
	mov.u32 	%r244, 2;
	shfl.sync.bfly.b32 	%r245|%p72, %r243, %r244, %r233, %r235;
	mov.b32 	%f319, %r245;
	add.f32 	%f320, %f318, %f319;
	mov.b32 	%r246, %f320;
	mov.u32 	%r247, 1;
	shfl.sync.bfly.b32 	%r248|%p73, %r246, %r247, %r233, %r235;
	mov.b32 	%f321, %r248;
	add.f32 	%f322, %f320, %f321;
	st.local.f32 	[%rd2+20], %f322;

$L__BB23_21:
	bar.sync 	0;
	mov.b32 	%r249, %f383;
	mov.u32 	%r250, 31;
	mov.u32 	%r251, 16;
	mov.u32 	%r252, -1;
	shfl.sync.bfly.b32 	%r253|%p75, %r249, %r251, %r250, %r252;
	mov.b32 	%f323, %r253;
	add.f32 	%f324, %f383, %f323;
	mov.b32 	%r254, %f324;
	mov.u32 	%r255, 8;
	shfl.sync.bfly.b32 	%r256|%p76, %r254, %r255, %r250, %r252;
	mov.b32 	%f325, %r256;
	add.f32 	%f326, %f324, %f325;
	mov.b32 	%r257, %f326;
	mov.u32 	%r258, 4;
	shfl.sync.bfly.b32 	%r259|%p77, %r257, %r258, %r250, %r252;
	mov.b32 	%f327, %r259;
	add.f32 	%f328, %f326, %f327;
	mov.b32 	%r260, %f328;
	mov.u32 	%r261, 2;
	shfl.sync.bfly.b32 	%r262|%p78, %r260, %r261, %r250, %r252;
	mov.b32 	%f329, %r262;
	add.f32 	%f330, %f328, %f329;
	mov.b32 	%r263, %f330;
	mov.u32 	%r264, 1;
	shfl.sync.bfly.b32 	%r265|%p79, %r263, %r264, %r250, %r252;
	mov.b32 	%f331, %r265;
	add.f32 	%f332, %f330, %f331;
	st.local.f32 	[%rd2+24], %f332;
	st.shared.f32 	[%r10], %f332;
	bar.sync 	0;
	@%p1 bra 	$L__BB23_23;

	ld.shared.f32 	%f333, [%r4];
	mov.b32 	%r266, %f333;
	shfl.sync.bfly.b32 	%r270|%p80, %r266, %r251, %r250, %r252;
	mov.b32 	%f334, %r270;
	add.f32 	%f335, %f333, %f334;
	mov.b32 	%r271, %f335;
	shfl.sync.bfly.b32 	%r273|%p81, %r271, %r255, %r250, %r252;
	mov.b32 	%f336, %r273;
	add.f32 	%f337, %f335, %f336;
	mov.b32 	%r274, %f337;
	shfl.sync.bfly.b32 	%r276|%p82, %r274, %r258, %r250, %r252;
	mov.b32 	%f338, %r276;
	add.f32 	%f339, %f337, %f338;
	mov.b32 	%r277, %f339;
	shfl.sync.bfly.b32 	%r279|%p83, %r277, %r261, %r250, %r252;
	mov.b32 	%f340, %r279;
	add.f32 	%f341, %f339, %f340;
	mov.b32 	%r280, %f341;
	shfl.sync.bfly.b32 	%r282|%p84, %r280, %r264, %r250, %r252;
	mov.b32 	%f342, %r282;
	add.f32 	%f343, %f341, %f342;
	st.local.f32 	[%rd2+24], %f343;

$L__BB23_23:
	bar.sync 	0;
	mov.b32 	%r283, %f382;
	shfl.sync.bfly.b32 	%r287|%p86, %r283, %r251, %r250, %r252;
	mov.b32 	%f344, %r287;
	add.f32 	%f345, %f382, %f344;
	mov.b32 	%r288, %f345;
	shfl.sync.bfly.b32 	%r290|%p87, %r288, %r255, %r250, %r252;
	mov.b32 	%f346, %r290;
	add.f32 	%f347, %f345, %f346;
	mov.b32 	%r291, %f347;
	shfl.sync.bfly.b32 	%r293|%p88, %r291, %r258, %r250, %r252;
	mov.b32 	%f348, %r293;
	add.f32 	%f349, %f347, %f348;
	mov.b32 	%r294, %f349;
	shfl.sync.bfly.b32 	%r296|%p89, %r294, %r261, %r250, %r252;
	mov.b32 	%f350, %r296;
	add.f32 	%f351, %f349, %f350;
	mov.b32 	%r297, %f351;
	shfl.sync.bfly.b32 	%r299|%p90, %r297, %r264, %r250, %r252;
	mov.b32 	%f352, %r299;
	add.f32 	%f353, %f351, %f352;
	st.local.f32 	[%rd2+28], %f353;
	st.shared.f32 	[%r10], %f353;
	bar.sync 	0;
	@%p1 bra 	$L__BB23_25;

	ld.shared.f32 	%f354, [%r4];
	mov.b32 	%r300, %f354;
	mov.u32 	%r301, 31;
	mov.u32 	%r302, 16;
	mov.u32 	%r303, -1;
	shfl.sync.bfly.b32 	%r304|%p91, %r300, %r302, %r301, %r303;
	mov.b32 	%f355, %r304;
	add.f32 	%f356, %f354, %f355;
	mov.b32 	%r305, %f356;
	mov.u32 	%r306, 8;
	shfl.sync.bfly.b32 	%r307|%p92, %r305, %r306, %r301, %r303;
	mov.b32 	%f357, %r307;
	add.f32 	%f358, %f356, %f357;
	mov.b32 	%r308, %f358;
	mov.u32 	%r309, 4;
	shfl.sync.bfly.b32 	%r310|%p93, %r308, %r309, %r301, %r303;
	mov.b32 	%f359, %r310;
	add.f32 	%f360, %f358, %f359;
	mov.b32 	%r311, %f360;
	mov.u32 	%r312, 2;
	shfl.sync.bfly.b32 	%r313|%p94, %r311, %r312, %r301, %r303;
	mov.b32 	%f361, %r313;
	add.f32 	%f362, %f360, %f361;
	mov.b32 	%r314, %f362;
	mov.u32 	%r315, 1;
	shfl.sync.bfly.b32 	%r316|%p95, %r314, %r315, %r301, %r303;
	mov.b32 	%f363, %r316;
	add.f32 	%f364, %f362, %f363;
	st.local.f32 	[%rd2+28], %f364;

$L__BB23_25:
	bar.sync 	0;
	setp.gt.s32 	%p96, %r3, 7;
	@%p96 bra 	$L__BB23_27;

	mul.wide.s32 	%rd68, %r3, 4;
	add.s64 	%rd69, %rd2, %rd68;
	ld.local.f32 	%f365, [%rd69];
	mad.lo.s32 	%r317, %r3, %r13, %r2;
	cvt.s64.s32 	%rd70, %r317;
	mul.lo.s32 	%r318, %r1, %r14;
	cvt.s64.s32 	%rd71, %r318;
	add.s64 	%rd72, %rd71, %rd70;
	cvta.to.global.u64 	%rd73, %rd21;
	shl.b64 	%rd74, %rd72, 2;
	add.s64 	%rd75, %rd73, %rd74;
	st.global.f32 	[%rd75], %f365;

$L__BB23_27:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_1_bs_128
.visible .entry ggml_matvec_f32_ncols_1_bs_128(
	.param .u64 ggml_matvec_f32_ncols_1_bs_128_param_0,
	.param .u64 ggml_matvec_f32_ncols_1_bs_128_param_1,
	.param .u64 ggml_matvec_f32_ncols_1_bs_128_param_2,
	.param .u32 ggml_matvec_f32_ncols_1_bs_128_param_3,
	.param .u32 ggml_matvec_f32_ncols_1_bs_128_param_4,
	.param .u32 ggml_matvec_f32_ncols_1_bs_128_param_5,
	.param .u32 ggml_matvec_f32_ncols_1_bs_128_param_6,
	.param .u32 ggml_matvec_f32_ncols_1_bs_128_param_7,
	.param .u32 ggml_matvec_f32_ncols_1_bs_128_param_8,
	.param .u32 ggml_matvec_f32_ncols_1_bs_128_param_9,
	.param .u32 ggml_matvec_f32_ncols_1_bs_128_param_10,
	.param .u32 ggml_matvec_f32_ncols_1_bs_128_param_11
)
{
	.reg .pred 	%p<19>;
	.reg .f32 	%f<88>;
	.reg .b32 	%r<79>;
	.reg .b64 	%rd<42>;


	ld.param.u64 	%rd18, [ggml_matvec_f32_ncols_1_bs_128_param_0];
	ld.param.u64 	%rd19, [ggml_matvec_f32_ncols_1_bs_128_param_1];
	ld.param.u64 	%rd17, [ggml_matvec_f32_ncols_1_bs_128_param_2];
	ld.param.u32 	%r13, [ggml_matvec_f32_ncols_1_bs_128_param_3];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_1_bs_128_param_5];
	ld.param.u32 	%r14, [ggml_matvec_f32_ncols_1_bs_128_param_7];
	ld.param.u32 	%r17, [ggml_matvec_f32_ncols_1_bs_128_param_8];
	ld.param.u32 	%r18, [ggml_matvec_f32_ncols_1_bs_128_param_9];
	ld.param.u32 	%r19, [ggml_matvec_f32_ncols_1_bs_128_param_10];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_1_bs_128_param_11];
	cvta.to.global.u64 	%rd1, %rd19;
	cvta.to.global.u64 	%rd2, %rd18;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r20, %r1, %r17;
	mov.u32 	%r21, %ctaid.x;
	mul.lo.s32 	%r22, %r21, %r16;
	mad.lo.s32 	%r23, %r20, %r18, %r22;
	cvt.s64.s32 	%rd3, %r23;
	mul.lo.s32 	%r24, %r1, %r19;
	cvt.s64.s32 	%rd4, %r24;
	mov.u32 	%r2, %tid.x;
	setp.gt.s32 	%p1, %r2, 31;
	shl.b32 	%r25, %r2, 2;
	mov.u32 	%r26, data_mmv;
	add.s32 	%r3, %r26, %r25;
	@%p1 bra 	$L__BB24_2;

	mov.u32 	%r27, 0;
	st.shared.u32 	[%r3], %r27;

$L__BB24_2:
	bar.sync 	0;
	setp.ge.s32 	%p2, %r2, %r13;
	mov.f32 	%f86, 0f00000000;
	@%p2 bra 	$L__BB24_9;

	not.b32 	%r28, %r2;
	add.s32 	%r4, %r28, %r13;
	shr.u32 	%r29, %r4, 7;
	add.s32 	%r30, %r29, 1;
	and.b32  	%r76, %r30, 3;
	setp.eq.s32 	%p3, %r76, 0;
	mov.f32 	%f86, 0f00000000;
	mov.u32 	%r77, %r2;
	@%p3 bra 	$L__BB24_6;

	mul.wide.s32 	%rd20, %r2, 2;
	add.s64 	%rd21, %rd20, %rd4;
	shl.b64 	%rd22, %rd21, 2;
	add.s64 	%rd39, %rd1, %rd22;
	add.s64 	%rd23, %rd20, %rd3;
	shl.b64 	%rd24, %rd23, 2;
	add.s64 	%rd38, %rd2, %rd24;
	mov.f32 	%f86, 0f00000000;
	mov.u32 	%r77, %r2;

$L__BB24_5:
	.pragma "nounroll";
	ld.global.nc.v2.f32 	{%f15, %f16}, [%rd38];
	ld.global.nc.v2.f32 	{%f19, %f20}, [%rd39];
	fma.rn.f32 	%f23, %f15, %f19, %f86;
	fma.rn.f32 	%f86, %f16, %f20, %f23;
	add.s32 	%r77, %r77, 128;
	add.s64 	%rd39, %rd39, 1024;
	add.s64 	%rd38, %rd38, 1024;
	add.s32 	%r76, %r76, -1;
	setp.ne.s32 	%p4, %r76, 0;
	@%p4 bra 	$L__BB24_5;

$L__BB24_6:
	setp.lt.u32 	%p5, %r4, 384;
	@%p5 bra 	$L__BB24_9;

	mul.wide.s32 	%rd25, %r77, 2;
	add.s64 	%rd26, %rd25, %rd3;
	shl.b64 	%rd27, %rd26, 2;
	add.s64 	%rd28, %rd2, %rd27;
	add.s64 	%rd41, %rd28, 2048;
	add.s64 	%rd29, %rd25, %rd4;
	shl.b64 	%rd30, %rd29, 2;
	add.s64 	%rd31, %rd1, %rd30;
	add.s64 	%rd40, %rd31, 2048;

$L__BB24_8:
	ld.global.nc.v2.f32 	{%f24, %f25}, [%rd41+-2048];
	ld.global.nc.v2.f32 	{%f28, %f29}, [%rd40+-2048];
	fma.rn.f32 	%f32, %f24, %f28, %f86;
	fma.rn.f32 	%f33, %f25, %f29, %f32;
	ld.global.nc.v2.f32 	{%f34, %f35}, [%rd41+-1024];
	ld.global.nc.v2.f32 	{%f38, %f39}, [%rd40+-1024];
	fma.rn.f32 	%f42, %f34, %f38, %f33;
	fma.rn.f32 	%f43, %f35, %f39, %f42;
	ld.global.nc.v2.f32 	{%f44, %f45}, [%rd41];
	ld.global.nc.v2.f32 	{%f48, %f49}, [%rd40];
	fma.rn.f32 	%f52, %f44, %f48, %f43;
	fma.rn.f32 	%f53, %f45, %f49, %f52;
	ld.global.nc.v2.f32 	{%f54, %f55}, [%rd41+1024];
	ld.global.nc.v2.f32 	{%f58, %f59}, [%rd40+1024];
	fma.rn.f32 	%f62, %f54, %f58, %f53;
	fma.rn.f32 	%f86, %f55, %f59, %f62;
	add.s64 	%rd41, %rd41, 4096;
	add.s64 	%rd40, %rd40, 4096;
	add.s32 	%r77, %r77, 512;
	setp.lt.s32 	%p6, %r77, %r13;
	@%p6 bra 	$L__BB24_8;

$L__BB24_9:
	mov.b32 	%r31, %f86;
	mov.u32 	%r32, 31;
	mov.u32 	%r33, 16;
	mov.u32 	%r34, -1;
	shfl.sync.bfly.b32 	%r35|%p7, %r31, %r33, %r32, %r34;
	mov.b32 	%f63, %r35;
	add.f32 	%f64, %f86, %f63;
	mov.b32 	%r36, %f64;
	mov.u32 	%r37, 8;
	shfl.sync.bfly.b32 	%r38|%p8, %r36, %r37, %r32, %r34;
	mov.b32 	%f65, %r38;
	add.f32 	%f66, %f64, %f65;
	mov.b32 	%r39, %f66;
	mov.u32 	%r40, 4;
	shfl.sync.bfly.b32 	%r41|%p9, %r39, %r40, %r32, %r34;
	mov.b32 	%f67, %r41;
	add.f32 	%f68, %f66, %f67;
	mov.b32 	%r42, %f68;
	mov.u32 	%r43, 2;
	shfl.sync.bfly.b32 	%r44|%p10, %r42, %r43, %r32, %r34;
	mov.b32 	%f69, %r44;
	add.f32 	%f70, %f68, %f69;
	mov.b32 	%r45, %f70;
	mov.u32 	%r46, 1;
	shfl.sync.bfly.b32 	%r47|%p11, %r45, %r46, %r32, %r34;
	mov.b32 	%f71, %r47;
	add.f32 	%f87, %f70, %f71;
	shr.s32 	%r48, %r2, 31;
	shr.u32 	%r49, %r48, 27;
	add.s32 	%r50, %r2, %r49;
	shr.s32 	%r51, %r50, 5;
	shl.b32 	%r52, %r51, 2;
	add.s32 	%r54, %r26, %r52;
	st.shared.f32 	[%r54], %f87;
	bar.sync 	0;
	@%p1 bra 	$L__BB24_11;

	ld.shared.f32 	%f72, [%r3];
	mov.b32 	%r55, %f72;
	shfl.sync.bfly.b32 	%r59|%p13, %r55, %r33, %r32, %r34;
	mov.b32 	%f73, %r59;
	add.f32 	%f74, %f72, %f73;
	mov.b32 	%r60, %f74;
	shfl.sync.bfly.b32 	%r62|%p14, %r60, %r37, %r32, %r34;
	mov.b32 	%f75, %r62;
	add.f32 	%f76, %f74, %f75;
	mov.b32 	%r63, %f76;
	shfl.sync.bfly.b32 	%r65|%p15, %r63, %r40, %r32, %r34;
	mov.b32 	%f77, %r65;
	add.f32 	%f78, %f76, %f77;
	mov.b32 	%r66, %f78;
	shfl.sync.bfly.b32 	%r68|%p16, %r66, %r43, %r32, %r34;
	mov.b32 	%f79, %r68;
	add.f32 	%f80, %f78, %f79;
	mov.b32 	%r69, %f80;
	shfl.sync.bfly.b32 	%r71|%p17, %r69, %r46, %r32, %r34;
	mov.b32 	%f81, %r71;
	add.f32 	%f87, %f80, %f81;

$L__BB24_11:
	bar.sync 	0;
	setp.gt.s32 	%p18, %r2, 0;
	@%p18 bra 	$L__BB24_13;

	mad.lo.s32 	%r73, %r2, %r14, %r21;
	cvt.s64.s32 	%rd32, %r73;
	mul.lo.s32 	%r74, %r1, %r15;
	cvt.s64.s32 	%rd33, %r74;
	add.s64 	%rd34, %rd33, %rd32;
	cvta.to.global.u64 	%rd35, %rd17;
	shl.b64 	%rd36, %rd34, 2;
	add.s64 	%rd37, %rd35, %rd36;
	st.global.f32 	[%rd37], %f87;

$L__BB24_13:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_2_bs_128
.visible .entry ggml_matvec_f32_ncols_2_bs_128(
	.param .u64 ggml_matvec_f32_ncols_2_bs_128_param_0,
	.param .u64 ggml_matvec_f32_ncols_2_bs_128_param_1,
	.param .u64 ggml_matvec_f32_ncols_2_bs_128_param_2,
	.param .u32 ggml_matvec_f32_ncols_2_bs_128_param_3,
	.param .u32 ggml_matvec_f32_ncols_2_bs_128_param_4,
	.param .u32 ggml_matvec_f32_ncols_2_bs_128_param_5,
	.param .u32 ggml_matvec_f32_ncols_2_bs_128_param_6,
	.param .u32 ggml_matvec_f32_ncols_2_bs_128_param_7,
	.param .u32 ggml_matvec_f32_ncols_2_bs_128_param_8,
	.param .u32 ggml_matvec_f32_ncols_2_bs_128_param_9,
	.param .u32 ggml_matvec_f32_ncols_2_bs_128_param_10,
	.param .u32 ggml_matvec_f32_ncols_2_bs_128_param_11
)
{
	.local .align 8 .b8 	__local_depot25[8];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<30>;
	.reg .f32 	%f<146>;
	.reg .b32 	%r<113>;
	.reg .b64 	%rd<64>;


	mov.u64 	%SPL, __local_depot25;
	ld.param.u64 	%rd27, [ggml_matvec_f32_ncols_2_bs_128_param_0];
	ld.param.u64 	%rd28, [ggml_matvec_f32_ncols_2_bs_128_param_1];
	ld.param.u64 	%rd26, [ggml_matvec_f32_ncols_2_bs_128_param_2];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_2_bs_128_param_3];
	ld.param.u32 	%r19, [ggml_matvec_f32_ncols_2_bs_128_param_5];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_2_bs_128_param_6];
	ld.param.u32 	%r17, [ggml_matvec_f32_ncols_2_bs_128_param_7];
	ld.param.u32 	%r20, [ggml_matvec_f32_ncols_2_bs_128_param_8];
	ld.param.u32 	%r21, [ggml_matvec_f32_ncols_2_bs_128_param_9];
	ld.param.u32 	%r22, [ggml_matvec_f32_ncols_2_bs_128_param_10];
	ld.param.u32 	%r18, [ggml_matvec_f32_ncols_2_bs_128_param_11];
	cvta.to.global.u64 	%rd1, %rd28;
	cvta.to.global.u64 	%rd2, %rd27;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r23, %r1, %r20;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r24, %r2, %r19;
	mad.lo.s32 	%r25, %r23, %r21, %r24;
	cvt.s64.s32 	%rd4, %r25;
	mul.lo.s32 	%r26, %r1, %r22;
	cvt.s64.s32 	%rd5, %r26;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r27, %r3, 2;
	mov.u32 	%r28, data_mmv;
	add.s32 	%r4, %r28, %r27;
	@%p1 bra 	$L__BB25_2;

	mov.u32 	%r29, 0;
	st.shared.u32 	[%r4], %r29;

$L__BB25_2:
	bar.sync 	0;
	mov.f32 	%f144, 0f00000000;
	st.local.v2.f32 	[%rd3], {%f144, %f144};
	setp.ge.s32 	%p2, %r3, %r15;
	mov.f32 	%f145, %f144;
	@%p2 bra 	$L__BB25_11;

	not.b32 	%r30, %r3;
	add.s32 	%r5, %r30, %r15;
	shr.u32 	%r31, %r5, 7;
	add.s32 	%r32, %r31, 1;
	and.b32  	%r110, %r32, 3;
	setp.eq.s32 	%p3, %r110, 0;
	mov.f32 	%f144, 0f00000000;
	mov.u32 	%r111, %r3;
	@%p3 bra 	$L__BB25_7;

	mul.wide.s32 	%rd30, %r16, 2;
	mul.wide.s32 	%rd31, %r3, 2;
	add.s64 	%rd32, %rd30, %rd31;
	add.s64 	%rd33, %rd32, %rd5;
	shl.b64 	%rd34, %rd33, 2;
	add.s64 	%rd60, %rd1, %rd34;
	add.s64 	%rd35, %rd31, %rd5;
	shl.b64 	%rd36, %rd35, 2;
	add.s64 	%rd59, %rd1, %rd36;
	add.s64 	%rd37, %rd31, %rd4;
	shl.b64 	%rd38, %rd37, 2;
	add.s64 	%rd58, %rd2, %rd38;
	mov.f32 	%f144, 0f00000000;
	mov.f32 	%f145, %f144;
	mov.u32 	%r111, %r3;

$L__BB25_5:
	.pragma "nounroll";
	ld.global.nc.v2.f32 	{%f19, %f20}, [%rd58];
	ld.global.nc.v2.f32 	{%f23, %f24}, [%rd59];
	fma.rn.f32 	%f27, %f19, %f23, %f145;
	fma.rn.f32 	%f145, %f20, %f24, %f27;
	ld.global.nc.v2.f32 	{%f28, %f29}, [%rd60];
	fma.rn.f32 	%f32, %f19, %f28, %f144;
	fma.rn.f32 	%f144, %f20, %f29, %f32;
	add.s32 	%r111, %r111, 128;
	add.s64 	%rd60, %rd60, 1024;
	add.s64 	%rd59, %rd59, 1024;
	add.s64 	%rd58, %rd58, 1024;
	add.s32 	%r110, %r110, -1;
	setp.ne.s32 	%p4, %r110, 0;
	@%p4 bra 	$L__BB25_5;

	st.local.v2.f32 	[%rd3], {%f145, %f144};

$L__BB25_7:
	setp.lt.u32 	%p5, %r5, 384;
	@%p5 bra 	$L__BB25_11;

	mul.wide.s32 	%rd39, %r111, 2;
	add.s64 	%rd40, %rd39, %rd4;
	shl.b64 	%rd41, %rd40, 2;
	add.s64 	%rd42, %rd2, %rd41;
	add.s64 	%rd63, %rd42, 2048;
	add.s64 	%rd43, %rd39, %rd5;
	shl.b64 	%rd44, %rd43, 2;
	add.s64 	%rd45, %rd1, %rd44;
	add.s64 	%rd62, %rd45, 3072;
	mul.wide.s32 	%rd46, %r16, 2;
	add.s64 	%rd47, %rd43, %rd46;
	shl.b64 	%rd48, %rd47, 2;
	add.s64 	%rd49, %rd1, %rd48;
	add.s64 	%rd61, %rd49, 2048;

$L__BB25_9:
	ld.global.nc.v2.f32 	{%f33, %f34}, [%rd63+-2048];
	ld.global.nc.v2.f32 	{%f37, %f38}, [%rd62+-3072];
	fma.rn.f32 	%f41, %f33, %f37, %f145;
	fma.rn.f32 	%f42, %f34, %f38, %f41;
	ld.global.nc.v2.f32 	{%f43, %f44}, [%rd61+-2048];
	fma.rn.f32 	%f47, %f33, %f43, %f144;
	fma.rn.f32 	%f48, %f34, %f44, %f47;
	ld.global.nc.v2.f32 	{%f49, %f50}, [%rd63+-1024];
	ld.global.nc.v2.f32 	{%f53, %f54}, [%rd62+-2048];
	fma.rn.f32 	%f57, %f49, %f53, %f42;
	fma.rn.f32 	%f58, %f50, %f54, %f57;
	ld.global.nc.v2.f32 	{%f59, %f60}, [%rd61+-1024];
	fma.rn.f32 	%f63, %f49, %f59, %f48;
	fma.rn.f32 	%f64, %f50, %f60, %f63;
	ld.global.nc.v2.f32 	{%f65, %f66}, [%rd63];
	ld.global.nc.v2.f32 	{%f69, %f70}, [%rd62+-1024];
	fma.rn.f32 	%f73, %f65, %f69, %f58;
	fma.rn.f32 	%f74, %f66, %f70, %f73;
	ld.global.nc.v2.f32 	{%f75, %f76}, [%rd61];
	fma.rn.f32 	%f79, %f65, %f75, %f64;
	fma.rn.f32 	%f80, %f66, %f76, %f79;
	ld.global.nc.v2.f32 	{%f81, %f82}, [%rd63+1024];
	ld.global.nc.v2.f32 	{%f85, %f86}, [%rd62];
	fma.rn.f32 	%f89, %f81, %f85, %f74;
	fma.rn.f32 	%f145, %f82, %f86, %f89;
	ld.global.nc.v2.f32 	{%f90, %f91}, [%rd61+1024];
	fma.rn.f32 	%f94, %f81, %f90, %f80;
	fma.rn.f32 	%f144, %f82, %f91, %f94;
	add.s64 	%rd63, %rd63, 4096;
	add.s64 	%rd62, %rd62, 4096;
	add.s64 	%rd61, %rd61, 4096;
	add.s32 	%r111, %r111, 512;
	setp.lt.s32 	%p6, %r111, %r15;
	@%p6 bra 	$L__BB25_9;

	st.local.v2.f32 	[%rd3], {%f145, %f144};

$L__BB25_11:
	shr.s32 	%r33, %r3, 31;
	shr.u32 	%r34, %r33, 27;
	add.s32 	%r35, %r3, %r34;
	shr.s32 	%r36, %r35, 5;
	shl.b32 	%r37, %r36, 2;
	add.s32 	%r14, %r28, %r37;
	mov.u32 	%r39, 2;
	mov.b32 	%r40, %f145;
	mov.u32 	%r41, 31;
	mov.u32 	%r42, 16;
	mov.u32 	%r43, -1;
	shfl.sync.bfly.b32 	%r44|%p7, %r40, %r42, %r41, %r43;
	mov.b32 	%f95, %r44;
	add.f32 	%f96, %f145, %f95;
	mov.b32 	%r45, %f96;
	mov.u32 	%r46, 8;
	shfl.sync.bfly.b32 	%r47|%p8, %r45, %r46, %r41, %r43;
	mov.b32 	%f97, %r47;
	add.f32 	%f98, %f96, %f97;
	mov.b32 	%r48, %f98;
	mov.u32 	%r49, 4;
	shfl.sync.bfly.b32 	%r50|%p9, %r48, %r49, %r41, %r43;
	mov.b32 	%f99, %r50;
	add.f32 	%f100, %f98, %f99;
	mov.b32 	%r51, %f100;
	shfl.sync.bfly.b32 	%r52|%p10, %r51, %r39, %r41, %r43;
	mov.b32 	%f101, %r52;
	add.f32 	%f102, %f100, %f101;
	mov.b32 	%r53, %f102;
	mov.u32 	%r54, 1;
	shfl.sync.bfly.b32 	%r55|%p11, %r53, %r54, %r41, %r43;
	mov.b32 	%f103, %r55;
	add.f32 	%f104, %f102, %f103;
	st.local.f32 	[%rd3], %f104;
	st.shared.f32 	[%r14], %f104;
	bar.sync 	0;
	@%p1 bra 	$L__BB25_13;

	ld.shared.f32 	%f105, [%r4];
	mov.b32 	%r56, %f105;
	shfl.sync.bfly.b32 	%r60|%p13, %r56, %r42, %r41, %r43;
	mov.b32 	%f106, %r60;
	add.f32 	%f107, %f105, %f106;
	mov.b32 	%r61, %f107;
	shfl.sync.bfly.b32 	%r63|%p14, %r61, %r46, %r41, %r43;
	mov.b32 	%f108, %r63;
	add.f32 	%f109, %f107, %f108;
	mov.b32 	%r64, %f109;
	shfl.sync.bfly.b32 	%r66|%p15, %r64, %r49, %r41, %r43;
	mov.b32 	%f110, %r66;
	add.f32 	%f111, %f109, %f110;
	mov.b32 	%r67, %f111;
	shfl.sync.bfly.b32 	%r69|%p16, %r67, %r39, %r41, %r43;
	mov.b32 	%f112, %r69;
	add.f32 	%f113, %f111, %f112;
	mov.b32 	%r70, %f113;
	shfl.sync.bfly.b32 	%r72|%p17, %r70, %r54, %r41, %r43;
	mov.b32 	%f114, %r72;
	add.f32 	%f115, %f113, %f114;
	st.local.f32 	[%rd3], %f115;

$L__BB25_13:
	bar.sync 	0;
	mov.b32 	%r73, %f144;
	shfl.sync.bfly.b32 	%r77|%p19, %r73, %r42, %r41, %r43;
	mov.b32 	%f116, %r77;
	add.f32 	%f117, %f144, %f116;
	mov.b32 	%r78, %f117;
	shfl.sync.bfly.b32 	%r80|%p20, %r78, %r46, %r41, %r43;
	mov.b32 	%f118, %r80;
	add.f32 	%f119, %f117, %f118;
	mov.b32 	%r81, %f119;
	shfl.sync.bfly.b32 	%r83|%p21, %r81, %r49, %r41, %r43;
	mov.b32 	%f120, %r83;
	add.f32 	%f121, %f119, %f120;
	mov.b32 	%r84, %f121;
	shfl.sync.bfly.b32 	%r86|%p22, %r84, %r39, %r41, %r43;
	mov.b32 	%f122, %r86;
	add.f32 	%f123, %f121, %f122;
	mov.b32 	%r87, %f123;
	shfl.sync.bfly.b32 	%r89|%p23, %r87, %r54, %r41, %r43;
	mov.b32 	%f124, %r89;
	add.f32 	%f125, %f123, %f124;
	st.local.f32 	[%rd3+4], %f125;
	st.shared.f32 	[%r14], %f125;
	bar.sync 	0;
	@%p1 bra 	$L__BB25_15;

	ld.shared.f32 	%f126, [%r4];
	mov.b32 	%r90, %f126;
	mov.u32 	%r91, 31;
	mov.u32 	%r92, 16;
	mov.u32 	%r93, -1;
	shfl.sync.bfly.b32 	%r94|%p24, %r90, %r92, %r91, %r93;
	mov.b32 	%f127, %r94;
	add.f32 	%f128, %f126, %f127;
	mov.b32 	%r95, %f128;
	mov.u32 	%r96, 8;
	shfl.sync.bfly.b32 	%r97|%p25, %r95, %r96, %r91, %r93;
	mov.b32 	%f129, %r97;
	add.f32 	%f130, %f128, %f129;
	mov.b32 	%r98, %f130;
	mov.u32 	%r99, 4;
	shfl.sync.bfly.b32 	%r100|%p26, %r98, %r99, %r91, %r93;
	mov.b32 	%f131, %r100;
	add.f32 	%f132, %f130, %f131;
	mov.b32 	%r101, %f132;
	mov.u32 	%r102, 2;
	shfl.sync.bfly.b32 	%r103|%p27, %r101, %r102, %r91, %r93;
	mov.b32 	%f133, %r103;
	add.f32 	%f134, %f132, %f133;
	mov.b32 	%r104, %f134;
	mov.u32 	%r105, 1;
	shfl.sync.bfly.b32 	%r106|%p28, %r104, %r105, %r91, %r93;
	mov.b32 	%f135, %r106;
	add.f32 	%f136, %f134, %f135;
	st.local.f32 	[%rd3+4], %f136;

$L__BB25_15:
	bar.sync 	0;
	setp.gt.s32 	%p29, %r3, 1;
	@%p29 bra 	$L__BB25_17;

	mul.wide.s32 	%rd50, %r3, 4;
	add.s64 	%rd51, %rd3, %rd50;
	ld.local.f32 	%f137, [%rd51];
	mad.lo.s32 	%r107, %r3, %r17, %r2;
	cvt.s64.s32 	%rd52, %r107;
	mul.lo.s32 	%r108, %r1, %r18;
	cvt.s64.s32 	%rd53, %r108;
	add.s64 	%rd54, %rd53, %rd52;
	cvta.to.global.u64 	%rd55, %rd26;
	shl.b64 	%rd56, %rd54, 2;
	add.s64 	%rd57, %rd55, %rd56;
	st.global.f32 	[%rd57], %f137;

$L__BB25_17:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_3_bs_128
.visible .entry ggml_matvec_f32_ncols_3_bs_128(
	.param .u64 ggml_matvec_f32_ncols_3_bs_128_param_0,
	.param .u64 ggml_matvec_f32_ncols_3_bs_128_param_1,
	.param .u64 ggml_matvec_f32_ncols_3_bs_128_param_2,
	.param .u32 ggml_matvec_f32_ncols_3_bs_128_param_3,
	.param .u32 ggml_matvec_f32_ncols_3_bs_128_param_4,
	.param .u32 ggml_matvec_f32_ncols_3_bs_128_param_5,
	.param .u32 ggml_matvec_f32_ncols_3_bs_128_param_6,
	.param .u32 ggml_matvec_f32_ncols_3_bs_128_param_7,
	.param .u32 ggml_matvec_f32_ncols_3_bs_128_param_8,
	.param .u32 ggml_matvec_f32_ncols_3_bs_128_param_9,
	.param .u32 ggml_matvec_f32_ncols_3_bs_128_param_10,
	.param .u32 ggml_matvec_f32_ncols_3_bs_128_param_11
)
{
	.local .align 4 .b8 	__local_depot26[12];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<41>;
	.reg .f32 	%f<208>;
	.reg .b32 	%r<154>;
	.reg .b64 	%rd<72>;


	mov.u64 	%SPL, __local_depot26;
	ld.param.u64 	%rd29, [ggml_matvec_f32_ncols_3_bs_128_param_0];
	ld.param.u64 	%rd30, [ggml_matvec_f32_ncols_3_bs_128_param_1];
	ld.param.u64 	%rd28, [ggml_matvec_f32_ncols_3_bs_128_param_2];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_3_bs_128_param_3];
	ld.param.u32 	%r19, [ggml_matvec_f32_ncols_3_bs_128_param_5];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_3_bs_128_param_6];
	ld.param.u32 	%r17, [ggml_matvec_f32_ncols_3_bs_128_param_7];
	ld.param.u32 	%r20, [ggml_matvec_f32_ncols_3_bs_128_param_8];
	ld.param.u32 	%r21, [ggml_matvec_f32_ncols_3_bs_128_param_9];
	ld.param.u32 	%r22, [ggml_matvec_f32_ncols_3_bs_128_param_10];
	ld.param.u32 	%r18, [ggml_matvec_f32_ncols_3_bs_128_param_11];
	cvta.to.global.u64 	%rd71, %rd30;
	cvta.to.global.u64 	%rd2, %rd29;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r23, %r1, %r20;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r24, %r2, %r19;
	mad.lo.s32 	%r25, %r23, %r21, %r24;
	cvt.s64.s32 	%rd4, %r25;
	mul.lo.s32 	%r26, %r1, %r22;
	cvt.s64.s32 	%rd5, %r26;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r27, %r3, 2;
	mov.u32 	%r28, data_mmv;
	add.s32 	%r4, %r28, %r27;
	@%p1 bra 	$L__BB26_2;

	mov.u32 	%r29, 0;
	st.shared.u32 	[%r4], %r29;

$L__BB26_2:
	bar.sync 	0;
	mov.f32 	%f205, 0f00000000;
	mov.u32 	%r30, 0;
	st.local.u32 	[%rd3], %r30;
	st.local.u32 	[%rd3+4], %r30;
	st.local.u32 	[%rd3+8], %r30;
	setp.ge.s32 	%p2, %r3, %r15;
	mov.f32 	%f206, %f205;
	mov.f32 	%f207, %f205;
	@%p2 bra 	$L__BB26_11;

	not.b32 	%r31, %r3;
	add.s32 	%r5, %r31, %r15;
	shr.u32 	%r32, %r5, 7;
	add.s32 	%r33, %r32, 1;
	and.b32  	%r151, %r33, 3;
	setp.eq.s32 	%p3, %r151, 0;
	mov.f32 	%f205, 0f00000000;
	mov.u32 	%r152, %r3;
	@%p3 bra 	$L__BB26_7;

	shl.b32 	%r34, %r16, 1;
	add.s32 	%r35, %r3, %r34;
	mul.wide.s32 	%rd32, %r35, 2;
	add.s64 	%rd33, %rd32, %rd5;
	shl.b64 	%rd34, %rd33, 2;
	add.s64 	%rd69, %rd71, %rd34;
	mul.wide.s32 	%rd35, %r16, 2;
	mul.wide.s32 	%rd36, %r3, 2;
	add.s64 	%rd37, %rd35, %rd36;
	add.s64 	%rd38, %rd37, %rd5;
	shl.b64 	%rd39, %rd38, 2;
	add.s64 	%rd68, %rd71, %rd39;
	add.s64 	%rd40, %rd36, %rd5;
	shl.b64 	%rd41, %rd40, 2;
	add.s64 	%rd67, %rd71, %rd41;
	add.s64 	%rd42, %rd36, %rd4;
	shl.b64 	%rd43, %rd42, 2;
	add.s64 	%rd66, %rd2, %rd43;
	mov.f32 	%f205, 0f00000000;
	mov.f32 	%f206, %f205;
	mov.f32 	%f207, %f205;
	mov.u32 	%r152, %r3;

$L__BB26_5:
	.pragma "nounroll";
	ld.global.nc.v2.f32 	{%f28, %f29}, [%rd66];
	ld.global.nc.v2.f32 	{%f32, %f33}, [%rd67];
	fma.rn.f32 	%f36, %f28, %f32, %f207;
	fma.rn.f32 	%f207, %f29, %f33, %f36;
	ld.global.nc.v2.f32 	{%f37, %f38}, [%rd68];
	fma.rn.f32 	%f41, %f28, %f37, %f206;
	fma.rn.f32 	%f206, %f29, %f38, %f41;
	ld.global.nc.v2.f32 	{%f42, %f43}, [%rd69];
	fma.rn.f32 	%f46, %f28, %f42, %f205;
	fma.rn.f32 	%f205, %f29, %f43, %f46;
	add.s32 	%r152, %r152, 128;
	add.s64 	%rd69, %rd69, 1024;
	add.s64 	%rd68, %rd68, 1024;
	add.s64 	%rd67, %rd67, 1024;
	add.s64 	%rd66, %rd66, 1024;
	add.s32 	%r151, %r151, -1;
	setp.ne.s32 	%p4, %r151, 0;
	@%p4 bra 	$L__BB26_5;

	st.local.f32 	[%rd3], %f207;
	st.local.f32 	[%rd3+4], %f206;
	st.local.f32 	[%rd3+8], %f205;

$L__BB26_7:
	setp.lt.u32 	%p5, %r5, 384;
	@%p5 bra 	$L__BB26_11;

	add.s32 	%r36, %r152, %r16;
	shl.b32 	%r37, %r16, 1;
	add.s32 	%r38, %r152, %r37;
	add.s32 	%r39, %r36, 128;
	mul.wide.s32 	%rd44, %r39, 8;
	shl.b64 	%rd45, %rd5, 2;
	add.s64 	%rd19, %rd44, %rd45;
	mul.wide.s32 	%rd46, %r38, 8;
	add.s64 	%rd20, %rd46, %rd45;
	mul.wide.s32 	%rd47, %r152, 2;
	add.s64 	%rd48, %rd47, %rd4;
	shl.b64 	%rd49, %rd48, 2;
	add.s64 	%rd50, %rd2, %rd49;
	add.s64 	%rd70, %rd50, 2048;
	mul.wide.s32 	%rd51, %r152, 8;
	add.s64 	%rd22, %rd51, %rd45;
	mul.wide.s32 	%rd52, %r16, 8;
	add.s64 	%rd53, %rd51, %rd52;
	add.s64 	%rd23, %rd53, %rd45;

$L__BB26_9:
	ld.global.nc.v2.f32 	{%f47, %f48}, [%rd70+-2048];
	add.s64 	%rd54, %rd71, %rd22;
	ld.global.nc.v2.f32 	{%f51, %f52}, [%rd54];
	fma.rn.f32 	%f55, %f47, %f51, %f207;
	fma.rn.f32 	%f56, %f48, %f52, %f55;
	add.s64 	%rd55, %rd71, %rd23;
	ld.global.nc.v2.f32 	{%f57, %f58}, [%rd55];
	fma.rn.f32 	%f61, %f47, %f57, %f206;
	fma.rn.f32 	%f62, %f48, %f58, %f61;
	add.s64 	%rd56, %rd71, %rd20;
	ld.global.nc.v2.f32 	{%f63, %f64}, [%rd56];
	fma.rn.f32 	%f67, %f47, %f63, %f205;
	fma.rn.f32 	%f68, %f48, %f64, %f67;
	ld.global.nc.v2.f32 	{%f69, %f70}, [%rd70+-1024];
	ld.global.nc.v2.f32 	{%f73, %f74}, [%rd54+1024];
	fma.rn.f32 	%f77, %f69, %f73, %f56;
	fma.rn.f32 	%f78, %f70, %f74, %f77;
	add.s64 	%rd57, %rd71, %rd19;
	ld.global.nc.v2.f32 	{%f79, %f80}, [%rd57];
	fma.rn.f32 	%f83, %f69, %f79, %f62;
	fma.rn.f32 	%f84, %f70, %f80, %f83;
	ld.global.nc.v2.f32 	{%f85, %f86}, [%rd56+1024];
	fma.rn.f32 	%f89, %f69, %f85, %f68;
	fma.rn.f32 	%f90, %f70, %f86, %f89;
	ld.global.nc.v2.f32 	{%f91, %f92}, [%rd70];
	ld.global.nc.v2.f32 	{%f95, %f96}, [%rd54+2048];
	fma.rn.f32 	%f99, %f91, %f95, %f78;
	fma.rn.f32 	%f100, %f92, %f96, %f99;
	ld.global.nc.v2.f32 	{%f101, %f102}, [%rd57+1024];
	fma.rn.f32 	%f105, %f91, %f101, %f84;
	fma.rn.f32 	%f106, %f92, %f102, %f105;
	ld.global.nc.v2.f32 	{%f107, %f108}, [%rd56+2048];
	fma.rn.f32 	%f111, %f91, %f107, %f90;
	fma.rn.f32 	%f112, %f92, %f108, %f111;
	ld.global.nc.v2.f32 	{%f113, %f114}, [%rd70+1024];
	ld.global.nc.v2.f32 	{%f117, %f118}, [%rd54+3072];
	fma.rn.f32 	%f121, %f113, %f117, %f100;
	fma.rn.f32 	%f207, %f114, %f118, %f121;
	ld.global.nc.v2.f32 	{%f122, %f123}, [%rd57+2048];
	fma.rn.f32 	%f126, %f113, %f122, %f106;
	fma.rn.f32 	%f206, %f114, %f123, %f126;
	ld.global.nc.v2.f32 	{%f127, %f128}, [%rd56+3072];
	fma.rn.f32 	%f131, %f113, %f127, %f112;
	fma.rn.f32 	%f205, %f114, %f128, %f131;
	add.s64 	%rd71, %rd71, 4096;
	add.s64 	%rd70, %rd70, 4096;
	add.s32 	%r152, %r152, 512;
	setp.lt.s32 	%p6, %r152, %r15;
	@%p6 bra 	$L__BB26_9;

	st.local.f32 	[%rd3], %f207;
	st.local.f32 	[%rd3+4], %f206;
	st.local.f32 	[%rd3+8], %f205;

$L__BB26_11:
	shr.s32 	%r40, %r3, 31;
	shr.u32 	%r41, %r40, 27;
	add.s32 	%r42, %r3, %r41;
	shr.s32 	%r43, %r42, 5;
	shl.b32 	%r44, %r43, 2;
	add.s32 	%r14, %r28, %r44;
	mov.u32 	%r46, 2;
	mov.b32 	%r47, %f207;
	mov.u32 	%r48, 31;
	mov.u32 	%r49, 16;
	mov.u32 	%r50, -1;
	shfl.sync.bfly.b32 	%r51|%p7, %r47, %r49, %r48, %r50;
	mov.b32 	%f132, %r51;
	add.f32 	%f133, %f207, %f132;
	mov.b32 	%r52, %f133;
	mov.u32 	%r53, 8;
	shfl.sync.bfly.b32 	%r54|%p8, %r52, %r53, %r48, %r50;
	mov.b32 	%f134, %r54;
	add.f32 	%f135, %f133, %f134;
	mov.b32 	%r55, %f135;
	mov.u32 	%r56, 4;
	shfl.sync.bfly.b32 	%r57|%p9, %r55, %r56, %r48, %r50;
	mov.b32 	%f136, %r57;
	add.f32 	%f137, %f135, %f136;
	mov.b32 	%r58, %f137;
	shfl.sync.bfly.b32 	%r59|%p10, %r58, %r46, %r48, %r50;
	mov.b32 	%f138, %r59;
	add.f32 	%f139, %f137, %f138;
	mov.b32 	%r60, %f139;
	mov.u32 	%r61, 1;
	shfl.sync.bfly.b32 	%r62|%p11, %r60, %r61, %r48, %r50;
	mov.b32 	%f140, %r62;
	add.f32 	%f141, %f139, %f140;
	st.local.f32 	[%rd3], %f141;
	st.shared.f32 	[%r14], %f141;
	bar.sync 	0;
	@%p1 bra 	$L__BB26_13;

	ld.shared.f32 	%f142, [%r4];
	mov.b32 	%r63, %f142;
	shfl.sync.bfly.b32 	%r67|%p13, %r63, %r49, %r48, %r50;
	mov.b32 	%f143, %r67;
	add.f32 	%f144, %f142, %f143;
	mov.b32 	%r68, %f144;
	shfl.sync.bfly.b32 	%r70|%p14, %r68, %r53, %r48, %r50;
	mov.b32 	%f145, %r70;
	add.f32 	%f146, %f144, %f145;
	mov.b32 	%r71, %f146;
	shfl.sync.bfly.b32 	%r73|%p15, %r71, %r56, %r48, %r50;
	mov.b32 	%f147, %r73;
	add.f32 	%f148, %f146, %f147;
	mov.b32 	%r74, %f148;
	shfl.sync.bfly.b32 	%r76|%p16, %r74, %r46, %r48, %r50;
	mov.b32 	%f149, %r76;
	add.f32 	%f150, %f148, %f149;
	mov.b32 	%r77, %f150;
	shfl.sync.bfly.b32 	%r79|%p17, %r77, %r61, %r48, %r50;
	mov.b32 	%f151, %r79;
	add.f32 	%f152, %f150, %f151;
	st.local.f32 	[%rd3], %f152;

$L__BB26_13:
	bar.sync 	0;
	mov.b32 	%r80, %f206;
	shfl.sync.bfly.b32 	%r84|%p19, %r80, %r49, %r48, %r50;
	mov.b32 	%f153, %r84;
	add.f32 	%f154, %f206, %f153;
	mov.b32 	%r85, %f154;
	shfl.sync.bfly.b32 	%r87|%p20, %r85, %r53, %r48, %r50;
	mov.b32 	%f155, %r87;
	add.f32 	%f156, %f154, %f155;
	mov.b32 	%r88, %f156;
	shfl.sync.bfly.b32 	%r90|%p21, %r88, %r56, %r48, %r50;
	mov.b32 	%f157, %r90;
	add.f32 	%f158, %f156, %f157;
	mov.b32 	%r91, %f158;
	shfl.sync.bfly.b32 	%r93|%p22, %r91, %r46, %r48, %r50;
	mov.b32 	%f159, %r93;
	add.f32 	%f160, %f158, %f159;
	mov.b32 	%r94, %f160;
	shfl.sync.bfly.b32 	%r96|%p23, %r94, %r61, %r48, %r50;
	mov.b32 	%f161, %r96;
	add.f32 	%f162, %f160, %f161;
	st.local.f32 	[%rd3+4], %f162;
	st.shared.f32 	[%r14], %f162;
	bar.sync 	0;
	@%p1 bra 	$L__BB26_15;

	ld.shared.f32 	%f163, [%r4];
	mov.b32 	%r97, %f163;
	mov.u32 	%r98, 31;
	mov.u32 	%r99, 16;
	mov.u32 	%r100, -1;
	shfl.sync.bfly.b32 	%r101|%p24, %r97, %r99, %r98, %r100;
	mov.b32 	%f164, %r101;
	add.f32 	%f165, %f163, %f164;
	mov.b32 	%r102, %f165;
	mov.u32 	%r103, 8;
	shfl.sync.bfly.b32 	%r104|%p25, %r102, %r103, %r98, %r100;
	mov.b32 	%f166, %r104;
	add.f32 	%f167, %f165, %f166;
	mov.b32 	%r105, %f167;
	mov.u32 	%r106, 4;
	shfl.sync.bfly.b32 	%r107|%p26, %r105, %r106, %r98, %r100;
	mov.b32 	%f168, %r107;
	add.f32 	%f169, %f167, %f168;
	mov.b32 	%r108, %f169;
	mov.u32 	%r109, 2;
	shfl.sync.bfly.b32 	%r110|%p27, %r108, %r109, %r98, %r100;
	mov.b32 	%f170, %r110;
	add.f32 	%f171, %f169, %f170;
	mov.b32 	%r111, %f171;
	mov.u32 	%r112, 1;
	shfl.sync.bfly.b32 	%r113|%p28, %r111, %r112, %r98, %r100;
	mov.b32 	%f172, %r113;
	add.f32 	%f173, %f171, %f172;
	st.local.f32 	[%rd3+4], %f173;

$L__BB26_15:
	bar.sync 	0;
	mov.b32 	%r114, %f205;
	mov.u32 	%r115, 31;
	mov.u32 	%r116, 16;
	mov.u32 	%r117, -1;
	shfl.sync.bfly.b32 	%r118|%p30, %r114, %r116, %r115, %r117;
	mov.b32 	%f174, %r118;
	add.f32 	%f175, %f205, %f174;
	mov.b32 	%r119, %f175;
	mov.u32 	%r120, 8;
	shfl.sync.bfly.b32 	%r121|%p31, %r119, %r120, %r115, %r117;
	mov.b32 	%f176, %r121;
	add.f32 	%f177, %f175, %f176;
	mov.b32 	%r122, %f177;
	mov.u32 	%r123, 4;
	shfl.sync.bfly.b32 	%r124|%p32, %r122, %r123, %r115, %r117;
	mov.b32 	%f178, %r124;
	add.f32 	%f179, %f177, %f178;
	mov.b32 	%r125, %f179;
	mov.u32 	%r126, 2;
	shfl.sync.bfly.b32 	%r127|%p33, %r125, %r126, %r115, %r117;
	mov.b32 	%f180, %r127;
	add.f32 	%f181, %f179, %f180;
	mov.b32 	%r128, %f181;
	mov.u32 	%r129, 1;
	shfl.sync.bfly.b32 	%r130|%p34, %r128, %r129, %r115, %r117;
	mov.b32 	%f182, %r130;
	add.f32 	%f183, %f181, %f182;
	st.local.f32 	[%rd3+8], %f183;
	st.shared.f32 	[%r14], %f183;
	bar.sync 	0;
	@%p1 bra 	$L__BB26_17;

	ld.shared.f32 	%f184, [%r4];
	mov.b32 	%r131, %f184;
	shfl.sync.bfly.b32 	%r135|%p35, %r131, %r116, %r115, %r117;
	mov.b32 	%f185, %r135;
	add.f32 	%f186, %f184, %f185;
	mov.b32 	%r136, %f186;
	shfl.sync.bfly.b32 	%r138|%p36, %r136, %r120, %r115, %r117;
	mov.b32 	%f187, %r138;
	add.f32 	%f188, %f186, %f187;
	mov.b32 	%r139, %f188;
	shfl.sync.bfly.b32 	%r141|%p37, %r139, %r123, %r115, %r117;
	mov.b32 	%f189, %r141;
	add.f32 	%f190, %f188, %f189;
	mov.b32 	%r142, %f190;
	shfl.sync.bfly.b32 	%r144|%p38, %r142, %r126, %r115, %r117;
	mov.b32 	%f191, %r144;
	add.f32 	%f192, %f190, %f191;
	mov.b32 	%r145, %f192;
	shfl.sync.bfly.b32 	%r147|%p39, %r145, %r129, %r115, %r117;
	mov.b32 	%f193, %r147;
	add.f32 	%f194, %f192, %f193;
	st.local.f32 	[%rd3+8], %f194;

$L__BB26_17:
	bar.sync 	0;
	setp.gt.s32 	%p40, %r3, 2;
	@%p40 bra 	$L__BB26_19;

	mul.wide.s32 	%rd58, %r3, 4;
	add.s64 	%rd59, %rd3, %rd58;
	ld.local.f32 	%f195, [%rd59];
	mad.lo.s32 	%r148, %r3, %r17, %r2;
	cvt.s64.s32 	%rd60, %r148;
	mul.lo.s32 	%r149, %r1, %r18;
	cvt.s64.s32 	%rd61, %r149;
	add.s64 	%rd62, %rd61, %rd60;
	cvta.to.global.u64 	%rd63, %rd28;
	shl.b64 	%rd64, %rd62, 2;
	add.s64 	%rd65, %rd63, %rd64;
	st.global.f32 	[%rd65], %f195;

$L__BB26_19:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_4_bs_128
.visible .entry ggml_matvec_f32_ncols_4_bs_128(
	.param .u64 ggml_matvec_f32_ncols_4_bs_128_param_0,
	.param .u64 ggml_matvec_f32_ncols_4_bs_128_param_1,
	.param .u64 ggml_matvec_f32_ncols_4_bs_128_param_2,
	.param .u32 ggml_matvec_f32_ncols_4_bs_128_param_3,
	.param .u32 ggml_matvec_f32_ncols_4_bs_128_param_4,
	.param .u32 ggml_matvec_f32_ncols_4_bs_128_param_5,
	.param .u32 ggml_matvec_f32_ncols_4_bs_128_param_6,
	.param .u32 ggml_matvec_f32_ncols_4_bs_128_param_7,
	.param .u32 ggml_matvec_f32_ncols_4_bs_128_param_8,
	.param .u32 ggml_matvec_f32_ncols_4_bs_128_param_9,
	.param .u32 ggml_matvec_f32_ncols_4_bs_128_param_10,
	.param .u32 ggml_matvec_f32_ncols_4_bs_128_param_11
)
{
	.local .align 16 .b8 	__local_depot27[16];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<52>;
	.reg .f32 	%f<270>;
	.reg .b32 	%r<189>;
	.reg .b64 	%rd<83>;


	mov.u64 	%SPL, __local_depot27;
	ld.param.u64 	%rd34, [ggml_matvec_f32_ncols_4_bs_128_param_0];
	ld.param.u64 	%rd35, [ggml_matvec_f32_ncols_4_bs_128_param_1];
	ld.param.u64 	%rd33, [ggml_matvec_f32_ncols_4_bs_128_param_2];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_4_bs_128_param_3];
	ld.param.u32 	%r19, [ggml_matvec_f32_ncols_4_bs_128_param_5];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_4_bs_128_param_6];
	ld.param.u32 	%r17, [ggml_matvec_f32_ncols_4_bs_128_param_7];
	ld.param.u32 	%r20, [ggml_matvec_f32_ncols_4_bs_128_param_8];
	ld.param.u32 	%r21, [ggml_matvec_f32_ncols_4_bs_128_param_9];
	ld.param.u32 	%r22, [ggml_matvec_f32_ncols_4_bs_128_param_10];
	ld.param.u32 	%r18, [ggml_matvec_f32_ncols_4_bs_128_param_11];
	cvta.to.global.u64 	%rd82, %rd35;
	cvta.to.global.u64 	%rd2, %rd34;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r23, %r1, %r20;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r24, %r2, %r19;
	mad.lo.s32 	%r25, %r23, %r21, %r24;
	cvt.s64.s32 	%rd4, %r25;
	mul.lo.s32 	%r26, %r1, %r22;
	cvt.s64.s32 	%rd5, %r26;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r27, %r3, 2;
	mov.u32 	%r28, data_mmv;
	add.s32 	%r4, %r28, %r27;
	@%p1 bra 	$L__BB27_2;

	mov.u32 	%r29, 0;
	st.shared.u32 	[%r4], %r29;

$L__BB27_2:
	bar.sync 	0;
	mov.f32 	%f266, 0f00000000;
	st.local.v4.f32 	[%rd3], {%f266, %f266, %f266, %f266};
	setp.ge.s32 	%p2, %r3, %r15;
	mov.f32 	%f267, %f266;
	mov.f32 	%f268, %f266;
	mov.f32 	%f269, %f266;
	@%p2 bra 	$L__BB27_11;

	not.b32 	%r30, %r3;
	add.s32 	%r5, %r30, %r15;
	shr.u32 	%r31, %r5, 7;
	add.s32 	%r32, %r31, 1;
	and.b32  	%r186, %r32, 3;
	setp.eq.s32 	%p3, %r186, 0;
	mov.f32 	%f266, 0f00000000;
	mov.u32 	%r187, %r3;
	@%p3 bra 	$L__BB27_7;

	shl.b32 	%r33, %r16, 1;
	add.s32 	%r34, %r3, %r33;
	mul.wide.s32 	%rd37, %r34, 2;
	add.s64 	%rd38, %rd37, %rd5;
	shl.b64 	%rd39, %rd38, 2;
	add.s64 	%rd80, %rd82, %rd39;
	mad.lo.s32 	%r35, %r16, 3, %r3;
	mul.wide.s32 	%rd40, %r35, 2;
	add.s64 	%rd41, %rd40, %rd5;
	shl.b64 	%rd42, %rd41, 2;
	add.s64 	%rd79, %rd82, %rd42;
	mul.wide.s32 	%rd43, %r16, 2;
	mul.wide.s32 	%rd44, %r3, 2;
	add.s64 	%rd45, %rd43, %rd44;
	add.s64 	%rd46, %rd45, %rd5;
	shl.b64 	%rd47, %rd46, 2;
	add.s64 	%rd78, %rd82, %rd47;
	add.s64 	%rd48, %rd44, %rd5;
	shl.b64 	%rd49, %rd48, 2;
	add.s64 	%rd77, %rd82, %rd49;
	add.s64 	%rd50, %rd44, %rd4;
	shl.b64 	%rd51, %rd50, 2;
	add.s64 	%rd76, %rd2, %rd51;
	mov.f32 	%f266, 0f00000000;
	mov.f32 	%f267, %f266;
	mov.f32 	%f268, %f266;
	mov.f32 	%f269, %f266;
	mov.u32 	%r187, %r3;

$L__BB27_5:
	.pragma "nounroll";
	ld.global.nc.v2.f32 	{%f37, %f38}, [%rd76];
	ld.global.nc.v2.f32 	{%f41, %f42}, [%rd77];
	fma.rn.f32 	%f45, %f37, %f41, %f269;
	fma.rn.f32 	%f269, %f38, %f42, %f45;
	ld.global.nc.v2.f32 	{%f46, %f47}, [%rd78];
	fma.rn.f32 	%f50, %f37, %f46, %f268;
	fma.rn.f32 	%f268, %f38, %f47, %f50;
	ld.global.nc.v2.f32 	{%f51, %f52}, [%rd80];
	fma.rn.f32 	%f55, %f37, %f51, %f267;
	fma.rn.f32 	%f267, %f38, %f52, %f55;
	ld.global.nc.v2.f32 	{%f56, %f57}, [%rd79];
	fma.rn.f32 	%f60, %f37, %f56, %f266;
	fma.rn.f32 	%f266, %f38, %f57, %f60;
	add.s32 	%r187, %r187, 128;
	add.s64 	%rd80, %rd80, 1024;
	add.s64 	%rd79, %rd79, 1024;
	add.s64 	%rd78, %rd78, 1024;
	add.s64 	%rd77, %rd77, 1024;
	add.s64 	%rd76, %rd76, 1024;
	add.s32 	%r186, %r186, -1;
	setp.ne.s32 	%p4, %r186, 0;
	@%p4 bra 	$L__BB27_5;

	st.local.v4.f32 	[%rd3], {%f269, %f268, %f267, %f266};

$L__BB27_7:
	setp.lt.u32 	%p5, %r5, 384;
	@%p5 bra 	$L__BB27_11;

	add.s32 	%r36, %r187, %r16;
	shl.b32 	%r37, %r16, 1;
	add.s32 	%r38, %r187, %r37;
	mad.lo.s32 	%r39, %r16, 3, %r187;
	add.s32 	%r40, %r36, 128;
	mul.wide.s32 	%rd52, %r40, 8;
	shl.b64 	%rd53, %rd5, 2;
	add.s64 	%rd23, %rd52, %rd53;
	mul.wide.s32 	%rd54, %r38, 8;
	add.s64 	%rd24, %rd54, %rd53;
	mul.wide.s32 	%rd55, %r39, 8;
	add.s64 	%rd25, %rd55, %rd53;
	mul.wide.s32 	%rd56, %r187, 2;
	add.s64 	%rd57, %rd56, %rd4;
	shl.b64 	%rd58, %rd57, 2;
	add.s64 	%rd59, %rd2, %rd58;
	add.s64 	%rd81, %rd59, 2048;
	mul.wide.s32 	%rd60, %r187, 8;
	add.s64 	%rd27, %rd60, %rd53;
	mul.wide.s32 	%rd61, %r16, 8;
	add.s64 	%rd62, %rd60, %rd61;
	add.s64 	%rd28, %rd62, %rd53;

$L__BB27_9:
	ld.global.nc.v2.f32 	{%f61, %f62}, [%rd81+-2048];
	add.s64 	%rd63, %rd82, %rd27;
	ld.global.nc.v2.f32 	{%f65, %f66}, [%rd63];
	fma.rn.f32 	%f69, %f61, %f65, %f269;
	fma.rn.f32 	%f70, %f62, %f66, %f69;
	add.s64 	%rd64, %rd82, %rd28;
	ld.global.nc.v2.f32 	{%f71, %f72}, [%rd64];
	fma.rn.f32 	%f75, %f61, %f71, %f268;
	fma.rn.f32 	%f76, %f62, %f72, %f75;
	add.s64 	%rd65, %rd82, %rd24;
	ld.global.nc.v2.f32 	{%f77, %f78}, [%rd65];
	fma.rn.f32 	%f81, %f61, %f77, %f267;
	fma.rn.f32 	%f82, %f62, %f78, %f81;
	add.s64 	%rd66, %rd82, %rd25;
	ld.global.nc.v2.f32 	{%f83, %f84}, [%rd66];
	fma.rn.f32 	%f87, %f61, %f83, %f266;
	fma.rn.f32 	%f88, %f62, %f84, %f87;
	ld.global.nc.v2.f32 	{%f89, %f90}, [%rd81+-1024];
	ld.global.nc.v2.f32 	{%f93, %f94}, [%rd63+1024];
	fma.rn.f32 	%f97, %f89, %f93, %f70;
	fma.rn.f32 	%f98, %f90, %f94, %f97;
	add.s64 	%rd67, %rd82, %rd23;
	ld.global.nc.v2.f32 	{%f99, %f100}, [%rd67];
	fma.rn.f32 	%f103, %f89, %f99, %f76;
	fma.rn.f32 	%f104, %f90, %f100, %f103;
	ld.global.nc.v2.f32 	{%f105, %f106}, [%rd65+1024];
	fma.rn.f32 	%f109, %f89, %f105, %f82;
	fma.rn.f32 	%f110, %f90, %f106, %f109;
	ld.global.nc.v2.f32 	{%f111, %f112}, [%rd66+1024];
	fma.rn.f32 	%f115, %f89, %f111, %f88;
	fma.rn.f32 	%f116, %f90, %f112, %f115;
	ld.global.nc.v2.f32 	{%f117, %f118}, [%rd81];
	ld.global.nc.v2.f32 	{%f121, %f122}, [%rd63+2048];
	fma.rn.f32 	%f125, %f117, %f121, %f98;
	fma.rn.f32 	%f126, %f118, %f122, %f125;
	ld.global.nc.v2.f32 	{%f127, %f128}, [%rd67+1024];
	fma.rn.f32 	%f131, %f117, %f127, %f104;
	fma.rn.f32 	%f132, %f118, %f128, %f131;
	ld.global.nc.v2.f32 	{%f133, %f134}, [%rd65+2048];
	fma.rn.f32 	%f137, %f117, %f133, %f110;
	fma.rn.f32 	%f138, %f118, %f134, %f137;
	ld.global.nc.v2.f32 	{%f139, %f140}, [%rd66+2048];
	fma.rn.f32 	%f143, %f117, %f139, %f116;
	fma.rn.f32 	%f144, %f118, %f140, %f143;
	ld.global.nc.v2.f32 	{%f145, %f146}, [%rd81+1024];
	ld.global.nc.v2.f32 	{%f149, %f150}, [%rd63+3072];
	fma.rn.f32 	%f153, %f145, %f149, %f126;
	fma.rn.f32 	%f269, %f146, %f150, %f153;
	ld.global.nc.v2.f32 	{%f154, %f155}, [%rd67+2048];
	fma.rn.f32 	%f158, %f145, %f154, %f132;
	fma.rn.f32 	%f268, %f146, %f155, %f158;
	ld.global.nc.v2.f32 	{%f159, %f160}, [%rd65+3072];
	fma.rn.f32 	%f163, %f145, %f159, %f138;
	fma.rn.f32 	%f267, %f146, %f160, %f163;
	ld.global.nc.v2.f32 	{%f164, %f165}, [%rd66+3072];
	fma.rn.f32 	%f168, %f145, %f164, %f144;
	fma.rn.f32 	%f266, %f146, %f165, %f168;
	add.s64 	%rd82, %rd82, 4096;
	add.s64 	%rd81, %rd81, 4096;
	add.s32 	%r187, %r187, 512;
	setp.lt.s32 	%p6, %r187, %r15;
	@%p6 bra 	$L__BB27_9;

	st.local.v4.f32 	[%rd3], {%f269, %f268, %f267, %f266};

$L__BB27_11:
	shr.s32 	%r41, %r3, 31;
	shr.u32 	%r42, %r41, 27;
	add.s32 	%r43, %r3, %r42;
	shr.s32 	%r44, %r43, 5;
	shl.b32 	%r45, %r44, 2;
	add.s32 	%r14, %r28, %r45;
	mov.u32 	%r47, 2;
	mov.b32 	%r48, %f269;
	mov.u32 	%r49, 31;
	mov.u32 	%r50, 16;
	mov.u32 	%r51, -1;
	shfl.sync.bfly.b32 	%r52|%p7, %r48, %r50, %r49, %r51;
	mov.b32 	%f169, %r52;
	add.f32 	%f170, %f269, %f169;
	mov.b32 	%r53, %f170;
	mov.u32 	%r54, 8;
	shfl.sync.bfly.b32 	%r55|%p8, %r53, %r54, %r49, %r51;
	mov.b32 	%f171, %r55;
	add.f32 	%f172, %f170, %f171;
	mov.b32 	%r56, %f172;
	mov.u32 	%r57, 4;
	shfl.sync.bfly.b32 	%r58|%p9, %r56, %r57, %r49, %r51;
	mov.b32 	%f173, %r58;
	add.f32 	%f174, %f172, %f173;
	mov.b32 	%r59, %f174;
	shfl.sync.bfly.b32 	%r60|%p10, %r59, %r47, %r49, %r51;
	mov.b32 	%f175, %r60;
	add.f32 	%f176, %f174, %f175;
	mov.b32 	%r61, %f176;
	mov.u32 	%r62, 1;
	shfl.sync.bfly.b32 	%r63|%p11, %r61, %r62, %r49, %r51;
	mov.b32 	%f177, %r63;
	add.f32 	%f178, %f176, %f177;
	st.local.f32 	[%rd3], %f178;
	st.shared.f32 	[%r14], %f178;
	bar.sync 	0;
	@%p1 bra 	$L__BB27_13;

	ld.shared.f32 	%f179, [%r4];
	mov.b32 	%r64, %f179;
	shfl.sync.bfly.b32 	%r68|%p13, %r64, %r50, %r49, %r51;
	mov.b32 	%f180, %r68;
	add.f32 	%f181, %f179, %f180;
	mov.b32 	%r69, %f181;
	shfl.sync.bfly.b32 	%r71|%p14, %r69, %r54, %r49, %r51;
	mov.b32 	%f182, %r71;
	add.f32 	%f183, %f181, %f182;
	mov.b32 	%r72, %f183;
	shfl.sync.bfly.b32 	%r74|%p15, %r72, %r57, %r49, %r51;
	mov.b32 	%f184, %r74;
	add.f32 	%f185, %f183, %f184;
	mov.b32 	%r75, %f185;
	shfl.sync.bfly.b32 	%r77|%p16, %r75, %r47, %r49, %r51;
	mov.b32 	%f186, %r77;
	add.f32 	%f187, %f185, %f186;
	mov.b32 	%r78, %f187;
	shfl.sync.bfly.b32 	%r80|%p17, %r78, %r62, %r49, %r51;
	mov.b32 	%f188, %r80;
	add.f32 	%f189, %f187, %f188;
	st.local.f32 	[%rd3], %f189;

$L__BB27_13:
	bar.sync 	0;
	mov.b32 	%r81, %f268;
	shfl.sync.bfly.b32 	%r85|%p19, %r81, %r50, %r49, %r51;
	mov.b32 	%f190, %r85;
	add.f32 	%f191, %f268, %f190;
	mov.b32 	%r86, %f191;
	shfl.sync.bfly.b32 	%r88|%p20, %r86, %r54, %r49, %r51;
	mov.b32 	%f192, %r88;
	add.f32 	%f193, %f191, %f192;
	mov.b32 	%r89, %f193;
	shfl.sync.bfly.b32 	%r91|%p21, %r89, %r57, %r49, %r51;
	mov.b32 	%f194, %r91;
	add.f32 	%f195, %f193, %f194;
	mov.b32 	%r92, %f195;
	shfl.sync.bfly.b32 	%r94|%p22, %r92, %r47, %r49, %r51;
	mov.b32 	%f196, %r94;
	add.f32 	%f197, %f195, %f196;
	mov.b32 	%r95, %f197;
	shfl.sync.bfly.b32 	%r97|%p23, %r95, %r62, %r49, %r51;
	mov.b32 	%f198, %r97;
	add.f32 	%f199, %f197, %f198;
	st.local.f32 	[%rd3+4], %f199;
	st.shared.f32 	[%r14], %f199;
	bar.sync 	0;
	@%p1 bra 	$L__BB27_15;

	ld.shared.f32 	%f200, [%r4];
	mov.b32 	%r98, %f200;
	mov.u32 	%r99, 31;
	mov.u32 	%r100, 16;
	mov.u32 	%r101, -1;
	shfl.sync.bfly.b32 	%r102|%p24, %r98, %r100, %r99, %r101;
	mov.b32 	%f201, %r102;
	add.f32 	%f202, %f200, %f201;
	mov.b32 	%r103, %f202;
	mov.u32 	%r104, 8;
	shfl.sync.bfly.b32 	%r105|%p25, %r103, %r104, %r99, %r101;
	mov.b32 	%f203, %r105;
	add.f32 	%f204, %f202, %f203;
	mov.b32 	%r106, %f204;
	mov.u32 	%r107, 4;
	shfl.sync.bfly.b32 	%r108|%p26, %r106, %r107, %r99, %r101;
	mov.b32 	%f205, %r108;
	add.f32 	%f206, %f204, %f205;
	mov.b32 	%r109, %f206;
	mov.u32 	%r110, 2;
	shfl.sync.bfly.b32 	%r111|%p27, %r109, %r110, %r99, %r101;
	mov.b32 	%f207, %r111;
	add.f32 	%f208, %f206, %f207;
	mov.b32 	%r112, %f208;
	mov.u32 	%r113, 1;
	shfl.sync.bfly.b32 	%r114|%p28, %r112, %r113, %r99, %r101;
	mov.b32 	%f209, %r114;
	add.f32 	%f210, %f208, %f209;
	st.local.f32 	[%rd3+4], %f210;

$L__BB27_15:
	bar.sync 	0;
	mov.b32 	%r115, %f267;
	mov.u32 	%r116, 31;
	mov.u32 	%r117, 16;
	mov.u32 	%r118, -1;
	shfl.sync.bfly.b32 	%r119|%p30, %r115, %r117, %r116, %r118;
	mov.b32 	%f211, %r119;
	add.f32 	%f212, %f267, %f211;
	mov.b32 	%r120, %f212;
	mov.u32 	%r121, 8;
	shfl.sync.bfly.b32 	%r122|%p31, %r120, %r121, %r116, %r118;
	mov.b32 	%f213, %r122;
	add.f32 	%f214, %f212, %f213;
	mov.b32 	%r123, %f214;
	mov.u32 	%r124, 4;
	shfl.sync.bfly.b32 	%r125|%p32, %r123, %r124, %r116, %r118;
	mov.b32 	%f215, %r125;
	add.f32 	%f216, %f214, %f215;
	mov.b32 	%r126, %f216;
	mov.u32 	%r127, 2;
	shfl.sync.bfly.b32 	%r128|%p33, %r126, %r127, %r116, %r118;
	mov.b32 	%f217, %r128;
	add.f32 	%f218, %f216, %f217;
	mov.b32 	%r129, %f218;
	mov.u32 	%r130, 1;
	shfl.sync.bfly.b32 	%r131|%p34, %r129, %r130, %r116, %r118;
	mov.b32 	%f219, %r131;
	add.f32 	%f220, %f218, %f219;
	st.local.f32 	[%rd3+8], %f220;
	st.shared.f32 	[%r14], %f220;
	bar.sync 	0;
	@%p1 bra 	$L__BB27_17;

	ld.shared.f32 	%f221, [%r4];
	mov.b32 	%r132, %f221;
	shfl.sync.bfly.b32 	%r136|%p35, %r132, %r117, %r116, %r118;
	mov.b32 	%f222, %r136;
	add.f32 	%f223, %f221, %f222;
	mov.b32 	%r137, %f223;
	shfl.sync.bfly.b32 	%r139|%p36, %r137, %r121, %r116, %r118;
	mov.b32 	%f224, %r139;
	add.f32 	%f225, %f223, %f224;
	mov.b32 	%r140, %f225;
	shfl.sync.bfly.b32 	%r142|%p37, %r140, %r124, %r116, %r118;
	mov.b32 	%f226, %r142;
	add.f32 	%f227, %f225, %f226;
	mov.b32 	%r143, %f227;
	shfl.sync.bfly.b32 	%r145|%p38, %r143, %r127, %r116, %r118;
	mov.b32 	%f228, %r145;
	add.f32 	%f229, %f227, %f228;
	mov.b32 	%r146, %f229;
	shfl.sync.bfly.b32 	%r148|%p39, %r146, %r130, %r116, %r118;
	mov.b32 	%f230, %r148;
	add.f32 	%f231, %f229, %f230;
	st.local.f32 	[%rd3+8], %f231;

$L__BB27_17:
	bar.sync 	0;
	mov.b32 	%r149, %f266;
	shfl.sync.bfly.b32 	%r153|%p41, %r149, %r117, %r116, %r118;
	mov.b32 	%f232, %r153;
	add.f32 	%f233, %f266, %f232;
	mov.b32 	%r154, %f233;
	shfl.sync.bfly.b32 	%r156|%p42, %r154, %r121, %r116, %r118;
	mov.b32 	%f234, %r156;
	add.f32 	%f235, %f233, %f234;
	mov.b32 	%r157, %f235;
	shfl.sync.bfly.b32 	%r159|%p43, %r157, %r124, %r116, %r118;
	mov.b32 	%f236, %r159;
	add.f32 	%f237, %f235, %f236;
	mov.b32 	%r160, %f237;
	shfl.sync.bfly.b32 	%r162|%p44, %r160, %r127, %r116, %r118;
	mov.b32 	%f238, %r162;
	add.f32 	%f239, %f237, %f238;
	mov.b32 	%r163, %f239;
	shfl.sync.bfly.b32 	%r165|%p45, %r163, %r130, %r116, %r118;
	mov.b32 	%f240, %r165;
	add.f32 	%f241, %f239, %f240;
	st.local.f32 	[%rd3+12], %f241;
	st.shared.f32 	[%r14], %f241;
	bar.sync 	0;
	@%p1 bra 	$L__BB27_19;

	ld.shared.f32 	%f242, [%r4];
	mov.b32 	%r166, %f242;
	mov.u32 	%r167, 31;
	mov.u32 	%r168, 16;
	mov.u32 	%r169, -1;
	shfl.sync.bfly.b32 	%r170|%p46, %r166, %r168, %r167, %r169;
	mov.b32 	%f243, %r170;
	add.f32 	%f244, %f242, %f243;
	mov.b32 	%r171, %f244;
	mov.u32 	%r172, 8;
	shfl.sync.bfly.b32 	%r173|%p47, %r171, %r172, %r167, %r169;
	mov.b32 	%f245, %r173;
	add.f32 	%f246, %f244, %f245;
	mov.b32 	%r174, %f246;
	mov.u32 	%r175, 4;
	shfl.sync.bfly.b32 	%r176|%p48, %r174, %r175, %r167, %r169;
	mov.b32 	%f247, %r176;
	add.f32 	%f248, %f246, %f247;
	mov.b32 	%r177, %f248;
	mov.u32 	%r178, 2;
	shfl.sync.bfly.b32 	%r179|%p49, %r177, %r178, %r167, %r169;
	mov.b32 	%f249, %r179;
	add.f32 	%f250, %f248, %f249;
	mov.b32 	%r180, %f250;
	mov.u32 	%r181, 1;
	shfl.sync.bfly.b32 	%r182|%p50, %r180, %r181, %r167, %r169;
	mov.b32 	%f251, %r182;
	add.f32 	%f252, %f250, %f251;
	st.local.f32 	[%rd3+12], %f252;

$L__BB27_19:
	bar.sync 	0;
	setp.gt.s32 	%p51, %r3, 3;
	@%p51 bra 	$L__BB27_21;

	mul.wide.s32 	%rd68, %r3, 4;
	add.s64 	%rd69, %rd3, %rd68;
	ld.local.f32 	%f253, [%rd69];
	mad.lo.s32 	%r183, %r3, %r17, %r2;
	cvt.s64.s32 	%rd70, %r183;
	mul.lo.s32 	%r184, %r1, %r18;
	cvt.s64.s32 	%rd71, %r184;
	add.s64 	%rd72, %rd71, %rd70;
	cvta.to.global.u64 	%rd73, %rd33;
	shl.b64 	%rd74, %rd72, 2;
	add.s64 	%rd75, %rd73, %rd74;
	st.global.f32 	[%rd75], %f253;

$L__BB27_21:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_5_bs_128
.visible .entry ggml_matvec_f32_ncols_5_bs_128(
	.param .u64 ggml_matvec_f32_ncols_5_bs_128_param_0,
	.param .u64 ggml_matvec_f32_ncols_5_bs_128_param_1,
	.param .u64 ggml_matvec_f32_ncols_5_bs_128_param_2,
	.param .u32 ggml_matvec_f32_ncols_5_bs_128_param_3,
	.param .u32 ggml_matvec_f32_ncols_5_bs_128_param_4,
	.param .u32 ggml_matvec_f32_ncols_5_bs_128_param_5,
	.param .u32 ggml_matvec_f32_ncols_5_bs_128_param_6,
	.param .u32 ggml_matvec_f32_ncols_5_bs_128_param_7,
	.param .u32 ggml_matvec_f32_ncols_5_bs_128_param_8,
	.param .u32 ggml_matvec_f32_ncols_5_bs_128_param_9,
	.param .u32 ggml_matvec_f32_ncols_5_bs_128_param_10,
	.param .u32 ggml_matvec_f32_ncols_5_bs_128_param_11
)
{
	.local .align 4 .b8 	__local_depot28[20];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<63>;
	.reg .f32 	%f<332>;
	.reg .b32 	%r<225>;
	.reg .b64 	%rd<74>;


	mov.u64 	%SPL, __local_depot28;
	ld.param.u64 	%rd28, [ggml_matvec_f32_ncols_5_bs_128_param_0];
	ld.param.u64 	%rd29, [ggml_matvec_f32_ncols_5_bs_128_param_1];
	ld.param.u64 	%rd27, [ggml_matvec_f32_ncols_5_bs_128_param_2];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_5_bs_128_param_3];
	ld.param.u32 	%r19, [ggml_matvec_f32_ncols_5_bs_128_param_5];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_5_bs_128_param_6];
	ld.param.u32 	%r17, [ggml_matvec_f32_ncols_5_bs_128_param_7];
	ld.param.u32 	%r20, [ggml_matvec_f32_ncols_5_bs_128_param_8];
	ld.param.u32 	%r21, [ggml_matvec_f32_ncols_5_bs_128_param_9];
	ld.param.u32 	%r22, [ggml_matvec_f32_ncols_5_bs_128_param_10];
	ld.param.u32 	%r18, [ggml_matvec_f32_ncols_5_bs_128_param_11];
	cvta.to.global.u64 	%rd73, %rd29;
	cvta.to.global.u64 	%rd2, %rd28;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r23, %r1, %r20;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r24, %r2, %r19;
	mad.lo.s32 	%r25, %r23, %r21, %r24;
	cvt.s64.s32 	%rd4, %r25;
	mul.lo.s32 	%r26, %r1, %r22;
	cvt.s64.s32 	%rd5, %r26;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r27, %r3, 2;
	mov.u32 	%r28, data_mmv;
	add.s32 	%r4, %r28, %r27;
	@%p1 bra 	$L__BB28_2;

	mov.u32 	%r29, 0;
	st.shared.u32 	[%r4], %r29;

$L__BB28_2:
	bar.sync 	0;
	mov.f32 	%f327, 0f00000000;
	mov.u32 	%r30, 0;
	st.local.u32 	[%rd3], %r30;
	st.local.u32 	[%rd3+4], %r30;
	st.local.u32 	[%rd3+8], %r30;
	st.local.u32 	[%rd3+12], %r30;
	st.local.u32 	[%rd3+16], %r30;
	setp.ge.s32 	%p2, %r3, %r15;
	mov.f32 	%f328, %f327;
	mov.f32 	%f329, %f327;
	mov.f32 	%f330, %f327;
	mov.f32 	%f331, %f327;
	@%p2 bra 	$L__BB28_11;

	not.b32 	%r31, %r3;
	add.s32 	%r5, %r31, %r15;
	shr.u32 	%r32, %r5, 7;
	add.s32 	%r33, %r32, 1;
	and.b32  	%r222, %r33, 3;
	setp.eq.s32 	%p3, %r222, 0;
	mov.f32 	%f327, 0f00000000;
	mov.u32 	%r223, %r3;
	@%p3 bra 	$L__BB28_7;

	shl.b32 	%r34, %r16, 1;
	mad.lo.s32 	%r35, %r16, 3, %r3;
	mul.wide.s32 	%rd31, %r35, 8;
	shl.b64 	%rd32, %rd5, 2;
	add.s64 	%rd7, %rd31, %rd32;
	mul.wide.s32 	%rd33, %r3, 8;
	mul.wide.s32 	%rd34, %r16, 8;
	add.s64 	%rd35, %rd33, %rd34;
	add.s64 	%rd8, %rd35, %rd32;
	add.s64 	%rd9, %rd33, %rd32;
	mul.wide.s32 	%rd36, %r3, 2;
	add.s64 	%rd37, %rd36, %rd4;
	shl.b64 	%rd38, %rd37, 2;
	add.s64 	%rd70, %rd2, %rd38;
	mul.wide.s32 	%rd11, %r34, 8;
	mov.f32 	%f327, 0f00000000;
	mov.u64 	%rd71, %rd73;
	mov.f32 	%f328, %f327;
	mov.f32 	%f329, %f327;
	mov.f32 	%f330, %f327;
	mov.f32 	%f331, %f327;
	mov.u32 	%r223, %r3;

$L__BB28_5:
	.pragma "nounroll";
	ld.global.nc.v2.f32 	{%f46, %f47}, [%rd70];
	add.s64 	%rd39, %rd71, %rd9;
	ld.global.nc.v2.f32 	{%f50, %f51}, [%rd39];
	fma.rn.f32 	%f54, %f46, %f50, %f331;
	fma.rn.f32 	%f331, %f47, %f51, %f54;
	add.s64 	%rd40, %rd71, %rd8;
	ld.global.nc.v2.f32 	{%f55, %f56}, [%rd40];
	fma.rn.f32 	%f59, %f46, %f55, %f330;
	fma.rn.f32 	%f330, %f47, %f56, %f59;
	add.s64 	%rd41, %rd39, %rd11;
	ld.global.nc.v2.f32 	{%f60, %f61}, [%rd41];
	fma.rn.f32 	%f64, %f46, %f60, %f329;
	fma.rn.f32 	%f329, %f47, %f61, %f64;
	add.s64 	%rd42, %rd71, %rd7;
	ld.global.nc.v2.f32 	{%f65, %f66}, [%rd42];
	fma.rn.f32 	%f69, %f46, %f65, %f328;
	fma.rn.f32 	%f328, %f47, %f66, %f69;
	add.s64 	%rd43, %rd41, %rd11;
	ld.global.nc.v2.f32 	{%f70, %f71}, [%rd43];
	fma.rn.f32 	%f74, %f46, %f70, %f327;
	fma.rn.f32 	%f327, %f47, %f71, %f74;
	add.s32 	%r223, %r223, 128;
	add.s64 	%rd71, %rd71, 1024;
	add.s64 	%rd70, %rd70, 1024;
	add.s32 	%r222, %r222, -1;
	setp.ne.s32 	%p4, %r222, 0;
	@%p4 bra 	$L__BB28_5;

	st.local.f32 	[%rd3], %f331;
	st.local.f32 	[%rd3+4], %f330;
	st.local.f32 	[%rd3+8], %f329;
	st.local.f32 	[%rd3+12], %f328;
	st.local.f32 	[%rd3+16], %f327;

$L__BB28_7:
	setp.lt.u32 	%p5, %r5, 384;
	@%p5 bra 	$L__BB28_11;

	add.s32 	%r36, %r223, %r16;
	shl.b32 	%r37, %r16, 1;
	add.s32 	%r38, %r223, %r37;
	mad.lo.s32 	%r39, %r16, 3, %r223;
	shl.b32 	%r40, %r16, 2;
	add.s32 	%r41, %r223, %r40;
	add.s32 	%r42, %r36, 128;
	mul.wide.s32 	%rd44, %r42, 8;
	shl.b64 	%rd45, %rd5, 2;
	add.s64 	%rd16, %rd44, %rd45;
	mul.wide.s32 	%rd46, %r38, 8;
	add.s64 	%rd17, %rd46, %rd45;
	mul.wide.s32 	%rd47, %r39, 8;
	add.s64 	%rd18, %rd47, %rd45;
	mul.wide.s32 	%rd48, %r41, 8;
	add.s64 	%rd19, %rd48, %rd45;
	mul.wide.s32 	%rd49, %r223, 2;
	add.s64 	%rd50, %rd49, %rd4;
	shl.b64 	%rd51, %rd50, 2;
	add.s64 	%rd52, %rd2, %rd51;
	add.s64 	%rd72, %rd52, 2048;
	mul.wide.s32 	%rd53, %r223, 8;
	add.s64 	%rd21, %rd53, %rd45;
	mul.wide.s32 	%rd54, %r16, 8;
	add.s64 	%rd55, %rd53, %rd54;
	add.s64 	%rd22, %rd55, %rd45;

$L__BB28_9:
	ld.global.nc.v2.f32 	{%f75, %f76}, [%rd72+-2048];
	add.s64 	%rd56, %rd73, %rd21;
	ld.global.nc.v2.f32 	{%f79, %f80}, [%rd56];
	fma.rn.f32 	%f83, %f75, %f79, %f331;
	fma.rn.f32 	%f84, %f76, %f80, %f83;
	add.s64 	%rd57, %rd73, %rd22;
	ld.global.nc.v2.f32 	{%f85, %f86}, [%rd57];
	fma.rn.f32 	%f89, %f75, %f85, %f330;
	fma.rn.f32 	%f90, %f76, %f86, %f89;
	add.s64 	%rd58, %rd73, %rd17;
	ld.global.nc.v2.f32 	{%f91, %f92}, [%rd58];
	fma.rn.f32 	%f95, %f75, %f91, %f329;
	fma.rn.f32 	%f96, %f76, %f92, %f95;
	add.s64 	%rd59, %rd73, %rd18;
	ld.global.nc.v2.f32 	{%f97, %f98}, [%rd59];
	fma.rn.f32 	%f101, %f75, %f97, %f328;
	fma.rn.f32 	%f102, %f76, %f98, %f101;
	add.s64 	%rd60, %rd73, %rd19;
	ld.global.nc.v2.f32 	{%f103, %f104}, [%rd60];
	fma.rn.f32 	%f107, %f75, %f103, %f327;
	fma.rn.f32 	%f108, %f76, %f104, %f107;
	ld.global.nc.v2.f32 	{%f109, %f110}, [%rd72+-1024];
	ld.global.nc.v2.f32 	{%f113, %f114}, [%rd56+1024];
	fma.rn.f32 	%f117, %f109, %f113, %f84;
	fma.rn.f32 	%f118, %f110, %f114, %f117;
	add.s64 	%rd61, %rd73, %rd16;
	ld.global.nc.v2.f32 	{%f119, %f120}, [%rd61];
	fma.rn.f32 	%f123, %f109, %f119, %f90;
	fma.rn.f32 	%f124, %f110, %f120, %f123;
	ld.global.nc.v2.f32 	{%f125, %f126}, [%rd58+1024];
	fma.rn.f32 	%f129, %f109, %f125, %f96;
	fma.rn.f32 	%f130, %f110, %f126, %f129;
	ld.global.nc.v2.f32 	{%f131, %f132}, [%rd59+1024];
	fma.rn.f32 	%f135, %f109, %f131, %f102;
	fma.rn.f32 	%f136, %f110, %f132, %f135;
	ld.global.nc.v2.f32 	{%f137, %f138}, [%rd60+1024];
	fma.rn.f32 	%f141, %f109, %f137, %f108;
	fma.rn.f32 	%f142, %f110, %f138, %f141;
	ld.global.nc.v2.f32 	{%f143, %f144}, [%rd72];
	ld.global.nc.v2.f32 	{%f147, %f148}, [%rd56+2048];
	fma.rn.f32 	%f151, %f143, %f147, %f118;
	fma.rn.f32 	%f152, %f144, %f148, %f151;
	ld.global.nc.v2.f32 	{%f153, %f154}, [%rd61+1024];
	fma.rn.f32 	%f157, %f143, %f153, %f124;
	fma.rn.f32 	%f158, %f144, %f154, %f157;
	ld.global.nc.v2.f32 	{%f159, %f160}, [%rd58+2048];
	fma.rn.f32 	%f163, %f143, %f159, %f130;
	fma.rn.f32 	%f164, %f144, %f160, %f163;
	ld.global.nc.v2.f32 	{%f165, %f166}, [%rd59+2048];
	fma.rn.f32 	%f169, %f143, %f165, %f136;
	fma.rn.f32 	%f170, %f144, %f166, %f169;
	ld.global.nc.v2.f32 	{%f171, %f172}, [%rd60+2048];
	fma.rn.f32 	%f175, %f143, %f171, %f142;
	fma.rn.f32 	%f176, %f144, %f172, %f175;
	ld.global.nc.v2.f32 	{%f177, %f178}, [%rd72+1024];
	ld.global.nc.v2.f32 	{%f181, %f182}, [%rd56+3072];
	fma.rn.f32 	%f185, %f177, %f181, %f152;
	fma.rn.f32 	%f331, %f178, %f182, %f185;
	ld.global.nc.v2.f32 	{%f186, %f187}, [%rd61+2048];
	fma.rn.f32 	%f190, %f177, %f186, %f158;
	fma.rn.f32 	%f330, %f178, %f187, %f190;
	ld.global.nc.v2.f32 	{%f191, %f192}, [%rd58+3072];
	fma.rn.f32 	%f195, %f177, %f191, %f164;
	fma.rn.f32 	%f329, %f178, %f192, %f195;
	ld.global.nc.v2.f32 	{%f196, %f197}, [%rd59+3072];
	fma.rn.f32 	%f200, %f177, %f196, %f170;
	fma.rn.f32 	%f328, %f178, %f197, %f200;
	ld.global.nc.v2.f32 	{%f201, %f202}, [%rd60+3072];
	fma.rn.f32 	%f205, %f177, %f201, %f176;
	fma.rn.f32 	%f327, %f178, %f202, %f205;
	add.s64 	%rd73, %rd73, 4096;
	add.s64 	%rd72, %rd72, 4096;
	add.s32 	%r223, %r223, 512;
	setp.lt.s32 	%p6, %r223, %r15;
	@%p6 bra 	$L__BB28_9;

	st.local.f32 	[%rd3], %f331;
	st.local.f32 	[%rd3+4], %f330;
	st.local.f32 	[%rd3+8], %f329;
	st.local.f32 	[%rd3+12], %f328;
	st.local.f32 	[%rd3+16], %f327;

$L__BB28_11:
	shr.s32 	%r43, %r3, 31;
	shr.u32 	%r44, %r43, 27;
	add.s32 	%r45, %r3, %r44;
	shr.s32 	%r46, %r45, 5;
	shl.b32 	%r47, %r46, 2;
	add.s32 	%r14, %r28, %r47;
	mov.u32 	%r49, 2;
	mov.b32 	%r50, %f331;
	mov.u32 	%r51, 31;
	mov.u32 	%r52, 16;
	mov.u32 	%r53, -1;
	shfl.sync.bfly.b32 	%r54|%p7, %r50, %r52, %r51, %r53;
	mov.b32 	%f206, %r54;
	add.f32 	%f207, %f331, %f206;
	mov.b32 	%r55, %f207;
	mov.u32 	%r56, 8;
	shfl.sync.bfly.b32 	%r57|%p8, %r55, %r56, %r51, %r53;
	mov.b32 	%f208, %r57;
	add.f32 	%f209, %f207, %f208;
	mov.b32 	%r58, %f209;
	mov.u32 	%r59, 4;
	shfl.sync.bfly.b32 	%r60|%p9, %r58, %r59, %r51, %r53;
	mov.b32 	%f210, %r60;
	add.f32 	%f211, %f209, %f210;
	mov.b32 	%r61, %f211;
	shfl.sync.bfly.b32 	%r62|%p10, %r61, %r49, %r51, %r53;
	mov.b32 	%f212, %r62;
	add.f32 	%f213, %f211, %f212;
	mov.b32 	%r63, %f213;
	mov.u32 	%r64, 1;
	shfl.sync.bfly.b32 	%r65|%p11, %r63, %r64, %r51, %r53;
	mov.b32 	%f214, %r65;
	add.f32 	%f215, %f213, %f214;
	st.local.f32 	[%rd3], %f215;
	st.shared.f32 	[%r14], %f215;
	bar.sync 	0;
	@%p1 bra 	$L__BB28_13;

	ld.shared.f32 	%f216, [%r4];
	mov.b32 	%r66, %f216;
	shfl.sync.bfly.b32 	%r70|%p13, %r66, %r52, %r51, %r53;
	mov.b32 	%f217, %r70;
	add.f32 	%f218, %f216, %f217;
	mov.b32 	%r71, %f218;
	shfl.sync.bfly.b32 	%r73|%p14, %r71, %r56, %r51, %r53;
	mov.b32 	%f219, %r73;
	add.f32 	%f220, %f218, %f219;
	mov.b32 	%r74, %f220;
	shfl.sync.bfly.b32 	%r76|%p15, %r74, %r59, %r51, %r53;
	mov.b32 	%f221, %r76;
	add.f32 	%f222, %f220, %f221;
	mov.b32 	%r77, %f222;
	shfl.sync.bfly.b32 	%r79|%p16, %r77, %r49, %r51, %r53;
	mov.b32 	%f223, %r79;
	add.f32 	%f224, %f222, %f223;
	mov.b32 	%r80, %f224;
	shfl.sync.bfly.b32 	%r82|%p17, %r80, %r64, %r51, %r53;
	mov.b32 	%f225, %r82;
	add.f32 	%f226, %f224, %f225;
	st.local.f32 	[%rd3], %f226;

$L__BB28_13:
	bar.sync 	0;
	mov.b32 	%r83, %f330;
	shfl.sync.bfly.b32 	%r87|%p19, %r83, %r52, %r51, %r53;
	mov.b32 	%f227, %r87;
	add.f32 	%f228, %f330, %f227;
	mov.b32 	%r88, %f228;
	shfl.sync.bfly.b32 	%r90|%p20, %r88, %r56, %r51, %r53;
	mov.b32 	%f229, %r90;
	add.f32 	%f230, %f228, %f229;
	mov.b32 	%r91, %f230;
	shfl.sync.bfly.b32 	%r93|%p21, %r91, %r59, %r51, %r53;
	mov.b32 	%f231, %r93;
	add.f32 	%f232, %f230, %f231;
	mov.b32 	%r94, %f232;
	shfl.sync.bfly.b32 	%r96|%p22, %r94, %r49, %r51, %r53;
	mov.b32 	%f233, %r96;
	add.f32 	%f234, %f232, %f233;
	mov.b32 	%r97, %f234;
	shfl.sync.bfly.b32 	%r99|%p23, %r97, %r64, %r51, %r53;
	mov.b32 	%f235, %r99;
	add.f32 	%f236, %f234, %f235;
	st.local.f32 	[%rd3+4], %f236;
	st.shared.f32 	[%r14], %f236;
	bar.sync 	0;
	@%p1 bra 	$L__BB28_15;

	ld.shared.f32 	%f237, [%r4];
	mov.b32 	%r100, %f237;
	mov.u32 	%r101, 31;
	mov.u32 	%r102, 16;
	mov.u32 	%r103, -1;
	shfl.sync.bfly.b32 	%r104|%p24, %r100, %r102, %r101, %r103;
	mov.b32 	%f238, %r104;
	add.f32 	%f239, %f237, %f238;
	mov.b32 	%r105, %f239;
	mov.u32 	%r106, 8;
	shfl.sync.bfly.b32 	%r107|%p25, %r105, %r106, %r101, %r103;
	mov.b32 	%f240, %r107;
	add.f32 	%f241, %f239, %f240;
	mov.b32 	%r108, %f241;
	mov.u32 	%r109, 4;
	shfl.sync.bfly.b32 	%r110|%p26, %r108, %r109, %r101, %r103;
	mov.b32 	%f242, %r110;
	add.f32 	%f243, %f241, %f242;
	mov.b32 	%r111, %f243;
	mov.u32 	%r112, 2;
	shfl.sync.bfly.b32 	%r113|%p27, %r111, %r112, %r101, %r103;
	mov.b32 	%f244, %r113;
	add.f32 	%f245, %f243, %f244;
	mov.b32 	%r114, %f245;
	mov.u32 	%r115, 1;
	shfl.sync.bfly.b32 	%r116|%p28, %r114, %r115, %r101, %r103;
	mov.b32 	%f246, %r116;
	add.f32 	%f247, %f245, %f246;
	st.local.f32 	[%rd3+4], %f247;

$L__BB28_15:
	bar.sync 	0;
	mov.b32 	%r117, %f329;
	mov.u32 	%r118, 31;
	mov.u32 	%r119, 16;
	mov.u32 	%r120, -1;
	shfl.sync.bfly.b32 	%r121|%p30, %r117, %r119, %r118, %r120;
	mov.b32 	%f248, %r121;
	add.f32 	%f249, %f329, %f248;
	mov.b32 	%r122, %f249;
	mov.u32 	%r123, 8;
	shfl.sync.bfly.b32 	%r124|%p31, %r122, %r123, %r118, %r120;
	mov.b32 	%f250, %r124;
	add.f32 	%f251, %f249, %f250;
	mov.b32 	%r125, %f251;
	mov.u32 	%r126, 4;
	shfl.sync.bfly.b32 	%r127|%p32, %r125, %r126, %r118, %r120;
	mov.b32 	%f252, %r127;
	add.f32 	%f253, %f251, %f252;
	mov.b32 	%r128, %f253;
	mov.u32 	%r129, 2;
	shfl.sync.bfly.b32 	%r130|%p33, %r128, %r129, %r118, %r120;
	mov.b32 	%f254, %r130;
	add.f32 	%f255, %f253, %f254;
	mov.b32 	%r131, %f255;
	mov.u32 	%r132, 1;
	shfl.sync.bfly.b32 	%r133|%p34, %r131, %r132, %r118, %r120;
	mov.b32 	%f256, %r133;
	add.f32 	%f257, %f255, %f256;
	st.local.f32 	[%rd3+8], %f257;
	st.shared.f32 	[%r14], %f257;
	bar.sync 	0;
	@%p1 bra 	$L__BB28_17;

	ld.shared.f32 	%f258, [%r4];
	mov.b32 	%r134, %f258;
	shfl.sync.bfly.b32 	%r138|%p35, %r134, %r119, %r118, %r120;
	mov.b32 	%f259, %r138;
	add.f32 	%f260, %f258, %f259;
	mov.b32 	%r139, %f260;
	shfl.sync.bfly.b32 	%r141|%p36, %r139, %r123, %r118, %r120;
	mov.b32 	%f261, %r141;
	add.f32 	%f262, %f260, %f261;
	mov.b32 	%r142, %f262;
	shfl.sync.bfly.b32 	%r144|%p37, %r142, %r126, %r118, %r120;
	mov.b32 	%f263, %r144;
	add.f32 	%f264, %f262, %f263;
	mov.b32 	%r145, %f264;
	shfl.sync.bfly.b32 	%r147|%p38, %r145, %r129, %r118, %r120;
	mov.b32 	%f265, %r147;
	add.f32 	%f266, %f264, %f265;
	mov.b32 	%r148, %f266;
	shfl.sync.bfly.b32 	%r150|%p39, %r148, %r132, %r118, %r120;
	mov.b32 	%f267, %r150;
	add.f32 	%f268, %f266, %f267;
	st.local.f32 	[%rd3+8], %f268;

$L__BB28_17:
	bar.sync 	0;
	mov.b32 	%r151, %f328;
	shfl.sync.bfly.b32 	%r155|%p41, %r151, %r119, %r118, %r120;
	mov.b32 	%f269, %r155;
	add.f32 	%f270, %f328, %f269;
	mov.b32 	%r156, %f270;
	shfl.sync.bfly.b32 	%r158|%p42, %r156, %r123, %r118, %r120;
	mov.b32 	%f271, %r158;
	add.f32 	%f272, %f270, %f271;
	mov.b32 	%r159, %f272;
	shfl.sync.bfly.b32 	%r161|%p43, %r159, %r126, %r118, %r120;
	mov.b32 	%f273, %r161;
	add.f32 	%f274, %f272, %f273;
	mov.b32 	%r162, %f274;
	shfl.sync.bfly.b32 	%r164|%p44, %r162, %r129, %r118, %r120;
	mov.b32 	%f275, %r164;
	add.f32 	%f276, %f274, %f275;
	mov.b32 	%r165, %f276;
	shfl.sync.bfly.b32 	%r167|%p45, %r165, %r132, %r118, %r120;
	mov.b32 	%f277, %r167;
	add.f32 	%f278, %f276, %f277;
	st.local.f32 	[%rd3+12], %f278;
	st.shared.f32 	[%r14], %f278;
	bar.sync 	0;
	@%p1 bra 	$L__BB28_19;

	ld.shared.f32 	%f279, [%r4];
	mov.b32 	%r168, %f279;
	mov.u32 	%r169, 31;
	mov.u32 	%r170, 16;
	mov.u32 	%r171, -1;
	shfl.sync.bfly.b32 	%r172|%p46, %r168, %r170, %r169, %r171;
	mov.b32 	%f280, %r172;
	add.f32 	%f281, %f279, %f280;
	mov.b32 	%r173, %f281;
	mov.u32 	%r174, 8;
	shfl.sync.bfly.b32 	%r175|%p47, %r173, %r174, %r169, %r171;
	mov.b32 	%f282, %r175;
	add.f32 	%f283, %f281, %f282;
	mov.b32 	%r176, %f283;
	mov.u32 	%r177, 4;
	shfl.sync.bfly.b32 	%r178|%p48, %r176, %r177, %r169, %r171;
	mov.b32 	%f284, %r178;
	add.f32 	%f285, %f283, %f284;
	mov.b32 	%r179, %f285;
	mov.u32 	%r180, 2;
	shfl.sync.bfly.b32 	%r181|%p49, %r179, %r180, %r169, %r171;
	mov.b32 	%f286, %r181;
	add.f32 	%f287, %f285, %f286;
	mov.b32 	%r182, %f287;
	mov.u32 	%r183, 1;
	shfl.sync.bfly.b32 	%r184|%p50, %r182, %r183, %r169, %r171;
	mov.b32 	%f288, %r184;
	add.f32 	%f289, %f287, %f288;
	st.local.f32 	[%rd3+12], %f289;

$L__BB28_19:
	bar.sync 	0;
	mov.b32 	%r185, %f327;
	mov.u32 	%r186, 31;
	mov.u32 	%r187, 16;
	mov.u32 	%r188, -1;
	shfl.sync.bfly.b32 	%r189|%p52, %r185, %r187, %r186, %r188;
	mov.b32 	%f290, %r189;
	add.f32 	%f291, %f327, %f290;
	mov.b32 	%r190, %f291;
	mov.u32 	%r191, 8;
	shfl.sync.bfly.b32 	%r192|%p53, %r190, %r191, %r186, %r188;
	mov.b32 	%f292, %r192;
	add.f32 	%f293, %f291, %f292;
	mov.b32 	%r193, %f293;
	mov.u32 	%r194, 4;
	shfl.sync.bfly.b32 	%r195|%p54, %r193, %r194, %r186, %r188;
	mov.b32 	%f294, %r195;
	add.f32 	%f295, %f293, %f294;
	mov.b32 	%r196, %f295;
	mov.u32 	%r197, 2;
	shfl.sync.bfly.b32 	%r198|%p55, %r196, %r197, %r186, %r188;
	mov.b32 	%f296, %r198;
	add.f32 	%f297, %f295, %f296;
	mov.b32 	%r199, %f297;
	mov.u32 	%r200, 1;
	shfl.sync.bfly.b32 	%r201|%p56, %r199, %r200, %r186, %r188;
	mov.b32 	%f298, %r201;
	add.f32 	%f299, %f297, %f298;
	st.local.f32 	[%rd3+16], %f299;
	st.shared.f32 	[%r14], %f299;
	bar.sync 	0;
	@%p1 bra 	$L__BB28_21;

	ld.shared.f32 	%f300, [%r4];
	mov.b32 	%r202, %f300;
	shfl.sync.bfly.b32 	%r206|%p57, %r202, %r187, %r186, %r188;
	mov.b32 	%f301, %r206;
	add.f32 	%f302, %f300, %f301;
	mov.b32 	%r207, %f302;
	shfl.sync.bfly.b32 	%r209|%p58, %r207, %r191, %r186, %r188;
	mov.b32 	%f303, %r209;
	add.f32 	%f304, %f302, %f303;
	mov.b32 	%r210, %f304;
	shfl.sync.bfly.b32 	%r212|%p59, %r210, %r194, %r186, %r188;
	mov.b32 	%f305, %r212;
	add.f32 	%f306, %f304, %f305;
	mov.b32 	%r213, %f306;
	shfl.sync.bfly.b32 	%r215|%p60, %r213, %r197, %r186, %r188;
	mov.b32 	%f307, %r215;
	add.f32 	%f308, %f306, %f307;
	mov.b32 	%r216, %f308;
	shfl.sync.bfly.b32 	%r218|%p61, %r216, %r200, %r186, %r188;
	mov.b32 	%f309, %r218;
	add.f32 	%f310, %f308, %f309;
	st.local.f32 	[%rd3+16], %f310;

$L__BB28_21:
	bar.sync 	0;
	setp.gt.s32 	%p62, %r3, 4;
	@%p62 bra 	$L__BB28_23;

	mul.wide.s32 	%rd62, %r3, 4;
	add.s64 	%rd63, %rd3, %rd62;
	ld.local.f32 	%f311, [%rd63];
	mad.lo.s32 	%r219, %r3, %r17, %r2;
	cvt.s64.s32 	%rd64, %r219;
	mul.lo.s32 	%r220, %r1, %r18;
	cvt.s64.s32 	%rd65, %r220;
	add.s64 	%rd66, %rd65, %rd64;
	cvta.to.global.u64 	%rd67, %rd27;
	shl.b64 	%rd68, %rd66, 2;
	add.s64 	%rd69, %rd67, %rd68;
	st.global.f32 	[%rd69], %f311;

$L__BB28_23:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_6_bs_128
.visible .entry ggml_matvec_f32_ncols_6_bs_128(
	.param .u64 ggml_matvec_f32_ncols_6_bs_128_param_0,
	.param .u64 ggml_matvec_f32_ncols_6_bs_128_param_1,
	.param .u64 ggml_matvec_f32_ncols_6_bs_128_param_2,
	.param .u32 ggml_matvec_f32_ncols_6_bs_128_param_3,
	.param .u32 ggml_matvec_f32_ncols_6_bs_128_param_4,
	.param .u32 ggml_matvec_f32_ncols_6_bs_128_param_5,
	.param .u32 ggml_matvec_f32_ncols_6_bs_128_param_6,
	.param .u32 ggml_matvec_f32_ncols_6_bs_128_param_7,
	.param .u32 ggml_matvec_f32_ncols_6_bs_128_param_8,
	.param .u32 ggml_matvec_f32_ncols_6_bs_128_param_9,
	.param .u32 ggml_matvec_f32_ncols_6_bs_128_param_10,
	.param .u32 ggml_matvec_f32_ncols_6_bs_128_param_11
)
{
	.local .align 8 .b8 	__local_depot29[24];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<73>;
	.reg .f32 	%f<296>;
	.reg .b32 	%r<253>;
	.reg .b64 	%rd<67>;


	mov.u64 	%SPL, __local_depot29;
	ld.param.u64 	%rd20, [ggml_matvec_f32_ncols_6_bs_128_param_0];
	ld.param.u64 	%rd21, [ggml_matvec_f32_ncols_6_bs_128_param_1];
	ld.param.u64 	%rd19, [ggml_matvec_f32_ncols_6_bs_128_param_2];
	ld.param.u32 	%r11, [ggml_matvec_f32_ncols_6_bs_128_param_3];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_6_bs_128_param_5];
	ld.param.u32 	%r12, [ggml_matvec_f32_ncols_6_bs_128_param_6];
	ld.param.u32 	%r13, [ggml_matvec_f32_ncols_6_bs_128_param_7];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_6_bs_128_param_8];
	ld.param.u32 	%r17, [ggml_matvec_f32_ncols_6_bs_128_param_9];
	ld.param.u32 	%r18, [ggml_matvec_f32_ncols_6_bs_128_param_10];
	ld.param.u32 	%r14, [ggml_matvec_f32_ncols_6_bs_128_param_11];
	cvta.to.global.u64 	%rd66, %rd21;
	add.u64 	%rd2, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r19, %r1, %r16;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r20, %r2, %r15;
	mad.lo.s32 	%r21, %r19, %r17, %r20;
	cvt.s64.s32 	%rd3, %r21;
	cvta.to.global.u64 	%rd4, %rd20;
	mul.lo.s32 	%r22, %r1, %r18;
	cvt.s64.s32 	%rd5, %r22;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r23, %r3, 2;
	mov.u32 	%r24, data_mmv;
	add.s32 	%r4, %r24, %r23;
	@%p1 bra 	$L__BB29_2;

	mov.u32 	%r25, 0;
	st.shared.u32 	[%r4], %r25;

$L__BB29_2:
	bar.sync 	0;
	mov.f32 	%f290, 0f00000000;
	st.local.v2.f32 	[%rd2], {%f290, %f290};
	st.local.v2.f32 	[%rd2+8], {%f290, %f290};
	st.local.v2.f32 	[%rd2+16], {%f290, %f290};
	setp.ge.s32 	%p2, %r3, %r11;
	mov.f32 	%f291, %f290;
	mov.f32 	%f292, %f290;
	mov.f32 	%f293, %f290;
	mov.f32 	%f294, %f290;
	mov.f32 	%f295, %f290;
	@%p2 bra 	$L__BB29_9;

	not.b32 	%r26, %r3;
	add.s32 	%r5, %r26, %r11;
	and.b32  	%r27, %r5, 128;
	setp.ne.s32 	%p3, %r27, 0;
	mov.f32 	%f290, 0f00000000;
	mov.u32 	%r252, %r3;
	@%p3 bra 	$L__BB29_5;

	shl.b64 	%rd23, %rd5, 2;
	add.s64 	%rd24, %rd66, %rd23;
	shl.b64 	%rd25, %rd3, 2;
	add.s64 	%rd26, %rd4, %rd25;
	mul.wide.s32 	%rd27, %r3, 8;
	add.s64 	%rd28, %rd26, %rd27;
	ld.global.nc.v2.f32 	{%f43, %f44}, [%rd28];
	add.s64 	%rd29, %rd24, %rd27;
	ld.global.nc.v2.f32 	{%f47, %f48}, [%rd29];
	fma.rn.f32 	%f51, %f43, %f47, 0f00000000;
	fma.rn.f32 	%f295, %f44, %f48, %f51;
	mul.wide.s32 	%rd30, %r12, 8;
	add.s64 	%rd31, %rd29, %rd30;
	ld.global.nc.v2.f32 	{%f52, %f53}, [%rd31];
	fma.rn.f32 	%f56, %f43, %f52, 0f00000000;
	fma.rn.f32 	%f294, %f44, %f53, %f56;
	st.local.v2.f32 	[%rd2], {%f295, %f294};
	add.s32 	%r28, %r3, %r12;
	add.s32 	%r29, %r28, %r12;
	mul.wide.s32 	%rd32, %r29, 8;
	add.s64 	%rd33, %rd24, %rd32;
	ld.global.nc.v2.f32 	{%f57, %f58}, [%rd33];
	fma.rn.f32 	%f61, %f43, %f57, 0f00000000;
	fma.rn.f32 	%f293, %f44, %f58, %f61;
	add.s64 	%rd34, %rd33, %rd30;
	ld.global.nc.v2.f32 	{%f62, %f63}, [%rd34];
	fma.rn.f32 	%f66, %f43, %f62, 0f00000000;
	fma.rn.f32 	%f292, %f44, %f63, %f66;
	st.local.v2.f32 	[%rd2+8], {%f293, %f292};
	add.s64 	%rd35, %rd34, %rd30;
	ld.global.nc.v2.f32 	{%f67, %f68}, [%rd35];
	fma.rn.f32 	%f71, %f43, %f67, 0f00000000;
	fma.rn.f32 	%f291, %f44, %f68, %f71;
	add.s64 	%rd36, %rd35, %rd30;
	ld.global.nc.v2.f32 	{%f72, %f73}, [%rd36];
	fma.rn.f32 	%f76, %f43, %f72, 0f00000000;
	fma.rn.f32 	%f290, %f44, %f73, %f76;
	st.local.v2.f32 	[%rd2+16], {%f291, %f290};
	add.s32 	%r252, %r3, 128;

$L__BB29_5:
	and.b32  	%r30, %r5, -128;
	setp.eq.s32 	%p4, %r30, 0;
	@%p4 bra 	$L__BB29_9;

	add.s32 	%r31, %r252, %r12;
	add.s32 	%r32, %r31, 128;
	mul.wide.s32 	%rd37, %r32, 8;
	shl.b64 	%rd38, %rd5, 2;
	add.s64 	%rd7, %rd37, %rd38;
	shl.b32 	%r33, %r12, 1;
	add.s32 	%r34, %r252, %r33;
	mad.lo.s32 	%r35, %r12, 3, %r252;
	shl.b32 	%r36, %r12, 2;
	add.s32 	%r37, %r252, %r36;
	mad.lo.s32 	%r38, %r12, 5, %r252;
	mul.wide.s32 	%rd39, %r34, 8;
	add.s64 	%rd8, %rd39, %rd38;
	mul.wide.s32 	%rd40, %r35, 8;
	add.s64 	%rd9, %rd40, %rd38;
	mul.wide.s32 	%rd41, %r37, 8;
	add.s64 	%rd10, %rd41, %rd38;
	mul.wide.s32 	%rd42, %r38, 8;
	add.s64 	%rd11, %rd42, %rd38;
	mul.wide.s32 	%rd43, %r252, 2;
	add.s64 	%rd44, %rd43, %rd3;
	shl.b64 	%rd45, %rd44, 2;
	add.s64 	%rd46, %rd4, %rd45;
	add.s64 	%rd65, %rd46, 1024;
	mul.wide.s32 	%rd47, %r252, 8;
	mul.wide.s32 	%rd48, %r12, 8;
	add.s64 	%rd49, %rd47, %rd48;
	add.s64 	%rd13, %rd49, %rd38;
	add.s64 	%rd14, %rd47, %rd38;

$L__BB29_7:
	ld.global.nc.v2.f32 	{%f77, %f78}, [%rd65+-1024];
	add.s64 	%rd50, %rd66, %rd14;
	ld.global.nc.v2.f32 	{%f81, %f82}, [%rd50];
	fma.rn.f32 	%f85, %f77, %f81, %f295;
	fma.rn.f32 	%f86, %f78, %f82, %f85;
	add.s64 	%rd51, %rd66, %rd13;
	ld.global.nc.v2.f32 	{%f87, %f88}, [%rd51];
	fma.rn.f32 	%f91, %f77, %f87, %f294;
	fma.rn.f32 	%f92, %f78, %f88, %f91;
	add.s64 	%rd52, %rd66, %rd8;
	ld.global.nc.v2.f32 	{%f93, %f94}, [%rd52];
	fma.rn.f32 	%f97, %f77, %f93, %f293;
	fma.rn.f32 	%f98, %f78, %f94, %f97;
	add.s64 	%rd53, %rd66, %rd9;
	ld.global.nc.v2.f32 	{%f99, %f100}, [%rd53];
	fma.rn.f32 	%f103, %f77, %f99, %f292;
	fma.rn.f32 	%f104, %f78, %f100, %f103;
	add.s64 	%rd54, %rd66, %rd10;
	ld.global.nc.v2.f32 	{%f105, %f106}, [%rd54];
	fma.rn.f32 	%f109, %f77, %f105, %f291;
	fma.rn.f32 	%f110, %f78, %f106, %f109;
	add.s64 	%rd55, %rd66, %rd11;
	ld.global.nc.v2.f32 	{%f111, %f112}, [%rd55];
	fma.rn.f32 	%f115, %f77, %f111, %f290;
	fma.rn.f32 	%f116, %f78, %f112, %f115;
	ld.global.nc.v2.f32 	{%f117, %f118}, [%rd65];
	ld.global.nc.v2.f32 	{%f121, %f122}, [%rd50+1024];
	fma.rn.f32 	%f125, %f117, %f121, %f86;
	fma.rn.f32 	%f295, %f118, %f122, %f125;
	add.s64 	%rd56, %rd66, %rd7;
	ld.global.nc.v2.f32 	{%f126, %f127}, [%rd56];
	fma.rn.f32 	%f130, %f117, %f126, %f92;
	fma.rn.f32 	%f294, %f118, %f127, %f130;
	ld.global.nc.v2.f32 	{%f131, %f132}, [%rd52+1024];
	fma.rn.f32 	%f135, %f117, %f131, %f98;
	fma.rn.f32 	%f293, %f118, %f132, %f135;
	ld.global.nc.v2.f32 	{%f136, %f137}, [%rd53+1024];
	fma.rn.f32 	%f140, %f117, %f136, %f104;
	fma.rn.f32 	%f292, %f118, %f137, %f140;
	ld.global.nc.v2.f32 	{%f141, %f142}, [%rd54+1024];
	fma.rn.f32 	%f145, %f117, %f141, %f110;
	fma.rn.f32 	%f291, %f118, %f142, %f145;
	ld.global.nc.v2.f32 	{%f146, %f147}, [%rd55+1024];
	fma.rn.f32 	%f150, %f117, %f146, %f116;
	fma.rn.f32 	%f290, %f118, %f147, %f150;
	add.s64 	%rd66, %rd66, 2048;
	add.s64 	%rd65, %rd65, 2048;
	add.s32 	%r252, %r252, 256;
	setp.lt.s32 	%p5, %r252, %r11;
	@%p5 bra 	$L__BB29_7;

	st.local.v2.f32 	[%rd2], {%f295, %f294};
	st.local.v2.f32 	[%rd2+8], {%f293, %f292};
	st.local.v2.f32 	[%rd2+16], {%f291, %f290};

$L__BB29_9:
	shr.s32 	%r39, %r3, 31;
	shr.u32 	%r40, %r39, 27;
	add.s32 	%r41, %r3, %r40;
	shr.s32 	%r42, %r41, 5;
	shl.b32 	%r43, %r42, 2;
	add.s32 	%r10, %r24, %r43;
	mov.u32 	%r45, 2;
	mov.b32 	%r46, %f295;
	mov.u32 	%r47, 31;
	mov.u32 	%r48, 16;
	mov.u32 	%r49, -1;
	shfl.sync.bfly.b32 	%r50|%p6, %r46, %r48, %r47, %r49;
	mov.b32 	%f151, %r50;
	add.f32 	%f152, %f295, %f151;
	mov.b32 	%r51, %f152;
	mov.u32 	%r52, 8;
	shfl.sync.bfly.b32 	%r53|%p7, %r51, %r52, %r47, %r49;
	mov.b32 	%f153, %r53;
	add.f32 	%f154, %f152, %f153;
	mov.b32 	%r54, %f154;
	mov.u32 	%r55, 4;
	shfl.sync.bfly.b32 	%r56|%p8, %r54, %r55, %r47, %r49;
	mov.b32 	%f155, %r56;
	add.f32 	%f156, %f154, %f155;
	mov.b32 	%r57, %f156;
	shfl.sync.bfly.b32 	%r58|%p9, %r57, %r45, %r47, %r49;
	mov.b32 	%f157, %r58;
	add.f32 	%f158, %f156, %f157;
	mov.b32 	%r59, %f158;
	mov.u32 	%r60, 1;
	shfl.sync.bfly.b32 	%r61|%p10, %r59, %r60, %r47, %r49;
	mov.b32 	%f159, %r61;
	add.f32 	%f160, %f158, %f159;
	st.local.f32 	[%rd2], %f160;
	st.shared.f32 	[%r10], %f160;
	bar.sync 	0;
	@%p1 bra 	$L__BB29_11;

	ld.shared.f32 	%f161, [%r4];
	mov.b32 	%r62, %f161;
	shfl.sync.bfly.b32 	%r66|%p12, %r62, %r48, %r47, %r49;
	mov.b32 	%f162, %r66;
	add.f32 	%f163, %f161, %f162;
	mov.b32 	%r67, %f163;
	shfl.sync.bfly.b32 	%r69|%p13, %r67, %r52, %r47, %r49;
	mov.b32 	%f164, %r69;
	add.f32 	%f165, %f163, %f164;
	mov.b32 	%r70, %f165;
	shfl.sync.bfly.b32 	%r72|%p14, %r70, %r55, %r47, %r49;
	mov.b32 	%f166, %r72;
	add.f32 	%f167, %f165, %f166;
	mov.b32 	%r73, %f167;
	shfl.sync.bfly.b32 	%r75|%p15, %r73, %r45, %r47, %r49;
	mov.b32 	%f168, %r75;
	add.f32 	%f169, %f167, %f168;
	mov.b32 	%r76, %f169;
	shfl.sync.bfly.b32 	%r78|%p16, %r76, %r60, %r47, %r49;
	mov.b32 	%f170, %r78;
	add.f32 	%f171, %f169, %f170;
	st.local.f32 	[%rd2], %f171;

$L__BB29_11:
	bar.sync 	0;
	mov.b32 	%r79, %f294;
	shfl.sync.bfly.b32 	%r83|%p18, %r79, %r48, %r47, %r49;
	mov.b32 	%f172, %r83;
	add.f32 	%f173, %f294, %f172;
	mov.b32 	%r84, %f173;
	shfl.sync.bfly.b32 	%r86|%p19, %r84, %r52, %r47, %r49;
	mov.b32 	%f174, %r86;
	add.f32 	%f175, %f173, %f174;
	mov.b32 	%r87, %f175;
	shfl.sync.bfly.b32 	%r89|%p20, %r87, %r55, %r47, %r49;
	mov.b32 	%f176, %r89;
	add.f32 	%f177, %f175, %f176;
	mov.b32 	%r90, %f177;
	shfl.sync.bfly.b32 	%r92|%p21, %r90, %r45, %r47, %r49;
	mov.b32 	%f178, %r92;
	add.f32 	%f179, %f177, %f178;
	mov.b32 	%r93, %f179;
	shfl.sync.bfly.b32 	%r95|%p22, %r93, %r60, %r47, %r49;
	mov.b32 	%f180, %r95;
	add.f32 	%f181, %f179, %f180;
	st.local.f32 	[%rd2+4], %f181;
	st.shared.f32 	[%r10], %f181;
	bar.sync 	0;
	@%p1 bra 	$L__BB29_13;

	ld.shared.f32 	%f182, [%r4];
	mov.b32 	%r96, %f182;
	mov.u32 	%r97, 31;
	mov.u32 	%r98, 16;
	mov.u32 	%r99, -1;
	shfl.sync.bfly.b32 	%r100|%p23, %r96, %r98, %r97, %r99;
	mov.b32 	%f183, %r100;
	add.f32 	%f184, %f182, %f183;
	mov.b32 	%r101, %f184;
	mov.u32 	%r102, 8;
	shfl.sync.bfly.b32 	%r103|%p24, %r101, %r102, %r97, %r99;
	mov.b32 	%f185, %r103;
	add.f32 	%f186, %f184, %f185;
	mov.b32 	%r104, %f186;
	mov.u32 	%r105, 4;
	shfl.sync.bfly.b32 	%r106|%p25, %r104, %r105, %r97, %r99;
	mov.b32 	%f187, %r106;
	add.f32 	%f188, %f186, %f187;
	mov.b32 	%r107, %f188;
	mov.u32 	%r108, 2;
	shfl.sync.bfly.b32 	%r109|%p26, %r107, %r108, %r97, %r99;
	mov.b32 	%f189, %r109;
	add.f32 	%f190, %f188, %f189;
	mov.b32 	%r110, %f190;
	mov.u32 	%r111, 1;
	shfl.sync.bfly.b32 	%r112|%p27, %r110, %r111, %r97, %r99;
	mov.b32 	%f191, %r112;
	add.f32 	%f192, %f190, %f191;
	st.local.f32 	[%rd2+4], %f192;

$L__BB29_13:
	bar.sync 	0;
	mov.b32 	%r113, %f293;
	mov.u32 	%r114, 31;
	mov.u32 	%r115, 16;
	mov.u32 	%r116, -1;
	shfl.sync.bfly.b32 	%r117|%p29, %r113, %r115, %r114, %r116;
	mov.b32 	%f193, %r117;
	add.f32 	%f194, %f293, %f193;
	mov.b32 	%r118, %f194;
	mov.u32 	%r119, 8;
	shfl.sync.bfly.b32 	%r120|%p30, %r118, %r119, %r114, %r116;
	mov.b32 	%f195, %r120;
	add.f32 	%f196, %f194, %f195;
	mov.b32 	%r121, %f196;
	mov.u32 	%r122, 4;
	shfl.sync.bfly.b32 	%r123|%p31, %r121, %r122, %r114, %r116;
	mov.b32 	%f197, %r123;
	add.f32 	%f198, %f196, %f197;
	mov.b32 	%r124, %f198;
	mov.u32 	%r125, 2;
	shfl.sync.bfly.b32 	%r126|%p32, %r124, %r125, %r114, %r116;
	mov.b32 	%f199, %r126;
	add.f32 	%f200, %f198, %f199;
	mov.b32 	%r127, %f200;
	mov.u32 	%r128, 1;
	shfl.sync.bfly.b32 	%r129|%p33, %r127, %r128, %r114, %r116;
	mov.b32 	%f201, %r129;
	add.f32 	%f202, %f200, %f201;
	st.local.f32 	[%rd2+8], %f202;
	st.shared.f32 	[%r10], %f202;
	bar.sync 	0;
	@%p1 bra 	$L__BB29_15;

	ld.shared.f32 	%f203, [%r4];
	mov.b32 	%r130, %f203;
	shfl.sync.bfly.b32 	%r134|%p34, %r130, %r115, %r114, %r116;
	mov.b32 	%f204, %r134;
	add.f32 	%f205, %f203, %f204;
	mov.b32 	%r135, %f205;
	shfl.sync.bfly.b32 	%r137|%p35, %r135, %r119, %r114, %r116;
	mov.b32 	%f206, %r137;
	add.f32 	%f207, %f205, %f206;
	mov.b32 	%r138, %f207;
	shfl.sync.bfly.b32 	%r140|%p36, %r138, %r122, %r114, %r116;
	mov.b32 	%f208, %r140;
	add.f32 	%f209, %f207, %f208;
	mov.b32 	%r141, %f209;
	shfl.sync.bfly.b32 	%r143|%p37, %r141, %r125, %r114, %r116;
	mov.b32 	%f210, %r143;
	add.f32 	%f211, %f209, %f210;
	mov.b32 	%r144, %f211;
	shfl.sync.bfly.b32 	%r146|%p38, %r144, %r128, %r114, %r116;
	mov.b32 	%f212, %r146;
	add.f32 	%f213, %f211, %f212;
	st.local.f32 	[%rd2+8], %f213;

$L__BB29_15:
	bar.sync 	0;
	mov.b32 	%r147, %f292;
	shfl.sync.bfly.b32 	%r151|%p40, %r147, %r115, %r114, %r116;
	mov.b32 	%f214, %r151;
	add.f32 	%f215, %f292, %f214;
	mov.b32 	%r152, %f215;
	shfl.sync.bfly.b32 	%r154|%p41, %r152, %r119, %r114, %r116;
	mov.b32 	%f216, %r154;
	add.f32 	%f217, %f215, %f216;
	mov.b32 	%r155, %f217;
	shfl.sync.bfly.b32 	%r157|%p42, %r155, %r122, %r114, %r116;
	mov.b32 	%f218, %r157;
	add.f32 	%f219, %f217, %f218;
	mov.b32 	%r158, %f219;
	shfl.sync.bfly.b32 	%r160|%p43, %r158, %r125, %r114, %r116;
	mov.b32 	%f220, %r160;
	add.f32 	%f221, %f219, %f220;
	mov.b32 	%r161, %f221;
	shfl.sync.bfly.b32 	%r163|%p44, %r161, %r128, %r114, %r116;
	mov.b32 	%f222, %r163;
	add.f32 	%f223, %f221, %f222;
	st.local.f32 	[%rd2+12], %f223;
	st.shared.f32 	[%r10], %f223;
	bar.sync 	0;
	@%p1 bra 	$L__BB29_17;

	ld.shared.f32 	%f224, [%r4];
	mov.b32 	%r164, %f224;
	mov.u32 	%r165, 31;
	mov.u32 	%r166, 16;
	mov.u32 	%r167, -1;
	shfl.sync.bfly.b32 	%r168|%p45, %r164, %r166, %r165, %r167;
	mov.b32 	%f225, %r168;
	add.f32 	%f226, %f224, %f225;
	mov.b32 	%r169, %f226;
	mov.u32 	%r170, 8;
	shfl.sync.bfly.b32 	%r171|%p46, %r169, %r170, %r165, %r167;
	mov.b32 	%f227, %r171;
	add.f32 	%f228, %f226, %f227;
	mov.b32 	%r172, %f228;
	mov.u32 	%r173, 4;
	shfl.sync.bfly.b32 	%r174|%p47, %r172, %r173, %r165, %r167;
	mov.b32 	%f229, %r174;
	add.f32 	%f230, %f228, %f229;
	mov.b32 	%r175, %f230;
	mov.u32 	%r176, 2;
	shfl.sync.bfly.b32 	%r177|%p48, %r175, %r176, %r165, %r167;
	mov.b32 	%f231, %r177;
	add.f32 	%f232, %f230, %f231;
	mov.b32 	%r178, %f232;
	mov.u32 	%r179, 1;
	shfl.sync.bfly.b32 	%r180|%p49, %r178, %r179, %r165, %r167;
	mov.b32 	%f233, %r180;
	add.f32 	%f234, %f232, %f233;
	st.local.f32 	[%rd2+12], %f234;

$L__BB29_17:
	bar.sync 	0;
	mov.b32 	%r181, %f291;
	mov.u32 	%r182, 31;
	mov.u32 	%r183, 16;
	mov.u32 	%r184, -1;
	shfl.sync.bfly.b32 	%r185|%p51, %r181, %r183, %r182, %r184;
	mov.b32 	%f235, %r185;
	add.f32 	%f236, %f291, %f235;
	mov.b32 	%r186, %f236;
	mov.u32 	%r187, 8;
	shfl.sync.bfly.b32 	%r188|%p52, %r186, %r187, %r182, %r184;
	mov.b32 	%f237, %r188;
	add.f32 	%f238, %f236, %f237;
	mov.b32 	%r189, %f238;
	mov.u32 	%r190, 4;
	shfl.sync.bfly.b32 	%r191|%p53, %r189, %r190, %r182, %r184;
	mov.b32 	%f239, %r191;
	add.f32 	%f240, %f238, %f239;
	mov.b32 	%r192, %f240;
	mov.u32 	%r193, 2;
	shfl.sync.bfly.b32 	%r194|%p54, %r192, %r193, %r182, %r184;
	mov.b32 	%f241, %r194;
	add.f32 	%f242, %f240, %f241;
	mov.b32 	%r195, %f242;
	mov.u32 	%r196, 1;
	shfl.sync.bfly.b32 	%r197|%p55, %r195, %r196, %r182, %r184;
	mov.b32 	%f243, %r197;
	add.f32 	%f244, %f242, %f243;
	st.local.f32 	[%rd2+16], %f244;
	st.shared.f32 	[%r10], %f244;
	bar.sync 	0;
	@%p1 bra 	$L__BB29_19;

	ld.shared.f32 	%f245, [%r4];
	mov.b32 	%r198, %f245;
	shfl.sync.bfly.b32 	%r202|%p56, %r198, %r183, %r182, %r184;
	mov.b32 	%f246, %r202;
	add.f32 	%f247, %f245, %f246;
	mov.b32 	%r203, %f247;
	shfl.sync.bfly.b32 	%r205|%p57, %r203, %r187, %r182, %r184;
	mov.b32 	%f248, %r205;
	add.f32 	%f249, %f247, %f248;
	mov.b32 	%r206, %f249;
	shfl.sync.bfly.b32 	%r208|%p58, %r206, %r190, %r182, %r184;
	mov.b32 	%f250, %r208;
	add.f32 	%f251, %f249, %f250;
	mov.b32 	%r209, %f251;
	shfl.sync.bfly.b32 	%r211|%p59, %r209, %r193, %r182, %r184;
	mov.b32 	%f252, %r211;
	add.f32 	%f253, %f251, %f252;
	mov.b32 	%r212, %f253;
	shfl.sync.bfly.b32 	%r214|%p60, %r212, %r196, %r182, %r184;
	mov.b32 	%f254, %r214;
	add.f32 	%f255, %f253, %f254;
	st.local.f32 	[%rd2+16], %f255;

$L__BB29_19:
	bar.sync 	0;
	mov.b32 	%r215, %f290;
	shfl.sync.bfly.b32 	%r219|%p62, %r215, %r183, %r182, %r184;
	mov.b32 	%f256, %r219;
	add.f32 	%f257, %f290, %f256;
	mov.b32 	%r220, %f257;
	shfl.sync.bfly.b32 	%r222|%p63, %r220, %r187, %r182, %r184;
	mov.b32 	%f258, %r222;
	add.f32 	%f259, %f257, %f258;
	mov.b32 	%r223, %f259;
	shfl.sync.bfly.b32 	%r225|%p64, %r223, %r190, %r182, %r184;
	mov.b32 	%f260, %r225;
	add.f32 	%f261, %f259, %f260;
	mov.b32 	%r226, %f261;
	shfl.sync.bfly.b32 	%r228|%p65, %r226, %r193, %r182, %r184;
	mov.b32 	%f262, %r228;
	add.f32 	%f263, %f261, %f262;
	mov.b32 	%r229, %f263;
	shfl.sync.bfly.b32 	%r231|%p66, %r229, %r196, %r182, %r184;
	mov.b32 	%f264, %r231;
	add.f32 	%f265, %f263, %f264;
	st.local.f32 	[%rd2+20], %f265;
	st.shared.f32 	[%r10], %f265;
	bar.sync 	0;
	@%p1 bra 	$L__BB29_21;

	ld.shared.f32 	%f266, [%r4];
	mov.b32 	%r232, %f266;
	mov.u32 	%r233, 31;
	mov.u32 	%r234, 16;
	mov.u32 	%r235, -1;
	shfl.sync.bfly.b32 	%r236|%p67, %r232, %r234, %r233, %r235;
	mov.b32 	%f267, %r236;
	add.f32 	%f268, %f266, %f267;
	mov.b32 	%r237, %f268;
	mov.u32 	%r238, 8;
	shfl.sync.bfly.b32 	%r239|%p68, %r237, %r238, %r233, %r235;
	mov.b32 	%f269, %r239;
	add.f32 	%f270, %f268, %f269;
	mov.b32 	%r240, %f270;
	mov.u32 	%r241, 4;
	shfl.sync.bfly.b32 	%r242|%p69, %r240, %r241, %r233, %r235;
	mov.b32 	%f271, %r242;
	add.f32 	%f272, %f270, %f271;
	mov.b32 	%r243, %f272;
	mov.u32 	%r244, 2;
	shfl.sync.bfly.b32 	%r245|%p70, %r243, %r244, %r233, %r235;
	mov.b32 	%f273, %r245;
	add.f32 	%f274, %f272, %f273;
	mov.b32 	%r246, %f274;
	mov.u32 	%r247, 1;
	shfl.sync.bfly.b32 	%r248|%p71, %r246, %r247, %r233, %r235;
	mov.b32 	%f275, %r248;
	add.f32 	%f276, %f274, %f275;
	st.local.f32 	[%rd2+20], %f276;

$L__BB29_21:
	bar.sync 	0;
	setp.gt.s32 	%p72, %r3, 5;
	@%p72 bra 	$L__BB29_23;

	mul.wide.s32 	%rd57, %r3, 4;
	add.s64 	%rd58, %rd2, %rd57;
	ld.local.f32 	%f277, [%rd58];
	mad.lo.s32 	%r249, %r3, %r13, %r2;
	cvt.s64.s32 	%rd59, %r249;
	mul.lo.s32 	%r250, %r1, %r14;
	cvt.s64.s32 	%rd60, %r250;
	add.s64 	%rd61, %rd60, %rd59;
	cvta.to.global.u64 	%rd62, %rd19;
	shl.b64 	%rd63, %rd61, 2;
	add.s64 	%rd64, %rd62, %rd63;
	st.global.f32 	[%rd64], %f277;

$L__BB29_23:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_7_bs_128
.visible .entry ggml_matvec_f32_ncols_7_bs_128(
	.param .u64 ggml_matvec_f32_ncols_7_bs_128_param_0,
	.param .u64 ggml_matvec_f32_ncols_7_bs_128_param_1,
	.param .u64 ggml_matvec_f32_ncols_7_bs_128_param_2,
	.param .u32 ggml_matvec_f32_ncols_7_bs_128_param_3,
	.param .u32 ggml_matvec_f32_ncols_7_bs_128_param_4,
	.param .u32 ggml_matvec_f32_ncols_7_bs_128_param_5,
	.param .u32 ggml_matvec_f32_ncols_7_bs_128_param_6,
	.param .u32 ggml_matvec_f32_ncols_7_bs_128_param_7,
	.param .u32 ggml_matvec_f32_ncols_7_bs_128_param_8,
	.param .u32 ggml_matvec_f32_ncols_7_bs_128_param_9,
	.param .u32 ggml_matvec_f32_ncols_7_bs_128_param_10,
	.param .u32 ggml_matvec_f32_ncols_7_bs_128_param_11
)
{
	.local .align 4 .b8 	__local_depot30[28];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<84>;
	.reg .f32 	%f<343>;
	.reg .b32 	%r<289>;
	.reg .b64 	%rd<71>;


	mov.u64 	%SPL, __local_depot30;
	ld.param.u64 	%rd21, [ggml_matvec_f32_ncols_7_bs_128_param_0];
	ld.param.u64 	%rd22, [ggml_matvec_f32_ncols_7_bs_128_param_1];
	ld.param.u64 	%rd20, [ggml_matvec_f32_ncols_7_bs_128_param_2];
	ld.param.u32 	%r11, [ggml_matvec_f32_ncols_7_bs_128_param_3];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_7_bs_128_param_5];
	ld.param.u32 	%r12, [ggml_matvec_f32_ncols_7_bs_128_param_6];
	ld.param.u32 	%r13, [ggml_matvec_f32_ncols_7_bs_128_param_7];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_7_bs_128_param_8];
	ld.param.u32 	%r17, [ggml_matvec_f32_ncols_7_bs_128_param_9];
	ld.param.u32 	%r18, [ggml_matvec_f32_ncols_7_bs_128_param_10];
	ld.param.u32 	%r14, [ggml_matvec_f32_ncols_7_bs_128_param_11];
	cvta.to.global.u64 	%rd70, %rd22;
	add.u64 	%rd2, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r19, %r1, %r16;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r20, %r2, %r15;
	mad.lo.s32 	%r21, %r19, %r17, %r20;
	cvt.s64.s32 	%rd3, %r21;
	cvta.to.global.u64 	%rd4, %rd21;
	mul.lo.s32 	%r22, %r1, %r18;
	cvt.s64.s32 	%rd5, %r22;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r23, %r3, 2;
	mov.u32 	%r24, data_mmv;
	add.s32 	%r4, %r24, %r23;
	@%p1 bra 	$L__BB30_2;

	mov.u32 	%r25, 0;
	st.shared.u32 	[%r4], %r25;

$L__BB30_2:
	bar.sync 	0;
	mov.f32 	%f336, 0f00000000;
	mov.u32 	%r26, 0;
	st.local.u32 	[%rd2], %r26;
	st.local.u32 	[%rd2+4], %r26;
	st.local.u32 	[%rd2+8], %r26;
	st.local.u32 	[%rd2+12], %r26;
	st.local.u32 	[%rd2+16], %r26;
	st.local.u32 	[%rd2+20], %r26;
	st.local.u32 	[%rd2+24], %r26;
	setp.ge.s32 	%p2, %r3, %r11;
	mov.f32 	%f337, %f336;
	mov.f32 	%f338, %f336;
	mov.f32 	%f339, %f336;
	mov.f32 	%f340, %f336;
	mov.f32 	%f341, %f336;
	mov.f32 	%f342, %f336;
	@%p2 bra 	$L__BB30_9;

	not.b32 	%r27, %r3;
	add.s32 	%r5, %r27, %r11;
	and.b32  	%r28, %r5, 128;
	setp.ne.s32 	%p3, %r28, 0;
	mov.f32 	%f336, 0f00000000;
	mov.u32 	%r288, %r3;
	@%p3 bra 	$L__BB30_5;

	shl.b64 	%rd24, %rd5, 2;
	add.s64 	%rd25, %rd70, %rd24;
	shl.b64 	%rd26, %rd3, 2;
	add.s64 	%rd27, %rd4, %rd26;
	mul.wide.s32 	%rd28, %r3, 8;
	add.s64 	%rd29, %rd27, %rd28;
	ld.global.nc.v2.f32 	{%f50, %f51}, [%rd29];
	add.s64 	%rd30, %rd25, %rd28;
	ld.global.nc.v2.f32 	{%f54, %f55}, [%rd30];
	fma.rn.f32 	%f58, %f50, %f54, 0f00000000;
	fma.rn.f32 	%f342, %f51, %f55, %f58;
	st.local.f32 	[%rd2], %f342;
	mul.wide.s32 	%rd31, %r12, 8;
	add.s64 	%rd32, %rd30, %rd31;
	ld.global.nc.v2.f32 	{%f59, %f60}, [%rd32];
	fma.rn.f32 	%f63, %f50, %f59, 0f00000000;
	fma.rn.f32 	%f341, %f51, %f60, %f63;
	st.local.f32 	[%rd2+4], %f341;
	add.s32 	%r29, %r3, %r12;
	add.s32 	%r30, %r29, %r12;
	mul.wide.s32 	%rd33, %r30, 8;
	add.s64 	%rd34, %rd25, %rd33;
	ld.global.nc.v2.f32 	{%f64, %f65}, [%rd34];
	fma.rn.f32 	%f68, %f50, %f64, 0f00000000;
	fma.rn.f32 	%f340, %f51, %f65, %f68;
	st.local.f32 	[%rd2+8], %f340;
	add.s64 	%rd35, %rd34, %rd31;
	ld.global.nc.v2.f32 	{%f69, %f70}, [%rd35];
	fma.rn.f32 	%f73, %f50, %f69, 0f00000000;
	fma.rn.f32 	%f339, %f51, %f70, %f73;
	st.local.f32 	[%rd2+12], %f339;
	add.s64 	%rd36, %rd35, %rd31;
	ld.global.nc.v2.f32 	{%f74, %f75}, [%rd36];
	fma.rn.f32 	%f78, %f50, %f74, 0f00000000;
	fma.rn.f32 	%f338, %f51, %f75, %f78;
	st.local.f32 	[%rd2+16], %f338;
	add.s64 	%rd37, %rd36, %rd31;
	ld.global.nc.v2.f32 	{%f79, %f80}, [%rd37];
	fma.rn.f32 	%f83, %f50, %f79, 0f00000000;
	fma.rn.f32 	%f337, %f51, %f80, %f83;
	st.local.f32 	[%rd2+20], %f337;
	add.s64 	%rd38, %rd37, %rd31;
	ld.global.nc.v2.f32 	{%f84, %f85}, [%rd38];
	fma.rn.f32 	%f88, %f50, %f84, 0f00000000;
	fma.rn.f32 	%f336, %f51, %f85, %f88;
	st.local.f32 	[%rd2+24], %f336;
	add.s32 	%r288, %r3, 128;

$L__BB30_5:
	and.b32  	%r31, %r5, -128;
	setp.eq.s32 	%p4, %r31, 0;
	@%p4 bra 	$L__BB30_9;

	add.s32 	%r32, %r288, %r12;
	add.s32 	%r33, %r32, 128;
	mul.wide.s32 	%rd39, %r33, 8;
	shl.b64 	%rd40, %rd5, 2;
	add.s64 	%rd7, %rd39, %rd40;
	shl.b32 	%r34, %r12, 1;
	add.s32 	%r35, %r288, %r34;
	mad.lo.s32 	%r36, %r12, 3, %r288;
	shl.b32 	%r37, %r12, 2;
	add.s32 	%r38, %r288, %r37;
	mad.lo.s32 	%r39, %r12, 5, %r288;
	mad.lo.s32 	%r40, %r12, 6, %r288;
	mul.wide.s32 	%rd41, %r35, 8;
	add.s64 	%rd8, %rd41, %rd40;
	mul.wide.s32 	%rd42, %r36, 8;
	add.s64 	%rd9, %rd42, %rd40;
	mul.wide.s32 	%rd43, %r38, 8;
	add.s64 	%rd10, %rd43, %rd40;
	mul.wide.s32 	%rd44, %r39, 8;
	add.s64 	%rd11, %rd44, %rd40;
	mul.wide.s32 	%rd45, %r40, 8;
	add.s64 	%rd12, %rd45, %rd40;
	mul.wide.s32 	%rd46, %r288, 2;
	add.s64 	%rd47, %rd46, %rd3;
	shl.b64 	%rd48, %rd47, 2;
	add.s64 	%rd49, %rd4, %rd48;
	add.s64 	%rd69, %rd49, 1024;
	mul.wide.s32 	%rd50, %r288, 8;
	mul.wide.s32 	%rd51, %r12, 8;
	add.s64 	%rd52, %rd50, %rd51;
	add.s64 	%rd14, %rd52, %rd40;
	add.s64 	%rd15, %rd50, %rd40;

$L__BB30_7:
	ld.global.nc.v2.f32 	{%f89, %f90}, [%rd69+-1024];
	add.s64 	%rd53, %rd70, %rd15;
	ld.global.nc.v2.f32 	{%f93, %f94}, [%rd53];
	fma.rn.f32 	%f97, %f89, %f93, %f342;
	fma.rn.f32 	%f98, %f90, %f94, %f97;
	add.s64 	%rd54, %rd70, %rd14;
	ld.global.nc.v2.f32 	{%f99, %f100}, [%rd54];
	fma.rn.f32 	%f103, %f89, %f99, %f341;
	fma.rn.f32 	%f104, %f90, %f100, %f103;
	add.s64 	%rd55, %rd70, %rd8;
	ld.global.nc.v2.f32 	{%f105, %f106}, [%rd55];
	fma.rn.f32 	%f109, %f89, %f105, %f340;
	fma.rn.f32 	%f110, %f90, %f106, %f109;
	add.s64 	%rd56, %rd70, %rd9;
	ld.global.nc.v2.f32 	{%f111, %f112}, [%rd56];
	fma.rn.f32 	%f115, %f89, %f111, %f339;
	fma.rn.f32 	%f116, %f90, %f112, %f115;
	add.s64 	%rd57, %rd70, %rd10;
	ld.global.nc.v2.f32 	{%f117, %f118}, [%rd57];
	fma.rn.f32 	%f121, %f89, %f117, %f338;
	fma.rn.f32 	%f122, %f90, %f118, %f121;
	add.s64 	%rd58, %rd70, %rd11;
	ld.global.nc.v2.f32 	{%f123, %f124}, [%rd58];
	fma.rn.f32 	%f127, %f89, %f123, %f337;
	fma.rn.f32 	%f128, %f90, %f124, %f127;
	add.s64 	%rd59, %rd70, %rd12;
	ld.global.nc.v2.f32 	{%f129, %f130}, [%rd59];
	fma.rn.f32 	%f133, %f89, %f129, %f336;
	fma.rn.f32 	%f134, %f90, %f130, %f133;
	ld.global.nc.v2.f32 	{%f135, %f136}, [%rd69];
	ld.global.nc.v2.f32 	{%f139, %f140}, [%rd53+1024];
	fma.rn.f32 	%f143, %f135, %f139, %f98;
	fma.rn.f32 	%f342, %f136, %f140, %f143;
	add.s64 	%rd60, %rd70, %rd7;
	ld.global.nc.v2.f32 	{%f144, %f145}, [%rd60];
	fma.rn.f32 	%f148, %f135, %f144, %f104;
	fma.rn.f32 	%f341, %f136, %f145, %f148;
	ld.global.nc.v2.f32 	{%f149, %f150}, [%rd55+1024];
	fma.rn.f32 	%f153, %f135, %f149, %f110;
	fma.rn.f32 	%f340, %f136, %f150, %f153;
	ld.global.nc.v2.f32 	{%f154, %f155}, [%rd56+1024];
	fma.rn.f32 	%f158, %f135, %f154, %f116;
	fma.rn.f32 	%f339, %f136, %f155, %f158;
	ld.global.nc.v2.f32 	{%f159, %f160}, [%rd57+1024];
	fma.rn.f32 	%f163, %f135, %f159, %f122;
	fma.rn.f32 	%f338, %f136, %f160, %f163;
	ld.global.nc.v2.f32 	{%f164, %f165}, [%rd58+1024];
	fma.rn.f32 	%f168, %f135, %f164, %f128;
	fma.rn.f32 	%f337, %f136, %f165, %f168;
	ld.global.nc.v2.f32 	{%f169, %f170}, [%rd59+1024];
	fma.rn.f32 	%f173, %f135, %f169, %f134;
	fma.rn.f32 	%f336, %f136, %f170, %f173;
	add.s64 	%rd70, %rd70, 2048;
	add.s64 	%rd69, %rd69, 2048;
	add.s32 	%r288, %r288, 256;
	setp.lt.s32 	%p5, %r288, %r11;
	@%p5 bra 	$L__BB30_7;

	st.local.f32 	[%rd2], %f342;
	st.local.f32 	[%rd2+4], %f341;
	st.local.f32 	[%rd2+8], %f340;
	st.local.f32 	[%rd2+12], %f339;
	st.local.f32 	[%rd2+16], %f338;
	st.local.f32 	[%rd2+20], %f337;
	st.local.f32 	[%rd2+24], %f336;

$L__BB30_9:
	shr.s32 	%r41, %r3, 31;
	shr.u32 	%r42, %r41, 27;
	add.s32 	%r43, %r3, %r42;
	shr.s32 	%r44, %r43, 5;
	shl.b32 	%r45, %r44, 2;
	add.s32 	%r10, %r24, %r45;
	mov.u32 	%r47, 2;
	mov.b32 	%r48, %f342;
	mov.u32 	%r49, 31;
	mov.u32 	%r50, 16;
	mov.u32 	%r51, -1;
	shfl.sync.bfly.b32 	%r52|%p6, %r48, %r50, %r49, %r51;
	mov.b32 	%f174, %r52;
	add.f32 	%f175, %f342, %f174;
	mov.b32 	%r53, %f175;
	mov.u32 	%r54, 8;
	shfl.sync.bfly.b32 	%r55|%p7, %r53, %r54, %r49, %r51;
	mov.b32 	%f176, %r55;
	add.f32 	%f177, %f175, %f176;
	mov.b32 	%r56, %f177;
	mov.u32 	%r57, 4;
	shfl.sync.bfly.b32 	%r58|%p8, %r56, %r57, %r49, %r51;
	mov.b32 	%f178, %r58;
	add.f32 	%f179, %f177, %f178;
	mov.b32 	%r59, %f179;
	shfl.sync.bfly.b32 	%r60|%p9, %r59, %r47, %r49, %r51;
	mov.b32 	%f180, %r60;
	add.f32 	%f181, %f179, %f180;
	mov.b32 	%r61, %f181;
	mov.u32 	%r62, 1;
	shfl.sync.bfly.b32 	%r63|%p10, %r61, %r62, %r49, %r51;
	mov.b32 	%f182, %r63;
	add.f32 	%f183, %f181, %f182;
	st.local.f32 	[%rd2], %f183;
	st.shared.f32 	[%r10], %f183;
	bar.sync 	0;
	@%p1 bra 	$L__BB30_11;

	ld.shared.f32 	%f184, [%r4];
	mov.b32 	%r64, %f184;
	shfl.sync.bfly.b32 	%r68|%p12, %r64, %r50, %r49, %r51;
	mov.b32 	%f185, %r68;
	add.f32 	%f186, %f184, %f185;
	mov.b32 	%r69, %f186;
	shfl.sync.bfly.b32 	%r71|%p13, %r69, %r54, %r49, %r51;
	mov.b32 	%f187, %r71;
	add.f32 	%f188, %f186, %f187;
	mov.b32 	%r72, %f188;
	shfl.sync.bfly.b32 	%r74|%p14, %r72, %r57, %r49, %r51;
	mov.b32 	%f189, %r74;
	add.f32 	%f190, %f188, %f189;
	mov.b32 	%r75, %f190;
	shfl.sync.bfly.b32 	%r77|%p15, %r75, %r47, %r49, %r51;
	mov.b32 	%f191, %r77;
	add.f32 	%f192, %f190, %f191;
	mov.b32 	%r78, %f192;
	shfl.sync.bfly.b32 	%r80|%p16, %r78, %r62, %r49, %r51;
	mov.b32 	%f193, %r80;
	add.f32 	%f194, %f192, %f193;
	st.local.f32 	[%rd2], %f194;

$L__BB30_11:
	bar.sync 	0;
	mov.b32 	%r81, %f341;
	shfl.sync.bfly.b32 	%r85|%p18, %r81, %r50, %r49, %r51;
	mov.b32 	%f195, %r85;
	add.f32 	%f196, %f341, %f195;
	mov.b32 	%r86, %f196;
	shfl.sync.bfly.b32 	%r88|%p19, %r86, %r54, %r49, %r51;
	mov.b32 	%f197, %r88;
	add.f32 	%f198, %f196, %f197;
	mov.b32 	%r89, %f198;
	shfl.sync.bfly.b32 	%r91|%p20, %r89, %r57, %r49, %r51;
	mov.b32 	%f199, %r91;
	add.f32 	%f200, %f198, %f199;
	mov.b32 	%r92, %f200;
	shfl.sync.bfly.b32 	%r94|%p21, %r92, %r47, %r49, %r51;
	mov.b32 	%f201, %r94;
	add.f32 	%f202, %f200, %f201;
	mov.b32 	%r95, %f202;
	shfl.sync.bfly.b32 	%r97|%p22, %r95, %r62, %r49, %r51;
	mov.b32 	%f203, %r97;
	add.f32 	%f204, %f202, %f203;
	st.local.f32 	[%rd2+4], %f204;
	st.shared.f32 	[%r10], %f204;
	bar.sync 	0;
	@%p1 bra 	$L__BB30_13;

	ld.shared.f32 	%f205, [%r4];
	mov.b32 	%r98, %f205;
	mov.u32 	%r99, 31;
	mov.u32 	%r100, 16;
	mov.u32 	%r101, -1;
	shfl.sync.bfly.b32 	%r102|%p23, %r98, %r100, %r99, %r101;
	mov.b32 	%f206, %r102;
	add.f32 	%f207, %f205, %f206;
	mov.b32 	%r103, %f207;
	mov.u32 	%r104, 8;
	shfl.sync.bfly.b32 	%r105|%p24, %r103, %r104, %r99, %r101;
	mov.b32 	%f208, %r105;
	add.f32 	%f209, %f207, %f208;
	mov.b32 	%r106, %f209;
	mov.u32 	%r107, 4;
	shfl.sync.bfly.b32 	%r108|%p25, %r106, %r107, %r99, %r101;
	mov.b32 	%f210, %r108;
	add.f32 	%f211, %f209, %f210;
	mov.b32 	%r109, %f211;
	mov.u32 	%r110, 2;
	shfl.sync.bfly.b32 	%r111|%p26, %r109, %r110, %r99, %r101;
	mov.b32 	%f212, %r111;
	add.f32 	%f213, %f211, %f212;
	mov.b32 	%r112, %f213;
	mov.u32 	%r113, 1;
	shfl.sync.bfly.b32 	%r114|%p27, %r112, %r113, %r99, %r101;
	mov.b32 	%f214, %r114;
	add.f32 	%f215, %f213, %f214;
	st.local.f32 	[%rd2+4], %f215;

$L__BB30_13:
	bar.sync 	0;
	mov.b32 	%r115, %f340;
	mov.u32 	%r116, 31;
	mov.u32 	%r117, 16;
	mov.u32 	%r118, -1;
	shfl.sync.bfly.b32 	%r119|%p29, %r115, %r117, %r116, %r118;
	mov.b32 	%f216, %r119;
	add.f32 	%f217, %f340, %f216;
	mov.b32 	%r120, %f217;
	mov.u32 	%r121, 8;
	shfl.sync.bfly.b32 	%r122|%p30, %r120, %r121, %r116, %r118;
	mov.b32 	%f218, %r122;
	add.f32 	%f219, %f217, %f218;
	mov.b32 	%r123, %f219;
	mov.u32 	%r124, 4;
	shfl.sync.bfly.b32 	%r125|%p31, %r123, %r124, %r116, %r118;
	mov.b32 	%f220, %r125;
	add.f32 	%f221, %f219, %f220;
	mov.b32 	%r126, %f221;
	mov.u32 	%r127, 2;
	shfl.sync.bfly.b32 	%r128|%p32, %r126, %r127, %r116, %r118;
	mov.b32 	%f222, %r128;
	add.f32 	%f223, %f221, %f222;
	mov.b32 	%r129, %f223;
	mov.u32 	%r130, 1;
	shfl.sync.bfly.b32 	%r131|%p33, %r129, %r130, %r116, %r118;
	mov.b32 	%f224, %r131;
	add.f32 	%f225, %f223, %f224;
	st.local.f32 	[%rd2+8], %f225;
	st.shared.f32 	[%r10], %f225;
	bar.sync 	0;
	@%p1 bra 	$L__BB30_15;

	ld.shared.f32 	%f226, [%r4];
	mov.b32 	%r132, %f226;
	shfl.sync.bfly.b32 	%r136|%p34, %r132, %r117, %r116, %r118;
	mov.b32 	%f227, %r136;
	add.f32 	%f228, %f226, %f227;
	mov.b32 	%r137, %f228;
	shfl.sync.bfly.b32 	%r139|%p35, %r137, %r121, %r116, %r118;
	mov.b32 	%f229, %r139;
	add.f32 	%f230, %f228, %f229;
	mov.b32 	%r140, %f230;
	shfl.sync.bfly.b32 	%r142|%p36, %r140, %r124, %r116, %r118;
	mov.b32 	%f231, %r142;
	add.f32 	%f232, %f230, %f231;
	mov.b32 	%r143, %f232;
	shfl.sync.bfly.b32 	%r145|%p37, %r143, %r127, %r116, %r118;
	mov.b32 	%f233, %r145;
	add.f32 	%f234, %f232, %f233;
	mov.b32 	%r146, %f234;
	shfl.sync.bfly.b32 	%r148|%p38, %r146, %r130, %r116, %r118;
	mov.b32 	%f235, %r148;
	add.f32 	%f236, %f234, %f235;
	st.local.f32 	[%rd2+8], %f236;

$L__BB30_15:
	bar.sync 	0;
	mov.b32 	%r149, %f339;
	shfl.sync.bfly.b32 	%r153|%p40, %r149, %r117, %r116, %r118;
	mov.b32 	%f237, %r153;
	add.f32 	%f238, %f339, %f237;
	mov.b32 	%r154, %f238;
	shfl.sync.bfly.b32 	%r156|%p41, %r154, %r121, %r116, %r118;
	mov.b32 	%f239, %r156;
	add.f32 	%f240, %f238, %f239;
	mov.b32 	%r157, %f240;
	shfl.sync.bfly.b32 	%r159|%p42, %r157, %r124, %r116, %r118;
	mov.b32 	%f241, %r159;
	add.f32 	%f242, %f240, %f241;
	mov.b32 	%r160, %f242;
	shfl.sync.bfly.b32 	%r162|%p43, %r160, %r127, %r116, %r118;
	mov.b32 	%f243, %r162;
	add.f32 	%f244, %f242, %f243;
	mov.b32 	%r163, %f244;
	shfl.sync.bfly.b32 	%r165|%p44, %r163, %r130, %r116, %r118;
	mov.b32 	%f245, %r165;
	add.f32 	%f246, %f244, %f245;
	st.local.f32 	[%rd2+12], %f246;
	st.shared.f32 	[%r10], %f246;
	bar.sync 	0;
	@%p1 bra 	$L__BB30_17;

	ld.shared.f32 	%f247, [%r4];
	mov.b32 	%r166, %f247;
	mov.u32 	%r167, 31;
	mov.u32 	%r168, 16;
	mov.u32 	%r169, -1;
	shfl.sync.bfly.b32 	%r170|%p45, %r166, %r168, %r167, %r169;
	mov.b32 	%f248, %r170;
	add.f32 	%f249, %f247, %f248;
	mov.b32 	%r171, %f249;
	mov.u32 	%r172, 8;
	shfl.sync.bfly.b32 	%r173|%p46, %r171, %r172, %r167, %r169;
	mov.b32 	%f250, %r173;
	add.f32 	%f251, %f249, %f250;
	mov.b32 	%r174, %f251;
	mov.u32 	%r175, 4;
	shfl.sync.bfly.b32 	%r176|%p47, %r174, %r175, %r167, %r169;
	mov.b32 	%f252, %r176;
	add.f32 	%f253, %f251, %f252;
	mov.b32 	%r177, %f253;
	mov.u32 	%r178, 2;
	shfl.sync.bfly.b32 	%r179|%p48, %r177, %r178, %r167, %r169;
	mov.b32 	%f254, %r179;
	add.f32 	%f255, %f253, %f254;
	mov.b32 	%r180, %f255;
	mov.u32 	%r181, 1;
	shfl.sync.bfly.b32 	%r182|%p49, %r180, %r181, %r167, %r169;
	mov.b32 	%f256, %r182;
	add.f32 	%f257, %f255, %f256;
	st.local.f32 	[%rd2+12], %f257;

$L__BB30_17:
	bar.sync 	0;
	mov.b32 	%r183, %f338;
	mov.u32 	%r184, 31;
	mov.u32 	%r185, 16;
	mov.u32 	%r186, -1;
	shfl.sync.bfly.b32 	%r187|%p51, %r183, %r185, %r184, %r186;
	mov.b32 	%f258, %r187;
	add.f32 	%f259, %f338, %f258;
	mov.b32 	%r188, %f259;
	mov.u32 	%r189, 8;
	shfl.sync.bfly.b32 	%r190|%p52, %r188, %r189, %r184, %r186;
	mov.b32 	%f260, %r190;
	add.f32 	%f261, %f259, %f260;
	mov.b32 	%r191, %f261;
	mov.u32 	%r192, 4;
	shfl.sync.bfly.b32 	%r193|%p53, %r191, %r192, %r184, %r186;
	mov.b32 	%f262, %r193;
	add.f32 	%f263, %f261, %f262;
	mov.b32 	%r194, %f263;
	mov.u32 	%r195, 2;
	shfl.sync.bfly.b32 	%r196|%p54, %r194, %r195, %r184, %r186;
	mov.b32 	%f264, %r196;
	add.f32 	%f265, %f263, %f264;
	mov.b32 	%r197, %f265;
	mov.u32 	%r198, 1;
	shfl.sync.bfly.b32 	%r199|%p55, %r197, %r198, %r184, %r186;
	mov.b32 	%f266, %r199;
	add.f32 	%f267, %f265, %f266;
	st.local.f32 	[%rd2+16], %f267;
	st.shared.f32 	[%r10], %f267;
	bar.sync 	0;
	@%p1 bra 	$L__BB30_19;

	ld.shared.f32 	%f268, [%r4];
	mov.b32 	%r200, %f268;
	shfl.sync.bfly.b32 	%r204|%p56, %r200, %r185, %r184, %r186;
	mov.b32 	%f269, %r204;
	add.f32 	%f270, %f268, %f269;
	mov.b32 	%r205, %f270;
	shfl.sync.bfly.b32 	%r207|%p57, %r205, %r189, %r184, %r186;
	mov.b32 	%f271, %r207;
	add.f32 	%f272, %f270, %f271;
	mov.b32 	%r208, %f272;
	shfl.sync.bfly.b32 	%r210|%p58, %r208, %r192, %r184, %r186;
	mov.b32 	%f273, %r210;
	add.f32 	%f274, %f272, %f273;
	mov.b32 	%r211, %f274;
	shfl.sync.bfly.b32 	%r213|%p59, %r211, %r195, %r184, %r186;
	mov.b32 	%f275, %r213;
	add.f32 	%f276, %f274, %f275;
	mov.b32 	%r214, %f276;
	shfl.sync.bfly.b32 	%r216|%p60, %r214, %r198, %r184, %r186;
	mov.b32 	%f277, %r216;
	add.f32 	%f278, %f276, %f277;
	st.local.f32 	[%rd2+16], %f278;

$L__BB30_19:
	bar.sync 	0;
	mov.b32 	%r217, %f337;
	shfl.sync.bfly.b32 	%r221|%p62, %r217, %r185, %r184, %r186;
	mov.b32 	%f279, %r221;
	add.f32 	%f280, %f337, %f279;
	mov.b32 	%r222, %f280;
	shfl.sync.bfly.b32 	%r224|%p63, %r222, %r189, %r184, %r186;
	mov.b32 	%f281, %r224;
	add.f32 	%f282, %f280, %f281;
	mov.b32 	%r225, %f282;
	shfl.sync.bfly.b32 	%r227|%p64, %r225, %r192, %r184, %r186;
	mov.b32 	%f283, %r227;
	add.f32 	%f284, %f282, %f283;
	mov.b32 	%r228, %f284;
	shfl.sync.bfly.b32 	%r230|%p65, %r228, %r195, %r184, %r186;
	mov.b32 	%f285, %r230;
	add.f32 	%f286, %f284, %f285;
	mov.b32 	%r231, %f286;
	shfl.sync.bfly.b32 	%r233|%p66, %r231, %r198, %r184, %r186;
	mov.b32 	%f287, %r233;
	add.f32 	%f288, %f286, %f287;
	st.local.f32 	[%rd2+20], %f288;
	st.shared.f32 	[%r10], %f288;
	bar.sync 	0;
	@%p1 bra 	$L__BB30_21;

	ld.shared.f32 	%f289, [%r4];
	mov.b32 	%r234, %f289;
	mov.u32 	%r235, 31;
	mov.u32 	%r236, 16;
	mov.u32 	%r237, -1;
	shfl.sync.bfly.b32 	%r238|%p67, %r234, %r236, %r235, %r237;
	mov.b32 	%f290, %r238;
	add.f32 	%f291, %f289, %f290;
	mov.b32 	%r239, %f291;
	mov.u32 	%r240, 8;
	shfl.sync.bfly.b32 	%r241|%p68, %r239, %r240, %r235, %r237;
	mov.b32 	%f292, %r241;
	add.f32 	%f293, %f291, %f292;
	mov.b32 	%r242, %f293;
	mov.u32 	%r243, 4;
	shfl.sync.bfly.b32 	%r244|%p69, %r242, %r243, %r235, %r237;
	mov.b32 	%f294, %r244;
	add.f32 	%f295, %f293, %f294;
	mov.b32 	%r245, %f295;
	mov.u32 	%r246, 2;
	shfl.sync.bfly.b32 	%r247|%p70, %r245, %r246, %r235, %r237;
	mov.b32 	%f296, %r247;
	add.f32 	%f297, %f295, %f296;
	mov.b32 	%r248, %f297;
	mov.u32 	%r249, 1;
	shfl.sync.bfly.b32 	%r250|%p71, %r248, %r249, %r235, %r237;
	mov.b32 	%f298, %r250;
	add.f32 	%f299, %f297, %f298;
	st.local.f32 	[%rd2+20], %f299;

$L__BB30_21:
	bar.sync 	0;
	mov.b32 	%r251, %f336;
	mov.u32 	%r252, 31;
	mov.u32 	%r253, 16;
	mov.u32 	%r254, -1;
	shfl.sync.bfly.b32 	%r255|%p73, %r251, %r253, %r252, %r254;
	mov.b32 	%f300, %r255;
	add.f32 	%f301, %f336, %f300;
	mov.b32 	%r256, %f301;
	mov.u32 	%r257, 8;
	shfl.sync.bfly.b32 	%r258|%p74, %r256, %r257, %r252, %r254;
	mov.b32 	%f302, %r258;
	add.f32 	%f303, %f301, %f302;
	mov.b32 	%r259, %f303;
	mov.u32 	%r260, 4;
	shfl.sync.bfly.b32 	%r261|%p75, %r259, %r260, %r252, %r254;
	mov.b32 	%f304, %r261;
	add.f32 	%f305, %f303, %f304;
	mov.b32 	%r262, %f305;
	mov.u32 	%r263, 2;
	shfl.sync.bfly.b32 	%r264|%p76, %r262, %r263, %r252, %r254;
	mov.b32 	%f306, %r264;
	add.f32 	%f307, %f305, %f306;
	mov.b32 	%r265, %f307;
	mov.u32 	%r266, 1;
	shfl.sync.bfly.b32 	%r267|%p77, %r265, %r266, %r252, %r254;
	mov.b32 	%f308, %r267;
	add.f32 	%f309, %f307, %f308;
	st.local.f32 	[%rd2+24], %f309;
	st.shared.f32 	[%r10], %f309;
	bar.sync 	0;
	@%p1 bra 	$L__BB30_23;

	ld.shared.f32 	%f310, [%r4];
	mov.b32 	%r268, %f310;
	shfl.sync.bfly.b32 	%r272|%p78, %r268, %r253, %r252, %r254;
	mov.b32 	%f311, %r272;
	add.f32 	%f312, %f310, %f311;
	mov.b32 	%r273, %f312;
	shfl.sync.bfly.b32 	%r275|%p79, %r273, %r257, %r252, %r254;
	mov.b32 	%f313, %r275;
	add.f32 	%f314, %f312, %f313;
	mov.b32 	%r276, %f314;
	shfl.sync.bfly.b32 	%r278|%p80, %r276, %r260, %r252, %r254;
	mov.b32 	%f315, %r278;
	add.f32 	%f316, %f314, %f315;
	mov.b32 	%r279, %f316;
	shfl.sync.bfly.b32 	%r281|%p81, %r279, %r263, %r252, %r254;
	mov.b32 	%f317, %r281;
	add.f32 	%f318, %f316, %f317;
	mov.b32 	%r282, %f318;
	shfl.sync.bfly.b32 	%r284|%p82, %r282, %r266, %r252, %r254;
	mov.b32 	%f319, %r284;
	add.f32 	%f320, %f318, %f319;
	st.local.f32 	[%rd2+24], %f320;

$L__BB30_23:
	bar.sync 	0;
	setp.gt.s32 	%p83, %r3, 6;
	@%p83 bra 	$L__BB30_25;

	mul.wide.s32 	%rd61, %r3, 4;
	add.s64 	%rd62, %rd2, %rd61;
	ld.local.f32 	%f321, [%rd62];
	mad.lo.s32 	%r285, %r3, %r13, %r2;
	cvt.s64.s32 	%rd63, %r285;
	mul.lo.s32 	%r286, %r1, %r14;
	cvt.s64.s32 	%rd64, %r286;
	add.s64 	%rd65, %rd64, %rd63;
	cvta.to.global.u64 	%rd66, %rd20;
	shl.b64 	%rd67, %rd65, 2;
	add.s64 	%rd68, %rd66, %rd67;
	st.global.f32 	[%rd68], %f321;

$L__BB30_25:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_8_bs_128
.visible .entry ggml_matvec_f32_ncols_8_bs_128(
	.param .u64 ggml_matvec_f32_ncols_8_bs_128_param_0,
	.param .u64 ggml_matvec_f32_ncols_8_bs_128_param_1,
	.param .u64 ggml_matvec_f32_ncols_8_bs_128_param_2,
	.param .u32 ggml_matvec_f32_ncols_8_bs_128_param_3,
	.param .u32 ggml_matvec_f32_ncols_8_bs_128_param_4,
	.param .u32 ggml_matvec_f32_ncols_8_bs_128_param_5,
	.param .u32 ggml_matvec_f32_ncols_8_bs_128_param_6,
	.param .u32 ggml_matvec_f32_ncols_8_bs_128_param_7,
	.param .u32 ggml_matvec_f32_ncols_8_bs_128_param_8,
	.param .u32 ggml_matvec_f32_ncols_8_bs_128_param_9,
	.param .u32 ggml_matvec_f32_ncols_8_bs_128_param_10,
	.param .u32 ggml_matvec_f32_ncols_8_bs_128_param_11
)
{
	.local .align 16 .b8 	__local_depot31[32];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<95>;
	.reg .f32 	%f<390>;
	.reg .b32 	%r<323>;
	.reg .b64 	%rd<75>;


	mov.u64 	%SPL, __local_depot31;
	ld.param.u64 	%rd22, [ggml_matvec_f32_ncols_8_bs_128_param_0];
	ld.param.u64 	%rd23, [ggml_matvec_f32_ncols_8_bs_128_param_1];
	ld.param.u64 	%rd21, [ggml_matvec_f32_ncols_8_bs_128_param_2];
	ld.param.u32 	%r11, [ggml_matvec_f32_ncols_8_bs_128_param_3];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_8_bs_128_param_5];
	ld.param.u32 	%r12, [ggml_matvec_f32_ncols_8_bs_128_param_6];
	ld.param.u32 	%r13, [ggml_matvec_f32_ncols_8_bs_128_param_7];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_8_bs_128_param_8];
	ld.param.u32 	%r17, [ggml_matvec_f32_ncols_8_bs_128_param_9];
	ld.param.u32 	%r18, [ggml_matvec_f32_ncols_8_bs_128_param_10];
	ld.param.u32 	%r14, [ggml_matvec_f32_ncols_8_bs_128_param_11];
	cvta.to.global.u64 	%rd74, %rd23;
	add.u64 	%rd2, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r19, %r1, %r16;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r20, %r2, %r15;
	mad.lo.s32 	%r21, %r19, %r17, %r20;
	cvt.s64.s32 	%rd3, %r21;
	cvta.to.global.u64 	%rd4, %rd22;
	mul.lo.s32 	%r22, %r1, %r18;
	cvt.s64.s32 	%rd5, %r22;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r23, %r3, 2;
	mov.u32 	%r24, data_mmv;
	add.s32 	%r4, %r24, %r23;
	@%p1 bra 	$L__BB31_2;

	mov.u32 	%r25, 0;
	st.shared.u32 	[%r4], %r25;

$L__BB31_2:
	bar.sync 	0;
	mov.f32 	%f382, 0f00000000;
	st.local.v4.f32 	[%rd2], {%f382, %f382, %f382, %f382};
	st.local.v4.f32 	[%rd2+16], {%f382, %f382, %f382, %f382};
	setp.ge.s32 	%p2, %r3, %r11;
	mov.f32 	%f383, %f382;
	mov.f32 	%f384, %f382;
	mov.f32 	%f385, %f382;
	mov.f32 	%f386, %f382;
	mov.f32 	%f387, %f382;
	mov.f32 	%f388, %f382;
	mov.f32 	%f389, %f382;
	@%p2 bra 	$L__BB31_9;

	not.b32 	%r26, %r3;
	add.s32 	%r5, %r26, %r11;
	and.b32  	%r27, %r5, 128;
	setp.ne.s32 	%p3, %r27, 0;
	mov.f32 	%f382, 0f00000000;
	mov.u32 	%r322, %r3;
	@%p3 bra 	$L__BB31_5;

	shl.b64 	%rd25, %rd5, 2;
	add.s64 	%rd26, %rd74, %rd25;
	shl.b64 	%rd27, %rd3, 2;
	add.s64 	%rd28, %rd4, %rd27;
	mul.wide.s32 	%rd29, %r3, 8;
	add.s64 	%rd30, %rd28, %rd29;
	ld.global.nc.v2.f32 	{%f57, %f58}, [%rd30];
	add.s64 	%rd31, %rd26, %rd29;
	ld.global.nc.v2.f32 	{%f61, %f62}, [%rd31];
	fma.rn.f32 	%f65, %f57, %f61, 0f00000000;
	fma.rn.f32 	%f389, %f58, %f62, %f65;
	mul.wide.s32 	%rd32, %r12, 8;
	add.s64 	%rd33, %rd31, %rd32;
	ld.global.nc.v2.f32 	{%f66, %f67}, [%rd33];
	fma.rn.f32 	%f70, %f57, %f66, 0f00000000;
	fma.rn.f32 	%f388, %f58, %f67, %f70;
	add.s32 	%r28, %r3, %r12;
	add.s32 	%r29, %r28, %r12;
	mul.wide.s32 	%rd34, %r29, 8;
	add.s64 	%rd35, %rd26, %rd34;
	ld.global.nc.v2.f32 	{%f71, %f72}, [%rd35];
	fma.rn.f32 	%f75, %f57, %f71, 0f00000000;
	fma.rn.f32 	%f387, %f58, %f72, %f75;
	add.s64 	%rd36, %rd35, %rd32;
	ld.global.nc.v2.f32 	{%f76, %f77}, [%rd36];
	fma.rn.f32 	%f80, %f57, %f76, 0f00000000;
	fma.rn.f32 	%f386, %f58, %f77, %f80;
	st.local.v4.f32 	[%rd2], {%f389, %f388, %f387, %f386};
	add.s64 	%rd37, %rd36, %rd32;
	ld.global.nc.v2.f32 	{%f81, %f82}, [%rd37];
	fma.rn.f32 	%f85, %f57, %f81, 0f00000000;
	fma.rn.f32 	%f385, %f58, %f82, %f85;
	add.s64 	%rd38, %rd37, %rd32;
	ld.global.nc.v2.f32 	{%f86, %f87}, [%rd38];
	fma.rn.f32 	%f90, %f57, %f86, 0f00000000;
	fma.rn.f32 	%f384, %f58, %f87, %f90;
	add.s64 	%rd39, %rd38, %rd32;
	ld.global.nc.v2.f32 	{%f91, %f92}, [%rd39];
	fma.rn.f32 	%f95, %f57, %f91, 0f00000000;
	fma.rn.f32 	%f383, %f58, %f92, %f95;
	add.s64 	%rd40, %rd39, %rd32;
	ld.global.nc.v2.f32 	{%f96, %f97}, [%rd40];
	fma.rn.f32 	%f100, %f57, %f96, 0f00000000;
	fma.rn.f32 	%f382, %f58, %f97, %f100;
	st.local.v4.f32 	[%rd2+16], {%f385, %f384, %f383, %f382};
	add.s32 	%r322, %r3, 128;

$L__BB31_5:
	and.b32  	%r30, %r5, -128;
	setp.eq.s32 	%p4, %r30, 0;
	@%p4 bra 	$L__BB31_9;

	add.s32 	%r31, %r322, %r12;
	add.s32 	%r32, %r31, 128;
	mul.wide.s32 	%rd41, %r32, 8;
	shl.b64 	%rd42, %rd5, 2;
	add.s64 	%rd7, %rd41, %rd42;
	shl.b32 	%r33, %r12, 1;
	add.s32 	%r34, %r322, %r33;
	mad.lo.s32 	%r35, %r12, 3, %r322;
	shl.b32 	%r36, %r12, 2;
	add.s32 	%r37, %r322, %r36;
	mad.lo.s32 	%r38, %r12, 5, %r322;
	mad.lo.s32 	%r39, %r12, 6, %r322;
	mad.lo.s32 	%r40, %r12, 7, %r322;
	mul.wide.s32 	%rd43, %r34, 8;
	add.s64 	%rd8, %rd43, %rd42;
	mul.wide.s32 	%rd44, %r35, 8;
	add.s64 	%rd9, %rd44, %rd42;
	mul.wide.s32 	%rd45, %r37, 8;
	add.s64 	%rd10, %rd45, %rd42;
	mul.wide.s32 	%rd46, %r38, 8;
	add.s64 	%rd11, %rd46, %rd42;
	mul.wide.s32 	%rd47, %r39, 8;
	add.s64 	%rd12, %rd47, %rd42;
	mul.wide.s32 	%rd48, %r40, 8;
	add.s64 	%rd13, %rd48, %rd42;
	mul.wide.s32 	%rd49, %r322, 2;
	add.s64 	%rd50, %rd49, %rd3;
	shl.b64 	%rd51, %rd50, 2;
	add.s64 	%rd52, %rd4, %rd51;
	add.s64 	%rd73, %rd52, 1024;
	mul.wide.s32 	%rd53, %r322, 8;
	mul.wide.s32 	%rd54, %r12, 8;
	add.s64 	%rd55, %rd53, %rd54;
	add.s64 	%rd15, %rd55, %rd42;
	add.s64 	%rd16, %rd53, %rd42;

$L__BB31_7:
	ld.global.nc.v2.f32 	{%f101, %f102}, [%rd73+-1024];
	add.s64 	%rd56, %rd74, %rd16;
	ld.global.nc.v2.f32 	{%f105, %f106}, [%rd56];
	fma.rn.f32 	%f109, %f101, %f105, %f389;
	fma.rn.f32 	%f110, %f102, %f106, %f109;
	add.s64 	%rd57, %rd74, %rd15;
	ld.global.nc.v2.f32 	{%f111, %f112}, [%rd57];
	fma.rn.f32 	%f115, %f101, %f111, %f388;
	fma.rn.f32 	%f116, %f102, %f112, %f115;
	add.s64 	%rd58, %rd74, %rd8;
	ld.global.nc.v2.f32 	{%f117, %f118}, [%rd58];
	fma.rn.f32 	%f121, %f101, %f117, %f387;
	fma.rn.f32 	%f122, %f102, %f118, %f121;
	add.s64 	%rd59, %rd74, %rd9;
	ld.global.nc.v2.f32 	{%f123, %f124}, [%rd59];
	fma.rn.f32 	%f127, %f101, %f123, %f386;
	fma.rn.f32 	%f128, %f102, %f124, %f127;
	add.s64 	%rd60, %rd74, %rd10;
	ld.global.nc.v2.f32 	{%f129, %f130}, [%rd60];
	fma.rn.f32 	%f133, %f101, %f129, %f385;
	fma.rn.f32 	%f134, %f102, %f130, %f133;
	add.s64 	%rd61, %rd74, %rd11;
	ld.global.nc.v2.f32 	{%f135, %f136}, [%rd61];
	fma.rn.f32 	%f139, %f101, %f135, %f384;
	fma.rn.f32 	%f140, %f102, %f136, %f139;
	add.s64 	%rd62, %rd74, %rd12;
	ld.global.nc.v2.f32 	{%f141, %f142}, [%rd62];
	fma.rn.f32 	%f145, %f101, %f141, %f383;
	fma.rn.f32 	%f146, %f102, %f142, %f145;
	add.s64 	%rd63, %rd74, %rd13;
	ld.global.nc.v2.f32 	{%f147, %f148}, [%rd63];
	fma.rn.f32 	%f151, %f101, %f147, %f382;
	fma.rn.f32 	%f152, %f102, %f148, %f151;
	ld.global.nc.v2.f32 	{%f153, %f154}, [%rd73];
	ld.global.nc.v2.f32 	{%f157, %f158}, [%rd56+1024];
	fma.rn.f32 	%f161, %f153, %f157, %f110;
	fma.rn.f32 	%f389, %f154, %f158, %f161;
	add.s64 	%rd64, %rd74, %rd7;
	ld.global.nc.v2.f32 	{%f162, %f163}, [%rd64];
	fma.rn.f32 	%f166, %f153, %f162, %f116;
	fma.rn.f32 	%f388, %f154, %f163, %f166;
	ld.global.nc.v2.f32 	{%f167, %f168}, [%rd58+1024];
	fma.rn.f32 	%f171, %f153, %f167, %f122;
	fma.rn.f32 	%f387, %f154, %f168, %f171;
	ld.global.nc.v2.f32 	{%f172, %f173}, [%rd59+1024];
	fma.rn.f32 	%f176, %f153, %f172, %f128;
	fma.rn.f32 	%f386, %f154, %f173, %f176;
	ld.global.nc.v2.f32 	{%f177, %f178}, [%rd60+1024];
	fma.rn.f32 	%f181, %f153, %f177, %f134;
	fma.rn.f32 	%f385, %f154, %f178, %f181;
	ld.global.nc.v2.f32 	{%f182, %f183}, [%rd61+1024];
	fma.rn.f32 	%f186, %f153, %f182, %f140;
	fma.rn.f32 	%f384, %f154, %f183, %f186;
	ld.global.nc.v2.f32 	{%f187, %f188}, [%rd62+1024];
	fma.rn.f32 	%f191, %f153, %f187, %f146;
	fma.rn.f32 	%f383, %f154, %f188, %f191;
	ld.global.nc.v2.f32 	{%f192, %f193}, [%rd63+1024];
	fma.rn.f32 	%f196, %f153, %f192, %f152;
	fma.rn.f32 	%f382, %f154, %f193, %f196;
	add.s64 	%rd74, %rd74, 2048;
	add.s64 	%rd73, %rd73, 2048;
	add.s32 	%r322, %r322, 256;
	setp.lt.s32 	%p5, %r322, %r11;
	@%p5 bra 	$L__BB31_7;

	st.local.v4.f32 	[%rd2], {%f389, %f388, %f387, %f386};
	st.local.v4.f32 	[%rd2+16], {%f385, %f384, %f383, %f382};

$L__BB31_9:
	shr.s32 	%r41, %r3, 31;
	shr.u32 	%r42, %r41, 27;
	add.s32 	%r43, %r3, %r42;
	shr.s32 	%r44, %r43, 5;
	shl.b32 	%r45, %r44, 2;
	add.s32 	%r10, %r24, %r45;
	mov.u32 	%r47, 2;
	mov.b32 	%r48, %f389;
	mov.u32 	%r49, 31;
	mov.u32 	%r50, 16;
	mov.u32 	%r51, -1;
	shfl.sync.bfly.b32 	%r52|%p6, %r48, %r50, %r49, %r51;
	mov.b32 	%f197, %r52;
	add.f32 	%f198, %f389, %f197;
	mov.b32 	%r53, %f198;
	mov.u32 	%r54, 8;
	shfl.sync.bfly.b32 	%r55|%p7, %r53, %r54, %r49, %r51;
	mov.b32 	%f199, %r55;
	add.f32 	%f200, %f198, %f199;
	mov.b32 	%r56, %f200;
	mov.u32 	%r57, 4;
	shfl.sync.bfly.b32 	%r58|%p8, %r56, %r57, %r49, %r51;
	mov.b32 	%f201, %r58;
	add.f32 	%f202, %f200, %f201;
	mov.b32 	%r59, %f202;
	shfl.sync.bfly.b32 	%r60|%p9, %r59, %r47, %r49, %r51;
	mov.b32 	%f203, %r60;
	add.f32 	%f204, %f202, %f203;
	mov.b32 	%r61, %f204;
	mov.u32 	%r62, 1;
	shfl.sync.bfly.b32 	%r63|%p10, %r61, %r62, %r49, %r51;
	mov.b32 	%f205, %r63;
	add.f32 	%f206, %f204, %f205;
	st.local.f32 	[%rd2], %f206;
	st.shared.f32 	[%r10], %f206;
	bar.sync 	0;
	@%p1 bra 	$L__BB31_11;

	ld.shared.f32 	%f207, [%r4];
	mov.b32 	%r64, %f207;
	shfl.sync.bfly.b32 	%r68|%p12, %r64, %r50, %r49, %r51;
	mov.b32 	%f208, %r68;
	add.f32 	%f209, %f207, %f208;
	mov.b32 	%r69, %f209;
	shfl.sync.bfly.b32 	%r71|%p13, %r69, %r54, %r49, %r51;
	mov.b32 	%f210, %r71;
	add.f32 	%f211, %f209, %f210;
	mov.b32 	%r72, %f211;
	shfl.sync.bfly.b32 	%r74|%p14, %r72, %r57, %r49, %r51;
	mov.b32 	%f212, %r74;
	add.f32 	%f213, %f211, %f212;
	mov.b32 	%r75, %f213;
	shfl.sync.bfly.b32 	%r77|%p15, %r75, %r47, %r49, %r51;
	mov.b32 	%f214, %r77;
	add.f32 	%f215, %f213, %f214;
	mov.b32 	%r78, %f215;
	shfl.sync.bfly.b32 	%r80|%p16, %r78, %r62, %r49, %r51;
	mov.b32 	%f216, %r80;
	add.f32 	%f217, %f215, %f216;
	st.local.f32 	[%rd2], %f217;

$L__BB31_11:
	bar.sync 	0;
	mov.b32 	%r81, %f388;
	shfl.sync.bfly.b32 	%r85|%p18, %r81, %r50, %r49, %r51;
	mov.b32 	%f218, %r85;
	add.f32 	%f219, %f388, %f218;
	mov.b32 	%r86, %f219;
	shfl.sync.bfly.b32 	%r88|%p19, %r86, %r54, %r49, %r51;
	mov.b32 	%f220, %r88;
	add.f32 	%f221, %f219, %f220;
	mov.b32 	%r89, %f221;
	shfl.sync.bfly.b32 	%r91|%p20, %r89, %r57, %r49, %r51;
	mov.b32 	%f222, %r91;
	add.f32 	%f223, %f221, %f222;
	mov.b32 	%r92, %f223;
	shfl.sync.bfly.b32 	%r94|%p21, %r92, %r47, %r49, %r51;
	mov.b32 	%f224, %r94;
	add.f32 	%f225, %f223, %f224;
	mov.b32 	%r95, %f225;
	shfl.sync.bfly.b32 	%r97|%p22, %r95, %r62, %r49, %r51;
	mov.b32 	%f226, %r97;
	add.f32 	%f227, %f225, %f226;
	st.local.f32 	[%rd2+4], %f227;
	st.shared.f32 	[%r10], %f227;
	bar.sync 	0;
	@%p1 bra 	$L__BB31_13;

	ld.shared.f32 	%f228, [%r4];
	mov.b32 	%r98, %f228;
	mov.u32 	%r99, 31;
	mov.u32 	%r100, 16;
	mov.u32 	%r101, -1;
	shfl.sync.bfly.b32 	%r102|%p23, %r98, %r100, %r99, %r101;
	mov.b32 	%f229, %r102;
	add.f32 	%f230, %f228, %f229;
	mov.b32 	%r103, %f230;
	mov.u32 	%r104, 8;
	shfl.sync.bfly.b32 	%r105|%p24, %r103, %r104, %r99, %r101;
	mov.b32 	%f231, %r105;
	add.f32 	%f232, %f230, %f231;
	mov.b32 	%r106, %f232;
	mov.u32 	%r107, 4;
	shfl.sync.bfly.b32 	%r108|%p25, %r106, %r107, %r99, %r101;
	mov.b32 	%f233, %r108;
	add.f32 	%f234, %f232, %f233;
	mov.b32 	%r109, %f234;
	mov.u32 	%r110, 2;
	shfl.sync.bfly.b32 	%r111|%p26, %r109, %r110, %r99, %r101;
	mov.b32 	%f235, %r111;
	add.f32 	%f236, %f234, %f235;
	mov.b32 	%r112, %f236;
	mov.u32 	%r113, 1;
	shfl.sync.bfly.b32 	%r114|%p27, %r112, %r113, %r99, %r101;
	mov.b32 	%f237, %r114;
	add.f32 	%f238, %f236, %f237;
	st.local.f32 	[%rd2+4], %f238;

$L__BB31_13:
	bar.sync 	0;
	mov.b32 	%r115, %f387;
	mov.u32 	%r116, 31;
	mov.u32 	%r117, 16;
	mov.u32 	%r118, -1;
	shfl.sync.bfly.b32 	%r119|%p29, %r115, %r117, %r116, %r118;
	mov.b32 	%f239, %r119;
	add.f32 	%f240, %f387, %f239;
	mov.b32 	%r120, %f240;
	mov.u32 	%r121, 8;
	shfl.sync.bfly.b32 	%r122|%p30, %r120, %r121, %r116, %r118;
	mov.b32 	%f241, %r122;
	add.f32 	%f242, %f240, %f241;
	mov.b32 	%r123, %f242;
	mov.u32 	%r124, 4;
	shfl.sync.bfly.b32 	%r125|%p31, %r123, %r124, %r116, %r118;
	mov.b32 	%f243, %r125;
	add.f32 	%f244, %f242, %f243;
	mov.b32 	%r126, %f244;
	mov.u32 	%r127, 2;
	shfl.sync.bfly.b32 	%r128|%p32, %r126, %r127, %r116, %r118;
	mov.b32 	%f245, %r128;
	add.f32 	%f246, %f244, %f245;
	mov.b32 	%r129, %f246;
	mov.u32 	%r130, 1;
	shfl.sync.bfly.b32 	%r131|%p33, %r129, %r130, %r116, %r118;
	mov.b32 	%f247, %r131;
	add.f32 	%f248, %f246, %f247;
	st.local.f32 	[%rd2+8], %f248;
	st.shared.f32 	[%r10], %f248;
	bar.sync 	0;
	@%p1 bra 	$L__BB31_15;

	ld.shared.f32 	%f249, [%r4];
	mov.b32 	%r132, %f249;
	shfl.sync.bfly.b32 	%r136|%p34, %r132, %r117, %r116, %r118;
	mov.b32 	%f250, %r136;
	add.f32 	%f251, %f249, %f250;
	mov.b32 	%r137, %f251;
	shfl.sync.bfly.b32 	%r139|%p35, %r137, %r121, %r116, %r118;
	mov.b32 	%f252, %r139;
	add.f32 	%f253, %f251, %f252;
	mov.b32 	%r140, %f253;
	shfl.sync.bfly.b32 	%r142|%p36, %r140, %r124, %r116, %r118;
	mov.b32 	%f254, %r142;
	add.f32 	%f255, %f253, %f254;
	mov.b32 	%r143, %f255;
	shfl.sync.bfly.b32 	%r145|%p37, %r143, %r127, %r116, %r118;
	mov.b32 	%f256, %r145;
	add.f32 	%f257, %f255, %f256;
	mov.b32 	%r146, %f257;
	shfl.sync.bfly.b32 	%r148|%p38, %r146, %r130, %r116, %r118;
	mov.b32 	%f258, %r148;
	add.f32 	%f259, %f257, %f258;
	st.local.f32 	[%rd2+8], %f259;

$L__BB31_15:
	bar.sync 	0;
	mov.b32 	%r149, %f386;
	shfl.sync.bfly.b32 	%r153|%p40, %r149, %r117, %r116, %r118;
	mov.b32 	%f260, %r153;
	add.f32 	%f261, %f386, %f260;
	mov.b32 	%r154, %f261;
	shfl.sync.bfly.b32 	%r156|%p41, %r154, %r121, %r116, %r118;
	mov.b32 	%f262, %r156;
	add.f32 	%f263, %f261, %f262;
	mov.b32 	%r157, %f263;
	shfl.sync.bfly.b32 	%r159|%p42, %r157, %r124, %r116, %r118;
	mov.b32 	%f264, %r159;
	add.f32 	%f265, %f263, %f264;
	mov.b32 	%r160, %f265;
	shfl.sync.bfly.b32 	%r162|%p43, %r160, %r127, %r116, %r118;
	mov.b32 	%f266, %r162;
	add.f32 	%f267, %f265, %f266;
	mov.b32 	%r163, %f267;
	shfl.sync.bfly.b32 	%r165|%p44, %r163, %r130, %r116, %r118;
	mov.b32 	%f268, %r165;
	add.f32 	%f269, %f267, %f268;
	st.local.f32 	[%rd2+12], %f269;
	st.shared.f32 	[%r10], %f269;
	bar.sync 	0;
	@%p1 bra 	$L__BB31_17;

	ld.shared.f32 	%f270, [%r4];
	mov.b32 	%r166, %f270;
	mov.u32 	%r167, 31;
	mov.u32 	%r168, 16;
	mov.u32 	%r169, -1;
	shfl.sync.bfly.b32 	%r170|%p45, %r166, %r168, %r167, %r169;
	mov.b32 	%f271, %r170;
	add.f32 	%f272, %f270, %f271;
	mov.b32 	%r171, %f272;
	mov.u32 	%r172, 8;
	shfl.sync.bfly.b32 	%r173|%p46, %r171, %r172, %r167, %r169;
	mov.b32 	%f273, %r173;
	add.f32 	%f274, %f272, %f273;
	mov.b32 	%r174, %f274;
	mov.u32 	%r175, 4;
	shfl.sync.bfly.b32 	%r176|%p47, %r174, %r175, %r167, %r169;
	mov.b32 	%f275, %r176;
	add.f32 	%f276, %f274, %f275;
	mov.b32 	%r177, %f276;
	mov.u32 	%r178, 2;
	shfl.sync.bfly.b32 	%r179|%p48, %r177, %r178, %r167, %r169;
	mov.b32 	%f277, %r179;
	add.f32 	%f278, %f276, %f277;
	mov.b32 	%r180, %f278;
	mov.u32 	%r181, 1;
	shfl.sync.bfly.b32 	%r182|%p49, %r180, %r181, %r167, %r169;
	mov.b32 	%f279, %r182;
	add.f32 	%f280, %f278, %f279;
	st.local.f32 	[%rd2+12], %f280;

$L__BB31_17:
	bar.sync 	0;
	mov.b32 	%r183, %f385;
	mov.u32 	%r184, 31;
	mov.u32 	%r185, 16;
	mov.u32 	%r186, -1;
	shfl.sync.bfly.b32 	%r187|%p51, %r183, %r185, %r184, %r186;
	mov.b32 	%f281, %r187;
	add.f32 	%f282, %f385, %f281;
	mov.b32 	%r188, %f282;
	mov.u32 	%r189, 8;
	shfl.sync.bfly.b32 	%r190|%p52, %r188, %r189, %r184, %r186;
	mov.b32 	%f283, %r190;
	add.f32 	%f284, %f282, %f283;
	mov.b32 	%r191, %f284;
	mov.u32 	%r192, 4;
	shfl.sync.bfly.b32 	%r193|%p53, %r191, %r192, %r184, %r186;
	mov.b32 	%f285, %r193;
	add.f32 	%f286, %f284, %f285;
	mov.b32 	%r194, %f286;
	mov.u32 	%r195, 2;
	shfl.sync.bfly.b32 	%r196|%p54, %r194, %r195, %r184, %r186;
	mov.b32 	%f287, %r196;
	add.f32 	%f288, %f286, %f287;
	mov.b32 	%r197, %f288;
	mov.u32 	%r198, 1;
	shfl.sync.bfly.b32 	%r199|%p55, %r197, %r198, %r184, %r186;
	mov.b32 	%f289, %r199;
	add.f32 	%f290, %f288, %f289;
	st.local.f32 	[%rd2+16], %f290;
	st.shared.f32 	[%r10], %f290;
	bar.sync 	0;
	@%p1 bra 	$L__BB31_19;

	ld.shared.f32 	%f291, [%r4];
	mov.b32 	%r200, %f291;
	shfl.sync.bfly.b32 	%r204|%p56, %r200, %r185, %r184, %r186;
	mov.b32 	%f292, %r204;
	add.f32 	%f293, %f291, %f292;
	mov.b32 	%r205, %f293;
	shfl.sync.bfly.b32 	%r207|%p57, %r205, %r189, %r184, %r186;
	mov.b32 	%f294, %r207;
	add.f32 	%f295, %f293, %f294;
	mov.b32 	%r208, %f295;
	shfl.sync.bfly.b32 	%r210|%p58, %r208, %r192, %r184, %r186;
	mov.b32 	%f296, %r210;
	add.f32 	%f297, %f295, %f296;
	mov.b32 	%r211, %f297;
	shfl.sync.bfly.b32 	%r213|%p59, %r211, %r195, %r184, %r186;
	mov.b32 	%f298, %r213;
	add.f32 	%f299, %f297, %f298;
	mov.b32 	%r214, %f299;
	shfl.sync.bfly.b32 	%r216|%p60, %r214, %r198, %r184, %r186;
	mov.b32 	%f300, %r216;
	add.f32 	%f301, %f299, %f300;
	st.local.f32 	[%rd2+16], %f301;

$L__BB31_19:
	bar.sync 	0;
	mov.b32 	%r217, %f384;
	shfl.sync.bfly.b32 	%r221|%p62, %r217, %r185, %r184, %r186;
	mov.b32 	%f302, %r221;
	add.f32 	%f303, %f384, %f302;
	mov.b32 	%r222, %f303;
	shfl.sync.bfly.b32 	%r224|%p63, %r222, %r189, %r184, %r186;
	mov.b32 	%f304, %r224;
	add.f32 	%f305, %f303, %f304;
	mov.b32 	%r225, %f305;
	shfl.sync.bfly.b32 	%r227|%p64, %r225, %r192, %r184, %r186;
	mov.b32 	%f306, %r227;
	add.f32 	%f307, %f305, %f306;
	mov.b32 	%r228, %f307;
	shfl.sync.bfly.b32 	%r230|%p65, %r228, %r195, %r184, %r186;
	mov.b32 	%f308, %r230;
	add.f32 	%f309, %f307, %f308;
	mov.b32 	%r231, %f309;
	shfl.sync.bfly.b32 	%r233|%p66, %r231, %r198, %r184, %r186;
	mov.b32 	%f310, %r233;
	add.f32 	%f311, %f309, %f310;
	st.local.f32 	[%rd2+20], %f311;
	st.shared.f32 	[%r10], %f311;
	bar.sync 	0;
	@%p1 bra 	$L__BB31_21;

	ld.shared.f32 	%f312, [%r4];
	mov.b32 	%r234, %f312;
	mov.u32 	%r235, 31;
	mov.u32 	%r236, 16;
	mov.u32 	%r237, -1;
	shfl.sync.bfly.b32 	%r238|%p67, %r234, %r236, %r235, %r237;
	mov.b32 	%f313, %r238;
	add.f32 	%f314, %f312, %f313;
	mov.b32 	%r239, %f314;
	mov.u32 	%r240, 8;
	shfl.sync.bfly.b32 	%r241|%p68, %r239, %r240, %r235, %r237;
	mov.b32 	%f315, %r241;
	add.f32 	%f316, %f314, %f315;
	mov.b32 	%r242, %f316;
	mov.u32 	%r243, 4;
	shfl.sync.bfly.b32 	%r244|%p69, %r242, %r243, %r235, %r237;
	mov.b32 	%f317, %r244;
	add.f32 	%f318, %f316, %f317;
	mov.b32 	%r245, %f318;
	mov.u32 	%r246, 2;
	shfl.sync.bfly.b32 	%r247|%p70, %r245, %r246, %r235, %r237;
	mov.b32 	%f319, %r247;
	add.f32 	%f320, %f318, %f319;
	mov.b32 	%r248, %f320;
	mov.u32 	%r249, 1;
	shfl.sync.bfly.b32 	%r250|%p71, %r248, %r249, %r235, %r237;
	mov.b32 	%f321, %r250;
	add.f32 	%f322, %f320, %f321;
	st.local.f32 	[%rd2+20], %f322;

$L__BB31_21:
	bar.sync 	0;
	mov.b32 	%r251, %f383;
	mov.u32 	%r252, 31;
	mov.u32 	%r253, 16;
	mov.u32 	%r254, -1;
	shfl.sync.bfly.b32 	%r255|%p73, %r251, %r253, %r252, %r254;
	mov.b32 	%f323, %r255;
	add.f32 	%f324, %f383, %f323;
	mov.b32 	%r256, %f324;
	mov.u32 	%r257, 8;
	shfl.sync.bfly.b32 	%r258|%p74, %r256, %r257, %r252, %r254;
	mov.b32 	%f325, %r258;
	add.f32 	%f326, %f324, %f325;
	mov.b32 	%r259, %f326;
	mov.u32 	%r260, 4;
	shfl.sync.bfly.b32 	%r261|%p75, %r259, %r260, %r252, %r254;
	mov.b32 	%f327, %r261;
	add.f32 	%f328, %f326, %f327;
	mov.b32 	%r262, %f328;
	mov.u32 	%r263, 2;
	shfl.sync.bfly.b32 	%r264|%p76, %r262, %r263, %r252, %r254;
	mov.b32 	%f329, %r264;
	add.f32 	%f330, %f328, %f329;
	mov.b32 	%r265, %f330;
	mov.u32 	%r266, 1;
	shfl.sync.bfly.b32 	%r267|%p77, %r265, %r266, %r252, %r254;
	mov.b32 	%f331, %r267;
	add.f32 	%f332, %f330, %f331;
	st.local.f32 	[%rd2+24], %f332;
	st.shared.f32 	[%r10], %f332;
	bar.sync 	0;
	@%p1 bra 	$L__BB31_23;

	ld.shared.f32 	%f333, [%r4];
	mov.b32 	%r268, %f333;
	shfl.sync.bfly.b32 	%r272|%p78, %r268, %r253, %r252, %r254;
	mov.b32 	%f334, %r272;
	add.f32 	%f335, %f333, %f334;
	mov.b32 	%r273, %f335;
	shfl.sync.bfly.b32 	%r275|%p79, %r273, %r257, %r252, %r254;
	mov.b32 	%f336, %r275;
	add.f32 	%f337, %f335, %f336;
	mov.b32 	%r276, %f337;
	shfl.sync.bfly.b32 	%r278|%p80, %r276, %r260, %r252, %r254;
	mov.b32 	%f338, %r278;
	add.f32 	%f339, %f337, %f338;
	mov.b32 	%r279, %f339;
	shfl.sync.bfly.b32 	%r281|%p81, %r279, %r263, %r252, %r254;
	mov.b32 	%f340, %r281;
	add.f32 	%f341, %f339, %f340;
	mov.b32 	%r282, %f341;
	shfl.sync.bfly.b32 	%r284|%p82, %r282, %r266, %r252, %r254;
	mov.b32 	%f342, %r284;
	add.f32 	%f343, %f341, %f342;
	st.local.f32 	[%rd2+24], %f343;

$L__BB31_23:
	bar.sync 	0;
	mov.b32 	%r285, %f382;
	shfl.sync.bfly.b32 	%r289|%p84, %r285, %r253, %r252, %r254;
	mov.b32 	%f344, %r289;
	add.f32 	%f345, %f382, %f344;
	mov.b32 	%r290, %f345;
	shfl.sync.bfly.b32 	%r292|%p85, %r290, %r257, %r252, %r254;
	mov.b32 	%f346, %r292;
	add.f32 	%f347, %f345, %f346;
	mov.b32 	%r293, %f347;
	shfl.sync.bfly.b32 	%r295|%p86, %r293, %r260, %r252, %r254;
	mov.b32 	%f348, %r295;
	add.f32 	%f349, %f347, %f348;
	mov.b32 	%r296, %f349;
	shfl.sync.bfly.b32 	%r298|%p87, %r296, %r263, %r252, %r254;
	mov.b32 	%f350, %r298;
	add.f32 	%f351, %f349, %f350;
	mov.b32 	%r299, %f351;
	shfl.sync.bfly.b32 	%r301|%p88, %r299, %r266, %r252, %r254;
	mov.b32 	%f352, %r301;
	add.f32 	%f353, %f351, %f352;
	st.local.f32 	[%rd2+28], %f353;
	st.shared.f32 	[%r10], %f353;
	bar.sync 	0;
	@%p1 bra 	$L__BB31_25;

	ld.shared.f32 	%f354, [%r4];
	mov.b32 	%r302, %f354;
	mov.u32 	%r303, 31;
	mov.u32 	%r304, 16;
	mov.u32 	%r305, -1;
	shfl.sync.bfly.b32 	%r306|%p89, %r302, %r304, %r303, %r305;
	mov.b32 	%f355, %r306;
	add.f32 	%f356, %f354, %f355;
	mov.b32 	%r307, %f356;
	mov.u32 	%r308, 8;
	shfl.sync.bfly.b32 	%r309|%p90, %r307, %r308, %r303, %r305;
	mov.b32 	%f357, %r309;
	add.f32 	%f358, %f356, %f357;
	mov.b32 	%r310, %f358;
	mov.u32 	%r311, 4;
	shfl.sync.bfly.b32 	%r312|%p91, %r310, %r311, %r303, %r305;
	mov.b32 	%f359, %r312;
	add.f32 	%f360, %f358, %f359;
	mov.b32 	%r313, %f360;
	mov.u32 	%r314, 2;
	shfl.sync.bfly.b32 	%r315|%p92, %r313, %r314, %r303, %r305;
	mov.b32 	%f361, %r315;
	add.f32 	%f362, %f360, %f361;
	mov.b32 	%r316, %f362;
	mov.u32 	%r317, 1;
	shfl.sync.bfly.b32 	%r318|%p93, %r316, %r317, %r303, %r305;
	mov.b32 	%f363, %r318;
	add.f32 	%f364, %f362, %f363;
	st.local.f32 	[%rd2+28], %f364;

$L__BB31_25:
	bar.sync 	0;
	setp.gt.s32 	%p94, %r3, 7;
	@%p94 bra 	$L__BB31_27;

	mul.wide.s32 	%rd65, %r3, 4;
	add.s64 	%rd66, %rd2, %rd65;
	ld.local.f32 	%f365, [%rd66];
	mad.lo.s32 	%r319, %r3, %r13, %r2;
	cvt.s64.s32 	%rd67, %r319;
	mul.lo.s32 	%r320, %r1, %r14;
	cvt.s64.s32 	%rd68, %r320;
	add.s64 	%rd69, %rd68, %rd67;
	cvta.to.global.u64 	%rd70, %rd21;
	shl.b64 	%rd71, %rd69, 2;
	add.s64 	%rd72, %rd70, %rd71;
	st.global.f32 	[%rd72], %f365;

$L__BB31_27:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_1_bs_160
.visible .entry ggml_matvec_f32_ncols_1_bs_160(
	.param .u64 ggml_matvec_f32_ncols_1_bs_160_param_0,
	.param .u64 ggml_matvec_f32_ncols_1_bs_160_param_1,
	.param .u64 ggml_matvec_f32_ncols_1_bs_160_param_2,
	.param .u32 ggml_matvec_f32_ncols_1_bs_160_param_3,
	.param .u32 ggml_matvec_f32_ncols_1_bs_160_param_4,
	.param .u32 ggml_matvec_f32_ncols_1_bs_160_param_5,
	.param .u32 ggml_matvec_f32_ncols_1_bs_160_param_6,
	.param .u32 ggml_matvec_f32_ncols_1_bs_160_param_7,
	.param .u32 ggml_matvec_f32_ncols_1_bs_160_param_8,
	.param .u32 ggml_matvec_f32_ncols_1_bs_160_param_9,
	.param .u32 ggml_matvec_f32_ncols_1_bs_160_param_10,
	.param .u32 ggml_matvec_f32_ncols_1_bs_160_param_11
)
{
	.reg .pred 	%p<19>;
	.reg .f32 	%f<88>;
	.reg .b32 	%r<79>;
	.reg .b64 	%rd<44>;


	ld.param.u64 	%rd18, [ggml_matvec_f32_ncols_1_bs_160_param_0];
	ld.param.u64 	%rd19, [ggml_matvec_f32_ncols_1_bs_160_param_1];
	ld.param.u64 	%rd17, [ggml_matvec_f32_ncols_1_bs_160_param_2];
	ld.param.u32 	%r13, [ggml_matvec_f32_ncols_1_bs_160_param_3];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_1_bs_160_param_5];
	ld.param.u32 	%r14, [ggml_matvec_f32_ncols_1_bs_160_param_7];
	ld.param.u32 	%r17, [ggml_matvec_f32_ncols_1_bs_160_param_8];
	ld.param.u32 	%r18, [ggml_matvec_f32_ncols_1_bs_160_param_9];
	ld.param.u32 	%r19, [ggml_matvec_f32_ncols_1_bs_160_param_10];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_1_bs_160_param_11];
	cvta.to.global.u64 	%rd1, %rd19;
	cvta.to.global.u64 	%rd2, %rd18;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r20, %r1, %r17;
	mov.u32 	%r21, %ctaid.x;
	mul.lo.s32 	%r22, %r21, %r16;
	mad.lo.s32 	%r23, %r20, %r18, %r22;
	cvt.s64.s32 	%rd3, %r23;
	mul.lo.s32 	%r24, %r1, %r19;
	cvt.s64.s32 	%rd4, %r24;
	mov.u32 	%r2, %tid.x;
	setp.gt.s32 	%p1, %r2, 31;
	shl.b32 	%r25, %r2, 2;
	mov.u32 	%r26, data_mmv;
	add.s32 	%r3, %r26, %r25;
	@%p1 bra 	$L__BB32_2;

	mov.u32 	%r27, 0;
	st.shared.u32 	[%r3], %r27;

$L__BB32_2:
	bar.sync 	0;
	setp.ge.s32 	%p2, %r2, %r13;
	mov.f32 	%f86, 0f00000000;
	@%p2 bra 	$L__BB32_9;

	not.b32 	%r28, %r2;
	add.s32 	%r4, %r28, %r13;
	mul.wide.u32 	%rd20, %r4, -858993459;
	shr.u64 	%rd21, %rd20, 39;
	cvt.u32.u64 	%r29, %rd21;
	add.s32 	%r30, %r29, 1;
	and.b32  	%r76, %r30, 3;
	setp.eq.s32 	%p3, %r76, 0;
	mov.f32 	%f86, 0f00000000;
	mov.u32 	%r77, %r2;
	@%p3 bra 	$L__BB32_6;

	mul.wide.s32 	%rd22, %r2, 2;
	add.s64 	%rd23, %rd22, %rd4;
	shl.b64 	%rd24, %rd23, 2;
	add.s64 	%rd41, %rd1, %rd24;
	add.s64 	%rd25, %rd22, %rd3;
	shl.b64 	%rd26, %rd25, 2;
	add.s64 	%rd40, %rd2, %rd26;
	mov.f32 	%f86, 0f00000000;
	mov.u32 	%r77, %r2;

$L__BB32_5:
	.pragma "nounroll";
	ld.global.nc.v2.f32 	{%f15, %f16}, [%rd40];
	ld.global.nc.v2.f32 	{%f19, %f20}, [%rd41];
	fma.rn.f32 	%f23, %f15, %f19, %f86;
	fma.rn.f32 	%f86, %f16, %f20, %f23;
	add.s32 	%r77, %r77, 160;
	add.s64 	%rd41, %rd41, 1280;
	add.s64 	%rd40, %rd40, 1280;
	add.s32 	%r76, %r76, -1;
	setp.ne.s32 	%p4, %r76, 0;
	@%p4 bra 	$L__BB32_5;

$L__BB32_6:
	setp.lt.u32 	%p5, %r4, 480;
	@%p5 bra 	$L__BB32_9;

	mul.wide.s32 	%rd27, %r77, 2;
	add.s64 	%rd28, %rd27, %rd3;
	shl.b64 	%rd29, %rd28, 2;
	add.s64 	%rd30, %rd2, %rd29;
	add.s64 	%rd43, %rd30, 2560;
	add.s64 	%rd31, %rd27, %rd4;
	shl.b64 	%rd32, %rd31, 2;
	add.s64 	%rd33, %rd1, %rd32;
	add.s64 	%rd42, %rd33, 2560;

$L__BB32_8:
	ld.global.nc.v2.f32 	{%f24, %f25}, [%rd43+-2560];
	ld.global.nc.v2.f32 	{%f28, %f29}, [%rd42+-2560];
	fma.rn.f32 	%f32, %f24, %f28, %f86;
	fma.rn.f32 	%f33, %f25, %f29, %f32;
	ld.global.nc.v2.f32 	{%f34, %f35}, [%rd43+-1280];
	ld.global.nc.v2.f32 	{%f38, %f39}, [%rd42+-1280];
	fma.rn.f32 	%f42, %f34, %f38, %f33;
	fma.rn.f32 	%f43, %f35, %f39, %f42;
	ld.global.nc.v2.f32 	{%f44, %f45}, [%rd43];
	ld.global.nc.v2.f32 	{%f48, %f49}, [%rd42];
	fma.rn.f32 	%f52, %f44, %f48, %f43;
	fma.rn.f32 	%f53, %f45, %f49, %f52;
	ld.global.nc.v2.f32 	{%f54, %f55}, [%rd43+1280];
	ld.global.nc.v2.f32 	{%f58, %f59}, [%rd42+1280];
	fma.rn.f32 	%f62, %f54, %f58, %f53;
	fma.rn.f32 	%f86, %f55, %f59, %f62;
	add.s64 	%rd43, %rd43, 5120;
	add.s64 	%rd42, %rd42, 5120;
	add.s32 	%r77, %r77, 640;
	setp.lt.s32 	%p6, %r77, %r13;
	@%p6 bra 	$L__BB32_8;

$L__BB32_9:
	mov.b32 	%r31, %f86;
	mov.u32 	%r32, 31;
	mov.u32 	%r33, 16;
	mov.u32 	%r34, -1;
	shfl.sync.bfly.b32 	%r35|%p7, %r31, %r33, %r32, %r34;
	mov.b32 	%f63, %r35;
	add.f32 	%f64, %f86, %f63;
	mov.b32 	%r36, %f64;
	mov.u32 	%r37, 8;
	shfl.sync.bfly.b32 	%r38|%p8, %r36, %r37, %r32, %r34;
	mov.b32 	%f65, %r38;
	add.f32 	%f66, %f64, %f65;
	mov.b32 	%r39, %f66;
	mov.u32 	%r40, 4;
	shfl.sync.bfly.b32 	%r41|%p9, %r39, %r40, %r32, %r34;
	mov.b32 	%f67, %r41;
	add.f32 	%f68, %f66, %f67;
	mov.b32 	%r42, %f68;
	mov.u32 	%r43, 2;
	shfl.sync.bfly.b32 	%r44|%p10, %r42, %r43, %r32, %r34;
	mov.b32 	%f69, %r44;
	add.f32 	%f70, %f68, %f69;
	mov.b32 	%r45, %f70;
	mov.u32 	%r46, 1;
	shfl.sync.bfly.b32 	%r47|%p11, %r45, %r46, %r32, %r34;
	mov.b32 	%f71, %r47;
	add.f32 	%f87, %f70, %f71;
	shr.s32 	%r48, %r2, 31;
	shr.u32 	%r49, %r48, 27;
	add.s32 	%r50, %r2, %r49;
	shr.s32 	%r51, %r50, 5;
	shl.b32 	%r52, %r51, 2;
	add.s32 	%r54, %r26, %r52;
	st.shared.f32 	[%r54], %f87;
	bar.sync 	0;
	@%p1 bra 	$L__BB32_11;

	ld.shared.f32 	%f72, [%r3];
	mov.b32 	%r55, %f72;
	shfl.sync.bfly.b32 	%r59|%p13, %r55, %r33, %r32, %r34;
	mov.b32 	%f73, %r59;
	add.f32 	%f74, %f72, %f73;
	mov.b32 	%r60, %f74;
	shfl.sync.bfly.b32 	%r62|%p14, %r60, %r37, %r32, %r34;
	mov.b32 	%f75, %r62;
	add.f32 	%f76, %f74, %f75;
	mov.b32 	%r63, %f76;
	shfl.sync.bfly.b32 	%r65|%p15, %r63, %r40, %r32, %r34;
	mov.b32 	%f77, %r65;
	add.f32 	%f78, %f76, %f77;
	mov.b32 	%r66, %f78;
	shfl.sync.bfly.b32 	%r68|%p16, %r66, %r43, %r32, %r34;
	mov.b32 	%f79, %r68;
	add.f32 	%f80, %f78, %f79;
	mov.b32 	%r69, %f80;
	shfl.sync.bfly.b32 	%r71|%p17, %r69, %r46, %r32, %r34;
	mov.b32 	%f81, %r71;
	add.f32 	%f87, %f80, %f81;

$L__BB32_11:
	bar.sync 	0;
	setp.gt.s32 	%p18, %r2, 0;
	@%p18 bra 	$L__BB32_13;

	mad.lo.s32 	%r73, %r2, %r14, %r21;
	cvt.s64.s32 	%rd34, %r73;
	mul.lo.s32 	%r74, %r1, %r15;
	cvt.s64.s32 	%rd35, %r74;
	add.s64 	%rd36, %rd35, %rd34;
	cvta.to.global.u64 	%rd37, %rd17;
	shl.b64 	%rd38, %rd36, 2;
	add.s64 	%rd39, %rd37, %rd38;
	st.global.f32 	[%rd39], %f87;

$L__BB32_13:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_2_bs_160
.visible .entry ggml_matvec_f32_ncols_2_bs_160(
	.param .u64 ggml_matvec_f32_ncols_2_bs_160_param_0,
	.param .u64 ggml_matvec_f32_ncols_2_bs_160_param_1,
	.param .u64 ggml_matvec_f32_ncols_2_bs_160_param_2,
	.param .u32 ggml_matvec_f32_ncols_2_bs_160_param_3,
	.param .u32 ggml_matvec_f32_ncols_2_bs_160_param_4,
	.param .u32 ggml_matvec_f32_ncols_2_bs_160_param_5,
	.param .u32 ggml_matvec_f32_ncols_2_bs_160_param_6,
	.param .u32 ggml_matvec_f32_ncols_2_bs_160_param_7,
	.param .u32 ggml_matvec_f32_ncols_2_bs_160_param_8,
	.param .u32 ggml_matvec_f32_ncols_2_bs_160_param_9,
	.param .u32 ggml_matvec_f32_ncols_2_bs_160_param_10,
	.param .u32 ggml_matvec_f32_ncols_2_bs_160_param_11
)
{
	.local .align 8 .b8 	__local_depot33[8];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<30>;
	.reg .f32 	%f<146>;
	.reg .b32 	%r<113>;
	.reg .b64 	%rd<66>;


	mov.u64 	%SPL, __local_depot33;
	ld.param.u64 	%rd27, [ggml_matvec_f32_ncols_2_bs_160_param_0];
	ld.param.u64 	%rd28, [ggml_matvec_f32_ncols_2_bs_160_param_1];
	ld.param.u64 	%rd26, [ggml_matvec_f32_ncols_2_bs_160_param_2];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_2_bs_160_param_3];
	ld.param.u32 	%r19, [ggml_matvec_f32_ncols_2_bs_160_param_5];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_2_bs_160_param_6];
	ld.param.u32 	%r17, [ggml_matvec_f32_ncols_2_bs_160_param_7];
	ld.param.u32 	%r20, [ggml_matvec_f32_ncols_2_bs_160_param_8];
	ld.param.u32 	%r21, [ggml_matvec_f32_ncols_2_bs_160_param_9];
	ld.param.u32 	%r22, [ggml_matvec_f32_ncols_2_bs_160_param_10];
	ld.param.u32 	%r18, [ggml_matvec_f32_ncols_2_bs_160_param_11];
	cvta.to.global.u64 	%rd1, %rd28;
	cvta.to.global.u64 	%rd2, %rd27;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r23, %r1, %r20;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r24, %r2, %r19;
	mad.lo.s32 	%r25, %r23, %r21, %r24;
	cvt.s64.s32 	%rd4, %r25;
	mul.lo.s32 	%r26, %r1, %r22;
	cvt.s64.s32 	%rd5, %r26;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r27, %r3, 2;
	mov.u32 	%r28, data_mmv;
	add.s32 	%r4, %r28, %r27;
	@%p1 bra 	$L__BB33_2;

	mov.u32 	%r29, 0;
	st.shared.u32 	[%r4], %r29;

$L__BB33_2:
	bar.sync 	0;
	mov.f32 	%f144, 0f00000000;
	st.local.v2.f32 	[%rd3], {%f144, %f144};
	setp.ge.s32 	%p2, %r3, %r15;
	mov.f32 	%f145, %f144;
	@%p2 bra 	$L__BB33_11;

	not.b32 	%r30, %r3;
	add.s32 	%r5, %r30, %r15;
	mul.wide.u32 	%rd30, %r5, -858993459;
	shr.u64 	%rd31, %rd30, 39;
	cvt.u32.u64 	%r31, %rd31;
	add.s32 	%r32, %r31, 1;
	and.b32  	%r110, %r32, 3;
	setp.eq.s32 	%p3, %r110, 0;
	mov.f32 	%f144, 0f00000000;
	mov.u32 	%r111, %r3;
	@%p3 bra 	$L__BB33_7;

	mul.wide.s32 	%rd32, %r16, 2;
	mul.wide.s32 	%rd33, %r3, 2;
	add.s64 	%rd34, %rd32, %rd33;
	add.s64 	%rd35, %rd34, %rd5;
	shl.b64 	%rd36, %rd35, 2;
	add.s64 	%rd62, %rd1, %rd36;
	add.s64 	%rd37, %rd33, %rd5;
	shl.b64 	%rd38, %rd37, 2;
	add.s64 	%rd61, %rd1, %rd38;
	add.s64 	%rd39, %rd33, %rd4;
	shl.b64 	%rd40, %rd39, 2;
	add.s64 	%rd60, %rd2, %rd40;
	mov.f32 	%f144, 0f00000000;
	mov.f32 	%f145, %f144;
	mov.u32 	%r111, %r3;

$L__BB33_5:
	.pragma "nounroll";
	ld.global.nc.v2.f32 	{%f19, %f20}, [%rd60];
	ld.global.nc.v2.f32 	{%f23, %f24}, [%rd61];
	fma.rn.f32 	%f27, %f19, %f23, %f145;
	fma.rn.f32 	%f145, %f20, %f24, %f27;
	ld.global.nc.v2.f32 	{%f28, %f29}, [%rd62];
	fma.rn.f32 	%f32, %f19, %f28, %f144;
	fma.rn.f32 	%f144, %f20, %f29, %f32;
	add.s32 	%r111, %r111, 160;
	add.s64 	%rd62, %rd62, 1280;
	add.s64 	%rd61, %rd61, 1280;
	add.s64 	%rd60, %rd60, 1280;
	add.s32 	%r110, %r110, -1;
	setp.ne.s32 	%p4, %r110, 0;
	@%p4 bra 	$L__BB33_5;

	st.local.v2.f32 	[%rd3], {%f145, %f144};

$L__BB33_7:
	setp.lt.u32 	%p5, %r5, 480;
	@%p5 bra 	$L__BB33_11;

	mul.wide.s32 	%rd41, %r111, 2;
	add.s64 	%rd42, %rd41, %rd4;
	shl.b64 	%rd43, %rd42, 2;
	add.s64 	%rd44, %rd2, %rd43;
	add.s64 	%rd65, %rd44, 2560;
	add.s64 	%rd45, %rd41, %rd5;
	shl.b64 	%rd46, %rd45, 2;
	add.s64 	%rd47, %rd1, %rd46;
	add.s64 	%rd64, %rd47, 3840;
	mul.wide.s32 	%rd48, %r16, 2;
	add.s64 	%rd49, %rd45, %rd48;
	shl.b64 	%rd50, %rd49, 2;
	add.s64 	%rd51, %rd1, %rd50;
	add.s64 	%rd63, %rd51, 2560;

$L__BB33_9:
	ld.global.nc.v2.f32 	{%f33, %f34}, [%rd65+-2560];
	ld.global.nc.v2.f32 	{%f37, %f38}, [%rd64+-3840];
	fma.rn.f32 	%f41, %f33, %f37, %f145;
	fma.rn.f32 	%f42, %f34, %f38, %f41;
	ld.global.nc.v2.f32 	{%f43, %f44}, [%rd63+-2560];
	fma.rn.f32 	%f47, %f33, %f43, %f144;
	fma.rn.f32 	%f48, %f34, %f44, %f47;
	ld.global.nc.v2.f32 	{%f49, %f50}, [%rd65+-1280];
	ld.global.nc.v2.f32 	{%f53, %f54}, [%rd64+-2560];
	fma.rn.f32 	%f57, %f49, %f53, %f42;
	fma.rn.f32 	%f58, %f50, %f54, %f57;
	ld.global.nc.v2.f32 	{%f59, %f60}, [%rd63+-1280];
	fma.rn.f32 	%f63, %f49, %f59, %f48;
	fma.rn.f32 	%f64, %f50, %f60, %f63;
	ld.global.nc.v2.f32 	{%f65, %f66}, [%rd65];
	ld.global.nc.v2.f32 	{%f69, %f70}, [%rd64+-1280];
	fma.rn.f32 	%f73, %f65, %f69, %f58;
	fma.rn.f32 	%f74, %f66, %f70, %f73;
	ld.global.nc.v2.f32 	{%f75, %f76}, [%rd63];
	fma.rn.f32 	%f79, %f65, %f75, %f64;
	fma.rn.f32 	%f80, %f66, %f76, %f79;
	ld.global.nc.v2.f32 	{%f81, %f82}, [%rd65+1280];
	ld.global.nc.v2.f32 	{%f85, %f86}, [%rd64];
	fma.rn.f32 	%f89, %f81, %f85, %f74;
	fma.rn.f32 	%f145, %f82, %f86, %f89;
	ld.global.nc.v2.f32 	{%f90, %f91}, [%rd63+1280];
	fma.rn.f32 	%f94, %f81, %f90, %f80;
	fma.rn.f32 	%f144, %f82, %f91, %f94;
	add.s64 	%rd65, %rd65, 5120;
	add.s64 	%rd64, %rd64, 5120;
	add.s64 	%rd63, %rd63, 5120;
	add.s32 	%r111, %r111, 640;
	setp.lt.s32 	%p6, %r111, %r15;
	@%p6 bra 	$L__BB33_9;

	st.local.v2.f32 	[%rd3], {%f145, %f144};

$L__BB33_11:
	shr.s32 	%r33, %r3, 31;
	shr.u32 	%r34, %r33, 27;
	add.s32 	%r35, %r3, %r34;
	shr.s32 	%r36, %r35, 5;
	shl.b32 	%r37, %r36, 2;
	add.s32 	%r14, %r28, %r37;
	mov.u32 	%r39, 2;
	mov.b32 	%r40, %f145;
	mov.u32 	%r41, 31;
	mov.u32 	%r42, 16;
	mov.u32 	%r43, -1;
	shfl.sync.bfly.b32 	%r44|%p7, %r40, %r42, %r41, %r43;
	mov.b32 	%f95, %r44;
	add.f32 	%f96, %f145, %f95;
	mov.b32 	%r45, %f96;
	mov.u32 	%r46, 8;
	shfl.sync.bfly.b32 	%r47|%p8, %r45, %r46, %r41, %r43;
	mov.b32 	%f97, %r47;
	add.f32 	%f98, %f96, %f97;
	mov.b32 	%r48, %f98;
	mov.u32 	%r49, 4;
	shfl.sync.bfly.b32 	%r50|%p9, %r48, %r49, %r41, %r43;
	mov.b32 	%f99, %r50;
	add.f32 	%f100, %f98, %f99;
	mov.b32 	%r51, %f100;
	shfl.sync.bfly.b32 	%r52|%p10, %r51, %r39, %r41, %r43;
	mov.b32 	%f101, %r52;
	add.f32 	%f102, %f100, %f101;
	mov.b32 	%r53, %f102;
	mov.u32 	%r54, 1;
	shfl.sync.bfly.b32 	%r55|%p11, %r53, %r54, %r41, %r43;
	mov.b32 	%f103, %r55;
	add.f32 	%f104, %f102, %f103;
	st.local.f32 	[%rd3], %f104;
	st.shared.f32 	[%r14], %f104;
	bar.sync 	0;
	@%p1 bra 	$L__BB33_13;

	ld.shared.f32 	%f105, [%r4];
	mov.b32 	%r56, %f105;
	shfl.sync.bfly.b32 	%r60|%p13, %r56, %r42, %r41, %r43;
	mov.b32 	%f106, %r60;
	add.f32 	%f107, %f105, %f106;
	mov.b32 	%r61, %f107;
	shfl.sync.bfly.b32 	%r63|%p14, %r61, %r46, %r41, %r43;
	mov.b32 	%f108, %r63;
	add.f32 	%f109, %f107, %f108;
	mov.b32 	%r64, %f109;
	shfl.sync.bfly.b32 	%r66|%p15, %r64, %r49, %r41, %r43;
	mov.b32 	%f110, %r66;
	add.f32 	%f111, %f109, %f110;
	mov.b32 	%r67, %f111;
	shfl.sync.bfly.b32 	%r69|%p16, %r67, %r39, %r41, %r43;
	mov.b32 	%f112, %r69;
	add.f32 	%f113, %f111, %f112;
	mov.b32 	%r70, %f113;
	shfl.sync.bfly.b32 	%r72|%p17, %r70, %r54, %r41, %r43;
	mov.b32 	%f114, %r72;
	add.f32 	%f115, %f113, %f114;
	st.local.f32 	[%rd3], %f115;

$L__BB33_13:
	bar.sync 	0;
	mov.b32 	%r73, %f144;
	shfl.sync.bfly.b32 	%r77|%p19, %r73, %r42, %r41, %r43;
	mov.b32 	%f116, %r77;
	add.f32 	%f117, %f144, %f116;
	mov.b32 	%r78, %f117;
	shfl.sync.bfly.b32 	%r80|%p20, %r78, %r46, %r41, %r43;
	mov.b32 	%f118, %r80;
	add.f32 	%f119, %f117, %f118;
	mov.b32 	%r81, %f119;
	shfl.sync.bfly.b32 	%r83|%p21, %r81, %r49, %r41, %r43;
	mov.b32 	%f120, %r83;
	add.f32 	%f121, %f119, %f120;
	mov.b32 	%r84, %f121;
	shfl.sync.bfly.b32 	%r86|%p22, %r84, %r39, %r41, %r43;
	mov.b32 	%f122, %r86;
	add.f32 	%f123, %f121, %f122;
	mov.b32 	%r87, %f123;
	shfl.sync.bfly.b32 	%r89|%p23, %r87, %r54, %r41, %r43;
	mov.b32 	%f124, %r89;
	add.f32 	%f125, %f123, %f124;
	st.local.f32 	[%rd3+4], %f125;
	st.shared.f32 	[%r14], %f125;
	bar.sync 	0;
	@%p1 bra 	$L__BB33_15;

	ld.shared.f32 	%f126, [%r4];
	mov.b32 	%r90, %f126;
	mov.u32 	%r91, 31;
	mov.u32 	%r92, 16;
	mov.u32 	%r93, -1;
	shfl.sync.bfly.b32 	%r94|%p24, %r90, %r92, %r91, %r93;
	mov.b32 	%f127, %r94;
	add.f32 	%f128, %f126, %f127;
	mov.b32 	%r95, %f128;
	mov.u32 	%r96, 8;
	shfl.sync.bfly.b32 	%r97|%p25, %r95, %r96, %r91, %r93;
	mov.b32 	%f129, %r97;
	add.f32 	%f130, %f128, %f129;
	mov.b32 	%r98, %f130;
	mov.u32 	%r99, 4;
	shfl.sync.bfly.b32 	%r100|%p26, %r98, %r99, %r91, %r93;
	mov.b32 	%f131, %r100;
	add.f32 	%f132, %f130, %f131;
	mov.b32 	%r101, %f132;
	mov.u32 	%r102, 2;
	shfl.sync.bfly.b32 	%r103|%p27, %r101, %r102, %r91, %r93;
	mov.b32 	%f133, %r103;
	add.f32 	%f134, %f132, %f133;
	mov.b32 	%r104, %f134;
	mov.u32 	%r105, 1;
	shfl.sync.bfly.b32 	%r106|%p28, %r104, %r105, %r91, %r93;
	mov.b32 	%f135, %r106;
	add.f32 	%f136, %f134, %f135;
	st.local.f32 	[%rd3+4], %f136;

$L__BB33_15:
	bar.sync 	0;
	setp.gt.s32 	%p29, %r3, 1;
	@%p29 bra 	$L__BB33_17;

	mul.wide.s32 	%rd52, %r3, 4;
	add.s64 	%rd53, %rd3, %rd52;
	ld.local.f32 	%f137, [%rd53];
	mad.lo.s32 	%r107, %r3, %r17, %r2;
	cvt.s64.s32 	%rd54, %r107;
	mul.lo.s32 	%r108, %r1, %r18;
	cvt.s64.s32 	%rd55, %r108;
	add.s64 	%rd56, %rd55, %rd54;
	cvta.to.global.u64 	%rd57, %rd26;
	shl.b64 	%rd58, %rd56, 2;
	add.s64 	%rd59, %rd57, %rd58;
	st.global.f32 	[%rd59], %f137;

$L__BB33_17:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_3_bs_160
.visible .entry ggml_matvec_f32_ncols_3_bs_160(
	.param .u64 ggml_matvec_f32_ncols_3_bs_160_param_0,
	.param .u64 ggml_matvec_f32_ncols_3_bs_160_param_1,
	.param .u64 ggml_matvec_f32_ncols_3_bs_160_param_2,
	.param .u32 ggml_matvec_f32_ncols_3_bs_160_param_3,
	.param .u32 ggml_matvec_f32_ncols_3_bs_160_param_4,
	.param .u32 ggml_matvec_f32_ncols_3_bs_160_param_5,
	.param .u32 ggml_matvec_f32_ncols_3_bs_160_param_6,
	.param .u32 ggml_matvec_f32_ncols_3_bs_160_param_7,
	.param .u32 ggml_matvec_f32_ncols_3_bs_160_param_8,
	.param .u32 ggml_matvec_f32_ncols_3_bs_160_param_9,
	.param .u32 ggml_matvec_f32_ncols_3_bs_160_param_10,
	.param .u32 ggml_matvec_f32_ncols_3_bs_160_param_11
)
{
	.local .align 4 .b8 	__local_depot34[12];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<41>;
	.reg .f32 	%f<208>;
	.reg .b32 	%r<154>;
	.reg .b64 	%rd<74>;


	mov.u64 	%SPL, __local_depot34;
	ld.param.u64 	%rd29, [ggml_matvec_f32_ncols_3_bs_160_param_0];
	ld.param.u64 	%rd30, [ggml_matvec_f32_ncols_3_bs_160_param_1];
	ld.param.u64 	%rd28, [ggml_matvec_f32_ncols_3_bs_160_param_2];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_3_bs_160_param_3];
	ld.param.u32 	%r19, [ggml_matvec_f32_ncols_3_bs_160_param_5];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_3_bs_160_param_6];
	ld.param.u32 	%r17, [ggml_matvec_f32_ncols_3_bs_160_param_7];
	ld.param.u32 	%r20, [ggml_matvec_f32_ncols_3_bs_160_param_8];
	ld.param.u32 	%r21, [ggml_matvec_f32_ncols_3_bs_160_param_9];
	ld.param.u32 	%r22, [ggml_matvec_f32_ncols_3_bs_160_param_10];
	ld.param.u32 	%r18, [ggml_matvec_f32_ncols_3_bs_160_param_11];
	cvta.to.global.u64 	%rd73, %rd30;
	cvta.to.global.u64 	%rd2, %rd29;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r23, %r1, %r20;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r24, %r2, %r19;
	mad.lo.s32 	%r25, %r23, %r21, %r24;
	cvt.s64.s32 	%rd4, %r25;
	mul.lo.s32 	%r26, %r1, %r22;
	cvt.s64.s32 	%rd5, %r26;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r27, %r3, 2;
	mov.u32 	%r28, data_mmv;
	add.s32 	%r4, %r28, %r27;
	@%p1 bra 	$L__BB34_2;

	mov.u32 	%r29, 0;
	st.shared.u32 	[%r4], %r29;

$L__BB34_2:
	bar.sync 	0;
	mov.f32 	%f205, 0f00000000;
	mov.u32 	%r30, 0;
	st.local.u32 	[%rd3], %r30;
	st.local.u32 	[%rd3+4], %r30;
	st.local.u32 	[%rd3+8], %r30;
	setp.ge.s32 	%p2, %r3, %r15;
	mov.f32 	%f206, %f205;
	mov.f32 	%f207, %f205;
	@%p2 bra 	$L__BB34_11;

	not.b32 	%r31, %r3;
	add.s32 	%r5, %r31, %r15;
	mul.wide.u32 	%rd32, %r5, -858993459;
	shr.u64 	%rd33, %rd32, 39;
	cvt.u32.u64 	%r32, %rd33;
	add.s32 	%r33, %r32, 1;
	and.b32  	%r151, %r33, 3;
	setp.eq.s32 	%p3, %r151, 0;
	mov.f32 	%f205, 0f00000000;
	mov.u32 	%r152, %r3;
	@%p3 bra 	$L__BB34_7;

	shl.b32 	%r34, %r16, 1;
	add.s32 	%r35, %r3, %r34;
	mul.wide.s32 	%rd34, %r35, 2;
	add.s64 	%rd35, %rd34, %rd5;
	shl.b64 	%rd36, %rd35, 2;
	add.s64 	%rd71, %rd73, %rd36;
	mul.wide.s32 	%rd37, %r16, 2;
	mul.wide.s32 	%rd38, %r3, 2;
	add.s64 	%rd39, %rd37, %rd38;
	add.s64 	%rd40, %rd39, %rd5;
	shl.b64 	%rd41, %rd40, 2;
	add.s64 	%rd70, %rd73, %rd41;
	add.s64 	%rd42, %rd38, %rd5;
	shl.b64 	%rd43, %rd42, 2;
	add.s64 	%rd69, %rd73, %rd43;
	add.s64 	%rd44, %rd38, %rd4;
	shl.b64 	%rd45, %rd44, 2;
	add.s64 	%rd68, %rd2, %rd45;
	mov.f32 	%f205, 0f00000000;
	mov.f32 	%f206, %f205;
	mov.f32 	%f207, %f205;
	mov.u32 	%r152, %r3;

$L__BB34_5:
	.pragma "nounroll";
	ld.global.nc.v2.f32 	{%f28, %f29}, [%rd68];
	ld.global.nc.v2.f32 	{%f32, %f33}, [%rd69];
	fma.rn.f32 	%f36, %f28, %f32, %f207;
	fma.rn.f32 	%f207, %f29, %f33, %f36;
	ld.global.nc.v2.f32 	{%f37, %f38}, [%rd70];
	fma.rn.f32 	%f41, %f28, %f37, %f206;
	fma.rn.f32 	%f206, %f29, %f38, %f41;
	ld.global.nc.v2.f32 	{%f42, %f43}, [%rd71];
	fma.rn.f32 	%f46, %f28, %f42, %f205;
	fma.rn.f32 	%f205, %f29, %f43, %f46;
	add.s32 	%r152, %r152, 160;
	add.s64 	%rd71, %rd71, 1280;
	add.s64 	%rd70, %rd70, 1280;
	add.s64 	%rd69, %rd69, 1280;
	add.s64 	%rd68, %rd68, 1280;
	add.s32 	%r151, %r151, -1;
	setp.ne.s32 	%p4, %r151, 0;
	@%p4 bra 	$L__BB34_5;

	st.local.f32 	[%rd3], %f207;
	st.local.f32 	[%rd3+4], %f206;
	st.local.f32 	[%rd3+8], %f205;

$L__BB34_7:
	setp.lt.u32 	%p5, %r5, 480;
	@%p5 bra 	$L__BB34_11;

	add.s32 	%r36, %r152, %r16;
	shl.b32 	%r37, %r16, 1;
	add.s32 	%r38, %r152, %r37;
	add.s32 	%r39, %r36, 160;
	mul.wide.s32 	%rd46, %r39, 8;
	shl.b64 	%rd47, %rd5, 2;
	add.s64 	%rd19, %rd46, %rd47;
	mul.wide.s32 	%rd48, %r38, 8;
	add.s64 	%rd20, %rd48, %rd47;
	mul.wide.s32 	%rd49, %r152, 2;
	add.s64 	%rd50, %rd49, %rd4;
	shl.b64 	%rd51, %rd50, 2;
	add.s64 	%rd52, %rd2, %rd51;
	add.s64 	%rd72, %rd52, 2560;
	mul.wide.s32 	%rd53, %r152, 8;
	add.s64 	%rd22, %rd53, %rd47;
	mul.wide.s32 	%rd54, %r16, 8;
	add.s64 	%rd55, %rd53, %rd54;
	add.s64 	%rd23, %rd55, %rd47;

$L__BB34_9:
	ld.global.nc.v2.f32 	{%f47, %f48}, [%rd72+-2560];
	add.s64 	%rd56, %rd73, %rd22;
	ld.global.nc.v2.f32 	{%f51, %f52}, [%rd56];
	fma.rn.f32 	%f55, %f47, %f51, %f207;
	fma.rn.f32 	%f56, %f48, %f52, %f55;
	add.s64 	%rd57, %rd73, %rd23;
	ld.global.nc.v2.f32 	{%f57, %f58}, [%rd57];
	fma.rn.f32 	%f61, %f47, %f57, %f206;
	fma.rn.f32 	%f62, %f48, %f58, %f61;
	add.s64 	%rd58, %rd73, %rd20;
	ld.global.nc.v2.f32 	{%f63, %f64}, [%rd58];
	fma.rn.f32 	%f67, %f47, %f63, %f205;
	fma.rn.f32 	%f68, %f48, %f64, %f67;
	ld.global.nc.v2.f32 	{%f69, %f70}, [%rd72+-1280];
	ld.global.nc.v2.f32 	{%f73, %f74}, [%rd56+1280];
	fma.rn.f32 	%f77, %f69, %f73, %f56;
	fma.rn.f32 	%f78, %f70, %f74, %f77;
	add.s64 	%rd59, %rd73, %rd19;
	ld.global.nc.v2.f32 	{%f79, %f80}, [%rd59];
	fma.rn.f32 	%f83, %f69, %f79, %f62;
	fma.rn.f32 	%f84, %f70, %f80, %f83;
	ld.global.nc.v2.f32 	{%f85, %f86}, [%rd58+1280];
	fma.rn.f32 	%f89, %f69, %f85, %f68;
	fma.rn.f32 	%f90, %f70, %f86, %f89;
	ld.global.nc.v2.f32 	{%f91, %f92}, [%rd72];
	ld.global.nc.v2.f32 	{%f95, %f96}, [%rd56+2560];
	fma.rn.f32 	%f99, %f91, %f95, %f78;
	fma.rn.f32 	%f100, %f92, %f96, %f99;
	ld.global.nc.v2.f32 	{%f101, %f102}, [%rd59+1280];
	fma.rn.f32 	%f105, %f91, %f101, %f84;
	fma.rn.f32 	%f106, %f92, %f102, %f105;
	ld.global.nc.v2.f32 	{%f107, %f108}, [%rd58+2560];
	fma.rn.f32 	%f111, %f91, %f107, %f90;
	fma.rn.f32 	%f112, %f92, %f108, %f111;
	ld.global.nc.v2.f32 	{%f113, %f114}, [%rd72+1280];
	ld.global.nc.v2.f32 	{%f117, %f118}, [%rd56+3840];
	fma.rn.f32 	%f121, %f113, %f117, %f100;
	fma.rn.f32 	%f207, %f114, %f118, %f121;
	ld.global.nc.v2.f32 	{%f122, %f123}, [%rd59+2560];
	fma.rn.f32 	%f126, %f113, %f122, %f106;
	fma.rn.f32 	%f206, %f114, %f123, %f126;
	ld.global.nc.v2.f32 	{%f127, %f128}, [%rd58+3840];
	fma.rn.f32 	%f131, %f113, %f127, %f112;
	fma.rn.f32 	%f205, %f114, %f128, %f131;
	add.s64 	%rd73, %rd73, 5120;
	add.s64 	%rd72, %rd72, 5120;
	add.s32 	%r152, %r152, 640;
	setp.lt.s32 	%p6, %r152, %r15;
	@%p6 bra 	$L__BB34_9;

	st.local.f32 	[%rd3], %f207;
	st.local.f32 	[%rd3+4], %f206;
	st.local.f32 	[%rd3+8], %f205;

$L__BB34_11:
	shr.s32 	%r40, %r3, 31;
	shr.u32 	%r41, %r40, 27;
	add.s32 	%r42, %r3, %r41;
	shr.s32 	%r43, %r42, 5;
	shl.b32 	%r44, %r43, 2;
	add.s32 	%r14, %r28, %r44;
	mov.u32 	%r46, 2;
	mov.b32 	%r47, %f207;
	mov.u32 	%r48, 31;
	mov.u32 	%r49, 16;
	mov.u32 	%r50, -1;
	shfl.sync.bfly.b32 	%r51|%p7, %r47, %r49, %r48, %r50;
	mov.b32 	%f132, %r51;
	add.f32 	%f133, %f207, %f132;
	mov.b32 	%r52, %f133;
	mov.u32 	%r53, 8;
	shfl.sync.bfly.b32 	%r54|%p8, %r52, %r53, %r48, %r50;
	mov.b32 	%f134, %r54;
	add.f32 	%f135, %f133, %f134;
	mov.b32 	%r55, %f135;
	mov.u32 	%r56, 4;
	shfl.sync.bfly.b32 	%r57|%p9, %r55, %r56, %r48, %r50;
	mov.b32 	%f136, %r57;
	add.f32 	%f137, %f135, %f136;
	mov.b32 	%r58, %f137;
	shfl.sync.bfly.b32 	%r59|%p10, %r58, %r46, %r48, %r50;
	mov.b32 	%f138, %r59;
	add.f32 	%f139, %f137, %f138;
	mov.b32 	%r60, %f139;
	mov.u32 	%r61, 1;
	shfl.sync.bfly.b32 	%r62|%p11, %r60, %r61, %r48, %r50;
	mov.b32 	%f140, %r62;
	add.f32 	%f141, %f139, %f140;
	st.local.f32 	[%rd3], %f141;
	st.shared.f32 	[%r14], %f141;
	bar.sync 	0;
	@%p1 bra 	$L__BB34_13;

	ld.shared.f32 	%f142, [%r4];
	mov.b32 	%r63, %f142;
	shfl.sync.bfly.b32 	%r67|%p13, %r63, %r49, %r48, %r50;
	mov.b32 	%f143, %r67;
	add.f32 	%f144, %f142, %f143;
	mov.b32 	%r68, %f144;
	shfl.sync.bfly.b32 	%r70|%p14, %r68, %r53, %r48, %r50;
	mov.b32 	%f145, %r70;
	add.f32 	%f146, %f144, %f145;
	mov.b32 	%r71, %f146;
	shfl.sync.bfly.b32 	%r73|%p15, %r71, %r56, %r48, %r50;
	mov.b32 	%f147, %r73;
	add.f32 	%f148, %f146, %f147;
	mov.b32 	%r74, %f148;
	shfl.sync.bfly.b32 	%r76|%p16, %r74, %r46, %r48, %r50;
	mov.b32 	%f149, %r76;
	add.f32 	%f150, %f148, %f149;
	mov.b32 	%r77, %f150;
	shfl.sync.bfly.b32 	%r79|%p17, %r77, %r61, %r48, %r50;
	mov.b32 	%f151, %r79;
	add.f32 	%f152, %f150, %f151;
	st.local.f32 	[%rd3], %f152;

$L__BB34_13:
	bar.sync 	0;
	mov.b32 	%r80, %f206;
	shfl.sync.bfly.b32 	%r84|%p19, %r80, %r49, %r48, %r50;
	mov.b32 	%f153, %r84;
	add.f32 	%f154, %f206, %f153;
	mov.b32 	%r85, %f154;
	shfl.sync.bfly.b32 	%r87|%p20, %r85, %r53, %r48, %r50;
	mov.b32 	%f155, %r87;
	add.f32 	%f156, %f154, %f155;
	mov.b32 	%r88, %f156;
	shfl.sync.bfly.b32 	%r90|%p21, %r88, %r56, %r48, %r50;
	mov.b32 	%f157, %r90;
	add.f32 	%f158, %f156, %f157;
	mov.b32 	%r91, %f158;
	shfl.sync.bfly.b32 	%r93|%p22, %r91, %r46, %r48, %r50;
	mov.b32 	%f159, %r93;
	add.f32 	%f160, %f158, %f159;
	mov.b32 	%r94, %f160;
	shfl.sync.bfly.b32 	%r96|%p23, %r94, %r61, %r48, %r50;
	mov.b32 	%f161, %r96;
	add.f32 	%f162, %f160, %f161;
	st.local.f32 	[%rd3+4], %f162;
	st.shared.f32 	[%r14], %f162;
	bar.sync 	0;
	@%p1 bra 	$L__BB34_15;

	ld.shared.f32 	%f163, [%r4];
	mov.b32 	%r97, %f163;
	mov.u32 	%r98, 31;
	mov.u32 	%r99, 16;
	mov.u32 	%r100, -1;
	shfl.sync.bfly.b32 	%r101|%p24, %r97, %r99, %r98, %r100;
	mov.b32 	%f164, %r101;
	add.f32 	%f165, %f163, %f164;
	mov.b32 	%r102, %f165;
	mov.u32 	%r103, 8;
	shfl.sync.bfly.b32 	%r104|%p25, %r102, %r103, %r98, %r100;
	mov.b32 	%f166, %r104;
	add.f32 	%f167, %f165, %f166;
	mov.b32 	%r105, %f167;
	mov.u32 	%r106, 4;
	shfl.sync.bfly.b32 	%r107|%p26, %r105, %r106, %r98, %r100;
	mov.b32 	%f168, %r107;
	add.f32 	%f169, %f167, %f168;
	mov.b32 	%r108, %f169;
	mov.u32 	%r109, 2;
	shfl.sync.bfly.b32 	%r110|%p27, %r108, %r109, %r98, %r100;
	mov.b32 	%f170, %r110;
	add.f32 	%f171, %f169, %f170;
	mov.b32 	%r111, %f171;
	mov.u32 	%r112, 1;
	shfl.sync.bfly.b32 	%r113|%p28, %r111, %r112, %r98, %r100;
	mov.b32 	%f172, %r113;
	add.f32 	%f173, %f171, %f172;
	st.local.f32 	[%rd3+4], %f173;

$L__BB34_15:
	bar.sync 	0;
	mov.b32 	%r114, %f205;
	mov.u32 	%r115, 31;
	mov.u32 	%r116, 16;
	mov.u32 	%r117, -1;
	shfl.sync.bfly.b32 	%r118|%p30, %r114, %r116, %r115, %r117;
	mov.b32 	%f174, %r118;
	add.f32 	%f175, %f205, %f174;
	mov.b32 	%r119, %f175;
	mov.u32 	%r120, 8;
	shfl.sync.bfly.b32 	%r121|%p31, %r119, %r120, %r115, %r117;
	mov.b32 	%f176, %r121;
	add.f32 	%f177, %f175, %f176;
	mov.b32 	%r122, %f177;
	mov.u32 	%r123, 4;
	shfl.sync.bfly.b32 	%r124|%p32, %r122, %r123, %r115, %r117;
	mov.b32 	%f178, %r124;
	add.f32 	%f179, %f177, %f178;
	mov.b32 	%r125, %f179;
	mov.u32 	%r126, 2;
	shfl.sync.bfly.b32 	%r127|%p33, %r125, %r126, %r115, %r117;
	mov.b32 	%f180, %r127;
	add.f32 	%f181, %f179, %f180;
	mov.b32 	%r128, %f181;
	mov.u32 	%r129, 1;
	shfl.sync.bfly.b32 	%r130|%p34, %r128, %r129, %r115, %r117;
	mov.b32 	%f182, %r130;
	add.f32 	%f183, %f181, %f182;
	st.local.f32 	[%rd3+8], %f183;
	st.shared.f32 	[%r14], %f183;
	bar.sync 	0;
	@%p1 bra 	$L__BB34_17;

	ld.shared.f32 	%f184, [%r4];
	mov.b32 	%r131, %f184;
	shfl.sync.bfly.b32 	%r135|%p35, %r131, %r116, %r115, %r117;
	mov.b32 	%f185, %r135;
	add.f32 	%f186, %f184, %f185;
	mov.b32 	%r136, %f186;
	shfl.sync.bfly.b32 	%r138|%p36, %r136, %r120, %r115, %r117;
	mov.b32 	%f187, %r138;
	add.f32 	%f188, %f186, %f187;
	mov.b32 	%r139, %f188;
	shfl.sync.bfly.b32 	%r141|%p37, %r139, %r123, %r115, %r117;
	mov.b32 	%f189, %r141;
	add.f32 	%f190, %f188, %f189;
	mov.b32 	%r142, %f190;
	shfl.sync.bfly.b32 	%r144|%p38, %r142, %r126, %r115, %r117;
	mov.b32 	%f191, %r144;
	add.f32 	%f192, %f190, %f191;
	mov.b32 	%r145, %f192;
	shfl.sync.bfly.b32 	%r147|%p39, %r145, %r129, %r115, %r117;
	mov.b32 	%f193, %r147;
	add.f32 	%f194, %f192, %f193;
	st.local.f32 	[%rd3+8], %f194;

$L__BB34_17:
	bar.sync 	0;
	setp.gt.s32 	%p40, %r3, 2;
	@%p40 bra 	$L__BB34_19;

	mul.wide.s32 	%rd60, %r3, 4;
	add.s64 	%rd61, %rd3, %rd60;
	ld.local.f32 	%f195, [%rd61];
	mad.lo.s32 	%r148, %r3, %r17, %r2;
	cvt.s64.s32 	%rd62, %r148;
	mul.lo.s32 	%r149, %r1, %r18;
	cvt.s64.s32 	%rd63, %r149;
	add.s64 	%rd64, %rd63, %rd62;
	cvta.to.global.u64 	%rd65, %rd28;
	shl.b64 	%rd66, %rd64, 2;
	add.s64 	%rd67, %rd65, %rd66;
	st.global.f32 	[%rd67], %f195;

$L__BB34_19:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_4_bs_160
.visible .entry ggml_matvec_f32_ncols_4_bs_160(
	.param .u64 ggml_matvec_f32_ncols_4_bs_160_param_0,
	.param .u64 ggml_matvec_f32_ncols_4_bs_160_param_1,
	.param .u64 ggml_matvec_f32_ncols_4_bs_160_param_2,
	.param .u32 ggml_matvec_f32_ncols_4_bs_160_param_3,
	.param .u32 ggml_matvec_f32_ncols_4_bs_160_param_4,
	.param .u32 ggml_matvec_f32_ncols_4_bs_160_param_5,
	.param .u32 ggml_matvec_f32_ncols_4_bs_160_param_6,
	.param .u32 ggml_matvec_f32_ncols_4_bs_160_param_7,
	.param .u32 ggml_matvec_f32_ncols_4_bs_160_param_8,
	.param .u32 ggml_matvec_f32_ncols_4_bs_160_param_9,
	.param .u32 ggml_matvec_f32_ncols_4_bs_160_param_10,
	.param .u32 ggml_matvec_f32_ncols_4_bs_160_param_11
)
{
	.local .align 16 .b8 	__local_depot35[16];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<52>;
	.reg .f32 	%f<270>;
	.reg .b32 	%r<189>;
	.reg .b64 	%rd<85>;


	mov.u64 	%SPL, __local_depot35;
	ld.param.u64 	%rd34, [ggml_matvec_f32_ncols_4_bs_160_param_0];
	ld.param.u64 	%rd35, [ggml_matvec_f32_ncols_4_bs_160_param_1];
	ld.param.u64 	%rd33, [ggml_matvec_f32_ncols_4_bs_160_param_2];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_4_bs_160_param_3];
	ld.param.u32 	%r19, [ggml_matvec_f32_ncols_4_bs_160_param_5];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_4_bs_160_param_6];
	ld.param.u32 	%r17, [ggml_matvec_f32_ncols_4_bs_160_param_7];
	ld.param.u32 	%r20, [ggml_matvec_f32_ncols_4_bs_160_param_8];
	ld.param.u32 	%r21, [ggml_matvec_f32_ncols_4_bs_160_param_9];
	ld.param.u32 	%r22, [ggml_matvec_f32_ncols_4_bs_160_param_10];
	ld.param.u32 	%r18, [ggml_matvec_f32_ncols_4_bs_160_param_11];
	cvta.to.global.u64 	%rd84, %rd35;
	cvta.to.global.u64 	%rd2, %rd34;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r23, %r1, %r20;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r24, %r2, %r19;
	mad.lo.s32 	%r25, %r23, %r21, %r24;
	cvt.s64.s32 	%rd4, %r25;
	mul.lo.s32 	%r26, %r1, %r22;
	cvt.s64.s32 	%rd5, %r26;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r27, %r3, 2;
	mov.u32 	%r28, data_mmv;
	add.s32 	%r4, %r28, %r27;
	@%p1 bra 	$L__BB35_2;

	mov.u32 	%r29, 0;
	st.shared.u32 	[%r4], %r29;

$L__BB35_2:
	bar.sync 	0;
	mov.f32 	%f266, 0f00000000;
	st.local.v4.f32 	[%rd3], {%f266, %f266, %f266, %f266};
	setp.ge.s32 	%p2, %r3, %r15;
	mov.f32 	%f267, %f266;
	mov.f32 	%f268, %f266;
	mov.f32 	%f269, %f266;
	@%p2 bra 	$L__BB35_11;

	not.b32 	%r30, %r3;
	add.s32 	%r5, %r30, %r15;
	mul.wide.u32 	%rd37, %r5, -858993459;
	shr.u64 	%rd38, %rd37, 39;
	cvt.u32.u64 	%r31, %rd38;
	add.s32 	%r32, %r31, 1;
	and.b32  	%r186, %r32, 3;
	setp.eq.s32 	%p3, %r186, 0;
	mov.f32 	%f266, 0f00000000;
	mov.u32 	%r187, %r3;
	@%p3 bra 	$L__BB35_7;

	shl.b32 	%r33, %r16, 1;
	add.s32 	%r34, %r3, %r33;
	mul.wide.s32 	%rd39, %r34, 2;
	add.s64 	%rd40, %rd39, %rd5;
	shl.b64 	%rd41, %rd40, 2;
	add.s64 	%rd82, %rd84, %rd41;
	mad.lo.s32 	%r35, %r16, 3, %r3;
	mul.wide.s32 	%rd42, %r35, 2;
	add.s64 	%rd43, %rd42, %rd5;
	shl.b64 	%rd44, %rd43, 2;
	add.s64 	%rd81, %rd84, %rd44;
	mul.wide.s32 	%rd45, %r16, 2;
	mul.wide.s32 	%rd46, %r3, 2;
	add.s64 	%rd47, %rd45, %rd46;
	add.s64 	%rd48, %rd47, %rd5;
	shl.b64 	%rd49, %rd48, 2;
	add.s64 	%rd80, %rd84, %rd49;
	add.s64 	%rd50, %rd46, %rd5;
	shl.b64 	%rd51, %rd50, 2;
	add.s64 	%rd79, %rd84, %rd51;
	add.s64 	%rd52, %rd46, %rd4;
	shl.b64 	%rd53, %rd52, 2;
	add.s64 	%rd78, %rd2, %rd53;
	mov.f32 	%f266, 0f00000000;
	mov.f32 	%f267, %f266;
	mov.f32 	%f268, %f266;
	mov.f32 	%f269, %f266;
	mov.u32 	%r187, %r3;

$L__BB35_5:
	.pragma "nounroll";
	ld.global.nc.v2.f32 	{%f37, %f38}, [%rd78];
	ld.global.nc.v2.f32 	{%f41, %f42}, [%rd79];
	fma.rn.f32 	%f45, %f37, %f41, %f269;
	fma.rn.f32 	%f269, %f38, %f42, %f45;
	ld.global.nc.v2.f32 	{%f46, %f47}, [%rd80];
	fma.rn.f32 	%f50, %f37, %f46, %f268;
	fma.rn.f32 	%f268, %f38, %f47, %f50;
	ld.global.nc.v2.f32 	{%f51, %f52}, [%rd82];
	fma.rn.f32 	%f55, %f37, %f51, %f267;
	fma.rn.f32 	%f267, %f38, %f52, %f55;
	ld.global.nc.v2.f32 	{%f56, %f57}, [%rd81];
	fma.rn.f32 	%f60, %f37, %f56, %f266;
	fma.rn.f32 	%f266, %f38, %f57, %f60;
	add.s32 	%r187, %r187, 160;
	add.s64 	%rd82, %rd82, 1280;
	add.s64 	%rd81, %rd81, 1280;
	add.s64 	%rd80, %rd80, 1280;
	add.s64 	%rd79, %rd79, 1280;
	add.s64 	%rd78, %rd78, 1280;
	add.s32 	%r186, %r186, -1;
	setp.ne.s32 	%p4, %r186, 0;
	@%p4 bra 	$L__BB35_5;

	st.local.v4.f32 	[%rd3], {%f269, %f268, %f267, %f266};

$L__BB35_7:
	setp.lt.u32 	%p5, %r5, 480;
	@%p5 bra 	$L__BB35_11;

	add.s32 	%r36, %r187, %r16;
	shl.b32 	%r37, %r16, 1;
	add.s32 	%r38, %r187, %r37;
	mad.lo.s32 	%r39, %r16, 3, %r187;
	add.s32 	%r40, %r36, 160;
	mul.wide.s32 	%rd54, %r40, 8;
	shl.b64 	%rd55, %rd5, 2;
	add.s64 	%rd23, %rd54, %rd55;
	mul.wide.s32 	%rd56, %r38, 8;
	add.s64 	%rd24, %rd56, %rd55;
	mul.wide.s32 	%rd57, %r39, 8;
	add.s64 	%rd25, %rd57, %rd55;
	mul.wide.s32 	%rd58, %r187, 2;
	add.s64 	%rd59, %rd58, %rd4;
	shl.b64 	%rd60, %rd59, 2;
	add.s64 	%rd61, %rd2, %rd60;
	add.s64 	%rd83, %rd61, 2560;
	mul.wide.s32 	%rd62, %r187, 8;
	add.s64 	%rd27, %rd62, %rd55;
	mul.wide.s32 	%rd63, %r16, 8;
	add.s64 	%rd64, %rd62, %rd63;
	add.s64 	%rd28, %rd64, %rd55;

$L__BB35_9:
	ld.global.nc.v2.f32 	{%f61, %f62}, [%rd83+-2560];
	add.s64 	%rd65, %rd84, %rd27;
	ld.global.nc.v2.f32 	{%f65, %f66}, [%rd65];
	fma.rn.f32 	%f69, %f61, %f65, %f269;
	fma.rn.f32 	%f70, %f62, %f66, %f69;
	add.s64 	%rd66, %rd84, %rd28;
	ld.global.nc.v2.f32 	{%f71, %f72}, [%rd66];
	fma.rn.f32 	%f75, %f61, %f71, %f268;
	fma.rn.f32 	%f76, %f62, %f72, %f75;
	add.s64 	%rd67, %rd84, %rd24;
	ld.global.nc.v2.f32 	{%f77, %f78}, [%rd67];
	fma.rn.f32 	%f81, %f61, %f77, %f267;
	fma.rn.f32 	%f82, %f62, %f78, %f81;
	add.s64 	%rd68, %rd84, %rd25;
	ld.global.nc.v2.f32 	{%f83, %f84}, [%rd68];
	fma.rn.f32 	%f87, %f61, %f83, %f266;
	fma.rn.f32 	%f88, %f62, %f84, %f87;
	ld.global.nc.v2.f32 	{%f89, %f90}, [%rd83+-1280];
	ld.global.nc.v2.f32 	{%f93, %f94}, [%rd65+1280];
	fma.rn.f32 	%f97, %f89, %f93, %f70;
	fma.rn.f32 	%f98, %f90, %f94, %f97;
	add.s64 	%rd69, %rd84, %rd23;
	ld.global.nc.v2.f32 	{%f99, %f100}, [%rd69];
	fma.rn.f32 	%f103, %f89, %f99, %f76;
	fma.rn.f32 	%f104, %f90, %f100, %f103;
	ld.global.nc.v2.f32 	{%f105, %f106}, [%rd67+1280];
	fma.rn.f32 	%f109, %f89, %f105, %f82;
	fma.rn.f32 	%f110, %f90, %f106, %f109;
	ld.global.nc.v2.f32 	{%f111, %f112}, [%rd68+1280];
	fma.rn.f32 	%f115, %f89, %f111, %f88;
	fma.rn.f32 	%f116, %f90, %f112, %f115;
	ld.global.nc.v2.f32 	{%f117, %f118}, [%rd83];
	ld.global.nc.v2.f32 	{%f121, %f122}, [%rd65+2560];
	fma.rn.f32 	%f125, %f117, %f121, %f98;
	fma.rn.f32 	%f126, %f118, %f122, %f125;
	ld.global.nc.v2.f32 	{%f127, %f128}, [%rd69+1280];
	fma.rn.f32 	%f131, %f117, %f127, %f104;
	fma.rn.f32 	%f132, %f118, %f128, %f131;
	ld.global.nc.v2.f32 	{%f133, %f134}, [%rd67+2560];
	fma.rn.f32 	%f137, %f117, %f133, %f110;
	fma.rn.f32 	%f138, %f118, %f134, %f137;
	ld.global.nc.v2.f32 	{%f139, %f140}, [%rd68+2560];
	fma.rn.f32 	%f143, %f117, %f139, %f116;
	fma.rn.f32 	%f144, %f118, %f140, %f143;
	ld.global.nc.v2.f32 	{%f145, %f146}, [%rd83+1280];
	ld.global.nc.v2.f32 	{%f149, %f150}, [%rd65+3840];
	fma.rn.f32 	%f153, %f145, %f149, %f126;
	fma.rn.f32 	%f269, %f146, %f150, %f153;
	ld.global.nc.v2.f32 	{%f154, %f155}, [%rd69+2560];
	fma.rn.f32 	%f158, %f145, %f154, %f132;
	fma.rn.f32 	%f268, %f146, %f155, %f158;
	ld.global.nc.v2.f32 	{%f159, %f160}, [%rd67+3840];
	fma.rn.f32 	%f163, %f145, %f159, %f138;
	fma.rn.f32 	%f267, %f146, %f160, %f163;
	ld.global.nc.v2.f32 	{%f164, %f165}, [%rd68+3840];
	fma.rn.f32 	%f168, %f145, %f164, %f144;
	fma.rn.f32 	%f266, %f146, %f165, %f168;
	add.s64 	%rd84, %rd84, 5120;
	add.s64 	%rd83, %rd83, 5120;
	add.s32 	%r187, %r187, 640;
	setp.lt.s32 	%p6, %r187, %r15;
	@%p6 bra 	$L__BB35_9;

	st.local.v4.f32 	[%rd3], {%f269, %f268, %f267, %f266};

$L__BB35_11:
	shr.s32 	%r41, %r3, 31;
	shr.u32 	%r42, %r41, 27;
	add.s32 	%r43, %r3, %r42;
	shr.s32 	%r44, %r43, 5;
	shl.b32 	%r45, %r44, 2;
	add.s32 	%r14, %r28, %r45;
	mov.u32 	%r47, 2;
	mov.b32 	%r48, %f269;
	mov.u32 	%r49, 31;
	mov.u32 	%r50, 16;
	mov.u32 	%r51, -1;
	shfl.sync.bfly.b32 	%r52|%p7, %r48, %r50, %r49, %r51;
	mov.b32 	%f169, %r52;
	add.f32 	%f170, %f269, %f169;
	mov.b32 	%r53, %f170;
	mov.u32 	%r54, 8;
	shfl.sync.bfly.b32 	%r55|%p8, %r53, %r54, %r49, %r51;
	mov.b32 	%f171, %r55;
	add.f32 	%f172, %f170, %f171;
	mov.b32 	%r56, %f172;
	mov.u32 	%r57, 4;
	shfl.sync.bfly.b32 	%r58|%p9, %r56, %r57, %r49, %r51;
	mov.b32 	%f173, %r58;
	add.f32 	%f174, %f172, %f173;
	mov.b32 	%r59, %f174;
	shfl.sync.bfly.b32 	%r60|%p10, %r59, %r47, %r49, %r51;
	mov.b32 	%f175, %r60;
	add.f32 	%f176, %f174, %f175;
	mov.b32 	%r61, %f176;
	mov.u32 	%r62, 1;
	shfl.sync.bfly.b32 	%r63|%p11, %r61, %r62, %r49, %r51;
	mov.b32 	%f177, %r63;
	add.f32 	%f178, %f176, %f177;
	st.local.f32 	[%rd3], %f178;
	st.shared.f32 	[%r14], %f178;
	bar.sync 	0;
	@%p1 bra 	$L__BB35_13;

	ld.shared.f32 	%f179, [%r4];
	mov.b32 	%r64, %f179;
	shfl.sync.bfly.b32 	%r68|%p13, %r64, %r50, %r49, %r51;
	mov.b32 	%f180, %r68;
	add.f32 	%f181, %f179, %f180;
	mov.b32 	%r69, %f181;
	shfl.sync.bfly.b32 	%r71|%p14, %r69, %r54, %r49, %r51;
	mov.b32 	%f182, %r71;
	add.f32 	%f183, %f181, %f182;
	mov.b32 	%r72, %f183;
	shfl.sync.bfly.b32 	%r74|%p15, %r72, %r57, %r49, %r51;
	mov.b32 	%f184, %r74;
	add.f32 	%f185, %f183, %f184;
	mov.b32 	%r75, %f185;
	shfl.sync.bfly.b32 	%r77|%p16, %r75, %r47, %r49, %r51;
	mov.b32 	%f186, %r77;
	add.f32 	%f187, %f185, %f186;
	mov.b32 	%r78, %f187;
	shfl.sync.bfly.b32 	%r80|%p17, %r78, %r62, %r49, %r51;
	mov.b32 	%f188, %r80;
	add.f32 	%f189, %f187, %f188;
	st.local.f32 	[%rd3], %f189;

$L__BB35_13:
	bar.sync 	0;
	mov.b32 	%r81, %f268;
	shfl.sync.bfly.b32 	%r85|%p19, %r81, %r50, %r49, %r51;
	mov.b32 	%f190, %r85;
	add.f32 	%f191, %f268, %f190;
	mov.b32 	%r86, %f191;
	shfl.sync.bfly.b32 	%r88|%p20, %r86, %r54, %r49, %r51;
	mov.b32 	%f192, %r88;
	add.f32 	%f193, %f191, %f192;
	mov.b32 	%r89, %f193;
	shfl.sync.bfly.b32 	%r91|%p21, %r89, %r57, %r49, %r51;
	mov.b32 	%f194, %r91;
	add.f32 	%f195, %f193, %f194;
	mov.b32 	%r92, %f195;
	shfl.sync.bfly.b32 	%r94|%p22, %r92, %r47, %r49, %r51;
	mov.b32 	%f196, %r94;
	add.f32 	%f197, %f195, %f196;
	mov.b32 	%r95, %f197;
	shfl.sync.bfly.b32 	%r97|%p23, %r95, %r62, %r49, %r51;
	mov.b32 	%f198, %r97;
	add.f32 	%f199, %f197, %f198;
	st.local.f32 	[%rd3+4], %f199;
	st.shared.f32 	[%r14], %f199;
	bar.sync 	0;
	@%p1 bra 	$L__BB35_15;

	ld.shared.f32 	%f200, [%r4];
	mov.b32 	%r98, %f200;
	mov.u32 	%r99, 31;
	mov.u32 	%r100, 16;
	mov.u32 	%r101, -1;
	shfl.sync.bfly.b32 	%r102|%p24, %r98, %r100, %r99, %r101;
	mov.b32 	%f201, %r102;
	add.f32 	%f202, %f200, %f201;
	mov.b32 	%r103, %f202;
	mov.u32 	%r104, 8;
	shfl.sync.bfly.b32 	%r105|%p25, %r103, %r104, %r99, %r101;
	mov.b32 	%f203, %r105;
	add.f32 	%f204, %f202, %f203;
	mov.b32 	%r106, %f204;
	mov.u32 	%r107, 4;
	shfl.sync.bfly.b32 	%r108|%p26, %r106, %r107, %r99, %r101;
	mov.b32 	%f205, %r108;
	add.f32 	%f206, %f204, %f205;
	mov.b32 	%r109, %f206;
	mov.u32 	%r110, 2;
	shfl.sync.bfly.b32 	%r111|%p27, %r109, %r110, %r99, %r101;
	mov.b32 	%f207, %r111;
	add.f32 	%f208, %f206, %f207;
	mov.b32 	%r112, %f208;
	mov.u32 	%r113, 1;
	shfl.sync.bfly.b32 	%r114|%p28, %r112, %r113, %r99, %r101;
	mov.b32 	%f209, %r114;
	add.f32 	%f210, %f208, %f209;
	st.local.f32 	[%rd3+4], %f210;

$L__BB35_15:
	bar.sync 	0;
	mov.b32 	%r115, %f267;
	mov.u32 	%r116, 31;
	mov.u32 	%r117, 16;
	mov.u32 	%r118, -1;
	shfl.sync.bfly.b32 	%r119|%p30, %r115, %r117, %r116, %r118;
	mov.b32 	%f211, %r119;
	add.f32 	%f212, %f267, %f211;
	mov.b32 	%r120, %f212;
	mov.u32 	%r121, 8;
	shfl.sync.bfly.b32 	%r122|%p31, %r120, %r121, %r116, %r118;
	mov.b32 	%f213, %r122;
	add.f32 	%f214, %f212, %f213;
	mov.b32 	%r123, %f214;
	mov.u32 	%r124, 4;
	shfl.sync.bfly.b32 	%r125|%p32, %r123, %r124, %r116, %r118;
	mov.b32 	%f215, %r125;
	add.f32 	%f216, %f214, %f215;
	mov.b32 	%r126, %f216;
	mov.u32 	%r127, 2;
	shfl.sync.bfly.b32 	%r128|%p33, %r126, %r127, %r116, %r118;
	mov.b32 	%f217, %r128;
	add.f32 	%f218, %f216, %f217;
	mov.b32 	%r129, %f218;
	mov.u32 	%r130, 1;
	shfl.sync.bfly.b32 	%r131|%p34, %r129, %r130, %r116, %r118;
	mov.b32 	%f219, %r131;
	add.f32 	%f220, %f218, %f219;
	st.local.f32 	[%rd3+8], %f220;
	st.shared.f32 	[%r14], %f220;
	bar.sync 	0;
	@%p1 bra 	$L__BB35_17;

	ld.shared.f32 	%f221, [%r4];
	mov.b32 	%r132, %f221;
	shfl.sync.bfly.b32 	%r136|%p35, %r132, %r117, %r116, %r118;
	mov.b32 	%f222, %r136;
	add.f32 	%f223, %f221, %f222;
	mov.b32 	%r137, %f223;
	shfl.sync.bfly.b32 	%r139|%p36, %r137, %r121, %r116, %r118;
	mov.b32 	%f224, %r139;
	add.f32 	%f225, %f223, %f224;
	mov.b32 	%r140, %f225;
	shfl.sync.bfly.b32 	%r142|%p37, %r140, %r124, %r116, %r118;
	mov.b32 	%f226, %r142;
	add.f32 	%f227, %f225, %f226;
	mov.b32 	%r143, %f227;
	shfl.sync.bfly.b32 	%r145|%p38, %r143, %r127, %r116, %r118;
	mov.b32 	%f228, %r145;
	add.f32 	%f229, %f227, %f228;
	mov.b32 	%r146, %f229;
	shfl.sync.bfly.b32 	%r148|%p39, %r146, %r130, %r116, %r118;
	mov.b32 	%f230, %r148;
	add.f32 	%f231, %f229, %f230;
	st.local.f32 	[%rd3+8], %f231;

$L__BB35_17:
	bar.sync 	0;
	mov.b32 	%r149, %f266;
	shfl.sync.bfly.b32 	%r153|%p41, %r149, %r117, %r116, %r118;
	mov.b32 	%f232, %r153;
	add.f32 	%f233, %f266, %f232;
	mov.b32 	%r154, %f233;
	shfl.sync.bfly.b32 	%r156|%p42, %r154, %r121, %r116, %r118;
	mov.b32 	%f234, %r156;
	add.f32 	%f235, %f233, %f234;
	mov.b32 	%r157, %f235;
	shfl.sync.bfly.b32 	%r159|%p43, %r157, %r124, %r116, %r118;
	mov.b32 	%f236, %r159;
	add.f32 	%f237, %f235, %f236;
	mov.b32 	%r160, %f237;
	shfl.sync.bfly.b32 	%r162|%p44, %r160, %r127, %r116, %r118;
	mov.b32 	%f238, %r162;
	add.f32 	%f239, %f237, %f238;
	mov.b32 	%r163, %f239;
	shfl.sync.bfly.b32 	%r165|%p45, %r163, %r130, %r116, %r118;
	mov.b32 	%f240, %r165;
	add.f32 	%f241, %f239, %f240;
	st.local.f32 	[%rd3+12], %f241;
	st.shared.f32 	[%r14], %f241;
	bar.sync 	0;
	@%p1 bra 	$L__BB35_19;

	ld.shared.f32 	%f242, [%r4];
	mov.b32 	%r166, %f242;
	mov.u32 	%r167, 31;
	mov.u32 	%r168, 16;
	mov.u32 	%r169, -1;
	shfl.sync.bfly.b32 	%r170|%p46, %r166, %r168, %r167, %r169;
	mov.b32 	%f243, %r170;
	add.f32 	%f244, %f242, %f243;
	mov.b32 	%r171, %f244;
	mov.u32 	%r172, 8;
	shfl.sync.bfly.b32 	%r173|%p47, %r171, %r172, %r167, %r169;
	mov.b32 	%f245, %r173;
	add.f32 	%f246, %f244, %f245;
	mov.b32 	%r174, %f246;
	mov.u32 	%r175, 4;
	shfl.sync.bfly.b32 	%r176|%p48, %r174, %r175, %r167, %r169;
	mov.b32 	%f247, %r176;
	add.f32 	%f248, %f246, %f247;
	mov.b32 	%r177, %f248;
	mov.u32 	%r178, 2;
	shfl.sync.bfly.b32 	%r179|%p49, %r177, %r178, %r167, %r169;
	mov.b32 	%f249, %r179;
	add.f32 	%f250, %f248, %f249;
	mov.b32 	%r180, %f250;
	mov.u32 	%r181, 1;
	shfl.sync.bfly.b32 	%r182|%p50, %r180, %r181, %r167, %r169;
	mov.b32 	%f251, %r182;
	add.f32 	%f252, %f250, %f251;
	st.local.f32 	[%rd3+12], %f252;

$L__BB35_19:
	bar.sync 	0;
	setp.gt.s32 	%p51, %r3, 3;
	@%p51 bra 	$L__BB35_21;

	mul.wide.s32 	%rd70, %r3, 4;
	add.s64 	%rd71, %rd3, %rd70;
	ld.local.f32 	%f253, [%rd71];
	mad.lo.s32 	%r183, %r3, %r17, %r2;
	cvt.s64.s32 	%rd72, %r183;
	mul.lo.s32 	%r184, %r1, %r18;
	cvt.s64.s32 	%rd73, %r184;
	add.s64 	%rd74, %rd73, %rd72;
	cvta.to.global.u64 	%rd75, %rd33;
	shl.b64 	%rd76, %rd74, 2;
	add.s64 	%rd77, %rd75, %rd76;
	st.global.f32 	[%rd77], %f253;

$L__BB35_21:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_5_bs_160
.visible .entry ggml_matvec_f32_ncols_5_bs_160(
	.param .u64 ggml_matvec_f32_ncols_5_bs_160_param_0,
	.param .u64 ggml_matvec_f32_ncols_5_bs_160_param_1,
	.param .u64 ggml_matvec_f32_ncols_5_bs_160_param_2,
	.param .u32 ggml_matvec_f32_ncols_5_bs_160_param_3,
	.param .u32 ggml_matvec_f32_ncols_5_bs_160_param_4,
	.param .u32 ggml_matvec_f32_ncols_5_bs_160_param_5,
	.param .u32 ggml_matvec_f32_ncols_5_bs_160_param_6,
	.param .u32 ggml_matvec_f32_ncols_5_bs_160_param_7,
	.param .u32 ggml_matvec_f32_ncols_5_bs_160_param_8,
	.param .u32 ggml_matvec_f32_ncols_5_bs_160_param_9,
	.param .u32 ggml_matvec_f32_ncols_5_bs_160_param_10,
	.param .u32 ggml_matvec_f32_ncols_5_bs_160_param_11
)
{
	.local .align 4 .b8 	__local_depot36[20];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<63>;
	.reg .f32 	%f<332>;
	.reg .b32 	%r<225>;
	.reg .b64 	%rd<76>;


	mov.u64 	%SPL, __local_depot36;
	ld.param.u64 	%rd28, [ggml_matvec_f32_ncols_5_bs_160_param_0];
	ld.param.u64 	%rd29, [ggml_matvec_f32_ncols_5_bs_160_param_1];
	ld.param.u64 	%rd27, [ggml_matvec_f32_ncols_5_bs_160_param_2];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_5_bs_160_param_3];
	ld.param.u32 	%r19, [ggml_matvec_f32_ncols_5_bs_160_param_5];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_5_bs_160_param_6];
	ld.param.u32 	%r17, [ggml_matvec_f32_ncols_5_bs_160_param_7];
	ld.param.u32 	%r20, [ggml_matvec_f32_ncols_5_bs_160_param_8];
	ld.param.u32 	%r21, [ggml_matvec_f32_ncols_5_bs_160_param_9];
	ld.param.u32 	%r22, [ggml_matvec_f32_ncols_5_bs_160_param_10];
	ld.param.u32 	%r18, [ggml_matvec_f32_ncols_5_bs_160_param_11];
	cvta.to.global.u64 	%rd75, %rd29;
	cvta.to.global.u64 	%rd2, %rd28;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r23, %r1, %r20;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r24, %r2, %r19;
	mad.lo.s32 	%r25, %r23, %r21, %r24;
	cvt.s64.s32 	%rd4, %r25;
	mul.lo.s32 	%r26, %r1, %r22;
	cvt.s64.s32 	%rd5, %r26;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r27, %r3, 2;
	mov.u32 	%r28, data_mmv;
	add.s32 	%r4, %r28, %r27;
	@%p1 bra 	$L__BB36_2;

	mov.u32 	%r29, 0;
	st.shared.u32 	[%r4], %r29;

$L__BB36_2:
	bar.sync 	0;
	mov.f32 	%f327, 0f00000000;
	mov.u32 	%r30, 0;
	st.local.u32 	[%rd3], %r30;
	st.local.u32 	[%rd3+4], %r30;
	st.local.u32 	[%rd3+8], %r30;
	st.local.u32 	[%rd3+12], %r30;
	st.local.u32 	[%rd3+16], %r30;
	setp.ge.s32 	%p2, %r3, %r15;
	mov.f32 	%f328, %f327;
	mov.f32 	%f329, %f327;
	mov.f32 	%f330, %f327;
	mov.f32 	%f331, %f327;
	@%p2 bra 	$L__BB36_11;

	not.b32 	%r31, %r3;
	add.s32 	%r5, %r31, %r15;
	mul.wide.u32 	%rd31, %r5, -858993459;
	shr.u64 	%rd32, %rd31, 39;
	cvt.u32.u64 	%r32, %rd32;
	add.s32 	%r33, %r32, 1;
	and.b32  	%r222, %r33, 3;
	setp.eq.s32 	%p3, %r222, 0;
	mov.f32 	%f327, 0f00000000;
	mov.u32 	%r223, %r3;
	@%p3 bra 	$L__BB36_7;

	shl.b32 	%r34, %r16, 1;
	mad.lo.s32 	%r35, %r16, 3, %r3;
	mul.wide.s32 	%rd33, %r35, 8;
	shl.b64 	%rd34, %rd5, 2;
	add.s64 	%rd7, %rd33, %rd34;
	mul.wide.s32 	%rd35, %r3, 8;
	mul.wide.s32 	%rd36, %r16, 8;
	add.s64 	%rd37, %rd35, %rd36;
	add.s64 	%rd8, %rd37, %rd34;
	add.s64 	%rd9, %rd35, %rd34;
	mul.wide.s32 	%rd38, %r3, 2;
	add.s64 	%rd39, %rd38, %rd4;
	shl.b64 	%rd40, %rd39, 2;
	add.s64 	%rd72, %rd2, %rd40;
	mul.wide.s32 	%rd11, %r34, 8;
	mov.f32 	%f327, 0f00000000;
	mov.u64 	%rd73, %rd75;
	mov.f32 	%f328, %f327;
	mov.f32 	%f329, %f327;
	mov.f32 	%f330, %f327;
	mov.f32 	%f331, %f327;
	mov.u32 	%r223, %r3;

$L__BB36_5:
	.pragma "nounroll";
	ld.global.nc.v2.f32 	{%f46, %f47}, [%rd72];
	add.s64 	%rd41, %rd73, %rd9;
	ld.global.nc.v2.f32 	{%f50, %f51}, [%rd41];
	fma.rn.f32 	%f54, %f46, %f50, %f331;
	fma.rn.f32 	%f331, %f47, %f51, %f54;
	add.s64 	%rd42, %rd73, %rd8;
	ld.global.nc.v2.f32 	{%f55, %f56}, [%rd42];
	fma.rn.f32 	%f59, %f46, %f55, %f330;
	fma.rn.f32 	%f330, %f47, %f56, %f59;
	add.s64 	%rd43, %rd41, %rd11;
	ld.global.nc.v2.f32 	{%f60, %f61}, [%rd43];
	fma.rn.f32 	%f64, %f46, %f60, %f329;
	fma.rn.f32 	%f329, %f47, %f61, %f64;
	add.s64 	%rd44, %rd73, %rd7;
	ld.global.nc.v2.f32 	{%f65, %f66}, [%rd44];
	fma.rn.f32 	%f69, %f46, %f65, %f328;
	fma.rn.f32 	%f328, %f47, %f66, %f69;
	add.s64 	%rd45, %rd43, %rd11;
	ld.global.nc.v2.f32 	{%f70, %f71}, [%rd45];
	fma.rn.f32 	%f74, %f46, %f70, %f327;
	fma.rn.f32 	%f327, %f47, %f71, %f74;
	add.s32 	%r223, %r223, 160;
	add.s64 	%rd73, %rd73, 1280;
	add.s64 	%rd72, %rd72, 1280;
	add.s32 	%r222, %r222, -1;
	setp.ne.s32 	%p4, %r222, 0;
	@%p4 bra 	$L__BB36_5;

	st.local.f32 	[%rd3], %f331;
	st.local.f32 	[%rd3+4], %f330;
	st.local.f32 	[%rd3+8], %f329;
	st.local.f32 	[%rd3+12], %f328;
	st.local.f32 	[%rd3+16], %f327;

$L__BB36_7:
	setp.lt.u32 	%p5, %r5, 480;
	@%p5 bra 	$L__BB36_11;

	add.s32 	%r36, %r223, %r16;
	shl.b32 	%r37, %r16, 1;
	add.s32 	%r38, %r223, %r37;
	mad.lo.s32 	%r39, %r16, 3, %r223;
	shl.b32 	%r40, %r16, 2;
	add.s32 	%r41, %r223, %r40;
	add.s32 	%r42, %r36, 160;
	mul.wide.s32 	%rd46, %r42, 8;
	shl.b64 	%rd47, %rd5, 2;
	add.s64 	%rd16, %rd46, %rd47;
	mul.wide.s32 	%rd48, %r38, 8;
	add.s64 	%rd17, %rd48, %rd47;
	mul.wide.s32 	%rd49, %r39, 8;
	add.s64 	%rd18, %rd49, %rd47;
	mul.wide.s32 	%rd50, %r41, 8;
	add.s64 	%rd19, %rd50, %rd47;
	mul.wide.s32 	%rd51, %r223, 2;
	add.s64 	%rd52, %rd51, %rd4;
	shl.b64 	%rd53, %rd52, 2;
	add.s64 	%rd54, %rd2, %rd53;
	add.s64 	%rd74, %rd54, 2560;
	mul.wide.s32 	%rd55, %r223, 8;
	add.s64 	%rd21, %rd55, %rd47;
	mul.wide.s32 	%rd56, %r16, 8;
	add.s64 	%rd57, %rd55, %rd56;
	add.s64 	%rd22, %rd57, %rd47;

$L__BB36_9:
	ld.global.nc.v2.f32 	{%f75, %f76}, [%rd74+-2560];
	add.s64 	%rd58, %rd75, %rd21;
	ld.global.nc.v2.f32 	{%f79, %f80}, [%rd58];
	fma.rn.f32 	%f83, %f75, %f79, %f331;
	fma.rn.f32 	%f84, %f76, %f80, %f83;
	add.s64 	%rd59, %rd75, %rd22;
	ld.global.nc.v2.f32 	{%f85, %f86}, [%rd59];
	fma.rn.f32 	%f89, %f75, %f85, %f330;
	fma.rn.f32 	%f90, %f76, %f86, %f89;
	add.s64 	%rd60, %rd75, %rd17;
	ld.global.nc.v2.f32 	{%f91, %f92}, [%rd60];
	fma.rn.f32 	%f95, %f75, %f91, %f329;
	fma.rn.f32 	%f96, %f76, %f92, %f95;
	add.s64 	%rd61, %rd75, %rd18;
	ld.global.nc.v2.f32 	{%f97, %f98}, [%rd61];
	fma.rn.f32 	%f101, %f75, %f97, %f328;
	fma.rn.f32 	%f102, %f76, %f98, %f101;
	add.s64 	%rd62, %rd75, %rd19;
	ld.global.nc.v2.f32 	{%f103, %f104}, [%rd62];
	fma.rn.f32 	%f107, %f75, %f103, %f327;
	fma.rn.f32 	%f108, %f76, %f104, %f107;
	ld.global.nc.v2.f32 	{%f109, %f110}, [%rd74+-1280];
	ld.global.nc.v2.f32 	{%f113, %f114}, [%rd58+1280];
	fma.rn.f32 	%f117, %f109, %f113, %f84;
	fma.rn.f32 	%f118, %f110, %f114, %f117;
	add.s64 	%rd63, %rd75, %rd16;
	ld.global.nc.v2.f32 	{%f119, %f120}, [%rd63];
	fma.rn.f32 	%f123, %f109, %f119, %f90;
	fma.rn.f32 	%f124, %f110, %f120, %f123;
	ld.global.nc.v2.f32 	{%f125, %f126}, [%rd60+1280];
	fma.rn.f32 	%f129, %f109, %f125, %f96;
	fma.rn.f32 	%f130, %f110, %f126, %f129;
	ld.global.nc.v2.f32 	{%f131, %f132}, [%rd61+1280];
	fma.rn.f32 	%f135, %f109, %f131, %f102;
	fma.rn.f32 	%f136, %f110, %f132, %f135;
	ld.global.nc.v2.f32 	{%f137, %f138}, [%rd62+1280];
	fma.rn.f32 	%f141, %f109, %f137, %f108;
	fma.rn.f32 	%f142, %f110, %f138, %f141;
	ld.global.nc.v2.f32 	{%f143, %f144}, [%rd74];
	ld.global.nc.v2.f32 	{%f147, %f148}, [%rd58+2560];
	fma.rn.f32 	%f151, %f143, %f147, %f118;
	fma.rn.f32 	%f152, %f144, %f148, %f151;
	ld.global.nc.v2.f32 	{%f153, %f154}, [%rd63+1280];
	fma.rn.f32 	%f157, %f143, %f153, %f124;
	fma.rn.f32 	%f158, %f144, %f154, %f157;
	ld.global.nc.v2.f32 	{%f159, %f160}, [%rd60+2560];
	fma.rn.f32 	%f163, %f143, %f159, %f130;
	fma.rn.f32 	%f164, %f144, %f160, %f163;
	ld.global.nc.v2.f32 	{%f165, %f166}, [%rd61+2560];
	fma.rn.f32 	%f169, %f143, %f165, %f136;
	fma.rn.f32 	%f170, %f144, %f166, %f169;
	ld.global.nc.v2.f32 	{%f171, %f172}, [%rd62+2560];
	fma.rn.f32 	%f175, %f143, %f171, %f142;
	fma.rn.f32 	%f176, %f144, %f172, %f175;
	ld.global.nc.v2.f32 	{%f177, %f178}, [%rd74+1280];
	ld.global.nc.v2.f32 	{%f181, %f182}, [%rd58+3840];
	fma.rn.f32 	%f185, %f177, %f181, %f152;
	fma.rn.f32 	%f331, %f178, %f182, %f185;
	ld.global.nc.v2.f32 	{%f186, %f187}, [%rd63+2560];
	fma.rn.f32 	%f190, %f177, %f186, %f158;
	fma.rn.f32 	%f330, %f178, %f187, %f190;
	ld.global.nc.v2.f32 	{%f191, %f192}, [%rd60+3840];
	fma.rn.f32 	%f195, %f177, %f191, %f164;
	fma.rn.f32 	%f329, %f178, %f192, %f195;
	ld.global.nc.v2.f32 	{%f196, %f197}, [%rd61+3840];
	fma.rn.f32 	%f200, %f177, %f196, %f170;
	fma.rn.f32 	%f328, %f178, %f197, %f200;
	ld.global.nc.v2.f32 	{%f201, %f202}, [%rd62+3840];
	fma.rn.f32 	%f205, %f177, %f201, %f176;
	fma.rn.f32 	%f327, %f178, %f202, %f205;
	add.s64 	%rd75, %rd75, 5120;
	add.s64 	%rd74, %rd74, 5120;
	add.s32 	%r223, %r223, 640;
	setp.lt.s32 	%p6, %r223, %r15;
	@%p6 bra 	$L__BB36_9;

	st.local.f32 	[%rd3], %f331;
	st.local.f32 	[%rd3+4], %f330;
	st.local.f32 	[%rd3+8], %f329;
	st.local.f32 	[%rd3+12], %f328;
	st.local.f32 	[%rd3+16], %f327;

$L__BB36_11:
	shr.s32 	%r43, %r3, 31;
	shr.u32 	%r44, %r43, 27;
	add.s32 	%r45, %r3, %r44;
	shr.s32 	%r46, %r45, 5;
	shl.b32 	%r47, %r46, 2;
	add.s32 	%r14, %r28, %r47;
	mov.u32 	%r49, 2;
	mov.b32 	%r50, %f331;
	mov.u32 	%r51, 31;
	mov.u32 	%r52, 16;
	mov.u32 	%r53, -1;
	shfl.sync.bfly.b32 	%r54|%p7, %r50, %r52, %r51, %r53;
	mov.b32 	%f206, %r54;
	add.f32 	%f207, %f331, %f206;
	mov.b32 	%r55, %f207;
	mov.u32 	%r56, 8;
	shfl.sync.bfly.b32 	%r57|%p8, %r55, %r56, %r51, %r53;
	mov.b32 	%f208, %r57;
	add.f32 	%f209, %f207, %f208;
	mov.b32 	%r58, %f209;
	mov.u32 	%r59, 4;
	shfl.sync.bfly.b32 	%r60|%p9, %r58, %r59, %r51, %r53;
	mov.b32 	%f210, %r60;
	add.f32 	%f211, %f209, %f210;
	mov.b32 	%r61, %f211;
	shfl.sync.bfly.b32 	%r62|%p10, %r61, %r49, %r51, %r53;
	mov.b32 	%f212, %r62;
	add.f32 	%f213, %f211, %f212;
	mov.b32 	%r63, %f213;
	mov.u32 	%r64, 1;
	shfl.sync.bfly.b32 	%r65|%p11, %r63, %r64, %r51, %r53;
	mov.b32 	%f214, %r65;
	add.f32 	%f215, %f213, %f214;
	st.local.f32 	[%rd3], %f215;
	st.shared.f32 	[%r14], %f215;
	bar.sync 	0;
	@%p1 bra 	$L__BB36_13;

	ld.shared.f32 	%f216, [%r4];
	mov.b32 	%r66, %f216;
	shfl.sync.bfly.b32 	%r70|%p13, %r66, %r52, %r51, %r53;
	mov.b32 	%f217, %r70;
	add.f32 	%f218, %f216, %f217;
	mov.b32 	%r71, %f218;
	shfl.sync.bfly.b32 	%r73|%p14, %r71, %r56, %r51, %r53;
	mov.b32 	%f219, %r73;
	add.f32 	%f220, %f218, %f219;
	mov.b32 	%r74, %f220;
	shfl.sync.bfly.b32 	%r76|%p15, %r74, %r59, %r51, %r53;
	mov.b32 	%f221, %r76;
	add.f32 	%f222, %f220, %f221;
	mov.b32 	%r77, %f222;
	shfl.sync.bfly.b32 	%r79|%p16, %r77, %r49, %r51, %r53;
	mov.b32 	%f223, %r79;
	add.f32 	%f224, %f222, %f223;
	mov.b32 	%r80, %f224;
	shfl.sync.bfly.b32 	%r82|%p17, %r80, %r64, %r51, %r53;
	mov.b32 	%f225, %r82;
	add.f32 	%f226, %f224, %f225;
	st.local.f32 	[%rd3], %f226;

$L__BB36_13:
	bar.sync 	0;
	mov.b32 	%r83, %f330;
	shfl.sync.bfly.b32 	%r87|%p19, %r83, %r52, %r51, %r53;
	mov.b32 	%f227, %r87;
	add.f32 	%f228, %f330, %f227;
	mov.b32 	%r88, %f228;
	shfl.sync.bfly.b32 	%r90|%p20, %r88, %r56, %r51, %r53;
	mov.b32 	%f229, %r90;
	add.f32 	%f230, %f228, %f229;
	mov.b32 	%r91, %f230;
	shfl.sync.bfly.b32 	%r93|%p21, %r91, %r59, %r51, %r53;
	mov.b32 	%f231, %r93;
	add.f32 	%f232, %f230, %f231;
	mov.b32 	%r94, %f232;
	shfl.sync.bfly.b32 	%r96|%p22, %r94, %r49, %r51, %r53;
	mov.b32 	%f233, %r96;
	add.f32 	%f234, %f232, %f233;
	mov.b32 	%r97, %f234;
	shfl.sync.bfly.b32 	%r99|%p23, %r97, %r64, %r51, %r53;
	mov.b32 	%f235, %r99;
	add.f32 	%f236, %f234, %f235;
	st.local.f32 	[%rd3+4], %f236;
	st.shared.f32 	[%r14], %f236;
	bar.sync 	0;
	@%p1 bra 	$L__BB36_15;

	ld.shared.f32 	%f237, [%r4];
	mov.b32 	%r100, %f237;
	mov.u32 	%r101, 31;
	mov.u32 	%r102, 16;
	mov.u32 	%r103, -1;
	shfl.sync.bfly.b32 	%r104|%p24, %r100, %r102, %r101, %r103;
	mov.b32 	%f238, %r104;
	add.f32 	%f239, %f237, %f238;
	mov.b32 	%r105, %f239;
	mov.u32 	%r106, 8;
	shfl.sync.bfly.b32 	%r107|%p25, %r105, %r106, %r101, %r103;
	mov.b32 	%f240, %r107;
	add.f32 	%f241, %f239, %f240;
	mov.b32 	%r108, %f241;
	mov.u32 	%r109, 4;
	shfl.sync.bfly.b32 	%r110|%p26, %r108, %r109, %r101, %r103;
	mov.b32 	%f242, %r110;
	add.f32 	%f243, %f241, %f242;
	mov.b32 	%r111, %f243;
	mov.u32 	%r112, 2;
	shfl.sync.bfly.b32 	%r113|%p27, %r111, %r112, %r101, %r103;
	mov.b32 	%f244, %r113;
	add.f32 	%f245, %f243, %f244;
	mov.b32 	%r114, %f245;
	mov.u32 	%r115, 1;
	shfl.sync.bfly.b32 	%r116|%p28, %r114, %r115, %r101, %r103;
	mov.b32 	%f246, %r116;
	add.f32 	%f247, %f245, %f246;
	st.local.f32 	[%rd3+4], %f247;

$L__BB36_15:
	bar.sync 	0;
	mov.b32 	%r117, %f329;
	mov.u32 	%r118, 31;
	mov.u32 	%r119, 16;
	mov.u32 	%r120, -1;
	shfl.sync.bfly.b32 	%r121|%p30, %r117, %r119, %r118, %r120;
	mov.b32 	%f248, %r121;
	add.f32 	%f249, %f329, %f248;
	mov.b32 	%r122, %f249;
	mov.u32 	%r123, 8;
	shfl.sync.bfly.b32 	%r124|%p31, %r122, %r123, %r118, %r120;
	mov.b32 	%f250, %r124;
	add.f32 	%f251, %f249, %f250;
	mov.b32 	%r125, %f251;
	mov.u32 	%r126, 4;
	shfl.sync.bfly.b32 	%r127|%p32, %r125, %r126, %r118, %r120;
	mov.b32 	%f252, %r127;
	add.f32 	%f253, %f251, %f252;
	mov.b32 	%r128, %f253;
	mov.u32 	%r129, 2;
	shfl.sync.bfly.b32 	%r130|%p33, %r128, %r129, %r118, %r120;
	mov.b32 	%f254, %r130;
	add.f32 	%f255, %f253, %f254;
	mov.b32 	%r131, %f255;
	mov.u32 	%r132, 1;
	shfl.sync.bfly.b32 	%r133|%p34, %r131, %r132, %r118, %r120;
	mov.b32 	%f256, %r133;
	add.f32 	%f257, %f255, %f256;
	st.local.f32 	[%rd3+8], %f257;
	st.shared.f32 	[%r14], %f257;
	bar.sync 	0;
	@%p1 bra 	$L__BB36_17;

	ld.shared.f32 	%f258, [%r4];
	mov.b32 	%r134, %f258;
	shfl.sync.bfly.b32 	%r138|%p35, %r134, %r119, %r118, %r120;
	mov.b32 	%f259, %r138;
	add.f32 	%f260, %f258, %f259;
	mov.b32 	%r139, %f260;
	shfl.sync.bfly.b32 	%r141|%p36, %r139, %r123, %r118, %r120;
	mov.b32 	%f261, %r141;
	add.f32 	%f262, %f260, %f261;
	mov.b32 	%r142, %f262;
	shfl.sync.bfly.b32 	%r144|%p37, %r142, %r126, %r118, %r120;
	mov.b32 	%f263, %r144;
	add.f32 	%f264, %f262, %f263;
	mov.b32 	%r145, %f264;
	shfl.sync.bfly.b32 	%r147|%p38, %r145, %r129, %r118, %r120;
	mov.b32 	%f265, %r147;
	add.f32 	%f266, %f264, %f265;
	mov.b32 	%r148, %f266;
	shfl.sync.bfly.b32 	%r150|%p39, %r148, %r132, %r118, %r120;
	mov.b32 	%f267, %r150;
	add.f32 	%f268, %f266, %f267;
	st.local.f32 	[%rd3+8], %f268;

$L__BB36_17:
	bar.sync 	0;
	mov.b32 	%r151, %f328;
	shfl.sync.bfly.b32 	%r155|%p41, %r151, %r119, %r118, %r120;
	mov.b32 	%f269, %r155;
	add.f32 	%f270, %f328, %f269;
	mov.b32 	%r156, %f270;
	shfl.sync.bfly.b32 	%r158|%p42, %r156, %r123, %r118, %r120;
	mov.b32 	%f271, %r158;
	add.f32 	%f272, %f270, %f271;
	mov.b32 	%r159, %f272;
	shfl.sync.bfly.b32 	%r161|%p43, %r159, %r126, %r118, %r120;
	mov.b32 	%f273, %r161;
	add.f32 	%f274, %f272, %f273;
	mov.b32 	%r162, %f274;
	shfl.sync.bfly.b32 	%r164|%p44, %r162, %r129, %r118, %r120;
	mov.b32 	%f275, %r164;
	add.f32 	%f276, %f274, %f275;
	mov.b32 	%r165, %f276;
	shfl.sync.bfly.b32 	%r167|%p45, %r165, %r132, %r118, %r120;
	mov.b32 	%f277, %r167;
	add.f32 	%f278, %f276, %f277;
	st.local.f32 	[%rd3+12], %f278;
	st.shared.f32 	[%r14], %f278;
	bar.sync 	0;
	@%p1 bra 	$L__BB36_19;

	ld.shared.f32 	%f279, [%r4];
	mov.b32 	%r168, %f279;
	mov.u32 	%r169, 31;
	mov.u32 	%r170, 16;
	mov.u32 	%r171, -1;
	shfl.sync.bfly.b32 	%r172|%p46, %r168, %r170, %r169, %r171;
	mov.b32 	%f280, %r172;
	add.f32 	%f281, %f279, %f280;
	mov.b32 	%r173, %f281;
	mov.u32 	%r174, 8;
	shfl.sync.bfly.b32 	%r175|%p47, %r173, %r174, %r169, %r171;
	mov.b32 	%f282, %r175;
	add.f32 	%f283, %f281, %f282;
	mov.b32 	%r176, %f283;
	mov.u32 	%r177, 4;
	shfl.sync.bfly.b32 	%r178|%p48, %r176, %r177, %r169, %r171;
	mov.b32 	%f284, %r178;
	add.f32 	%f285, %f283, %f284;
	mov.b32 	%r179, %f285;
	mov.u32 	%r180, 2;
	shfl.sync.bfly.b32 	%r181|%p49, %r179, %r180, %r169, %r171;
	mov.b32 	%f286, %r181;
	add.f32 	%f287, %f285, %f286;
	mov.b32 	%r182, %f287;
	mov.u32 	%r183, 1;
	shfl.sync.bfly.b32 	%r184|%p50, %r182, %r183, %r169, %r171;
	mov.b32 	%f288, %r184;
	add.f32 	%f289, %f287, %f288;
	st.local.f32 	[%rd3+12], %f289;

$L__BB36_19:
	bar.sync 	0;
	mov.b32 	%r185, %f327;
	mov.u32 	%r186, 31;
	mov.u32 	%r187, 16;
	mov.u32 	%r188, -1;
	shfl.sync.bfly.b32 	%r189|%p52, %r185, %r187, %r186, %r188;
	mov.b32 	%f290, %r189;
	add.f32 	%f291, %f327, %f290;
	mov.b32 	%r190, %f291;
	mov.u32 	%r191, 8;
	shfl.sync.bfly.b32 	%r192|%p53, %r190, %r191, %r186, %r188;
	mov.b32 	%f292, %r192;
	add.f32 	%f293, %f291, %f292;
	mov.b32 	%r193, %f293;
	mov.u32 	%r194, 4;
	shfl.sync.bfly.b32 	%r195|%p54, %r193, %r194, %r186, %r188;
	mov.b32 	%f294, %r195;
	add.f32 	%f295, %f293, %f294;
	mov.b32 	%r196, %f295;
	mov.u32 	%r197, 2;
	shfl.sync.bfly.b32 	%r198|%p55, %r196, %r197, %r186, %r188;
	mov.b32 	%f296, %r198;
	add.f32 	%f297, %f295, %f296;
	mov.b32 	%r199, %f297;
	mov.u32 	%r200, 1;
	shfl.sync.bfly.b32 	%r201|%p56, %r199, %r200, %r186, %r188;
	mov.b32 	%f298, %r201;
	add.f32 	%f299, %f297, %f298;
	st.local.f32 	[%rd3+16], %f299;
	st.shared.f32 	[%r14], %f299;
	bar.sync 	0;
	@%p1 bra 	$L__BB36_21;

	ld.shared.f32 	%f300, [%r4];
	mov.b32 	%r202, %f300;
	shfl.sync.bfly.b32 	%r206|%p57, %r202, %r187, %r186, %r188;
	mov.b32 	%f301, %r206;
	add.f32 	%f302, %f300, %f301;
	mov.b32 	%r207, %f302;
	shfl.sync.bfly.b32 	%r209|%p58, %r207, %r191, %r186, %r188;
	mov.b32 	%f303, %r209;
	add.f32 	%f304, %f302, %f303;
	mov.b32 	%r210, %f304;
	shfl.sync.bfly.b32 	%r212|%p59, %r210, %r194, %r186, %r188;
	mov.b32 	%f305, %r212;
	add.f32 	%f306, %f304, %f305;
	mov.b32 	%r213, %f306;
	shfl.sync.bfly.b32 	%r215|%p60, %r213, %r197, %r186, %r188;
	mov.b32 	%f307, %r215;
	add.f32 	%f308, %f306, %f307;
	mov.b32 	%r216, %f308;
	shfl.sync.bfly.b32 	%r218|%p61, %r216, %r200, %r186, %r188;
	mov.b32 	%f309, %r218;
	add.f32 	%f310, %f308, %f309;
	st.local.f32 	[%rd3+16], %f310;

$L__BB36_21:
	bar.sync 	0;
	setp.gt.s32 	%p62, %r3, 4;
	@%p62 bra 	$L__BB36_23;

	mul.wide.s32 	%rd64, %r3, 4;
	add.s64 	%rd65, %rd3, %rd64;
	ld.local.f32 	%f311, [%rd65];
	mad.lo.s32 	%r219, %r3, %r17, %r2;
	cvt.s64.s32 	%rd66, %r219;
	mul.lo.s32 	%r220, %r1, %r18;
	cvt.s64.s32 	%rd67, %r220;
	add.s64 	%rd68, %rd67, %rd66;
	cvta.to.global.u64 	%rd69, %rd27;
	shl.b64 	%rd70, %rd68, 2;
	add.s64 	%rd71, %rd69, %rd70;
	st.global.f32 	[%rd71], %f311;

$L__BB36_23:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_6_bs_160
.visible .entry ggml_matvec_f32_ncols_6_bs_160(
	.param .u64 ggml_matvec_f32_ncols_6_bs_160_param_0,
	.param .u64 ggml_matvec_f32_ncols_6_bs_160_param_1,
	.param .u64 ggml_matvec_f32_ncols_6_bs_160_param_2,
	.param .u32 ggml_matvec_f32_ncols_6_bs_160_param_3,
	.param .u32 ggml_matvec_f32_ncols_6_bs_160_param_4,
	.param .u32 ggml_matvec_f32_ncols_6_bs_160_param_5,
	.param .u32 ggml_matvec_f32_ncols_6_bs_160_param_6,
	.param .u32 ggml_matvec_f32_ncols_6_bs_160_param_7,
	.param .u32 ggml_matvec_f32_ncols_6_bs_160_param_8,
	.param .u32 ggml_matvec_f32_ncols_6_bs_160_param_9,
	.param .u32 ggml_matvec_f32_ncols_6_bs_160_param_10,
	.param .u32 ggml_matvec_f32_ncols_6_bs_160_param_11
)
{
	.local .align 8 .b8 	__local_depot37[24];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<75>;
	.reg .f32 	%f<296>;
	.reg .b32 	%r<251>;
	.reg .b64 	%rd<70>;


	mov.u64 	%SPL, __local_depot37;
	ld.param.u64 	%rd20, [ggml_matvec_f32_ncols_6_bs_160_param_0];
	ld.param.u64 	%rd21, [ggml_matvec_f32_ncols_6_bs_160_param_1];
	ld.param.u64 	%rd19, [ggml_matvec_f32_ncols_6_bs_160_param_2];
	ld.param.u32 	%r11, [ggml_matvec_f32_ncols_6_bs_160_param_3];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_6_bs_160_param_5];
	ld.param.u32 	%r12, [ggml_matvec_f32_ncols_6_bs_160_param_6];
	ld.param.u32 	%r13, [ggml_matvec_f32_ncols_6_bs_160_param_7];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_6_bs_160_param_8];
	ld.param.u32 	%r17, [ggml_matvec_f32_ncols_6_bs_160_param_9];
	ld.param.u32 	%r18, [ggml_matvec_f32_ncols_6_bs_160_param_10];
	ld.param.u32 	%r14, [ggml_matvec_f32_ncols_6_bs_160_param_11];
	cvta.to.global.u64 	%rd69, %rd21;
	add.u64 	%rd2, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r19, %r1, %r16;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r20, %r2, %r15;
	mad.lo.s32 	%r21, %r19, %r17, %r20;
	cvt.s64.s32 	%rd3, %r21;
	cvta.to.global.u64 	%rd4, %rd20;
	mul.lo.s32 	%r22, %r1, %r18;
	cvt.s64.s32 	%rd5, %r22;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r23, %r3, 2;
	mov.u32 	%r24, data_mmv;
	add.s32 	%r4, %r24, %r23;
	@%p1 bra 	$L__BB37_2;

	mov.u32 	%r25, 0;
	st.shared.u32 	[%r4], %r25;

$L__BB37_2:
	bar.sync 	0;
	mov.f32 	%f290, 0f00000000;
	st.local.v2.f32 	[%rd2], {%f290, %f290};
	st.local.v2.f32 	[%rd2+8], {%f290, %f290};
	st.local.v2.f32 	[%rd2+16], {%f290, %f290};
	setp.ge.s32 	%p2, %r3, %r11;
	mov.f32 	%f291, %f290;
	mov.f32 	%f292, %f290;
	mov.f32 	%f293, %f290;
	mov.f32 	%f294, %f290;
	mov.f32 	%f295, %f290;
	@%p2 bra 	$L__BB37_9;

	not.b32 	%r26, %r3;
	add.s32 	%r5, %r26, %r11;
	mul.wide.u32 	%rd23, %r5, -858993459;
	shr.u64 	%rd24, %rd23, 39;
	and.b64  	%rd25, %rd24, 1;
	setp.eq.b64 	%p3, %rd25, 1;
	mov.pred 	%p4, 0;
	xor.pred  	%p5, %p3, %p4;
	mov.f32 	%f290, 0f00000000;
	mov.u32 	%r250, %r3;
	@%p5 bra 	$L__BB37_5;

	shl.b64 	%rd26, %rd5, 2;
	add.s64 	%rd27, %rd69, %rd26;
	shl.b64 	%rd28, %rd3, 2;
	add.s64 	%rd29, %rd4, %rd28;
	mul.wide.s32 	%rd30, %r3, 8;
	add.s64 	%rd31, %rd29, %rd30;
	ld.global.nc.v2.f32 	{%f43, %f44}, [%rd31];
	add.s64 	%rd32, %rd27, %rd30;
	ld.global.nc.v2.f32 	{%f47, %f48}, [%rd32];
	fma.rn.f32 	%f51, %f43, %f47, 0f00000000;
	fma.rn.f32 	%f295, %f44, %f48, %f51;
	mul.wide.s32 	%rd33, %r12, 8;
	add.s64 	%rd34, %rd32, %rd33;
	ld.global.nc.v2.f32 	{%f52, %f53}, [%rd34];
	fma.rn.f32 	%f56, %f43, %f52, 0f00000000;
	fma.rn.f32 	%f294, %f44, %f53, %f56;
	st.local.v2.f32 	[%rd2], {%f295, %f294};
	add.s32 	%r27, %r3, %r12;
	add.s32 	%r28, %r27, %r12;
	mul.wide.s32 	%rd35, %r28, 8;
	add.s64 	%rd36, %rd27, %rd35;
	ld.global.nc.v2.f32 	{%f57, %f58}, [%rd36];
	fma.rn.f32 	%f61, %f43, %f57, 0f00000000;
	fma.rn.f32 	%f293, %f44, %f58, %f61;
	add.s64 	%rd37, %rd36, %rd33;
	ld.global.nc.v2.f32 	{%f62, %f63}, [%rd37];
	fma.rn.f32 	%f66, %f43, %f62, 0f00000000;
	fma.rn.f32 	%f292, %f44, %f63, %f66;
	st.local.v2.f32 	[%rd2+8], {%f293, %f292};
	add.s64 	%rd38, %rd37, %rd33;
	ld.global.nc.v2.f32 	{%f67, %f68}, [%rd38];
	fma.rn.f32 	%f71, %f43, %f67, 0f00000000;
	fma.rn.f32 	%f291, %f44, %f68, %f71;
	add.s64 	%rd39, %rd38, %rd33;
	ld.global.nc.v2.f32 	{%f72, %f73}, [%rd39];
	fma.rn.f32 	%f76, %f43, %f72, 0f00000000;
	fma.rn.f32 	%f290, %f44, %f73, %f76;
	st.local.v2.f32 	[%rd2+16], {%f291, %f290};
	add.s32 	%r250, %r3, 160;

$L__BB37_5:
	setp.lt.u32 	%p6, %r5, 160;
	@%p6 bra 	$L__BB37_9;

	add.s32 	%r29, %r250, %r12;
	add.s32 	%r30, %r29, 160;
	mul.wide.s32 	%rd40, %r30, 8;
	shl.b64 	%rd41, %rd5, 2;
	add.s64 	%rd7, %rd40, %rd41;
	shl.b32 	%r31, %r12, 1;
	add.s32 	%r32, %r250, %r31;
	mad.lo.s32 	%r33, %r12, 3, %r250;
	shl.b32 	%r34, %r12, 2;
	add.s32 	%r35, %r250, %r34;
	mad.lo.s32 	%r36, %r12, 5, %r250;
	mul.wide.s32 	%rd42, %r32, 8;
	add.s64 	%rd8, %rd42, %rd41;
	mul.wide.s32 	%rd43, %r33, 8;
	add.s64 	%rd9, %rd43, %rd41;
	mul.wide.s32 	%rd44, %r35, 8;
	add.s64 	%rd10, %rd44, %rd41;
	mul.wide.s32 	%rd45, %r36, 8;
	add.s64 	%rd11, %rd45, %rd41;
	mul.wide.s32 	%rd46, %r250, 2;
	add.s64 	%rd47, %rd46, %rd3;
	shl.b64 	%rd48, %rd47, 2;
	add.s64 	%rd49, %rd4, %rd48;
	add.s64 	%rd68, %rd49, 1280;
	mul.wide.s32 	%rd50, %r250, 8;
	mul.wide.s32 	%rd51, %r12, 8;
	add.s64 	%rd52, %rd50, %rd51;
	add.s64 	%rd13, %rd52, %rd41;
	add.s64 	%rd14, %rd50, %rd41;

$L__BB37_7:
	ld.global.nc.v2.f32 	{%f77, %f78}, [%rd68+-1280];
	add.s64 	%rd53, %rd69, %rd14;
	ld.global.nc.v2.f32 	{%f81, %f82}, [%rd53];
	fma.rn.f32 	%f85, %f77, %f81, %f295;
	fma.rn.f32 	%f86, %f78, %f82, %f85;
	add.s64 	%rd54, %rd69, %rd13;
	ld.global.nc.v2.f32 	{%f87, %f88}, [%rd54];
	fma.rn.f32 	%f91, %f77, %f87, %f294;
	fma.rn.f32 	%f92, %f78, %f88, %f91;
	add.s64 	%rd55, %rd69, %rd8;
	ld.global.nc.v2.f32 	{%f93, %f94}, [%rd55];
	fma.rn.f32 	%f97, %f77, %f93, %f293;
	fma.rn.f32 	%f98, %f78, %f94, %f97;
	add.s64 	%rd56, %rd69, %rd9;
	ld.global.nc.v2.f32 	{%f99, %f100}, [%rd56];
	fma.rn.f32 	%f103, %f77, %f99, %f292;
	fma.rn.f32 	%f104, %f78, %f100, %f103;
	add.s64 	%rd57, %rd69, %rd10;
	ld.global.nc.v2.f32 	{%f105, %f106}, [%rd57];
	fma.rn.f32 	%f109, %f77, %f105, %f291;
	fma.rn.f32 	%f110, %f78, %f106, %f109;
	add.s64 	%rd58, %rd69, %rd11;
	ld.global.nc.v2.f32 	{%f111, %f112}, [%rd58];
	fma.rn.f32 	%f115, %f77, %f111, %f290;
	fma.rn.f32 	%f116, %f78, %f112, %f115;
	ld.global.nc.v2.f32 	{%f117, %f118}, [%rd68];
	ld.global.nc.v2.f32 	{%f121, %f122}, [%rd53+1280];
	fma.rn.f32 	%f125, %f117, %f121, %f86;
	fma.rn.f32 	%f295, %f118, %f122, %f125;
	add.s64 	%rd59, %rd69, %rd7;
	ld.global.nc.v2.f32 	{%f126, %f127}, [%rd59];
	fma.rn.f32 	%f130, %f117, %f126, %f92;
	fma.rn.f32 	%f294, %f118, %f127, %f130;
	ld.global.nc.v2.f32 	{%f131, %f132}, [%rd55+1280];
	fma.rn.f32 	%f135, %f117, %f131, %f98;
	fma.rn.f32 	%f293, %f118, %f132, %f135;
	ld.global.nc.v2.f32 	{%f136, %f137}, [%rd56+1280];
	fma.rn.f32 	%f140, %f117, %f136, %f104;
	fma.rn.f32 	%f292, %f118, %f137, %f140;
	ld.global.nc.v2.f32 	{%f141, %f142}, [%rd57+1280];
	fma.rn.f32 	%f145, %f117, %f141, %f110;
	fma.rn.f32 	%f291, %f118, %f142, %f145;
	ld.global.nc.v2.f32 	{%f146, %f147}, [%rd58+1280];
	fma.rn.f32 	%f150, %f117, %f146, %f116;
	fma.rn.f32 	%f290, %f118, %f147, %f150;
	add.s64 	%rd69, %rd69, 2560;
	add.s64 	%rd68, %rd68, 2560;
	add.s32 	%r250, %r250, 320;
	setp.lt.s32 	%p7, %r250, %r11;
	@%p7 bra 	$L__BB37_7;

	st.local.v2.f32 	[%rd2], {%f295, %f294};
	st.local.v2.f32 	[%rd2+8], {%f293, %f292};
	st.local.v2.f32 	[%rd2+16], {%f291, %f290};

$L__BB37_9:
	shr.s32 	%r37, %r3, 31;
	shr.u32 	%r38, %r37, 27;
	add.s32 	%r39, %r3, %r38;
	shr.s32 	%r40, %r39, 5;
	shl.b32 	%r41, %r40, 2;
	add.s32 	%r10, %r24, %r41;
	mov.u32 	%r43, 2;
	mov.b32 	%r44, %f295;
	mov.u32 	%r45, 31;
	mov.u32 	%r46, 16;
	mov.u32 	%r47, -1;
	shfl.sync.bfly.b32 	%r48|%p8, %r44, %r46, %r45, %r47;
	mov.b32 	%f151, %r48;
	add.f32 	%f152, %f295, %f151;
	mov.b32 	%r49, %f152;
	mov.u32 	%r50, 8;
	shfl.sync.bfly.b32 	%r51|%p9, %r49, %r50, %r45, %r47;
	mov.b32 	%f153, %r51;
	add.f32 	%f154, %f152, %f153;
	mov.b32 	%r52, %f154;
	mov.u32 	%r53, 4;
	shfl.sync.bfly.b32 	%r54|%p10, %r52, %r53, %r45, %r47;
	mov.b32 	%f155, %r54;
	add.f32 	%f156, %f154, %f155;
	mov.b32 	%r55, %f156;
	shfl.sync.bfly.b32 	%r56|%p11, %r55, %r43, %r45, %r47;
	mov.b32 	%f157, %r56;
	add.f32 	%f158, %f156, %f157;
	mov.b32 	%r57, %f158;
	mov.u32 	%r58, 1;
	shfl.sync.bfly.b32 	%r59|%p12, %r57, %r58, %r45, %r47;
	mov.b32 	%f159, %r59;
	add.f32 	%f160, %f158, %f159;
	st.local.f32 	[%rd2], %f160;
	st.shared.f32 	[%r10], %f160;
	bar.sync 	0;
	@%p1 bra 	$L__BB37_11;

	ld.shared.f32 	%f161, [%r4];
	mov.b32 	%r60, %f161;
	shfl.sync.bfly.b32 	%r64|%p14, %r60, %r46, %r45, %r47;
	mov.b32 	%f162, %r64;
	add.f32 	%f163, %f161, %f162;
	mov.b32 	%r65, %f163;
	shfl.sync.bfly.b32 	%r67|%p15, %r65, %r50, %r45, %r47;
	mov.b32 	%f164, %r67;
	add.f32 	%f165, %f163, %f164;
	mov.b32 	%r68, %f165;
	shfl.sync.bfly.b32 	%r70|%p16, %r68, %r53, %r45, %r47;
	mov.b32 	%f166, %r70;
	add.f32 	%f167, %f165, %f166;
	mov.b32 	%r71, %f167;
	shfl.sync.bfly.b32 	%r73|%p17, %r71, %r43, %r45, %r47;
	mov.b32 	%f168, %r73;
	add.f32 	%f169, %f167, %f168;
	mov.b32 	%r74, %f169;
	shfl.sync.bfly.b32 	%r76|%p18, %r74, %r58, %r45, %r47;
	mov.b32 	%f170, %r76;
	add.f32 	%f171, %f169, %f170;
	st.local.f32 	[%rd2], %f171;

$L__BB37_11:
	bar.sync 	0;
	mov.b32 	%r77, %f294;
	shfl.sync.bfly.b32 	%r81|%p20, %r77, %r46, %r45, %r47;
	mov.b32 	%f172, %r81;
	add.f32 	%f173, %f294, %f172;
	mov.b32 	%r82, %f173;
	shfl.sync.bfly.b32 	%r84|%p21, %r82, %r50, %r45, %r47;
	mov.b32 	%f174, %r84;
	add.f32 	%f175, %f173, %f174;
	mov.b32 	%r85, %f175;
	shfl.sync.bfly.b32 	%r87|%p22, %r85, %r53, %r45, %r47;
	mov.b32 	%f176, %r87;
	add.f32 	%f177, %f175, %f176;
	mov.b32 	%r88, %f177;
	shfl.sync.bfly.b32 	%r90|%p23, %r88, %r43, %r45, %r47;
	mov.b32 	%f178, %r90;
	add.f32 	%f179, %f177, %f178;
	mov.b32 	%r91, %f179;
	shfl.sync.bfly.b32 	%r93|%p24, %r91, %r58, %r45, %r47;
	mov.b32 	%f180, %r93;
	add.f32 	%f181, %f179, %f180;
	st.local.f32 	[%rd2+4], %f181;
	st.shared.f32 	[%r10], %f181;
	bar.sync 	0;
	@%p1 bra 	$L__BB37_13;

	ld.shared.f32 	%f182, [%r4];
	mov.b32 	%r94, %f182;
	mov.u32 	%r95, 31;
	mov.u32 	%r96, 16;
	mov.u32 	%r97, -1;
	shfl.sync.bfly.b32 	%r98|%p25, %r94, %r96, %r95, %r97;
	mov.b32 	%f183, %r98;
	add.f32 	%f184, %f182, %f183;
	mov.b32 	%r99, %f184;
	mov.u32 	%r100, 8;
	shfl.sync.bfly.b32 	%r101|%p26, %r99, %r100, %r95, %r97;
	mov.b32 	%f185, %r101;
	add.f32 	%f186, %f184, %f185;
	mov.b32 	%r102, %f186;
	mov.u32 	%r103, 4;
	shfl.sync.bfly.b32 	%r104|%p27, %r102, %r103, %r95, %r97;
	mov.b32 	%f187, %r104;
	add.f32 	%f188, %f186, %f187;
	mov.b32 	%r105, %f188;
	mov.u32 	%r106, 2;
	shfl.sync.bfly.b32 	%r107|%p28, %r105, %r106, %r95, %r97;
	mov.b32 	%f189, %r107;
	add.f32 	%f190, %f188, %f189;
	mov.b32 	%r108, %f190;
	mov.u32 	%r109, 1;
	shfl.sync.bfly.b32 	%r110|%p29, %r108, %r109, %r95, %r97;
	mov.b32 	%f191, %r110;
	add.f32 	%f192, %f190, %f191;
	st.local.f32 	[%rd2+4], %f192;

$L__BB37_13:
	bar.sync 	0;
	mov.b32 	%r111, %f293;
	mov.u32 	%r112, 31;
	mov.u32 	%r113, 16;
	mov.u32 	%r114, -1;
	shfl.sync.bfly.b32 	%r115|%p31, %r111, %r113, %r112, %r114;
	mov.b32 	%f193, %r115;
	add.f32 	%f194, %f293, %f193;
	mov.b32 	%r116, %f194;
	mov.u32 	%r117, 8;
	shfl.sync.bfly.b32 	%r118|%p32, %r116, %r117, %r112, %r114;
	mov.b32 	%f195, %r118;
	add.f32 	%f196, %f194, %f195;
	mov.b32 	%r119, %f196;
	mov.u32 	%r120, 4;
	shfl.sync.bfly.b32 	%r121|%p33, %r119, %r120, %r112, %r114;
	mov.b32 	%f197, %r121;
	add.f32 	%f198, %f196, %f197;
	mov.b32 	%r122, %f198;
	mov.u32 	%r123, 2;
	shfl.sync.bfly.b32 	%r124|%p34, %r122, %r123, %r112, %r114;
	mov.b32 	%f199, %r124;
	add.f32 	%f200, %f198, %f199;
	mov.b32 	%r125, %f200;
	mov.u32 	%r126, 1;
	shfl.sync.bfly.b32 	%r127|%p35, %r125, %r126, %r112, %r114;
	mov.b32 	%f201, %r127;
	add.f32 	%f202, %f200, %f201;
	st.local.f32 	[%rd2+8], %f202;
	st.shared.f32 	[%r10], %f202;
	bar.sync 	0;
	@%p1 bra 	$L__BB37_15;

	ld.shared.f32 	%f203, [%r4];
	mov.b32 	%r128, %f203;
	shfl.sync.bfly.b32 	%r132|%p36, %r128, %r113, %r112, %r114;
	mov.b32 	%f204, %r132;
	add.f32 	%f205, %f203, %f204;
	mov.b32 	%r133, %f205;
	shfl.sync.bfly.b32 	%r135|%p37, %r133, %r117, %r112, %r114;
	mov.b32 	%f206, %r135;
	add.f32 	%f207, %f205, %f206;
	mov.b32 	%r136, %f207;
	shfl.sync.bfly.b32 	%r138|%p38, %r136, %r120, %r112, %r114;
	mov.b32 	%f208, %r138;
	add.f32 	%f209, %f207, %f208;
	mov.b32 	%r139, %f209;
	shfl.sync.bfly.b32 	%r141|%p39, %r139, %r123, %r112, %r114;
	mov.b32 	%f210, %r141;
	add.f32 	%f211, %f209, %f210;
	mov.b32 	%r142, %f211;
	shfl.sync.bfly.b32 	%r144|%p40, %r142, %r126, %r112, %r114;
	mov.b32 	%f212, %r144;
	add.f32 	%f213, %f211, %f212;
	st.local.f32 	[%rd2+8], %f213;

$L__BB37_15:
	bar.sync 	0;
	mov.b32 	%r145, %f292;
	shfl.sync.bfly.b32 	%r149|%p42, %r145, %r113, %r112, %r114;
	mov.b32 	%f214, %r149;
	add.f32 	%f215, %f292, %f214;
	mov.b32 	%r150, %f215;
	shfl.sync.bfly.b32 	%r152|%p43, %r150, %r117, %r112, %r114;
	mov.b32 	%f216, %r152;
	add.f32 	%f217, %f215, %f216;
	mov.b32 	%r153, %f217;
	shfl.sync.bfly.b32 	%r155|%p44, %r153, %r120, %r112, %r114;
	mov.b32 	%f218, %r155;
	add.f32 	%f219, %f217, %f218;
	mov.b32 	%r156, %f219;
	shfl.sync.bfly.b32 	%r158|%p45, %r156, %r123, %r112, %r114;
	mov.b32 	%f220, %r158;
	add.f32 	%f221, %f219, %f220;
	mov.b32 	%r159, %f221;
	shfl.sync.bfly.b32 	%r161|%p46, %r159, %r126, %r112, %r114;
	mov.b32 	%f222, %r161;
	add.f32 	%f223, %f221, %f222;
	st.local.f32 	[%rd2+12], %f223;
	st.shared.f32 	[%r10], %f223;
	bar.sync 	0;
	@%p1 bra 	$L__BB37_17;

	ld.shared.f32 	%f224, [%r4];
	mov.b32 	%r162, %f224;
	mov.u32 	%r163, 31;
	mov.u32 	%r164, 16;
	mov.u32 	%r165, -1;
	shfl.sync.bfly.b32 	%r166|%p47, %r162, %r164, %r163, %r165;
	mov.b32 	%f225, %r166;
	add.f32 	%f226, %f224, %f225;
	mov.b32 	%r167, %f226;
	mov.u32 	%r168, 8;
	shfl.sync.bfly.b32 	%r169|%p48, %r167, %r168, %r163, %r165;
	mov.b32 	%f227, %r169;
	add.f32 	%f228, %f226, %f227;
	mov.b32 	%r170, %f228;
	mov.u32 	%r171, 4;
	shfl.sync.bfly.b32 	%r172|%p49, %r170, %r171, %r163, %r165;
	mov.b32 	%f229, %r172;
	add.f32 	%f230, %f228, %f229;
	mov.b32 	%r173, %f230;
	mov.u32 	%r174, 2;
	shfl.sync.bfly.b32 	%r175|%p50, %r173, %r174, %r163, %r165;
	mov.b32 	%f231, %r175;
	add.f32 	%f232, %f230, %f231;
	mov.b32 	%r176, %f232;
	mov.u32 	%r177, 1;
	shfl.sync.bfly.b32 	%r178|%p51, %r176, %r177, %r163, %r165;
	mov.b32 	%f233, %r178;
	add.f32 	%f234, %f232, %f233;
	st.local.f32 	[%rd2+12], %f234;

$L__BB37_17:
	bar.sync 	0;
	mov.b32 	%r179, %f291;
	mov.u32 	%r180, 31;
	mov.u32 	%r181, 16;
	mov.u32 	%r182, -1;
	shfl.sync.bfly.b32 	%r183|%p53, %r179, %r181, %r180, %r182;
	mov.b32 	%f235, %r183;
	add.f32 	%f236, %f291, %f235;
	mov.b32 	%r184, %f236;
	mov.u32 	%r185, 8;
	shfl.sync.bfly.b32 	%r186|%p54, %r184, %r185, %r180, %r182;
	mov.b32 	%f237, %r186;
	add.f32 	%f238, %f236, %f237;
	mov.b32 	%r187, %f238;
	mov.u32 	%r188, 4;
	shfl.sync.bfly.b32 	%r189|%p55, %r187, %r188, %r180, %r182;
	mov.b32 	%f239, %r189;
	add.f32 	%f240, %f238, %f239;
	mov.b32 	%r190, %f240;
	mov.u32 	%r191, 2;
	shfl.sync.bfly.b32 	%r192|%p56, %r190, %r191, %r180, %r182;
	mov.b32 	%f241, %r192;
	add.f32 	%f242, %f240, %f241;
	mov.b32 	%r193, %f242;
	mov.u32 	%r194, 1;
	shfl.sync.bfly.b32 	%r195|%p57, %r193, %r194, %r180, %r182;
	mov.b32 	%f243, %r195;
	add.f32 	%f244, %f242, %f243;
	st.local.f32 	[%rd2+16], %f244;
	st.shared.f32 	[%r10], %f244;
	bar.sync 	0;
	@%p1 bra 	$L__BB37_19;

	ld.shared.f32 	%f245, [%r4];
	mov.b32 	%r196, %f245;
	shfl.sync.bfly.b32 	%r200|%p58, %r196, %r181, %r180, %r182;
	mov.b32 	%f246, %r200;
	add.f32 	%f247, %f245, %f246;
	mov.b32 	%r201, %f247;
	shfl.sync.bfly.b32 	%r203|%p59, %r201, %r185, %r180, %r182;
	mov.b32 	%f248, %r203;
	add.f32 	%f249, %f247, %f248;
	mov.b32 	%r204, %f249;
	shfl.sync.bfly.b32 	%r206|%p60, %r204, %r188, %r180, %r182;
	mov.b32 	%f250, %r206;
	add.f32 	%f251, %f249, %f250;
	mov.b32 	%r207, %f251;
	shfl.sync.bfly.b32 	%r209|%p61, %r207, %r191, %r180, %r182;
	mov.b32 	%f252, %r209;
	add.f32 	%f253, %f251, %f252;
	mov.b32 	%r210, %f253;
	shfl.sync.bfly.b32 	%r212|%p62, %r210, %r194, %r180, %r182;
	mov.b32 	%f254, %r212;
	add.f32 	%f255, %f253, %f254;
	st.local.f32 	[%rd2+16], %f255;

$L__BB37_19:
	bar.sync 	0;
	mov.b32 	%r213, %f290;
	shfl.sync.bfly.b32 	%r217|%p64, %r213, %r181, %r180, %r182;
	mov.b32 	%f256, %r217;
	add.f32 	%f257, %f290, %f256;
	mov.b32 	%r218, %f257;
	shfl.sync.bfly.b32 	%r220|%p65, %r218, %r185, %r180, %r182;
	mov.b32 	%f258, %r220;
	add.f32 	%f259, %f257, %f258;
	mov.b32 	%r221, %f259;
	shfl.sync.bfly.b32 	%r223|%p66, %r221, %r188, %r180, %r182;
	mov.b32 	%f260, %r223;
	add.f32 	%f261, %f259, %f260;
	mov.b32 	%r224, %f261;
	shfl.sync.bfly.b32 	%r226|%p67, %r224, %r191, %r180, %r182;
	mov.b32 	%f262, %r226;
	add.f32 	%f263, %f261, %f262;
	mov.b32 	%r227, %f263;
	shfl.sync.bfly.b32 	%r229|%p68, %r227, %r194, %r180, %r182;
	mov.b32 	%f264, %r229;
	add.f32 	%f265, %f263, %f264;
	st.local.f32 	[%rd2+20], %f265;
	st.shared.f32 	[%r10], %f265;
	bar.sync 	0;
	@%p1 bra 	$L__BB37_21;

	ld.shared.f32 	%f266, [%r4];
	mov.b32 	%r230, %f266;
	mov.u32 	%r231, 31;
	mov.u32 	%r232, 16;
	mov.u32 	%r233, -1;
	shfl.sync.bfly.b32 	%r234|%p69, %r230, %r232, %r231, %r233;
	mov.b32 	%f267, %r234;
	add.f32 	%f268, %f266, %f267;
	mov.b32 	%r235, %f268;
	mov.u32 	%r236, 8;
	shfl.sync.bfly.b32 	%r237|%p70, %r235, %r236, %r231, %r233;
	mov.b32 	%f269, %r237;
	add.f32 	%f270, %f268, %f269;
	mov.b32 	%r238, %f270;
	mov.u32 	%r239, 4;
	shfl.sync.bfly.b32 	%r240|%p71, %r238, %r239, %r231, %r233;
	mov.b32 	%f271, %r240;
	add.f32 	%f272, %f270, %f271;
	mov.b32 	%r241, %f272;
	mov.u32 	%r242, 2;
	shfl.sync.bfly.b32 	%r243|%p72, %r241, %r242, %r231, %r233;
	mov.b32 	%f273, %r243;
	add.f32 	%f274, %f272, %f273;
	mov.b32 	%r244, %f274;
	mov.u32 	%r245, 1;
	shfl.sync.bfly.b32 	%r246|%p73, %r244, %r245, %r231, %r233;
	mov.b32 	%f275, %r246;
	add.f32 	%f276, %f274, %f275;
	st.local.f32 	[%rd2+20], %f276;

$L__BB37_21:
	bar.sync 	0;
	setp.gt.s32 	%p74, %r3, 5;
	@%p74 bra 	$L__BB37_23;

	mul.wide.s32 	%rd60, %r3, 4;
	add.s64 	%rd61, %rd2, %rd60;
	ld.local.f32 	%f277, [%rd61];
	mad.lo.s32 	%r247, %r3, %r13, %r2;
	cvt.s64.s32 	%rd62, %r247;
	mul.lo.s32 	%r248, %r1, %r14;
	cvt.s64.s32 	%rd63, %r248;
	add.s64 	%rd64, %rd63, %rd62;
	cvta.to.global.u64 	%rd65, %rd19;
	shl.b64 	%rd66, %rd64, 2;
	add.s64 	%rd67, %rd65, %rd66;
	st.global.f32 	[%rd67], %f277;

$L__BB37_23:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_7_bs_160
.visible .entry ggml_matvec_f32_ncols_7_bs_160(
	.param .u64 ggml_matvec_f32_ncols_7_bs_160_param_0,
	.param .u64 ggml_matvec_f32_ncols_7_bs_160_param_1,
	.param .u64 ggml_matvec_f32_ncols_7_bs_160_param_2,
	.param .u32 ggml_matvec_f32_ncols_7_bs_160_param_3,
	.param .u32 ggml_matvec_f32_ncols_7_bs_160_param_4,
	.param .u32 ggml_matvec_f32_ncols_7_bs_160_param_5,
	.param .u32 ggml_matvec_f32_ncols_7_bs_160_param_6,
	.param .u32 ggml_matvec_f32_ncols_7_bs_160_param_7,
	.param .u32 ggml_matvec_f32_ncols_7_bs_160_param_8,
	.param .u32 ggml_matvec_f32_ncols_7_bs_160_param_9,
	.param .u32 ggml_matvec_f32_ncols_7_bs_160_param_10,
	.param .u32 ggml_matvec_f32_ncols_7_bs_160_param_11
)
{
	.local .align 4 .b8 	__local_depot38[28];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<86>;
	.reg .f32 	%f<343>;
	.reg .b32 	%r<287>;
	.reg .b64 	%rd<74>;


	mov.u64 	%SPL, __local_depot38;
	ld.param.u64 	%rd21, [ggml_matvec_f32_ncols_7_bs_160_param_0];
	ld.param.u64 	%rd22, [ggml_matvec_f32_ncols_7_bs_160_param_1];
	ld.param.u64 	%rd20, [ggml_matvec_f32_ncols_7_bs_160_param_2];
	ld.param.u32 	%r11, [ggml_matvec_f32_ncols_7_bs_160_param_3];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_7_bs_160_param_5];
	ld.param.u32 	%r12, [ggml_matvec_f32_ncols_7_bs_160_param_6];
	ld.param.u32 	%r13, [ggml_matvec_f32_ncols_7_bs_160_param_7];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_7_bs_160_param_8];
	ld.param.u32 	%r17, [ggml_matvec_f32_ncols_7_bs_160_param_9];
	ld.param.u32 	%r18, [ggml_matvec_f32_ncols_7_bs_160_param_10];
	ld.param.u32 	%r14, [ggml_matvec_f32_ncols_7_bs_160_param_11];
	cvta.to.global.u64 	%rd73, %rd22;
	add.u64 	%rd2, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r19, %r1, %r16;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r20, %r2, %r15;
	mad.lo.s32 	%r21, %r19, %r17, %r20;
	cvt.s64.s32 	%rd3, %r21;
	cvta.to.global.u64 	%rd4, %rd21;
	mul.lo.s32 	%r22, %r1, %r18;
	cvt.s64.s32 	%rd5, %r22;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r23, %r3, 2;
	mov.u32 	%r24, data_mmv;
	add.s32 	%r4, %r24, %r23;
	@%p1 bra 	$L__BB38_2;

	mov.u32 	%r25, 0;
	st.shared.u32 	[%r4], %r25;

$L__BB38_2:
	bar.sync 	0;
	mov.f32 	%f336, 0f00000000;
	mov.u32 	%r26, 0;
	st.local.u32 	[%rd2], %r26;
	st.local.u32 	[%rd2+4], %r26;
	st.local.u32 	[%rd2+8], %r26;
	st.local.u32 	[%rd2+12], %r26;
	st.local.u32 	[%rd2+16], %r26;
	st.local.u32 	[%rd2+20], %r26;
	st.local.u32 	[%rd2+24], %r26;
	setp.ge.s32 	%p2, %r3, %r11;
	mov.f32 	%f337, %f336;
	mov.f32 	%f338, %f336;
	mov.f32 	%f339, %f336;
	mov.f32 	%f340, %f336;
	mov.f32 	%f341, %f336;
	mov.f32 	%f342, %f336;
	@%p2 bra 	$L__BB38_9;

	not.b32 	%r27, %r3;
	add.s32 	%r5, %r27, %r11;
	mul.wide.u32 	%rd24, %r5, -858993459;
	shr.u64 	%rd25, %rd24, 39;
	and.b64  	%rd26, %rd25, 1;
	setp.eq.b64 	%p3, %rd26, 1;
	mov.pred 	%p4, 0;
	xor.pred  	%p5, %p3, %p4;
	mov.f32 	%f336, 0f00000000;
	mov.u32 	%r286, %r3;
	@%p5 bra 	$L__BB38_5;

	shl.b64 	%rd27, %rd5, 2;
	add.s64 	%rd28, %rd73, %rd27;
	shl.b64 	%rd29, %rd3, 2;
	add.s64 	%rd30, %rd4, %rd29;
	mul.wide.s32 	%rd31, %r3, 8;
	add.s64 	%rd32, %rd30, %rd31;
	ld.global.nc.v2.f32 	{%f50, %f51}, [%rd32];
	add.s64 	%rd33, %rd28, %rd31;
	ld.global.nc.v2.f32 	{%f54, %f55}, [%rd33];
	fma.rn.f32 	%f58, %f50, %f54, 0f00000000;
	fma.rn.f32 	%f342, %f51, %f55, %f58;
	st.local.f32 	[%rd2], %f342;
	mul.wide.s32 	%rd34, %r12, 8;
	add.s64 	%rd35, %rd33, %rd34;
	ld.global.nc.v2.f32 	{%f59, %f60}, [%rd35];
	fma.rn.f32 	%f63, %f50, %f59, 0f00000000;
	fma.rn.f32 	%f341, %f51, %f60, %f63;
	st.local.f32 	[%rd2+4], %f341;
	add.s32 	%r28, %r3, %r12;
	add.s32 	%r29, %r28, %r12;
	mul.wide.s32 	%rd36, %r29, 8;
	add.s64 	%rd37, %rd28, %rd36;
	ld.global.nc.v2.f32 	{%f64, %f65}, [%rd37];
	fma.rn.f32 	%f68, %f50, %f64, 0f00000000;
	fma.rn.f32 	%f340, %f51, %f65, %f68;
	st.local.f32 	[%rd2+8], %f340;
	add.s64 	%rd38, %rd37, %rd34;
	ld.global.nc.v2.f32 	{%f69, %f70}, [%rd38];
	fma.rn.f32 	%f73, %f50, %f69, 0f00000000;
	fma.rn.f32 	%f339, %f51, %f70, %f73;
	st.local.f32 	[%rd2+12], %f339;
	add.s64 	%rd39, %rd38, %rd34;
	ld.global.nc.v2.f32 	{%f74, %f75}, [%rd39];
	fma.rn.f32 	%f78, %f50, %f74, 0f00000000;
	fma.rn.f32 	%f338, %f51, %f75, %f78;
	st.local.f32 	[%rd2+16], %f338;
	add.s64 	%rd40, %rd39, %rd34;
	ld.global.nc.v2.f32 	{%f79, %f80}, [%rd40];
	fma.rn.f32 	%f83, %f50, %f79, 0f00000000;
	fma.rn.f32 	%f337, %f51, %f80, %f83;
	st.local.f32 	[%rd2+20], %f337;
	add.s64 	%rd41, %rd40, %rd34;
	ld.global.nc.v2.f32 	{%f84, %f85}, [%rd41];
	fma.rn.f32 	%f88, %f50, %f84, 0f00000000;
	fma.rn.f32 	%f336, %f51, %f85, %f88;
	st.local.f32 	[%rd2+24], %f336;
	add.s32 	%r286, %r3, 160;

$L__BB38_5:
	setp.lt.u32 	%p6, %r5, 160;
	@%p6 bra 	$L__BB38_9;

	add.s32 	%r30, %r286, %r12;
	add.s32 	%r31, %r30, 160;
	mul.wide.s32 	%rd42, %r31, 8;
	shl.b64 	%rd43, %rd5, 2;
	add.s64 	%rd7, %rd42, %rd43;
	shl.b32 	%r32, %r12, 1;
	add.s32 	%r33, %r286, %r32;
	mad.lo.s32 	%r34, %r12, 3, %r286;
	shl.b32 	%r35, %r12, 2;
	add.s32 	%r36, %r286, %r35;
	mad.lo.s32 	%r37, %r12, 5, %r286;
	mad.lo.s32 	%r38, %r12, 6, %r286;
	mul.wide.s32 	%rd44, %r33, 8;
	add.s64 	%rd8, %rd44, %rd43;
	mul.wide.s32 	%rd45, %r34, 8;
	add.s64 	%rd9, %rd45, %rd43;
	mul.wide.s32 	%rd46, %r36, 8;
	add.s64 	%rd10, %rd46, %rd43;
	mul.wide.s32 	%rd47, %r37, 8;
	add.s64 	%rd11, %rd47, %rd43;
	mul.wide.s32 	%rd48, %r38, 8;
	add.s64 	%rd12, %rd48, %rd43;
	mul.wide.s32 	%rd49, %r286, 2;
	add.s64 	%rd50, %rd49, %rd3;
	shl.b64 	%rd51, %rd50, 2;
	add.s64 	%rd52, %rd4, %rd51;
	add.s64 	%rd72, %rd52, 1280;
	mul.wide.s32 	%rd53, %r286, 8;
	mul.wide.s32 	%rd54, %r12, 8;
	add.s64 	%rd55, %rd53, %rd54;
	add.s64 	%rd14, %rd55, %rd43;
	add.s64 	%rd15, %rd53, %rd43;

$L__BB38_7:
	ld.global.nc.v2.f32 	{%f89, %f90}, [%rd72+-1280];
	add.s64 	%rd56, %rd73, %rd15;
	ld.global.nc.v2.f32 	{%f93, %f94}, [%rd56];
	fma.rn.f32 	%f97, %f89, %f93, %f342;
	fma.rn.f32 	%f98, %f90, %f94, %f97;
	add.s64 	%rd57, %rd73, %rd14;
	ld.global.nc.v2.f32 	{%f99, %f100}, [%rd57];
	fma.rn.f32 	%f103, %f89, %f99, %f341;
	fma.rn.f32 	%f104, %f90, %f100, %f103;
	add.s64 	%rd58, %rd73, %rd8;
	ld.global.nc.v2.f32 	{%f105, %f106}, [%rd58];
	fma.rn.f32 	%f109, %f89, %f105, %f340;
	fma.rn.f32 	%f110, %f90, %f106, %f109;
	add.s64 	%rd59, %rd73, %rd9;
	ld.global.nc.v2.f32 	{%f111, %f112}, [%rd59];
	fma.rn.f32 	%f115, %f89, %f111, %f339;
	fma.rn.f32 	%f116, %f90, %f112, %f115;
	add.s64 	%rd60, %rd73, %rd10;
	ld.global.nc.v2.f32 	{%f117, %f118}, [%rd60];
	fma.rn.f32 	%f121, %f89, %f117, %f338;
	fma.rn.f32 	%f122, %f90, %f118, %f121;
	add.s64 	%rd61, %rd73, %rd11;
	ld.global.nc.v2.f32 	{%f123, %f124}, [%rd61];
	fma.rn.f32 	%f127, %f89, %f123, %f337;
	fma.rn.f32 	%f128, %f90, %f124, %f127;
	add.s64 	%rd62, %rd73, %rd12;
	ld.global.nc.v2.f32 	{%f129, %f130}, [%rd62];
	fma.rn.f32 	%f133, %f89, %f129, %f336;
	fma.rn.f32 	%f134, %f90, %f130, %f133;
	ld.global.nc.v2.f32 	{%f135, %f136}, [%rd72];
	ld.global.nc.v2.f32 	{%f139, %f140}, [%rd56+1280];
	fma.rn.f32 	%f143, %f135, %f139, %f98;
	fma.rn.f32 	%f342, %f136, %f140, %f143;
	add.s64 	%rd63, %rd73, %rd7;
	ld.global.nc.v2.f32 	{%f144, %f145}, [%rd63];
	fma.rn.f32 	%f148, %f135, %f144, %f104;
	fma.rn.f32 	%f341, %f136, %f145, %f148;
	ld.global.nc.v2.f32 	{%f149, %f150}, [%rd58+1280];
	fma.rn.f32 	%f153, %f135, %f149, %f110;
	fma.rn.f32 	%f340, %f136, %f150, %f153;
	ld.global.nc.v2.f32 	{%f154, %f155}, [%rd59+1280];
	fma.rn.f32 	%f158, %f135, %f154, %f116;
	fma.rn.f32 	%f339, %f136, %f155, %f158;
	ld.global.nc.v2.f32 	{%f159, %f160}, [%rd60+1280];
	fma.rn.f32 	%f163, %f135, %f159, %f122;
	fma.rn.f32 	%f338, %f136, %f160, %f163;
	ld.global.nc.v2.f32 	{%f164, %f165}, [%rd61+1280];
	fma.rn.f32 	%f168, %f135, %f164, %f128;
	fma.rn.f32 	%f337, %f136, %f165, %f168;
	ld.global.nc.v2.f32 	{%f169, %f170}, [%rd62+1280];
	fma.rn.f32 	%f173, %f135, %f169, %f134;
	fma.rn.f32 	%f336, %f136, %f170, %f173;
	add.s64 	%rd73, %rd73, 2560;
	add.s64 	%rd72, %rd72, 2560;
	add.s32 	%r286, %r286, 320;
	setp.lt.s32 	%p7, %r286, %r11;
	@%p7 bra 	$L__BB38_7;

	st.local.f32 	[%rd2], %f342;
	st.local.f32 	[%rd2+4], %f341;
	st.local.f32 	[%rd2+8], %f340;
	st.local.f32 	[%rd2+12], %f339;
	st.local.f32 	[%rd2+16], %f338;
	st.local.f32 	[%rd2+20], %f337;
	st.local.f32 	[%rd2+24], %f336;

$L__BB38_9:
	shr.s32 	%r39, %r3, 31;
	shr.u32 	%r40, %r39, 27;
	add.s32 	%r41, %r3, %r40;
	shr.s32 	%r42, %r41, 5;
	shl.b32 	%r43, %r42, 2;
	add.s32 	%r10, %r24, %r43;
	mov.u32 	%r45, 2;
	mov.b32 	%r46, %f342;
	mov.u32 	%r47, 31;
	mov.u32 	%r48, 16;
	mov.u32 	%r49, -1;
	shfl.sync.bfly.b32 	%r50|%p8, %r46, %r48, %r47, %r49;
	mov.b32 	%f174, %r50;
	add.f32 	%f175, %f342, %f174;
	mov.b32 	%r51, %f175;
	mov.u32 	%r52, 8;
	shfl.sync.bfly.b32 	%r53|%p9, %r51, %r52, %r47, %r49;
	mov.b32 	%f176, %r53;
	add.f32 	%f177, %f175, %f176;
	mov.b32 	%r54, %f177;
	mov.u32 	%r55, 4;
	shfl.sync.bfly.b32 	%r56|%p10, %r54, %r55, %r47, %r49;
	mov.b32 	%f178, %r56;
	add.f32 	%f179, %f177, %f178;
	mov.b32 	%r57, %f179;
	shfl.sync.bfly.b32 	%r58|%p11, %r57, %r45, %r47, %r49;
	mov.b32 	%f180, %r58;
	add.f32 	%f181, %f179, %f180;
	mov.b32 	%r59, %f181;
	mov.u32 	%r60, 1;
	shfl.sync.bfly.b32 	%r61|%p12, %r59, %r60, %r47, %r49;
	mov.b32 	%f182, %r61;
	add.f32 	%f183, %f181, %f182;
	st.local.f32 	[%rd2], %f183;
	st.shared.f32 	[%r10], %f183;
	bar.sync 	0;
	@%p1 bra 	$L__BB38_11;

	ld.shared.f32 	%f184, [%r4];
	mov.b32 	%r62, %f184;
	shfl.sync.bfly.b32 	%r66|%p14, %r62, %r48, %r47, %r49;
	mov.b32 	%f185, %r66;
	add.f32 	%f186, %f184, %f185;
	mov.b32 	%r67, %f186;
	shfl.sync.bfly.b32 	%r69|%p15, %r67, %r52, %r47, %r49;
	mov.b32 	%f187, %r69;
	add.f32 	%f188, %f186, %f187;
	mov.b32 	%r70, %f188;
	shfl.sync.bfly.b32 	%r72|%p16, %r70, %r55, %r47, %r49;
	mov.b32 	%f189, %r72;
	add.f32 	%f190, %f188, %f189;
	mov.b32 	%r73, %f190;
	shfl.sync.bfly.b32 	%r75|%p17, %r73, %r45, %r47, %r49;
	mov.b32 	%f191, %r75;
	add.f32 	%f192, %f190, %f191;
	mov.b32 	%r76, %f192;
	shfl.sync.bfly.b32 	%r78|%p18, %r76, %r60, %r47, %r49;
	mov.b32 	%f193, %r78;
	add.f32 	%f194, %f192, %f193;
	st.local.f32 	[%rd2], %f194;

$L__BB38_11:
	bar.sync 	0;
	mov.b32 	%r79, %f341;
	shfl.sync.bfly.b32 	%r83|%p20, %r79, %r48, %r47, %r49;
	mov.b32 	%f195, %r83;
	add.f32 	%f196, %f341, %f195;
	mov.b32 	%r84, %f196;
	shfl.sync.bfly.b32 	%r86|%p21, %r84, %r52, %r47, %r49;
	mov.b32 	%f197, %r86;
	add.f32 	%f198, %f196, %f197;
	mov.b32 	%r87, %f198;
	shfl.sync.bfly.b32 	%r89|%p22, %r87, %r55, %r47, %r49;
	mov.b32 	%f199, %r89;
	add.f32 	%f200, %f198, %f199;
	mov.b32 	%r90, %f200;
	shfl.sync.bfly.b32 	%r92|%p23, %r90, %r45, %r47, %r49;
	mov.b32 	%f201, %r92;
	add.f32 	%f202, %f200, %f201;
	mov.b32 	%r93, %f202;
	shfl.sync.bfly.b32 	%r95|%p24, %r93, %r60, %r47, %r49;
	mov.b32 	%f203, %r95;
	add.f32 	%f204, %f202, %f203;
	st.local.f32 	[%rd2+4], %f204;
	st.shared.f32 	[%r10], %f204;
	bar.sync 	0;
	@%p1 bra 	$L__BB38_13;

	ld.shared.f32 	%f205, [%r4];
	mov.b32 	%r96, %f205;
	mov.u32 	%r97, 31;
	mov.u32 	%r98, 16;
	mov.u32 	%r99, -1;
	shfl.sync.bfly.b32 	%r100|%p25, %r96, %r98, %r97, %r99;
	mov.b32 	%f206, %r100;
	add.f32 	%f207, %f205, %f206;
	mov.b32 	%r101, %f207;
	mov.u32 	%r102, 8;
	shfl.sync.bfly.b32 	%r103|%p26, %r101, %r102, %r97, %r99;
	mov.b32 	%f208, %r103;
	add.f32 	%f209, %f207, %f208;
	mov.b32 	%r104, %f209;
	mov.u32 	%r105, 4;
	shfl.sync.bfly.b32 	%r106|%p27, %r104, %r105, %r97, %r99;
	mov.b32 	%f210, %r106;
	add.f32 	%f211, %f209, %f210;
	mov.b32 	%r107, %f211;
	mov.u32 	%r108, 2;
	shfl.sync.bfly.b32 	%r109|%p28, %r107, %r108, %r97, %r99;
	mov.b32 	%f212, %r109;
	add.f32 	%f213, %f211, %f212;
	mov.b32 	%r110, %f213;
	mov.u32 	%r111, 1;
	shfl.sync.bfly.b32 	%r112|%p29, %r110, %r111, %r97, %r99;
	mov.b32 	%f214, %r112;
	add.f32 	%f215, %f213, %f214;
	st.local.f32 	[%rd2+4], %f215;

$L__BB38_13:
	bar.sync 	0;
	mov.b32 	%r113, %f340;
	mov.u32 	%r114, 31;
	mov.u32 	%r115, 16;
	mov.u32 	%r116, -1;
	shfl.sync.bfly.b32 	%r117|%p31, %r113, %r115, %r114, %r116;
	mov.b32 	%f216, %r117;
	add.f32 	%f217, %f340, %f216;
	mov.b32 	%r118, %f217;
	mov.u32 	%r119, 8;
	shfl.sync.bfly.b32 	%r120|%p32, %r118, %r119, %r114, %r116;
	mov.b32 	%f218, %r120;
	add.f32 	%f219, %f217, %f218;
	mov.b32 	%r121, %f219;
	mov.u32 	%r122, 4;
	shfl.sync.bfly.b32 	%r123|%p33, %r121, %r122, %r114, %r116;
	mov.b32 	%f220, %r123;
	add.f32 	%f221, %f219, %f220;
	mov.b32 	%r124, %f221;
	mov.u32 	%r125, 2;
	shfl.sync.bfly.b32 	%r126|%p34, %r124, %r125, %r114, %r116;
	mov.b32 	%f222, %r126;
	add.f32 	%f223, %f221, %f222;
	mov.b32 	%r127, %f223;
	mov.u32 	%r128, 1;
	shfl.sync.bfly.b32 	%r129|%p35, %r127, %r128, %r114, %r116;
	mov.b32 	%f224, %r129;
	add.f32 	%f225, %f223, %f224;
	st.local.f32 	[%rd2+8], %f225;
	st.shared.f32 	[%r10], %f225;
	bar.sync 	0;
	@%p1 bra 	$L__BB38_15;

	ld.shared.f32 	%f226, [%r4];
	mov.b32 	%r130, %f226;
	shfl.sync.bfly.b32 	%r134|%p36, %r130, %r115, %r114, %r116;
	mov.b32 	%f227, %r134;
	add.f32 	%f228, %f226, %f227;
	mov.b32 	%r135, %f228;
	shfl.sync.bfly.b32 	%r137|%p37, %r135, %r119, %r114, %r116;
	mov.b32 	%f229, %r137;
	add.f32 	%f230, %f228, %f229;
	mov.b32 	%r138, %f230;
	shfl.sync.bfly.b32 	%r140|%p38, %r138, %r122, %r114, %r116;
	mov.b32 	%f231, %r140;
	add.f32 	%f232, %f230, %f231;
	mov.b32 	%r141, %f232;
	shfl.sync.bfly.b32 	%r143|%p39, %r141, %r125, %r114, %r116;
	mov.b32 	%f233, %r143;
	add.f32 	%f234, %f232, %f233;
	mov.b32 	%r144, %f234;
	shfl.sync.bfly.b32 	%r146|%p40, %r144, %r128, %r114, %r116;
	mov.b32 	%f235, %r146;
	add.f32 	%f236, %f234, %f235;
	st.local.f32 	[%rd2+8], %f236;

$L__BB38_15:
	bar.sync 	0;
	mov.b32 	%r147, %f339;
	shfl.sync.bfly.b32 	%r151|%p42, %r147, %r115, %r114, %r116;
	mov.b32 	%f237, %r151;
	add.f32 	%f238, %f339, %f237;
	mov.b32 	%r152, %f238;
	shfl.sync.bfly.b32 	%r154|%p43, %r152, %r119, %r114, %r116;
	mov.b32 	%f239, %r154;
	add.f32 	%f240, %f238, %f239;
	mov.b32 	%r155, %f240;
	shfl.sync.bfly.b32 	%r157|%p44, %r155, %r122, %r114, %r116;
	mov.b32 	%f241, %r157;
	add.f32 	%f242, %f240, %f241;
	mov.b32 	%r158, %f242;
	shfl.sync.bfly.b32 	%r160|%p45, %r158, %r125, %r114, %r116;
	mov.b32 	%f243, %r160;
	add.f32 	%f244, %f242, %f243;
	mov.b32 	%r161, %f244;
	shfl.sync.bfly.b32 	%r163|%p46, %r161, %r128, %r114, %r116;
	mov.b32 	%f245, %r163;
	add.f32 	%f246, %f244, %f245;
	st.local.f32 	[%rd2+12], %f246;
	st.shared.f32 	[%r10], %f246;
	bar.sync 	0;
	@%p1 bra 	$L__BB38_17;

	ld.shared.f32 	%f247, [%r4];
	mov.b32 	%r164, %f247;
	mov.u32 	%r165, 31;
	mov.u32 	%r166, 16;
	mov.u32 	%r167, -1;
	shfl.sync.bfly.b32 	%r168|%p47, %r164, %r166, %r165, %r167;
	mov.b32 	%f248, %r168;
	add.f32 	%f249, %f247, %f248;
	mov.b32 	%r169, %f249;
	mov.u32 	%r170, 8;
	shfl.sync.bfly.b32 	%r171|%p48, %r169, %r170, %r165, %r167;
	mov.b32 	%f250, %r171;
	add.f32 	%f251, %f249, %f250;
	mov.b32 	%r172, %f251;
	mov.u32 	%r173, 4;
	shfl.sync.bfly.b32 	%r174|%p49, %r172, %r173, %r165, %r167;
	mov.b32 	%f252, %r174;
	add.f32 	%f253, %f251, %f252;
	mov.b32 	%r175, %f253;
	mov.u32 	%r176, 2;
	shfl.sync.bfly.b32 	%r177|%p50, %r175, %r176, %r165, %r167;
	mov.b32 	%f254, %r177;
	add.f32 	%f255, %f253, %f254;
	mov.b32 	%r178, %f255;
	mov.u32 	%r179, 1;
	shfl.sync.bfly.b32 	%r180|%p51, %r178, %r179, %r165, %r167;
	mov.b32 	%f256, %r180;
	add.f32 	%f257, %f255, %f256;
	st.local.f32 	[%rd2+12], %f257;

$L__BB38_17:
	bar.sync 	0;
	mov.b32 	%r181, %f338;
	mov.u32 	%r182, 31;
	mov.u32 	%r183, 16;
	mov.u32 	%r184, -1;
	shfl.sync.bfly.b32 	%r185|%p53, %r181, %r183, %r182, %r184;
	mov.b32 	%f258, %r185;
	add.f32 	%f259, %f338, %f258;
	mov.b32 	%r186, %f259;
	mov.u32 	%r187, 8;
	shfl.sync.bfly.b32 	%r188|%p54, %r186, %r187, %r182, %r184;
	mov.b32 	%f260, %r188;
	add.f32 	%f261, %f259, %f260;
	mov.b32 	%r189, %f261;
	mov.u32 	%r190, 4;
	shfl.sync.bfly.b32 	%r191|%p55, %r189, %r190, %r182, %r184;
	mov.b32 	%f262, %r191;
	add.f32 	%f263, %f261, %f262;
	mov.b32 	%r192, %f263;
	mov.u32 	%r193, 2;
	shfl.sync.bfly.b32 	%r194|%p56, %r192, %r193, %r182, %r184;
	mov.b32 	%f264, %r194;
	add.f32 	%f265, %f263, %f264;
	mov.b32 	%r195, %f265;
	mov.u32 	%r196, 1;
	shfl.sync.bfly.b32 	%r197|%p57, %r195, %r196, %r182, %r184;
	mov.b32 	%f266, %r197;
	add.f32 	%f267, %f265, %f266;
	st.local.f32 	[%rd2+16], %f267;
	st.shared.f32 	[%r10], %f267;
	bar.sync 	0;
	@%p1 bra 	$L__BB38_19;

	ld.shared.f32 	%f268, [%r4];
	mov.b32 	%r198, %f268;
	shfl.sync.bfly.b32 	%r202|%p58, %r198, %r183, %r182, %r184;
	mov.b32 	%f269, %r202;
	add.f32 	%f270, %f268, %f269;
	mov.b32 	%r203, %f270;
	shfl.sync.bfly.b32 	%r205|%p59, %r203, %r187, %r182, %r184;
	mov.b32 	%f271, %r205;
	add.f32 	%f272, %f270, %f271;
	mov.b32 	%r206, %f272;
	shfl.sync.bfly.b32 	%r208|%p60, %r206, %r190, %r182, %r184;
	mov.b32 	%f273, %r208;
	add.f32 	%f274, %f272, %f273;
	mov.b32 	%r209, %f274;
	shfl.sync.bfly.b32 	%r211|%p61, %r209, %r193, %r182, %r184;
	mov.b32 	%f275, %r211;
	add.f32 	%f276, %f274, %f275;
	mov.b32 	%r212, %f276;
	shfl.sync.bfly.b32 	%r214|%p62, %r212, %r196, %r182, %r184;
	mov.b32 	%f277, %r214;
	add.f32 	%f278, %f276, %f277;
	st.local.f32 	[%rd2+16], %f278;

$L__BB38_19:
	bar.sync 	0;
	mov.b32 	%r215, %f337;
	shfl.sync.bfly.b32 	%r219|%p64, %r215, %r183, %r182, %r184;
	mov.b32 	%f279, %r219;
	add.f32 	%f280, %f337, %f279;
	mov.b32 	%r220, %f280;
	shfl.sync.bfly.b32 	%r222|%p65, %r220, %r187, %r182, %r184;
	mov.b32 	%f281, %r222;
	add.f32 	%f282, %f280, %f281;
	mov.b32 	%r223, %f282;
	shfl.sync.bfly.b32 	%r225|%p66, %r223, %r190, %r182, %r184;
	mov.b32 	%f283, %r225;
	add.f32 	%f284, %f282, %f283;
	mov.b32 	%r226, %f284;
	shfl.sync.bfly.b32 	%r228|%p67, %r226, %r193, %r182, %r184;
	mov.b32 	%f285, %r228;
	add.f32 	%f286, %f284, %f285;
	mov.b32 	%r229, %f286;
	shfl.sync.bfly.b32 	%r231|%p68, %r229, %r196, %r182, %r184;
	mov.b32 	%f287, %r231;
	add.f32 	%f288, %f286, %f287;
	st.local.f32 	[%rd2+20], %f288;
	st.shared.f32 	[%r10], %f288;
	bar.sync 	0;
	@%p1 bra 	$L__BB38_21;

	ld.shared.f32 	%f289, [%r4];
	mov.b32 	%r232, %f289;
	mov.u32 	%r233, 31;
	mov.u32 	%r234, 16;
	mov.u32 	%r235, -1;
	shfl.sync.bfly.b32 	%r236|%p69, %r232, %r234, %r233, %r235;
	mov.b32 	%f290, %r236;
	add.f32 	%f291, %f289, %f290;
	mov.b32 	%r237, %f291;
	mov.u32 	%r238, 8;
	shfl.sync.bfly.b32 	%r239|%p70, %r237, %r238, %r233, %r235;
	mov.b32 	%f292, %r239;
	add.f32 	%f293, %f291, %f292;
	mov.b32 	%r240, %f293;
	mov.u32 	%r241, 4;
	shfl.sync.bfly.b32 	%r242|%p71, %r240, %r241, %r233, %r235;
	mov.b32 	%f294, %r242;
	add.f32 	%f295, %f293, %f294;
	mov.b32 	%r243, %f295;
	mov.u32 	%r244, 2;
	shfl.sync.bfly.b32 	%r245|%p72, %r243, %r244, %r233, %r235;
	mov.b32 	%f296, %r245;
	add.f32 	%f297, %f295, %f296;
	mov.b32 	%r246, %f297;
	mov.u32 	%r247, 1;
	shfl.sync.bfly.b32 	%r248|%p73, %r246, %r247, %r233, %r235;
	mov.b32 	%f298, %r248;
	add.f32 	%f299, %f297, %f298;
	st.local.f32 	[%rd2+20], %f299;

$L__BB38_21:
	bar.sync 	0;
	mov.b32 	%r249, %f336;
	mov.u32 	%r250, 31;
	mov.u32 	%r251, 16;
	mov.u32 	%r252, -1;
	shfl.sync.bfly.b32 	%r253|%p75, %r249, %r251, %r250, %r252;
	mov.b32 	%f300, %r253;
	add.f32 	%f301, %f336, %f300;
	mov.b32 	%r254, %f301;
	mov.u32 	%r255, 8;
	shfl.sync.bfly.b32 	%r256|%p76, %r254, %r255, %r250, %r252;
	mov.b32 	%f302, %r256;
	add.f32 	%f303, %f301, %f302;
	mov.b32 	%r257, %f303;
	mov.u32 	%r258, 4;
	shfl.sync.bfly.b32 	%r259|%p77, %r257, %r258, %r250, %r252;
	mov.b32 	%f304, %r259;
	add.f32 	%f305, %f303, %f304;
	mov.b32 	%r260, %f305;
	mov.u32 	%r261, 2;
	shfl.sync.bfly.b32 	%r262|%p78, %r260, %r261, %r250, %r252;
	mov.b32 	%f306, %r262;
	add.f32 	%f307, %f305, %f306;
	mov.b32 	%r263, %f307;
	mov.u32 	%r264, 1;
	shfl.sync.bfly.b32 	%r265|%p79, %r263, %r264, %r250, %r252;
	mov.b32 	%f308, %r265;
	add.f32 	%f309, %f307, %f308;
	st.local.f32 	[%rd2+24], %f309;
	st.shared.f32 	[%r10], %f309;
	bar.sync 	0;
	@%p1 bra 	$L__BB38_23;

	ld.shared.f32 	%f310, [%r4];
	mov.b32 	%r266, %f310;
	shfl.sync.bfly.b32 	%r270|%p80, %r266, %r251, %r250, %r252;
	mov.b32 	%f311, %r270;
	add.f32 	%f312, %f310, %f311;
	mov.b32 	%r271, %f312;
	shfl.sync.bfly.b32 	%r273|%p81, %r271, %r255, %r250, %r252;
	mov.b32 	%f313, %r273;
	add.f32 	%f314, %f312, %f313;
	mov.b32 	%r274, %f314;
	shfl.sync.bfly.b32 	%r276|%p82, %r274, %r258, %r250, %r252;
	mov.b32 	%f315, %r276;
	add.f32 	%f316, %f314, %f315;
	mov.b32 	%r277, %f316;
	shfl.sync.bfly.b32 	%r279|%p83, %r277, %r261, %r250, %r252;
	mov.b32 	%f317, %r279;
	add.f32 	%f318, %f316, %f317;
	mov.b32 	%r280, %f318;
	shfl.sync.bfly.b32 	%r282|%p84, %r280, %r264, %r250, %r252;
	mov.b32 	%f319, %r282;
	add.f32 	%f320, %f318, %f319;
	st.local.f32 	[%rd2+24], %f320;

$L__BB38_23:
	bar.sync 	0;
	setp.gt.s32 	%p85, %r3, 6;
	@%p85 bra 	$L__BB38_25;

	mul.wide.s32 	%rd64, %r3, 4;
	add.s64 	%rd65, %rd2, %rd64;
	ld.local.f32 	%f321, [%rd65];
	mad.lo.s32 	%r283, %r3, %r13, %r2;
	cvt.s64.s32 	%rd66, %r283;
	mul.lo.s32 	%r284, %r1, %r14;
	cvt.s64.s32 	%rd67, %r284;
	add.s64 	%rd68, %rd67, %rd66;
	cvta.to.global.u64 	%rd69, %rd20;
	shl.b64 	%rd70, %rd68, 2;
	add.s64 	%rd71, %rd69, %rd70;
	st.global.f32 	[%rd71], %f321;

$L__BB38_25:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_8_bs_160
.visible .entry ggml_matvec_f32_ncols_8_bs_160(
	.param .u64 ggml_matvec_f32_ncols_8_bs_160_param_0,
	.param .u64 ggml_matvec_f32_ncols_8_bs_160_param_1,
	.param .u64 ggml_matvec_f32_ncols_8_bs_160_param_2,
	.param .u32 ggml_matvec_f32_ncols_8_bs_160_param_3,
	.param .u32 ggml_matvec_f32_ncols_8_bs_160_param_4,
	.param .u32 ggml_matvec_f32_ncols_8_bs_160_param_5,
	.param .u32 ggml_matvec_f32_ncols_8_bs_160_param_6,
	.param .u32 ggml_matvec_f32_ncols_8_bs_160_param_7,
	.param .u32 ggml_matvec_f32_ncols_8_bs_160_param_8,
	.param .u32 ggml_matvec_f32_ncols_8_bs_160_param_9,
	.param .u32 ggml_matvec_f32_ncols_8_bs_160_param_10,
	.param .u32 ggml_matvec_f32_ncols_8_bs_160_param_11
)
{
	.local .align 16 .b8 	__local_depot39[32];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<97>;
	.reg .f32 	%f<390>;
	.reg .b32 	%r<321>;
	.reg .b64 	%rd<78>;


	mov.u64 	%SPL, __local_depot39;
	ld.param.u64 	%rd22, [ggml_matvec_f32_ncols_8_bs_160_param_0];
	ld.param.u64 	%rd23, [ggml_matvec_f32_ncols_8_bs_160_param_1];
	ld.param.u64 	%rd21, [ggml_matvec_f32_ncols_8_bs_160_param_2];
	ld.param.u32 	%r11, [ggml_matvec_f32_ncols_8_bs_160_param_3];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_8_bs_160_param_5];
	ld.param.u32 	%r12, [ggml_matvec_f32_ncols_8_bs_160_param_6];
	ld.param.u32 	%r13, [ggml_matvec_f32_ncols_8_bs_160_param_7];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_8_bs_160_param_8];
	ld.param.u32 	%r17, [ggml_matvec_f32_ncols_8_bs_160_param_9];
	ld.param.u32 	%r18, [ggml_matvec_f32_ncols_8_bs_160_param_10];
	ld.param.u32 	%r14, [ggml_matvec_f32_ncols_8_bs_160_param_11];
	cvta.to.global.u64 	%rd77, %rd23;
	add.u64 	%rd2, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r19, %r1, %r16;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r20, %r2, %r15;
	mad.lo.s32 	%r21, %r19, %r17, %r20;
	cvt.s64.s32 	%rd3, %r21;
	cvta.to.global.u64 	%rd4, %rd22;
	mul.lo.s32 	%r22, %r1, %r18;
	cvt.s64.s32 	%rd5, %r22;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r23, %r3, 2;
	mov.u32 	%r24, data_mmv;
	add.s32 	%r4, %r24, %r23;
	@%p1 bra 	$L__BB39_2;

	mov.u32 	%r25, 0;
	st.shared.u32 	[%r4], %r25;

$L__BB39_2:
	bar.sync 	0;
	mov.f32 	%f382, 0f00000000;
	st.local.v4.f32 	[%rd2], {%f382, %f382, %f382, %f382};
	st.local.v4.f32 	[%rd2+16], {%f382, %f382, %f382, %f382};
	setp.ge.s32 	%p2, %r3, %r11;
	mov.f32 	%f383, %f382;
	mov.f32 	%f384, %f382;
	mov.f32 	%f385, %f382;
	mov.f32 	%f386, %f382;
	mov.f32 	%f387, %f382;
	mov.f32 	%f388, %f382;
	mov.f32 	%f389, %f382;
	@%p2 bra 	$L__BB39_9;

	not.b32 	%r26, %r3;
	add.s32 	%r5, %r26, %r11;
	mul.wide.u32 	%rd25, %r5, -858993459;
	shr.u64 	%rd26, %rd25, 39;
	and.b64  	%rd27, %rd26, 1;
	setp.eq.b64 	%p3, %rd27, 1;
	mov.pred 	%p4, 0;
	xor.pred  	%p5, %p3, %p4;
	mov.f32 	%f382, 0f00000000;
	mov.u32 	%r320, %r3;
	@%p5 bra 	$L__BB39_5;

	shl.b64 	%rd28, %rd5, 2;
	add.s64 	%rd29, %rd77, %rd28;
	shl.b64 	%rd30, %rd3, 2;
	add.s64 	%rd31, %rd4, %rd30;
	mul.wide.s32 	%rd32, %r3, 8;
	add.s64 	%rd33, %rd31, %rd32;
	ld.global.nc.v2.f32 	{%f57, %f58}, [%rd33];
	add.s64 	%rd34, %rd29, %rd32;
	ld.global.nc.v2.f32 	{%f61, %f62}, [%rd34];
	fma.rn.f32 	%f65, %f57, %f61, 0f00000000;
	fma.rn.f32 	%f389, %f58, %f62, %f65;
	mul.wide.s32 	%rd35, %r12, 8;
	add.s64 	%rd36, %rd34, %rd35;
	ld.global.nc.v2.f32 	{%f66, %f67}, [%rd36];
	fma.rn.f32 	%f70, %f57, %f66, 0f00000000;
	fma.rn.f32 	%f388, %f58, %f67, %f70;
	add.s32 	%r27, %r3, %r12;
	add.s32 	%r28, %r27, %r12;
	mul.wide.s32 	%rd37, %r28, 8;
	add.s64 	%rd38, %rd29, %rd37;
	ld.global.nc.v2.f32 	{%f71, %f72}, [%rd38];
	fma.rn.f32 	%f75, %f57, %f71, 0f00000000;
	fma.rn.f32 	%f387, %f58, %f72, %f75;
	add.s64 	%rd39, %rd38, %rd35;
	ld.global.nc.v2.f32 	{%f76, %f77}, [%rd39];
	fma.rn.f32 	%f80, %f57, %f76, 0f00000000;
	fma.rn.f32 	%f386, %f58, %f77, %f80;
	st.local.v4.f32 	[%rd2], {%f389, %f388, %f387, %f386};
	add.s64 	%rd40, %rd39, %rd35;
	ld.global.nc.v2.f32 	{%f81, %f82}, [%rd40];
	fma.rn.f32 	%f85, %f57, %f81, 0f00000000;
	fma.rn.f32 	%f385, %f58, %f82, %f85;
	add.s64 	%rd41, %rd40, %rd35;
	ld.global.nc.v2.f32 	{%f86, %f87}, [%rd41];
	fma.rn.f32 	%f90, %f57, %f86, 0f00000000;
	fma.rn.f32 	%f384, %f58, %f87, %f90;
	add.s64 	%rd42, %rd41, %rd35;
	ld.global.nc.v2.f32 	{%f91, %f92}, [%rd42];
	fma.rn.f32 	%f95, %f57, %f91, 0f00000000;
	fma.rn.f32 	%f383, %f58, %f92, %f95;
	add.s64 	%rd43, %rd42, %rd35;
	ld.global.nc.v2.f32 	{%f96, %f97}, [%rd43];
	fma.rn.f32 	%f100, %f57, %f96, 0f00000000;
	fma.rn.f32 	%f382, %f58, %f97, %f100;
	st.local.v4.f32 	[%rd2+16], {%f385, %f384, %f383, %f382};
	add.s32 	%r320, %r3, 160;

$L__BB39_5:
	setp.lt.u32 	%p6, %r5, 160;
	@%p6 bra 	$L__BB39_9;

	add.s32 	%r29, %r320, %r12;
	add.s32 	%r30, %r29, 160;
	mul.wide.s32 	%rd44, %r30, 8;
	shl.b64 	%rd45, %rd5, 2;
	add.s64 	%rd7, %rd44, %rd45;
	shl.b32 	%r31, %r12, 1;
	add.s32 	%r32, %r320, %r31;
	mad.lo.s32 	%r33, %r12, 3, %r320;
	shl.b32 	%r34, %r12, 2;
	add.s32 	%r35, %r320, %r34;
	mad.lo.s32 	%r36, %r12, 5, %r320;
	mad.lo.s32 	%r37, %r12, 6, %r320;
	mad.lo.s32 	%r38, %r12, 7, %r320;
	mul.wide.s32 	%rd46, %r32, 8;
	add.s64 	%rd8, %rd46, %rd45;
	mul.wide.s32 	%rd47, %r33, 8;
	add.s64 	%rd9, %rd47, %rd45;
	mul.wide.s32 	%rd48, %r35, 8;
	add.s64 	%rd10, %rd48, %rd45;
	mul.wide.s32 	%rd49, %r36, 8;
	add.s64 	%rd11, %rd49, %rd45;
	mul.wide.s32 	%rd50, %r37, 8;
	add.s64 	%rd12, %rd50, %rd45;
	mul.wide.s32 	%rd51, %r38, 8;
	add.s64 	%rd13, %rd51, %rd45;
	mul.wide.s32 	%rd52, %r320, 2;
	add.s64 	%rd53, %rd52, %rd3;
	shl.b64 	%rd54, %rd53, 2;
	add.s64 	%rd55, %rd4, %rd54;
	add.s64 	%rd76, %rd55, 1280;
	mul.wide.s32 	%rd56, %r320, 8;
	mul.wide.s32 	%rd57, %r12, 8;
	add.s64 	%rd58, %rd56, %rd57;
	add.s64 	%rd15, %rd58, %rd45;
	add.s64 	%rd16, %rd56, %rd45;

$L__BB39_7:
	ld.global.nc.v2.f32 	{%f101, %f102}, [%rd76+-1280];
	add.s64 	%rd59, %rd77, %rd16;
	ld.global.nc.v2.f32 	{%f105, %f106}, [%rd59];
	fma.rn.f32 	%f109, %f101, %f105, %f389;
	fma.rn.f32 	%f110, %f102, %f106, %f109;
	add.s64 	%rd60, %rd77, %rd15;
	ld.global.nc.v2.f32 	{%f111, %f112}, [%rd60];
	fma.rn.f32 	%f115, %f101, %f111, %f388;
	fma.rn.f32 	%f116, %f102, %f112, %f115;
	add.s64 	%rd61, %rd77, %rd8;
	ld.global.nc.v2.f32 	{%f117, %f118}, [%rd61];
	fma.rn.f32 	%f121, %f101, %f117, %f387;
	fma.rn.f32 	%f122, %f102, %f118, %f121;
	add.s64 	%rd62, %rd77, %rd9;
	ld.global.nc.v2.f32 	{%f123, %f124}, [%rd62];
	fma.rn.f32 	%f127, %f101, %f123, %f386;
	fma.rn.f32 	%f128, %f102, %f124, %f127;
	add.s64 	%rd63, %rd77, %rd10;
	ld.global.nc.v2.f32 	{%f129, %f130}, [%rd63];
	fma.rn.f32 	%f133, %f101, %f129, %f385;
	fma.rn.f32 	%f134, %f102, %f130, %f133;
	add.s64 	%rd64, %rd77, %rd11;
	ld.global.nc.v2.f32 	{%f135, %f136}, [%rd64];
	fma.rn.f32 	%f139, %f101, %f135, %f384;
	fma.rn.f32 	%f140, %f102, %f136, %f139;
	add.s64 	%rd65, %rd77, %rd12;
	ld.global.nc.v2.f32 	{%f141, %f142}, [%rd65];
	fma.rn.f32 	%f145, %f101, %f141, %f383;
	fma.rn.f32 	%f146, %f102, %f142, %f145;
	add.s64 	%rd66, %rd77, %rd13;
	ld.global.nc.v2.f32 	{%f147, %f148}, [%rd66];
	fma.rn.f32 	%f151, %f101, %f147, %f382;
	fma.rn.f32 	%f152, %f102, %f148, %f151;
	ld.global.nc.v2.f32 	{%f153, %f154}, [%rd76];
	ld.global.nc.v2.f32 	{%f157, %f158}, [%rd59+1280];
	fma.rn.f32 	%f161, %f153, %f157, %f110;
	fma.rn.f32 	%f389, %f154, %f158, %f161;
	add.s64 	%rd67, %rd77, %rd7;
	ld.global.nc.v2.f32 	{%f162, %f163}, [%rd67];
	fma.rn.f32 	%f166, %f153, %f162, %f116;
	fma.rn.f32 	%f388, %f154, %f163, %f166;
	ld.global.nc.v2.f32 	{%f167, %f168}, [%rd61+1280];
	fma.rn.f32 	%f171, %f153, %f167, %f122;
	fma.rn.f32 	%f387, %f154, %f168, %f171;
	ld.global.nc.v2.f32 	{%f172, %f173}, [%rd62+1280];
	fma.rn.f32 	%f176, %f153, %f172, %f128;
	fma.rn.f32 	%f386, %f154, %f173, %f176;
	ld.global.nc.v2.f32 	{%f177, %f178}, [%rd63+1280];
	fma.rn.f32 	%f181, %f153, %f177, %f134;
	fma.rn.f32 	%f385, %f154, %f178, %f181;
	ld.global.nc.v2.f32 	{%f182, %f183}, [%rd64+1280];
	fma.rn.f32 	%f186, %f153, %f182, %f140;
	fma.rn.f32 	%f384, %f154, %f183, %f186;
	ld.global.nc.v2.f32 	{%f187, %f188}, [%rd65+1280];
	fma.rn.f32 	%f191, %f153, %f187, %f146;
	fma.rn.f32 	%f383, %f154, %f188, %f191;
	ld.global.nc.v2.f32 	{%f192, %f193}, [%rd66+1280];
	fma.rn.f32 	%f196, %f153, %f192, %f152;
	fma.rn.f32 	%f382, %f154, %f193, %f196;
	add.s64 	%rd77, %rd77, 2560;
	add.s64 	%rd76, %rd76, 2560;
	add.s32 	%r320, %r320, 320;
	setp.lt.s32 	%p7, %r320, %r11;
	@%p7 bra 	$L__BB39_7;

	st.local.v4.f32 	[%rd2], {%f389, %f388, %f387, %f386};
	st.local.v4.f32 	[%rd2+16], {%f385, %f384, %f383, %f382};

$L__BB39_9:
	shr.s32 	%r39, %r3, 31;
	shr.u32 	%r40, %r39, 27;
	add.s32 	%r41, %r3, %r40;
	shr.s32 	%r42, %r41, 5;
	shl.b32 	%r43, %r42, 2;
	add.s32 	%r10, %r24, %r43;
	mov.u32 	%r45, 2;
	mov.b32 	%r46, %f389;
	mov.u32 	%r47, 31;
	mov.u32 	%r48, 16;
	mov.u32 	%r49, -1;
	shfl.sync.bfly.b32 	%r50|%p8, %r46, %r48, %r47, %r49;
	mov.b32 	%f197, %r50;
	add.f32 	%f198, %f389, %f197;
	mov.b32 	%r51, %f198;
	mov.u32 	%r52, 8;
	shfl.sync.bfly.b32 	%r53|%p9, %r51, %r52, %r47, %r49;
	mov.b32 	%f199, %r53;
	add.f32 	%f200, %f198, %f199;
	mov.b32 	%r54, %f200;
	mov.u32 	%r55, 4;
	shfl.sync.bfly.b32 	%r56|%p10, %r54, %r55, %r47, %r49;
	mov.b32 	%f201, %r56;
	add.f32 	%f202, %f200, %f201;
	mov.b32 	%r57, %f202;
	shfl.sync.bfly.b32 	%r58|%p11, %r57, %r45, %r47, %r49;
	mov.b32 	%f203, %r58;
	add.f32 	%f204, %f202, %f203;
	mov.b32 	%r59, %f204;
	mov.u32 	%r60, 1;
	shfl.sync.bfly.b32 	%r61|%p12, %r59, %r60, %r47, %r49;
	mov.b32 	%f205, %r61;
	add.f32 	%f206, %f204, %f205;
	st.local.f32 	[%rd2], %f206;
	st.shared.f32 	[%r10], %f206;
	bar.sync 	0;
	@%p1 bra 	$L__BB39_11;

	ld.shared.f32 	%f207, [%r4];
	mov.b32 	%r62, %f207;
	shfl.sync.bfly.b32 	%r66|%p14, %r62, %r48, %r47, %r49;
	mov.b32 	%f208, %r66;
	add.f32 	%f209, %f207, %f208;
	mov.b32 	%r67, %f209;
	shfl.sync.bfly.b32 	%r69|%p15, %r67, %r52, %r47, %r49;
	mov.b32 	%f210, %r69;
	add.f32 	%f211, %f209, %f210;
	mov.b32 	%r70, %f211;
	shfl.sync.bfly.b32 	%r72|%p16, %r70, %r55, %r47, %r49;
	mov.b32 	%f212, %r72;
	add.f32 	%f213, %f211, %f212;
	mov.b32 	%r73, %f213;
	shfl.sync.bfly.b32 	%r75|%p17, %r73, %r45, %r47, %r49;
	mov.b32 	%f214, %r75;
	add.f32 	%f215, %f213, %f214;
	mov.b32 	%r76, %f215;
	shfl.sync.bfly.b32 	%r78|%p18, %r76, %r60, %r47, %r49;
	mov.b32 	%f216, %r78;
	add.f32 	%f217, %f215, %f216;
	st.local.f32 	[%rd2], %f217;

$L__BB39_11:
	bar.sync 	0;
	mov.b32 	%r79, %f388;
	shfl.sync.bfly.b32 	%r83|%p20, %r79, %r48, %r47, %r49;
	mov.b32 	%f218, %r83;
	add.f32 	%f219, %f388, %f218;
	mov.b32 	%r84, %f219;
	shfl.sync.bfly.b32 	%r86|%p21, %r84, %r52, %r47, %r49;
	mov.b32 	%f220, %r86;
	add.f32 	%f221, %f219, %f220;
	mov.b32 	%r87, %f221;
	shfl.sync.bfly.b32 	%r89|%p22, %r87, %r55, %r47, %r49;
	mov.b32 	%f222, %r89;
	add.f32 	%f223, %f221, %f222;
	mov.b32 	%r90, %f223;
	shfl.sync.bfly.b32 	%r92|%p23, %r90, %r45, %r47, %r49;
	mov.b32 	%f224, %r92;
	add.f32 	%f225, %f223, %f224;
	mov.b32 	%r93, %f225;
	shfl.sync.bfly.b32 	%r95|%p24, %r93, %r60, %r47, %r49;
	mov.b32 	%f226, %r95;
	add.f32 	%f227, %f225, %f226;
	st.local.f32 	[%rd2+4], %f227;
	st.shared.f32 	[%r10], %f227;
	bar.sync 	0;
	@%p1 bra 	$L__BB39_13;

	ld.shared.f32 	%f228, [%r4];
	mov.b32 	%r96, %f228;
	mov.u32 	%r97, 31;
	mov.u32 	%r98, 16;
	mov.u32 	%r99, -1;
	shfl.sync.bfly.b32 	%r100|%p25, %r96, %r98, %r97, %r99;
	mov.b32 	%f229, %r100;
	add.f32 	%f230, %f228, %f229;
	mov.b32 	%r101, %f230;
	mov.u32 	%r102, 8;
	shfl.sync.bfly.b32 	%r103|%p26, %r101, %r102, %r97, %r99;
	mov.b32 	%f231, %r103;
	add.f32 	%f232, %f230, %f231;
	mov.b32 	%r104, %f232;
	mov.u32 	%r105, 4;
	shfl.sync.bfly.b32 	%r106|%p27, %r104, %r105, %r97, %r99;
	mov.b32 	%f233, %r106;
	add.f32 	%f234, %f232, %f233;
	mov.b32 	%r107, %f234;
	mov.u32 	%r108, 2;
	shfl.sync.bfly.b32 	%r109|%p28, %r107, %r108, %r97, %r99;
	mov.b32 	%f235, %r109;
	add.f32 	%f236, %f234, %f235;
	mov.b32 	%r110, %f236;
	mov.u32 	%r111, 1;
	shfl.sync.bfly.b32 	%r112|%p29, %r110, %r111, %r97, %r99;
	mov.b32 	%f237, %r112;
	add.f32 	%f238, %f236, %f237;
	st.local.f32 	[%rd2+4], %f238;

$L__BB39_13:
	bar.sync 	0;
	mov.b32 	%r113, %f387;
	mov.u32 	%r114, 31;
	mov.u32 	%r115, 16;
	mov.u32 	%r116, -1;
	shfl.sync.bfly.b32 	%r117|%p31, %r113, %r115, %r114, %r116;
	mov.b32 	%f239, %r117;
	add.f32 	%f240, %f387, %f239;
	mov.b32 	%r118, %f240;
	mov.u32 	%r119, 8;
	shfl.sync.bfly.b32 	%r120|%p32, %r118, %r119, %r114, %r116;
	mov.b32 	%f241, %r120;
	add.f32 	%f242, %f240, %f241;
	mov.b32 	%r121, %f242;
	mov.u32 	%r122, 4;
	shfl.sync.bfly.b32 	%r123|%p33, %r121, %r122, %r114, %r116;
	mov.b32 	%f243, %r123;
	add.f32 	%f244, %f242, %f243;
	mov.b32 	%r124, %f244;
	mov.u32 	%r125, 2;
	shfl.sync.bfly.b32 	%r126|%p34, %r124, %r125, %r114, %r116;
	mov.b32 	%f245, %r126;
	add.f32 	%f246, %f244, %f245;
	mov.b32 	%r127, %f246;
	mov.u32 	%r128, 1;
	shfl.sync.bfly.b32 	%r129|%p35, %r127, %r128, %r114, %r116;
	mov.b32 	%f247, %r129;
	add.f32 	%f248, %f246, %f247;
	st.local.f32 	[%rd2+8], %f248;
	st.shared.f32 	[%r10], %f248;
	bar.sync 	0;
	@%p1 bra 	$L__BB39_15;

	ld.shared.f32 	%f249, [%r4];
	mov.b32 	%r130, %f249;
	shfl.sync.bfly.b32 	%r134|%p36, %r130, %r115, %r114, %r116;
	mov.b32 	%f250, %r134;
	add.f32 	%f251, %f249, %f250;
	mov.b32 	%r135, %f251;
	shfl.sync.bfly.b32 	%r137|%p37, %r135, %r119, %r114, %r116;
	mov.b32 	%f252, %r137;
	add.f32 	%f253, %f251, %f252;
	mov.b32 	%r138, %f253;
	shfl.sync.bfly.b32 	%r140|%p38, %r138, %r122, %r114, %r116;
	mov.b32 	%f254, %r140;
	add.f32 	%f255, %f253, %f254;
	mov.b32 	%r141, %f255;
	shfl.sync.bfly.b32 	%r143|%p39, %r141, %r125, %r114, %r116;
	mov.b32 	%f256, %r143;
	add.f32 	%f257, %f255, %f256;
	mov.b32 	%r144, %f257;
	shfl.sync.bfly.b32 	%r146|%p40, %r144, %r128, %r114, %r116;
	mov.b32 	%f258, %r146;
	add.f32 	%f259, %f257, %f258;
	st.local.f32 	[%rd2+8], %f259;

$L__BB39_15:
	bar.sync 	0;
	mov.b32 	%r147, %f386;
	shfl.sync.bfly.b32 	%r151|%p42, %r147, %r115, %r114, %r116;
	mov.b32 	%f260, %r151;
	add.f32 	%f261, %f386, %f260;
	mov.b32 	%r152, %f261;
	shfl.sync.bfly.b32 	%r154|%p43, %r152, %r119, %r114, %r116;
	mov.b32 	%f262, %r154;
	add.f32 	%f263, %f261, %f262;
	mov.b32 	%r155, %f263;
	shfl.sync.bfly.b32 	%r157|%p44, %r155, %r122, %r114, %r116;
	mov.b32 	%f264, %r157;
	add.f32 	%f265, %f263, %f264;
	mov.b32 	%r158, %f265;
	shfl.sync.bfly.b32 	%r160|%p45, %r158, %r125, %r114, %r116;
	mov.b32 	%f266, %r160;
	add.f32 	%f267, %f265, %f266;
	mov.b32 	%r161, %f267;
	shfl.sync.bfly.b32 	%r163|%p46, %r161, %r128, %r114, %r116;
	mov.b32 	%f268, %r163;
	add.f32 	%f269, %f267, %f268;
	st.local.f32 	[%rd2+12], %f269;
	st.shared.f32 	[%r10], %f269;
	bar.sync 	0;
	@%p1 bra 	$L__BB39_17;

	ld.shared.f32 	%f270, [%r4];
	mov.b32 	%r164, %f270;
	mov.u32 	%r165, 31;
	mov.u32 	%r166, 16;
	mov.u32 	%r167, -1;
	shfl.sync.bfly.b32 	%r168|%p47, %r164, %r166, %r165, %r167;
	mov.b32 	%f271, %r168;
	add.f32 	%f272, %f270, %f271;
	mov.b32 	%r169, %f272;
	mov.u32 	%r170, 8;
	shfl.sync.bfly.b32 	%r171|%p48, %r169, %r170, %r165, %r167;
	mov.b32 	%f273, %r171;
	add.f32 	%f274, %f272, %f273;
	mov.b32 	%r172, %f274;
	mov.u32 	%r173, 4;
	shfl.sync.bfly.b32 	%r174|%p49, %r172, %r173, %r165, %r167;
	mov.b32 	%f275, %r174;
	add.f32 	%f276, %f274, %f275;
	mov.b32 	%r175, %f276;
	mov.u32 	%r176, 2;
	shfl.sync.bfly.b32 	%r177|%p50, %r175, %r176, %r165, %r167;
	mov.b32 	%f277, %r177;
	add.f32 	%f278, %f276, %f277;
	mov.b32 	%r178, %f278;
	mov.u32 	%r179, 1;
	shfl.sync.bfly.b32 	%r180|%p51, %r178, %r179, %r165, %r167;
	mov.b32 	%f279, %r180;
	add.f32 	%f280, %f278, %f279;
	st.local.f32 	[%rd2+12], %f280;

$L__BB39_17:
	bar.sync 	0;
	mov.b32 	%r181, %f385;
	mov.u32 	%r182, 31;
	mov.u32 	%r183, 16;
	mov.u32 	%r184, -1;
	shfl.sync.bfly.b32 	%r185|%p53, %r181, %r183, %r182, %r184;
	mov.b32 	%f281, %r185;
	add.f32 	%f282, %f385, %f281;
	mov.b32 	%r186, %f282;
	mov.u32 	%r187, 8;
	shfl.sync.bfly.b32 	%r188|%p54, %r186, %r187, %r182, %r184;
	mov.b32 	%f283, %r188;
	add.f32 	%f284, %f282, %f283;
	mov.b32 	%r189, %f284;
	mov.u32 	%r190, 4;
	shfl.sync.bfly.b32 	%r191|%p55, %r189, %r190, %r182, %r184;
	mov.b32 	%f285, %r191;
	add.f32 	%f286, %f284, %f285;
	mov.b32 	%r192, %f286;
	mov.u32 	%r193, 2;
	shfl.sync.bfly.b32 	%r194|%p56, %r192, %r193, %r182, %r184;
	mov.b32 	%f287, %r194;
	add.f32 	%f288, %f286, %f287;
	mov.b32 	%r195, %f288;
	mov.u32 	%r196, 1;
	shfl.sync.bfly.b32 	%r197|%p57, %r195, %r196, %r182, %r184;
	mov.b32 	%f289, %r197;
	add.f32 	%f290, %f288, %f289;
	st.local.f32 	[%rd2+16], %f290;
	st.shared.f32 	[%r10], %f290;
	bar.sync 	0;
	@%p1 bra 	$L__BB39_19;

	ld.shared.f32 	%f291, [%r4];
	mov.b32 	%r198, %f291;
	shfl.sync.bfly.b32 	%r202|%p58, %r198, %r183, %r182, %r184;
	mov.b32 	%f292, %r202;
	add.f32 	%f293, %f291, %f292;
	mov.b32 	%r203, %f293;
	shfl.sync.bfly.b32 	%r205|%p59, %r203, %r187, %r182, %r184;
	mov.b32 	%f294, %r205;
	add.f32 	%f295, %f293, %f294;
	mov.b32 	%r206, %f295;
	shfl.sync.bfly.b32 	%r208|%p60, %r206, %r190, %r182, %r184;
	mov.b32 	%f296, %r208;
	add.f32 	%f297, %f295, %f296;
	mov.b32 	%r209, %f297;
	shfl.sync.bfly.b32 	%r211|%p61, %r209, %r193, %r182, %r184;
	mov.b32 	%f298, %r211;
	add.f32 	%f299, %f297, %f298;
	mov.b32 	%r212, %f299;
	shfl.sync.bfly.b32 	%r214|%p62, %r212, %r196, %r182, %r184;
	mov.b32 	%f300, %r214;
	add.f32 	%f301, %f299, %f300;
	st.local.f32 	[%rd2+16], %f301;

$L__BB39_19:
	bar.sync 	0;
	mov.b32 	%r215, %f384;
	shfl.sync.bfly.b32 	%r219|%p64, %r215, %r183, %r182, %r184;
	mov.b32 	%f302, %r219;
	add.f32 	%f303, %f384, %f302;
	mov.b32 	%r220, %f303;
	shfl.sync.bfly.b32 	%r222|%p65, %r220, %r187, %r182, %r184;
	mov.b32 	%f304, %r222;
	add.f32 	%f305, %f303, %f304;
	mov.b32 	%r223, %f305;
	shfl.sync.bfly.b32 	%r225|%p66, %r223, %r190, %r182, %r184;
	mov.b32 	%f306, %r225;
	add.f32 	%f307, %f305, %f306;
	mov.b32 	%r226, %f307;
	shfl.sync.bfly.b32 	%r228|%p67, %r226, %r193, %r182, %r184;
	mov.b32 	%f308, %r228;
	add.f32 	%f309, %f307, %f308;
	mov.b32 	%r229, %f309;
	shfl.sync.bfly.b32 	%r231|%p68, %r229, %r196, %r182, %r184;
	mov.b32 	%f310, %r231;
	add.f32 	%f311, %f309, %f310;
	st.local.f32 	[%rd2+20], %f311;
	st.shared.f32 	[%r10], %f311;
	bar.sync 	0;
	@%p1 bra 	$L__BB39_21;

	ld.shared.f32 	%f312, [%r4];
	mov.b32 	%r232, %f312;
	mov.u32 	%r233, 31;
	mov.u32 	%r234, 16;
	mov.u32 	%r235, -1;
	shfl.sync.bfly.b32 	%r236|%p69, %r232, %r234, %r233, %r235;
	mov.b32 	%f313, %r236;
	add.f32 	%f314, %f312, %f313;
	mov.b32 	%r237, %f314;
	mov.u32 	%r238, 8;
	shfl.sync.bfly.b32 	%r239|%p70, %r237, %r238, %r233, %r235;
	mov.b32 	%f315, %r239;
	add.f32 	%f316, %f314, %f315;
	mov.b32 	%r240, %f316;
	mov.u32 	%r241, 4;
	shfl.sync.bfly.b32 	%r242|%p71, %r240, %r241, %r233, %r235;
	mov.b32 	%f317, %r242;
	add.f32 	%f318, %f316, %f317;
	mov.b32 	%r243, %f318;
	mov.u32 	%r244, 2;
	shfl.sync.bfly.b32 	%r245|%p72, %r243, %r244, %r233, %r235;
	mov.b32 	%f319, %r245;
	add.f32 	%f320, %f318, %f319;
	mov.b32 	%r246, %f320;
	mov.u32 	%r247, 1;
	shfl.sync.bfly.b32 	%r248|%p73, %r246, %r247, %r233, %r235;
	mov.b32 	%f321, %r248;
	add.f32 	%f322, %f320, %f321;
	st.local.f32 	[%rd2+20], %f322;

$L__BB39_21:
	bar.sync 	0;
	mov.b32 	%r249, %f383;
	mov.u32 	%r250, 31;
	mov.u32 	%r251, 16;
	mov.u32 	%r252, -1;
	shfl.sync.bfly.b32 	%r253|%p75, %r249, %r251, %r250, %r252;
	mov.b32 	%f323, %r253;
	add.f32 	%f324, %f383, %f323;
	mov.b32 	%r254, %f324;
	mov.u32 	%r255, 8;
	shfl.sync.bfly.b32 	%r256|%p76, %r254, %r255, %r250, %r252;
	mov.b32 	%f325, %r256;
	add.f32 	%f326, %f324, %f325;
	mov.b32 	%r257, %f326;
	mov.u32 	%r258, 4;
	shfl.sync.bfly.b32 	%r259|%p77, %r257, %r258, %r250, %r252;
	mov.b32 	%f327, %r259;
	add.f32 	%f328, %f326, %f327;
	mov.b32 	%r260, %f328;
	mov.u32 	%r261, 2;
	shfl.sync.bfly.b32 	%r262|%p78, %r260, %r261, %r250, %r252;
	mov.b32 	%f329, %r262;
	add.f32 	%f330, %f328, %f329;
	mov.b32 	%r263, %f330;
	mov.u32 	%r264, 1;
	shfl.sync.bfly.b32 	%r265|%p79, %r263, %r264, %r250, %r252;
	mov.b32 	%f331, %r265;
	add.f32 	%f332, %f330, %f331;
	st.local.f32 	[%rd2+24], %f332;
	st.shared.f32 	[%r10], %f332;
	bar.sync 	0;
	@%p1 bra 	$L__BB39_23;

	ld.shared.f32 	%f333, [%r4];
	mov.b32 	%r266, %f333;
	shfl.sync.bfly.b32 	%r270|%p80, %r266, %r251, %r250, %r252;
	mov.b32 	%f334, %r270;
	add.f32 	%f335, %f333, %f334;
	mov.b32 	%r271, %f335;
	shfl.sync.bfly.b32 	%r273|%p81, %r271, %r255, %r250, %r252;
	mov.b32 	%f336, %r273;
	add.f32 	%f337, %f335, %f336;
	mov.b32 	%r274, %f337;
	shfl.sync.bfly.b32 	%r276|%p82, %r274, %r258, %r250, %r252;
	mov.b32 	%f338, %r276;
	add.f32 	%f339, %f337, %f338;
	mov.b32 	%r277, %f339;
	shfl.sync.bfly.b32 	%r279|%p83, %r277, %r261, %r250, %r252;
	mov.b32 	%f340, %r279;
	add.f32 	%f341, %f339, %f340;
	mov.b32 	%r280, %f341;
	shfl.sync.bfly.b32 	%r282|%p84, %r280, %r264, %r250, %r252;
	mov.b32 	%f342, %r282;
	add.f32 	%f343, %f341, %f342;
	st.local.f32 	[%rd2+24], %f343;

$L__BB39_23:
	bar.sync 	0;
	mov.b32 	%r283, %f382;
	shfl.sync.bfly.b32 	%r287|%p86, %r283, %r251, %r250, %r252;
	mov.b32 	%f344, %r287;
	add.f32 	%f345, %f382, %f344;
	mov.b32 	%r288, %f345;
	shfl.sync.bfly.b32 	%r290|%p87, %r288, %r255, %r250, %r252;
	mov.b32 	%f346, %r290;
	add.f32 	%f347, %f345, %f346;
	mov.b32 	%r291, %f347;
	shfl.sync.bfly.b32 	%r293|%p88, %r291, %r258, %r250, %r252;
	mov.b32 	%f348, %r293;
	add.f32 	%f349, %f347, %f348;
	mov.b32 	%r294, %f349;
	shfl.sync.bfly.b32 	%r296|%p89, %r294, %r261, %r250, %r252;
	mov.b32 	%f350, %r296;
	add.f32 	%f351, %f349, %f350;
	mov.b32 	%r297, %f351;
	shfl.sync.bfly.b32 	%r299|%p90, %r297, %r264, %r250, %r252;
	mov.b32 	%f352, %r299;
	add.f32 	%f353, %f351, %f352;
	st.local.f32 	[%rd2+28], %f353;
	st.shared.f32 	[%r10], %f353;
	bar.sync 	0;
	@%p1 bra 	$L__BB39_25;

	ld.shared.f32 	%f354, [%r4];
	mov.b32 	%r300, %f354;
	mov.u32 	%r301, 31;
	mov.u32 	%r302, 16;
	mov.u32 	%r303, -1;
	shfl.sync.bfly.b32 	%r304|%p91, %r300, %r302, %r301, %r303;
	mov.b32 	%f355, %r304;
	add.f32 	%f356, %f354, %f355;
	mov.b32 	%r305, %f356;
	mov.u32 	%r306, 8;
	shfl.sync.bfly.b32 	%r307|%p92, %r305, %r306, %r301, %r303;
	mov.b32 	%f357, %r307;
	add.f32 	%f358, %f356, %f357;
	mov.b32 	%r308, %f358;
	mov.u32 	%r309, 4;
	shfl.sync.bfly.b32 	%r310|%p93, %r308, %r309, %r301, %r303;
	mov.b32 	%f359, %r310;
	add.f32 	%f360, %f358, %f359;
	mov.b32 	%r311, %f360;
	mov.u32 	%r312, 2;
	shfl.sync.bfly.b32 	%r313|%p94, %r311, %r312, %r301, %r303;
	mov.b32 	%f361, %r313;
	add.f32 	%f362, %f360, %f361;
	mov.b32 	%r314, %f362;
	mov.u32 	%r315, 1;
	shfl.sync.bfly.b32 	%r316|%p95, %r314, %r315, %r301, %r303;
	mov.b32 	%f363, %r316;
	add.f32 	%f364, %f362, %f363;
	st.local.f32 	[%rd2+28], %f364;

$L__BB39_25:
	bar.sync 	0;
	setp.gt.s32 	%p96, %r3, 7;
	@%p96 bra 	$L__BB39_27;

	mul.wide.s32 	%rd68, %r3, 4;
	add.s64 	%rd69, %rd2, %rd68;
	ld.local.f32 	%f365, [%rd69];
	mad.lo.s32 	%r317, %r3, %r13, %r2;
	cvt.s64.s32 	%rd70, %r317;
	mul.lo.s32 	%r318, %r1, %r14;
	cvt.s64.s32 	%rd71, %r318;
	add.s64 	%rd72, %rd71, %rd70;
	cvta.to.global.u64 	%rd73, %rd21;
	shl.b64 	%rd74, %rd72, 2;
	add.s64 	%rd75, %rd73, %rd74;
	st.global.f32 	[%rd75], %f365;

$L__BB39_27:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_1_bs_192
.visible .entry ggml_matvec_f32_ncols_1_bs_192(
	.param .u64 ggml_matvec_f32_ncols_1_bs_192_param_0,
	.param .u64 ggml_matvec_f32_ncols_1_bs_192_param_1,
	.param .u64 ggml_matvec_f32_ncols_1_bs_192_param_2,
	.param .u32 ggml_matvec_f32_ncols_1_bs_192_param_3,
	.param .u32 ggml_matvec_f32_ncols_1_bs_192_param_4,
	.param .u32 ggml_matvec_f32_ncols_1_bs_192_param_5,
	.param .u32 ggml_matvec_f32_ncols_1_bs_192_param_6,
	.param .u32 ggml_matvec_f32_ncols_1_bs_192_param_7,
	.param .u32 ggml_matvec_f32_ncols_1_bs_192_param_8,
	.param .u32 ggml_matvec_f32_ncols_1_bs_192_param_9,
	.param .u32 ggml_matvec_f32_ncols_1_bs_192_param_10,
	.param .u32 ggml_matvec_f32_ncols_1_bs_192_param_11
)
{
	.reg .pred 	%p<19>;
	.reg .f32 	%f<88>;
	.reg .b32 	%r<79>;
	.reg .b64 	%rd<44>;


	ld.param.u64 	%rd18, [ggml_matvec_f32_ncols_1_bs_192_param_0];
	ld.param.u64 	%rd19, [ggml_matvec_f32_ncols_1_bs_192_param_1];
	ld.param.u64 	%rd17, [ggml_matvec_f32_ncols_1_bs_192_param_2];
	ld.param.u32 	%r13, [ggml_matvec_f32_ncols_1_bs_192_param_3];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_1_bs_192_param_5];
	ld.param.u32 	%r14, [ggml_matvec_f32_ncols_1_bs_192_param_7];
	ld.param.u32 	%r17, [ggml_matvec_f32_ncols_1_bs_192_param_8];
	ld.param.u32 	%r18, [ggml_matvec_f32_ncols_1_bs_192_param_9];
	ld.param.u32 	%r19, [ggml_matvec_f32_ncols_1_bs_192_param_10];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_1_bs_192_param_11];
	cvta.to.global.u64 	%rd1, %rd19;
	cvta.to.global.u64 	%rd2, %rd18;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r20, %r1, %r17;
	mov.u32 	%r21, %ctaid.x;
	mul.lo.s32 	%r22, %r21, %r16;
	mad.lo.s32 	%r23, %r20, %r18, %r22;
	cvt.s64.s32 	%rd3, %r23;
	mul.lo.s32 	%r24, %r1, %r19;
	cvt.s64.s32 	%rd4, %r24;
	mov.u32 	%r2, %tid.x;
	setp.gt.s32 	%p1, %r2, 31;
	shl.b32 	%r25, %r2, 2;
	mov.u32 	%r26, data_mmv;
	add.s32 	%r3, %r26, %r25;
	@%p1 bra 	$L__BB40_2;

	mov.u32 	%r27, 0;
	st.shared.u32 	[%r3], %r27;

$L__BB40_2:
	bar.sync 	0;
	setp.ge.s32 	%p2, %r2, %r13;
	mov.f32 	%f86, 0f00000000;
	@%p2 bra 	$L__BB40_9;

	not.b32 	%r28, %r2;
	add.s32 	%r4, %r28, %r13;
	mul.wide.u32 	%rd20, %r4, -1431655765;
	shr.u64 	%rd21, %rd20, 39;
	cvt.u32.u64 	%r29, %rd21;
	add.s32 	%r30, %r29, 1;
	and.b32  	%r76, %r30, 3;
	setp.eq.s32 	%p3, %r76, 0;
	mov.f32 	%f86, 0f00000000;
	mov.u32 	%r77, %r2;
	@%p3 bra 	$L__BB40_6;

	mul.wide.s32 	%rd22, %r2, 2;
	add.s64 	%rd23, %rd22, %rd4;
	shl.b64 	%rd24, %rd23, 2;
	add.s64 	%rd41, %rd1, %rd24;
	add.s64 	%rd25, %rd22, %rd3;
	shl.b64 	%rd26, %rd25, 2;
	add.s64 	%rd40, %rd2, %rd26;
	mov.f32 	%f86, 0f00000000;
	mov.u32 	%r77, %r2;

$L__BB40_5:
	.pragma "nounroll";
	ld.global.nc.v2.f32 	{%f15, %f16}, [%rd40];
	ld.global.nc.v2.f32 	{%f19, %f20}, [%rd41];
	fma.rn.f32 	%f23, %f15, %f19, %f86;
	fma.rn.f32 	%f86, %f16, %f20, %f23;
	add.s32 	%r77, %r77, 192;
	add.s64 	%rd41, %rd41, 1536;
	add.s64 	%rd40, %rd40, 1536;
	add.s32 	%r76, %r76, -1;
	setp.ne.s32 	%p4, %r76, 0;
	@%p4 bra 	$L__BB40_5;

$L__BB40_6:
	setp.lt.u32 	%p5, %r4, 576;
	@%p5 bra 	$L__BB40_9;

	mul.wide.s32 	%rd27, %r77, 2;
	add.s64 	%rd28, %rd27, %rd3;
	shl.b64 	%rd29, %rd28, 2;
	add.s64 	%rd30, %rd2, %rd29;
	add.s64 	%rd43, %rd30, 3072;
	add.s64 	%rd31, %rd27, %rd4;
	shl.b64 	%rd32, %rd31, 2;
	add.s64 	%rd33, %rd1, %rd32;
	add.s64 	%rd42, %rd33, 3072;

$L__BB40_8:
	ld.global.nc.v2.f32 	{%f24, %f25}, [%rd43+-3072];
	ld.global.nc.v2.f32 	{%f28, %f29}, [%rd42+-3072];
	fma.rn.f32 	%f32, %f24, %f28, %f86;
	fma.rn.f32 	%f33, %f25, %f29, %f32;
	ld.global.nc.v2.f32 	{%f34, %f35}, [%rd43+-1536];
	ld.global.nc.v2.f32 	{%f38, %f39}, [%rd42+-1536];
	fma.rn.f32 	%f42, %f34, %f38, %f33;
	fma.rn.f32 	%f43, %f35, %f39, %f42;
	ld.global.nc.v2.f32 	{%f44, %f45}, [%rd43];
	ld.global.nc.v2.f32 	{%f48, %f49}, [%rd42];
	fma.rn.f32 	%f52, %f44, %f48, %f43;
	fma.rn.f32 	%f53, %f45, %f49, %f52;
	ld.global.nc.v2.f32 	{%f54, %f55}, [%rd43+1536];
	ld.global.nc.v2.f32 	{%f58, %f59}, [%rd42+1536];
	fma.rn.f32 	%f62, %f54, %f58, %f53;
	fma.rn.f32 	%f86, %f55, %f59, %f62;
	add.s64 	%rd43, %rd43, 6144;
	add.s64 	%rd42, %rd42, 6144;
	add.s32 	%r77, %r77, 768;
	setp.lt.s32 	%p6, %r77, %r13;
	@%p6 bra 	$L__BB40_8;

$L__BB40_9:
	mov.b32 	%r31, %f86;
	mov.u32 	%r32, 31;
	mov.u32 	%r33, 16;
	mov.u32 	%r34, -1;
	shfl.sync.bfly.b32 	%r35|%p7, %r31, %r33, %r32, %r34;
	mov.b32 	%f63, %r35;
	add.f32 	%f64, %f86, %f63;
	mov.b32 	%r36, %f64;
	mov.u32 	%r37, 8;
	shfl.sync.bfly.b32 	%r38|%p8, %r36, %r37, %r32, %r34;
	mov.b32 	%f65, %r38;
	add.f32 	%f66, %f64, %f65;
	mov.b32 	%r39, %f66;
	mov.u32 	%r40, 4;
	shfl.sync.bfly.b32 	%r41|%p9, %r39, %r40, %r32, %r34;
	mov.b32 	%f67, %r41;
	add.f32 	%f68, %f66, %f67;
	mov.b32 	%r42, %f68;
	mov.u32 	%r43, 2;
	shfl.sync.bfly.b32 	%r44|%p10, %r42, %r43, %r32, %r34;
	mov.b32 	%f69, %r44;
	add.f32 	%f70, %f68, %f69;
	mov.b32 	%r45, %f70;
	mov.u32 	%r46, 1;
	shfl.sync.bfly.b32 	%r47|%p11, %r45, %r46, %r32, %r34;
	mov.b32 	%f71, %r47;
	add.f32 	%f87, %f70, %f71;
	shr.s32 	%r48, %r2, 31;
	shr.u32 	%r49, %r48, 27;
	add.s32 	%r50, %r2, %r49;
	shr.s32 	%r51, %r50, 5;
	shl.b32 	%r52, %r51, 2;
	add.s32 	%r54, %r26, %r52;
	st.shared.f32 	[%r54], %f87;
	bar.sync 	0;
	@%p1 bra 	$L__BB40_11;

	ld.shared.f32 	%f72, [%r3];
	mov.b32 	%r55, %f72;
	shfl.sync.bfly.b32 	%r59|%p13, %r55, %r33, %r32, %r34;
	mov.b32 	%f73, %r59;
	add.f32 	%f74, %f72, %f73;
	mov.b32 	%r60, %f74;
	shfl.sync.bfly.b32 	%r62|%p14, %r60, %r37, %r32, %r34;
	mov.b32 	%f75, %r62;
	add.f32 	%f76, %f74, %f75;
	mov.b32 	%r63, %f76;
	shfl.sync.bfly.b32 	%r65|%p15, %r63, %r40, %r32, %r34;
	mov.b32 	%f77, %r65;
	add.f32 	%f78, %f76, %f77;
	mov.b32 	%r66, %f78;
	shfl.sync.bfly.b32 	%r68|%p16, %r66, %r43, %r32, %r34;
	mov.b32 	%f79, %r68;
	add.f32 	%f80, %f78, %f79;
	mov.b32 	%r69, %f80;
	shfl.sync.bfly.b32 	%r71|%p17, %r69, %r46, %r32, %r34;
	mov.b32 	%f81, %r71;
	add.f32 	%f87, %f80, %f81;

$L__BB40_11:
	bar.sync 	0;
	setp.gt.s32 	%p18, %r2, 0;
	@%p18 bra 	$L__BB40_13;

	mad.lo.s32 	%r73, %r2, %r14, %r21;
	cvt.s64.s32 	%rd34, %r73;
	mul.lo.s32 	%r74, %r1, %r15;
	cvt.s64.s32 	%rd35, %r74;
	add.s64 	%rd36, %rd35, %rd34;
	cvta.to.global.u64 	%rd37, %rd17;
	shl.b64 	%rd38, %rd36, 2;
	add.s64 	%rd39, %rd37, %rd38;
	st.global.f32 	[%rd39], %f87;

$L__BB40_13:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_2_bs_192
.visible .entry ggml_matvec_f32_ncols_2_bs_192(
	.param .u64 ggml_matvec_f32_ncols_2_bs_192_param_0,
	.param .u64 ggml_matvec_f32_ncols_2_bs_192_param_1,
	.param .u64 ggml_matvec_f32_ncols_2_bs_192_param_2,
	.param .u32 ggml_matvec_f32_ncols_2_bs_192_param_3,
	.param .u32 ggml_matvec_f32_ncols_2_bs_192_param_4,
	.param .u32 ggml_matvec_f32_ncols_2_bs_192_param_5,
	.param .u32 ggml_matvec_f32_ncols_2_bs_192_param_6,
	.param .u32 ggml_matvec_f32_ncols_2_bs_192_param_7,
	.param .u32 ggml_matvec_f32_ncols_2_bs_192_param_8,
	.param .u32 ggml_matvec_f32_ncols_2_bs_192_param_9,
	.param .u32 ggml_matvec_f32_ncols_2_bs_192_param_10,
	.param .u32 ggml_matvec_f32_ncols_2_bs_192_param_11
)
{
	.local .align 8 .b8 	__local_depot41[8];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<30>;
	.reg .f32 	%f<146>;
	.reg .b32 	%r<113>;
	.reg .b64 	%rd<66>;


	mov.u64 	%SPL, __local_depot41;
	ld.param.u64 	%rd27, [ggml_matvec_f32_ncols_2_bs_192_param_0];
	ld.param.u64 	%rd28, [ggml_matvec_f32_ncols_2_bs_192_param_1];
	ld.param.u64 	%rd26, [ggml_matvec_f32_ncols_2_bs_192_param_2];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_2_bs_192_param_3];
	ld.param.u32 	%r19, [ggml_matvec_f32_ncols_2_bs_192_param_5];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_2_bs_192_param_6];
	ld.param.u32 	%r17, [ggml_matvec_f32_ncols_2_bs_192_param_7];
	ld.param.u32 	%r20, [ggml_matvec_f32_ncols_2_bs_192_param_8];
	ld.param.u32 	%r21, [ggml_matvec_f32_ncols_2_bs_192_param_9];
	ld.param.u32 	%r22, [ggml_matvec_f32_ncols_2_bs_192_param_10];
	ld.param.u32 	%r18, [ggml_matvec_f32_ncols_2_bs_192_param_11];
	cvta.to.global.u64 	%rd1, %rd28;
	cvta.to.global.u64 	%rd2, %rd27;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r23, %r1, %r20;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r24, %r2, %r19;
	mad.lo.s32 	%r25, %r23, %r21, %r24;
	cvt.s64.s32 	%rd4, %r25;
	mul.lo.s32 	%r26, %r1, %r22;
	cvt.s64.s32 	%rd5, %r26;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r27, %r3, 2;
	mov.u32 	%r28, data_mmv;
	add.s32 	%r4, %r28, %r27;
	@%p1 bra 	$L__BB41_2;

	mov.u32 	%r29, 0;
	st.shared.u32 	[%r4], %r29;

$L__BB41_2:
	bar.sync 	0;
	mov.f32 	%f144, 0f00000000;
	st.local.v2.f32 	[%rd3], {%f144, %f144};
	setp.ge.s32 	%p2, %r3, %r15;
	mov.f32 	%f145, %f144;
	@%p2 bra 	$L__BB41_11;

	not.b32 	%r30, %r3;
	add.s32 	%r5, %r30, %r15;
	mul.wide.u32 	%rd30, %r5, -1431655765;
	shr.u64 	%rd31, %rd30, 39;
	cvt.u32.u64 	%r31, %rd31;
	add.s32 	%r32, %r31, 1;
	and.b32  	%r110, %r32, 3;
	setp.eq.s32 	%p3, %r110, 0;
	mov.f32 	%f144, 0f00000000;
	mov.u32 	%r111, %r3;
	@%p3 bra 	$L__BB41_7;

	mul.wide.s32 	%rd32, %r16, 2;
	mul.wide.s32 	%rd33, %r3, 2;
	add.s64 	%rd34, %rd32, %rd33;
	add.s64 	%rd35, %rd34, %rd5;
	shl.b64 	%rd36, %rd35, 2;
	add.s64 	%rd62, %rd1, %rd36;
	add.s64 	%rd37, %rd33, %rd5;
	shl.b64 	%rd38, %rd37, 2;
	add.s64 	%rd61, %rd1, %rd38;
	add.s64 	%rd39, %rd33, %rd4;
	shl.b64 	%rd40, %rd39, 2;
	add.s64 	%rd60, %rd2, %rd40;
	mov.f32 	%f144, 0f00000000;
	mov.f32 	%f145, %f144;
	mov.u32 	%r111, %r3;

$L__BB41_5:
	.pragma "nounroll";
	ld.global.nc.v2.f32 	{%f19, %f20}, [%rd60];
	ld.global.nc.v2.f32 	{%f23, %f24}, [%rd61];
	fma.rn.f32 	%f27, %f19, %f23, %f145;
	fma.rn.f32 	%f145, %f20, %f24, %f27;
	ld.global.nc.v2.f32 	{%f28, %f29}, [%rd62];
	fma.rn.f32 	%f32, %f19, %f28, %f144;
	fma.rn.f32 	%f144, %f20, %f29, %f32;
	add.s32 	%r111, %r111, 192;
	add.s64 	%rd62, %rd62, 1536;
	add.s64 	%rd61, %rd61, 1536;
	add.s64 	%rd60, %rd60, 1536;
	add.s32 	%r110, %r110, -1;
	setp.ne.s32 	%p4, %r110, 0;
	@%p4 bra 	$L__BB41_5;

	st.local.v2.f32 	[%rd3], {%f145, %f144};

$L__BB41_7:
	setp.lt.u32 	%p5, %r5, 576;
	@%p5 bra 	$L__BB41_11;

	mul.wide.s32 	%rd41, %r111, 2;
	add.s64 	%rd42, %rd41, %rd4;
	shl.b64 	%rd43, %rd42, 2;
	add.s64 	%rd44, %rd2, %rd43;
	add.s64 	%rd65, %rd44, 3072;
	add.s64 	%rd45, %rd41, %rd5;
	shl.b64 	%rd46, %rd45, 2;
	add.s64 	%rd47, %rd1, %rd46;
	add.s64 	%rd64, %rd47, 4608;
	mul.wide.s32 	%rd48, %r16, 2;
	add.s64 	%rd49, %rd45, %rd48;
	shl.b64 	%rd50, %rd49, 2;
	add.s64 	%rd51, %rd1, %rd50;
	add.s64 	%rd63, %rd51, 3072;

$L__BB41_9:
	ld.global.nc.v2.f32 	{%f33, %f34}, [%rd65+-3072];
	ld.global.nc.v2.f32 	{%f37, %f38}, [%rd64+-4608];
	fma.rn.f32 	%f41, %f33, %f37, %f145;
	fma.rn.f32 	%f42, %f34, %f38, %f41;
	ld.global.nc.v2.f32 	{%f43, %f44}, [%rd63+-3072];
	fma.rn.f32 	%f47, %f33, %f43, %f144;
	fma.rn.f32 	%f48, %f34, %f44, %f47;
	ld.global.nc.v2.f32 	{%f49, %f50}, [%rd65+-1536];
	ld.global.nc.v2.f32 	{%f53, %f54}, [%rd64+-3072];
	fma.rn.f32 	%f57, %f49, %f53, %f42;
	fma.rn.f32 	%f58, %f50, %f54, %f57;
	ld.global.nc.v2.f32 	{%f59, %f60}, [%rd63+-1536];
	fma.rn.f32 	%f63, %f49, %f59, %f48;
	fma.rn.f32 	%f64, %f50, %f60, %f63;
	ld.global.nc.v2.f32 	{%f65, %f66}, [%rd65];
	ld.global.nc.v2.f32 	{%f69, %f70}, [%rd64+-1536];
	fma.rn.f32 	%f73, %f65, %f69, %f58;
	fma.rn.f32 	%f74, %f66, %f70, %f73;
	ld.global.nc.v2.f32 	{%f75, %f76}, [%rd63];
	fma.rn.f32 	%f79, %f65, %f75, %f64;
	fma.rn.f32 	%f80, %f66, %f76, %f79;
	ld.global.nc.v2.f32 	{%f81, %f82}, [%rd65+1536];
	ld.global.nc.v2.f32 	{%f85, %f86}, [%rd64];
	fma.rn.f32 	%f89, %f81, %f85, %f74;
	fma.rn.f32 	%f145, %f82, %f86, %f89;
	ld.global.nc.v2.f32 	{%f90, %f91}, [%rd63+1536];
	fma.rn.f32 	%f94, %f81, %f90, %f80;
	fma.rn.f32 	%f144, %f82, %f91, %f94;
	add.s64 	%rd65, %rd65, 6144;
	add.s64 	%rd64, %rd64, 6144;
	add.s64 	%rd63, %rd63, 6144;
	add.s32 	%r111, %r111, 768;
	setp.lt.s32 	%p6, %r111, %r15;
	@%p6 bra 	$L__BB41_9;

	st.local.v2.f32 	[%rd3], {%f145, %f144};

$L__BB41_11:
	shr.s32 	%r33, %r3, 31;
	shr.u32 	%r34, %r33, 27;
	add.s32 	%r35, %r3, %r34;
	shr.s32 	%r36, %r35, 5;
	shl.b32 	%r37, %r36, 2;
	add.s32 	%r14, %r28, %r37;
	mov.u32 	%r39, 2;
	mov.b32 	%r40, %f145;
	mov.u32 	%r41, 31;
	mov.u32 	%r42, 16;
	mov.u32 	%r43, -1;
	shfl.sync.bfly.b32 	%r44|%p7, %r40, %r42, %r41, %r43;
	mov.b32 	%f95, %r44;
	add.f32 	%f96, %f145, %f95;
	mov.b32 	%r45, %f96;
	mov.u32 	%r46, 8;
	shfl.sync.bfly.b32 	%r47|%p8, %r45, %r46, %r41, %r43;
	mov.b32 	%f97, %r47;
	add.f32 	%f98, %f96, %f97;
	mov.b32 	%r48, %f98;
	mov.u32 	%r49, 4;
	shfl.sync.bfly.b32 	%r50|%p9, %r48, %r49, %r41, %r43;
	mov.b32 	%f99, %r50;
	add.f32 	%f100, %f98, %f99;
	mov.b32 	%r51, %f100;
	shfl.sync.bfly.b32 	%r52|%p10, %r51, %r39, %r41, %r43;
	mov.b32 	%f101, %r52;
	add.f32 	%f102, %f100, %f101;
	mov.b32 	%r53, %f102;
	mov.u32 	%r54, 1;
	shfl.sync.bfly.b32 	%r55|%p11, %r53, %r54, %r41, %r43;
	mov.b32 	%f103, %r55;
	add.f32 	%f104, %f102, %f103;
	st.local.f32 	[%rd3], %f104;
	st.shared.f32 	[%r14], %f104;
	bar.sync 	0;
	@%p1 bra 	$L__BB41_13;

	ld.shared.f32 	%f105, [%r4];
	mov.b32 	%r56, %f105;
	shfl.sync.bfly.b32 	%r60|%p13, %r56, %r42, %r41, %r43;
	mov.b32 	%f106, %r60;
	add.f32 	%f107, %f105, %f106;
	mov.b32 	%r61, %f107;
	shfl.sync.bfly.b32 	%r63|%p14, %r61, %r46, %r41, %r43;
	mov.b32 	%f108, %r63;
	add.f32 	%f109, %f107, %f108;
	mov.b32 	%r64, %f109;
	shfl.sync.bfly.b32 	%r66|%p15, %r64, %r49, %r41, %r43;
	mov.b32 	%f110, %r66;
	add.f32 	%f111, %f109, %f110;
	mov.b32 	%r67, %f111;
	shfl.sync.bfly.b32 	%r69|%p16, %r67, %r39, %r41, %r43;
	mov.b32 	%f112, %r69;
	add.f32 	%f113, %f111, %f112;
	mov.b32 	%r70, %f113;
	shfl.sync.bfly.b32 	%r72|%p17, %r70, %r54, %r41, %r43;
	mov.b32 	%f114, %r72;
	add.f32 	%f115, %f113, %f114;
	st.local.f32 	[%rd3], %f115;

$L__BB41_13:
	bar.sync 	0;
	mov.b32 	%r73, %f144;
	shfl.sync.bfly.b32 	%r77|%p19, %r73, %r42, %r41, %r43;
	mov.b32 	%f116, %r77;
	add.f32 	%f117, %f144, %f116;
	mov.b32 	%r78, %f117;
	shfl.sync.bfly.b32 	%r80|%p20, %r78, %r46, %r41, %r43;
	mov.b32 	%f118, %r80;
	add.f32 	%f119, %f117, %f118;
	mov.b32 	%r81, %f119;
	shfl.sync.bfly.b32 	%r83|%p21, %r81, %r49, %r41, %r43;
	mov.b32 	%f120, %r83;
	add.f32 	%f121, %f119, %f120;
	mov.b32 	%r84, %f121;
	shfl.sync.bfly.b32 	%r86|%p22, %r84, %r39, %r41, %r43;
	mov.b32 	%f122, %r86;
	add.f32 	%f123, %f121, %f122;
	mov.b32 	%r87, %f123;
	shfl.sync.bfly.b32 	%r89|%p23, %r87, %r54, %r41, %r43;
	mov.b32 	%f124, %r89;
	add.f32 	%f125, %f123, %f124;
	st.local.f32 	[%rd3+4], %f125;
	st.shared.f32 	[%r14], %f125;
	bar.sync 	0;
	@%p1 bra 	$L__BB41_15;

	ld.shared.f32 	%f126, [%r4];
	mov.b32 	%r90, %f126;
	mov.u32 	%r91, 31;
	mov.u32 	%r92, 16;
	mov.u32 	%r93, -1;
	shfl.sync.bfly.b32 	%r94|%p24, %r90, %r92, %r91, %r93;
	mov.b32 	%f127, %r94;
	add.f32 	%f128, %f126, %f127;
	mov.b32 	%r95, %f128;
	mov.u32 	%r96, 8;
	shfl.sync.bfly.b32 	%r97|%p25, %r95, %r96, %r91, %r93;
	mov.b32 	%f129, %r97;
	add.f32 	%f130, %f128, %f129;
	mov.b32 	%r98, %f130;
	mov.u32 	%r99, 4;
	shfl.sync.bfly.b32 	%r100|%p26, %r98, %r99, %r91, %r93;
	mov.b32 	%f131, %r100;
	add.f32 	%f132, %f130, %f131;
	mov.b32 	%r101, %f132;
	mov.u32 	%r102, 2;
	shfl.sync.bfly.b32 	%r103|%p27, %r101, %r102, %r91, %r93;
	mov.b32 	%f133, %r103;
	add.f32 	%f134, %f132, %f133;
	mov.b32 	%r104, %f134;
	mov.u32 	%r105, 1;
	shfl.sync.bfly.b32 	%r106|%p28, %r104, %r105, %r91, %r93;
	mov.b32 	%f135, %r106;
	add.f32 	%f136, %f134, %f135;
	st.local.f32 	[%rd3+4], %f136;

$L__BB41_15:
	bar.sync 	0;
	setp.gt.s32 	%p29, %r3, 1;
	@%p29 bra 	$L__BB41_17;

	mul.wide.s32 	%rd52, %r3, 4;
	add.s64 	%rd53, %rd3, %rd52;
	ld.local.f32 	%f137, [%rd53];
	mad.lo.s32 	%r107, %r3, %r17, %r2;
	cvt.s64.s32 	%rd54, %r107;
	mul.lo.s32 	%r108, %r1, %r18;
	cvt.s64.s32 	%rd55, %r108;
	add.s64 	%rd56, %rd55, %rd54;
	cvta.to.global.u64 	%rd57, %rd26;
	shl.b64 	%rd58, %rd56, 2;
	add.s64 	%rd59, %rd57, %rd58;
	st.global.f32 	[%rd59], %f137;

$L__BB41_17:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_3_bs_192
.visible .entry ggml_matvec_f32_ncols_3_bs_192(
	.param .u64 ggml_matvec_f32_ncols_3_bs_192_param_0,
	.param .u64 ggml_matvec_f32_ncols_3_bs_192_param_1,
	.param .u64 ggml_matvec_f32_ncols_3_bs_192_param_2,
	.param .u32 ggml_matvec_f32_ncols_3_bs_192_param_3,
	.param .u32 ggml_matvec_f32_ncols_3_bs_192_param_4,
	.param .u32 ggml_matvec_f32_ncols_3_bs_192_param_5,
	.param .u32 ggml_matvec_f32_ncols_3_bs_192_param_6,
	.param .u32 ggml_matvec_f32_ncols_3_bs_192_param_7,
	.param .u32 ggml_matvec_f32_ncols_3_bs_192_param_8,
	.param .u32 ggml_matvec_f32_ncols_3_bs_192_param_9,
	.param .u32 ggml_matvec_f32_ncols_3_bs_192_param_10,
	.param .u32 ggml_matvec_f32_ncols_3_bs_192_param_11
)
{
	.local .align 4 .b8 	__local_depot42[12];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<41>;
	.reg .f32 	%f<208>;
	.reg .b32 	%r<154>;
	.reg .b64 	%rd<74>;


	mov.u64 	%SPL, __local_depot42;
	ld.param.u64 	%rd29, [ggml_matvec_f32_ncols_3_bs_192_param_0];
	ld.param.u64 	%rd30, [ggml_matvec_f32_ncols_3_bs_192_param_1];
	ld.param.u64 	%rd28, [ggml_matvec_f32_ncols_3_bs_192_param_2];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_3_bs_192_param_3];
	ld.param.u32 	%r19, [ggml_matvec_f32_ncols_3_bs_192_param_5];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_3_bs_192_param_6];
	ld.param.u32 	%r17, [ggml_matvec_f32_ncols_3_bs_192_param_7];
	ld.param.u32 	%r20, [ggml_matvec_f32_ncols_3_bs_192_param_8];
	ld.param.u32 	%r21, [ggml_matvec_f32_ncols_3_bs_192_param_9];
	ld.param.u32 	%r22, [ggml_matvec_f32_ncols_3_bs_192_param_10];
	ld.param.u32 	%r18, [ggml_matvec_f32_ncols_3_bs_192_param_11];
	cvta.to.global.u64 	%rd73, %rd30;
	cvta.to.global.u64 	%rd2, %rd29;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r23, %r1, %r20;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r24, %r2, %r19;
	mad.lo.s32 	%r25, %r23, %r21, %r24;
	cvt.s64.s32 	%rd4, %r25;
	mul.lo.s32 	%r26, %r1, %r22;
	cvt.s64.s32 	%rd5, %r26;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r27, %r3, 2;
	mov.u32 	%r28, data_mmv;
	add.s32 	%r4, %r28, %r27;
	@%p1 bra 	$L__BB42_2;

	mov.u32 	%r29, 0;
	st.shared.u32 	[%r4], %r29;

$L__BB42_2:
	bar.sync 	0;
	mov.f32 	%f205, 0f00000000;
	mov.u32 	%r30, 0;
	st.local.u32 	[%rd3], %r30;
	st.local.u32 	[%rd3+4], %r30;
	st.local.u32 	[%rd3+8], %r30;
	setp.ge.s32 	%p2, %r3, %r15;
	mov.f32 	%f206, %f205;
	mov.f32 	%f207, %f205;
	@%p2 bra 	$L__BB42_11;

	not.b32 	%r31, %r3;
	add.s32 	%r5, %r31, %r15;
	mul.wide.u32 	%rd32, %r5, -1431655765;
	shr.u64 	%rd33, %rd32, 39;
	cvt.u32.u64 	%r32, %rd33;
	add.s32 	%r33, %r32, 1;
	and.b32  	%r151, %r33, 3;
	setp.eq.s32 	%p3, %r151, 0;
	mov.f32 	%f205, 0f00000000;
	mov.u32 	%r152, %r3;
	@%p3 bra 	$L__BB42_7;

	shl.b32 	%r34, %r16, 1;
	add.s32 	%r35, %r3, %r34;
	mul.wide.s32 	%rd34, %r35, 2;
	add.s64 	%rd35, %rd34, %rd5;
	shl.b64 	%rd36, %rd35, 2;
	add.s64 	%rd71, %rd73, %rd36;
	mul.wide.s32 	%rd37, %r16, 2;
	mul.wide.s32 	%rd38, %r3, 2;
	add.s64 	%rd39, %rd37, %rd38;
	add.s64 	%rd40, %rd39, %rd5;
	shl.b64 	%rd41, %rd40, 2;
	add.s64 	%rd70, %rd73, %rd41;
	add.s64 	%rd42, %rd38, %rd5;
	shl.b64 	%rd43, %rd42, 2;
	add.s64 	%rd69, %rd73, %rd43;
	add.s64 	%rd44, %rd38, %rd4;
	shl.b64 	%rd45, %rd44, 2;
	add.s64 	%rd68, %rd2, %rd45;
	mov.f32 	%f205, 0f00000000;
	mov.f32 	%f206, %f205;
	mov.f32 	%f207, %f205;
	mov.u32 	%r152, %r3;

$L__BB42_5:
	.pragma "nounroll";
	ld.global.nc.v2.f32 	{%f28, %f29}, [%rd68];
	ld.global.nc.v2.f32 	{%f32, %f33}, [%rd69];
	fma.rn.f32 	%f36, %f28, %f32, %f207;
	fma.rn.f32 	%f207, %f29, %f33, %f36;
	ld.global.nc.v2.f32 	{%f37, %f38}, [%rd70];
	fma.rn.f32 	%f41, %f28, %f37, %f206;
	fma.rn.f32 	%f206, %f29, %f38, %f41;
	ld.global.nc.v2.f32 	{%f42, %f43}, [%rd71];
	fma.rn.f32 	%f46, %f28, %f42, %f205;
	fma.rn.f32 	%f205, %f29, %f43, %f46;
	add.s32 	%r152, %r152, 192;
	add.s64 	%rd71, %rd71, 1536;
	add.s64 	%rd70, %rd70, 1536;
	add.s64 	%rd69, %rd69, 1536;
	add.s64 	%rd68, %rd68, 1536;
	add.s32 	%r151, %r151, -1;
	setp.ne.s32 	%p4, %r151, 0;
	@%p4 bra 	$L__BB42_5;

	st.local.f32 	[%rd3], %f207;
	st.local.f32 	[%rd3+4], %f206;
	st.local.f32 	[%rd3+8], %f205;

$L__BB42_7:
	setp.lt.u32 	%p5, %r5, 576;
	@%p5 bra 	$L__BB42_11;

	add.s32 	%r36, %r152, %r16;
	shl.b32 	%r37, %r16, 1;
	add.s32 	%r38, %r152, %r37;
	add.s32 	%r39, %r36, 192;
	mul.wide.s32 	%rd46, %r39, 8;
	shl.b64 	%rd47, %rd5, 2;
	add.s64 	%rd19, %rd46, %rd47;
	mul.wide.s32 	%rd48, %r38, 8;
	add.s64 	%rd20, %rd48, %rd47;
	mul.wide.s32 	%rd49, %r152, 2;
	add.s64 	%rd50, %rd49, %rd4;
	shl.b64 	%rd51, %rd50, 2;
	add.s64 	%rd52, %rd2, %rd51;
	add.s64 	%rd72, %rd52, 3072;
	mul.wide.s32 	%rd53, %r152, 8;
	add.s64 	%rd22, %rd53, %rd47;
	mul.wide.s32 	%rd54, %r16, 8;
	add.s64 	%rd55, %rd53, %rd54;
	add.s64 	%rd23, %rd55, %rd47;

$L__BB42_9:
	ld.global.nc.v2.f32 	{%f47, %f48}, [%rd72+-3072];
	add.s64 	%rd56, %rd73, %rd22;
	ld.global.nc.v2.f32 	{%f51, %f52}, [%rd56];
	fma.rn.f32 	%f55, %f47, %f51, %f207;
	fma.rn.f32 	%f56, %f48, %f52, %f55;
	add.s64 	%rd57, %rd73, %rd23;
	ld.global.nc.v2.f32 	{%f57, %f58}, [%rd57];
	fma.rn.f32 	%f61, %f47, %f57, %f206;
	fma.rn.f32 	%f62, %f48, %f58, %f61;
	add.s64 	%rd58, %rd73, %rd20;
	ld.global.nc.v2.f32 	{%f63, %f64}, [%rd58];
	fma.rn.f32 	%f67, %f47, %f63, %f205;
	fma.rn.f32 	%f68, %f48, %f64, %f67;
	ld.global.nc.v2.f32 	{%f69, %f70}, [%rd72+-1536];
	ld.global.nc.v2.f32 	{%f73, %f74}, [%rd56+1536];
	fma.rn.f32 	%f77, %f69, %f73, %f56;
	fma.rn.f32 	%f78, %f70, %f74, %f77;
	add.s64 	%rd59, %rd73, %rd19;
	ld.global.nc.v2.f32 	{%f79, %f80}, [%rd59];
	fma.rn.f32 	%f83, %f69, %f79, %f62;
	fma.rn.f32 	%f84, %f70, %f80, %f83;
	ld.global.nc.v2.f32 	{%f85, %f86}, [%rd58+1536];
	fma.rn.f32 	%f89, %f69, %f85, %f68;
	fma.rn.f32 	%f90, %f70, %f86, %f89;
	ld.global.nc.v2.f32 	{%f91, %f92}, [%rd72];
	ld.global.nc.v2.f32 	{%f95, %f96}, [%rd56+3072];
	fma.rn.f32 	%f99, %f91, %f95, %f78;
	fma.rn.f32 	%f100, %f92, %f96, %f99;
	ld.global.nc.v2.f32 	{%f101, %f102}, [%rd59+1536];
	fma.rn.f32 	%f105, %f91, %f101, %f84;
	fma.rn.f32 	%f106, %f92, %f102, %f105;
	ld.global.nc.v2.f32 	{%f107, %f108}, [%rd58+3072];
	fma.rn.f32 	%f111, %f91, %f107, %f90;
	fma.rn.f32 	%f112, %f92, %f108, %f111;
	ld.global.nc.v2.f32 	{%f113, %f114}, [%rd72+1536];
	ld.global.nc.v2.f32 	{%f117, %f118}, [%rd56+4608];
	fma.rn.f32 	%f121, %f113, %f117, %f100;
	fma.rn.f32 	%f207, %f114, %f118, %f121;
	ld.global.nc.v2.f32 	{%f122, %f123}, [%rd59+3072];
	fma.rn.f32 	%f126, %f113, %f122, %f106;
	fma.rn.f32 	%f206, %f114, %f123, %f126;
	ld.global.nc.v2.f32 	{%f127, %f128}, [%rd58+4608];
	fma.rn.f32 	%f131, %f113, %f127, %f112;
	fma.rn.f32 	%f205, %f114, %f128, %f131;
	add.s64 	%rd73, %rd73, 6144;
	add.s64 	%rd72, %rd72, 6144;
	add.s32 	%r152, %r152, 768;
	setp.lt.s32 	%p6, %r152, %r15;
	@%p6 bra 	$L__BB42_9;

	st.local.f32 	[%rd3], %f207;
	st.local.f32 	[%rd3+4], %f206;
	st.local.f32 	[%rd3+8], %f205;

$L__BB42_11:
	shr.s32 	%r40, %r3, 31;
	shr.u32 	%r41, %r40, 27;
	add.s32 	%r42, %r3, %r41;
	shr.s32 	%r43, %r42, 5;
	shl.b32 	%r44, %r43, 2;
	add.s32 	%r14, %r28, %r44;
	mov.u32 	%r46, 2;
	mov.b32 	%r47, %f207;
	mov.u32 	%r48, 31;
	mov.u32 	%r49, 16;
	mov.u32 	%r50, -1;
	shfl.sync.bfly.b32 	%r51|%p7, %r47, %r49, %r48, %r50;
	mov.b32 	%f132, %r51;
	add.f32 	%f133, %f207, %f132;
	mov.b32 	%r52, %f133;
	mov.u32 	%r53, 8;
	shfl.sync.bfly.b32 	%r54|%p8, %r52, %r53, %r48, %r50;
	mov.b32 	%f134, %r54;
	add.f32 	%f135, %f133, %f134;
	mov.b32 	%r55, %f135;
	mov.u32 	%r56, 4;
	shfl.sync.bfly.b32 	%r57|%p9, %r55, %r56, %r48, %r50;
	mov.b32 	%f136, %r57;
	add.f32 	%f137, %f135, %f136;
	mov.b32 	%r58, %f137;
	shfl.sync.bfly.b32 	%r59|%p10, %r58, %r46, %r48, %r50;
	mov.b32 	%f138, %r59;
	add.f32 	%f139, %f137, %f138;
	mov.b32 	%r60, %f139;
	mov.u32 	%r61, 1;
	shfl.sync.bfly.b32 	%r62|%p11, %r60, %r61, %r48, %r50;
	mov.b32 	%f140, %r62;
	add.f32 	%f141, %f139, %f140;
	st.local.f32 	[%rd3], %f141;
	st.shared.f32 	[%r14], %f141;
	bar.sync 	0;
	@%p1 bra 	$L__BB42_13;

	ld.shared.f32 	%f142, [%r4];
	mov.b32 	%r63, %f142;
	shfl.sync.bfly.b32 	%r67|%p13, %r63, %r49, %r48, %r50;
	mov.b32 	%f143, %r67;
	add.f32 	%f144, %f142, %f143;
	mov.b32 	%r68, %f144;
	shfl.sync.bfly.b32 	%r70|%p14, %r68, %r53, %r48, %r50;
	mov.b32 	%f145, %r70;
	add.f32 	%f146, %f144, %f145;
	mov.b32 	%r71, %f146;
	shfl.sync.bfly.b32 	%r73|%p15, %r71, %r56, %r48, %r50;
	mov.b32 	%f147, %r73;
	add.f32 	%f148, %f146, %f147;
	mov.b32 	%r74, %f148;
	shfl.sync.bfly.b32 	%r76|%p16, %r74, %r46, %r48, %r50;
	mov.b32 	%f149, %r76;
	add.f32 	%f150, %f148, %f149;
	mov.b32 	%r77, %f150;
	shfl.sync.bfly.b32 	%r79|%p17, %r77, %r61, %r48, %r50;
	mov.b32 	%f151, %r79;
	add.f32 	%f152, %f150, %f151;
	st.local.f32 	[%rd3], %f152;

$L__BB42_13:
	bar.sync 	0;
	mov.b32 	%r80, %f206;
	shfl.sync.bfly.b32 	%r84|%p19, %r80, %r49, %r48, %r50;
	mov.b32 	%f153, %r84;
	add.f32 	%f154, %f206, %f153;
	mov.b32 	%r85, %f154;
	shfl.sync.bfly.b32 	%r87|%p20, %r85, %r53, %r48, %r50;
	mov.b32 	%f155, %r87;
	add.f32 	%f156, %f154, %f155;
	mov.b32 	%r88, %f156;
	shfl.sync.bfly.b32 	%r90|%p21, %r88, %r56, %r48, %r50;
	mov.b32 	%f157, %r90;
	add.f32 	%f158, %f156, %f157;
	mov.b32 	%r91, %f158;
	shfl.sync.bfly.b32 	%r93|%p22, %r91, %r46, %r48, %r50;
	mov.b32 	%f159, %r93;
	add.f32 	%f160, %f158, %f159;
	mov.b32 	%r94, %f160;
	shfl.sync.bfly.b32 	%r96|%p23, %r94, %r61, %r48, %r50;
	mov.b32 	%f161, %r96;
	add.f32 	%f162, %f160, %f161;
	st.local.f32 	[%rd3+4], %f162;
	st.shared.f32 	[%r14], %f162;
	bar.sync 	0;
	@%p1 bra 	$L__BB42_15;

	ld.shared.f32 	%f163, [%r4];
	mov.b32 	%r97, %f163;
	mov.u32 	%r98, 31;
	mov.u32 	%r99, 16;
	mov.u32 	%r100, -1;
	shfl.sync.bfly.b32 	%r101|%p24, %r97, %r99, %r98, %r100;
	mov.b32 	%f164, %r101;
	add.f32 	%f165, %f163, %f164;
	mov.b32 	%r102, %f165;
	mov.u32 	%r103, 8;
	shfl.sync.bfly.b32 	%r104|%p25, %r102, %r103, %r98, %r100;
	mov.b32 	%f166, %r104;
	add.f32 	%f167, %f165, %f166;
	mov.b32 	%r105, %f167;
	mov.u32 	%r106, 4;
	shfl.sync.bfly.b32 	%r107|%p26, %r105, %r106, %r98, %r100;
	mov.b32 	%f168, %r107;
	add.f32 	%f169, %f167, %f168;
	mov.b32 	%r108, %f169;
	mov.u32 	%r109, 2;
	shfl.sync.bfly.b32 	%r110|%p27, %r108, %r109, %r98, %r100;
	mov.b32 	%f170, %r110;
	add.f32 	%f171, %f169, %f170;
	mov.b32 	%r111, %f171;
	mov.u32 	%r112, 1;
	shfl.sync.bfly.b32 	%r113|%p28, %r111, %r112, %r98, %r100;
	mov.b32 	%f172, %r113;
	add.f32 	%f173, %f171, %f172;
	st.local.f32 	[%rd3+4], %f173;

$L__BB42_15:
	bar.sync 	0;
	mov.b32 	%r114, %f205;
	mov.u32 	%r115, 31;
	mov.u32 	%r116, 16;
	mov.u32 	%r117, -1;
	shfl.sync.bfly.b32 	%r118|%p30, %r114, %r116, %r115, %r117;
	mov.b32 	%f174, %r118;
	add.f32 	%f175, %f205, %f174;
	mov.b32 	%r119, %f175;
	mov.u32 	%r120, 8;
	shfl.sync.bfly.b32 	%r121|%p31, %r119, %r120, %r115, %r117;
	mov.b32 	%f176, %r121;
	add.f32 	%f177, %f175, %f176;
	mov.b32 	%r122, %f177;
	mov.u32 	%r123, 4;
	shfl.sync.bfly.b32 	%r124|%p32, %r122, %r123, %r115, %r117;
	mov.b32 	%f178, %r124;
	add.f32 	%f179, %f177, %f178;
	mov.b32 	%r125, %f179;
	mov.u32 	%r126, 2;
	shfl.sync.bfly.b32 	%r127|%p33, %r125, %r126, %r115, %r117;
	mov.b32 	%f180, %r127;
	add.f32 	%f181, %f179, %f180;
	mov.b32 	%r128, %f181;
	mov.u32 	%r129, 1;
	shfl.sync.bfly.b32 	%r130|%p34, %r128, %r129, %r115, %r117;
	mov.b32 	%f182, %r130;
	add.f32 	%f183, %f181, %f182;
	st.local.f32 	[%rd3+8], %f183;
	st.shared.f32 	[%r14], %f183;
	bar.sync 	0;
	@%p1 bra 	$L__BB42_17;

	ld.shared.f32 	%f184, [%r4];
	mov.b32 	%r131, %f184;
	shfl.sync.bfly.b32 	%r135|%p35, %r131, %r116, %r115, %r117;
	mov.b32 	%f185, %r135;
	add.f32 	%f186, %f184, %f185;
	mov.b32 	%r136, %f186;
	shfl.sync.bfly.b32 	%r138|%p36, %r136, %r120, %r115, %r117;
	mov.b32 	%f187, %r138;
	add.f32 	%f188, %f186, %f187;
	mov.b32 	%r139, %f188;
	shfl.sync.bfly.b32 	%r141|%p37, %r139, %r123, %r115, %r117;
	mov.b32 	%f189, %r141;
	add.f32 	%f190, %f188, %f189;
	mov.b32 	%r142, %f190;
	shfl.sync.bfly.b32 	%r144|%p38, %r142, %r126, %r115, %r117;
	mov.b32 	%f191, %r144;
	add.f32 	%f192, %f190, %f191;
	mov.b32 	%r145, %f192;
	shfl.sync.bfly.b32 	%r147|%p39, %r145, %r129, %r115, %r117;
	mov.b32 	%f193, %r147;
	add.f32 	%f194, %f192, %f193;
	st.local.f32 	[%rd3+8], %f194;

$L__BB42_17:
	bar.sync 	0;
	setp.gt.s32 	%p40, %r3, 2;
	@%p40 bra 	$L__BB42_19;

	mul.wide.s32 	%rd60, %r3, 4;
	add.s64 	%rd61, %rd3, %rd60;
	ld.local.f32 	%f195, [%rd61];
	mad.lo.s32 	%r148, %r3, %r17, %r2;
	cvt.s64.s32 	%rd62, %r148;
	mul.lo.s32 	%r149, %r1, %r18;
	cvt.s64.s32 	%rd63, %r149;
	add.s64 	%rd64, %rd63, %rd62;
	cvta.to.global.u64 	%rd65, %rd28;
	shl.b64 	%rd66, %rd64, 2;
	add.s64 	%rd67, %rd65, %rd66;
	st.global.f32 	[%rd67], %f195;

$L__BB42_19:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_4_bs_192
.visible .entry ggml_matvec_f32_ncols_4_bs_192(
	.param .u64 ggml_matvec_f32_ncols_4_bs_192_param_0,
	.param .u64 ggml_matvec_f32_ncols_4_bs_192_param_1,
	.param .u64 ggml_matvec_f32_ncols_4_bs_192_param_2,
	.param .u32 ggml_matvec_f32_ncols_4_bs_192_param_3,
	.param .u32 ggml_matvec_f32_ncols_4_bs_192_param_4,
	.param .u32 ggml_matvec_f32_ncols_4_bs_192_param_5,
	.param .u32 ggml_matvec_f32_ncols_4_bs_192_param_6,
	.param .u32 ggml_matvec_f32_ncols_4_bs_192_param_7,
	.param .u32 ggml_matvec_f32_ncols_4_bs_192_param_8,
	.param .u32 ggml_matvec_f32_ncols_4_bs_192_param_9,
	.param .u32 ggml_matvec_f32_ncols_4_bs_192_param_10,
	.param .u32 ggml_matvec_f32_ncols_4_bs_192_param_11
)
{
	.local .align 16 .b8 	__local_depot43[16];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<52>;
	.reg .f32 	%f<270>;
	.reg .b32 	%r<189>;
	.reg .b64 	%rd<85>;


	mov.u64 	%SPL, __local_depot43;
	ld.param.u64 	%rd34, [ggml_matvec_f32_ncols_4_bs_192_param_0];
	ld.param.u64 	%rd35, [ggml_matvec_f32_ncols_4_bs_192_param_1];
	ld.param.u64 	%rd33, [ggml_matvec_f32_ncols_4_bs_192_param_2];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_4_bs_192_param_3];
	ld.param.u32 	%r19, [ggml_matvec_f32_ncols_4_bs_192_param_5];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_4_bs_192_param_6];
	ld.param.u32 	%r17, [ggml_matvec_f32_ncols_4_bs_192_param_7];
	ld.param.u32 	%r20, [ggml_matvec_f32_ncols_4_bs_192_param_8];
	ld.param.u32 	%r21, [ggml_matvec_f32_ncols_4_bs_192_param_9];
	ld.param.u32 	%r22, [ggml_matvec_f32_ncols_4_bs_192_param_10];
	ld.param.u32 	%r18, [ggml_matvec_f32_ncols_4_bs_192_param_11];
	cvta.to.global.u64 	%rd84, %rd35;
	cvta.to.global.u64 	%rd2, %rd34;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r23, %r1, %r20;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r24, %r2, %r19;
	mad.lo.s32 	%r25, %r23, %r21, %r24;
	cvt.s64.s32 	%rd4, %r25;
	mul.lo.s32 	%r26, %r1, %r22;
	cvt.s64.s32 	%rd5, %r26;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r27, %r3, 2;
	mov.u32 	%r28, data_mmv;
	add.s32 	%r4, %r28, %r27;
	@%p1 bra 	$L__BB43_2;

	mov.u32 	%r29, 0;
	st.shared.u32 	[%r4], %r29;

$L__BB43_2:
	bar.sync 	0;
	mov.f32 	%f266, 0f00000000;
	st.local.v4.f32 	[%rd3], {%f266, %f266, %f266, %f266};
	setp.ge.s32 	%p2, %r3, %r15;
	mov.f32 	%f267, %f266;
	mov.f32 	%f268, %f266;
	mov.f32 	%f269, %f266;
	@%p2 bra 	$L__BB43_11;

	not.b32 	%r30, %r3;
	add.s32 	%r5, %r30, %r15;
	mul.wide.u32 	%rd37, %r5, -1431655765;
	shr.u64 	%rd38, %rd37, 39;
	cvt.u32.u64 	%r31, %rd38;
	add.s32 	%r32, %r31, 1;
	and.b32  	%r186, %r32, 3;
	setp.eq.s32 	%p3, %r186, 0;
	mov.f32 	%f266, 0f00000000;
	mov.u32 	%r187, %r3;
	@%p3 bra 	$L__BB43_7;

	shl.b32 	%r33, %r16, 1;
	add.s32 	%r34, %r3, %r33;
	mul.wide.s32 	%rd39, %r34, 2;
	add.s64 	%rd40, %rd39, %rd5;
	shl.b64 	%rd41, %rd40, 2;
	add.s64 	%rd82, %rd84, %rd41;
	mad.lo.s32 	%r35, %r16, 3, %r3;
	mul.wide.s32 	%rd42, %r35, 2;
	add.s64 	%rd43, %rd42, %rd5;
	shl.b64 	%rd44, %rd43, 2;
	add.s64 	%rd81, %rd84, %rd44;
	mul.wide.s32 	%rd45, %r16, 2;
	mul.wide.s32 	%rd46, %r3, 2;
	add.s64 	%rd47, %rd45, %rd46;
	add.s64 	%rd48, %rd47, %rd5;
	shl.b64 	%rd49, %rd48, 2;
	add.s64 	%rd80, %rd84, %rd49;
	add.s64 	%rd50, %rd46, %rd5;
	shl.b64 	%rd51, %rd50, 2;
	add.s64 	%rd79, %rd84, %rd51;
	add.s64 	%rd52, %rd46, %rd4;
	shl.b64 	%rd53, %rd52, 2;
	add.s64 	%rd78, %rd2, %rd53;
	mov.f32 	%f266, 0f00000000;
	mov.f32 	%f267, %f266;
	mov.f32 	%f268, %f266;
	mov.f32 	%f269, %f266;
	mov.u32 	%r187, %r3;

$L__BB43_5:
	.pragma "nounroll";
	ld.global.nc.v2.f32 	{%f37, %f38}, [%rd78];
	ld.global.nc.v2.f32 	{%f41, %f42}, [%rd79];
	fma.rn.f32 	%f45, %f37, %f41, %f269;
	fma.rn.f32 	%f269, %f38, %f42, %f45;
	ld.global.nc.v2.f32 	{%f46, %f47}, [%rd80];
	fma.rn.f32 	%f50, %f37, %f46, %f268;
	fma.rn.f32 	%f268, %f38, %f47, %f50;
	ld.global.nc.v2.f32 	{%f51, %f52}, [%rd82];
	fma.rn.f32 	%f55, %f37, %f51, %f267;
	fma.rn.f32 	%f267, %f38, %f52, %f55;
	ld.global.nc.v2.f32 	{%f56, %f57}, [%rd81];
	fma.rn.f32 	%f60, %f37, %f56, %f266;
	fma.rn.f32 	%f266, %f38, %f57, %f60;
	add.s32 	%r187, %r187, 192;
	add.s64 	%rd82, %rd82, 1536;
	add.s64 	%rd81, %rd81, 1536;
	add.s64 	%rd80, %rd80, 1536;
	add.s64 	%rd79, %rd79, 1536;
	add.s64 	%rd78, %rd78, 1536;
	add.s32 	%r186, %r186, -1;
	setp.ne.s32 	%p4, %r186, 0;
	@%p4 bra 	$L__BB43_5;

	st.local.v4.f32 	[%rd3], {%f269, %f268, %f267, %f266};

$L__BB43_7:
	setp.lt.u32 	%p5, %r5, 576;
	@%p5 bra 	$L__BB43_11;

	add.s32 	%r36, %r187, %r16;
	shl.b32 	%r37, %r16, 1;
	add.s32 	%r38, %r187, %r37;
	mad.lo.s32 	%r39, %r16, 3, %r187;
	add.s32 	%r40, %r36, 192;
	mul.wide.s32 	%rd54, %r40, 8;
	shl.b64 	%rd55, %rd5, 2;
	add.s64 	%rd23, %rd54, %rd55;
	mul.wide.s32 	%rd56, %r38, 8;
	add.s64 	%rd24, %rd56, %rd55;
	mul.wide.s32 	%rd57, %r39, 8;
	add.s64 	%rd25, %rd57, %rd55;
	mul.wide.s32 	%rd58, %r187, 2;
	add.s64 	%rd59, %rd58, %rd4;
	shl.b64 	%rd60, %rd59, 2;
	add.s64 	%rd61, %rd2, %rd60;
	add.s64 	%rd83, %rd61, 3072;
	mul.wide.s32 	%rd62, %r187, 8;
	add.s64 	%rd27, %rd62, %rd55;
	mul.wide.s32 	%rd63, %r16, 8;
	add.s64 	%rd64, %rd62, %rd63;
	add.s64 	%rd28, %rd64, %rd55;

$L__BB43_9:
	ld.global.nc.v2.f32 	{%f61, %f62}, [%rd83+-3072];
	add.s64 	%rd65, %rd84, %rd27;
	ld.global.nc.v2.f32 	{%f65, %f66}, [%rd65];
	fma.rn.f32 	%f69, %f61, %f65, %f269;
	fma.rn.f32 	%f70, %f62, %f66, %f69;
	add.s64 	%rd66, %rd84, %rd28;
	ld.global.nc.v2.f32 	{%f71, %f72}, [%rd66];
	fma.rn.f32 	%f75, %f61, %f71, %f268;
	fma.rn.f32 	%f76, %f62, %f72, %f75;
	add.s64 	%rd67, %rd84, %rd24;
	ld.global.nc.v2.f32 	{%f77, %f78}, [%rd67];
	fma.rn.f32 	%f81, %f61, %f77, %f267;
	fma.rn.f32 	%f82, %f62, %f78, %f81;
	add.s64 	%rd68, %rd84, %rd25;
	ld.global.nc.v2.f32 	{%f83, %f84}, [%rd68];
	fma.rn.f32 	%f87, %f61, %f83, %f266;
	fma.rn.f32 	%f88, %f62, %f84, %f87;
	ld.global.nc.v2.f32 	{%f89, %f90}, [%rd83+-1536];
	ld.global.nc.v2.f32 	{%f93, %f94}, [%rd65+1536];
	fma.rn.f32 	%f97, %f89, %f93, %f70;
	fma.rn.f32 	%f98, %f90, %f94, %f97;
	add.s64 	%rd69, %rd84, %rd23;
	ld.global.nc.v2.f32 	{%f99, %f100}, [%rd69];
	fma.rn.f32 	%f103, %f89, %f99, %f76;
	fma.rn.f32 	%f104, %f90, %f100, %f103;
	ld.global.nc.v2.f32 	{%f105, %f106}, [%rd67+1536];
	fma.rn.f32 	%f109, %f89, %f105, %f82;
	fma.rn.f32 	%f110, %f90, %f106, %f109;
	ld.global.nc.v2.f32 	{%f111, %f112}, [%rd68+1536];
	fma.rn.f32 	%f115, %f89, %f111, %f88;
	fma.rn.f32 	%f116, %f90, %f112, %f115;
	ld.global.nc.v2.f32 	{%f117, %f118}, [%rd83];
	ld.global.nc.v2.f32 	{%f121, %f122}, [%rd65+3072];
	fma.rn.f32 	%f125, %f117, %f121, %f98;
	fma.rn.f32 	%f126, %f118, %f122, %f125;
	ld.global.nc.v2.f32 	{%f127, %f128}, [%rd69+1536];
	fma.rn.f32 	%f131, %f117, %f127, %f104;
	fma.rn.f32 	%f132, %f118, %f128, %f131;
	ld.global.nc.v2.f32 	{%f133, %f134}, [%rd67+3072];
	fma.rn.f32 	%f137, %f117, %f133, %f110;
	fma.rn.f32 	%f138, %f118, %f134, %f137;
	ld.global.nc.v2.f32 	{%f139, %f140}, [%rd68+3072];
	fma.rn.f32 	%f143, %f117, %f139, %f116;
	fma.rn.f32 	%f144, %f118, %f140, %f143;
	ld.global.nc.v2.f32 	{%f145, %f146}, [%rd83+1536];
	ld.global.nc.v2.f32 	{%f149, %f150}, [%rd65+4608];
	fma.rn.f32 	%f153, %f145, %f149, %f126;
	fma.rn.f32 	%f269, %f146, %f150, %f153;
	ld.global.nc.v2.f32 	{%f154, %f155}, [%rd69+3072];
	fma.rn.f32 	%f158, %f145, %f154, %f132;
	fma.rn.f32 	%f268, %f146, %f155, %f158;
	ld.global.nc.v2.f32 	{%f159, %f160}, [%rd67+4608];
	fma.rn.f32 	%f163, %f145, %f159, %f138;
	fma.rn.f32 	%f267, %f146, %f160, %f163;
	ld.global.nc.v2.f32 	{%f164, %f165}, [%rd68+4608];
	fma.rn.f32 	%f168, %f145, %f164, %f144;
	fma.rn.f32 	%f266, %f146, %f165, %f168;
	add.s64 	%rd84, %rd84, 6144;
	add.s64 	%rd83, %rd83, 6144;
	add.s32 	%r187, %r187, 768;
	setp.lt.s32 	%p6, %r187, %r15;
	@%p6 bra 	$L__BB43_9;

	st.local.v4.f32 	[%rd3], {%f269, %f268, %f267, %f266};

$L__BB43_11:
	shr.s32 	%r41, %r3, 31;
	shr.u32 	%r42, %r41, 27;
	add.s32 	%r43, %r3, %r42;
	shr.s32 	%r44, %r43, 5;
	shl.b32 	%r45, %r44, 2;
	add.s32 	%r14, %r28, %r45;
	mov.u32 	%r47, 2;
	mov.b32 	%r48, %f269;
	mov.u32 	%r49, 31;
	mov.u32 	%r50, 16;
	mov.u32 	%r51, -1;
	shfl.sync.bfly.b32 	%r52|%p7, %r48, %r50, %r49, %r51;
	mov.b32 	%f169, %r52;
	add.f32 	%f170, %f269, %f169;
	mov.b32 	%r53, %f170;
	mov.u32 	%r54, 8;
	shfl.sync.bfly.b32 	%r55|%p8, %r53, %r54, %r49, %r51;
	mov.b32 	%f171, %r55;
	add.f32 	%f172, %f170, %f171;
	mov.b32 	%r56, %f172;
	mov.u32 	%r57, 4;
	shfl.sync.bfly.b32 	%r58|%p9, %r56, %r57, %r49, %r51;
	mov.b32 	%f173, %r58;
	add.f32 	%f174, %f172, %f173;
	mov.b32 	%r59, %f174;
	shfl.sync.bfly.b32 	%r60|%p10, %r59, %r47, %r49, %r51;
	mov.b32 	%f175, %r60;
	add.f32 	%f176, %f174, %f175;
	mov.b32 	%r61, %f176;
	mov.u32 	%r62, 1;
	shfl.sync.bfly.b32 	%r63|%p11, %r61, %r62, %r49, %r51;
	mov.b32 	%f177, %r63;
	add.f32 	%f178, %f176, %f177;
	st.local.f32 	[%rd3], %f178;
	st.shared.f32 	[%r14], %f178;
	bar.sync 	0;
	@%p1 bra 	$L__BB43_13;

	ld.shared.f32 	%f179, [%r4];
	mov.b32 	%r64, %f179;
	shfl.sync.bfly.b32 	%r68|%p13, %r64, %r50, %r49, %r51;
	mov.b32 	%f180, %r68;
	add.f32 	%f181, %f179, %f180;
	mov.b32 	%r69, %f181;
	shfl.sync.bfly.b32 	%r71|%p14, %r69, %r54, %r49, %r51;
	mov.b32 	%f182, %r71;
	add.f32 	%f183, %f181, %f182;
	mov.b32 	%r72, %f183;
	shfl.sync.bfly.b32 	%r74|%p15, %r72, %r57, %r49, %r51;
	mov.b32 	%f184, %r74;
	add.f32 	%f185, %f183, %f184;
	mov.b32 	%r75, %f185;
	shfl.sync.bfly.b32 	%r77|%p16, %r75, %r47, %r49, %r51;
	mov.b32 	%f186, %r77;
	add.f32 	%f187, %f185, %f186;
	mov.b32 	%r78, %f187;
	shfl.sync.bfly.b32 	%r80|%p17, %r78, %r62, %r49, %r51;
	mov.b32 	%f188, %r80;
	add.f32 	%f189, %f187, %f188;
	st.local.f32 	[%rd3], %f189;

$L__BB43_13:
	bar.sync 	0;
	mov.b32 	%r81, %f268;
	shfl.sync.bfly.b32 	%r85|%p19, %r81, %r50, %r49, %r51;
	mov.b32 	%f190, %r85;
	add.f32 	%f191, %f268, %f190;
	mov.b32 	%r86, %f191;
	shfl.sync.bfly.b32 	%r88|%p20, %r86, %r54, %r49, %r51;
	mov.b32 	%f192, %r88;
	add.f32 	%f193, %f191, %f192;
	mov.b32 	%r89, %f193;
	shfl.sync.bfly.b32 	%r91|%p21, %r89, %r57, %r49, %r51;
	mov.b32 	%f194, %r91;
	add.f32 	%f195, %f193, %f194;
	mov.b32 	%r92, %f195;
	shfl.sync.bfly.b32 	%r94|%p22, %r92, %r47, %r49, %r51;
	mov.b32 	%f196, %r94;
	add.f32 	%f197, %f195, %f196;
	mov.b32 	%r95, %f197;
	shfl.sync.bfly.b32 	%r97|%p23, %r95, %r62, %r49, %r51;
	mov.b32 	%f198, %r97;
	add.f32 	%f199, %f197, %f198;
	st.local.f32 	[%rd3+4], %f199;
	st.shared.f32 	[%r14], %f199;
	bar.sync 	0;
	@%p1 bra 	$L__BB43_15;

	ld.shared.f32 	%f200, [%r4];
	mov.b32 	%r98, %f200;
	mov.u32 	%r99, 31;
	mov.u32 	%r100, 16;
	mov.u32 	%r101, -1;
	shfl.sync.bfly.b32 	%r102|%p24, %r98, %r100, %r99, %r101;
	mov.b32 	%f201, %r102;
	add.f32 	%f202, %f200, %f201;
	mov.b32 	%r103, %f202;
	mov.u32 	%r104, 8;
	shfl.sync.bfly.b32 	%r105|%p25, %r103, %r104, %r99, %r101;
	mov.b32 	%f203, %r105;
	add.f32 	%f204, %f202, %f203;
	mov.b32 	%r106, %f204;
	mov.u32 	%r107, 4;
	shfl.sync.bfly.b32 	%r108|%p26, %r106, %r107, %r99, %r101;
	mov.b32 	%f205, %r108;
	add.f32 	%f206, %f204, %f205;
	mov.b32 	%r109, %f206;
	mov.u32 	%r110, 2;
	shfl.sync.bfly.b32 	%r111|%p27, %r109, %r110, %r99, %r101;
	mov.b32 	%f207, %r111;
	add.f32 	%f208, %f206, %f207;
	mov.b32 	%r112, %f208;
	mov.u32 	%r113, 1;
	shfl.sync.bfly.b32 	%r114|%p28, %r112, %r113, %r99, %r101;
	mov.b32 	%f209, %r114;
	add.f32 	%f210, %f208, %f209;
	st.local.f32 	[%rd3+4], %f210;

$L__BB43_15:
	bar.sync 	0;
	mov.b32 	%r115, %f267;
	mov.u32 	%r116, 31;
	mov.u32 	%r117, 16;
	mov.u32 	%r118, -1;
	shfl.sync.bfly.b32 	%r119|%p30, %r115, %r117, %r116, %r118;
	mov.b32 	%f211, %r119;
	add.f32 	%f212, %f267, %f211;
	mov.b32 	%r120, %f212;
	mov.u32 	%r121, 8;
	shfl.sync.bfly.b32 	%r122|%p31, %r120, %r121, %r116, %r118;
	mov.b32 	%f213, %r122;
	add.f32 	%f214, %f212, %f213;
	mov.b32 	%r123, %f214;
	mov.u32 	%r124, 4;
	shfl.sync.bfly.b32 	%r125|%p32, %r123, %r124, %r116, %r118;
	mov.b32 	%f215, %r125;
	add.f32 	%f216, %f214, %f215;
	mov.b32 	%r126, %f216;
	mov.u32 	%r127, 2;
	shfl.sync.bfly.b32 	%r128|%p33, %r126, %r127, %r116, %r118;
	mov.b32 	%f217, %r128;
	add.f32 	%f218, %f216, %f217;
	mov.b32 	%r129, %f218;
	mov.u32 	%r130, 1;
	shfl.sync.bfly.b32 	%r131|%p34, %r129, %r130, %r116, %r118;
	mov.b32 	%f219, %r131;
	add.f32 	%f220, %f218, %f219;
	st.local.f32 	[%rd3+8], %f220;
	st.shared.f32 	[%r14], %f220;
	bar.sync 	0;
	@%p1 bra 	$L__BB43_17;

	ld.shared.f32 	%f221, [%r4];
	mov.b32 	%r132, %f221;
	shfl.sync.bfly.b32 	%r136|%p35, %r132, %r117, %r116, %r118;
	mov.b32 	%f222, %r136;
	add.f32 	%f223, %f221, %f222;
	mov.b32 	%r137, %f223;
	shfl.sync.bfly.b32 	%r139|%p36, %r137, %r121, %r116, %r118;
	mov.b32 	%f224, %r139;
	add.f32 	%f225, %f223, %f224;
	mov.b32 	%r140, %f225;
	shfl.sync.bfly.b32 	%r142|%p37, %r140, %r124, %r116, %r118;
	mov.b32 	%f226, %r142;
	add.f32 	%f227, %f225, %f226;
	mov.b32 	%r143, %f227;
	shfl.sync.bfly.b32 	%r145|%p38, %r143, %r127, %r116, %r118;
	mov.b32 	%f228, %r145;
	add.f32 	%f229, %f227, %f228;
	mov.b32 	%r146, %f229;
	shfl.sync.bfly.b32 	%r148|%p39, %r146, %r130, %r116, %r118;
	mov.b32 	%f230, %r148;
	add.f32 	%f231, %f229, %f230;
	st.local.f32 	[%rd3+8], %f231;

$L__BB43_17:
	bar.sync 	0;
	mov.b32 	%r149, %f266;
	shfl.sync.bfly.b32 	%r153|%p41, %r149, %r117, %r116, %r118;
	mov.b32 	%f232, %r153;
	add.f32 	%f233, %f266, %f232;
	mov.b32 	%r154, %f233;
	shfl.sync.bfly.b32 	%r156|%p42, %r154, %r121, %r116, %r118;
	mov.b32 	%f234, %r156;
	add.f32 	%f235, %f233, %f234;
	mov.b32 	%r157, %f235;
	shfl.sync.bfly.b32 	%r159|%p43, %r157, %r124, %r116, %r118;
	mov.b32 	%f236, %r159;
	add.f32 	%f237, %f235, %f236;
	mov.b32 	%r160, %f237;
	shfl.sync.bfly.b32 	%r162|%p44, %r160, %r127, %r116, %r118;
	mov.b32 	%f238, %r162;
	add.f32 	%f239, %f237, %f238;
	mov.b32 	%r163, %f239;
	shfl.sync.bfly.b32 	%r165|%p45, %r163, %r130, %r116, %r118;
	mov.b32 	%f240, %r165;
	add.f32 	%f241, %f239, %f240;
	st.local.f32 	[%rd3+12], %f241;
	st.shared.f32 	[%r14], %f241;
	bar.sync 	0;
	@%p1 bra 	$L__BB43_19;

	ld.shared.f32 	%f242, [%r4];
	mov.b32 	%r166, %f242;
	mov.u32 	%r167, 31;
	mov.u32 	%r168, 16;
	mov.u32 	%r169, -1;
	shfl.sync.bfly.b32 	%r170|%p46, %r166, %r168, %r167, %r169;
	mov.b32 	%f243, %r170;
	add.f32 	%f244, %f242, %f243;
	mov.b32 	%r171, %f244;
	mov.u32 	%r172, 8;
	shfl.sync.bfly.b32 	%r173|%p47, %r171, %r172, %r167, %r169;
	mov.b32 	%f245, %r173;
	add.f32 	%f246, %f244, %f245;
	mov.b32 	%r174, %f246;
	mov.u32 	%r175, 4;
	shfl.sync.bfly.b32 	%r176|%p48, %r174, %r175, %r167, %r169;
	mov.b32 	%f247, %r176;
	add.f32 	%f248, %f246, %f247;
	mov.b32 	%r177, %f248;
	mov.u32 	%r178, 2;
	shfl.sync.bfly.b32 	%r179|%p49, %r177, %r178, %r167, %r169;
	mov.b32 	%f249, %r179;
	add.f32 	%f250, %f248, %f249;
	mov.b32 	%r180, %f250;
	mov.u32 	%r181, 1;
	shfl.sync.bfly.b32 	%r182|%p50, %r180, %r181, %r167, %r169;
	mov.b32 	%f251, %r182;
	add.f32 	%f252, %f250, %f251;
	st.local.f32 	[%rd3+12], %f252;

$L__BB43_19:
	bar.sync 	0;
	setp.gt.s32 	%p51, %r3, 3;
	@%p51 bra 	$L__BB43_21;

	mul.wide.s32 	%rd70, %r3, 4;
	add.s64 	%rd71, %rd3, %rd70;
	ld.local.f32 	%f253, [%rd71];
	mad.lo.s32 	%r183, %r3, %r17, %r2;
	cvt.s64.s32 	%rd72, %r183;
	mul.lo.s32 	%r184, %r1, %r18;
	cvt.s64.s32 	%rd73, %r184;
	add.s64 	%rd74, %rd73, %rd72;
	cvta.to.global.u64 	%rd75, %rd33;
	shl.b64 	%rd76, %rd74, 2;
	add.s64 	%rd77, %rd75, %rd76;
	st.global.f32 	[%rd77], %f253;

$L__BB43_21:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_5_bs_192
.visible .entry ggml_matvec_f32_ncols_5_bs_192(
	.param .u64 ggml_matvec_f32_ncols_5_bs_192_param_0,
	.param .u64 ggml_matvec_f32_ncols_5_bs_192_param_1,
	.param .u64 ggml_matvec_f32_ncols_5_bs_192_param_2,
	.param .u32 ggml_matvec_f32_ncols_5_bs_192_param_3,
	.param .u32 ggml_matvec_f32_ncols_5_bs_192_param_4,
	.param .u32 ggml_matvec_f32_ncols_5_bs_192_param_5,
	.param .u32 ggml_matvec_f32_ncols_5_bs_192_param_6,
	.param .u32 ggml_matvec_f32_ncols_5_bs_192_param_7,
	.param .u32 ggml_matvec_f32_ncols_5_bs_192_param_8,
	.param .u32 ggml_matvec_f32_ncols_5_bs_192_param_9,
	.param .u32 ggml_matvec_f32_ncols_5_bs_192_param_10,
	.param .u32 ggml_matvec_f32_ncols_5_bs_192_param_11
)
{
	.local .align 4 .b8 	__local_depot44[20];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<63>;
	.reg .f32 	%f<332>;
	.reg .b32 	%r<225>;
	.reg .b64 	%rd<76>;


	mov.u64 	%SPL, __local_depot44;
	ld.param.u64 	%rd28, [ggml_matvec_f32_ncols_5_bs_192_param_0];
	ld.param.u64 	%rd29, [ggml_matvec_f32_ncols_5_bs_192_param_1];
	ld.param.u64 	%rd27, [ggml_matvec_f32_ncols_5_bs_192_param_2];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_5_bs_192_param_3];
	ld.param.u32 	%r19, [ggml_matvec_f32_ncols_5_bs_192_param_5];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_5_bs_192_param_6];
	ld.param.u32 	%r17, [ggml_matvec_f32_ncols_5_bs_192_param_7];
	ld.param.u32 	%r20, [ggml_matvec_f32_ncols_5_bs_192_param_8];
	ld.param.u32 	%r21, [ggml_matvec_f32_ncols_5_bs_192_param_9];
	ld.param.u32 	%r22, [ggml_matvec_f32_ncols_5_bs_192_param_10];
	ld.param.u32 	%r18, [ggml_matvec_f32_ncols_5_bs_192_param_11];
	cvta.to.global.u64 	%rd75, %rd29;
	cvta.to.global.u64 	%rd2, %rd28;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r23, %r1, %r20;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r24, %r2, %r19;
	mad.lo.s32 	%r25, %r23, %r21, %r24;
	cvt.s64.s32 	%rd4, %r25;
	mul.lo.s32 	%r26, %r1, %r22;
	cvt.s64.s32 	%rd5, %r26;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r27, %r3, 2;
	mov.u32 	%r28, data_mmv;
	add.s32 	%r4, %r28, %r27;
	@%p1 bra 	$L__BB44_2;

	mov.u32 	%r29, 0;
	st.shared.u32 	[%r4], %r29;

$L__BB44_2:
	bar.sync 	0;
	mov.f32 	%f327, 0f00000000;
	mov.u32 	%r30, 0;
	st.local.u32 	[%rd3], %r30;
	st.local.u32 	[%rd3+4], %r30;
	st.local.u32 	[%rd3+8], %r30;
	st.local.u32 	[%rd3+12], %r30;
	st.local.u32 	[%rd3+16], %r30;
	setp.ge.s32 	%p2, %r3, %r15;
	mov.f32 	%f328, %f327;
	mov.f32 	%f329, %f327;
	mov.f32 	%f330, %f327;
	mov.f32 	%f331, %f327;
	@%p2 bra 	$L__BB44_11;

	not.b32 	%r31, %r3;
	add.s32 	%r5, %r31, %r15;
	mul.wide.u32 	%rd31, %r5, -1431655765;
	shr.u64 	%rd32, %rd31, 39;
	cvt.u32.u64 	%r32, %rd32;
	add.s32 	%r33, %r32, 1;
	and.b32  	%r222, %r33, 3;
	setp.eq.s32 	%p3, %r222, 0;
	mov.f32 	%f327, 0f00000000;
	mov.u32 	%r223, %r3;
	@%p3 bra 	$L__BB44_7;

	shl.b32 	%r34, %r16, 1;
	mad.lo.s32 	%r35, %r16, 3, %r3;
	mul.wide.s32 	%rd33, %r35, 8;
	shl.b64 	%rd34, %rd5, 2;
	add.s64 	%rd7, %rd33, %rd34;
	mul.wide.s32 	%rd35, %r3, 8;
	mul.wide.s32 	%rd36, %r16, 8;
	add.s64 	%rd37, %rd35, %rd36;
	add.s64 	%rd8, %rd37, %rd34;
	add.s64 	%rd9, %rd35, %rd34;
	mul.wide.s32 	%rd38, %r3, 2;
	add.s64 	%rd39, %rd38, %rd4;
	shl.b64 	%rd40, %rd39, 2;
	add.s64 	%rd72, %rd2, %rd40;
	mul.wide.s32 	%rd11, %r34, 8;
	mov.f32 	%f327, 0f00000000;
	mov.u64 	%rd73, %rd75;
	mov.f32 	%f328, %f327;
	mov.f32 	%f329, %f327;
	mov.f32 	%f330, %f327;
	mov.f32 	%f331, %f327;
	mov.u32 	%r223, %r3;

$L__BB44_5:
	.pragma "nounroll";
	ld.global.nc.v2.f32 	{%f46, %f47}, [%rd72];
	add.s64 	%rd41, %rd73, %rd9;
	ld.global.nc.v2.f32 	{%f50, %f51}, [%rd41];
	fma.rn.f32 	%f54, %f46, %f50, %f331;
	fma.rn.f32 	%f331, %f47, %f51, %f54;
	add.s64 	%rd42, %rd73, %rd8;
	ld.global.nc.v2.f32 	{%f55, %f56}, [%rd42];
	fma.rn.f32 	%f59, %f46, %f55, %f330;
	fma.rn.f32 	%f330, %f47, %f56, %f59;
	add.s64 	%rd43, %rd41, %rd11;
	ld.global.nc.v2.f32 	{%f60, %f61}, [%rd43];
	fma.rn.f32 	%f64, %f46, %f60, %f329;
	fma.rn.f32 	%f329, %f47, %f61, %f64;
	add.s64 	%rd44, %rd73, %rd7;
	ld.global.nc.v2.f32 	{%f65, %f66}, [%rd44];
	fma.rn.f32 	%f69, %f46, %f65, %f328;
	fma.rn.f32 	%f328, %f47, %f66, %f69;
	add.s64 	%rd45, %rd43, %rd11;
	ld.global.nc.v2.f32 	{%f70, %f71}, [%rd45];
	fma.rn.f32 	%f74, %f46, %f70, %f327;
	fma.rn.f32 	%f327, %f47, %f71, %f74;
	add.s32 	%r223, %r223, 192;
	add.s64 	%rd73, %rd73, 1536;
	add.s64 	%rd72, %rd72, 1536;
	add.s32 	%r222, %r222, -1;
	setp.ne.s32 	%p4, %r222, 0;
	@%p4 bra 	$L__BB44_5;

	st.local.f32 	[%rd3], %f331;
	st.local.f32 	[%rd3+4], %f330;
	st.local.f32 	[%rd3+8], %f329;
	st.local.f32 	[%rd3+12], %f328;
	st.local.f32 	[%rd3+16], %f327;

$L__BB44_7:
	setp.lt.u32 	%p5, %r5, 576;
	@%p5 bra 	$L__BB44_11;

	add.s32 	%r36, %r223, %r16;
	shl.b32 	%r37, %r16, 1;
	add.s32 	%r38, %r223, %r37;
	mad.lo.s32 	%r39, %r16, 3, %r223;
	shl.b32 	%r40, %r16, 2;
	add.s32 	%r41, %r223, %r40;
	add.s32 	%r42, %r36, 192;
	mul.wide.s32 	%rd46, %r42, 8;
	shl.b64 	%rd47, %rd5, 2;
	add.s64 	%rd16, %rd46, %rd47;
	mul.wide.s32 	%rd48, %r38, 8;
	add.s64 	%rd17, %rd48, %rd47;
	mul.wide.s32 	%rd49, %r39, 8;
	add.s64 	%rd18, %rd49, %rd47;
	mul.wide.s32 	%rd50, %r41, 8;
	add.s64 	%rd19, %rd50, %rd47;
	mul.wide.s32 	%rd51, %r223, 2;
	add.s64 	%rd52, %rd51, %rd4;
	shl.b64 	%rd53, %rd52, 2;
	add.s64 	%rd54, %rd2, %rd53;
	add.s64 	%rd74, %rd54, 3072;
	mul.wide.s32 	%rd55, %r223, 8;
	add.s64 	%rd21, %rd55, %rd47;
	mul.wide.s32 	%rd56, %r16, 8;
	add.s64 	%rd57, %rd55, %rd56;
	add.s64 	%rd22, %rd57, %rd47;

$L__BB44_9:
	ld.global.nc.v2.f32 	{%f75, %f76}, [%rd74+-3072];
	add.s64 	%rd58, %rd75, %rd21;
	ld.global.nc.v2.f32 	{%f79, %f80}, [%rd58];
	fma.rn.f32 	%f83, %f75, %f79, %f331;
	fma.rn.f32 	%f84, %f76, %f80, %f83;
	add.s64 	%rd59, %rd75, %rd22;
	ld.global.nc.v2.f32 	{%f85, %f86}, [%rd59];
	fma.rn.f32 	%f89, %f75, %f85, %f330;
	fma.rn.f32 	%f90, %f76, %f86, %f89;
	add.s64 	%rd60, %rd75, %rd17;
	ld.global.nc.v2.f32 	{%f91, %f92}, [%rd60];
	fma.rn.f32 	%f95, %f75, %f91, %f329;
	fma.rn.f32 	%f96, %f76, %f92, %f95;
	add.s64 	%rd61, %rd75, %rd18;
	ld.global.nc.v2.f32 	{%f97, %f98}, [%rd61];
	fma.rn.f32 	%f101, %f75, %f97, %f328;
	fma.rn.f32 	%f102, %f76, %f98, %f101;
	add.s64 	%rd62, %rd75, %rd19;
	ld.global.nc.v2.f32 	{%f103, %f104}, [%rd62];
	fma.rn.f32 	%f107, %f75, %f103, %f327;
	fma.rn.f32 	%f108, %f76, %f104, %f107;
	ld.global.nc.v2.f32 	{%f109, %f110}, [%rd74+-1536];
	ld.global.nc.v2.f32 	{%f113, %f114}, [%rd58+1536];
	fma.rn.f32 	%f117, %f109, %f113, %f84;
	fma.rn.f32 	%f118, %f110, %f114, %f117;
	add.s64 	%rd63, %rd75, %rd16;
	ld.global.nc.v2.f32 	{%f119, %f120}, [%rd63];
	fma.rn.f32 	%f123, %f109, %f119, %f90;
	fma.rn.f32 	%f124, %f110, %f120, %f123;
	ld.global.nc.v2.f32 	{%f125, %f126}, [%rd60+1536];
	fma.rn.f32 	%f129, %f109, %f125, %f96;
	fma.rn.f32 	%f130, %f110, %f126, %f129;
	ld.global.nc.v2.f32 	{%f131, %f132}, [%rd61+1536];
	fma.rn.f32 	%f135, %f109, %f131, %f102;
	fma.rn.f32 	%f136, %f110, %f132, %f135;
	ld.global.nc.v2.f32 	{%f137, %f138}, [%rd62+1536];
	fma.rn.f32 	%f141, %f109, %f137, %f108;
	fma.rn.f32 	%f142, %f110, %f138, %f141;
	ld.global.nc.v2.f32 	{%f143, %f144}, [%rd74];
	ld.global.nc.v2.f32 	{%f147, %f148}, [%rd58+3072];
	fma.rn.f32 	%f151, %f143, %f147, %f118;
	fma.rn.f32 	%f152, %f144, %f148, %f151;
	ld.global.nc.v2.f32 	{%f153, %f154}, [%rd63+1536];
	fma.rn.f32 	%f157, %f143, %f153, %f124;
	fma.rn.f32 	%f158, %f144, %f154, %f157;
	ld.global.nc.v2.f32 	{%f159, %f160}, [%rd60+3072];
	fma.rn.f32 	%f163, %f143, %f159, %f130;
	fma.rn.f32 	%f164, %f144, %f160, %f163;
	ld.global.nc.v2.f32 	{%f165, %f166}, [%rd61+3072];
	fma.rn.f32 	%f169, %f143, %f165, %f136;
	fma.rn.f32 	%f170, %f144, %f166, %f169;
	ld.global.nc.v2.f32 	{%f171, %f172}, [%rd62+3072];
	fma.rn.f32 	%f175, %f143, %f171, %f142;
	fma.rn.f32 	%f176, %f144, %f172, %f175;
	ld.global.nc.v2.f32 	{%f177, %f178}, [%rd74+1536];
	ld.global.nc.v2.f32 	{%f181, %f182}, [%rd58+4608];
	fma.rn.f32 	%f185, %f177, %f181, %f152;
	fma.rn.f32 	%f331, %f178, %f182, %f185;
	ld.global.nc.v2.f32 	{%f186, %f187}, [%rd63+3072];
	fma.rn.f32 	%f190, %f177, %f186, %f158;
	fma.rn.f32 	%f330, %f178, %f187, %f190;
	ld.global.nc.v2.f32 	{%f191, %f192}, [%rd60+4608];
	fma.rn.f32 	%f195, %f177, %f191, %f164;
	fma.rn.f32 	%f329, %f178, %f192, %f195;
	ld.global.nc.v2.f32 	{%f196, %f197}, [%rd61+4608];
	fma.rn.f32 	%f200, %f177, %f196, %f170;
	fma.rn.f32 	%f328, %f178, %f197, %f200;
	ld.global.nc.v2.f32 	{%f201, %f202}, [%rd62+4608];
	fma.rn.f32 	%f205, %f177, %f201, %f176;
	fma.rn.f32 	%f327, %f178, %f202, %f205;
	add.s64 	%rd75, %rd75, 6144;
	add.s64 	%rd74, %rd74, 6144;
	add.s32 	%r223, %r223, 768;
	setp.lt.s32 	%p6, %r223, %r15;
	@%p6 bra 	$L__BB44_9;

	st.local.f32 	[%rd3], %f331;
	st.local.f32 	[%rd3+4], %f330;
	st.local.f32 	[%rd3+8], %f329;
	st.local.f32 	[%rd3+12], %f328;
	st.local.f32 	[%rd3+16], %f327;

$L__BB44_11:
	shr.s32 	%r43, %r3, 31;
	shr.u32 	%r44, %r43, 27;
	add.s32 	%r45, %r3, %r44;
	shr.s32 	%r46, %r45, 5;
	shl.b32 	%r47, %r46, 2;
	add.s32 	%r14, %r28, %r47;
	mov.u32 	%r49, 2;
	mov.b32 	%r50, %f331;
	mov.u32 	%r51, 31;
	mov.u32 	%r52, 16;
	mov.u32 	%r53, -1;
	shfl.sync.bfly.b32 	%r54|%p7, %r50, %r52, %r51, %r53;
	mov.b32 	%f206, %r54;
	add.f32 	%f207, %f331, %f206;
	mov.b32 	%r55, %f207;
	mov.u32 	%r56, 8;
	shfl.sync.bfly.b32 	%r57|%p8, %r55, %r56, %r51, %r53;
	mov.b32 	%f208, %r57;
	add.f32 	%f209, %f207, %f208;
	mov.b32 	%r58, %f209;
	mov.u32 	%r59, 4;
	shfl.sync.bfly.b32 	%r60|%p9, %r58, %r59, %r51, %r53;
	mov.b32 	%f210, %r60;
	add.f32 	%f211, %f209, %f210;
	mov.b32 	%r61, %f211;
	shfl.sync.bfly.b32 	%r62|%p10, %r61, %r49, %r51, %r53;
	mov.b32 	%f212, %r62;
	add.f32 	%f213, %f211, %f212;
	mov.b32 	%r63, %f213;
	mov.u32 	%r64, 1;
	shfl.sync.bfly.b32 	%r65|%p11, %r63, %r64, %r51, %r53;
	mov.b32 	%f214, %r65;
	add.f32 	%f215, %f213, %f214;
	st.local.f32 	[%rd3], %f215;
	st.shared.f32 	[%r14], %f215;
	bar.sync 	0;
	@%p1 bra 	$L__BB44_13;

	ld.shared.f32 	%f216, [%r4];
	mov.b32 	%r66, %f216;
	shfl.sync.bfly.b32 	%r70|%p13, %r66, %r52, %r51, %r53;
	mov.b32 	%f217, %r70;
	add.f32 	%f218, %f216, %f217;
	mov.b32 	%r71, %f218;
	shfl.sync.bfly.b32 	%r73|%p14, %r71, %r56, %r51, %r53;
	mov.b32 	%f219, %r73;
	add.f32 	%f220, %f218, %f219;
	mov.b32 	%r74, %f220;
	shfl.sync.bfly.b32 	%r76|%p15, %r74, %r59, %r51, %r53;
	mov.b32 	%f221, %r76;
	add.f32 	%f222, %f220, %f221;
	mov.b32 	%r77, %f222;
	shfl.sync.bfly.b32 	%r79|%p16, %r77, %r49, %r51, %r53;
	mov.b32 	%f223, %r79;
	add.f32 	%f224, %f222, %f223;
	mov.b32 	%r80, %f224;
	shfl.sync.bfly.b32 	%r82|%p17, %r80, %r64, %r51, %r53;
	mov.b32 	%f225, %r82;
	add.f32 	%f226, %f224, %f225;
	st.local.f32 	[%rd3], %f226;

$L__BB44_13:
	bar.sync 	0;
	mov.b32 	%r83, %f330;
	shfl.sync.bfly.b32 	%r87|%p19, %r83, %r52, %r51, %r53;
	mov.b32 	%f227, %r87;
	add.f32 	%f228, %f330, %f227;
	mov.b32 	%r88, %f228;
	shfl.sync.bfly.b32 	%r90|%p20, %r88, %r56, %r51, %r53;
	mov.b32 	%f229, %r90;
	add.f32 	%f230, %f228, %f229;
	mov.b32 	%r91, %f230;
	shfl.sync.bfly.b32 	%r93|%p21, %r91, %r59, %r51, %r53;
	mov.b32 	%f231, %r93;
	add.f32 	%f232, %f230, %f231;
	mov.b32 	%r94, %f232;
	shfl.sync.bfly.b32 	%r96|%p22, %r94, %r49, %r51, %r53;
	mov.b32 	%f233, %r96;
	add.f32 	%f234, %f232, %f233;
	mov.b32 	%r97, %f234;
	shfl.sync.bfly.b32 	%r99|%p23, %r97, %r64, %r51, %r53;
	mov.b32 	%f235, %r99;
	add.f32 	%f236, %f234, %f235;
	st.local.f32 	[%rd3+4], %f236;
	st.shared.f32 	[%r14], %f236;
	bar.sync 	0;
	@%p1 bra 	$L__BB44_15;

	ld.shared.f32 	%f237, [%r4];
	mov.b32 	%r100, %f237;
	mov.u32 	%r101, 31;
	mov.u32 	%r102, 16;
	mov.u32 	%r103, -1;
	shfl.sync.bfly.b32 	%r104|%p24, %r100, %r102, %r101, %r103;
	mov.b32 	%f238, %r104;
	add.f32 	%f239, %f237, %f238;
	mov.b32 	%r105, %f239;
	mov.u32 	%r106, 8;
	shfl.sync.bfly.b32 	%r107|%p25, %r105, %r106, %r101, %r103;
	mov.b32 	%f240, %r107;
	add.f32 	%f241, %f239, %f240;
	mov.b32 	%r108, %f241;
	mov.u32 	%r109, 4;
	shfl.sync.bfly.b32 	%r110|%p26, %r108, %r109, %r101, %r103;
	mov.b32 	%f242, %r110;
	add.f32 	%f243, %f241, %f242;
	mov.b32 	%r111, %f243;
	mov.u32 	%r112, 2;
	shfl.sync.bfly.b32 	%r113|%p27, %r111, %r112, %r101, %r103;
	mov.b32 	%f244, %r113;
	add.f32 	%f245, %f243, %f244;
	mov.b32 	%r114, %f245;
	mov.u32 	%r115, 1;
	shfl.sync.bfly.b32 	%r116|%p28, %r114, %r115, %r101, %r103;
	mov.b32 	%f246, %r116;
	add.f32 	%f247, %f245, %f246;
	st.local.f32 	[%rd3+4], %f247;

$L__BB44_15:
	bar.sync 	0;
	mov.b32 	%r117, %f329;
	mov.u32 	%r118, 31;
	mov.u32 	%r119, 16;
	mov.u32 	%r120, -1;
	shfl.sync.bfly.b32 	%r121|%p30, %r117, %r119, %r118, %r120;
	mov.b32 	%f248, %r121;
	add.f32 	%f249, %f329, %f248;
	mov.b32 	%r122, %f249;
	mov.u32 	%r123, 8;
	shfl.sync.bfly.b32 	%r124|%p31, %r122, %r123, %r118, %r120;
	mov.b32 	%f250, %r124;
	add.f32 	%f251, %f249, %f250;
	mov.b32 	%r125, %f251;
	mov.u32 	%r126, 4;
	shfl.sync.bfly.b32 	%r127|%p32, %r125, %r126, %r118, %r120;
	mov.b32 	%f252, %r127;
	add.f32 	%f253, %f251, %f252;
	mov.b32 	%r128, %f253;
	mov.u32 	%r129, 2;
	shfl.sync.bfly.b32 	%r130|%p33, %r128, %r129, %r118, %r120;
	mov.b32 	%f254, %r130;
	add.f32 	%f255, %f253, %f254;
	mov.b32 	%r131, %f255;
	mov.u32 	%r132, 1;
	shfl.sync.bfly.b32 	%r133|%p34, %r131, %r132, %r118, %r120;
	mov.b32 	%f256, %r133;
	add.f32 	%f257, %f255, %f256;
	st.local.f32 	[%rd3+8], %f257;
	st.shared.f32 	[%r14], %f257;
	bar.sync 	0;
	@%p1 bra 	$L__BB44_17;

	ld.shared.f32 	%f258, [%r4];
	mov.b32 	%r134, %f258;
	shfl.sync.bfly.b32 	%r138|%p35, %r134, %r119, %r118, %r120;
	mov.b32 	%f259, %r138;
	add.f32 	%f260, %f258, %f259;
	mov.b32 	%r139, %f260;
	shfl.sync.bfly.b32 	%r141|%p36, %r139, %r123, %r118, %r120;
	mov.b32 	%f261, %r141;
	add.f32 	%f262, %f260, %f261;
	mov.b32 	%r142, %f262;
	shfl.sync.bfly.b32 	%r144|%p37, %r142, %r126, %r118, %r120;
	mov.b32 	%f263, %r144;
	add.f32 	%f264, %f262, %f263;
	mov.b32 	%r145, %f264;
	shfl.sync.bfly.b32 	%r147|%p38, %r145, %r129, %r118, %r120;
	mov.b32 	%f265, %r147;
	add.f32 	%f266, %f264, %f265;
	mov.b32 	%r148, %f266;
	shfl.sync.bfly.b32 	%r150|%p39, %r148, %r132, %r118, %r120;
	mov.b32 	%f267, %r150;
	add.f32 	%f268, %f266, %f267;
	st.local.f32 	[%rd3+8], %f268;

$L__BB44_17:
	bar.sync 	0;
	mov.b32 	%r151, %f328;
	shfl.sync.bfly.b32 	%r155|%p41, %r151, %r119, %r118, %r120;
	mov.b32 	%f269, %r155;
	add.f32 	%f270, %f328, %f269;
	mov.b32 	%r156, %f270;
	shfl.sync.bfly.b32 	%r158|%p42, %r156, %r123, %r118, %r120;
	mov.b32 	%f271, %r158;
	add.f32 	%f272, %f270, %f271;
	mov.b32 	%r159, %f272;
	shfl.sync.bfly.b32 	%r161|%p43, %r159, %r126, %r118, %r120;
	mov.b32 	%f273, %r161;
	add.f32 	%f274, %f272, %f273;
	mov.b32 	%r162, %f274;
	shfl.sync.bfly.b32 	%r164|%p44, %r162, %r129, %r118, %r120;
	mov.b32 	%f275, %r164;
	add.f32 	%f276, %f274, %f275;
	mov.b32 	%r165, %f276;
	shfl.sync.bfly.b32 	%r167|%p45, %r165, %r132, %r118, %r120;
	mov.b32 	%f277, %r167;
	add.f32 	%f278, %f276, %f277;
	st.local.f32 	[%rd3+12], %f278;
	st.shared.f32 	[%r14], %f278;
	bar.sync 	0;
	@%p1 bra 	$L__BB44_19;

	ld.shared.f32 	%f279, [%r4];
	mov.b32 	%r168, %f279;
	mov.u32 	%r169, 31;
	mov.u32 	%r170, 16;
	mov.u32 	%r171, -1;
	shfl.sync.bfly.b32 	%r172|%p46, %r168, %r170, %r169, %r171;
	mov.b32 	%f280, %r172;
	add.f32 	%f281, %f279, %f280;
	mov.b32 	%r173, %f281;
	mov.u32 	%r174, 8;
	shfl.sync.bfly.b32 	%r175|%p47, %r173, %r174, %r169, %r171;
	mov.b32 	%f282, %r175;
	add.f32 	%f283, %f281, %f282;
	mov.b32 	%r176, %f283;
	mov.u32 	%r177, 4;
	shfl.sync.bfly.b32 	%r178|%p48, %r176, %r177, %r169, %r171;
	mov.b32 	%f284, %r178;
	add.f32 	%f285, %f283, %f284;
	mov.b32 	%r179, %f285;
	mov.u32 	%r180, 2;
	shfl.sync.bfly.b32 	%r181|%p49, %r179, %r180, %r169, %r171;
	mov.b32 	%f286, %r181;
	add.f32 	%f287, %f285, %f286;
	mov.b32 	%r182, %f287;
	mov.u32 	%r183, 1;
	shfl.sync.bfly.b32 	%r184|%p50, %r182, %r183, %r169, %r171;
	mov.b32 	%f288, %r184;
	add.f32 	%f289, %f287, %f288;
	st.local.f32 	[%rd3+12], %f289;

$L__BB44_19:
	bar.sync 	0;
	mov.b32 	%r185, %f327;
	mov.u32 	%r186, 31;
	mov.u32 	%r187, 16;
	mov.u32 	%r188, -1;
	shfl.sync.bfly.b32 	%r189|%p52, %r185, %r187, %r186, %r188;
	mov.b32 	%f290, %r189;
	add.f32 	%f291, %f327, %f290;
	mov.b32 	%r190, %f291;
	mov.u32 	%r191, 8;
	shfl.sync.bfly.b32 	%r192|%p53, %r190, %r191, %r186, %r188;
	mov.b32 	%f292, %r192;
	add.f32 	%f293, %f291, %f292;
	mov.b32 	%r193, %f293;
	mov.u32 	%r194, 4;
	shfl.sync.bfly.b32 	%r195|%p54, %r193, %r194, %r186, %r188;
	mov.b32 	%f294, %r195;
	add.f32 	%f295, %f293, %f294;
	mov.b32 	%r196, %f295;
	mov.u32 	%r197, 2;
	shfl.sync.bfly.b32 	%r198|%p55, %r196, %r197, %r186, %r188;
	mov.b32 	%f296, %r198;
	add.f32 	%f297, %f295, %f296;
	mov.b32 	%r199, %f297;
	mov.u32 	%r200, 1;
	shfl.sync.bfly.b32 	%r201|%p56, %r199, %r200, %r186, %r188;
	mov.b32 	%f298, %r201;
	add.f32 	%f299, %f297, %f298;
	st.local.f32 	[%rd3+16], %f299;
	st.shared.f32 	[%r14], %f299;
	bar.sync 	0;
	@%p1 bra 	$L__BB44_21;

	ld.shared.f32 	%f300, [%r4];
	mov.b32 	%r202, %f300;
	shfl.sync.bfly.b32 	%r206|%p57, %r202, %r187, %r186, %r188;
	mov.b32 	%f301, %r206;
	add.f32 	%f302, %f300, %f301;
	mov.b32 	%r207, %f302;
	shfl.sync.bfly.b32 	%r209|%p58, %r207, %r191, %r186, %r188;
	mov.b32 	%f303, %r209;
	add.f32 	%f304, %f302, %f303;
	mov.b32 	%r210, %f304;
	shfl.sync.bfly.b32 	%r212|%p59, %r210, %r194, %r186, %r188;
	mov.b32 	%f305, %r212;
	add.f32 	%f306, %f304, %f305;
	mov.b32 	%r213, %f306;
	shfl.sync.bfly.b32 	%r215|%p60, %r213, %r197, %r186, %r188;
	mov.b32 	%f307, %r215;
	add.f32 	%f308, %f306, %f307;
	mov.b32 	%r216, %f308;
	shfl.sync.bfly.b32 	%r218|%p61, %r216, %r200, %r186, %r188;
	mov.b32 	%f309, %r218;
	add.f32 	%f310, %f308, %f309;
	st.local.f32 	[%rd3+16], %f310;

$L__BB44_21:
	bar.sync 	0;
	setp.gt.s32 	%p62, %r3, 4;
	@%p62 bra 	$L__BB44_23;

	mul.wide.s32 	%rd64, %r3, 4;
	add.s64 	%rd65, %rd3, %rd64;
	ld.local.f32 	%f311, [%rd65];
	mad.lo.s32 	%r219, %r3, %r17, %r2;
	cvt.s64.s32 	%rd66, %r219;
	mul.lo.s32 	%r220, %r1, %r18;
	cvt.s64.s32 	%rd67, %r220;
	add.s64 	%rd68, %rd67, %rd66;
	cvta.to.global.u64 	%rd69, %rd27;
	shl.b64 	%rd70, %rd68, 2;
	add.s64 	%rd71, %rd69, %rd70;
	st.global.f32 	[%rd71], %f311;

$L__BB44_23:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_6_bs_192
.visible .entry ggml_matvec_f32_ncols_6_bs_192(
	.param .u64 ggml_matvec_f32_ncols_6_bs_192_param_0,
	.param .u64 ggml_matvec_f32_ncols_6_bs_192_param_1,
	.param .u64 ggml_matvec_f32_ncols_6_bs_192_param_2,
	.param .u32 ggml_matvec_f32_ncols_6_bs_192_param_3,
	.param .u32 ggml_matvec_f32_ncols_6_bs_192_param_4,
	.param .u32 ggml_matvec_f32_ncols_6_bs_192_param_5,
	.param .u32 ggml_matvec_f32_ncols_6_bs_192_param_6,
	.param .u32 ggml_matvec_f32_ncols_6_bs_192_param_7,
	.param .u32 ggml_matvec_f32_ncols_6_bs_192_param_8,
	.param .u32 ggml_matvec_f32_ncols_6_bs_192_param_9,
	.param .u32 ggml_matvec_f32_ncols_6_bs_192_param_10,
	.param .u32 ggml_matvec_f32_ncols_6_bs_192_param_11
)
{
	.local .align 8 .b8 	__local_depot45[24];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<75>;
	.reg .f32 	%f<296>;
	.reg .b32 	%r<251>;
	.reg .b64 	%rd<70>;


	mov.u64 	%SPL, __local_depot45;
	ld.param.u64 	%rd20, [ggml_matvec_f32_ncols_6_bs_192_param_0];
	ld.param.u64 	%rd21, [ggml_matvec_f32_ncols_6_bs_192_param_1];
	ld.param.u64 	%rd19, [ggml_matvec_f32_ncols_6_bs_192_param_2];
	ld.param.u32 	%r11, [ggml_matvec_f32_ncols_6_bs_192_param_3];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_6_bs_192_param_5];
	ld.param.u32 	%r12, [ggml_matvec_f32_ncols_6_bs_192_param_6];
	ld.param.u32 	%r13, [ggml_matvec_f32_ncols_6_bs_192_param_7];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_6_bs_192_param_8];
	ld.param.u32 	%r17, [ggml_matvec_f32_ncols_6_bs_192_param_9];
	ld.param.u32 	%r18, [ggml_matvec_f32_ncols_6_bs_192_param_10];
	ld.param.u32 	%r14, [ggml_matvec_f32_ncols_6_bs_192_param_11];
	cvta.to.global.u64 	%rd69, %rd21;
	add.u64 	%rd2, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r19, %r1, %r16;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r20, %r2, %r15;
	mad.lo.s32 	%r21, %r19, %r17, %r20;
	cvt.s64.s32 	%rd3, %r21;
	cvta.to.global.u64 	%rd4, %rd20;
	mul.lo.s32 	%r22, %r1, %r18;
	cvt.s64.s32 	%rd5, %r22;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r23, %r3, 2;
	mov.u32 	%r24, data_mmv;
	add.s32 	%r4, %r24, %r23;
	@%p1 bra 	$L__BB45_2;

	mov.u32 	%r25, 0;
	st.shared.u32 	[%r4], %r25;

$L__BB45_2:
	bar.sync 	0;
	mov.f32 	%f290, 0f00000000;
	st.local.v2.f32 	[%rd2], {%f290, %f290};
	st.local.v2.f32 	[%rd2+8], {%f290, %f290};
	st.local.v2.f32 	[%rd2+16], {%f290, %f290};
	setp.ge.s32 	%p2, %r3, %r11;
	mov.f32 	%f291, %f290;
	mov.f32 	%f292, %f290;
	mov.f32 	%f293, %f290;
	mov.f32 	%f294, %f290;
	mov.f32 	%f295, %f290;
	@%p2 bra 	$L__BB45_9;

	not.b32 	%r26, %r3;
	add.s32 	%r5, %r26, %r11;
	mul.wide.u32 	%rd23, %r5, -1431655765;
	shr.u64 	%rd24, %rd23, 39;
	and.b64  	%rd25, %rd24, 1;
	setp.eq.b64 	%p3, %rd25, 1;
	mov.pred 	%p4, 0;
	xor.pred  	%p5, %p3, %p4;
	mov.f32 	%f290, 0f00000000;
	mov.u32 	%r250, %r3;
	@%p5 bra 	$L__BB45_5;

	shl.b64 	%rd26, %rd5, 2;
	add.s64 	%rd27, %rd69, %rd26;
	shl.b64 	%rd28, %rd3, 2;
	add.s64 	%rd29, %rd4, %rd28;
	mul.wide.s32 	%rd30, %r3, 8;
	add.s64 	%rd31, %rd29, %rd30;
	ld.global.nc.v2.f32 	{%f43, %f44}, [%rd31];
	add.s64 	%rd32, %rd27, %rd30;
	ld.global.nc.v2.f32 	{%f47, %f48}, [%rd32];
	fma.rn.f32 	%f51, %f43, %f47, 0f00000000;
	fma.rn.f32 	%f295, %f44, %f48, %f51;
	mul.wide.s32 	%rd33, %r12, 8;
	add.s64 	%rd34, %rd32, %rd33;
	ld.global.nc.v2.f32 	{%f52, %f53}, [%rd34];
	fma.rn.f32 	%f56, %f43, %f52, 0f00000000;
	fma.rn.f32 	%f294, %f44, %f53, %f56;
	st.local.v2.f32 	[%rd2], {%f295, %f294};
	add.s32 	%r27, %r3, %r12;
	add.s32 	%r28, %r27, %r12;
	mul.wide.s32 	%rd35, %r28, 8;
	add.s64 	%rd36, %rd27, %rd35;
	ld.global.nc.v2.f32 	{%f57, %f58}, [%rd36];
	fma.rn.f32 	%f61, %f43, %f57, 0f00000000;
	fma.rn.f32 	%f293, %f44, %f58, %f61;
	add.s64 	%rd37, %rd36, %rd33;
	ld.global.nc.v2.f32 	{%f62, %f63}, [%rd37];
	fma.rn.f32 	%f66, %f43, %f62, 0f00000000;
	fma.rn.f32 	%f292, %f44, %f63, %f66;
	st.local.v2.f32 	[%rd2+8], {%f293, %f292};
	add.s64 	%rd38, %rd37, %rd33;
	ld.global.nc.v2.f32 	{%f67, %f68}, [%rd38];
	fma.rn.f32 	%f71, %f43, %f67, 0f00000000;
	fma.rn.f32 	%f291, %f44, %f68, %f71;
	add.s64 	%rd39, %rd38, %rd33;
	ld.global.nc.v2.f32 	{%f72, %f73}, [%rd39];
	fma.rn.f32 	%f76, %f43, %f72, 0f00000000;
	fma.rn.f32 	%f290, %f44, %f73, %f76;
	st.local.v2.f32 	[%rd2+16], {%f291, %f290};
	add.s32 	%r250, %r3, 192;

$L__BB45_5:
	setp.lt.u32 	%p6, %r5, 192;
	@%p6 bra 	$L__BB45_9;

	add.s32 	%r29, %r250, %r12;
	add.s32 	%r30, %r29, 192;
	mul.wide.s32 	%rd40, %r30, 8;
	shl.b64 	%rd41, %rd5, 2;
	add.s64 	%rd7, %rd40, %rd41;
	shl.b32 	%r31, %r12, 1;
	add.s32 	%r32, %r250, %r31;
	mad.lo.s32 	%r33, %r12, 3, %r250;
	shl.b32 	%r34, %r12, 2;
	add.s32 	%r35, %r250, %r34;
	mad.lo.s32 	%r36, %r12, 5, %r250;
	mul.wide.s32 	%rd42, %r32, 8;
	add.s64 	%rd8, %rd42, %rd41;
	mul.wide.s32 	%rd43, %r33, 8;
	add.s64 	%rd9, %rd43, %rd41;
	mul.wide.s32 	%rd44, %r35, 8;
	add.s64 	%rd10, %rd44, %rd41;
	mul.wide.s32 	%rd45, %r36, 8;
	add.s64 	%rd11, %rd45, %rd41;
	mul.wide.s32 	%rd46, %r250, 2;
	add.s64 	%rd47, %rd46, %rd3;
	shl.b64 	%rd48, %rd47, 2;
	add.s64 	%rd49, %rd4, %rd48;
	add.s64 	%rd68, %rd49, 1536;
	mul.wide.s32 	%rd50, %r250, 8;
	mul.wide.s32 	%rd51, %r12, 8;
	add.s64 	%rd52, %rd50, %rd51;
	add.s64 	%rd13, %rd52, %rd41;
	add.s64 	%rd14, %rd50, %rd41;

$L__BB45_7:
	ld.global.nc.v2.f32 	{%f77, %f78}, [%rd68+-1536];
	add.s64 	%rd53, %rd69, %rd14;
	ld.global.nc.v2.f32 	{%f81, %f82}, [%rd53];
	fma.rn.f32 	%f85, %f77, %f81, %f295;
	fma.rn.f32 	%f86, %f78, %f82, %f85;
	add.s64 	%rd54, %rd69, %rd13;
	ld.global.nc.v2.f32 	{%f87, %f88}, [%rd54];
	fma.rn.f32 	%f91, %f77, %f87, %f294;
	fma.rn.f32 	%f92, %f78, %f88, %f91;
	add.s64 	%rd55, %rd69, %rd8;
	ld.global.nc.v2.f32 	{%f93, %f94}, [%rd55];
	fma.rn.f32 	%f97, %f77, %f93, %f293;
	fma.rn.f32 	%f98, %f78, %f94, %f97;
	add.s64 	%rd56, %rd69, %rd9;
	ld.global.nc.v2.f32 	{%f99, %f100}, [%rd56];
	fma.rn.f32 	%f103, %f77, %f99, %f292;
	fma.rn.f32 	%f104, %f78, %f100, %f103;
	add.s64 	%rd57, %rd69, %rd10;
	ld.global.nc.v2.f32 	{%f105, %f106}, [%rd57];
	fma.rn.f32 	%f109, %f77, %f105, %f291;
	fma.rn.f32 	%f110, %f78, %f106, %f109;
	add.s64 	%rd58, %rd69, %rd11;
	ld.global.nc.v2.f32 	{%f111, %f112}, [%rd58];
	fma.rn.f32 	%f115, %f77, %f111, %f290;
	fma.rn.f32 	%f116, %f78, %f112, %f115;
	ld.global.nc.v2.f32 	{%f117, %f118}, [%rd68];
	ld.global.nc.v2.f32 	{%f121, %f122}, [%rd53+1536];
	fma.rn.f32 	%f125, %f117, %f121, %f86;
	fma.rn.f32 	%f295, %f118, %f122, %f125;
	add.s64 	%rd59, %rd69, %rd7;
	ld.global.nc.v2.f32 	{%f126, %f127}, [%rd59];
	fma.rn.f32 	%f130, %f117, %f126, %f92;
	fma.rn.f32 	%f294, %f118, %f127, %f130;
	ld.global.nc.v2.f32 	{%f131, %f132}, [%rd55+1536];
	fma.rn.f32 	%f135, %f117, %f131, %f98;
	fma.rn.f32 	%f293, %f118, %f132, %f135;
	ld.global.nc.v2.f32 	{%f136, %f137}, [%rd56+1536];
	fma.rn.f32 	%f140, %f117, %f136, %f104;
	fma.rn.f32 	%f292, %f118, %f137, %f140;
	ld.global.nc.v2.f32 	{%f141, %f142}, [%rd57+1536];
	fma.rn.f32 	%f145, %f117, %f141, %f110;
	fma.rn.f32 	%f291, %f118, %f142, %f145;
	ld.global.nc.v2.f32 	{%f146, %f147}, [%rd58+1536];
	fma.rn.f32 	%f150, %f117, %f146, %f116;
	fma.rn.f32 	%f290, %f118, %f147, %f150;
	add.s64 	%rd69, %rd69, 3072;
	add.s64 	%rd68, %rd68, 3072;
	add.s32 	%r250, %r250, 384;
	setp.lt.s32 	%p7, %r250, %r11;
	@%p7 bra 	$L__BB45_7;

	st.local.v2.f32 	[%rd2], {%f295, %f294};
	st.local.v2.f32 	[%rd2+8], {%f293, %f292};
	st.local.v2.f32 	[%rd2+16], {%f291, %f290};

$L__BB45_9:
	shr.s32 	%r37, %r3, 31;
	shr.u32 	%r38, %r37, 27;
	add.s32 	%r39, %r3, %r38;
	shr.s32 	%r40, %r39, 5;
	shl.b32 	%r41, %r40, 2;
	add.s32 	%r10, %r24, %r41;
	mov.u32 	%r43, 2;
	mov.b32 	%r44, %f295;
	mov.u32 	%r45, 31;
	mov.u32 	%r46, 16;
	mov.u32 	%r47, -1;
	shfl.sync.bfly.b32 	%r48|%p8, %r44, %r46, %r45, %r47;
	mov.b32 	%f151, %r48;
	add.f32 	%f152, %f295, %f151;
	mov.b32 	%r49, %f152;
	mov.u32 	%r50, 8;
	shfl.sync.bfly.b32 	%r51|%p9, %r49, %r50, %r45, %r47;
	mov.b32 	%f153, %r51;
	add.f32 	%f154, %f152, %f153;
	mov.b32 	%r52, %f154;
	mov.u32 	%r53, 4;
	shfl.sync.bfly.b32 	%r54|%p10, %r52, %r53, %r45, %r47;
	mov.b32 	%f155, %r54;
	add.f32 	%f156, %f154, %f155;
	mov.b32 	%r55, %f156;
	shfl.sync.bfly.b32 	%r56|%p11, %r55, %r43, %r45, %r47;
	mov.b32 	%f157, %r56;
	add.f32 	%f158, %f156, %f157;
	mov.b32 	%r57, %f158;
	mov.u32 	%r58, 1;
	shfl.sync.bfly.b32 	%r59|%p12, %r57, %r58, %r45, %r47;
	mov.b32 	%f159, %r59;
	add.f32 	%f160, %f158, %f159;
	st.local.f32 	[%rd2], %f160;
	st.shared.f32 	[%r10], %f160;
	bar.sync 	0;
	@%p1 bra 	$L__BB45_11;

	ld.shared.f32 	%f161, [%r4];
	mov.b32 	%r60, %f161;
	shfl.sync.bfly.b32 	%r64|%p14, %r60, %r46, %r45, %r47;
	mov.b32 	%f162, %r64;
	add.f32 	%f163, %f161, %f162;
	mov.b32 	%r65, %f163;
	shfl.sync.bfly.b32 	%r67|%p15, %r65, %r50, %r45, %r47;
	mov.b32 	%f164, %r67;
	add.f32 	%f165, %f163, %f164;
	mov.b32 	%r68, %f165;
	shfl.sync.bfly.b32 	%r70|%p16, %r68, %r53, %r45, %r47;
	mov.b32 	%f166, %r70;
	add.f32 	%f167, %f165, %f166;
	mov.b32 	%r71, %f167;
	shfl.sync.bfly.b32 	%r73|%p17, %r71, %r43, %r45, %r47;
	mov.b32 	%f168, %r73;
	add.f32 	%f169, %f167, %f168;
	mov.b32 	%r74, %f169;
	shfl.sync.bfly.b32 	%r76|%p18, %r74, %r58, %r45, %r47;
	mov.b32 	%f170, %r76;
	add.f32 	%f171, %f169, %f170;
	st.local.f32 	[%rd2], %f171;

$L__BB45_11:
	bar.sync 	0;
	mov.b32 	%r77, %f294;
	shfl.sync.bfly.b32 	%r81|%p20, %r77, %r46, %r45, %r47;
	mov.b32 	%f172, %r81;
	add.f32 	%f173, %f294, %f172;
	mov.b32 	%r82, %f173;
	shfl.sync.bfly.b32 	%r84|%p21, %r82, %r50, %r45, %r47;
	mov.b32 	%f174, %r84;
	add.f32 	%f175, %f173, %f174;
	mov.b32 	%r85, %f175;
	shfl.sync.bfly.b32 	%r87|%p22, %r85, %r53, %r45, %r47;
	mov.b32 	%f176, %r87;
	add.f32 	%f177, %f175, %f176;
	mov.b32 	%r88, %f177;
	shfl.sync.bfly.b32 	%r90|%p23, %r88, %r43, %r45, %r47;
	mov.b32 	%f178, %r90;
	add.f32 	%f179, %f177, %f178;
	mov.b32 	%r91, %f179;
	shfl.sync.bfly.b32 	%r93|%p24, %r91, %r58, %r45, %r47;
	mov.b32 	%f180, %r93;
	add.f32 	%f181, %f179, %f180;
	st.local.f32 	[%rd2+4], %f181;
	st.shared.f32 	[%r10], %f181;
	bar.sync 	0;
	@%p1 bra 	$L__BB45_13;

	ld.shared.f32 	%f182, [%r4];
	mov.b32 	%r94, %f182;
	mov.u32 	%r95, 31;
	mov.u32 	%r96, 16;
	mov.u32 	%r97, -1;
	shfl.sync.bfly.b32 	%r98|%p25, %r94, %r96, %r95, %r97;
	mov.b32 	%f183, %r98;
	add.f32 	%f184, %f182, %f183;
	mov.b32 	%r99, %f184;
	mov.u32 	%r100, 8;
	shfl.sync.bfly.b32 	%r101|%p26, %r99, %r100, %r95, %r97;
	mov.b32 	%f185, %r101;
	add.f32 	%f186, %f184, %f185;
	mov.b32 	%r102, %f186;
	mov.u32 	%r103, 4;
	shfl.sync.bfly.b32 	%r104|%p27, %r102, %r103, %r95, %r97;
	mov.b32 	%f187, %r104;
	add.f32 	%f188, %f186, %f187;
	mov.b32 	%r105, %f188;
	mov.u32 	%r106, 2;
	shfl.sync.bfly.b32 	%r107|%p28, %r105, %r106, %r95, %r97;
	mov.b32 	%f189, %r107;
	add.f32 	%f190, %f188, %f189;
	mov.b32 	%r108, %f190;
	mov.u32 	%r109, 1;
	shfl.sync.bfly.b32 	%r110|%p29, %r108, %r109, %r95, %r97;
	mov.b32 	%f191, %r110;
	add.f32 	%f192, %f190, %f191;
	st.local.f32 	[%rd2+4], %f192;

$L__BB45_13:
	bar.sync 	0;
	mov.b32 	%r111, %f293;
	mov.u32 	%r112, 31;
	mov.u32 	%r113, 16;
	mov.u32 	%r114, -1;
	shfl.sync.bfly.b32 	%r115|%p31, %r111, %r113, %r112, %r114;
	mov.b32 	%f193, %r115;
	add.f32 	%f194, %f293, %f193;
	mov.b32 	%r116, %f194;
	mov.u32 	%r117, 8;
	shfl.sync.bfly.b32 	%r118|%p32, %r116, %r117, %r112, %r114;
	mov.b32 	%f195, %r118;
	add.f32 	%f196, %f194, %f195;
	mov.b32 	%r119, %f196;
	mov.u32 	%r120, 4;
	shfl.sync.bfly.b32 	%r121|%p33, %r119, %r120, %r112, %r114;
	mov.b32 	%f197, %r121;
	add.f32 	%f198, %f196, %f197;
	mov.b32 	%r122, %f198;
	mov.u32 	%r123, 2;
	shfl.sync.bfly.b32 	%r124|%p34, %r122, %r123, %r112, %r114;
	mov.b32 	%f199, %r124;
	add.f32 	%f200, %f198, %f199;
	mov.b32 	%r125, %f200;
	mov.u32 	%r126, 1;
	shfl.sync.bfly.b32 	%r127|%p35, %r125, %r126, %r112, %r114;
	mov.b32 	%f201, %r127;
	add.f32 	%f202, %f200, %f201;
	st.local.f32 	[%rd2+8], %f202;
	st.shared.f32 	[%r10], %f202;
	bar.sync 	0;
	@%p1 bra 	$L__BB45_15;

	ld.shared.f32 	%f203, [%r4];
	mov.b32 	%r128, %f203;
	shfl.sync.bfly.b32 	%r132|%p36, %r128, %r113, %r112, %r114;
	mov.b32 	%f204, %r132;
	add.f32 	%f205, %f203, %f204;
	mov.b32 	%r133, %f205;
	shfl.sync.bfly.b32 	%r135|%p37, %r133, %r117, %r112, %r114;
	mov.b32 	%f206, %r135;
	add.f32 	%f207, %f205, %f206;
	mov.b32 	%r136, %f207;
	shfl.sync.bfly.b32 	%r138|%p38, %r136, %r120, %r112, %r114;
	mov.b32 	%f208, %r138;
	add.f32 	%f209, %f207, %f208;
	mov.b32 	%r139, %f209;
	shfl.sync.bfly.b32 	%r141|%p39, %r139, %r123, %r112, %r114;
	mov.b32 	%f210, %r141;
	add.f32 	%f211, %f209, %f210;
	mov.b32 	%r142, %f211;
	shfl.sync.bfly.b32 	%r144|%p40, %r142, %r126, %r112, %r114;
	mov.b32 	%f212, %r144;
	add.f32 	%f213, %f211, %f212;
	st.local.f32 	[%rd2+8], %f213;

$L__BB45_15:
	bar.sync 	0;
	mov.b32 	%r145, %f292;
	shfl.sync.bfly.b32 	%r149|%p42, %r145, %r113, %r112, %r114;
	mov.b32 	%f214, %r149;
	add.f32 	%f215, %f292, %f214;
	mov.b32 	%r150, %f215;
	shfl.sync.bfly.b32 	%r152|%p43, %r150, %r117, %r112, %r114;
	mov.b32 	%f216, %r152;
	add.f32 	%f217, %f215, %f216;
	mov.b32 	%r153, %f217;
	shfl.sync.bfly.b32 	%r155|%p44, %r153, %r120, %r112, %r114;
	mov.b32 	%f218, %r155;
	add.f32 	%f219, %f217, %f218;
	mov.b32 	%r156, %f219;
	shfl.sync.bfly.b32 	%r158|%p45, %r156, %r123, %r112, %r114;
	mov.b32 	%f220, %r158;
	add.f32 	%f221, %f219, %f220;
	mov.b32 	%r159, %f221;
	shfl.sync.bfly.b32 	%r161|%p46, %r159, %r126, %r112, %r114;
	mov.b32 	%f222, %r161;
	add.f32 	%f223, %f221, %f222;
	st.local.f32 	[%rd2+12], %f223;
	st.shared.f32 	[%r10], %f223;
	bar.sync 	0;
	@%p1 bra 	$L__BB45_17;

	ld.shared.f32 	%f224, [%r4];
	mov.b32 	%r162, %f224;
	mov.u32 	%r163, 31;
	mov.u32 	%r164, 16;
	mov.u32 	%r165, -1;
	shfl.sync.bfly.b32 	%r166|%p47, %r162, %r164, %r163, %r165;
	mov.b32 	%f225, %r166;
	add.f32 	%f226, %f224, %f225;
	mov.b32 	%r167, %f226;
	mov.u32 	%r168, 8;
	shfl.sync.bfly.b32 	%r169|%p48, %r167, %r168, %r163, %r165;
	mov.b32 	%f227, %r169;
	add.f32 	%f228, %f226, %f227;
	mov.b32 	%r170, %f228;
	mov.u32 	%r171, 4;
	shfl.sync.bfly.b32 	%r172|%p49, %r170, %r171, %r163, %r165;
	mov.b32 	%f229, %r172;
	add.f32 	%f230, %f228, %f229;
	mov.b32 	%r173, %f230;
	mov.u32 	%r174, 2;
	shfl.sync.bfly.b32 	%r175|%p50, %r173, %r174, %r163, %r165;
	mov.b32 	%f231, %r175;
	add.f32 	%f232, %f230, %f231;
	mov.b32 	%r176, %f232;
	mov.u32 	%r177, 1;
	shfl.sync.bfly.b32 	%r178|%p51, %r176, %r177, %r163, %r165;
	mov.b32 	%f233, %r178;
	add.f32 	%f234, %f232, %f233;
	st.local.f32 	[%rd2+12], %f234;

$L__BB45_17:
	bar.sync 	0;
	mov.b32 	%r179, %f291;
	mov.u32 	%r180, 31;
	mov.u32 	%r181, 16;
	mov.u32 	%r182, -1;
	shfl.sync.bfly.b32 	%r183|%p53, %r179, %r181, %r180, %r182;
	mov.b32 	%f235, %r183;
	add.f32 	%f236, %f291, %f235;
	mov.b32 	%r184, %f236;
	mov.u32 	%r185, 8;
	shfl.sync.bfly.b32 	%r186|%p54, %r184, %r185, %r180, %r182;
	mov.b32 	%f237, %r186;
	add.f32 	%f238, %f236, %f237;
	mov.b32 	%r187, %f238;
	mov.u32 	%r188, 4;
	shfl.sync.bfly.b32 	%r189|%p55, %r187, %r188, %r180, %r182;
	mov.b32 	%f239, %r189;
	add.f32 	%f240, %f238, %f239;
	mov.b32 	%r190, %f240;
	mov.u32 	%r191, 2;
	shfl.sync.bfly.b32 	%r192|%p56, %r190, %r191, %r180, %r182;
	mov.b32 	%f241, %r192;
	add.f32 	%f242, %f240, %f241;
	mov.b32 	%r193, %f242;
	mov.u32 	%r194, 1;
	shfl.sync.bfly.b32 	%r195|%p57, %r193, %r194, %r180, %r182;
	mov.b32 	%f243, %r195;
	add.f32 	%f244, %f242, %f243;
	st.local.f32 	[%rd2+16], %f244;
	st.shared.f32 	[%r10], %f244;
	bar.sync 	0;
	@%p1 bra 	$L__BB45_19;

	ld.shared.f32 	%f245, [%r4];
	mov.b32 	%r196, %f245;
	shfl.sync.bfly.b32 	%r200|%p58, %r196, %r181, %r180, %r182;
	mov.b32 	%f246, %r200;
	add.f32 	%f247, %f245, %f246;
	mov.b32 	%r201, %f247;
	shfl.sync.bfly.b32 	%r203|%p59, %r201, %r185, %r180, %r182;
	mov.b32 	%f248, %r203;
	add.f32 	%f249, %f247, %f248;
	mov.b32 	%r204, %f249;
	shfl.sync.bfly.b32 	%r206|%p60, %r204, %r188, %r180, %r182;
	mov.b32 	%f250, %r206;
	add.f32 	%f251, %f249, %f250;
	mov.b32 	%r207, %f251;
	shfl.sync.bfly.b32 	%r209|%p61, %r207, %r191, %r180, %r182;
	mov.b32 	%f252, %r209;
	add.f32 	%f253, %f251, %f252;
	mov.b32 	%r210, %f253;
	shfl.sync.bfly.b32 	%r212|%p62, %r210, %r194, %r180, %r182;
	mov.b32 	%f254, %r212;
	add.f32 	%f255, %f253, %f254;
	st.local.f32 	[%rd2+16], %f255;

$L__BB45_19:
	bar.sync 	0;
	mov.b32 	%r213, %f290;
	shfl.sync.bfly.b32 	%r217|%p64, %r213, %r181, %r180, %r182;
	mov.b32 	%f256, %r217;
	add.f32 	%f257, %f290, %f256;
	mov.b32 	%r218, %f257;
	shfl.sync.bfly.b32 	%r220|%p65, %r218, %r185, %r180, %r182;
	mov.b32 	%f258, %r220;
	add.f32 	%f259, %f257, %f258;
	mov.b32 	%r221, %f259;
	shfl.sync.bfly.b32 	%r223|%p66, %r221, %r188, %r180, %r182;
	mov.b32 	%f260, %r223;
	add.f32 	%f261, %f259, %f260;
	mov.b32 	%r224, %f261;
	shfl.sync.bfly.b32 	%r226|%p67, %r224, %r191, %r180, %r182;
	mov.b32 	%f262, %r226;
	add.f32 	%f263, %f261, %f262;
	mov.b32 	%r227, %f263;
	shfl.sync.bfly.b32 	%r229|%p68, %r227, %r194, %r180, %r182;
	mov.b32 	%f264, %r229;
	add.f32 	%f265, %f263, %f264;
	st.local.f32 	[%rd2+20], %f265;
	st.shared.f32 	[%r10], %f265;
	bar.sync 	0;
	@%p1 bra 	$L__BB45_21;

	ld.shared.f32 	%f266, [%r4];
	mov.b32 	%r230, %f266;
	mov.u32 	%r231, 31;
	mov.u32 	%r232, 16;
	mov.u32 	%r233, -1;
	shfl.sync.bfly.b32 	%r234|%p69, %r230, %r232, %r231, %r233;
	mov.b32 	%f267, %r234;
	add.f32 	%f268, %f266, %f267;
	mov.b32 	%r235, %f268;
	mov.u32 	%r236, 8;
	shfl.sync.bfly.b32 	%r237|%p70, %r235, %r236, %r231, %r233;
	mov.b32 	%f269, %r237;
	add.f32 	%f270, %f268, %f269;
	mov.b32 	%r238, %f270;
	mov.u32 	%r239, 4;
	shfl.sync.bfly.b32 	%r240|%p71, %r238, %r239, %r231, %r233;
	mov.b32 	%f271, %r240;
	add.f32 	%f272, %f270, %f271;
	mov.b32 	%r241, %f272;
	mov.u32 	%r242, 2;
	shfl.sync.bfly.b32 	%r243|%p72, %r241, %r242, %r231, %r233;
	mov.b32 	%f273, %r243;
	add.f32 	%f274, %f272, %f273;
	mov.b32 	%r244, %f274;
	mov.u32 	%r245, 1;
	shfl.sync.bfly.b32 	%r246|%p73, %r244, %r245, %r231, %r233;
	mov.b32 	%f275, %r246;
	add.f32 	%f276, %f274, %f275;
	st.local.f32 	[%rd2+20], %f276;

$L__BB45_21:
	bar.sync 	0;
	setp.gt.s32 	%p74, %r3, 5;
	@%p74 bra 	$L__BB45_23;

	mul.wide.s32 	%rd60, %r3, 4;
	add.s64 	%rd61, %rd2, %rd60;
	ld.local.f32 	%f277, [%rd61];
	mad.lo.s32 	%r247, %r3, %r13, %r2;
	cvt.s64.s32 	%rd62, %r247;
	mul.lo.s32 	%r248, %r1, %r14;
	cvt.s64.s32 	%rd63, %r248;
	add.s64 	%rd64, %rd63, %rd62;
	cvta.to.global.u64 	%rd65, %rd19;
	shl.b64 	%rd66, %rd64, 2;
	add.s64 	%rd67, %rd65, %rd66;
	st.global.f32 	[%rd67], %f277;

$L__BB45_23:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_7_bs_192
.visible .entry ggml_matvec_f32_ncols_7_bs_192(
	.param .u64 ggml_matvec_f32_ncols_7_bs_192_param_0,
	.param .u64 ggml_matvec_f32_ncols_7_bs_192_param_1,
	.param .u64 ggml_matvec_f32_ncols_7_bs_192_param_2,
	.param .u32 ggml_matvec_f32_ncols_7_bs_192_param_3,
	.param .u32 ggml_matvec_f32_ncols_7_bs_192_param_4,
	.param .u32 ggml_matvec_f32_ncols_7_bs_192_param_5,
	.param .u32 ggml_matvec_f32_ncols_7_bs_192_param_6,
	.param .u32 ggml_matvec_f32_ncols_7_bs_192_param_7,
	.param .u32 ggml_matvec_f32_ncols_7_bs_192_param_8,
	.param .u32 ggml_matvec_f32_ncols_7_bs_192_param_9,
	.param .u32 ggml_matvec_f32_ncols_7_bs_192_param_10,
	.param .u32 ggml_matvec_f32_ncols_7_bs_192_param_11
)
{
	.local .align 4 .b8 	__local_depot46[28];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<86>;
	.reg .f32 	%f<343>;
	.reg .b32 	%r<287>;
	.reg .b64 	%rd<74>;


	mov.u64 	%SPL, __local_depot46;
	ld.param.u64 	%rd21, [ggml_matvec_f32_ncols_7_bs_192_param_0];
	ld.param.u64 	%rd22, [ggml_matvec_f32_ncols_7_bs_192_param_1];
	ld.param.u64 	%rd20, [ggml_matvec_f32_ncols_7_bs_192_param_2];
	ld.param.u32 	%r11, [ggml_matvec_f32_ncols_7_bs_192_param_3];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_7_bs_192_param_5];
	ld.param.u32 	%r12, [ggml_matvec_f32_ncols_7_bs_192_param_6];
	ld.param.u32 	%r13, [ggml_matvec_f32_ncols_7_bs_192_param_7];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_7_bs_192_param_8];
	ld.param.u32 	%r17, [ggml_matvec_f32_ncols_7_bs_192_param_9];
	ld.param.u32 	%r18, [ggml_matvec_f32_ncols_7_bs_192_param_10];
	ld.param.u32 	%r14, [ggml_matvec_f32_ncols_7_bs_192_param_11];
	cvta.to.global.u64 	%rd73, %rd22;
	add.u64 	%rd2, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r19, %r1, %r16;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r20, %r2, %r15;
	mad.lo.s32 	%r21, %r19, %r17, %r20;
	cvt.s64.s32 	%rd3, %r21;
	cvta.to.global.u64 	%rd4, %rd21;
	mul.lo.s32 	%r22, %r1, %r18;
	cvt.s64.s32 	%rd5, %r22;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r23, %r3, 2;
	mov.u32 	%r24, data_mmv;
	add.s32 	%r4, %r24, %r23;
	@%p1 bra 	$L__BB46_2;

	mov.u32 	%r25, 0;
	st.shared.u32 	[%r4], %r25;

$L__BB46_2:
	bar.sync 	0;
	mov.f32 	%f336, 0f00000000;
	mov.u32 	%r26, 0;
	st.local.u32 	[%rd2], %r26;
	st.local.u32 	[%rd2+4], %r26;
	st.local.u32 	[%rd2+8], %r26;
	st.local.u32 	[%rd2+12], %r26;
	st.local.u32 	[%rd2+16], %r26;
	st.local.u32 	[%rd2+20], %r26;
	st.local.u32 	[%rd2+24], %r26;
	setp.ge.s32 	%p2, %r3, %r11;
	mov.f32 	%f337, %f336;
	mov.f32 	%f338, %f336;
	mov.f32 	%f339, %f336;
	mov.f32 	%f340, %f336;
	mov.f32 	%f341, %f336;
	mov.f32 	%f342, %f336;
	@%p2 bra 	$L__BB46_9;

	not.b32 	%r27, %r3;
	add.s32 	%r5, %r27, %r11;
	mul.wide.u32 	%rd24, %r5, -1431655765;
	shr.u64 	%rd25, %rd24, 39;
	and.b64  	%rd26, %rd25, 1;
	setp.eq.b64 	%p3, %rd26, 1;
	mov.pred 	%p4, 0;
	xor.pred  	%p5, %p3, %p4;
	mov.f32 	%f336, 0f00000000;
	mov.u32 	%r286, %r3;
	@%p5 bra 	$L__BB46_5;

	shl.b64 	%rd27, %rd5, 2;
	add.s64 	%rd28, %rd73, %rd27;
	shl.b64 	%rd29, %rd3, 2;
	add.s64 	%rd30, %rd4, %rd29;
	mul.wide.s32 	%rd31, %r3, 8;
	add.s64 	%rd32, %rd30, %rd31;
	ld.global.nc.v2.f32 	{%f50, %f51}, [%rd32];
	add.s64 	%rd33, %rd28, %rd31;
	ld.global.nc.v2.f32 	{%f54, %f55}, [%rd33];
	fma.rn.f32 	%f58, %f50, %f54, 0f00000000;
	fma.rn.f32 	%f342, %f51, %f55, %f58;
	st.local.f32 	[%rd2], %f342;
	mul.wide.s32 	%rd34, %r12, 8;
	add.s64 	%rd35, %rd33, %rd34;
	ld.global.nc.v2.f32 	{%f59, %f60}, [%rd35];
	fma.rn.f32 	%f63, %f50, %f59, 0f00000000;
	fma.rn.f32 	%f341, %f51, %f60, %f63;
	st.local.f32 	[%rd2+4], %f341;
	add.s32 	%r28, %r3, %r12;
	add.s32 	%r29, %r28, %r12;
	mul.wide.s32 	%rd36, %r29, 8;
	add.s64 	%rd37, %rd28, %rd36;
	ld.global.nc.v2.f32 	{%f64, %f65}, [%rd37];
	fma.rn.f32 	%f68, %f50, %f64, 0f00000000;
	fma.rn.f32 	%f340, %f51, %f65, %f68;
	st.local.f32 	[%rd2+8], %f340;
	add.s64 	%rd38, %rd37, %rd34;
	ld.global.nc.v2.f32 	{%f69, %f70}, [%rd38];
	fma.rn.f32 	%f73, %f50, %f69, 0f00000000;
	fma.rn.f32 	%f339, %f51, %f70, %f73;
	st.local.f32 	[%rd2+12], %f339;
	add.s64 	%rd39, %rd38, %rd34;
	ld.global.nc.v2.f32 	{%f74, %f75}, [%rd39];
	fma.rn.f32 	%f78, %f50, %f74, 0f00000000;
	fma.rn.f32 	%f338, %f51, %f75, %f78;
	st.local.f32 	[%rd2+16], %f338;
	add.s64 	%rd40, %rd39, %rd34;
	ld.global.nc.v2.f32 	{%f79, %f80}, [%rd40];
	fma.rn.f32 	%f83, %f50, %f79, 0f00000000;
	fma.rn.f32 	%f337, %f51, %f80, %f83;
	st.local.f32 	[%rd2+20], %f337;
	add.s64 	%rd41, %rd40, %rd34;
	ld.global.nc.v2.f32 	{%f84, %f85}, [%rd41];
	fma.rn.f32 	%f88, %f50, %f84, 0f00000000;
	fma.rn.f32 	%f336, %f51, %f85, %f88;
	st.local.f32 	[%rd2+24], %f336;
	add.s32 	%r286, %r3, 192;

$L__BB46_5:
	setp.lt.u32 	%p6, %r5, 192;
	@%p6 bra 	$L__BB46_9;

	add.s32 	%r30, %r286, %r12;
	add.s32 	%r31, %r30, 192;
	mul.wide.s32 	%rd42, %r31, 8;
	shl.b64 	%rd43, %rd5, 2;
	add.s64 	%rd7, %rd42, %rd43;
	shl.b32 	%r32, %r12, 1;
	add.s32 	%r33, %r286, %r32;
	mad.lo.s32 	%r34, %r12, 3, %r286;
	shl.b32 	%r35, %r12, 2;
	add.s32 	%r36, %r286, %r35;
	mad.lo.s32 	%r37, %r12, 5, %r286;
	mad.lo.s32 	%r38, %r12, 6, %r286;
	mul.wide.s32 	%rd44, %r33, 8;
	add.s64 	%rd8, %rd44, %rd43;
	mul.wide.s32 	%rd45, %r34, 8;
	add.s64 	%rd9, %rd45, %rd43;
	mul.wide.s32 	%rd46, %r36, 8;
	add.s64 	%rd10, %rd46, %rd43;
	mul.wide.s32 	%rd47, %r37, 8;
	add.s64 	%rd11, %rd47, %rd43;
	mul.wide.s32 	%rd48, %r38, 8;
	add.s64 	%rd12, %rd48, %rd43;
	mul.wide.s32 	%rd49, %r286, 2;
	add.s64 	%rd50, %rd49, %rd3;
	shl.b64 	%rd51, %rd50, 2;
	add.s64 	%rd52, %rd4, %rd51;
	add.s64 	%rd72, %rd52, 1536;
	mul.wide.s32 	%rd53, %r286, 8;
	mul.wide.s32 	%rd54, %r12, 8;
	add.s64 	%rd55, %rd53, %rd54;
	add.s64 	%rd14, %rd55, %rd43;
	add.s64 	%rd15, %rd53, %rd43;

$L__BB46_7:
	ld.global.nc.v2.f32 	{%f89, %f90}, [%rd72+-1536];
	add.s64 	%rd56, %rd73, %rd15;
	ld.global.nc.v2.f32 	{%f93, %f94}, [%rd56];
	fma.rn.f32 	%f97, %f89, %f93, %f342;
	fma.rn.f32 	%f98, %f90, %f94, %f97;
	add.s64 	%rd57, %rd73, %rd14;
	ld.global.nc.v2.f32 	{%f99, %f100}, [%rd57];
	fma.rn.f32 	%f103, %f89, %f99, %f341;
	fma.rn.f32 	%f104, %f90, %f100, %f103;
	add.s64 	%rd58, %rd73, %rd8;
	ld.global.nc.v2.f32 	{%f105, %f106}, [%rd58];
	fma.rn.f32 	%f109, %f89, %f105, %f340;
	fma.rn.f32 	%f110, %f90, %f106, %f109;
	add.s64 	%rd59, %rd73, %rd9;
	ld.global.nc.v2.f32 	{%f111, %f112}, [%rd59];
	fma.rn.f32 	%f115, %f89, %f111, %f339;
	fma.rn.f32 	%f116, %f90, %f112, %f115;
	add.s64 	%rd60, %rd73, %rd10;
	ld.global.nc.v2.f32 	{%f117, %f118}, [%rd60];
	fma.rn.f32 	%f121, %f89, %f117, %f338;
	fma.rn.f32 	%f122, %f90, %f118, %f121;
	add.s64 	%rd61, %rd73, %rd11;
	ld.global.nc.v2.f32 	{%f123, %f124}, [%rd61];
	fma.rn.f32 	%f127, %f89, %f123, %f337;
	fma.rn.f32 	%f128, %f90, %f124, %f127;
	add.s64 	%rd62, %rd73, %rd12;
	ld.global.nc.v2.f32 	{%f129, %f130}, [%rd62];
	fma.rn.f32 	%f133, %f89, %f129, %f336;
	fma.rn.f32 	%f134, %f90, %f130, %f133;
	ld.global.nc.v2.f32 	{%f135, %f136}, [%rd72];
	ld.global.nc.v2.f32 	{%f139, %f140}, [%rd56+1536];
	fma.rn.f32 	%f143, %f135, %f139, %f98;
	fma.rn.f32 	%f342, %f136, %f140, %f143;
	add.s64 	%rd63, %rd73, %rd7;
	ld.global.nc.v2.f32 	{%f144, %f145}, [%rd63];
	fma.rn.f32 	%f148, %f135, %f144, %f104;
	fma.rn.f32 	%f341, %f136, %f145, %f148;
	ld.global.nc.v2.f32 	{%f149, %f150}, [%rd58+1536];
	fma.rn.f32 	%f153, %f135, %f149, %f110;
	fma.rn.f32 	%f340, %f136, %f150, %f153;
	ld.global.nc.v2.f32 	{%f154, %f155}, [%rd59+1536];
	fma.rn.f32 	%f158, %f135, %f154, %f116;
	fma.rn.f32 	%f339, %f136, %f155, %f158;
	ld.global.nc.v2.f32 	{%f159, %f160}, [%rd60+1536];
	fma.rn.f32 	%f163, %f135, %f159, %f122;
	fma.rn.f32 	%f338, %f136, %f160, %f163;
	ld.global.nc.v2.f32 	{%f164, %f165}, [%rd61+1536];
	fma.rn.f32 	%f168, %f135, %f164, %f128;
	fma.rn.f32 	%f337, %f136, %f165, %f168;
	ld.global.nc.v2.f32 	{%f169, %f170}, [%rd62+1536];
	fma.rn.f32 	%f173, %f135, %f169, %f134;
	fma.rn.f32 	%f336, %f136, %f170, %f173;
	add.s64 	%rd73, %rd73, 3072;
	add.s64 	%rd72, %rd72, 3072;
	add.s32 	%r286, %r286, 384;
	setp.lt.s32 	%p7, %r286, %r11;
	@%p7 bra 	$L__BB46_7;

	st.local.f32 	[%rd2], %f342;
	st.local.f32 	[%rd2+4], %f341;
	st.local.f32 	[%rd2+8], %f340;
	st.local.f32 	[%rd2+12], %f339;
	st.local.f32 	[%rd2+16], %f338;
	st.local.f32 	[%rd2+20], %f337;
	st.local.f32 	[%rd2+24], %f336;

$L__BB46_9:
	shr.s32 	%r39, %r3, 31;
	shr.u32 	%r40, %r39, 27;
	add.s32 	%r41, %r3, %r40;
	shr.s32 	%r42, %r41, 5;
	shl.b32 	%r43, %r42, 2;
	add.s32 	%r10, %r24, %r43;
	mov.u32 	%r45, 2;
	mov.b32 	%r46, %f342;
	mov.u32 	%r47, 31;
	mov.u32 	%r48, 16;
	mov.u32 	%r49, -1;
	shfl.sync.bfly.b32 	%r50|%p8, %r46, %r48, %r47, %r49;
	mov.b32 	%f174, %r50;
	add.f32 	%f175, %f342, %f174;
	mov.b32 	%r51, %f175;
	mov.u32 	%r52, 8;
	shfl.sync.bfly.b32 	%r53|%p9, %r51, %r52, %r47, %r49;
	mov.b32 	%f176, %r53;
	add.f32 	%f177, %f175, %f176;
	mov.b32 	%r54, %f177;
	mov.u32 	%r55, 4;
	shfl.sync.bfly.b32 	%r56|%p10, %r54, %r55, %r47, %r49;
	mov.b32 	%f178, %r56;
	add.f32 	%f179, %f177, %f178;
	mov.b32 	%r57, %f179;
	shfl.sync.bfly.b32 	%r58|%p11, %r57, %r45, %r47, %r49;
	mov.b32 	%f180, %r58;
	add.f32 	%f181, %f179, %f180;
	mov.b32 	%r59, %f181;
	mov.u32 	%r60, 1;
	shfl.sync.bfly.b32 	%r61|%p12, %r59, %r60, %r47, %r49;
	mov.b32 	%f182, %r61;
	add.f32 	%f183, %f181, %f182;
	st.local.f32 	[%rd2], %f183;
	st.shared.f32 	[%r10], %f183;
	bar.sync 	0;
	@%p1 bra 	$L__BB46_11;

	ld.shared.f32 	%f184, [%r4];
	mov.b32 	%r62, %f184;
	shfl.sync.bfly.b32 	%r66|%p14, %r62, %r48, %r47, %r49;
	mov.b32 	%f185, %r66;
	add.f32 	%f186, %f184, %f185;
	mov.b32 	%r67, %f186;
	shfl.sync.bfly.b32 	%r69|%p15, %r67, %r52, %r47, %r49;
	mov.b32 	%f187, %r69;
	add.f32 	%f188, %f186, %f187;
	mov.b32 	%r70, %f188;
	shfl.sync.bfly.b32 	%r72|%p16, %r70, %r55, %r47, %r49;
	mov.b32 	%f189, %r72;
	add.f32 	%f190, %f188, %f189;
	mov.b32 	%r73, %f190;
	shfl.sync.bfly.b32 	%r75|%p17, %r73, %r45, %r47, %r49;
	mov.b32 	%f191, %r75;
	add.f32 	%f192, %f190, %f191;
	mov.b32 	%r76, %f192;
	shfl.sync.bfly.b32 	%r78|%p18, %r76, %r60, %r47, %r49;
	mov.b32 	%f193, %r78;
	add.f32 	%f194, %f192, %f193;
	st.local.f32 	[%rd2], %f194;

$L__BB46_11:
	bar.sync 	0;
	mov.b32 	%r79, %f341;
	shfl.sync.bfly.b32 	%r83|%p20, %r79, %r48, %r47, %r49;
	mov.b32 	%f195, %r83;
	add.f32 	%f196, %f341, %f195;
	mov.b32 	%r84, %f196;
	shfl.sync.bfly.b32 	%r86|%p21, %r84, %r52, %r47, %r49;
	mov.b32 	%f197, %r86;
	add.f32 	%f198, %f196, %f197;
	mov.b32 	%r87, %f198;
	shfl.sync.bfly.b32 	%r89|%p22, %r87, %r55, %r47, %r49;
	mov.b32 	%f199, %r89;
	add.f32 	%f200, %f198, %f199;
	mov.b32 	%r90, %f200;
	shfl.sync.bfly.b32 	%r92|%p23, %r90, %r45, %r47, %r49;
	mov.b32 	%f201, %r92;
	add.f32 	%f202, %f200, %f201;
	mov.b32 	%r93, %f202;
	shfl.sync.bfly.b32 	%r95|%p24, %r93, %r60, %r47, %r49;
	mov.b32 	%f203, %r95;
	add.f32 	%f204, %f202, %f203;
	st.local.f32 	[%rd2+4], %f204;
	st.shared.f32 	[%r10], %f204;
	bar.sync 	0;
	@%p1 bra 	$L__BB46_13;

	ld.shared.f32 	%f205, [%r4];
	mov.b32 	%r96, %f205;
	mov.u32 	%r97, 31;
	mov.u32 	%r98, 16;
	mov.u32 	%r99, -1;
	shfl.sync.bfly.b32 	%r100|%p25, %r96, %r98, %r97, %r99;
	mov.b32 	%f206, %r100;
	add.f32 	%f207, %f205, %f206;
	mov.b32 	%r101, %f207;
	mov.u32 	%r102, 8;
	shfl.sync.bfly.b32 	%r103|%p26, %r101, %r102, %r97, %r99;
	mov.b32 	%f208, %r103;
	add.f32 	%f209, %f207, %f208;
	mov.b32 	%r104, %f209;
	mov.u32 	%r105, 4;
	shfl.sync.bfly.b32 	%r106|%p27, %r104, %r105, %r97, %r99;
	mov.b32 	%f210, %r106;
	add.f32 	%f211, %f209, %f210;
	mov.b32 	%r107, %f211;
	mov.u32 	%r108, 2;
	shfl.sync.bfly.b32 	%r109|%p28, %r107, %r108, %r97, %r99;
	mov.b32 	%f212, %r109;
	add.f32 	%f213, %f211, %f212;
	mov.b32 	%r110, %f213;
	mov.u32 	%r111, 1;
	shfl.sync.bfly.b32 	%r112|%p29, %r110, %r111, %r97, %r99;
	mov.b32 	%f214, %r112;
	add.f32 	%f215, %f213, %f214;
	st.local.f32 	[%rd2+4], %f215;

$L__BB46_13:
	bar.sync 	0;
	mov.b32 	%r113, %f340;
	mov.u32 	%r114, 31;
	mov.u32 	%r115, 16;
	mov.u32 	%r116, -1;
	shfl.sync.bfly.b32 	%r117|%p31, %r113, %r115, %r114, %r116;
	mov.b32 	%f216, %r117;
	add.f32 	%f217, %f340, %f216;
	mov.b32 	%r118, %f217;
	mov.u32 	%r119, 8;
	shfl.sync.bfly.b32 	%r120|%p32, %r118, %r119, %r114, %r116;
	mov.b32 	%f218, %r120;
	add.f32 	%f219, %f217, %f218;
	mov.b32 	%r121, %f219;
	mov.u32 	%r122, 4;
	shfl.sync.bfly.b32 	%r123|%p33, %r121, %r122, %r114, %r116;
	mov.b32 	%f220, %r123;
	add.f32 	%f221, %f219, %f220;
	mov.b32 	%r124, %f221;
	mov.u32 	%r125, 2;
	shfl.sync.bfly.b32 	%r126|%p34, %r124, %r125, %r114, %r116;
	mov.b32 	%f222, %r126;
	add.f32 	%f223, %f221, %f222;
	mov.b32 	%r127, %f223;
	mov.u32 	%r128, 1;
	shfl.sync.bfly.b32 	%r129|%p35, %r127, %r128, %r114, %r116;
	mov.b32 	%f224, %r129;
	add.f32 	%f225, %f223, %f224;
	st.local.f32 	[%rd2+8], %f225;
	st.shared.f32 	[%r10], %f225;
	bar.sync 	0;
	@%p1 bra 	$L__BB46_15;

	ld.shared.f32 	%f226, [%r4];
	mov.b32 	%r130, %f226;
	shfl.sync.bfly.b32 	%r134|%p36, %r130, %r115, %r114, %r116;
	mov.b32 	%f227, %r134;
	add.f32 	%f228, %f226, %f227;
	mov.b32 	%r135, %f228;
	shfl.sync.bfly.b32 	%r137|%p37, %r135, %r119, %r114, %r116;
	mov.b32 	%f229, %r137;
	add.f32 	%f230, %f228, %f229;
	mov.b32 	%r138, %f230;
	shfl.sync.bfly.b32 	%r140|%p38, %r138, %r122, %r114, %r116;
	mov.b32 	%f231, %r140;
	add.f32 	%f232, %f230, %f231;
	mov.b32 	%r141, %f232;
	shfl.sync.bfly.b32 	%r143|%p39, %r141, %r125, %r114, %r116;
	mov.b32 	%f233, %r143;
	add.f32 	%f234, %f232, %f233;
	mov.b32 	%r144, %f234;
	shfl.sync.bfly.b32 	%r146|%p40, %r144, %r128, %r114, %r116;
	mov.b32 	%f235, %r146;
	add.f32 	%f236, %f234, %f235;
	st.local.f32 	[%rd2+8], %f236;

$L__BB46_15:
	bar.sync 	0;
	mov.b32 	%r147, %f339;
	shfl.sync.bfly.b32 	%r151|%p42, %r147, %r115, %r114, %r116;
	mov.b32 	%f237, %r151;
	add.f32 	%f238, %f339, %f237;
	mov.b32 	%r152, %f238;
	shfl.sync.bfly.b32 	%r154|%p43, %r152, %r119, %r114, %r116;
	mov.b32 	%f239, %r154;
	add.f32 	%f240, %f238, %f239;
	mov.b32 	%r155, %f240;
	shfl.sync.bfly.b32 	%r157|%p44, %r155, %r122, %r114, %r116;
	mov.b32 	%f241, %r157;
	add.f32 	%f242, %f240, %f241;
	mov.b32 	%r158, %f242;
	shfl.sync.bfly.b32 	%r160|%p45, %r158, %r125, %r114, %r116;
	mov.b32 	%f243, %r160;
	add.f32 	%f244, %f242, %f243;
	mov.b32 	%r161, %f244;
	shfl.sync.bfly.b32 	%r163|%p46, %r161, %r128, %r114, %r116;
	mov.b32 	%f245, %r163;
	add.f32 	%f246, %f244, %f245;
	st.local.f32 	[%rd2+12], %f246;
	st.shared.f32 	[%r10], %f246;
	bar.sync 	0;
	@%p1 bra 	$L__BB46_17;

	ld.shared.f32 	%f247, [%r4];
	mov.b32 	%r164, %f247;
	mov.u32 	%r165, 31;
	mov.u32 	%r166, 16;
	mov.u32 	%r167, -1;
	shfl.sync.bfly.b32 	%r168|%p47, %r164, %r166, %r165, %r167;
	mov.b32 	%f248, %r168;
	add.f32 	%f249, %f247, %f248;
	mov.b32 	%r169, %f249;
	mov.u32 	%r170, 8;
	shfl.sync.bfly.b32 	%r171|%p48, %r169, %r170, %r165, %r167;
	mov.b32 	%f250, %r171;
	add.f32 	%f251, %f249, %f250;
	mov.b32 	%r172, %f251;
	mov.u32 	%r173, 4;
	shfl.sync.bfly.b32 	%r174|%p49, %r172, %r173, %r165, %r167;
	mov.b32 	%f252, %r174;
	add.f32 	%f253, %f251, %f252;
	mov.b32 	%r175, %f253;
	mov.u32 	%r176, 2;
	shfl.sync.bfly.b32 	%r177|%p50, %r175, %r176, %r165, %r167;
	mov.b32 	%f254, %r177;
	add.f32 	%f255, %f253, %f254;
	mov.b32 	%r178, %f255;
	mov.u32 	%r179, 1;
	shfl.sync.bfly.b32 	%r180|%p51, %r178, %r179, %r165, %r167;
	mov.b32 	%f256, %r180;
	add.f32 	%f257, %f255, %f256;
	st.local.f32 	[%rd2+12], %f257;

$L__BB46_17:
	bar.sync 	0;
	mov.b32 	%r181, %f338;
	mov.u32 	%r182, 31;
	mov.u32 	%r183, 16;
	mov.u32 	%r184, -1;
	shfl.sync.bfly.b32 	%r185|%p53, %r181, %r183, %r182, %r184;
	mov.b32 	%f258, %r185;
	add.f32 	%f259, %f338, %f258;
	mov.b32 	%r186, %f259;
	mov.u32 	%r187, 8;
	shfl.sync.bfly.b32 	%r188|%p54, %r186, %r187, %r182, %r184;
	mov.b32 	%f260, %r188;
	add.f32 	%f261, %f259, %f260;
	mov.b32 	%r189, %f261;
	mov.u32 	%r190, 4;
	shfl.sync.bfly.b32 	%r191|%p55, %r189, %r190, %r182, %r184;
	mov.b32 	%f262, %r191;
	add.f32 	%f263, %f261, %f262;
	mov.b32 	%r192, %f263;
	mov.u32 	%r193, 2;
	shfl.sync.bfly.b32 	%r194|%p56, %r192, %r193, %r182, %r184;
	mov.b32 	%f264, %r194;
	add.f32 	%f265, %f263, %f264;
	mov.b32 	%r195, %f265;
	mov.u32 	%r196, 1;
	shfl.sync.bfly.b32 	%r197|%p57, %r195, %r196, %r182, %r184;
	mov.b32 	%f266, %r197;
	add.f32 	%f267, %f265, %f266;
	st.local.f32 	[%rd2+16], %f267;
	st.shared.f32 	[%r10], %f267;
	bar.sync 	0;
	@%p1 bra 	$L__BB46_19;

	ld.shared.f32 	%f268, [%r4];
	mov.b32 	%r198, %f268;
	shfl.sync.bfly.b32 	%r202|%p58, %r198, %r183, %r182, %r184;
	mov.b32 	%f269, %r202;
	add.f32 	%f270, %f268, %f269;
	mov.b32 	%r203, %f270;
	shfl.sync.bfly.b32 	%r205|%p59, %r203, %r187, %r182, %r184;
	mov.b32 	%f271, %r205;
	add.f32 	%f272, %f270, %f271;
	mov.b32 	%r206, %f272;
	shfl.sync.bfly.b32 	%r208|%p60, %r206, %r190, %r182, %r184;
	mov.b32 	%f273, %r208;
	add.f32 	%f274, %f272, %f273;
	mov.b32 	%r209, %f274;
	shfl.sync.bfly.b32 	%r211|%p61, %r209, %r193, %r182, %r184;
	mov.b32 	%f275, %r211;
	add.f32 	%f276, %f274, %f275;
	mov.b32 	%r212, %f276;
	shfl.sync.bfly.b32 	%r214|%p62, %r212, %r196, %r182, %r184;
	mov.b32 	%f277, %r214;
	add.f32 	%f278, %f276, %f277;
	st.local.f32 	[%rd2+16], %f278;

$L__BB46_19:
	bar.sync 	0;
	mov.b32 	%r215, %f337;
	shfl.sync.bfly.b32 	%r219|%p64, %r215, %r183, %r182, %r184;
	mov.b32 	%f279, %r219;
	add.f32 	%f280, %f337, %f279;
	mov.b32 	%r220, %f280;
	shfl.sync.bfly.b32 	%r222|%p65, %r220, %r187, %r182, %r184;
	mov.b32 	%f281, %r222;
	add.f32 	%f282, %f280, %f281;
	mov.b32 	%r223, %f282;
	shfl.sync.bfly.b32 	%r225|%p66, %r223, %r190, %r182, %r184;
	mov.b32 	%f283, %r225;
	add.f32 	%f284, %f282, %f283;
	mov.b32 	%r226, %f284;
	shfl.sync.bfly.b32 	%r228|%p67, %r226, %r193, %r182, %r184;
	mov.b32 	%f285, %r228;
	add.f32 	%f286, %f284, %f285;
	mov.b32 	%r229, %f286;
	shfl.sync.bfly.b32 	%r231|%p68, %r229, %r196, %r182, %r184;
	mov.b32 	%f287, %r231;
	add.f32 	%f288, %f286, %f287;
	st.local.f32 	[%rd2+20], %f288;
	st.shared.f32 	[%r10], %f288;
	bar.sync 	0;
	@%p1 bra 	$L__BB46_21;

	ld.shared.f32 	%f289, [%r4];
	mov.b32 	%r232, %f289;
	mov.u32 	%r233, 31;
	mov.u32 	%r234, 16;
	mov.u32 	%r235, -1;
	shfl.sync.bfly.b32 	%r236|%p69, %r232, %r234, %r233, %r235;
	mov.b32 	%f290, %r236;
	add.f32 	%f291, %f289, %f290;
	mov.b32 	%r237, %f291;
	mov.u32 	%r238, 8;
	shfl.sync.bfly.b32 	%r239|%p70, %r237, %r238, %r233, %r235;
	mov.b32 	%f292, %r239;
	add.f32 	%f293, %f291, %f292;
	mov.b32 	%r240, %f293;
	mov.u32 	%r241, 4;
	shfl.sync.bfly.b32 	%r242|%p71, %r240, %r241, %r233, %r235;
	mov.b32 	%f294, %r242;
	add.f32 	%f295, %f293, %f294;
	mov.b32 	%r243, %f295;
	mov.u32 	%r244, 2;
	shfl.sync.bfly.b32 	%r245|%p72, %r243, %r244, %r233, %r235;
	mov.b32 	%f296, %r245;
	add.f32 	%f297, %f295, %f296;
	mov.b32 	%r246, %f297;
	mov.u32 	%r247, 1;
	shfl.sync.bfly.b32 	%r248|%p73, %r246, %r247, %r233, %r235;
	mov.b32 	%f298, %r248;
	add.f32 	%f299, %f297, %f298;
	st.local.f32 	[%rd2+20], %f299;

$L__BB46_21:
	bar.sync 	0;
	mov.b32 	%r249, %f336;
	mov.u32 	%r250, 31;
	mov.u32 	%r251, 16;
	mov.u32 	%r252, -1;
	shfl.sync.bfly.b32 	%r253|%p75, %r249, %r251, %r250, %r252;
	mov.b32 	%f300, %r253;
	add.f32 	%f301, %f336, %f300;
	mov.b32 	%r254, %f301;
	mov.u32 	%r255, 8;
	shfl.sync.bfly.b32 	%r256|%p76, %r254, %r255, %r250, %r252;
	mov.b32 	%f302, %r256;
	add.f32 	%f303, %f301, %f302;
	mov.b32 	%r257, %f303;
	mov.u32 	%r258, 4;
	shfl.sync.bfly.b32 	%r259|%p77, %r257, %r258, %r250, %r252;
	mov.b32 	%f304, %r259;
	add.f32 	%f305, %f303, %f304;
	mov.b32 	%r260, %f305;
	mov.u32 	%r261, 2;
	shfl.sync.bfly.b32 	%r262|%p78, %r260, %r261, %r250, %r252;
	mov.b32 	%f306, %r262;
	add.f32 	%f307, %f305, %f306;
	mov.b32 	%r263, %f307;
	mov.u32 	%r264, 1;
	shfl.sync.bfly.b32 	%r265|%p79, %r263, %r264, %r250, %r252;
	mov.b32 	%f308, %r265;
	add.f32 	%f309, %f307, %f308;
	st.local.f32 	[%rd2+24], %f309;
	st.shared.f32 	[%r10], %f309;
	bar.sync 	0;
	@%p1 bra 	$L__BB46_23;

	ld.shared.f32 	%f310, [%r4];
	mov.b32 	%r266, %f310;
	shfl.sync.bfly.b32 	%r270|%p80, %r266, %r251, %r250, %r252;
	mov.b32 	%f311, %r270;
	add.f32 	%f312, %f310, %f311;
	mov.b32 	%r271, %f312;
	shfl.sync.bfly.b32 	%r273|%p81, %r271, %r255, %r250, %r252;
	mov.b32 	%f313, %r273;
	add.f32 	%f314, %f312, %f313;
	mov.b32 	%r274, %f314;
	shfl.sync.bfly.b32 	%r276|%p82, %r274, %r258, %r250, %r252;
	mov.b32 	%f315, %r276;
	add.f32 	%f316, %f314, %f315;
	mov.b32 	%r277, %f316;
	shfl.sync.bfly.b32 	%r279|%p83, %r277, %r261, %r250, %r252;
	mov.b32 	%f317, %r279;
	add.f32 	%f318, %f316, %f317;
	mov.b32 	%r280, %f318;
	shfl.sync.bfly.b32 	%r282|%p84, %r280, %r264, %r250, %r252;
	mov.b32 	%f319, %r282;
	add.f32 	%f320, %f318, %f319;
	st.local.f32 	[%rd2+24], %f320;

$L__BB46_23:
	bar.sync 	0;
	setp.gt.s32 	%p85, %r3, 6;
	@%p85 bra 	$L__BB46_25;

	mul.wide.s32 	%rd64, %r3, 4;
	add.s64 	%rd65, %rd2, %rd64;
	ld.local.f32 	%f321, [%rd65];
	mad.lo.s32 	%r283, %r3, %r13, %r2;
	cvt.s64.s32 	%rd66, %r283;
	mul.lo.s32 	%r284, %r1, %r14;
	cvt.s64.s32 	%rd67, %r284;
	add.s64 	%rd68, %rd67, %rd66;
	cvta.to.global.u64 	%rd69, %rd20;
	shl.b64 	%rd70, %rd68, 2;
	add.s64 	%rd71, %rd69, %rd70;
	st.global.f32 	[%rd71], %f321;

$L__BB46_25:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_8_bs_192
.visible .entry ggml_matvec_f32_ncols_8_bs_192(
	.param .u64 ggml_matvec_f32_ncols_8_bs_192_param_0,
	.param .u64 ggml_matvec_f32_ncols_8_bs_192_param_1,
	.param .u64 ggml_matvec_f32_ncols_8_bs_192_param_2,
	.param .u32 ggml_matvec_f32_ncols_8_bs_192_param_3,
	.param .u32 ggml_matvec_f32_ncols_8_bs_192_param_4,
	.param .u32 ggml_matvec_f32_ncols_8_bs_192_param_5,
	.param .u32 ggml_matvec_f32_ncols_8_bs_192_param_6,
	.param .u32 ggml_matvec_f32_ncols_8_bs_192_param_7,
	.param .u32 ggml_matvec_f32_ncols_8_bs_192_param_8,
	.param .u32 ggml_matvec_f32_ncols_8_bs_192_param_9,
	.param .u32 ggml_matvec_f32_ncols_8_bs_192_param_10,
	.param .u32 ggml_matvec_f32_ncols_8_bs_192_param_11
)
{
	.local .align 16 .b8 	__local_depot47[32];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<97>;
	.reg .f32 	%f<390>;
	.reg .b32 	%r<321>;
	.reg .b64 	%rd<78>;


	mov.u64 	%SPL, __local_depot47;
	ld.param.u64 	%rd22, [ggml_matvec_f32_ncols_8_bs_192_param_0];
	ld.param.u64 	%rd23, [ggml_matvec_f32_ncols_8_bs_192_param_1];
	ld.param.u64 	%rd21, [ggml_matvec_f32_ncols_8_bs_192_param_2];
	ld.param.u32 	%r11, [ggml_matvec_f32_ncols_8_bs_192_param_3];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_8_bs_192_param_5];
	ld.param.u32 	%r12, [ggml_matvec_f32_ncols_8_bs_192_param_6];
	ld.param.u32 	%r13, [ggml_matvec_f32_ncols_8_bs_192_param_7];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_8_bs_192_param_8];
	ld.param.u32 	%r17, [ggml_matvec_f32_ncols_8_bs_192_param_9];
	ld.param.u32 	%r18, [ggml_matvec_f32_ncols_8_bs_192_param_10];
	ld.param.u32 	%r14, [ggml_matvec_f32_ncols_8_bs_192_param_11];
	cvta.to.global.u64 	%rd77, %rd23;
	add.u64 	%rd2, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r19, %r1, %r16;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r20, %r2, %r15;
	mad.lo.s32 	%r21, %r19, %r17, %r20;
	cvt.s64.s32 	%rd3, %r21;
	cvta.to.global.u64 	%rd4, %rd22;
	mul.lo.s32 	%r22, %r1, %r18;
	cvt.s64.s32 	%rd5, %r22;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r23, %r3, 2;
	mov.u32 	%r24, data_mmv;
	add.s32 	%r4, %r24, %r23;
	@%p1 bra 	$L__BB47_2;

	mov.u32 	%r25, 0;
	st.shared.u32 	[%r4], %r25;

$L__BB47_2:
	bar.sync 	0;
	mov.f32 	%f382, 0f00000000;
	st.local.v4.f32 	[%rd2], {%f382, %f382, %f382, %f382};
	st.local.v4.f32 	[%rd2+16], {%f382, %f382, %f382, %f382};
	setp.ge.s32 	%p2, %r3, %r11;
	mov.f32 	%f383, %f382;
	mov.f32 	%f384, %f382;
	mov.f32 	%f385, %f382;
	mov.f32 	%f386, %f382;
	mov.f32 	%f387, %f382;
	mov.f32 	%f388, %f382;
	mov.f32 	%f389, %f382;
	@%p2 bra 	$L__BB47_9;

	not.b32 	%r26, %r3;
	add.s32 	%r5, %r26, %r11;
	mul.wide.u32 	%rd25, %r5, -1431655765;
	shr.u64 	%rd26, %rd25, 39;
	and.b64  	%rd27, %rd26, 1;
	setp.eq.b64 	%p3, %rd27, 1;
	mov.pred 	%p4, 0;
	xor.pred  	%p5, %p3, %p4;
	mov.f32 	%f382, 0f00000000;
	mov.u32 	%r320, %r3;
	@%p5 bra 	$L__BB47_5;

	shl.b64 	%rd28, %rd5, 2;
	add.s64 	%rd29, %rd77, %rd28;
	shl.b64 	%rd30, %rd3, 2;
	add.s64 	%rd31, %rd4, %rd30;
	mul.wide.s32 	%rd32, %r3, 8;
	add.s64 	%rd33, %rd31, %rd32;
	ld.global.nc.v2.f32 	{%f57, %f58}, [%rd33];
	add.s64 	%rd34, %rd29, %rd32;
	ld.global.nc.v2.f32 	{%f61, %f62}, [%rd34];
	fma.rn.f32 	%f65, %f57, %f61, 0f00000000;
	fma.rn.f32 	%f389, %f58, %f62, %f65;
	mul.wide.s32 	%rd35, %r12, 8;
	add.s64 	%rd36, %rd34, %rd35;
	ld.global.nc.v2.f32 	{%f66, %f67}, [%rd36];
	fma.rn.f32 	%f70, %f57, %f66, 0f00000000;
	fma.rn.f32 	%f388, %f58, %f67, %f70;
	add.s32 	%r27, %r3, %r12;
	add.s32 	%r28, %r27, %r12;
	mul.wide.s32 	%rd37, %r28, 8;
	add.s64 	%rd38, %rd29, %rd37;
	ld.global.nc.v2.f32 	{%f71, %f72}, [%rd38];
	fma.rn.f32 	%f75, %f57, %f71, 0f00000000;
	fma.rn.f32 	%f387, %f58, %f72, %f75;
	add.s64 	%rd39, %rd38, %rd35;
	ld.global.nc.v2.f32 	{%f76, %f77}, [%rd39];
	fma.rn.f32 	%f80, %f57, %f76, 0f00000000;
	fma.rn.f32 	%f386, %f58, %f77, %f80;
	st.local.v4.f32 	[%rd2], {%f389, %f388, %f387, %f386};
	add.s64 	%rd40, %rd39, %rd35;
	ld.global.nc.v2.f32 	{%f81, %f82}, [%rd40];
	fma.rn.f32 	%f85, %f57, %f81, 0f00000000;
	fma.rn.f32 	%f385, %f58, %f82, %f85;
	add.s64 	%rd41, %rd40, %rd35;
	ld.global.nc.v2.f32 	{%f86, %f87}, [%rd41];
	fma.rn.f32 	%f90, %f57, %f86, 0f00000000;
	fma.rn.f32 	%f384, %f58, %f87, %f90;
	add.s64 	%rd42, %rd41, %rd35;
	ld.global.nc.v2.f32 	{%f91, %f92}, [%rd42];
	fma.rn.f32 	%f95, %f57, %f91, 0f00000000;
	fma.rn.f32 	%f383, %f58, %f92, %f95;
	add.s64 	%rd43, %rd42, %rd35;
	ld.global.nc.v2.f32 	{%f96, %f97}, [%rd43];
	fma.rn.f32 	%f100, %f57, %f96, 0f00000000;
	fma.rn.f32 	%f382, %f58, %f97, %f100;
	st.local.v4.f32 	[%rd2+16], {%f385, %f384, %f383, %f382};
	add.s32 	%r320, %r3, 192;

$L__BB47_5:
	setp.lt.u32 	%p6, %r5, 192;
	@%p6 bra 	$L__BB47_9;

	add.s32 	%r29, %r320, %r12;
	add.s32 	%r30, %r29, 192;
	mul.wide.s32 	%rd44, %r30, 8;
	shl.b64 	%rd45, %rd5, 2;
	add.s64 	%rd7, %rd44, %rd45;
	shl.b32 	%r31, %r12, 1;
	add.s32 	%r32, %r320, %r31;
	mad.lo.s32 	%r33, %r12, 3, %r320;
	shl.b32 	%r34, %r12, 2;
	add.s32 	%r35, %r320, %r34;
	mad.lo.s32 	%r36, %r12, 5, %r320;
	mad.lo.s32 	%r37, %r12, 6, %r320;
	mad.lo.s32 	%r38, %r12, 7, %r320;
	mul.wide.s32 	%rd46, %r32, 8;
	add.s64 	%rd8, %rd46, %rd45;
	mul.wide.s32 	%rd47, %r33, 8;
	add.s64 	%rd9, %rd47, %rd45;
	mul.wide.s32 	%rd48, %r35, 8;
	add.s64 	%rd10, %rd48, %rd45;
	mul.wide.s32 	%rd49, %r36, 8;
	add.s64 	%rd11, %rd49, %rd45;
	mul.wide.s32 	%rd50, %r37, 8;
	add.s64 	%rd12, %rd50, %rd45;
	mul.wide.s32 	%rd51, %r38, 8;
	add.s64 	%rd13, %rd51, %rd45;
	mul.wide.s32 	%rd52, %r320, 2;
	add.s64 	%rd53, %rd52, %rd3;
	shl.b64 	%rd54, %rd53, 2;
	add.s64 	%rd55, %rd4, %rd54;
	add.s64 	%rd76, %rd55, 1536;
	mul.wide.s32 	%rd56, %r320, 8;
	mul.wide.s32 	%rd57, %r12, 8;
	add.s64 	%rd58, %rd56, %rd57;
	add.s64 	%rd15, %rd58, %rd45;
	add.s64 	%rd16, %rd56, %rd45;

$L__BB47_7:
	ld.global.nc.v2.f32 	{%f101, %f102}, [%rd76+-1536];
	add.s64 	%rd59, %rd77, %rd16;
	ld.global.nc.v2.f32 	{%f105, %f106}, [%rd59];
	fma.rn.f32 	%f109, %f101, %f105, %f389;
	fma.rn.f32 	%f110, %f102, %f106, %f109;
	add.s64 	%rd60, %rd77, %rd15;
	ld.global.nc.v2.f32 	{%f111, %f112}, [%rd60];
	fma.rn.f32 	%f115, %f101, %f111, %f388;
	fma.rn.f32 	%f116, %f102, %f112, %f115;
	add.s64 	%rd61, %rd77, %rd8;
	ld.global.nc.v2.f32 	{%f117, %f118}, [%rd61];
	fma.rn.f32 	%f121, %f101, %f117, %f387;
	fma.rn.f32 	%f122, %f102, %f118, %f121;
	add.s64 	%rd62, %rd77, %rd9;
	ld.global.nc.v2.f32 	{%f123, %f124}, [%rd62];
	fma.rn.f32 	%f127, %f101, %f123, %f386;
	fma.rn.f32 	%f128, %f102, %f124, %f127;
	add.s64 	%rd63, %rd77, %rd10;
	ld.global.nc.v2.f32 	{%f129, %f130}, [%rd63];
	fma.rn.f32 	%f133, %f101, %f129, %f385;
	fma.rn.f32 	%f134, %f102, %f130, %f133;
	add.s64 	%rd64, %rd77, %rd11;
	ld.global.nc.v2.f32 	{%f135, %f136}, [%rd64];
	fma.rn.f32 	%f139, %f101, %f135, %f384;
	fma.rn.f32 	%f140, %f102, %f136, %f139;
	add.s64 	%rd65, %rd77, %rd12;
	ld.global.nc.v2.f32 	{%f141, %f142}, [%rd65];
	fma.rn.f32 	%f145, %f101, %f141, %f383;
	fma.rn.f32 	%f146, %f102, %f142, %f145;
	add.s64 	%rd66, %rd77, %rd13;
	ld.global.nc.v2.f32 	{%f147, %f148}, [%rd66];
	fma.rn.f32 	%f151, %f101, %f147, %f382;
	fma.rn.f32 	%f152, %f102, %f148, %f151;
	ld.global.nc.v2.f32 	{%f153, %f154}, [%rd76];
	ld.global.nc.v2.f32 	{%f157, %f158}, [%rd59+1536];
	fma.rn.f32 	%f161, %f153, %f157, %f110;
	fma.rn.f32 	%f389, %f154, %f158, %f161;
	add.s64 	%rd67, %rd77, %rd7;
	ld.global.nc.v2.f32 	{%f162, %f163}, [%rd67];
	fma.rn.f32 	%f166, %f153, %f162, %f116;
	fma.rn.f32 	%f388, %f154, %f163, %f166;
	ld.global.nc.v2.f32 	{%f167, %f168}, [%rd61+1536];
	fma.rn.f32 	%f171, %f153, %f167, %f122;
	fma.rn.f32 	%f387, %f154, %f168, %f171;
	ld.global.nc.v2.f32 	{%f172, %f173}, [%rd62+1536];
	fma.rn.f32 	%f176, %f153, %f172, %f128;
	fma.rn.f32 	%f386, %f154, %f173, %f176;
	ld.global.nc.v2.f32 	{%f177, %f178}, [%rd63+1536];
	fma.rn.f32 	%f181, %f153, %f177, %f134;
	fma.rn.f32 	%f385, %f154, %f178, %f181;
	ld.global.nc.v2.f32 	{%f182, %f183}, [%rd64+1536];
	fma.rn.f32 	%f186, %f153, %f182, %f140;
	fma.rn.f32 	%f384, %f154, %f183, %f186;
	ld.global.nc.v2.f32 	{%f187, %f188}, [%rd65+1536];
	fma.rn.f32 	%f191, %f153, %f187, %f146;
	fma.rn.f32 	%f383, %f154, %f188, %f191;
	ld.global.nc.v2.f32 	{%f192, %f193}, [%rd66+1536];
	fma.rn.f32 	%f196, %f153, %f192, %f152;
	fma.rn.f32 	%f382, %f154, %f193, %f196;
	add.s64 	%rd77, %rd77, 3072;
	add.s64 	%rd76, %rd76, 3072;
	add.s32 	%r320, %r320, 384;
	setp.lt.s32 	%p7, %r320, %r11;
	@%p7 bra 	$L__BB47_7;

	st.local.v4.f32 	[%rd2], {%f389, %f388, %f387, %f386};
	st.local.v4.f32 	[%rd2+16], {%f385, %f384, %f383, %f382};

$L__BB47_9:
	shr.s32 	%r39, %r3, 31;
	shr.u32 	%r40, %r39, 27;
	add.s32 	%r41, %r3, %r40;
	shr.s32 	%r42, %r41, 5;
	shl.b32 	%r43, %r42, 2;
	add.s32 	%r10, %r24, %r43;
	mov.u32 	%r45, 2;
	mov.b32 	%r46, %f389;
	mov.u32 	%r47, 31;
	mov.u32 	%r48, 16;
	mov.u32 	%r49, -1;
	shfl.sync.bfly.b32 	%r50|%p8, %r46, %r48, %r47, %r49;
	mov.b32 	%f197, %r50;
	add.f32 	%f198, %f389, %f197;
	mov.b32 	%r51, %f198;
	mov.u32 	%r52, 8;
	shfl.sync.bfly.b32 	%r53|%p9, %r51, %r52, %r47, %r49;
	mov.b32 	%f199, %r53;
	add.f32 	%f200, %f198, %f199;
	mov.b32 	%r54, %f200;
	mov.u32 	%r55, 4;
	shfl.sync.bfly.b32 	%r56|%p10, %r54, %r55, %r47, %r49;
	mov.b32 	%f201, %r56;
	add.f32 	%f202, %f200, %f201;
	mov.b32 	%r57, %f202;
	shfl.sync.bfly.b32 	%r58|%p11, %r57, %r45, %r47, %r49;
	mov.b32 	%f203, %r58;
	add.f32 	%f204, %f202, %f203;
	mov.b32 	%r59, %f204;
	mov.u32 	%r60, 1;
	shfl.sync.bfly.b32 	%r61|%p12, %r59, %r60, %r47, %r49;
	mov.b32 	%f205, %r61;
	add.f32 	%f206, %f204, %f205;
	st.local.f32 	[%rd2], %f206;
	st.shared.f32 	[%r10], %f206;
	bar.sync 	0;
	@%p1 bra 	$L__BB47_11;

	ld.shared.f32 	%f207, [%r4];
	mov.b32 	%r62, %f207;
	shfl.sync.bfly.b32 	%r66|%p14, %r62, %r48, %r47, %r49;
	mov.b32 	%f208, %r66;
	add.f32 	%f209, %f207, %f208;
	mov.b32 	%r67, %f209;
	shfl.sync.bfly.b32 	%r69|%p15, %r67, %r52, %r47, %r49;
	mov.b32 	%f210, %r69;
	add.f32 	%f211, %f209, %f210;
	mov.b32 	%r70, %f211;
	shfl.sync.bfly.b32 	%r72|%p16, %r70, %r55, %r47, %r49;
	mov.b32 	%f212, %r72;
	add.f32 	%f213, %f211, %f212;
	mov.b32 	%r73, %f213;
	shfl.sync.bfly.b32 	%r75|%p17, %r73, %r45, %r47, %r49;
	mov.b32 	%f214, %r75;
	add.f32 	%f215, %f213, %f214;
	mov.b32 	%r76, %f215;
	shfl.sync.bfly.b32 	%r78|%p18, %r76, %r60, %r47, %r49;
	mov.b32 	%f216, %r78;
	add.f32 	%f217, %f215, %f216;
	st.local.f32 	[%rd2], %f217;

$L__BB47_11:
	bar.sync 	0;
	mov.b32 	%r79, %f388;
	shfl.sync.bfly.b32 	%r83|%p20, %r79, %r48, %r47, %r49;
	mov.b32 	%f218, %r83;
	add.f32 	%f219, %f388, %f218;
	mov.b32 	%r84, %f219;
	shfl.sync.bfly.b32 	%r86|%p21, %r84, %r52, %r47, %r49;
	mov.b32 	%f220, %r86;
	add.f32 	%f221, %f219, %f220;
	mov.b32 	%r87, %f221;
	shfl.sync.bfly.b32 	%r89|%p22, %r87, %r55, %r47, %r49;
	mov.b32 	%f222, %r89;
	add.f32 	%f223, %f221, %f222;
	mov.b32 	%r90, %f223;
	shfl.sync.bfly.b32 	%r92|%p23, %r90, %r45, %r47, %r49;
	mov.b32 	%f224, %r92;
	add.f32 	%f225, %f223, %f224;
	mov.b32 	%r93, %f225;
	shfl.sync.bfly.b32 	%r95|%p24, %r93, %r60, %r47, %r49;
	mov.b32 	%f226, %r95;
	add.f32 	%f227, %f225, %f226;
	st.local.f32 	[%rd2+4], %f227;
	st.shared.f32 	[%r10], %f227;
	bar.sync 	0;
	@%p1 bra 	$L__BB47_13;

	ld.shared.f32 	%f228, [%r4];
	mov.b32 	%r96, %f228;
	mov.u32 	%r97, 31;
	mov.u32 	%r98, 16;
	mov.u32 	%r99, -1;
	shfl.sync.bfly.b32 	%r100|%p25, %r96, %r98, %r97, %r99;
	mov.b32 	%f229, %r100;
	add.f32 	%f230, %f228, %f229;
	mov.b32 	%r101, %f230;
	mov.u32 	%r102, 8;
	shfl.sync.bfly.b32 	%r103|%p26, %r101, %r102, %r97, %r99;
	mov.b32 	%f231, %r103;
	add.f32 	%f232, %f230, %f231;
	mov.b32 	%r104, %f232;
	mov.u32 	%r105, 4;
	shfl.sync.bfly.b32 	%r106|%p27, %r104, %r105, %r97, %r99;
	mov.b32 	%f233, %r106;
	add.f32 	%f234, %f232, %f233;
	mov.b32 	%r107, %f234;
	mov.u32 	%r108, 2;
	shfl.sync.bfly.b32 	%r109|%p28, %r107, %r108, %r97, %r99;
	mov.b32 	%f235, %r109;
	add.f32 	%f236, %f234, %f235;
	mov.b32 	%r110, %f236;
	mov.u32 	%r111, 1;
	shfl.sync.bfly.b32 	%r112|%p29, %r110, %r111, %r97, %r99;
	mov.b32 	%f237, %r112;
	add.f32 	%f238, %f236, %f237;
	st.local.f32 	[%rd2+4], %f238;

$L__BB47_13:
	bar.sync 	0;
	mov.b32 	%r113, %f387;
	mov.u32 	%r114, 31;
	mov.u32 	%r115, 16;
	mov.u32 	%r116, -1;
	shfl.sync.bfly.b32 	%r117|%p31, %r113, %r115, %r114, %r116;
	mov.b32 	%f239, %r117;
	add.f32 	%f240, %f387, %f239;
	mov.b32 	%r118, %f240;
	mov.u32 	%r119, 8;
	shfl.sync.bfly.b32 	%r120|%p32, %r118, %r119, %r114, %r116;
	mov.b32 	%f241, %r120;
	add.f32 	%f242, %f240, %f241;
	mov.b32 	%r121, %f242;
	mov.u32 	%r122, 4;
	shfl.sync.bfly.b32 	%r123|%p33, %r121, %r122, %r114, %r116;
	mov.b32 	%f243, %r123;
	add.f32 	%f244, %f242, %f243;
	mov.b32 	%r124, %f244;
	mov.u32 	%r125, 2;
	shfl.sync.bfly.b32 	%r126|%p34, %r124, %r125, %r114, %r116;
	mov.b32 	%f245, %r126;
	add.f32 	%f246, %f244, %f245;
	mov.b32 	%r127, %f246;
	mov.u32 	%r128, 1;
	shfl.sync.bfly.b32 	%r129|%p35, %r127, %r128, %r114, %r116;
	mov.b32 	%f247, %r129;
	add.f32 	%f248, %f246, %f247;
	st.local.f32 	[%rd2+8], %f248;
	st.shared.f32 	[%r10], %f248;
	bar.sync 	0;
	@%p1 bra 	$L__BB47_15;

	ld.shared.f32 	%f249, [%r4];
	mov.b32 	%r130, %f249;
	shfl.sync.bfly.b32 	%r134|%p36, %r130, %r115, %r114, %r116;
	mov.b32 	%f250, %r134;
	add.f32 	%f251, %f249, %f250;
	mov.b32 	%r135, %f251;
	shfl.sync.bfly.b32 	%r137|%p37, %r135, %r119, %r114, %r116;
	mov.b32 	%f252, %r137;
	add.f32 	%f253, %f251, %f252;
	mov.b32 	%r138, %f253;
	shfl.sync.bfly.b32 	%r140|%p38, %r138, %r122, %r114, %r116;
	mov.b32 	%f254, %r140;
	add.f32 	%f255, %f253, %f254;
	mov.b32 	%r141, %f255;
	shfl.sync.bfly.b32 	%r143|%p39, %r141, %r125, %r114, %r116;
	mov.b32 	%f256, %r143;
	add.f32 	%f257, %f255, %f256;
	mov.b32 	%r144, %f257;
	shfl.sync.bfly.b32 	%r146|%p40, %r144, %r128, %r114, %r116;
	mov.b32 	%f258, %r146;
	add.f32 	%f259, %f257, %f258;
	st.local.f32 	[%rd2+8], %f259;

$L__BB47_15:
	bar.sync 	0;
	mov.b32 	%r147, %f386;
	shfl.sync.bfly.b32 	%r151|%p42, %r147, %r115, %r114, %r116;
	mov.b32 	%f260, %r151;
	add.f32 	%f261, %f386, %f260;
	mov.b32 	%r152, %f261;
	shfl.sync.bfly.b32 	%r154|%p43, %r152, %r119, %r114, %r116;
	mov.b32 	%f262, %r154;
	add.f32 	%f263, %f261, %f262;
	mov.b32 	%r155, %f263;
	shfl.sync.bfly.b32 	%r157|%p44, %r155, %r122, %r114, %r116;
	mov.b32 	%f264, %r157;
	add.f32 	%f265, %f263, %f264;
	mov.b32 	%r158, %f265;
	shfl.sync.bfly.b32 	%r160|%p45, %r158, %r125, %r114, %r116;
	mov.b32 	%f266, %r160;
	add.f32 	%f267, %f265, %f266;
	mov.b32 	%r161, %f267;
	shfl.sync.bfly.b32 	%r163|%p46, %r161, %r128, %r114, %r116;
	mov.b32 	%f268, %r163;
	add.f32 	%f269, %f267, %f268;
	st.local.f32 	[%rd2+12], %f269;
	st.shared.f32 	[%r10], %f269;
	bar.sync 	0;
	@%p1 bra 	$L__BB47_17;

	ld.shared.f32 	%f270, [%r4];
	mov.b32 	%r164, %f270;
	mov.u32 	%r165, 31;
	mov.u32 	%r166, 16;
	mov.u32 	%r167, -1;
	shfl.sync.bfly.b32 	%r168|%p47, %r164, %r166, %r165, %r167;
	mov.b32 	%f271, %r168;
	add.f32 	%f272, %f270, %f271;
	mov.b32 	%r169, %f272;
	mov.u32 	%r170, 8;
	shfl.sync.bfly.b32 	%r171|%p48, %r169, %r170, %r165, %r167;
	mov.b32 	%f273, %r171;
	add.f32 	%f274, %f272, %f273;
	mov.b32 	%r172, %f274;
	mov.u32 	%r173, 4;
	shfl.sync.bfly.b32 	%r174|%p49, %r172, %r173, %r165, %r167;
	mov.b32 	%f275, %r174;
	add.f32 	%f276, %f274, %f275;
	mov.b32 	%r175, %f276;
	mov.u32 	%r176, 2;
	shfl.sync.bfly.b32 	%r177|%p50, %r175, %r176, %r165, %r167;
	mov.b32 	%f277, %r177;
	add.f32 	%f278, %f276, %f277;
	mov.b32 	%r178, %f278;
	mov.u32 	%r179, 1;
	shfl.sync.bfly.b32 	%r180|%p51, %r178, %r179, %r165, %r167;
	mov.b32 	%f279, %r180;
	add.f32 	%f280, %f278, %f279;
	st.local.f32 	[%rd2+12], %f280;

$L__BB47_17:
	bar.sync 	0;
	mov.b32 	%r181, %f385;
	mov.u32 	%r182, 31;
	mov.u32 	%r183, 16;
	mov.u32 	%r184, -1;
	shfl.sync.bfly.b32 	%r185|%p53, %r181, %r183, %r182, %r184;
	mov.b32 	%f281, %r185;
	add.f32 	%f282, %f385, %f281;
	mov.b32 	%r186, %f282;
	mov.u32 	%r187, 8;
	shfl.sync.bfly.b32 	%r188|%p54, %r186, %r187, %r182, %r184;
	mov.b32 	%f283, %r188;
	add.f32 	%f284, %f282, %f283;
	mov.b32 	%r189, %f284;
	mov.u32 	%r190, 4;
	shfl.sync.bfly.b32 	%r191|%p55, %r189, %r190, %r182, %r184;
	mov.b32 	%f285, %r191;
	add.f32 	%f286, %f284, %f285;
	mov.b32 	%r192, %f286;
	mov.u32 	%r193, 2;
	shfl.sync.bfly.b32 	%r194|%p56, %r192, %r193, %r182, %r184;
	mov.b32 	%f287, %r194;
	add.f32 	%f288, %f286, %f287;
	mov.b32 	%r195, %f288;
	mov.u32 	%r196, 1;
	shfl.sync.bfly.b32 	%r197|%p57, %r195, %r196, %r182, %r184;
	mov.b32 	%f289, %r197;
	add.f32 	%f290, %f288, %f289;
	st.local.f32 	[%rd2+16], %f290;
	st.shared.f32 	[%r10], %f290;
	bar.sync 	0;
	@%p1 bra 	$L__BB47_19;

	ld.shared.f32 	%f291, [%r4];
	mov.b32 	%r198, %f291;
	shfl.sync.bfly.b32 	%r202|%p58, %r198, %r183, %r182, %r184;
	mov.b32 	%f292, %r202;
	add.f32 	%f293, %f291, %f292;
	mov.b32 	%r203, %f293;
	shfl.sync.bfly.b32 	%r205|%p59, %r203, %r187, %r182, %r184;
	mov.b32 	%f294, %r205;
	add.f32 	%f295, %f293, %f294;
	mov.b32 	%r206, %f295;
	shfl.sync.bfly.b32 	%r208|%p60, %r206, %r190, %r182, %r184;
	mov.b32 	%f296, %r208;
	add.f32 	%f297, %f295, %f296;
	mov.b32 	%r209, %f297;
	shfl.sync.bfly.b32 	%r211|%p61, %r209, %r193, %r182, %r184;
	mov.b32 	%f298, %r211;
	add.f32 	%f299, %f297, %f298;
	mov.b32 	%r212, %f299;
	shfl.sync.bfly.b32 	%r214|%p62, %r212, %r196, %r182, %r184;
	mov.b32 	%f300, %r214;
	add.f32 	%f301, %f299, %f300;
	st.local.f32 	[%rd2+16], %f301;

$L__BB47_19:
	bar.sync 	0;
	mov.b32 	%r215, %f384;
	shfl.sync.bfly.b32 	%r219|%p64, %r215, %r183, %r182, %r184;
	mov.b32 	%f302, %r219;
	add.f32 	%f303, %f384, %f302;
	mov.b32 	%r220, %f303;
	shfl.sync.bfly.b32 	%r222|%p65, %r220, %r187, %r182, %r184;
	mov.b32 	%f304, %r222;
	add.f32 	%f305, %f303, %f304;
	mov.b32 	%r223, %f305;
	shfl.sync.bfly.b32 	%r225|%p66, %r223, %r190, %r182, %r184;
	mov.b32 	%f306, %r225;
	add.f32 	%f307, %f305, %f306;
	mov.b32 	%r226, %f307;
	shfl.sync.bfly.b32 	%r228|%p67, %r226, %r193, %r182, %r184;
	mov.b32 	%f308, %r228;
	add.f32 	%f309, %f307, %f308;
	mov.b32 	%r229, %f309;
	shfl.sync.bfly.b32 	%r231|%p68, %r229, %r196, %r182, %r184;
	mov.b32 	%f310, %r231;
	add.f32 	%f311, %f309, %f310;
	st.local.f32 	[%rd2+20], %f311;
	st.shared.f32 	[%r10], %f311;
	bar.sync 	0;
	@%p1 bra 	$L__BB47_21;

	ld.shared.f32 	%f312, [%r4];
	mov.b32 	%r232, %f312;
	mov.u32 	%r233, 31;
	mov.u32 	%r234, 16;
	mov.u32 	%r235, -1;
	shfl.sync.bfly.b32 	%r236|%p69, %r232, %r234, %r233, %r235;
	mov.b32 	%f313, %r236;
	add.f32 	%f314, %f312, %f313;
	mov.b32 	%r237, %f314;
	mov.u32 	%r238, 8;
	shfl.sync.bfly.b32 	%r239|%p70, %r237, %r238, %r233, %r235;
	mov.b32 	%f315, %r239;
	add.f32 	%f316, %f314, %f315;
	mov.b32 	%r240, %f316;
	mov.u32 	%r241, 4;
	shfl.sync.bfly.b32 	%r242|%p71, %r240, %r241, %r233, %r235;
	mov.b32 	%f317, %r242;
	add.f32 	%f318, %f316, %f317;
	mov.b32 	%r243, %f318;
	mov.u32 	%r244, 2;
	shfl.sync.bfly.b32 	%r245|%p72, %r243, %r244, %r233, %r235;
	mov.b32 	%f319, %r245;
	add.f32 	%f320, %f318, %f319;
	mov.b32 	%r246, %f320;
	mov.u32 	%r247, 1;
	shfl.sync.bfly.b32 	%r248|%p73, %r246, %r247, %r233, %r235;
	mov.b32 	%f321, %r248;
	add.f32 	%f322, %f320, %f321;
	st.local.f32 	[%rd2+20], %f322;

$L__BB47_21:
	bar.sync 	0;
	mov.b32 	%r249, %f383;
	mov.u32 	%r250, 31;
	mov.u32 	%r251, 16;
	mov.u32 	%r252, -1;
	shfl.sync.bfly.b32 	%r253|%p75, %r249, %r251, %r250, %r252;
	mov.b32 	%f323, %r253;
	add.f32 	%f324, %f383, %f323;
	mov.b32 	%r254, %f324;
	mov.u32 	%r255, 8;
	shfl.sync.bfly.b32 	%r256|%p76, %r254, %r255, %r250, %r252;
	mov.b32 	%f325, %r256;
	add.f32 	%f326, %f324, %f325;
	mov.b32 	%r257, %f326;
	mov.u32 	%r258, 4;
	shfl.sync.bfly.b32 	%r259|%p77, %r257, %r258, %r250, %r252;
	mov.b32 	%f327, %r259;
	add.f32 	%f328, %f326, %f327;
	mov.b32 	%r260, %f328;
	mov.u32 	%r261, 2;
	shfl.sync.bfly.b32 	%r262|%p78, %r260, %r261, %r250, %r252;
	mov.b32 	%f329, %r262;
	add.f32 	%f330, %f328, %f329;
	mov.b32 	%r263, %f330;
	mov.u32 	%r264, 1;
	shfl.sync.bfly.b32 	%r265|%p79, %r263, %r264, %r250, %r252;
	mov.b32 	%f331, %r265;
	add.f32 	%f332, %f330, %f331;
	st.local.f32 	[%rd2+24], %f332;
	st.shared.f32 	[%r10], %f332;
	bar.sync 	0;
	@%p1 bra 	$L__BB47_23;

	ld.shared.f32 	%f333, [%r4];
	mov.b32 	%r266, %f333;
	shfl.sync.bfly.b32 	%r270|%p80, %r266, %r251, %r250, %r252;
	mov.b32 	%f334, %r270;
	add.f32 	%f335, %f333, %f334;
	mov.b32 	%r271, %f335;
	shfl.sync.bfly.b32 	%r273|%p81, %r271, %r255, %r250, %r252;
	mov.b32 	%f336, %r273;
	add.f32 	%f337, %f335, %f336;
	mov.b32 	%r274, %f337;
	shfl.sync.bfly.b32 	%r276|%p82, %r274, %r258, %r250, %r252;
	mov.b32 	%f338, %r276;
	add.f32 	%f339, %f337, %f338;
	mov.b32 	%r277, %f339;
	shfl.sync.bfly.b32 	%r279|%p83, %r277, %r261, %r250, %r252;
	mov.b32 	%f340, %r279;
	add.f32 	%f341, %f339, %f340;
	mov.b32 	%r280, %f341;
	shfl.sync.bfly.b32 	%r282|%p84, %r280, %r264, %r250, %r252;
	mov.b32 	%f342, %r282;
	add.f32 	%f343, %f341, %f342;
	st.local.f32 	[%rd2+24], %f343;

$L__BB47_23:
	bar.sync 	0;
	mov.b32 	%r283, %f382;
	shfl.sync.bfly.b32 	%r287|%p86, %r283, %r251, %r250, %r252;
	mov.b32 	%f344, %r287;
	add.f32 	%f345, %f382, %f344;
	mov.b32 	%r288, %f345;
	shfl.sync.bfly.b32 	%r290|%p87, %r288, %r255, %r250, %r252;
	mov.b32 	%f346, %r290;
	add.f32 	%f347, %f345, %f346;
	mov.b32 	%r291, %f347;
	shfl.sync.bfly.b32 	%r293|%p88, %r291, %r258, %r250, %r252;
	mov.b32 	%f348, %r293;
	add.f32 	%f349, %f347, %f348;
	mov.b32 	%r294, %f349;
	shfl.sync.bfly.b32 	%r296|%p89, %r294, %r261, %r250, %r252;
	mov.b32 	%f350, %r296;
	add.f32 	%f351, %f349, %f350;
	mov.b32 	%r297, %f351;
	shfl.sync.bfly.b32 	%r299|%p90, %r297, %r264, %r250, %r252;
	mov.b32 	%f352, %r299;
	add.f32 	%f353, %f351, %f352;
	st.local.f32 	[%rd2+28], %f353;
	st.shared.f32 	[%r10], %f353;
	bar.sync 	0;
	@%p1 bra 	$L__BB47_25;

	ld.shared.f32 	%f354, [%r4];
	mov.b32 	%r300, %f354;
	mov.u32 	%r301, 31;
	mov.u32 	%r302, 16;
	mov.u32 	%r303, -1;
	shfl.sync.bfly.b32 	%r304|%p91, %r300, %r302, %r301, %r303;
	mov.b32 	%f355, %r304;
	add.f32 	%f356, %f354, %f355;
	mov.b32 	%r305, %f356;
	mov.u32 	%r306, 8;
	shfl.sync.bfly.b32 	%r307|%p92, %r305, %r306, %r301, %r303;
	mov.b32 	%f357, %r307;
	add.f32 	%f358, %f356, %f357;
	mov.b32 	%r308, %f358;
	mov.u32 	%r309, 4;
	shfl.sync.bfly.b32 	%r310|%p93, %r308, %r309, %r301, %r303;
	mov.b32 	%f359, %r310;
	add.f32 	%f360, %f358, %f359;
	mov.b32 	%r311, %f360;
	mov.u32 	%r312, 2;
	shfl.sync.bfly.b32 	%r313|%p94, %r311, %r312, %r301, %r303;
	mov.b32 	%f361, %r313;
	add.f32 	%f362, %f360, %f361;
	mov.b32 	%r314, %f362;
	mov.u32 	%r315, 1;
	shfl.sync.bfly.b32 	%r316|%p95, %r314, %r315, %r301, %r303;
	mov.b32 	%f363, %r316;
	add.f32 	%f364, %f362, %f363;
	st.local.f32 	[%rd2+28], %f364;

$L__BB47_25:
	bar.sync 	0;
	setp.gt.s32 	%p96, %r3, 7;
	@%p96 bra 	$L__BB47_27;

	mul.wide.s32 	%rd68, %r3, 4;
	add.s64 	%rd69, %rd2, %rd68;
	ld.local.f32 	%f365, [%rd69];
	mad.lo.s32 	%r317, %r3, %r13, %r2;
	cvt.s64.s32 	%rd70, %r317;
	mul.lo.s32 	%r318, %r1, %r14;
	cvt.s64.s32 	%rd71, %r318;
	add.s64 	%rd72, %rd71, %rd70;
	cvta.to.global.u64 	%rd73, %rd21;
	shl.b64 	%rd74, %rd72, 2;
	add.s64 	%rd75, %rd73, %rd74;
	st.global.f32 	[%rd75], %f365;

$L__BB47_27:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_1_bs_224
.visible .entry ggml_matvec_f32_ncols_1_bs_224(
	.param .u64 ggml_matvec_f32_ncols_1_bs_224_param_0,
	.param .u64 ggml_matvec_f32_ncols_1_bs_224_param_1,
	.param .u64 ggml_matvec_f32_ncols_1_bs_224_param_2,
	.param .u32 ggml_matvec_f32_ncols_1_bs_224_param_3,
	.param .u32 ggml_matvec_f32_ncols_1_bs_224_param_4,
	.param .u32 ggml_matvec_f32_ncols_1_bs_224_param_5,
	.param .u32 ggml_matvec_f32_ncols_1_bs_224_param_6,
	.param .u32 ggml_matvec_f32_ncols_1_bs_224_param_7,
	.param .u32 ggml_matvec_f32_ncols_1_bs_224_param_8,
	.param .u32 ggml_matvec_f32_ncols_1_bs_224_param_9,
	.param .u32 ggml_matvec_f32_ncols_1_bs_224_param_10,
	.param .u32 ggml_matvec_f32_ncols_1_bs_224_param_11
)
{
	.reg .pred 	%p<19>;
	.reg .f32 	%f<88>;
	.reg .b32 	%r<80>;
	.reg .b64 	%rd<44>;


	ld.param.u64 	%rd18, [ggml_matvec_f32_ncols_1_bs_224_param_0];
	ld.param.u64 	%rd19, [ggml_matvec_f32_ncols_1_bs_224_param_1];
	ld.param.u64 	%rd17, [ggml_matvec_f32_ncols_1_bs_224_param_2];
	ld.param.u32 	%r13, [ggml_matvec_f32_ncols_1_bs_224_param_3];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_1_bs_224_param_5];
	ld.param.u32 	%r14, [ggml_matvec_f32_ncols_1_bs_224_param_7];
	ld.param.u32 	%r17, [ggml_matvec_f32_ncols_1_bs_224_param_8];
	ld.param.u32 	%r18, [ggml_matvec_f32_ncols_1_bs_224_param_9];
	ld.param.u32 	%r19, [ggml_matvec_f32_ncols_1_bs_224_param_10];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_1_bs_224_param_11];
	cvta.to.global.u64 	%rd1, %rd19;
	cvta.to.global.u64 	%rd2, %rd18;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r20, %r1, %r17;
	mov.u32 	%r21, %ctaid.x;
	mul.lo.s32 	%r22, %r21, %r16;
	mad.lo.s32 	%r23, %r20, %r18, %r22;
	cvt.s64.s32 	%rd3, %r23;
	mul.lo.s32 	%r24, %r1, %r19;
	cvt.s64.s32 	%rd4, %r24;
	mov.u32 	%r2, %tid.x;
	setp.gt.s32 	%p1, %r2, 31;
	shl.b32 	%r25, %r2, 2;
	mov.u32 	%r26, data_mmv;
	add.s32 	%r3, %r26, %r25;
	@%p1 bra 	$L__BB48_2;

	mov.u32 	%r27, 0;
	st.shared.u32 	[%r3], %r27;

$L__BB48_2:
	bar.sync 	0;
	setp.ge.s32 	%p2, %r2, %r13;
	mov.f32 	%f86, 0f00000000;
	@%p2 bra 	$L__BB48_9;

	not.b32 	%r28, %r2;
	add.s32 	%r4, %r28, %r13;
	shr.u32 	%r29, %r4, 5;
	mul.wide.u32 	%rd20, %r29, 613566757;
	shr.u64 	%rd21, %rd20, 32;
	cvt.u32.u64 	%r30, %rd21;
	add.s32 	%r31, %r30, 1;
	and.b32  	%r77, %r31, 3;
	setp.eq.s32 	%p3, %r77, 0;
	mov.f32 	%f86, 0f00000000;
	mov.u32 	%r78, %r2;
	@%p3 bra 	$L__BB48_6;

	mul.wide.s32 	%rd22, %r2, 2;
	add.s64 	%rd23, %rd22, %rd4;
	shl.b64 	%rd24, %rd23, 2;
	add.s64 	%rd41, %rd1, %rd24;
	add.s64 	%rd25, %rd22, %rd3;
	shl.b64 	%rd26, %rd25, 2;
	add.s64 	%rd40, %rd2, %rd26;
	mov.f32 	%f86, 0f00000000;
	mov.u32 	%r78, %r2;

$L__BB48_5:
	.pragma "nounroll";
	ld.global.nc.v2.f32 	{%f15, %f16}, [%rd40];
	ld.global.nc.v2.f32 	{%f19, %f20}, [%rd41];
	fma.rn.f32 	%f23, %f15, %f19, %f86;
	fma.rn.f32 	%f86, %f16, %f20, %f23;
	add.s32 	%r78, %r78, 224;
	add.s64 	%rd41, %rd41, 1792;
	add.s64 	%rd40, %rd40, 1792;
	add.s32 	%r77, %r77, -1;
	setp.ne.s32 	%p4, %r77, 0;
	@%p4 bra 	$L__BB48_5;

$L__BB48_6:
	setp.lt.u32 	%p5, %r4, 672;
	@%p5 bra 	$L__BB48_9;

	mul.wide.s32 	%rd27, %r78, 2;
	add.s64 	%rd28, %rd27, %rd3;
	shl.b64 	%rd29, %rd28, 2;
	add.s64 	%rd30, %rd2, %rd29;
	add.s64 	%rd43, %rd30, 3584;
	add.s64 	%rd31, %rd27, %rd4;
	shl.b64 	%rd32, %rd31, 2;
	add.s64 	%rd33, %rd1, %rd32;
	add.s64 	%rd42, %rd33, 3584;

$L__BB48_8:
	ld.global.nc.v2.f32 	{%f24, %f25}, [%rd43+-3584];
	ld.global.nc.v2.f32 	{%f28, %f29}, [%rd42+-3584];
	fma.rn.f32 	%f32, %f24, %f28, %f86;
	fma.rn.f32 	%f33, %f25, %f29, %f32;
	ld.global.nc.v2.f32 	{%f34, %f35}, [%rd43+-1792];
	ld.global.nc.v2.f32 	{%f38, %f39}, [%rd42+-1792];
	fma.rn.f32 	%f42, %f34, %f38, %f33;
	fma.rn.f32 	%f43, %f35, %f39, %f42;
	ld.global.nc.v2.f32 	{%f44, %f45}, [%rd43];
	ld.global.nc.v2.f32 	{%f48, %f49}, [%rd42];
	fma.rn.f32 	%f52, %f44, %f48, %f43;
	fma.rn.f32 	%f53, %f45, %f49, %f52;
	ld.global.nc.v2.f32 	{%f54, %f55}, [%rd43+1792];
	ld.global.nc.v2.f32 	{%f58, %f59}, [%rd42+1792];
	fma.rn.f32 	%f62, %f54, %f58, %f53;
	fma.rn.f32 	%f86, %f55, %f59, %f62;
	add.s64 	%rd43, %rd43, 7168;
	add.s64 	%rd42, %rd42, 7168;
	add.s32 	%r78, %r78, 896;
	setp.lt.s32 	%p6, %r78, %r13;
	@%p6 bra 	$L__BB48_8;

$L__BB48_9:
	mov.b32 	%r32, %f86;
	mov.u32 	%r33, 31;
	mov.u32 	%r34, 16;
	mov.u32 	%r35, -1;
	shfl.sync.bfly.b32 	%r36|%p7, %r32, %r34, %r33, %r35;
	mov.b32 	%f63, %r36;
	add.f32 	%f64, %f86, %f63;
	mov.b32 	%r37, %f64;
	mov.u32 	%r38, 8;
	shfl.sync.bfly.b32 	%r39|%p8, %r37, %r38, %r33, %r35;
	mov.b32 	%f65, %r39;
	add.f32 	%f66, %f64, %f65;
	mov.b32 	%r40, %f66;
	mov.u32 	%r41, 4;
	shfl.sync.bfly.b32 	%r42|%p9, %r40, %r41, %r33, %r35;
	mov.b32 	%f67, %r42;
	add.f32 	%f68, %f66, %f67;
	mov.b32 	%r43, %f68;
	mov.u32 	%r44, 2;
	shfl.sync.bfly.b32 	%r45|%p10, %r43, %r44, %r33, %r35;
	mov.b32 	%f69, %r45;
	add.f32 	%f70, %f68, %f69;
	mov.b32 	%r46, %f70;
	mov.u32 	%r47, 1;
	shfl.sync.bfly.b32 	%r48|%p11, %r46, %r47, %r33, %r35;
	mov.b32 	%f71, %r48;
	add.f32 	%f87, %f70, %f71;
	shr.s32 	%r49, %r2, 31;
	shr.u32 	%r50, %r49, 27;
	add.s32 	%r51, %r2, %r50;
	shr.s32 	%r52, %r51, 5;
	shl.b32 	%r53, %r52, 2;
	add.s32 	%r55, %r26, %r53;
	st.shared.f32 	[%r55], %f87;
	bar.sync 	0;
	@%p1 bra 	$L__BB48_11;

	ld.shared.f32 	%f72, [%r3];
	mov.b32 	%r56, %f72;
	shfl.sync.bfly.b32 	%r60|%p13, %r56, %r34, %r33, %r35;
	mov.b32 	%f73, %r60;
	add.f32 	%f74, %f72, %f73;
	mov.b32 	%r61, %f74;
	shfl.sync.bfly.b32 	%r63|%p14, %r61, %r38, %r33, %r35;
	mov.b32 	%f75, %r63;
	add.f32 	%f76, %f74, %f75;
	mov.b32 	%r64, %f76;
	shfl.sync.bfly.b32 	%r66|%p15, %r64, %r41, %r33, %r35;
	mov.b32 	%f77, %r66;
	add.f32 	%f78, %f76, %f77;
	mov.b32 	%r67, %f78;
	shfl.sync.bfly.b32 	%r69|%p16, %r67, %r44, %r33, %r35;
	mov.b32 	%f79, %r69;
	add.f32 	%f80, %f78, %f79;
	mov.b32 	%r70, %f80;
	shfl.sync.bfly.b32 	%r72|%p17, %r70, %r47, %r33, %r35;
	mov.b32 	%f81, %r72;
	add.f32 	%f87, %f80, %f81;

$L__BB48_11:
	bar.sync 	0;
	setp.gt.s32 	%p18, %r2, 0;
	@%p18 bra 	$L__BB48_13;

	mad.lo.s32 	%r74, %r2, %r14, %r21;
	cvt.s64.s32 	%rd34, %r74;
	mul.lo.s32 	%r75, %r1, %r15;
	cvt.s64.s32 	%rd35, %r75;
	add.s64 	%rd36, %rd35, %rd34;
	cvta.to.global.u64 	%rd37, %rd17;
	shl.b64 	%rd38, %rd36, 2;
	add.s64 	%rd39, %rd37, %rd38;
	st.global.f32 	[%rd39], %f87;

$L__BB48_13:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_2_bs_224
.visible .entry ggml_matvec_f32_ncols_2_bs_224(
	.param .u64 ggml_matvec_f32_ncols_2_bs_224_param_0,
	.param .u64 ggml_matvec_f32_ncols_2_bs_224_param_1,
	.param .u64 ggml_matvec_f32_ncols_2_bs_224_param_2,
	.param .u32 ggml_matvec_f32_ncols_2_bs_224_param_3,
	.param .u32 ggml_matvec_f32_ncols_2_bs_224_param_4,
	.param .u32 ggml_matvec_f32_ncols_2_bs_224_param_5,
	.param .u32 ggml_matvec_f32_ncols_2_bs_224_param_6,
	.param .u32 ggml_matvec_f32_ncols_2_bs_224_param_7,
	.param .u32 ggml_matvec_f32_ncols_2_bs_224_param_8,
	.param .u32 ggml_matvec_f32_ncols_2_bs_224_param_9,
	.param .u32 ggml_matvec_f32_ncols_2_bs_224_param_10,
	.param .u32 ggml_matvec_f32_ncols_2_bs_224_param_11
)
{
	.local .align 8 .b8 	__local_depot49[8];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<30>;
	.reg .f32 	%f<146>;
	.reg .b32 	%r<114>;
	.reg .b64 	%rd<66>;


	mov.u64 	%SPL, __local_depot49;
	ld.param.u64 	%rd27, [ggml_matvec_f32_ncols_2_bs_224_param_0];
	ld.param.u64 	%rd28, [ggml_matvec_f32_ncols_2_bs_224_param_1];
	ld.param.u64 	%rd26, [ggml_matvec_f32_ncols_2_bs_224_param_2];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_2_bs_224_param_3];
	ld.param.u32 	%r19, [ggml_matvec_f32_ncols_2_bs_224_param_5];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_2_bs_224_param_6];
	ld.param.u32 	%r17, [ggml_matvec_f32_ncols_2_bs_224_param_7];
	ld.param.u32 	%r20, [ggml_matvec_f32_ncols_2_bs_224_param_8];
	ld.param.u32 	%r21, [ggml_matvec_f32_ncols_2_bs_224_param_9];
	ld.param.u32 	%r22, [ggml_matvec_f32_ncols_2_bs_224_param_10];
	ld.param.u32 	%r18, [ggml_matvec_f32_ncols_2_bs_224_param_11];
	cvta.to.global.u64 	%rd1, %rd28;
	cvta.to.global.u64 	%rd2, %rd27;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r23, %r1, %r20;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r24, %r2, %r19;
	mad.lo.s32 	%r25, %r23, %r21, %r24;
	cvt.s64.s32 	%rd4, %r25;
	mul.lo.s32 	%r26, %r1, %r22;
	cvt.s64.s32 	%rd5, %r26;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r27, %r3, 2;
	mov.u32 	%r28, data_mmv;
	add.s32 	%r4, %r28, %r27;
	@%p1 bra 	$L__BB49_2;

	mov.u32 	%r29, 0;
	st.shared.u32 	[%r4], %r29;

$L__BB49_2:
	bar.sync 	0;
	mov.f32 	%f144, 0f00000000;
	st.local.v2.f32 	[%rd3], {%f144, %f144};
	setp.ge.s32 	%p2, %r3, %r15;
	mov.f32 	%f145, %f144;
	@%p2 bra 	$L__BB49_11;

	not.b32 	%r30, %r3;
	add.s32 	%r5, %r30, %r15;
	shr.u32 	%r31, %r5, 5;
	mul.wide.u32 	%rd30, %r31, 613566757;
	shr.u64 	%rd31, %rd30, 32;
	cvt.u32.u64 	%r32, %rd31;
	add.s32 	%r33, %r32, 1;
	and.b32  	%r111, %r33, 3;
	setp.eq.s32 	%p3, %r111, 0;
	mov.f32 	%f144, 0f00000000;
	mov.u32 	%r112, %r3;
	@%p3 bra 	$L__BB49_7;

	mul.wide.s32 	%rd32, %r16, 2;
	mul.wide.s32 	%rd33, %r3, 2;
	add.s64 	%rd34, %rd32, %rd33;
	add.s64 	%rd35, %rd34, %rd5;
	shl.b64 	%rd36, %rd35, 2;
	add.s64 	%rd62, %rd1, %rd36;
	add.s64 	%rd37, %rd33, %rd5;
	shl.b64 	%rd38, %rd37, 2;
	add.s64 	%rd61, %rd1, %rd38;
	add.s64 	%rd39, %rd33, %rd4;
	shl.b64 	%rd40, %rd39, 2;
	add.s64 	%rd60, %rd2, %rd40;
	mov.f32 	%f144, 0f00000000;
	mov.f32 	%f145, %f144;
	mov.u32 	%r112, %r3;

$L__BB49_5:
	.pragma "nounroll";
	ld.global.nc.v2.f32 	{%f19, %f20}, [%rd60];
	ld.global.nc.v2.f32 	{%f23, %f24}, [%rd61];
	fma.rn.f32 	%f27, %f19, %f23, %f145;
	fma.rn.f32 	%f145, %f20, %f24, %f27;
	ld.global.nc.v2.f32 	{%f28, %f29}, [%rd62];
	fma.rn.f32 	%f32, %f19, %f28, %f144;
	fma.rn.f32 	%f144, %f20, %f29, %f32;
	add.s32 	%r112, %r112, 224;
	add.s64 	%rd62, %rd62, 1792;
	add.s64 	%rd61, %rd61, 1792;
	add.s64 	%rd60, %rd60, 1792;
	add.s32 	%r111, %r111, -1;
	setp.ne.s32 	%p4, %r111, 0;
	@%p4 bra 	$L__BB49_5;

	st.local.v2.f32 	[%rd3], {%f145, %f144};

$L__BB49_7:
	setp.lt.u32 	%p5, %r5, 672;
	@%p5 bra 	$L__BB49_11;

	mul.wide.s32 	%rd41, %r112, 2;
	add.s64 	%rd42, %rd41, %rd4;
	shl.b64 	%rd43, %rd42, 2;
	add.s64 	%rd44, %rd2, %rd43;
	add.s64 	%rd65, %rd44, 3584;
	add.s64 	%rd45, %rd41, %rd5;
	shl.b64 	%rd46, %rd45, 2;
	add.s64 	%rd47, %rd1, %rd46;
	add.s64 	%rd64, %rd47, 5376;
	mul.wide.s32 	%rd48, %r16, 2;
	add.s64 	%rd49, %rd45, %rd48;
	shl.b64 	%rd50, %rd49, 2;
	add.s64 	%rd51, %rd1, %rd50;
	add.s64 	%rd63, %rd51, 3584;

$L__BB49_9:
	ld.global.nc.v2.f32 	{%f33, %f34}, [%rd65+-3584];
	ld.global.nc.v2.f32 	{%f37, %f38}, [%rd64+-5376];
	fma.rn.f32 	%f41, %f33, %f37, %f145;
	fma.rn.f32 	%f42, %f34, %f38, %f41;
	ld.global.nc.v2.f32 	{%f43, %f44}, [%rd63+-3584];
	fma.rn.f32 	%f47, %f33, %f43, %f144;
	fma.rn.f32 	%f48, %f34, %f44, %f47;
	ld.global.nc.v2.f32 	{%f49, %f50}, [%rd65+-1792];
	ld.global.nc.v2.f32 	{%f53, %f54}, [%rd64+-3584];
	fma.rn.f32 	%f57, %f49, %f53, %f42;
	fma.rn.f32 	%f58, %f50, %f54, %f57;
	ld.global.nc.v2.f32 	{%f59, %f60}, [%rd63+-1792];
	fma.rn.f32 	%f63, %f49, %f59, %f48;
	fma.rn.f32 	%f64, %f50, %f60, %f63;
	ld.global.nc.v2.f32 	{%f65, %f66}, [%rd65];
	ld.global.nc.v2.f32 	{%f69, %f70}, [%rd64+-1792];
	fma.rn.f32 	%f73, %f65, %f69, %f58;
	fma.rn.f32 	%f74, %f66, %f70, %f73;
	ld.global.nc.v2.f32 	{%f75, %f76}, [%rd63];
	fma.rn.f32 	%f79, %f65, %f75, %f64;
	fma.rn.f32 	%f80, %f66, %f76, %f79;
	ld.global.nc.v2.f32 	{%f81, %f82}, [%rd65+1792];
	ld.global.nc.v2.f32 	{%f85, %f86}, [%rd64];
	fma.rn.f32 	%f89, %f81, %f85, %f74;
	fma.rn.f32 	%f145, %f82, %f86, %f89;
	ld.global.nc.v2.f32 	{%f90, %f91}, [%rd63+1792];
	fma.rn.f32 	%f94, %f81, %f90, %f80;
	fma.rn.f32 	%f144, %f82, %f91, %f94;
	add.s64 	%rd65, %rd65, 7168;
	add.s64 	%rd64, %rd64, 7168;
	add.s64 	%rd63, %rd63, 7168;
	add.s32 	%r112, %r112, 896;
	setp.lt.s32 	%p6, %r112, %r15;
	@%p6 bra 	$L__BB49_9;

	st.local.v2.f32 	[%rd3], {%f145, %f144};

$L__BB49_11:
	shr.s32 	%r34, %r3, 31;
	shr.u32 	%r35, %r34, 27;
	add.s32 	%r36, %r3, %r35;
	shr.s32 	%r37, %r36, 5;
	shl.b32 	%r38, %r37, 2;
	add.s32 	%r14, %r28, %r38;
	mov.u32 	%r40, 2;
	mov.b32 	%r41, %f145;
	mov.u32 	%r42, 31;
	mov.u32 	%r43, 16;
	mov.u32 	%r44, -1;
	shfl.sync.bfly.b32 	%r45|%p7, %r41, %r43, %r42, %r44;
	mov.b32 	%f95, %r45;
	add.f32 	%f96, %f145, %f95;
	mov.b32 	%r46, %f96;
	mov.u32 	%r47, 8;
	shfl.sync.bfly.b32 	%r48|%p8, %r46, %r47, %r42, %r44;
	mov.b32 	%f97, %r48;
	add.f32 	%f98, %f96, %f97;
	mov.b32 	%r49, %f98;
	mov.u32 	%r50, 4;
	shfl.sync.bfly.b32 	%r51|%p9, %r49, %r50, %r42, %r44;
	mov.b32 	%f99, %r51;
	add.f32 	%f100, %f98, %f99;
	mov.b32 	%r52, %f100;
	shfl.sync.bfly.b32 	%r53|%p10, %r52, %r40, %r42, %r44;
	mov.b32 	%f101, %r53;
	add.f32 	%f102, %f100, %f101;
	mov.b32 	%r54, %f102;
	mov.u32 	%r55, 1;
	shfl.sync.bfly.b32 	%r56|%p11, %r54, %r55, %r42, %r44;
	mov.b32 	%f103, %r56;
	add.f32 	%f104, %f102, %f103;
	st.local.f32 	[%rd3], %f104;
	st.shared.f32 	[%r14], %f104;
	bar.sync 	0;
	@%p1 bra 	$L__BB49_13;

	ld.shared.f32 	%f105, [%r4];
	mov.b32 	%r57, %f105;
	shfl.sync.bfly.b32 	%r61|%p13, %r57, %r43, %r42, %r44;
	mov.b32 	%f106, %r61;
	add.f32 	%f107, %f105, %f106;
	mov.b32 	%r62, %f107;
	shfl.sync.bfly.b32 	%r64|%p14, %r62, %r47, %r42, %r44;
	mov.b32 	%f108, %r64;
	add.f32 	%f109, %f107, %f108;
	mov.b32 	%r65, %f109;
	shfl.sync.bfly.b32 	%r67|%p15, %r65, %r50, %r42, %r44;
	mov.b32 	%f110, %r67;
	add.f32 	%f111, %f109, %f110;
	mov.b32 	%r68, %f111;
	shfl.sync.bfly.b32 	%r70|%p16, %r68, %r40, %r42, %r44;
	mov.b32 	%f112, %r70;
	add.f32 	%f113, %f111, %f112;
	mov.b32 	%r71, %f113;
	shfl.sync.bfly.b32 	%r73|%p17, %r71, %r55, %r42, %r44;
	mov.b32 	%f114, %r73;
	add.f32 	%f115, %f113, %f114;
	st.local.f32 	[%rd3], %f115;

$L__BB49_13:
	bar.sync 	0;
	mov.b32 	%r74, %f144;
	shfl.sync.bfly.b32 	%r78|%p19, %r74, %r43, %r42, %r44;
	mov.b32 	%f116, %r78;
	add.f32 	%f117, %f144, %f116;
	mov.b32 	%r79, %f117;
	shfl.sync.bfly.b32 	%r81|%p20, %r79, %r47, %r42, %r44;
	mov.b32 	%f118, %r81;
	add.f32 	%f119, %f117, %f118;
	mov.b32 	%r82, %f119;
	shfl.sync.bfly.b32 	%r84|%p21, %r82, %r50, %r42, %r44;
	mov.b32 	%f120, %r84;
	add.f32 	%f121, %f119, %f120;
	mov.b32 	%r85, %f121;
	shfl.sync.bfly.b32 	%r87|%p22, %r85, %r40, %r42, %r44;
	mov.b32 	%f122, %r87;
	add.f32 	%f123, %f121, %f122;
	mov.b32 	%r88, %f123;
	shfl.sync.bfly.b32 	%r90|%p23, %r88, %r55, %r42, %r44;
	mov.b32 	%f124, %r90;
	add.f32 	%f125, %f123, %f124;
	st.local.f32 	[%rd3+4], %f125;
	st.shared.f32 	[%r14], %f125;
	bar.sync 	0;
	@%p1 bra 	$L__BB49_15;

	ld.shared.f32 	%f126, [%r4];
	mov.b32 	%r91, %f126;
	mov.u32 	%r92, 31;
	mov.u32 	%r93, 16;
	mov.u32 	%r94, -1;
	shfl.sync.bfly.b32 	%r95|%p24, %r91, %r93, %r92, %r94;
	mov.b32 	%f127, %r95;
	add.f32 	%f128, %f126, %f127;
	mov.b32 	%r96, %f128;
	mov.u32 	%r97, 8;
	shfl.sync.bfly.b32 	%r98|%p25, %r96, %r97, %r92, %r94;
	mov.b32 	%f129, %r98;
	add.f32 	%f130, %f128, %f129;
	mov.b32 	%r99, %f130;
	mov.u32 	%r100, 4;
	shfl.sync.bfly.b32 	%r101|%p26, %r99, %r100, %r92, %r94;
	mov.b32 	%f131, %r101;
	add.f32 	%f132, %f130, %f131;
	mov.b32 	%r102, %f132;
	mov.u32 	%r103, 2;
	shfl.sync.bfly.b32 	%r104|%p27, %r102, %r103, %r92, %r94;
	mov.b32 	%f133, %r104;
	add.f32 	%f134, %f132, %f133;
	mov.b32 	%r105, %f134;
	mov.u32 	%r106, 1;
	shfl.sync.bfly.b32 	%r107|%p28, %r105, %r106, %r92, %r94;
	mov.b32 	%f135, %r107;
	add.f32 	%f136, %f134, %f135;
	st.local.f32 	[%rd3+4], %f136;

$L__BB49_15:
	bar.sync 	0;
	setp.gt.s32 	%p29, %r3, 1;
	@%p29 bra 	$L__BB49_17;

	mul.wide.s32 	%rd52, %r3, 4;
	add.s64 	%rd53, %rd3, %rd52;
	ld.local.f32 	%f137, [%rd53];
	mad.lo.s32 	%r108, %r3, %r17, %r2;
	cvt.s64.s32 	%rd54, %r108;
	mul.lo.s32 	%r109, %r1, %r18;
	cvt.s64.s32 	%rd55, %r109;
	add.s64 	%rd56, %rd55, %rd54;
	cvta.to.global.u64 	%rd57, %rd26;
	shl.b64 	%rd58, %rd56, 2;
	add.s64 	%rd59, %rd57, %rd58;
	st.global.f32 	[%rd59], %f137;

$L__BB49_17:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_3_bs_224
.visible .entry ggml_matvec_f32_ncols_3_bs_224(
	.param .u64 ggml_matvec_f32_ncols_3_bs_224_param_0,
	.param .u64 ggml_matvec_f32_ncols_3_bs_224_param_1,
	.param .u64 ggml_matvec_f32_ncols_3_bs_224_param_2,
	.param .u32 ggml_matvec_f32_ncols_3_bs_224_param_3,
	.param .u32 ggml_matvec_f32_ncols_3_bs_224_param_4,
	.param .u32 ggml_matvec_f32_ncols_3_bs_224_param_5,
	.param .u32 ggml_matvec_f32_ncols_3_bs_224_param_6,
	.param .u32 ggml_matvec_f32_ncols_3_bs_224_param_7,
	.param .u32 ggml_matvec_f32_ncols_3_bs_224_param_8,
	.param .u32 ggml_matvec_f32_ncols_3_bs_224_param_9,
	.param .u32 ggml_matvec_f32_ncols_3_bs_224_param_10,
	.param .u32 ggml_matvec_f32_ncols_3_bs_224_param_11
)
{
	.local .align 4 .b8 	__local_depot50[12];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<41>;
	.reg .f32 	%f<208>;
	.reg .b32 	%r<155>;
	.reg .b64 	%rd<74>;


	mov.u64 	%SPL, __local_depot50;
	ld.param.u64 	%rd29, [ggml_matvec_f32_ncols_3_bs_224_param_0];
	ld.param.u64 	%rd30, [ggml_matvec_f32_ncols_3_bs_224_param_1];
	ld.param.u64 	%rd28, [ggml_matvec_f32_ncols_3_bs_224_param_2];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_3_bs_224_param_3];
	ld.param.u32 	%r19, [ggml_matvec_f32_ncols_3_bs_224_param_5];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_3_bs_224_param_6];
	ld.param.u32 	%r17, [ggml_matvec_f32_ncols_3_bs_224_param_7];
	ld.param.u32 	%r20, [ggml_matvec_f32_ncols_3_bs_224_param_8];
	ld.param.u32 	%r21, [ggml_matvec_f32_ncols_3_bs_224_param_9];
	ld.param.u32 	%r22, [ggml_matvec_f32_ncols_3_bs_224_param_10];
	ld.param.u32 	%r18, [ggml_matvec_f32_ncols_3_bs_224_param_11];
	cvta.to.global.u64 	%rd73, %rd30;
	cvta.to.global.u64 	%rd2, %rd29;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r23, %r1, %r20;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r24, %r2, %r19;
	mad.lo.s32 	%r25, %r23, %r21, %r24;
	cvt.s64.s32 	%rd4, %r25;
	mul.lo.s32 	%r26, %r1, %r22;
	cvt.s64.s32 	%rd5, %r26;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r27, %r3, 2;
	mov.u32 	%r28, data_mmv;
	add.s32 	%r4, %r28, %r27;
	@%p1 bra 	$L__BB50_2;

	mov.u32 	%r29, 0;
	st.shared.u32 	[%r4], %r29;

$L__BB50_2:
	bar.sync 	0;
	mov.f32 	%f205, 0f00000000;
	mov.u32 	%r30, 0;
	st.local.u32 	[%rd3], %r30;
	st.local.u32 	[%rd3+4], %r30;
	st.local.u32 	[%rd3+8], %r30;
	setp.ge.s32 	%p2, %r3, %r15;
	mov.f32 	%f206, %f205;
	mov.f32 	%f207, %f205;
	@%p2 bra 	$L__BB50_11;

	not.b32 	%r31, %r3;
	add.s32 	%r5, %r31, %r15;
	shr.u32 	%r32, %r5, 5;
	mul.wide.u32 	%rd32, %r32, 613566757;
	shr.u64 	%rd33, %rd32, 32;
	cvt.u32.u64 	%r33, %rd33;
	add.s32 	%r34, %r33, 1;
	and.b32  	%r152, %r34, 3;
	setp.eq.s32 	%p3, %r152, 0;
	mov.f32 	%f205, 0f00000000;
	mov.u32 	%r153, %r3;
	@%p3 bra 	$L__BB50_7;

	shl.b32 	%r35, %r16, 1;
	add.s32 	%r36, %r3, %r35;
	mul.wide.s32 	%rd34, %r36, 2;
	add.s64 	%rd35, %rd34, %rd5;
	shl.b64 	%rd36, %rd35, 2;
	add.s64 	%rd71, %rd73, %rd36;
	mul.wide.s32 	%rd37, %r16, 2;
	mul.wide.s32 	%rd38, %r3, 2;
	add.s64 	%rd39, %rd37, %rd38;
	add.s64 	%rd40, %rd39, %rd5;
	shl.b64 	%rd41, %rd40, 2;
	add.s64 	%rd70, %rd73, %rd41;
	add.s64 	%rd42, %rd38, %rd5;
	shl.b64 	%rd43, %rd42, 2;
	add.s64 	%rd69, %rd73, %rd43;
	add.s64 	%rd44, %rd38, %rd4;
	shl.b64 	%rd45, %rd44, 2;
	add.s64 	%rd68, %rd2, %rd45;
	mov.f32 	%f205, 0f00000000;
	mov.f32 	%f206, %f205;
	mov.f32 	%f207, %f205;
	mov.u32 	%r153, %r3;

$L__BB50_5:
	.pragma "nounroll";
	ld.global.nc.v2.f32 	{%f28, %f29}, [%rd68];
	ld.global.nc.v2.f32 	{%f32, %f33}, [%rd69];
	fma.rn.f32 	%f36, %f28, %f32, %f207;
	fma.rn.f32 	%f207, %f29, %f33, %f36;
	ld.global.nc.v2.f32 	{%f37, %f38}, [%rd70];
	fma.rn.f32 	%f41, %f28, %f37, %f206;
	fma.rn.f32 	%f206, %f29, %f38, %f41;
	ld.global.nc.v2.f32 	{%f42, %f43}, [%rd71];
	fma.rn.f32 	%f46, %f28, %f42, %f205;
	fma.rn.f32 	%f205, %f29, %f43, %f46;
	add.s32 	%r153, %r153, 224;
	add.s64 	%rd71, %rd71, 1792;
	add.s64 	%rd70, %rd70, 1792;
	add.s64 	%rd69, %rd69, 1792;
	add.s64 	%rd68, %rd68, 1792;
	add.s32 	%r152, %r152, -1;
	setp.ne.s32 	%p4, %r152, 0;
	@%p4 bra 	$L__BB50_5;

	st.local.f32 	[%rd3], %f207;
	st.local.f32 	[%rd3+4], %f206;
	st.local.f32 	[%rd3+8], %f205;

$L__BB50_7:
	setp.lt.u32 	%p5, %r5, 672;
	@%p5 bra 	$L__BB50_11;

	add.s32 	%r37, %r153, %r16;
	shl.b32 	%r38, %r16, 1;
	add.s32 	%r39, %r153, %r38;
	add.s32 	%r40, %r37, 224;
	mul.wide.s32 	%rd46, %r40, 8;
	shl.b64 	%rd47, %rd5, 2;
	add.s64 	%rd19, %rd46, %rd47;
	mul.wide.s32 	%rd48, %r39, 8;
	add.s64 	%rd20, %rd48, %rd47;
	mul.wide.s32 	%rd49, %r153, 2;
	add.s64 	%rd50, %rd49, %rd4;
	shl.b64 	%rd51, %rd50, 2;
	add.s64 	%rd52, %rd2, %rd51;
	add.s64 	%rd72, %rd52, 3584;
	mul.wide.s32 	%rd53, %r153, 8;
	add.s64 	%rd22, %rd53, %rd47;
	mul.wide.s32 	%rd54, %r16, 8;
	add.s64 	%rd55, %rd53, %rd54;
	add.s64 	%rd23, %rd55, %rd47;

$L__BB50_9:
	ld.global.nc.v2.f32 	{%f47, %f48}, [%rd72+-3584];
	add.s64 	%rd56, %rd73, %rd22;
	ld.global.nc.v2.f32 	{%f51, %f52}, [%rd56];
	fma.rn.f32 	%f55, %f47, %f51, %f207;
	fma.rn.f32 	%f56, %f48, %f52, %f55;
	add.s64 	%rd57, %rd73, %rd23;
	ld.global.nc.v2.f32 	{%f57, %f58}, [%rd57];
	fma.rn.f32 	%f61, %f47, %f57, %f206;
	fma.rn.f32 	%f62, %f48, %f58, %f61;
	add.s64 	%rd58, %rd73, %rd20;
	ld.global.nc.v2.f32 	{%f63, %f64}, [%rd58];
	fma.rn.f32 	%f67, %f47, %f63, %f205;
	fma.rn.f32 	%f68, %f48, %f64, %f67;
	ld.global.nc.v2.f32 	{%f69, %f70}, [%rd72+-1792];
	ld.global.nc.v2.f32 	{%f73, %f74}, [%rd56+1792];
	fma.rn.f32 	%f77, %f69, %f73, %f56;
	fma.rn.f32 	%f78, %f70, %f74, %f77;
	add.s64 	%rd59, %rd73, %rd19;
	ld.global.nc.v2.f32 	{%f79, %f80}, [%rd59];
	fma.rn.f32 	%f83, %f69, %f79, %f62;
	fma.rn.f32 	%f84, %f70, %f80, %f83;
	ld.global.nc.v2.f32 	{%f85, %f86}, [%rd58+1792];
	fma.rn.f32 	%f89, %f69, %f85, %f68;
	fma.rn.f32 	%f90, %f70, %f86, %f89;
	ld.global.nc.v2.f32 	{%f91, %f92}, [%rd72];
	ld.global.nc.v2.f32 	{%f95, %f96}, [%rd56+3584];
	fma.rn.f32 	%f99, %f91, %f95, %f78;
	fma.rn.f32 	%f100, %f92, %f96, %f99;
	ld.global.nc.v2.f32 	{%f101, %f102}, [%rd59+1792];
	fma.rn.f32 	%f105, %f91, %f101, %f84;
	fma.rn.f32 	%f106, %f92, %f102, %f105;
	ld.global.nc.v2.f32 	{%f107, %f108}, [%rd58+3584];
	fma.rn.f32 	%f111, %f91, %f107, %f90;
	fma.rn.f32 	%f112, %f92, %f108, %f111;
	ld.global.nc.v2.f32 	{%f113, %f114}, [%rd72+1792];
	ld.global.nc.v2.f32 	{%f117, %f118}, [%rd56+5376];
	fma.rn.f32 	%f121, %f113, %f117, %f100;
	fma.rn.f32 	%f207, %f114, %f118, %f121;
	ld.global.nc.v2.f32 	{%f122, %f123}, [%rd59+3584];
	fma.rn.f32 	%f126, %f113, %f122, %f106;
	fma.rn.f32 	%f206, %f114, %f123, %f126;
	ld.global.nc.v2.f32 	{%f127, %f128}, [%rd58+5376];
	fma.rn.f32 	%f131, %f113, %f127, %f112;
	fma.rn.f32 	%f205, %f114, %f128, %f131;
	add.s64 	%rd73, %rd73, 7168;
	add.s64 	%rd72, %rd72, 7168;
	add.s32 	%r153, %r153, 896;
	setp.lt.s32 	%p6, %r153, %r15;
	@%p6 bra 	$L__BB50_9;

	st.local.f32 	[%rd3], %f207;
	st.local.f32 	[%rd3+4], %f206;
	st.local.f32 	[%rd3+8], %f205;

$L__BB50_11:
	shr.s32 	%r41, %r3, 31;
	shr.u32 	%r42, %r41, 27;
	add.s32 	%r43, %r3, %r42;
	shr.s32 	%r44, %r43, 5;
	shl.b32 	%r45, %r44, 2;
	add.s32 	%r14, %r28, %r45;
	mov.u32 	%r47, 2;
	mov.b32 	%r48, %f207;
	mov.u32 	%r49, 31;
	mov.u32 	%r50, 16;
	mov.u32 	%r51, -1;
	shfl.sync.bfly.b32 	%r52|%p7, %r48, %r50, %r49, %r51;
	mov.b32 	%f132, %r52;
	add.f32 	%f133, %f207, %f132;
	mov.b32 	%r53, %f133;
	mov.u32 	%r54, 8;
	shfl.sync.bfly.b32 	%r55|%p8, %r53, %r54, %r49, %r51;
	mov.b32 	%f134, %r55;
	add.f32 	%f135, %f133, %f134;
	mov.b32 	%r56, %f135;
	mov.u32 	%r57, 4;
	shfl.sync.bfly.b32 	%r58|%p9, %r56, %r57, %r49, %r51;
	mov.b32 	%f136, %r58;
	add.f32 	%f137, %f135, %f136;
	mov.b32 	%r59, %f137;
	shfl.sync.bfly.b32 	%r60|%p10, %r59, %r47, %r49, %r51;
	mov.b32 	%f138, %r60;
	add.f32 	%f139, %f137, %f138;
	mov.b32 	%r61, %f139;
	mov.u32 	%r62, 1;
	shfl.sync.bfly.b32 	%r63|%p11, %r61, %r62, %r49, %r51;
	mov.b32 	%f140, %r63;
	add.f32 	%f141, %f139, %f140;
	st.local.f32 	[%rd3], %f141;
	st.shared.f32 	[%r14], %f141;
	bar.sync 	0;
	@%p1 bra 	$L__BB50_13;

	ld.shared.f32 	%f142, [%r4];
	mov.b32 	%r64, %f142;
	shfl.sync.bfly.b32 	%r68|%p13, %r64, %r50, %r49, %r51;
	mov.b32 	%f143, %r68;
	add.f32 	%f144, %f142, %f143;
	mov.b32 	%r69, %f144;
	shfl.sync.bfly.b32 	%r71|%p14, %r69, %r54, %r49, %r51;
	mov.b32 	%f145, %r71;
	add.f32 	%f146, %f144, %f145;
	mov.b32 	%r72, %f146;
	shfl.sync.bfly.b32 	%r74|%p15, %r72, %r57, %r49, %r51;
	mov.b32 	%f147, %r74;
	add.f32 	%f148, %f146, %f147;
	mov.b32 	%r75, %f148;
	shfl.sync.bfly.b32 	%r77|%p16, %r75, %r47, %r49, %r51;
	mov.b32 	%f149, %r77;
	add.f32 	%f150, %f148, %f149;
	mov.b32 	%r78, %f150;
	shfl.sync.bfly.b32 	%r80|%p17, %r78, %r62, %r49, %r51;
	mov.b32 	%f151, %r80;
	add.f32 	%f152, %f150, %f151;
	st.local.f32 	[%rd3], %f152;

$L__BB50_13:
	bar.sync 	0;
	mov.b32 	%r81, %f206;
	shfl.sync.bfly.b32 	%r85|%p19, %r81, %r50, %r49, %r51;
	mov.b32 	%f153, %r85;
	add.f32 	%f154, %f206, %f153;
	mov.b32 	%r86, %f154;
	shfl.sync.bfly.b32 	%r88|%p20, %r86, %r54, %r49, %r51;
	mov.b32 	%f155, %r88;
	add.f32 	%f156, %f154, %f155;
	mov.b32 	%r89, %f156;
	shfl.sync.bfly.b32 	%r91|%p21, %r89, %r57, %r49, %r51;
	mov.b32 	%f157, %r91;
	add.f32 	%f158, %f156, %f157;
	mov.b32 	%r92, %f158;
	shfl.sync.bfly.b32 	%r94|%p22, %r92, %r47, %r49, %r51;
	mov.b32 	%f159, %r94;
	add.f32 	%f160, %f158, %f159;
	mov.b32 	%r95, %f160;
	shfl.sync.bfly.b32 	%r97|%p23, %r95, %r62, %r49, %r51;
	mov.b32 	%f161, %r97;
	add.f32 	%f162, %f160, %f161;
	st.local.f32 	[%rd3+4], %f162;
	st.shared.f32 	[%r14], %f162;
	bar.sync 	0;
	@%p1 bra 	$L__BB50_15;

	ld.shared.f32 	%f163, [%r4];
	mov.b32 	%r98, %f163;
	mov.u32 	%r99, 31;
	mov.u32 	%r100, 16;
	mov.u32 	%r101, -1;
	shfl.sync.bfly.b32 	%r102|%p24, %r98, %r100, %r99, %r101;
	mov.b32 	%f164, %r102;
	add.f32 	%f165, %f163, %f164;
	mov.b32 	%r103, %f165;
	mov.u32 	%r104, 8;
	shfl.sync.bfly.b32 	%r105|%p25, %r103, %r104, %r99, %r101;
	mov.b32 	%f166, %r105;
	add.f32 	%f167, %f165, %f166;
	mov.b32 	%r106, %f167;
	mov.u32 	%r107, 4;
	shfl.sync.bfly.b32 	%r108|%p26, %r106, %r107, %r99, %r101;
	mov.b32 	%f168, %r108;
	add.f32 	%f169, %f167, %f168;
	mov.b32 	%r109, %f169;
	mov.u32 	%r110, 2;
	shfl.sync.bfly.b32 	%r111|%p27, %r109, %r110, %r99, %r101;
	mov.b32 	%f170, %r111;
	add.f32 	%f171, %f169, %f170;
	mov.b32 	%r112, %f171;
	mov.u32 	%r113, 1;
	shfl.sync.bfly.b32 	%r114|%p28, %r112, %r113, %r99, %r101;
	mov.b32 	%f172, %r114;
	add.f32 	%f173, %f171, %f172;
	st.local.f32 	[%rd3+4], %f173;

$L__BB50_15:
	bar.sync 	0;
	mov.b32 	%r115, %f205;
	mov.u32 	%r116, 31;
	mov.u32 	%r117, 16;
	mov.u32 	%r118, -1;
	shfl.sync.bfly.b32 	%r119|%p30, %r115, %r117, %r116, %r118;
	mov.b32 	%f174, %r119;
	add.f32 	%f175, %f205, %f174;
	mov.b32 	%r120, %f175;
	mov.u32 	%r121, 8;
	shfl.sync.bfly.b32 	%r122|%p31, %r120, %r121, %r116, %r118;
	mov.b32 	%f176, %r122;
	add.f32 	%f177, %f175, %f176;
	mov.b32 	%r123, %f177;
	mov.u32 	%r124, 4;
	shfl.sync.bfly.b32 	%r125|%p32, %r123, %r124, %r116, %r118;
	mov.b32 	%f178, %r125;
	add.f32 	%f179, %f177, %f178;
	mov.b32 	%r126, %f179;
	mov.u32 	%r127, 2;
	shfl.sync.bfly.b32 	%r128|%p33, %r126, %r127, %r116, %r118;
	mov.b32 	%f180, %r128;
	add.f32 	%f181, %f179, %f180;
	mov.b32 	%r129, %f181;
	mov.u32 	%r130, 1;
	shfl.sync.bfly.b32 	%r131|%p34, %r129, %r130, %r116, %r118;
	mov.b32 	%f182, %r131;
	add.f32 	%f183, %f181, %f182;
	st.local.f32 	[%rd3+8], %f183;
	st.shared.f32 	[%r14], %f183;
	bar.sync 	0;
	@%p1 bra 	$L__BB50_17;

	ld.shared.f32 	%f184, [%r4];
	mov.b32 	%r132, %f184;
	shfl.sync.bfly.b32 	%r136|%p35, %r132, %r117, %r116, %r118;
	mov.b32 	%f185, %r136;
	add.f32 	%f186, %f184, %f185;
	mov.b32 	%r137, %f186;
	shfl.sync.bfly.b32 	%r139|%p36, %r137, %r121, %r116, %r118;
	mov.b32 	%f187, %r139;
	add.f32 	%f188, %f186, %f187;
	mov.b32 	%r140, %f188;
	shfl.sync.bfly.b32 	%r142|%p37, %r140, %r124, %r116, %r118;
	mov.b32 	%f189, %r142;
	add.f32 	%f190, %f188, %f189;
	mov.b32 	%r143, %f190;
	shfl.sync.bfly.b32 	%r145|%p38, %r143, %r127, %r116, %r118;
	mov.b32 	%f191, %r145;
	add.f32 	%f192, %f190, %f191;
	mov.b32 	%r146, %f192;
	shfl.sync.bfly.b32 	%r148|%p39, %r146, %r130, %r116, %r118;
	mov.b32 	%f193, %r148;
	add.f32 	%f194, %f192, %f193;
	st.local.f32 	[%rd3+8], %f194;

$L__BB50_17:
	bar.sync 	0;
	setp.gt.s32 	%p40, %r3, 2;
	@%p40 bra 	$L__BB50_19;

	mul.wide.s32 	%rd60, %r3, 4;
	add.s64 	%rd61, %rd3, %rd60;
	ld.local.f32 	%f195, [%rd61];
	mad.lo.s32 	%r149, %r3, %r17, %r2;
	cvt.s64.s32 	%rd62, %r149;
	mul.lo.s32 	%r150, %r1, %r18;
	cvt.s64.s32 	%rd63, %r150;
	add.s64 	%rd64, %rd63, %rd62;
	cvta.to.global.u64 	%rd65, %rd28;
	shl.b64 	%rd66, %rd64, 2;
	add.s64 	%rd67, %rd65, %rd66;
	st.global.f32 	[%rd67], %f195;

$L__BB50_19:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_4_bs_224
.visible .entry ggml_matvec_f32_ncols_4_bs_224(
	.param .u64 ggml_matvec_f32_ncols_4_bs_224_param_0,
	.param .u64 ggml_matvec_f32_ncols_4_bs_224_param_1,
	.param .u64 ggml_matvec_f32_ncols_4_bs_224_param_2,
	.param .u32 ggml_matvec_f32_ncols_4_bs_224_param_3,
	.param .u32 ggml_matvec_f32_ncols_4_bs_224_param_4,
	.param .u32 ggml_matvec_f32_ncols_4_bs_224_param_5,
	.param .u32 ggml_matvec_f32_ncols_4_bs_224_param_6,
	.param .u32 ggml_matvec_f32_ncols_4_bs_224_param_7,
	.param .u32 ggml_matvec_f32_ncols_4_bs_224_param_8,
	.param .u32 ggml_matvec_f32_ncols_4_bs_224_param_9,
	.param .u32 ggml_matvec_f32_ncols_4_bs_224_param_10,
	.param .u32 ggml_matvec_f32_ncols_4_bs_224_param_11
)
{
	.local .align 16 .b8 	__local_depot51[16];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<52>;
	.reg .f32 	%f<270>;
	.reg .b32 	%r<190>;
	.reg .b64 	%rd<85>;


	mov.u64 	%SPL, __local_depot51;
	ld.param.u64 	%rd34, [ggml_matvec_f32_ncols_4_bs_224_param_0];
	ld.param.u64 	%rd35, [ggml_matvec_f32_ncols_4_bs_224_param_1];
	ld.param.u64 	%rd33, [ggml_matvec_f32_ncols_4_bs_224_param_2];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_4_bs_224_param_3];
	ld.param.u32 	%r19, [ggml_matvec_f32_ncols_4_bs_224_param_5];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_4_bs_224_param_6];
	ld.param.u32 	%r17, [ggml_matvec_f32_ncols_4_bs_224_param_7];
	ld.param.u32 	%r20, [ggml_matvec_f32_ncols_4_bs_224_param_8];
	ld.param.u32 	%r21, [ggml_matvec_f32_ncols_4_bs_224_param_9];
	ld.param.u32 	%r22, [ggml_matvec_f32_ncols_4_bs_224_param_10];
	ld.param.u32 	%r18, [ggml_matvec_f32_ncols_4_bs_224_param_11];
	cvta.to.global.u64 	%rd84, %rd35;
	cvta.to.global.u64 	%rd2, %rd34;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r23, %r1, %r20;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r24, %r2, %r19;
	mad.lo.s32 	%r25, %r23, %r21, %r24;
	cvt.s64.s32 	%rd4, %r25;
	mul.lo.s32 	%r26, %r1, %r22;
	cvt.s64.s32 	%rd5, %r26;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r27, %r3, 2;
	mov.u32 	%r28, data_mmv;
	add.s32 	%r4, %r28, %r27;
	@%p1 bra 	$L__BB51_2;

	mov.u32 	%r29, 0;
	st.shared.u32 	[%r4], %r29;

$L__BB51_2:
	bar.sync 	0;
	mov.f32 	%f266, 0f00000000;
	st.local.v4.f32 	[%rd3], {%f266, %f266, %f266, %f266};
	setp.ge.s32 	%p2, %r3, %r15;
	mov.f32 	%f267, %f266;
	mov.f32 	%f268, %f266;
	mov.f32 	%f269, %f266;
	@%p2 bra 	$L__BB51_11;

	not.b32 	%r30, %r3;
	add.s32 	%r5, %r30, %r15;
	shr.u32 	%r31, %r5, 5;
	mul.wide.u32 	%rd37, %r31, 613566757;
	shr.u64 	%rd38, %rd37, 32;
	cvt.u32.u64 	%r32, %rd38;
	add.s32 	%r33, %r32, 1;
	and.b32  	%r187, %r33, 3;
	setp.eq.s32 	%p3, %r187, 0;
	mov.f32 	%f266, 0f00000000;
	mov.u32 	%r188, %r3;
	@%p3 bra 	$L__BB51_7;

	shl.b32 	%r34, %r16, 1;
	add.s32 	%r35, %r3, %r34;
	mul.wide.s32 	%rd39, %r35, 2;
	add.s64 	%rd40, %rd39, %rd5;
	shl.b64 	%rd41, %rd40, 2;
	add.s64 	%rd82, %rd84, %rd41;
	mad.lo.s32 	%r36, %r16, 3, %r3;
	mul.wide.s32 	%rd42, %r36, 2;
	add.s64 	%rd43, %rd42, %rd5;
	shl.b64 	%rd44, %rd43, 2;
	add.s64 	%rd81, %rd84, %rd44;
	mul.wide.s32 	%rd45, %r16, 2;
	mul.wide.s32 	%rd46, %r3, 2;
	add.s64 	%rd47, %rd45, %rd46;
	add.s64 	%rd48, %rd47, %rd5;
	shl.b64 	%rd49, %rd48, 2;
	add.s64 	%rd80, %rd84, %rd49;
	add.s64 	%rd50, %rd46, %rd5;
	shl.b64 	%rd51, %rd50, 2;
	add.s64 	%rd79, %rd84, %rd51;
	add.s64 	%rd52, %rd46, %rd4;
	shl.b64 	%rd53, %rd52, 2;
	add.s64 	%rd78, %rd2, %rd53;
	mov.f32 	%f266, 0f00000000;
	mov.f32 	%f267, %f266;
	mov.f32 	%f268, %f266;
	mov.f32 	%f269, %f266;
	mov.u32 	%r188, %r3;

$L__BB51_5:
	.pragma "nounroll";
	ld.global.nc.v2.f32 	{%f37, %f38}, [%rd78];
	ld.global.nc.v2.f32 	{%f41, %f42}, [%rd79];
	fma.rn.f32 	%f45, %f37, %f41, %f269;
	fma.rn.f32 	%f269, %f38, %f42, %f45;
	ld.global.nc.v2.f32 	{%f46, %f47}, [%rd80];
	fma.rn.f32 	%f50, %f37, %f46, %f268;
	fma.rn.f32 	%f268, %f38, %f47, %f50;
	ld.global.nc.v2.f32 	{%f51, %f52}, [%rd82];
	fma.rn.f32 	%f55, %f37, %f51, %f267;
	fma.rn.f32 	%f267, %f38, %f52, %f55;
	ld.global.nc.v2.f32 	{%f56, %f57}, [%rd81];
	fma.rn.f32 	%f60, %f37, %f56, %f266;
	fma.rn.f32 	%f266, %f38, %f57, %f60;
	add.s32 	%r188, %r188, 224;
	add.s64 	%rd82, %rd82, 1792;
	add.s64 	%rd81, %rd81, 1792;
	add.s64 	%rd80, %rd80, 1792;
	add.s64 	%rd79, %rd79, 1792;
	add.s64 	%rd78, %rd78, 1792;
	add.s32 	%r187, %r187, -1;
	setp.ne.s32 	%p4, %r187, 0;
	@%p4 bra 	$L__BB51_5;

	st.local.v4.f32 	[%rd3], {%f269, %f268, %f267, %f266};

$L__BB51_7:
	setp.lt.u32 	%p5, %r5, 672;
	@%p5 bra 	$L__BB51_11;

	add.s32 	%r37, %r188, %r16;
	shl.b32 	%r38, %r16, 1;
	add.s32 	%r39, %r188, %r38;
	mad.lo.s32 	%r40, %r16, 3, %r188;
	add.s32 	%r41, %r37, 224;
	mul.wide.s32 	%rd54, %r41, 8;
	shl.b64 	%rd55, %rd5, 2;
	add.s64 	%rd23, %rd54, %rd55;
	mul.wide.s32 	%rd56, %r39, 8;
	add.s64 	%rd24, %rd56, %rd55;
	mul.wide.s32 	%rd57, %r40, 8;
	add.s64 	%rd25, %rd57, %rd55;
	mul.wide.s32 	%rd58, %r188, 2;
	add.s64 	%rd59, %rd58, %rd4;
	shl.b64 	%rd60, %rd59, 2;
	add.s64 	%rd61, %rd2, %rd60;
	add.s64 	%rd83, %rd61, 3584;
	mul.wide.s32 	%rd62, %r188, 8;
	add.s64 	%rd27, %rd62, %rd55;
	mul.wide.s32 	%rd63, %r16, 8;
	add.s64 	%rd64, %rd62, %rd63;
	add.s64 	%rd28, %rd64, %rd55;

$L__BB51_9:
	ld.global.nc.v2.f32 	{%f61, %f62}, [%rd83+-3584];
	add.s64 	%rd65, %rd84, %rd27;
	ld.global.nc.v2.f32 	{%f65, %f66}, [%rd65];
	fma.rn.f32 	%f69, %f61, %f65, %f269;
	fma.rn.f32 	%f70, %f62, %f66, %f69;
	add.s64 	%rd66, %rd84, %rd28;
	ld.global.nc.v2.f32 	{%f71, %f72}, [%rd66];
	fma.rn.f32 	%f75, %f61, %f71, %f268;
	fma.rn.f32 	%f76, %f62, %f72, %f75;
	add.s64 	%rd67, %rd84, %rd24;
	ld.global.nc.v2.f32 	{%f77, %f78}, [%rd67];
	fma.rn.f32 	%f81, %f61, %f77, %f267;
	fma.rn.f32 	%f82, %f62, %f78, %f81;
	add.s64 	%rd68, %rd84, %rd25;
	ld.global.nc.v2.f32 	{%f83, %f84}, [%rd68];
	fma.rn.f32 	%f87, %f61, %f83, %f266;
	fma.rn.f32 	%f88, %f62, %f84, %f87;
	ld.global.nc.v2.f32 	{%f89, %f90}, [%rd83+-1792];
	ld.global.nc.v2.f32 	{%f93, %f94}, [%rd65+1792];
	fma.rn.f32 	%f97, %f89, %f93, %f70;
	fma.rn.f32 	%f98, %f90, %f94, %f97;
	add.s64 	%rd69, %rd84, %rd23;
	ld.global.nc.v2.f32 	{%f99, %f100}, [%rd69];
	fma.rn.f32 	%f103, %f89, %f99, %f76;
	fma.rn.f32 	%f104, %f90, %f100, %f103;
	ld.global.nc.v2.f32 	{%f105, %f106}, [%rd67+1792];
	fma.rn.f32 	%f109, %f89, %f105, %f82;
	fma.rn.f32 	%f110, %f90, %f106, %f109;
	ld.global.nc.v2.f32 	{%f111, %f112}, [%rd68+1792];
	fma.rn.f32 	%f115, %f89, %f111, %f88;
	fma.rn.f32 	%f116, %f90, %f112, %f115;
	ld.global.nc.v2.f32 	{%f117, %f118}, [%rd83];
	ld.global.nc.v2.f32 	{%f121, %f122}, [%rd65+3584];
	fma.rn.f32 	%f125, %f117, %f121, %f98;
	fma.rn.f32 	%f126, %f118, %f122, %f125;
	ld.global.nc.v2.f32 	{%f127, %f128}, [%rd69+1792];
	fma.rn.f32 	%f131, %f117, %f127, %f104;
	fma.rn.f32 	%f132, %f118, %f128, %f131;
	ld.global.nc.v2.f32 	{%f133, %f134}, [%rd67+3584];
	fma.rn.f32 	%f137, %f117, %f133, %f110;
	fma.rn.f32 	%f138, %f118, %f134, %f137;
	ld.global.nc.v2.f32 	{%f139, %f140}, [%rd68+3584];
	fma.rn.f32 	%f143, %f117, %f139, %f116;
	fma.rn.f32 	%f144, %f118, %f140, %f143;
	ld.global.nc.v2.f32 	{%f145, %f146}, [%rd83+1792];
	ld.global.nc.v2.f32 	{%f149, %f150}, [%rd65+5376];
	fma.rn.f32 	%f153, %f145, %f149, %f126;
	fma.rn.f32 	%f269, %f146, %f150, %f153;
	ld.global.nc.v2.f32 	{%f154, %f155}, [%rd69+3584];
	fma.rn.f32 	%f158, %f145, %f154, %f132;
	fma.rn.f32 	%f268, %f146, %f155, %f158;
	ld.global.nc.v2.f32 	{%f159, %f160}, [%rd67+5376];
	fma.rn.f32 	%f163, %f145, %f159, %f138;
	fma.rn.f32 	%f267, %f146, %f160, %f163;
	ld.global.nc.v2.f32 	{%f164, %f165}, [%rd68+5376];
	fma.rn.f32 	%f168, %f145, %f164, %f144;
	fma.rn.f32 	%f266, %f146, %f165, %f168;
	add.s64 	%rd84, %rd84, 7168;
	add.s64 	%rd83, %rd83, 7168;
	add.s32 	%r188, %r188, 896;
	setp.lt.s32 	%p6, %r188, %r15;
	@%p6 bra 	$L__BB51_9;

	st.local.v4.f32 	[%rd3], {%f269, %f268, %f267, %f266};

$L__BB51_11:
	shr.s32 	%r42, %r3, 31;
	shr.u32 	%r43, %r42, 27;
	add.s32 	%r44, %r3, %r43;
	shr.s32 	%r45, %r44, 5;
	shl.b32 	%r46, %r45, 2;
	add.s32 	%r14, %r28, %r46;
	mov.u32 	%r48, 2;
	mov.b32 	%r49, %f269;
	mov.u32 	%r50, 31;
	mov.u32 	%r51, 16;
	mov.u32 	%r52, -1;
	shfl.sync.bfly.b32 	%r53|%p7, %r49, %r51, %r50, %r52;
	mov.b32 	%f169, %r53;
	add.f32 	%f170, %f269, %f169;
	mov.b32 	%r54, %f170;
	mov.u32 	%r55, 8;
	shfl.sync.bfly.b32 	%r56|%p8, %r54, %r55, %r50, %r52;
	mov.b32 	%f171, %r56;
	add.f32 	%f172, %f170, %f171;
	mov.b32 	%r57, %f172;
	mov.u32 	%r58, 4;
	shfl.sync.bfly.b32 	%r59|%p9, %r57, %r58, %r50, %r52;
	mov.b32 	%f173, %r59;
	add.f32 	%f174, %f172, %f173;
	mov.b32 	%r60, %f174;
	shfl.sync.bfly.b32 	%r61|%p10, %r60, %r48, %r50, %r52;
	mov.b32 	%f175, %r61;
	add.f32 	%f176, %f174, %f175;
	mov.b32 	%r62, %f176;
	mov.u32 	%r63, 1;
	shfl.sync.bfly.b32 	%r64|%p11, %r62, %r63, %r50, %r52;
	mov.b32 	%f177, %r64;
	add.f32 	%f178, %f176, %f177;
	st.local.f32 	[%rd3], %f178;
	st.shared.f32 	[%r14], %f178;
	bar.sync 	0;
	@%p1 bra 	$L__BB51_13;

	ld.shared.f32 	%f179, [%r4];
	mov.b32 	%r65, %f179;
	shfl.sync.bfly.b32 	%r69|%p13, %r65, %r51, %r50, %r52;
	mov.b32 	%f180, %r69;
	add.f32 	%f181, %f179, %f180;
	mov.b32 	%r70, %f181;
	shfl.sync.bfly.b32 	%r72|%p14, %r70, %r55, %r50, %r52;
	mov.b32 	%f182, %r72;
	add.f32 	%f183, %f181, %f182;
	mov.b32 	%r73, %f183;
	shfl.sync.bfly.b32 	%r75|%p15, %r73, %r58, %r50, %r52;
	mov.b32 	%f184, %r75;
	add.f32 	%f185, %f183, %f184;
	mov.b32 	%r76, %f185;
	shfl.sync.bfly.b32 	%r78|%p16, %r76, %r48, %r50, %r52;
	mov.b32 	%f186, %r78;
	add.f32 	%f187, %f185, %f186;
	mov.b32 	%r79, %f187;
	shfl.sync.bfly.b32 	%r81|%p17, %r79, %r63, %r50, %r52;
	mov.b32 	%f188, %r81;
	add.f32 	%f189, %f187, %f188;
	st.local.f32 	[%rd3], %f189;

$L__BB51_13:
	bar.sync 	0;
	mov.b32 	%r82, %f268;
	shfl.sync.bfly.b32 	%r86|%p19, %r82, %r51, %r50, %r52;
	mov.b32 	%f190, %r86;
	add.f32 	%f191, %f268, %f190;
	mov.b32 	%r87, %f191;
	shfl.sync.bfly.b32 	%r89|%p20, %r87, %r55, %r50, %r52;
	mov.b32 	%f192, %r89;
	add.f32 	%f193, %f191, %f192;
	mov.b32 	%r90, %f193;
	shfl.sync.bfly.b32 	%r92|%p21, %r90, %r58, %r50, %r52;
	mov.b32 	%f194, %r92;
	add.f32 	%f195, %f193, %f194;
	mov.b32 	%r93, %f195;
	shfl.sync.bfly.b32 	%r95|%p22, %r93, %r48, %r50, %r52;
	mov.b32 	%f196, %r95;
	add.f32 	%f197, %f195, %f196;
	mov.b32 	%r96, %f197;
	shfl.sync.bfly.b32 	%r98|%p23, %r96, %r63, %r50, %r52;
	mov.b32 	%f198, %r98;
	add.f32 	%f199, %f197, %f198;
	st.local.f32 	[%rd3+4], %f199;
	st.shared.f32 	[%r14], %f199;
	bar.sync 	0;
	@%p1 bra 	$L__BB51_15;

	ld.shared.f32 	%f200, [%r4];
	mov.b32 	%r99, %f200;
	mov.u32 	%r100, 31;
	mov.u32 	%r101, 16;
	mov.u32 	%r102, -1;
	shfl.sync.bfly.b32 	%r103|%p24, %r99, %r101, %r100, %r102;
	mov.b32 	%f201, %r103;
	add.f32 	%f202, %f200, %f201;
	mov.b32 	%r104, %f202;
	mov.u32 	%r105, 8;
	shfl.sync.bfly.b32 	%r106|%p25, %r104, %r105, %r100, %r102;
	mov.b32 	%f203, %r106;
	add.f32 	%f204, %f202, %f203;
	mov.b32 	%r107, %f204;
	mov.u32 	%r108, 4;
	shfl.sync.bfly.b32 	%r109|%p26, %r107, %r108, %r100, %r102;
	mov.b32 	%f205, %r109;
	add.f32 	%f206, %f204, %f205;
	mov.b32 	%r110, %f206;
	mov.u32 	%r111, 2;
	shfl.sync.bfly.b32 	%r112|%p27, %r110, %r111, %r100, %r102;
	mov.b32 	%f207, %r112;
	add.f32 	%f208, %f206, %f207;
	mov.b32 	%r113, %f208;
	mov.u32 	%r114, 1;
	shfl.sync.bfly.b32 	%r115|%p28, %r113, %r114, %r100, %r102;
	mov.b32 	%f209, %r115;
	add.f32 	%f210, %f208, %f209;
	st.local.f32 	[%rd3+4], %f210;

$L__BB51_15:
	bar.sync 	0;
	mov.b32 	%r116, %f267;
	mov.u32 	%r117, 31;
	mov.u32 	%r118, 16;
	mov.u32 	%r119, -1;
	shfl.sync.bfly.b32 	%r120|%p30, %r116, %r118, %r117, %r119;
	mov.b32 	%f211, %r120;
	add.f32 	%f212, %f267, %f211;
	mov.b32 	%r121, %f212;
	mov.u32 	%r122, 8;
	shfl.sync.bfly.b32 	%r123|%p31, %r121, %r122, %r117, %r119;
	mov.b32 	%f213, %r123;
	add.f32 	%f214, %f212, %f213;
	mov.b32 	%r124, %f214;
	mov.u32 	%r125, 4;
	shfl.sync.bfly.b32 	%r126|%p32, %r124, %r125, %r117, %r119;
	mov.b32 	%f215, %r126;
	add.f32 	%f216, %f214, %f215;
	mov.b32 	%r127, %f216;
	mov.u32 	%r128, 2;
	shfl.sync.bfly.b32 	%r129|%p33, %r127, %r128, %r117, %r119;
	mov.b32 	%f217, %r129;
	add.f32 	%f218, %f216, %f217;
	mov.b32 	%r130, %f218;
	mov.u32 	%r131, 1;
	shfl.sync.bfly.b32 	%r132|%p34, %r130, %r131, %r117, %r119;
	mov.b32 	%f219, %r132;
	add.f32 	%f220, %f218, %f219;
	st.local.f32 	[%rd3+8], %f220;
	st.shared.f32 	[%r14], %f220;
	bar.sync 	0;
	@%p1 bra 	$L__BB51_17;

	ld.shared.f32 	%f221, [%r4];
	mov.b32 	%r133, %f221;
	shfl.sync.bfly.b32 	%r137|%p35, %r133, %r118, %r117, %r119;
	mov.b32 	%f222, %r137;
	add.f32 	%f223, %f221, %f222;
	mov.b32 	%r138, %f223;
	shfl.sync.bfly.b32 	%r140|%p36, %r138, %r122, %r117, %r119;
	mov.b32 	%f224, %r140;
	add.f32 	%f225, %f223, %f224;
	mov.b32 	%r141, %f225;
	shfl.sync.bfly.b32 	%r143|%p37, %r141, %r125, %r117, %r119;
	mov.b32 	%f226, %r143;
	add.f32 	%f227, %f225, %f226;
	mov.b32 	%r144, %f227;
	shfl.sync.bfly.b32 	%r146|%p38, %r144, %r128, %r117, %r119;
	mov.b32 	%f228, %r146;
	add.f32 	%f229, %f227, %f228;
	mov.b32 	%r147, %f229;
	shfl.sync.bfly.b32 	%r149|%p39, %r147, %r131, %r117, %r119;
	mov.b32 	%f230, %r149;
	add.f32 	%f231, %f229, %f230;
	st.local.f32 	[%rd3+8], %f231;

$L__BB51_17:
	bar.sync 	0;
	mov.b32 	%r150, %f266;
	shfl.sync.bfly.b32 	%r154|%p41, %r150, %r118, %r117, %r119;
	mov.b32 	%f232, %r154;
	add.f32 	%f233, %f266, %f232;
	mov.b32 	%r155, %f233;
	shfl.sync.bfly.b32 	%r157|%p42, %r155, %r122, %r117, %r119;
	mov.b32 	%f234, %r157;
	add.f32 	%f235, %f233, %f234;
	mov.b32 	%r158, %f235;
	shfl.sync.bfly.b32 	%r160|%p43, %r158, %r125, %r117, %r119;
	mov.b32 	%f236, %r160;
	add.f32 	%f237, %f235, %f236;
	mov.b32 	%r161, %f237;
	shfl.sync.bfly.b32 	%r163|%p44, %r161, %r128, %r117, %r119;
	mov.b32 	%f238, %r163;
	add.f32 	%f239, %f237, %f238;
	mov.b32 	%r164, %f239;
	shfl.sync.bfly.b32 	%r166|%p45, %r164, %r131, %r117, %r119;
	mov.b32 	%f240, %r166;
	add.f32 	%f241, %f239, %f240;
	st.local.f32 	[%rd3+12], %f241;
	st.shared.f32 	[%r14], %f241;
	bar.sync 	0;
	@%p1 bra 	$L__BB51_19;

	ld.shared.f32 	%f242, [%r4];
	mov.b32 	%r167, %f242;
	mov.u32 	%r168, 31;
	mov.u32 	%r169, 16;
	mov.u32 	%r170, -1;
	shfl.sync.bfly.b32 	%r171|%p46, %r167, %r169, %r168, %r170;
	mov.b32 	%f243, %r171;
	add.f32 	%f244, %f242, %f243;
	mov.b32 	%r172, %f244;
	mov.u32 	%r173, 8;
	shfl.sync.bfly.b32 	%r174|%p47, %r172, %r173, %r168, %r170;
	mov.b32 	%f245, %r174;
	add.f32 	%f246, %f244, %f245;
	mov.b32 	%r175, %f246;
	mov.u32 	%r176, 4;
	shfl.sync.bfly.b32 	%r177|%p48, %r175, %r176, %r168, %r170;
	mov.b32 	%f247, %r177;
	add.f32 	%f248, %f246, %f247;
	mov.b32 	%r178, %f248;
	mov.u32 	%r179, 2;
	shfl.sync.bfly.b32 	%r180|%p49, %r178, %r179, %r168, %r170;
	mov.b32 	%f249, %r180;
	add.f32 	%f250, %f248, %f249;
	mov.b32 	%r181, %f250;
	mov.u32 	%r182, 1;
	shfl.sync.bfly.b32 	%r183|%p50, %r181, %r182, %r168, %r170;
	mov.b32 	%f251, %r183;
	add.f32 	%f252, %f250, %f251;
	st.local.f32 	[%rd3+12], %f252;

$L__BB51_19:
	bar.sync 	0;
	setp.gt.s32 	%p51, %r3, 3;
	@%p51 bra 	$L__BB51_21;

	mul.wide.s32 	%rd70, %r3, 4;
	add.s64 	%rd71, %rd3, %rd70;
	ld.local.f32 	%f253, [%rd71];
	mad.lo.s32 	%r184, %r3, %r17, %r2;
	cvt.s64.s32 	%rd72, %r184;
	mul.lo.s32 	%r185, %r1, %r18;
	cvt.s64.s32 	%rd73, %r185;
	add.s64 	%rd74, %rd73, %rd72;
	cvta.to.global.u64 	%rd75, %rd33;
	shl.b64 	%rd76, %rd74, 2;
	add.s64 	%rd77, %rd75, %rd76;
	st.global.f32 	[%rd77], %f253;

$L__BB51_21:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_5_bs_224
.visible .entry ggml_matvec_f32_ncols_5_bs_224(
	.param .u64 ggml_matvec_f32_ncols_5_bs_224_param_0,
	.param .u64 ggml_matvec_f32_ncols_5_bs_224_param_1,
	.param .u64 ggml_matvec_f32_ncols_5_bs_224_param_2,
	.param .u32 ggml_matvec_f32_ncols_5_bs_224_param_3,
	.param .u32 ggml_matvec_f32_ncols_5_bs_224_param_4,
	.param .u32 ggml_matvec_f32_ncols_5_bs_224_param_5,
	.param .u32 ggml_matvec_f32_ncols_5_bs_224_param_6,
	.param .u32 ggml_matvec_f32_ncols_5_bs_224_param_7,
	.param .u32 ggml_matvec_f32_ncols_5_bs_224_param_8,
	.param .u32 ggml_matvec_f32_ncols_5_bs_224_param_9,
	.param .u32 ggml_matvec_f32_ncols_5_bs_224_param_10,
	.param .u32 ggml_matvec_f32_ncols_5_bs_224_param_11
)
{
	.local .align 4 .b8 	__local_depot52[20];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<63>;
	.reg .f32 	%f<332>;
	.reg .b32 	%r<226>;
	.reg .b64 	%rd<76>;


	mov.u64 	%SPL, __local_depot52;
	ld.param.u64 	%rd28, [ggml_matvec_f32_ncols_5_bs_224_param_0];
	ld.param.u64 	%rd29, [ggml_matvec_f32_ncols_5_bs_224_param_1];
	ld.param.u64 	%rd27, [ggml_matvec_f32_ncols_5_bs_224_param_2];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_5_bs_224_param_3];
	ld.param.u32 	%r19, [ggml_matvec_f32_ncols_5_bs_224_param_5];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_5_bs_224_param_6];
	ld.param.u32 	%r17, [ggml_matvec_f32_ncols_5_bs_224_param_7];
	ld.param.u32 	%r20, [ggml_matvec_f32_ncols_5_bs_224_param_8];
	ld.param.u32 	%r21, [ggml_matvec_f32_ncols_5_bs_224_param_9];
	ld.param.u32 	%r22, [ggml_matvec_f32_ncols_5_bs_224_param_10];
	ld.param.u32 	%r18, [ggml_matvec_f32_ncols_5_bs_224_param_11];
	cvta.to.global.u64 	%rd75, %rd29;
	cvta.to.global.u64 	%rd2, %rd28;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r23, %r1, %r20;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r24, %r2, %r19;
	mad.lo.s32 	%r25, %r23, %r21, %r24;
	cvt.s64.s32 	%rd4, %r25;
	mul.lo.s32 	%r26, %r1, %r22;
	cvt.s64.s32 	%rd5, %r26;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r27, %r3, 2;
	mov.u32 	%r28, data_mmv;
	add.s32 	%r4, %r28, %r27;
	@%p1 bra 	$L__BB52_2;

	mov.u32 	%r29, 0;
	st.shared.u32 	[%r4], %r29;

$L__BB52_2:
	bar.sync 	0;
	mov.f32 	%f327, 0f00000000;
	mov.u32 	%r30, 0;
	st.local.u32 	[%rd3], %r30;
	st.local.u32 	[%rd3+4], %r30;
	st.local.u32 	[%rd3+8], %r30;
	st.local.u32 	[%rd3+12], %r30;
	st.local.u32 	[%rd3+16], %r30;
	setp.ge.s32 	%p2, %r3, %r15;
	mov.f32 	%f328, %f327;
	mov.f32 	%f329, %f327;
	mov.f32 	%f330, %f327;
	mov.f32 	%f331, %f327;
	@%p2 bra 	$L__BB52_11;

	not.b32 	%r31, %r3;
	add.s32 	%r5, %r31, %r15;
	shr.u32 	%r32, %r5, 5;
	mul.wide.u32 	%rd31, %r32, 613566757;
	shr.u64 	%rd32, %rd31, 32;
	cvt.u32.u64 	%r33, %rd32;
	add.s32 	%r34, %r33, 1;
	and.b32  	%r223, %r34, 3;
	setp.eq.s32 	%p3, %r223, 0;
	mov.f32 	%f327, 0f00000000;
	mov.u32 	%r224, %r3;
	@%p3 bra 	$L__BB52_7;

	shl.b32 	%r35, %r16, 1;
	mad.lo.s32 	%r36, %r16, 3, %r3;
	mul.wide.s32 	%rd33, %r36, 8;
	shl.b64 	%rd34, %rd5, 2;
	add.s64 	%rd7, %rd33, %rd34;
	mul.wide.s32 	%rd35, %r3, 8;
	mul.wide.s32 	%rd36, %r16, 8;
	add.s64 	%rd37, %rd35, %rd36;
	add.s64 	%rd8, %rd37, %rd34;
	add.s64 	%rd9, %rd35, %rd34;
	mul.wide.s32 	%rd38, %r3, 2;
	add.s64 	%rd39, %rd38, %rd4;
	shl.b64 	%rd40, %rd39, 2;
	add.s64 	%rd72, %rd2, %rd40;
	mul.wide.s32 	%rd11, %r35, 8;
	mov.f32 	%f327, 0f00000000;
	mov.u64 	%rd73, %rd75;
	mov.f32 	%f328, %f327;
	mov.f32 	%f329, %f327;
	mov.f32 	%f330, %f327;
	mov.f32 	%f331, %f327;
	mov.u32 	%r224, %r3;

$L__BB52_5:
	.pragma "nounroll";
	ld.global.nc.v2.f32 	{%f46, %f47}, [%rd72];
	add.s64 	%rd41, %rd73, %rd9;
	ld.global.nc.v2.f32 	{%f50, %f51}, [%rd41];
	fma.rn.f32 	%f54, %f46, %f50, %f331;
	fma.rn.f32 	%f331, %f47, %f51, %f54;
	add.s64 	%rd42, %rd73, %rd8;
	ld.global.nc.v2.f32 	{%f55, %f56}, [%rd42];
	fma.rn.f32 	%f59, %f46, %f55, %f330;
	fma.rn.f32 	%f330, %f47, %f56, %f59;
	add.s64 	%rd43, %rd41, %rd11;
	ld.global.nc.v2.f32 	{%f60, %f61}, [%rd43];
	fma.rn.f32 	%f64, %f46, %f60, %f329;
	fma.rn.f32 	%f329, %f47, %f61, %f64;
	add.s64 	%rd44, %rd73, %rd7;
	ld.global.nc.v2.f32 	{%f65, %f66}, [%rd44];
	fma.rn.f32 	%f69, %f46, %f65, %f328;
	fma.rn.f32 	%f328, %f47, %f66, %f69;
	add.s64 	%rd45, %rd43, %rd11;
	ld.global.nc.v2.f32 	{%f70, %f71}, [%rd45];
	fma.rn.f32 	%f74, %f46, %f70, %f327;
	fma.rn.f32 	%f327, %f47, %f71, %f74;
	add.s32 	%r224, %r224, 224;
	add.s64 	%rd73, %rd73, 1792;
	add.s64 	%rd72, %rd72, 1792;
	add.s32 	%r223, %r223, -1;
	setp.ne.s32 	%p4, %r223, 0;
	@%p4 bra 	$L__BB52_5;

	st.local.f32 	[%rd3], %f331;
	st.local.f32 	[%rd3+4], %f330;
	st.local.f32 	[%rd3+8], %f329;
	st.local.f32 	[%rd3+12], %f328;
	st.local.f32 	[%rd3+16], %f327;

$L__BB52_7:
	setp.lt.u32 	%p5, %r5, 672;
	@%p5 bra 	$L__BB52_11;

	add.s32 	%r37, %r224, %r16;
	shl.b32 	%r38, %r16, 1;
	add.s32 	%r39, %r224, %r38;
	mad.lo.s32 	%r40, %r16, 3, %r224;
	shl.b32 	%r41, %r16, 2;
	add.s32 	%r42, %r224, %r41;
	add.s32 	%r43, %r37, 224;
	mul.wide.s32 	%rd46, %r43, 8;
	shl.b64 	%rd47, %rd5, 2;
	add.s64 	%rd16, %rd46, %rd47;
	mul.wide.s32 	%rd48, %r39, 8;
	add.s64 	%rd17, %rd48, %rd47;
	mul.wide.s32 	%rd49, %r40, 8;
	add.s64 	%rd18, %rd49, %rd47;
	mul.wide.s32 	%rd50, %r42, 8;
	add.s64 	%rd19, %rd50, %rd47;
	mul.wide.s32 	%rd51, %r224, 2;
	add.s64 	%rd52, %rd51, %rd4;
	shl.b64 	%rd53, %rd52, 2;
	add.s64 	%rd54, %rd2, %rd53;
	add.s64 	%rd74, %rd54, 3584;
	mul.wide.s32 	%rd55, %r224, 8;
	add.s64 	%rd21, %rd55, %rd47;
	mul.wide.s32 	%rd56, %r16, 8;
	add.s64 	%rd57, %rd55, %rd56;
	add.s64 	%rd22, %rd57, %rd47;

$L__BB52_9:
	ld.global.nc.v2.f32 	{%f75, %f76}, [%rd74+-3584];
	add.s64 	%rd58, %rd75, %rd21;
	ld.global.nc.v2.f32 	{%f79, %f80}, [%rd58];
	fma.rn.f32 	%f83, %f75, %f79, %f331;
	fma.rn.f32 	%f84, %f76, %f80, %f83;
	add.s64 	%rd59, %rd75, %rd22;
	ld.global.nc.v2.f32 	{%f85, %f86}, [%rd59];
	fma.rn.f32 	%f89, %f75, %f85, %f330;
	fma.rn.f32 	%f90, %f76, %f86, %f89;
	add.s64 	%rd60, %rd75, %rd17;
	ld.global.nc.v2.f32 	{%f91, %f92}, [%rd60];
	fma.rn.f32 	%f95, %f75, %f91, %f329;
	fma.rn.f32 	%f96, %f76, %f92, %f95;
	add.s64 	%rd61, %rd75, %rd18;
	ld.global.nc.v2.f32 	{%f97, %f98}, [%rd61];
	fma.rn.f32 	%f101, %f75, %f97, %f328;
	fma.rn.f32 	%f102, %f76, %f98, %f101;
	add.s64 	%rd62, %rd75, %rd19;
	ld.global.nc.v2.f32 	{%f103, %f104}, [%rd62];
	fma.rn.f32 	%f107, %f75, %f103, %f327;
	fma.rn.f32 	%f108, %f76, %f104, %f107;
	ld.global.nc.v2.f32 	{%f109, %f110}, [%rd74+-1792];
	ld.global.nc.v2.f32 	{%f113, %f114}, [%rd58+1792];
	fma.rn.f32 	%f117, %f109, %f113, %f84;
	fma.rn.f32 	%f118, %f110, %f114, %f117;
	add.s64 	%rd63, %rd75, %rd16;
	ld.global.nc.v2.f32 	{%f119, %f120}, [%rd63];
	fma.rn.f32 	%f123, %f109, %f119, %f90;
	fma.rn.f32 	%f124, %f110, %f120, %f123;
	ld.global.nc.v2.f32 	{%f125, %f126}, [%rd60+1792];
	fma.rn.f32 	%f129, %f109, %f125, %f96;
	fma.rn.f32 	%f130, %f110, %f126, %f129;
	ld.global.nc.v2.f32 	{%f131, %f132}, [%rd61+1792];
	fma.rn.f32 	%f135, %f109, %f131, %f102;
	fma.rn.f32 	%f136, %f110, %f132, %f135;
	ld.global.nc.v2.f32 	{%f137, %f138}, [%rd62+1792];
	fma.rn.f32 	%f141, %f109, %f137, %f108;
	fma.rn.f32 	%f142, %f110, %f138, %f141;
	ld.global.nc.v2.f32 	{%f143, %f144}, [%rd74];
	ld.global.nc.v2.f32 	{%f147, %f148}, [%rd58+3584];
	fma.rn.f32 	%f151, %f143, %f147, %f118;
	fma.rn.f32 	%f152, %f144, %f148, %f151;
	ld.global.nc.v2.f32 	{%f153, %f154}, [%rd63+1792];
	fma.rn.f32 	%f157, %f143, %f153, %f124;
	fma.rn.f32 	%f158, %f144, %f154, %f157;
	ld.global.nc.v2.f32 	{%f159, %f160}, [%rd60+3584];
	fma.rn.f32 	%f163, %f143, %f159, %f130;
	fma.rn.f32 	%f164, %f144, %f160, %f163;
	ld.global.nc.v2.f32 	{%f165, %f166}, [%rd61+3584];
	fma.rn.f32 	%f169, %f143, %f165, %f136;
	fma.rn.f32 	%f170, %f144, %f166, %f169;
	ld.global.nc.v2.f32 	{%f171, %f172}, [%rd62+3584];
	fma.rn.f32 	%f175, %f143, %f171, %f142;
	fma.rn.f32 	%f176, %f144, %f172, %f175;
	ld.global.nc.v2.f32 	{%f177, %f178}, [%rd74+1792];
	ld.global.nc.v2.f32 	{%f181, %f182}, [%rd58+5376];
	fma.rn.f32 	%f185, %f177, %f181, %f152;
	fma.rn.f32 	%f331, %f178, %f182, %f185;
	ld.global.nc.v2.f32 	{%f186, %f187}, [%rd63+3584];
	fma.rn.f32 	%f190, %f177, %f186, %f158;
	fma.rn.f32 	%f330, %f178, %f187, %f190;
	ld.global.nc.v2.f32 	{%f191, %f192}, [%rd60+5376];
	fma.rn.f32 	%f195, %f177, %f191, %f164;
	fma.rn.f32 	%f329, %f178, %f192, %f195;
	ld.global.nc.v2.f32 	{%f196, %f197}, [%rd61+5376];
	fma.rn.f32 	%f200, %f177, %f196, %f170;
	fma.rn.f32 	%f328, %f178, %f197, %f200;
	ld.global.nc.v2.f32 	{%f201, %f202}, [%rd62+5376];
	fma.rn.f32 	%f205, %f177, %f201, %f176;
	fma.rn.f32 	%f327, %f178, %f202, %f205;
	add.s64 	%rd75, %rd75, 7168;
	add.s64 	%rd74, %rd74, 7168;
	add.s32 	%r224, %r224, 896;
	setp.lt.s32 	%p6, %r224, %r15;
	@%p6 bra 	$L__BB52_9;

	st.local.f32 	[%rd3], %f331;
	st.local.f32 	[%rd3+4], %f330;
	st.local.f32 	[%rd3+8], %f329;
	st.local.f32 	[%rd3+12], %f328;
	st.local.f32 	[%rd3+16], %f327;

$L__BB52_11:
	shr.s32 	%r44, %r3, 31;
	shr.u32 	%r45, %r44, 27;
	add.s32 	%r46, %r3, %r45;
	shr.s32 	%r47, %r46, 5;
	shl.b32 	%r48, %r47, 2;
	add.s32 	%r14, %r28, %r48;
	mov.u32 	%r50, 2;
	mov.b32 	%r51, %f331;
	mov.u32 	%r52, 31;
	mov.u32 	%r53, 16;
	mov.u32 	%r54, -1;
	shfl.sync.bfly.b32 	%r55|%p7, %r51, %r53, %r52, %r54;
	mov.b32 	%f206, %r55;
	add.f32 	%f207, %f331, %f206;
	mov.b32 	%r56, %f207;
	mov.u32 	%r57, 8;
	shfl.sync.bfly.b32 	%r58|%p8, %r56, %r57, %r52, %r54;
	mov.b32 	%f208, %r58;
	add.f32 	%f209, %f207, %f208;
	mov.b32 	%r59, %f209;
	mov.u32 	%r60, 4;
	shfl.sync.bfly.b32 	%r61|%p9, %r59, %r60, %r52, %r54;
	mov.b32 	%f210, %r61;
	add.f32 	%f211, %f209, %f210;
	mov.b32 	%r62, %f211;
	shfl.sync.bfly.b32 	%r63|%p10, %r62, %r50, %r52, %r54;
	mov.b32 	%f212, %r63;
	add.f32 	%f213, %f211, %f212;
	mov.b32 	%r64, %f213;
	mov.u32 	%r65, 1;
	shfl.sync.bfly.b32 	%r66|%p11, %r64, %r65, %r52, %r54;
	mov.b32 	%f214, %r66;
	add.f32 	%f215, %f213, %f214;
	st.local.f32 	[%rd3], %f215;
	st.shared.f32 	[%r14], %f215;
	bar.sync 	0;
	@%p1 bra 	$L__BB52_13;

	ld.shared.f32 	%f216, [%r4];
	mov.b32 	%r67, %f216;
	shfl.sync.bfly.b32 	%r71|%p13, %r67, %r53, %r52, %r54;
	mov.b32 	%f217, %r71;
	add.f32 	%f218, %f216, %f217;
	mov.b32 	%r72, %f218;
	shfl.sync.bfly.b32 	%r74|%p14, %r72, %r57, %r52, %r54;
	mov.b32 	%f219, %r74;
	add.f32 	%f220, %f218, %f219;
	mov.b32 	%r75, %f220;
	shfl.sync.bfly.b32 	%r77|%p15, %r75, %r60, %r52, %r54;
	mov.b32 	%f221, %r77;
	add.f32 	%f222, %f220, %f221;
	mov.b32 	%r78, %f222;
	shfl.sync.bfly.b32 	%r80|%p16, %r78, %r50, %r52, %r54;
	mov.b32 	%f223, %r80;
	add.f32 	%f224, %f222, %f223;
	mov.b32 	%r81, %f224;
	shfl.sync.bfly.b32 	%r83|%p17, %r81, %r65, %r52, %r54;
	mov.b32 	%f225, %r83;
	add.f32 	%f226, %f224, %f225;
	st.local.f32 	[%rd3], %f226;

$L__BB52_13:
	bar.sync 	0;
	mov.b32 	%r84, %f330;
	shfl.sync.bfly.b32 	%r88|%p19, %r84, %r53, %r52, %r54;
	mov.b32 	%f227, %r88;
	add.f32 	%f228, %f330, %f227;
	mov.b32 	%r89, %f228;
	shfl.sync.bfly.b32 	%r91|%p20, %r89, %r57, %r52, %r54;
	mov.b32 	%f229, %r91;
	add.f32 	%f230, %f228, %f229;
	mov.b32 	%r92, %f230;
	shfl.sync.bfly.b32 	%r94|%p21, %r92, %r60, %r52, %r54;
	mov.b32 	%f231, %r94;
	add.f32 	%f232, %f230, %f231;
	mov.b32 	%r95, %f232;
	shfl.sync.bfly.b32 	%r97|%p22, %r95, %r50, %r52, %r54;
	mov.b32 	%f233, %r97;
	add.f32 	%f234, %f232, %f233;
	mov.b32 	%r98, %f234;
	shfl.sync.bfly.b32 	%r100|%p23, %r98, %r65, %r52, %r54;
	mov.b32 	%f235, %r100;
	add.f32 	%f236, %f234, %f235;
	st.local.f32 	[%rd3+4], %f236;
	st.shared.f32 	[%r14], %f236;
	bar.sync 	0;
	@%p1 bra 	$L__BB52_15;

	ld.shared.f32 	%f237, [%r4];
	mov.b32 	%r101, %f237;
	mov.u32 	%r102, 31;
	mov.u32 	%r103, 16;
	mov.u32 	%r104, -1;
	shfl.sync.bfly.b32 	%r105|%p24, %r101, %r103, %r102, %r104;
	mov.b32 	%f238, %r105;
	add.f32 	%f239, %f237, %f238;
	mov.b32 	%r106, %f239;
	mov.u32 	%r107, 8;
	shfl.sync.bfly.b32 	%r108|%p25, %r106, %r107, %r102, %r104;
	mov.b32 	%f240, %r108;
	add.f32 	%f241, %f239, %f240;
	mov.b32 	%r109, %f241;
	mov.u32 	%r110, 4;
	shfl.sync.bfly.b32 	%r111|%p26, %r109, %r110, %r102, %r104;
	mov.b32 	%f242, %r111;
	add.f32 	%f243, %f241, %f242;
	mov.b32 	%r112, %f243;
	mov.u32 	%r113, 2;
	shfl.sync.bfly.b32 	%r114|%p27, %r112, %r113, %r102, %r104;
	mov.b32 	%f244, %r114;
	add.f32 	%f245, %f243, %f244;
	mov.b32 	%r115, %f245;
	mov.u32 	%r116, 1;
	shfl.sync.bfly.b32 	%r117|%p28, %r115, %r116, %r102, %r104;
	mov.b32 	%f246, %r117;
	add.f32 	%f247, %f245, %f246;
	st.local.f32 	[%rd3+4], %f247;

$L__BB52_15:
	bar.sync 	0;
	mov.b32 	%r118, %f329;
	mov.u32 	%r119, 31;
	mov.u32 	%r120, 16;
	mov.u32 	%r121, -1;
	shfl.sync.bfly.b32 	%r122|%p30, %r118, %r120, %r119, %r121;
	mov.b32 	%f248, %r122;
	add.f32 	%f249, %f329, %f248;
	mov.b32 	%r123, %f249;
	mov.u32 	%r124, 8;
	shfl.sync.bfly.b32 	%r125|%p31, %r123, %r124, %r119, %r121;
	mov.b32 	%f250, %r125;
	add.f32 	%f251, %f249, %f250;
	mov.b32 	%r126, %f251;
	mov.u32 	%r127, 4;
	shfl.sync.bfly.b32 	%r128|%p32, %r126, %r127, %r119, %r121;
	mov.b32 	%f252, %r128;
	add.f32 	%f253, %f251, %f252;
	mov.b32 	%r129, %f253;
	mov.u32 	%r130, 2;
	shfl.sync.bfly.b32 	%r131|%p33, %r129, %r130, %r119, %r121;
	mov.b32 	%f254, %r131;
	add.f32 	%f255, %f253, %f254;
	mov.b32 	%r132, %f255;
	mov.u32 	%r133, 1;
	shfl.sync.bfly.b32 	%r134|%p34, %r132, %r133, %r119, %r121;
	mov.b32 	%f256, %r134;
	add.f32 	%f257, %f255, %f256;
	st.local.f32 	[%rd3+8], %f257;
	st.shared.f32 	[%r14], %f257;
	bar.sync 	0;
	@%p1 bra 	$L__BB52_17;

	ld.shared.f32 	%f258, [%r4];
	mov.b32 	%r135, %f258;
	shfl.sync.bfly.b32 	%r139|%p35, %r135, %r120, %r119, %r121;
	mov.b32 	%f259, %r139;
	add.f32 	%f260, %f258, %f259;
	mov.b32 	%r140, %f260;
	shfl.sync.bfly.b32 	%r142|%p36, %r140, %r124, %r119, %r121;
	mov.b32 	%f261, %r142;
	add.f32 	%f262, %f260, %f261;
	mov.b32 	%r143, %f262;
	shfl.sync.bfly.b32 	%r145|%p37, %r143, %r127, %r119, %r121;
	mov.b32 	%f263, %r145;
	add.f32 	%f264, %f262, %f263;
	mov.b32 	%r146, %f264;
	shfl.sync.bfly.b32 	%r148|%p38, %r146, %r130, %r119, %r121;
	mov.b32 	%f265, %r148;
	add.f32 	%f266, %f264, %f265;
	mov.b32 	%r149, %f266;
	shfl.sync.bfly.b32 	%r151|%p39, %r149, %r133, %r119, %r121;
	mov.b32 	%f267, %r151;
	add.f32 	%f268, %f266, %f267;
	st.local.f32 	[%rd3+8], %f268;

$L__BB52_17:
	bar.sync 	0;
	mov.b32 	%r152, %f328;
	shfl.sync.bfly.b32 	%r156|%p41, %r152, %r120, %r119, %r121;
	mov.b32 	%f269, %r156;
	add.f32 	%f270, %f328, %f269;
	mov.b32 	%r157, %f270;
	shfl.sync.bfly.b32 	%r159|%p42, %r157, %r124, %r119, %r121;
	mov.b32 	%f271, %r159;
	add.f32 	%f272, %f270, %f271;
	mov.b32 	%r160, %f272;
	shfl.sync.bfly.b32 	%r162|%p43, %r160, %r127, %r119, %r121;
	mov.b32 	%f273, %r162;
	add.f32 	%f274, %f272, %f273;
	mov.b32 	%r163, %f274;
	shfl.sync.bfly.b32 	%r165|%p44, %r163, %r130, %r119, %r121;
	mov.b32 	%f275, %r165;
	add.f32 	%f276, %f274, %f275;
	mov.b32 	%r166, %f276;
	shfl.sync.bfly.b32 	%r168|%p45, %r166, %r133, %r119, %r121;
	mov.b32 	%f277, %r168;
	add.f32 	%f278, %f276, %f277;
	st.local.f32 	[%rd3+12], %f278;
	st.shared.f32 	[%r14], %f278;
	bar.sync 	0;
	@%p1 bra 	$L__BB52_19;

	ld.shared.f32 	%f279, [%r4];
	mov.b32 	%r169, %f279;
	mov.u32 	%r170, 31;
	mov.u32 	%r171, 16;
	mov.u32 	%r172, -1;
	shfl.sync.bfly.b32 	%r173|%p46, %r169, %r171, %r170, %r172;
	mov.b32 	%f280, %r173;
	add.f32 	%f281, %f279, %f280;
	mov.b32 	%r174, %f281;
	mov.u32 	%r175, 8;
	shfl.sync.bfly.b32 	%r176|%p47, %r174, %r175, %r170, %r172;
	mov.b32 	%f282, %r176;
	add.f32 	%f283, %f281, %f282;
	mov.b32 	%r177, %f283;
	mov.u32 	%r178, 4;
	shfl.sync.bfly.b32 	%r179|%p48, %r177, %r178, %r170, %r172;
	mov.b32 	%f284, %r179;
	add.f32 	%f285, %f283, %f284;
	mov.b32 	%r180, %f285;
	mov.u32 	%r181, 2;
	shfl.sync.bfly.b32 	%r182|%p49, %r180, %r181, %r170, %r172;
	mov.b32 	%f286, %r182;
	add.f32 	%f287, %f285, %f286;
	mov.b32 	%r183, %f287;
	mov.u32 	%r184, 1;
	shfl.sync.bfly.b32 	%r185|%p50, %r183, %r184, %r170, %r172;
	mov.b32 	%f288, %r185;
	add.f32 	%f289, %f287, %f288;
	st.local.f32 	[%rd3+12], %f289;

$L__BB52_19:
	bar.sync 	0;
	mov.b32 	%r186, %f327;
	mov.u32 	%r187, 31;
	mov.u32 	%r188, 16;
	mov.u32 	%r189, -1;
	shfl.sync.bfly.b32 	%r190|%p52, %r186, %r188, %r187, %r189;
	mov.b32 	%f290, %r190;
	add.f32 	%f291, %f327, %f290;
	mov.b32 	%r191, %f291;
	mov.u32 	%r192, 8;
	shfl.sync.bfly.b32 	%r193|%p53, %r191, %r192, %r187, %r189;
	mov.b32 	%f292, %r193;
	add.f32 	%f293, %f291, %f292;
	mov.b32 	%r194, %f293;
	mov.u32 	%r195, 4;
	shfl.sync.bfly.b32 	%r196|%p54, %r194, %r195, %r187, %r189;
	mov.b32 	%f294, %r196;
	add.f32 	%f295, %f293, %f294;
	mov.b32 	%r197, %f295;
	mov.u32 	%r198, 2;
	shfl.sync.bfly.b32 	%r199|%p55, %r197, %r198, %r187, %r189;
	mov.b32 	%f296, %r199;
	add.f32 	%f297, %f295, %f296;
	mov.b32 	%r200, %f297;
	mov.u32 	%r201, 1;
	shfl.sync.bfly.b32 	%r202|%p56, %r200, %r201, %r187, %r189;
	mov.b32 	%f298, %r202;
	add.f32 	%f299, %f297, %f298;
	st.local.f32 	[%rd3+16], %f299;
	st.shared.f32 	[%r14], %f299;
	bar.sync 	0;
	@%p1 bra 	$L__BB52_21;

	ld.shared.f32 	%f300, [%r4];
	mov.b32 	%r203, %f300;
	shfl.sync.bfly.b32 	%r207|%p57, %r203, %r188, %r187, %r189;
	mov.b32 	%f301, %r207;
	add.f32 	%f302, %f300, %f301;
	mov.b32 	%r208, %f302;
	shfl.sync.bfly.b32 	%r210|%p58, %r208, %r192, %r187, %r189;
	mov.b32 	%f303, %r210;
	add.f32 	%f304, %f302, %f303;
	mov.b32 	%r211, %f304;
	shfl.sync.bfly.b32 	%r213|%p59, %r211, %r195, %r187, %r189;
	mov.b32 	%f305, %r213;
	add.f32 	%f306, %f304, %f305;
	mov.b32 	%r214, %f306;
	shfl.sync.bfly.b32 	%r216|%p60, %r214, %r198, %r187, %r189;
	mov.b32 	%f307, %r216;
	add.f32 	%f308, %f306, %f307;
	mov.b32 	%r217, %f308;
	shfl.sync.bfly.b32 	%r219|%p61, %r217, %r201, %r187, %r189;
	mov.b32 	%f309, %r219;
	add.f32 	%f310, %f308, %f309;
	st.local.f32 	[%rd3+16], %f310;

$L__BB52_21:
	bar.sync 	0;
	setp.gt.s32 	%p62, %r3, 4;
	@%p62 bra 	$L__BB52_23;

	mul.wide.s32 	%rd64, %r3, 4;
	add.s64 	%rd65, %rd3, %rd64;
	ld.local.f32 	%f311, [%rd65];
	mad.lo.s32 	%r220, %r3, %r17, %r2;
	cvt.s64.s32 	%rd66, %r220;
	mul.lo.s32 	%r221, %r1, %r18;
	cvt.s64.s32 	%rd67, %r221;
	add.s64 	%rd68, %rd67, %rd66;
	cvta.to.global.u64 	%rd69, %rd27;
	shl.b64 	%rd70, %rd68, 2;
	add.s64 	%rd71, %rd69, %rd70;
	st.global.f32 	[%rd71], %f311;

$L__BB52_23:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_6_bs_224
.visible .entry ggml_matvec_f32_ncols_6_bs_224(
	.param .u64 ggml_matvec_f32_ncols_6_bs_224_param_0,
	.param .u64 ggml_matvec_f32_ncols_6_bs_224_param_1,
	.param .u64 ggml_matvec_f32_ncols_6_bs_224_param_2,
	.param .u32 ggml_matvec_f32_ncols_6_bs_224_param_3,
	.param .u32 ggml_matvec_f32_ncols_6_bs_224_param_4,
	.param .u32 ggml_matvec_f32_ncols_6_bs_224_param_5,
	.param .u32 ggml_matvec_f32_ncols_6_bs_224_param_6,
	.param .u32 ggml_matvec_f32_ncols_6_bs_224_param_7,
	.param .u32 ggml_matvec_f32_ncols_6_bs_224_param_8,
	.param .u32 ggml_matvec_f32_ncols_6_bs_224_param_9,
	.param .u32 ggml_matvec_f32_ncols_6_bs_224_param_10,
	.param .u32 ggml_matvec_f32_ncols_6_bs_224_param_11
)
{
	.local .align 8 .b8 	__local_depot53[24];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<75>;
	.reg .f32 	%f<296>;
	.reg .b32 	%r<252>;
	.reg .b64 	%rd<70>;


	mov.u64 	%SPL, __local_depot53;
	ld.param.u64 	%rd20, [ggml_matvec_f32_ncols_6_bs_224_param_0];
	ld.param.u64 	%rd21, [ggml_matvec_f32_ncols_6_bs_224_param_1];
	ld.param.u64 	%rd19, [ggml_matvec_f32_ncols_6_bs_224_param_2];
	ld.param.u32 	%r11, [ggml_matvec_f32_ncols_6_bs_224_param_3];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_6_bs_224_param_5];
	ld.param.u32 	%r12, [ggml_matvec_f32_ncols_6_bs_224_param_6];
	ld.param.u32 	%r13, [ggml_matvec_f32_ncols_6_bs_224_param_7];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_6_bs_224_param_8];
	ld.param.u32 	%r17, [ggml_matvec_f32_ncols_6_bs_224_param_9];
	ld.param.u32 	%r18, [ggml_matvec_f32_ncols_6_bs_224_param_10];
	ld.param.u32 	%r14, [ggml_matvec_f32_ncols_6_bs_224_param_11];
	cvta.to.global.u64 	%rd69, %rd21;
	add.u64 	%rd2, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r19, %r1, %r16;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r20, %r2, %r15;
	mad.lo.s32 	%r21, %r19, %r17, %r20;
	cvt.s64.s32 	%rd3, %r21;
	cvta.to.global.u64 	%rd4, %rd20;
	mul.lo.s32 	%r22, %r1, %r18;
	cvt.s64.s32 	%rd5, %r22;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r23, %r3, 2;
	mov.u32 	%r24, data_mmv;
	add.s32 	%r4, %r24, %r23;
	@%p1 bra 	$L__BB53_2;

	mov.u32 	%r25, 0;
	st.shared.u32 	[%r4], %r25;

$L__BB53_2:
	bar.sync 	0;
	mov.f32 	%f290, 0f00000000;
	st.local.v2.f32 	[%rd2], {%f290, %f290};
	st.local.v2.f32 	[%rd2+8], {%f290, %f290};
	st.local.v2.f32 	[%rd2+16], {%f290, %f290};
	setp.ge.s32 	%p2, %r3, %r11;
	mov.f32 	%f291, %f290;
	mov.f32 	%f292, %f290;
	mov.f32 	%f293, %f290;
	mov.f32 	%f294, %f290;
	mov.f32 	%f295, %f290;
	@%p2 bra 	$L__BB53_9;

	not.b32 	%r26, %r3;
	add.s32 	%r5, %r26, %r11;
	shr.u32 	%r27, %r5, 5;
	mul.wide.u32 	%rd23, %r27, 613566757;
	shr.u64 	%rd24, %rd23, 32;
	and.b64  	%rd25, %rd24, 1;
	setp.eq.b64 	%p3, %rd25, 1;
	mov.pred 	%p4, 0;
	xor.pred  	%p5, %p3, %p4;
	mov.f32 	%f290, 0f00000000;
	mov.u32 	%r251, %r3;
	@%p5 bra 	$L__BB53_5;

	shl.b64 	%rd26, %rd5, 2;
	add.s64 	%rd27, %rd69, %rd26;
	shl.b64 	%rd28, %rd3, 2;
	add.s64 	%rd29, %rd4, %rd28;
	mul.wide.s32 	%rd30, %r3, 8;
	add.s64 	%rd31, %rd29, %rd30;
	ld.global.nc.v2.f32 	{%f43, %f44}, [%rd31];
	add.s64 	%rd32, %rd27, %rd30;
	ld.global.nc.v2.f32 	{%f47, %f48}, [%rd32];
	fma.rn.f32 	%f51, %f43, %f47, 0f00000000;
	fma.rn.f32 	%f295, %f44, %f48, %f51;
	mul.wide.s32 	%rd33, %r12, 8;
	add.s64 	%rd34, %rd32, %rd33;
	ld.global.nc.v2.f32 	{%f52, %f53}, [%rd34];
	fma.rn.f32 	%f56, %f43, %f52, 0f00000000;
	fma.rn.f32 	%f294, %f44, %f53, %f56;
	st.local.v2.f32 	[%rd2], {%f295, %f294};
	add.s32 	%r28, %r3, %r12;
	add.s32 	%r29, %r28, %r12;
	mul.wide.s32 	%rd35, %r29, 8;
	add.s64 	%rd36, %rd27, %rd35;
	ld.global.nc.v2.f32 	{%f57, %f58}, [%rd36];
	fma.rn.f32 	%f61, %f43, %f57, 0f00000000;
	fma.rn.f32 	%f293, %f44, %f58, %f61;
	add.s64 	%rd37, %rd36, %rd33;
	ld.global.nc.v2.f32 	{%f62, %f63}, [%rd37];
	fma.rn.f32 	%f66, %f43, %f62, 0f00000000;
	fma.rn.f32 	%f292, %f44, %f63, %f66;
	st.local.v2.f32 	[%rd2+8], {%f293, %f292};
	add.s64 	%rd38, %rd37, %rd33;
	ld.global.nc.v2.f32 	{%f67, %f68}, [%rd38];
	fma.rn.f32 	%f71, %f43, %f67, 0f00000000;
	fma.rn.f32 	%f291, %f44, %f68, %f71;
	add.s64 	%rd39, %rd38, %rd33;
	ld.global.nc.v2.f32 	{%f72, %f73}, [%rd39];
	fma.rn.f32 	%f76, %f43, %f72, 0f00000000;
	fma.rn.f32 	%f290, %f44, %f73, %f76;
	st.local.v2.f32 	[%rd2+16], {%f291, %f290};
	add.s32 	%r251, %r3, 224;

$L__BB53_5:
	setp.lt.u32 	%p6, %r5, 224;
	@%p6 bra 	$L__BB53_9;

	add.s32 	%r30, %r251, %r12;
	add.s32 	%r31, %r30, 224;
	mul.wide.s32 	%rd40, %r31, 8;
	shl.b64 	%rd41, %rd5, 2;
	add.s64 	%rd7, %rd40, %rd41;
	shl.b32 	%r32, %r12, 1;
	add.s32 	%r33, %r251, %r32;
	mad.lo.s32 	%r34, %r12, 3, %r251;
	shl.b32 	%r35, %r12, 2;
	add.s32 	%r36, %r251, %r35;
	mad.lo.s32 	%r37, %r12, 5, %r251;
	mul.wide.s32 	%rd42, %r33, 8;
	add.s64 	%rd8, %rd42, %rd41;
	mul.wide.s32 	%rd43, %r34, 8;
	add.s64 	%rd9, %rd43, %rd41;
	mul.wide.s32 	%rd44, %r36, 8;
	add.s64 	%rd10, %rd44, %rd41;
	mul.wide.s32 	%rd45, %r37, 8;
	add.s64 	%rd11, %rd45, %rd41;
	mul.wide.s32 	%rd46, %r251, 2;
	add.s64 	%rd47, %rd46, %rd3;
	shl.b64 	%rd48, %rd47, 2;
	add.s64 	%rd49, %rd4, %rd48;
	add.s64 	%rd68, %rd49, 1792;
	mul.wide.s32 	%rd50, %r251, 8;
	mul.wide.s32 	%rd51, %r12, 8;
	add.s64 	%rd52, %rd50, %rd51;
	add.s64 	%rd13, %rd52, %rd41;
	add.s64 	%rd14, %rd50, %rd41;

$L__BB53_7:
	ld.global.nc.v2.f32 	{%f77, %f78}, [%rd68+-1792];
	add.s64 	%rd53, %rd69, %rd14;
	ld.global.nc.v2.f32 	{%f81, %f82}, [%rd53];
	fma.rn.f32 	%f85, %f77, %f81, %f295;
	fma.rn.f32 	%f86, %f78, %f82, %f85;
	add.s64 	%rd54, %rd69, %rd13;
	ld.global.nc.v2.f32 	{%f87, %f88}, [%rd54];
	fma.rn.f32 	%f91, %f77, %f87, %f294;
	fma.rn.f32 	%f92, %f78, %f88, %f91;
	add.s64 	%rd55, %rd69, %rd8;
	ld.global.nc.v2.f32 	{%f93, %f94}, [%rd55];
	fma.rn.f32 	%f97, %f77, %f93, %f293;
	fma.rn.f32 	%f98, %f78, %f94, %f97;
	add.s64 	%rd56, %rd69, %rd9;
	ld.global.nc.v2.f32 	{%f99, %f100}, [%rd56];
	fma.rn.f32 	%f103, %f77, %f99, %f292;
	fma.rn.f32 	%f104, %f78, %f100, %f103;
	add.s64 	%rd57, %rd69, %rd10;
	ld.global.nc.v2.f32 	{%f105, %f106}, [%rd57];
	fma.rn.f32 	%f109, %f77, %f105, %f291;
	fma.rn.f32 	%f110, %f78, %f106, %f109;
	add.s64 	%rd58, %rd69, %rd11;
	ld.global.nc.v2.f32 	{%f111, %f112}, [%rd58];
	fma.rn.f32 	%f115, %f77, %f111, %f290;
	fma.rn.f32 	%f116, %f78, %f112, %f115;
	ld.global.nc.v2.f32 	{%f117, %f118}, [%rd68];
	ld.global.nc.v2.f32 	{%f121, %f122}, [%rd53+1792];
	fma.rn.f32 	%f125, %f117, %f121, %f86;
	fma.rn.f32 	%f295, %f118, %f122, %f125;
	add.s64 	%rd59, %rd69, %rd7;
	ld.global.nc.v2.f32 	{%f126, %f127}, [%rd59];
	fma.rn.f32 	%f130, %f117, %f126, %f92;
	fma.rn.f32 	%f294, %f118, %f127, %f130;
	ld.global.nc.v2.f32 	{%f131, %f132}, [%rd55+1792];
	fma.rn.f32 	%f135, %f117, %f131, %f98;
	fma.rn.f32 	%f293, %f118, %f132, %f135;
	ld.global.nc.v2.f32 	{%f136, %f137}, [%rd56+1792];
	fma.rn.f32 	%f140, %f117, %f136, %f104;
	fma.rn.f32 	%f292, %f118, %f137, %f140;
	ld.global.nc.v2.f32 	{%f141, %f142}, [%rd57+1792];
	fma.rn.f32 	%f145, %f117, %f141, %f110;
	fma.rn.f32 	%f291, %f118, %f142, %f145;
	ld.global.nc.v2.f32 	{%f146, %f147}, [%rd58+1792];
	fma.rn.f32 	%f150, %f117, %f146, %f116;
	fma.rn.f32 	%f290, %f118, %f147, %f150;
	add.s64 	%rd69, %rd69, 3584;
	add.s64 	%rd68, %rd68, 3584;
	add.s32 	%r251, %r251, 448;
	setp.lt.s32 	%p7, %r251, %r11;
	@%p7 bra 	$L__BB53_7;

	st.local.v2.f32 	[%rd2], {%f295, %f294};
	st.local.v2.f32 	[%rd2+8], {%f293, %f292};
	st.local.v2.f32 	[%rd2+16], {%f291, %f290};

$L__BB53_9:
	shr.s32 	%r38, %r3, 31;
	shr.u32 	%r39, %r38, 27;
	add.s32 	%r40, %r3, %r39;
	shr.s32 	%r41, %r40, 5;
	shl.b32 	%r42, %r41, 2;
	add.s32 	%r10, %r24, %r42;
	mov.u32 	%r44, 2;
	mov.b32 	%r45, %f295;
	mov.u32 	%r46, 31;
	mov.u32 	%r47, 16;
	mov.u32 	%r48, -1;
	shfl.sync.bfly.b32 	%r49|%p8, %r45, %r47, %r46, %r48;
	mov.b32 	%f151, %r49;
	add.f32 	%f152, %f295, %f151;
	mov.b32 	%r50, %f152;
	mov.u32 	%r51, 8;
	shfl.sync.bfly.b32 	%r52|%p9, %r50, %r51, %r46, %r48;
	mov.b32 	%f153, %r52;
	add.f32 	%f154, %f152, %f153;
	mov.b32 	%r53, %f154;
	mov.u32 	%r54, 4;
	shfl.sync.bfly.b32 	%r55|%p10, %r53, %r54, %r46, %r48;
	mov.b32 	%f155, %r55;
	add.f32 	%f156, %f154, %f155;
	mov.b32 	%r56, %f156;
	shfl.sync.bfly.b32 	%r57|%p11, %r56, %r44, %r46, %r48;
	mov.b32 	%f157, %r57;
	add.f32 	%f158, %f156, %f157;
	mov.b32 	%r58, %f158;
	mov.u32 	%r59, 1;
	shfl.sync.bfly.b32 	%r60|%p12, %r58, %r59, %r46, %r48;
	mov.b32 	%f159, %r60;
	add.f32 	%f160, %f158, %f159;
	st.local.f32 	[%rd2], %f160;
	st.shared.f32 	[%r10], %f160;
	bar.sync 	0;
	@%p1 bra 	$L__BB53_11;

	ld.shared.f32 	%f161, [%r4];
	mov.b32 	%r61, %f161;
	shfl.sync.bfly.b32 	%r65|%p14, %r61, %r47, %r46, %r48;
	mov.b32 	%f162, %r65;
	add.f32 	%f163, %f161, %f162;
	mov.b32 	%r66, %f163;
	shfl.sync.bfly.b32 	%r68|%p15, %r66, %r51, %r46, %r48;
	mov.b32 	%f164, %r68;
	add.f32 	%f165, %f163, %f164;
	mov.b32 	%r69, %f165;
	shfl.sync.bfly.b32 	%r71|%p16, %r69, %r54, %r46, %r48;
	mov.b32 	%f166, %r71;
	add.f32 	%f167, %f165, %f166;
	mov.b32 	%r72, %f167;
	shfl.sync.bfly.b32 	%r74|%p17, %r72, %r44, %r46, %r48;
	mov.b32 	%f168, %r74;
	add.f32 	%f169, %f167, %f168;
	mov.b32 	%r75, %f169;
	shfl.sync.bfly.b32 	%r77|%p18, %r75, %r59, %r46, %r48;
	mov.b32 	%f170, %r77;
	add.f32 	%f171, %f169, %f170;
	st.local.f32 	[%rd2], %f171;

$L__BB53_11:
	bar.sync 	0;
	mov.b32 	%r78, %f294;
	shfl.sync.bfly.b32 	%r82|%p20, %r78, %r47, %r46, %r48;
	mov.b32 	%f172, %r82;
	add.f32 	%f173, %f294, %f172;
	mov.b32 	%r83, %f173;
	shfl.sync.bfly.b32 	%r85|%p21, %r83, %r51, %r46, %r48;
	mov.b32 	%f174, %r85;
	add.f32 	%f175, %f173, %f174;
	mov.b32 	%r86, %f175;
	shfl.sync.bfly.b32 	%r88|%p22, %r86, %r54, %r46, %r48;
	mov.b32 	%f176, %r88;
	add.f32 	%f177, %f175, %f176;
	mov.b32 	%r89, %f177;
	shfl.sync.bfly.b32 	%r91|%p23, %r89, %r44, %r46, %r48;
	mov.b32 	%f178, %r91;
	add.f32 	%f179, %f177, %f178;
	mov.b32 	%r92, %f179;
	shfl.sync.bfly.b32 	%r94|%p24, %r92, %r59, %r46, %r48;
	mov.b32 	%f180, %r94;
	add.f32 	%f181, %f179, %f180;
	st.local.f32 	[%rd2+4], %f181;
	st.shared.f32 	[%r10], %f181;
	bar.sync 	0;
	@%p1 bra 	$L__BB53_13;

	ld.shared.f32 	%f182, [%r4];
	mov.b32 	%r95, %f182;
	mov.u32 	%r96, 31;
	mov.u32 	%r97, 16;
	mov.u32 	%r98, -1;
	shfl.sync.bfly.b32 	%r99|%p25, %r95, %r97, %r96, %r98;
	mov.b32 	%f183, %r99;
	add.f32 	%f184, %f182, %f183;
	mov.b32 	%r100, %f184;
	mov.u32 	%r101, 8;
	shfl.sync.bfly.b32 	%r102|%p26, %r100, %r101, %r96, %r98;
	mov.b32 	%f185, %r102;
	add.f32 	%f186, %f184, %f185;
	mov.b32 	%r103, %f186;
	mov.u32 	%r104, 4;
	shfl.sync.bfly.b32 	%r105|%p27, %r103, %r104, %r96, %r98;
	mov.b32 	%f187, %r105;
	add.f32 	%f188, %f186, %f187;
	mov.b32 	%r106, %f188;
	mov.u32 	%r107, 2;
	shfl.sync.bfly.b32 	%r108|%p28, %r106, %r107, %r96, %r98;
	mov.b32 	%f189, %r108;
	add.f32 	%f190, %f188, %f189;
	mov.b32 	%r109, %f190;
	mov.u32 	%r110, 1;
	shfl.sync.bfly.b32 	%r111|%p29, %r109, %r110, %r96, %r98;
	mov.b32 	%f191, %r111;
	add.f32 	%f192, %f190, %f191;
	st.local.f32 	[%rd2+4], %f192;

$L__BB53_13:
	bar.sync 	0;
	mov.b32 	%r112, %f293;
	mov.u32 	%r113, 31;
	mov.u32 	%r114, 16;
	mov.u32 	%r115, -1;
	shfl.sync.bfly.b32 	%r116|%p31, %r112, %r114, %r113, %r115;
	mov.b32 	%f193, %r116;
	add.f32 	%f194, %f293, %f193;
	mov.b32 	%r117, %f194;
	mov.u32 	%r118, 8;
	shfl.sync.bfly.b32 	%r119|%p32, %r117, %r118, %r113, %r115;
	mov.b32 	%f195, %r119;
	add.f32 	%f196, %f194, %f195;
	mov.b32 	%r120, %f196;
	mov.u32 	%r121, 4;
	shfl.sync.bfly.b32 	%r122|%p33, %r120, %r121, %r113, %r115;
	mov.b32 	%f197, %r122;
	add.f32 	%f198, %f196, %f197;
	mov.b32 	%r123, %f198;
	mov.u32 	%r124, 2;
	shfl.sync.bfly.b32 	%r125|%p34, %r123, %r124, %r113, %r115;
	mov.b32 	%f199, %r125;
	add.f32 	%f200, %f198, %f199;
	mov.b32 	%r126, %f200;
	mov.u32 	%r127, 1;
	shfl.sync.bfly.b32 	%r128|%p35, %r126, %r127, %r113, %r115;
	mov.b32 	%f201, %r128;
	add.f32 	%f202, %f200, %f201;
	st.local.f32 	[%rd2+8], %f202;
	st.shared.f32 	[%r10], %f202;
	bar.sync 	0;
	@%p1 bra 	$L__BB53_15;

	ld.shared.f32 	%f203, [%r4];
	mov.b32 	%r129, %f203;
	shfl.sync.bfly.b32 	%r133|%p36, %r129, %r114, %r113, %r115;
	mov.b32 	%f204, %r133;
	add.f32 	%f205, %f203, %f204;
	mov.b32 	%r134, %f205;
	shfl.sync.bfly.b32 	%r136|%p37, %r134, %r118, %r113, %r115;
	mov.b32 	%f206, %r136;
	add.f32 	%f207, %f205, %f206;
	mov.b32 	%r137, %f207;
	shfl.sync.bfly.b32 	%r139|%p38, %r137, %r121, %r113, %r115;
	mov.b32 	%f208, %r139;
	add.f32 	%f209, %f207, %f208;
	mov.b32 	%r140, %f209;
	shfl.sync.bfly.b32 	%r142|%p39, %r140, %r124, %r113, %r115;
	mov.b32 	%f210, %r142;
	add.f32 	%f211, %f209, %f210;
	mov.b32 	%r143, %f211;
	shfl.sync.bfly.b32 	%r145|%p40, %r143, %r127, %r113, %r115;
	mov.b32 	%f212, %r145;
	add.f32 	%f213, %f211, %f212;
	st.local.f32 	[%rd2+8], %f213;

$L__BB53_15:
	bar.sync 	0;
	mov.b32 	%r146, %f292;
	shfl.sync.bfly.b32 	%r150|%p42, %r146, %r114, %r113, %r115;
	mov.b32 	%f214, %r150;
	add.f32 	%f215, %f292, %f214;
	mov.b32 	%r151, %f215;
	shfl.sync.bfly.b32 	%r153|%p43, %r151, %r118, %r113, %r115;
	mov.b32 	%f216, %r153;
	add.f32 	%f217, %f215, %f216;
	mov.b32 	%r154, %f217;
	shfl.sync.bfly.b32 	%r156|%p44, %r154, %r121, %r113, %r115;
	mov.b32 	%f218, %r156;
	add.f32 	%f219, %f217, %f218;
	mov.b32 	%r157, %f219;
	shfl.sync.bfly.b32 	%r159|%p45, %r157, %r124, %r113, %r115;
	mov.b32 	%f220, %r159;
	add.f32 	%f221, %f219, %f220;
	mov.b32 	%r160, %f221;
	shfl.sync.bfly.b32 	%r162|%p46, %r160, %r127, %r113, %r115;
	mov.b32 	%f222, %r162;
	add.f32 	%f223, %f221, %f222;
	st.local.f32 	[%rd2+12], %f223;
	st.shared.f32 	[%r10], %f223;
	bar.sync 	0;
	@%p1 bra 	$L__BB53_17;

	ld.shared.f32 	%f224, [%r4];
	mov.b32 	%r163, %f224;
	mov.u32 	%r164, 31;
	mov.u32 	%r165, 16;
	mov.u32 	%r166, -1;
	shfl.sync.bfly.b32 	%r167|%p47, %r163, %r165, %r164, %r166;
	mov.b32 	%f225, %r167;
	add.f32 	%f226, %f224, %f225;
	mov.b32 	%r168, %f226;
	mov.u32 	%r169, 8;
	shfl.sync.bfly.b32 	%r170|%p48, %r168, %r169, %r164, %r166;
	mov.b32 	%f227, %r170;
	add.f32 	%f228, %f226, %f227;
	mov.b32 	%r171, %f228;
	mov.u32 	%r172, 4;
	shfl.sync.bfly.b32 	%r173|%p49, %r171, %r172, %r164, %r166;
	mov.b32 	%f229, %r173;
	add.f32 	%f230, %f228, %f229;
	mov.b32 	%r174, %f230;
	mov.u32 	%r175, 2;
	shfl.sync.bfly.b32 	%r176|%p50, %r174, %r175, %r164, %r166;
	mov.b32 	%f231, %r176;
	add.f32 	%f232, %f230, %f231;
	mov.b32 	%r177, %f232;
	mov.u32 	%r178, 1;
	shfl.sync.bfly.b32 	%r179|%p51, %r177, %r178, %r164, %r166;
	mov.b32 	%f233, %r179;
	add.f32 	%f234, %f232, %f233;
	st.local.f32 	[%rd2+12], %f234;

$L__BB53_17:
	bar.sync 	0;
	mov.b32 	%r180, %f291;
	mov.u32 	%r181, 31;
	mov.u32 	%r182, 16;
	mov.u32 	%r183, -1;
	shfl.sync.bfly.b32 	%r184|%p53, %r180, %r182, %r181, %r183;
	mov.b32 	%f235, %r184;
	add.f32 	%f236, %f291, %f235;
	mov.b32 	%r185, %f236;
	mov.u32 	%r186, 8;
	shfl.sync.bfly.b32 	%r187|%p54, %r185, %r186, %r181, %r183;
	mov.b32 	%f237, %r187;
	add.f32 	%f238, %f236, %f237;
	mov.b32 	%r188, %f238;
	mov.u32 	%r189, 4;
	shfl.sync.bfly.b32 	%r190|%p55, %r188, %r189, %r181, %r183;
	mov.b32 	%f239, %r190;
	add.f32 	%f240, %f238, %f239;
	mov.b32 	%r191, %f240;
	mov.u32 	%r192, 2;
	shfl.sync.bfly.b32 	%r193|%p56, %r191, %r192, %r181, %r183;
	mov.b32 	%f241, %r193;
	add.f32 	%f242, %f240, %f241;
	mov.b32 	%r194, %f242;
	mov.u32 	%r195, 1;
	shfl.sync.bfly.b32 	%r196|%p57, %r194, %r195, %r181, %r183;
	mov.b32 	%f243, %r196;
	add.f32 	%f244, %f242, %f243;
	st.local.f32 	[%rd2+16], %f244;
	st.shared.f32 	[%r10], %f244;
	bar.sync 	0;
	@%p1 bra 	$L__BB53_19;

	ld.shared.f32 	%f245, [%r4];
	mov.b32 	%r197, %f245;
	shfl.sync.bfly.b32 	%r201|%p58, %r197, %r182, %r181, %r183;
	mov.b32 	%f246, %r201;
	add.f32 	%f247, %f245, %f246;
	mov.b32 	%r202, %f247;
	shfl.sync.bfly.b32 	%r204|%p59, %r202, %r186, %r181, %r183;
	mov.b32 	%f248, %r204;
	add.f32 	%f249, %f247, %f248;
	mov.b32 	%r205, %f249;
	shfl.sync.bfly.b32 	%r207|%p60, %r205, %r189, %r181, %r183;
	mov.b32 	%f250, %r207;
	add.f32 	%f251, %f249, %f250;
	mov.b32 	%r208, %f251;
	shfl.sync.bfly.b32 	%r210|%p61, %r208, %r192, %r181, %r183;
	mov.b32 	%f252, %r210;
	add.f32 	%f253, %f251, %f252;
	mov.b32 	%r211, %f253;
	shfl.sync.bfly.b32 	%r213|%p62, %r211, %r195, %r181, %r183;
	mov.b32 	%f254, %r213;
	add.f32 	%f255, %f253, %f254;
	st.local.f32 	[%rd2+16], %f255;

$L__BB53_19:
	bar.sync 	0;
	mov.b32 	%r214, %f290;
	shfl.sync.bfly.b32 	%r218|%p64, %r214, %r182, %r181, %r183;
	mov.b32 	%f256, %r218;
	add.f32 	%f257, %f290, %f256;
	mov.b32 	%r219, %f257;
	shfl.sync.bfly.b32 	%r221|%p65, %r219, %r186, %r181, %r183;
	mov.b32 	%f258, %r221;
	add.f32 	%f259, %f257, %f258;
	mov.b32 	%r222, %f259;
	shfl.sync.bfly.b32 	%r224|%p66, %r222, %r189, %r181, %r183;
	mov.b32 	%f260, %r224;
	add.f32 	%f261, %f259, %f260;
	mov.b32 	%r225, %f261;
	shfl.sync.bfly.b32 	%r227|%p67, %r225, %r192, %r181, %r183;
	mov.b32 	%f262, %r227;
	add.f32 	%f263, %f261, %f262;
	mov.b32 	%r228, %f263;
	shfl.sync.bfly.b32 	%r230|%p68, %r228, %r195, %r181, %r183;
	mov.b32 	%f264, %r230;
	add.f32 	%f265, %f263, %f264;
	st.local.f32 	[%rd2+20], %f265;
	st.shared.f32 	[%r10], %f265;
	bar.sync 	0;
	@%p1 bra 	$L__BB53_21;

	ld.shared.f32 	%f266, [%r4];
	mov.b32 	%r231, %f266;
	mov.u32 	%r232, 31;
	mov.u32 	%r233, 16;
	mov.u32 	%r234, -1;
	shfl.sync.bfly.b32 	%r235|%p69, %r231, %r233, %r232, %r234;
	mov.b32 	%f267, %r235;
	add.f32 	%f268, %f266, %f267;
	mov.b32 	%r236, %f268;
	mov.u32 	%r237, 8;
	shfl.sync.bfly.b32 	%r238|%p70, %r236, %r237, %r232, %r234;
	mov.b32 	%f269, %r238;
	add.f32 	%f270, %f268, %f269;
	mov.b32 	%r239, %f270;
	mov.u32 	%r240, 4;
	shfl.sync.bfly.b32 	%r241|%p71, %r239, %r240, %r232, %r234;
	mov.b32 	%f271, %r241;
	add.f32 	%f272, %f270, %f271;
	mov.b32 	%r242, %f272;
	mov.u32 	%r243, 2;
	shfl.sync.bfly.b32 	%r244|%p72, %r242, %r243, %r232, %r234;
	mov.b32 	%f273, %r244;
	add.f32 	%f274, %f272, %f273;
	mov.b32 	%r245, %f274;
	mov.u32 	%r246, 1;
	shfl.sync.bfly.b32 	%r247|%p73, %r245, %r246, %r232, %r234;
	mov.b32 	%f275, %r247;
	add.f32 	%f276, %f274, %f275;
	st.local.f32 	[%rd2+20], %f276;

$L__BB53_21:
	bar.sync 	0;
	setp.gt.s32 	%p74, %r3, 5;
	@%p74 bra 	$L__BB53_23;

	mul.wide.s32 	%rd60, %r3, 4;
	add.s64 	%rd61, %rd2, %rd60;
	ld.local.f32 	%f277, [%rd61];
	mad.lo.s32 	%r248, %r3, %r13, %r2;
	cvt.s64.s32 	%rd62, %r248;
	mul.lo.s32 	%r249, %r1, %r14;
	cvt.s64.s32 	%rd63, %r249;
	add.s64 	%rd64, %rd63, %rd62;
	cvta.to.global.u64 	%rd65, %rd19;
	shl.b64 	%rd66, %rd64, 2;
	add.s64 	%rd67, %rd65, %rd66;
	st.global.f32 	[%rd67], %f277;

$L__BB53_23:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_7_bs_224
.visible .entry ggml_matvec_f32_ncols_7_bs_224(
	.param .u64 ggml_matvec_f32_ncols_7_bs_224_param_0,
	.param .u64 ggml_matvec_f32_ncols_7_bs_224_param_1,
	.param .u64 ggml_matvec_f32_ncols_7_bs_224_param_2,
	.param .u32 ggml_matvec_f32_ncols_7_bs_224_param_3,
	.param .u32 ggml_matvec_f32_ncols_7_bs_224_param_4,
	.param .u32 ggml_matvec_f32_ncols_7_bs_224_param_5,
	.param .u32 ggml_matvec_f32_ncols_7_bs_224_param_6,
	.param .u32 ggml_matvec_f32_ncols_7_bs_224_param_7,
	.param .u32 ggml_matvec_f32_ncols_7_bs_224_param_8,
	.param .u32 ggml_matvec_f32_ncols_7_bs_224_param_9,
	.param .u32 ggml_matvec_f32_ncols_7_bs_224_param_10,
	.param .u32 ggml_matvec_f32_ncols_7_bs_224_param_11
)
{
	.local .align 4 .b8 	__local_depot54[28];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<86>;
	.reg .f32 	%f<343>;
	.reg .b32 	%r<288>;
	.reg .b64 	%rd<74>;


	mov.u64 	%SPL, __local_depot54;
	ld.param.u64 	%rd21, [ggml_matvec_f32_ncols_7_bs_224_param_0];
	ld.param.u64 	%rd22, [ggml_matvec_f32_ncols_7_bs_224_param_1];
	ld.param.u64 	%rd20, [ggml_matvec_f32_ncols_7_bs_224_param_2];
	ld.param.u32 	%r11, [ggml_matvec_f32_ncols_7_bs_224_param_3];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_7_bs_224_param_5];
	ld.param.u32 	%r12, [ggml_matvec_f32_ncols_7_bs_224_param_6];
	ld.param.u32 	%r13, [ggml_matvec_f32_ncols_7_bs_224_param_7];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_7_bs_224_param_8];
	ld.param.u32 	%r17, [ggml_matvec_f32_ncols_7_bs_224_param_9];
	ld.param.u32 	%r18, [ggml_matvec_f32_ncols_7_bs_224_param_10];
	ld.param.u32 	%r14, [ggml_matvec_f32_ncols_7_bs_224_param_11];
	cvta.to.global.u64 	%rd73, %rd22;
	add.u64 	%rd2, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r19, %r1, %r16;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r20, %r2, %r15;
	mad.lo.s32 	%r21, %r19, %r17, %r20;
	cvt.s64.s32 	%rd3, %r21;
	cvta.to.global.u64 	%rd4, %rd21;
	mul.lo.s32 	%r22, %r1, %r18;
	cvt.s64.s32 	%rd5, %r22;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r23, %r3, 2;
	mov.u32 	%r24, data_mmv;
	add.s32 	%r4, %r24, %r23;
	@%p1 bra 	$L__BB54_2;

	mov.u32 	%r25, 0;
	st.shared.u32 	[%r4], %r25;

$L__BB54_2:
	bar.sync 	0;
	mov.f32 	%f336, 0f00000000;
	mov.u32 	%r26, 0;
	st.local.u32 	[%rd2], %r26;
	st.local.u32 	[%rd2+4], %r26;
	st.local.u32 	[%rd2+8], %r26;
	st.local.u32 	[%rd2+12], %r26;
	st.local.u32 	[%rd2+16], %r26;
	st.local.u32 	[%rd2+20], %r26;
	st.local.u32 	[%rd2+24], %r26;
	setp.ge.s32 	%p2, %r3, %r11;
	mov.f32 	%f337, %f336;
	mov.f32 	%f338, %f336;
	mov.f32 	%f339, %f336;
	mov.f32 	%f340, %f336;
	mov.f32 	%f341, %f336;
	mov.f32 	%f342, %f336;
	@%p2 bra 	$L__BB54_9;

	not.b32 	%r27, %r3;
	add.s32 	%r5, %r27, %r11;
	shr.u32 	%r28, %r5, 5;
	mul.wide.u32 	%rd24, %r28, 613566757;
	shr.u64 	%rd25, %rd24, 32;
	and.b64  	%rd26, %rd25, 1;
	setp.eq.b64 	%p3, %rd26, 1;
	mov.pred 	%p4, 0;
	xor.pred  	%p5, %p3, %p4;
	mov.f32 	%f336, 0f00000000;
	mov.u32 	%r287, %r3;
	@%p5 bra 	$L__BB54_5;

	shl.b64 	%rd27, %rd5, 2;
	add.s64 	%rd28, %rd73, %rd27;
	shl.b64 	%rd29, %rd3, 2;
	add.s64 	%rd30, %rd4, %rd29;
	mul.wide.s32 	%rd31, %r3, 8;
	add.s64 	%rd32, %rd30, %rd31;
	ld.global.nc.v2.f32 	{%f50, %f51}, [%rd32];
	add.s64 	%rd33, %rd28, %rd31;
	ld.global.nc.v2.f32 	{%f54, %f55}, [%rd33];
	fma.rn.f32 	%f58, %f50, %f54, 0f00000000;
	fma.rn.f32 	%f342, %f51, %f55, %f58;
	st.local.f32 	[%rd2], %f342;
	mul.wide.s32 	%rd34, %r12, 8;
	add.s64 	%rd35, %rd33, %rd34;
	ld.global.nc.v2.f32 	{%f59, %f60}, [%rd35];
	fma.rn.f32 	%f63, %f50, %f59, 0f00000000;
	fma.rn.f32 	%f341, %f51, %f60, %f63;
	st.local.f32 	[%rd2+4], %f341;
	add.s32 	%r29, %r3, %r12;
	add.s32 	%r30, %r29, %r12;
	mul.wide.s32 	%rd36, %r30, 8;
	add.s64 	%rd37, %rd28, %rd36;
	ld.global.nc.v2.f32 	{%f64, %f65}, [%rd37];
	fma.rn.f32 	%f68, %f50, %f64, 0f00000000;
	fma.rn.f32 	%f340, %f51, %f65, %f68;
	st.local.f32 	[%rd2+8], %f340;
	add.s64 	%rd38, %rd37, %rd34;
	ld.global.nc.v2.f32 	{%f69, %f70}, [%rd38];
	fma.rn.f32 	%f73, %f50, %f69, 0f00000000;
	fma.rn.f32 	%f339, %f51, %f70, %f73;
	st.local.f32 	[%rd2+12], %f339;
	add.s64 	%rd39, %rd38, %rd34;
	ld.global.nc.v2.f32 	{%f74, %f75}, [%rd39];
	fma.rn.f32 	%f78, %f50, %f74, 0f00000000;
	fma.rn.f32 	%f338, %f51, %f75, %f78;
	st.local.f32 	[%rd2+16], %f338;
	add.s64 	%rd40, %rd39, %rd34;
	ld.global.nc.v2.f32 	{%f79, %f80}, [%rd40];
	fma.rn.f32 	%f83, %f50, %f79, 0f00000000;
	fma.rn.f32 	%f337, %f51, %f80, %f83;
	st.local.f32 	[%rd2+20], %f337;
	add.s64 	%rd41, %rd40, %rd34;
	ld.global.nc.v2.f32 	{%f84, %f85}, [%rd41];
	fma.rn.f32 	%f88, %f50, %f84, 0f00000000;
	fma.rn.f32 	%f336, %f51, %f85, %f88;
	st.local.f32 	[%rd2+24], %f336;
	add.s32 	%r287, %r3, 224;

$L__BB54_5:
	setp.lt.u32 	%p6, %r5, 224;
	@%p6 bra 	$L__BB54_9;

	add.s32 	%r31, %r287, %r12;
	add.s32 	%r32, %r31, 224;
	mul.wide.s32 	%rd42, %r32, 8;
	shl.b64 	%rd43, %rd5, 2;
	add.s64 	%rd7, %rd42, %rd43;
	shl.b32 	%r33, %r12, 1;
	add.s32 	%r34, %r287, %r33;
	mad.lo.s32 	%r35, %r12, 3, %r287;
	shl.b32 	%r36, %r12, 2;
	add.s32 	%r37, %r287, %r36;
	mad.lo.s32 	%r38, %r12, 5, %r287;
	mad.lo.s32 	%r39, %r12, 6, %r287;
	mul.wide.s32 	%rd44, %r34, 8;
	add.s64 	%rd8, %rd44, %rd43;
	mul.wide.s32 	%rd45, %r35, 8;
	add.s64 	%rd9, %rd45, %rd43;
	mul.wide.s32 	%rd46, %r37, 8;
	add.s64 	%rd10, %rd46, %rd43;
	mul.wide.s32 	%rd47, %r38, 8;
	add.s64 	%rd11, %rd47, %rd43;
	mul.wide.s32 	%rd48, %r39, 8;
	add.s64 	%rd12, %rd48, %rd43;
	mul.wide.s32 	%rd49, %r287, 2;
	add.s64 	%rd50, %rd49, %rd3;
	shl.b64 	%rd51, %rd50, 2;
	add.s64 	%rd52, %rd4, %rd51;
	add.s64 	%rd72, %rd52, 1792;
	mul.wide.s32 	%rd53, %r287, 8;
	mul.wide.s32 	%rd54, %r12, 8;
	add.s64 	%rd55, %rd53, %rd54;
	add.s64 	%rd14, %rd55, %rd43;
	add.s64 	%rd15, %rd53, %rd43;

$L__BB54_7:
	ld.global.nc.v2.f32 	{%f89, %f90}, [%rd72+-1792];
	add.s64 	%rd56, %rd73, %rd15;
	ld.global.nc.v2.f32 	{%f93, %f94}, [%rd56];
	fma.rn.f32 	%f97, %f89, %f93, %f342;
	fma.rn.f32 	%f98, %f90, %f94, %f97;
	add.s64 	%rd57, %rd73, %rd14;
	ld.global.nc.v2.f32 	{%f99, %f100}, [%rd57];
	fma.rn.f32 	%f103, %f89, %f99, %f341;
	fma.rn.f32 	%f104, %f90, %f100, %f103;
	add.s64 	%rd58, %rd73, %rd8;
	ld.global.nc.v2.f32 	{%f105, %f106}, [%rd58];
	fma.rn.f32 	%f109, %f89, %f105, %f340;
	fma.rn.f32 	%f110, %f90, %f106, %f109;
	add.s64 	%rd59, %rd73, %rd9;
	ld.global.nc.v2.f32 	{%f111, %f112}, [%rd59];
	fma.rn.f32 	%f115, %f89, %f111, %f339;
	fma.rn.f32 	%f116, %f90, %f112, %f115;
	add.s64 	%rd60, %rd73, %rd10;
	ld.global.nc.v2.f32 	{%f117, %f118}, [%rd60];
	fma.rn.f32 	%f121, %f89, %f117, %f338;
	fma.rn.f32 	%f122, %f90, %f118, %f121;
	add.s64 	%rd61, %rd73, %rd11;
	ld.global.nc.v2.f32 	{%f123, %f124}, [%rd61];
	fma.rn.f32 	%f127, %f89, %f123, %f337;
	fma.rn.f32 	%f128, %f90, %f124, %f127;
	add.s64 	%rd62, %rd73, %rd12;
	ld.global.nc.v2.f32 	{%f129, %f130}, [%rd62];
	fma.rn.f32 	%f133, %f89, %f129, %f336;
	fma.rn.f32 	%f134, %f90, %f130, %f133;
	ld.global.nc.v2.f32 	{%f135, %f136}, [%rd72];
	ld.global.nc.v2.f32 	{%f139, %f140}, [%rd56+1792];
	fma.rn.f32 	%f143, %f135, %f139, %f98;
	fma.rn.f32 	%f342, %f136, %f140, %f143;
	add.s64 	%rd63, %rd73, %rd7;
	ld.global.nc.v2.f32 	{%f144, %f145}, [%rd63];
	fma.rn.f32 	%f148, %f135, %f144, %f104;
	fma.rn.f32 	%f341, %f136, %f145, %f148;
	ld.global.nc.v2.f32 	{%f149, %f150}, [%rd58+1792];
	fma.rn.f32 	%f153, %f135, %f149, %f110;
	fma.rn.f32 	%f340, %f136, %f150, %f153;
	ld.global.nc.v2.f32 	{%f154, %f155}, [%rd59+1792];
	fma.rn.f32 	%f158, %f135, %f154, %f116;
	fma.rn.f32 	%f339, %f136, %f155, %f158;
	ld.global.nc.v2.f32 	{%f159, %f160}, [%rd60+1792];
	fma.rn.f32 	%f163, %f135, %f159, %f122;
	fma.rn.f32 	%f338, %f136, %f160, %f163;
	ld.global.nc.v2.f32 	{%f164, %f165}, [%rd61+1792];
	fma.rn.f32 	%f168, %f135, %f164, %f128;
	fma.rn.f32 	%f337, %f136, %f165, %f168;
	ld.global.nc.v2.f32 	{%f169, %f170}, [%rd62+1792];
	fma.rn.f32 	%f173, %f135, %f169, %f134;
	fma.rn.f32 	%f336, %f136, %f170, %f173;
	add.s64 	%rd73, %rd73, 3584;
	add.s64 	%rd72, %rd72, 3584;
	add.s32 	%r287, %r287, 448;
	setp.lt.s32 	%p7, %r287, %r11;
	@%p7 bra 	$L__BB54_7;

	st.local.f32 	[%rd2], %f342;
	st.local.f32 	[%rd2+4], %f341;
	st.local.f32 	[%rd2+8], %f340;
	st.local.f32 	[%rd2+12], %f339;
	st.local.f32 	[%rd2+16], %f338;
	st.local.f32 	[%rd2+20], %f337;
	st.local.f32 	[%rd2+24], %f336;

$L__BB54_9:
	shr.s32 	%r40, %r3, 31;
	shr.u32 	%r41, %r40, 27;
	add.s32 	%r42, %r3, %r41;
	shr.s32 	%r43, %r42, 5;
	shl.b32 	%r44, %r43, 2;
	add.s32 	%r10, %r24, %r44;
	mov.u32 	%r46, 2;
	mov.b32 	%r47, %f342;
	mov.u32 	%r48, 31;
	mov.u32 	%r49, 16;
	mov.u32 	%r50, -1;
	shfl.sync.bfly.b32 	%r51|%p8, %r47, %r49, %r48, %r50;
	mov.b32 	%f174, %r51;
	add.f32 	%f175, %f342, %f174;
	mov.b32 	%r52, %f175;
	mov.u32 	%r53, 8;
	shfl.sync.bfly.b32 	%r54|%p9, %r52, %r53, %r48, %r50;
	mov.b32 	%f176, %r54;
	add.f32 	%f177, %f175, %f176;
	mov.b32 	%r55, %f177;
	mov.u32 	%r56, 4;
	shfl.sync.bfly.b32 	%r57|%p10, %r55, %r56, %r48, %r50;
	mov.b32 	%f178, %r57;
	add.f32 	%f179, %f177, %f178;
	mov.b32 	%r58, %f179;
	shfl.sync.bfly.b32 	%r59|%p11, %r58, %r46, %r48, %r50;
	mov.b32 	%f180, %r59;
	add.f32 	%f181, %f179, %f180;
	mov.b32 	%r60, %f181;
	mov.u32 	%r61, 1;
	shfl.sync.bfly.b32 	%r62|%p12, %r60, %r61, %r48, %r50;
	mov.b32 	%f182, %r62;
	add.f32 	%f183, %f181, %f182;
	st.local.f32 	[%rd2], %f183;
	st.shared.f32 	[%r10], %f183;
	bar.sync 	0;
	@%p1 bra 	$L__BB54_11;

	ld.shared.f32 	%f184, [%r4];
	mov.b32 	%r63, %f184;
	shfl.sync.bfly.b32 	%r67|%p14, %r63, %r49, %r48, %r50;
	mov.b32 	%f185, %r67;
	add.f32 	%f186, %f184, %f185;
	mov.b32 	%r68, %f186;
	shfl.sync.bfly.b32 	%r70|%p15, %r68, %r53, %r48, %r50;
	mov.b32 	%f187, %r70;
	add.f32 	%f188, %f186, %f187;
	mov.b32 	%r71, %f188;
	shfl.sync.bfly.b32 	%r73|%p16, %r71, %r56, %r48, %r50;
	mov.b32 	%f189, %r73;
	add.f32 	%f190, %f188, %f189;
	mov.b32 	%r74, %f190;
	shfl.sync.bfly.b32 	%r76|%p17, %r74, %r46, %r48, %r50;
	mov.b32 	%f191, %r76;
	add.f32 	%f192, %f190, %f191;
	mov.b32 	%r77, %f192;
	shfl.sync.bfly.b32 	%r79|%p18, %r77, %r61, %r48, %r50;
	mov.b32 	%f193, %r79;
	add.f32 	%f194, %f192, %f193;
	st.local.f32 	[%rd2], %f194;

$L__BB54_11:
	bar.sync 	0;
	mov.b32 	%r80, %f341;
	shfl.sync.bfly.b32 	%r84|%p20, %r80, %r49, %r48, %r50;
	mov.b32 	%f195, %r84;
	add.f32 	%f196, %f341, %f195;
	mov.b32 	%r85, %f196;
	shfl.sync.bfly.b32 	%r87|%p21, %r85, %r53, %r48, %r50;
	mov.b32 	%f197, %r87;
	add.f32 	%f198, %f196, %f197;
	mov.b32 	%r88, %f198;
	shfl.sync.bfly.b32 	%r90|%p22, %r88, %r56, %r48, %r50;
	mov.b32 	%f199, %r90;
	add.f32 	%f200, %f198, %f199;
	mov.b32 	%r91, %f200;
	shfl.sync.bfly.b32 	%r93|%p23, %r91, %r46, %r48, %r50;
	mov.b32 	%f201, %r93;
	add.f32 	%f202, %f200, %f201;
	mov.b32 	%r94, %f202;
	shfl.sync.bfly.b32 	%r96|%p24, %r94, %r61, %r48, %r50;
	mov.b32 	%f203, %r96;
	add.f32 	%f204, %f202, %f203;
	st.local.f32 	[%rd2+4], %f204;
	st.shared.f32 	[%r10], %f204;
	bar.sync 	0;
	@%p1 bra 	$L__BB54_13;

	ld.shared.f32 	%f205, [%r4];
	mov.b32 	%r97, %f205;
	mov.u32 	%r98, 31;
	mov.u32 	%r99, 16;
	mov.u32 	%r100, -1;
	shfl.sync.bfly.b32 	%r101|%p25, %r97, %r99, %r98, %r100;
	mov.b32 	%f206, %r101;
	add.f32 	%f207, %f205, %f206;
	mov.b32 	%r102, %f207;
	mov.u32 	%r103, 8;
	shfl.sync.bfly.b32 	%r104|%p26, %r102, %r103, %r98, %r100;
	mov.b32 	%f208, %r104;
	add.f32 	%f209, %f207, %f208;
	mov.b32 	%r105, %f209;
	mov.u32 	%r106, 4;
	shfl.sync.bfly.b32 	%r107|%p27, %r105, %r106, %r98, %r100;
	mov.b32 	%f210, %r107;
	add.f32 	%f211, %f209, %f210;
	mov.b32 	%r108, %f211;
	mov.u32 	%r109, 2;
	shfl.sync.bfly.b32 	%r110|%p28, %r108, %r109, %r98, %r100;
	mov.b32 	%f212, %r110;
	add.f32 	%f213, %f211, %f212;
	mov.b32 	%r111, %f213;
	mov.u32 	%r112, 1;
	shfl.sync.bfly.b32 	%r113|%p29, %r111, %r112, %r98, %r100;
	mov.b32 	%f214, %r113;
	add.f32 	%f215, %f213, %f214;
	st.local.f32 	[%rd2+4], %f215;

$L__BB54_13:
	bar.sync 	0;
	mov.b32 	%r114, %f340;
	mov.u32 	%r115, 31;
	mov.u32 	%r116, 16;
	mov.u32 	%r117, -1;
	shfl.sync.bfly.b32 	%r118|%p31, %r114, %r116, %r115, %r117;
	mov.b32 	%f216, %r118;
	add.f32 	%f217, %f340, %f216;
	mov.b32 	%r119, %f217;
	mov.u32 	%r120, 8;
	shfl.sync.bfly.b32 	%r121|%p32, %r119, %r120, %r115, %r117;
	mov.b32 	%f218, %r121;
	add.f32 	%f219, %f217, %f218;
	mov.b32 	%r122, %f219;
	mov.u32 	%r123, 4;
	shfl.sync.bfly.b32 	%r124|%p33, %r122, %r123, %r115, %r117;
	mov.b32 	%f220, %r124;
	add.f32 	%f221, %f219, %f220;
	mov.b32 	%r125, %f221;
	mov.u32 	%r126, 2;
	shfl.sync.bfly.b32 	%r127|%p34, %r125, %r126, %r115, %r117;
	mov.b32 	%f222, %r127;
	add.f32 	%f223, %f221, %f222;
	mov.b32 	%r128, %f223;
	mov.u32 	%r129, 1;
	shfl.sync.bfly.b32 	%r130|%p35, %r128, %r129, %r115, %r117;
	mov.b32 	%f224, %r130;
	add.f32 	%f225, %f223, %f224;
	st.local.f32 	[%rd2+8], %f225;
	st.shared.f32 	[%r10], %f225;
	bar.sync 	0;
	@%p1 bra 	$L__BB54_15;

	ld.shared.f32 	%f226, [%r4];
	mov.b32 	%r131, %f226;
	shfl.sync.bfly.b32 	%r135|%p36, %r131, %r116, %r115, %r117;
	mov.b32 	%f227, %r135;
	add.f32 	%f228, %f226, %f227;
	mov.b32 	%r136, %f228;
	shfl.sync.bfly.b32 	%r138|%p37, %r136, %r120, %r115, %r117;
	mov.b32 	%f229, %r138;
	add.f32 	%f230, %f228, %f229;
	mov.b32 	%r139, %f230;
	shfl.sync.bfly.b32 	%r141|%p38, %r139, %r123, %r115, %r117;
	mov.b32 	%f231, %r141;
	add.f32 	%f232, %f230, %f231;
	mov.b32 	%r142, %f232;
	shfl.sync.bfly.b32 	%r144|%p39, %r142, %r126, %r115, %r117;
	mov.b32 	%f233, %r144;
	add.f32 	%f234, %f232, %f233;
	mov.b32 	%r145, %f234;
	shfl.sync.bfly.b32 	%r147|%p40, %r145, %r129, %r115, %r117;
	mov.b32 	%f235, %r147;
	add.f32 	%f236, %f234, %f235;
	st.local.f32 	[%rd2+8], %f236;

$L__BB54_15:
	bar.sync 	0;
	mov.b32 	%r148, %f339;
	shfl.sync.bfly.b32 	%r152|%p42, %r148, %r116, %r115, %r117;
	mov.b32 	%f237, %r152;
	add.f32 	%f238, %f339, %f237;
	mov.b32 	%r153, %f238;
	shfl.sync.bfly.b32 	%r155|%p43, %r153, %r120, %r115, %r117;
	mov.b32 	%f239, %r155;
	add.f32 	%f240, %f238, %f239;
	mov.b32 	%r156, %f240;
	shfl.sync.bfly.b32 	%r158|%p44, %r156, %r123, %r115, %r117;
	mov.b32 	%f241, %r158;
	add.f32 	%f242, %f240, %f241;
	mov.b32 	%r159, %f242;
	shfl.sync.bfly.b32 	%r161|%p45, %r159, %r126, %r115, %r117;
	mov.b32 	%f243, %r161;
	add.f32 	%f244, %f242, %f243;
	mov.b32 	%r162, %f244;
	shfl.sync.bfly.b32 	%r164|%p46, %r162, %r129, %r115, %r117;
	mov.b32 	%f245, %r164;
	add.f32 	%f246, %f244, %f245;
	st.local.f32 	[%rd2+12], %f246;
	st.shared.f32 	[%r10], %f246;
	bar.sync 	0;
	@%p1 bra 	$L__BB54_17;

	ld.shared.f32 	%f247, [%r4];
	mov.b32 	%r165, %f247;
	mov.u32 	%r166, 31;
	mov.u32 	%r167, 16;
	mov.u32 	%r168, -1;
	shfl.sync.bfly.b32 	%r169|%p47, %r165, %r167, %r166, %r168;
	mov.b32 	%f248, %r169;
	add.f32 	%f249, %f247, %f248;
	mov.b32 	%r170, %f249;
	mov.u32 	%r171, 8;
	shfl.sync.bfly.b32 	%r172|%p48, %r170, %r171, %r166, %r168;
	mov.b32 	%f250, %r172;
	add.f32 	%f251, %f249, %f250;
	mov.b32 	%r173, %f251;
	mov.u32 	%r174, 4;
	shfl.sync.bfly.b32 	%r175|%p49, %r173, %r174, %r166, %r168;
	mov.b32 	%f252, %r175;
	add.f32 	%f253, %f251, %f252;
	mov.b32 	%r176, %f253;
	mov.u32 	%r177, 2;
	shfl.sync.bfly.b32 	%r178|%p50, %r176, %r177, %r166, %r168;
	mov.b32 	%f254, %r178;
	add.f32 	%f255, %f253, %f254;
	mov.b32 	%r179, %f255;
	mov.u32 	%r180, 1;
	shfl.sync.bfly.b32 	%r181|%p51, %r179, %r180, %r166, %r168;
	mov.b32 	%f256, %r181;
	add.f32 	%f257, %f255, %f256;
	st.local.f32 	[%rd2+12], %f257;

$L__BB54_17:
	bar.sync 	0;
	mov.b32 	%r182, %f338;
	mov.u32 	%r183, 31;
	mov.u32 	%r184, 16;
	mov.u32 	%r185, -1;
	shfl.sync.bfly.b32 	%r186|%p53, %r182, %r184, %r183, %r185;
	mov.b32 	%f258, %r186;
	add.f32 	%f259, %f338, %f258;
	mov.b32 	%r187, %f259;
	mov.u32 	%r188, 8;
	shfl.sync.bfly.b32 	%r189|%p54, %r187, %r188, %r183, %r185;
	mov.b32 	%f260, %r189;
	add.f32 	%f261, %f259, %f260;
	mov.b32 	%r190, %f261;
	mov.u32 	%r191, 4;
	shfl.sync.bfly.b32 	%r192|%p55, %r190, %r191, %r183, %r185;
	mov.b32 	%f262, %r192;
	add.f32 	%f263, %f261, %f262;
	mov.b32 	%r193, %f263;
	mov.u32 	%r194, 2;
	shfl.sync.bfly.b32 	%r195|%p56, %r193, %r194, %r183, %r185;
	mov.b32 	%f264, %r195;
	add.f32 	%f265, %f263, %f264;
	mov.b32 	%r196, %f265;
	mov.u32 	%r197, 1;
	shfl.sync.bfly.b32 	%r198|%p57, %r196, %r197, %r183, %r185;
	mov.b32 	%f266, %r198;
	add.f32 	%f267, %f265, %f266;
	st.local.f32 	[%rd2+16], %f267;
	st.shared.f32 	[%r10], %f267;
	bar.sync 	0;
	@%p1 bra 	$L__BB54_19;

	ld.shared.f32 	%f268, [%r4];
	mov.b32 	%r199, %f268;
	shfl.sync.bfly.b32 	%r203|%p58, %r199, %r184, %r183, %r185;
	mov.b32 	%f269, %r203;
	add.f32 	%f270, %f268, %f269;
	mov.b32 	%r204, %f270;
	shfl.sync.bfly.b32 	%r206|%p59, %r204, %r188, %r183, %r185;
	mov.b32 	%f271, %r206;
	add.f32 	%f272, %f270, %f271;
	mov.b32 	%r207, %f272;
	shfl.sync.bfly.b32 	%r209|%p60, %r207, %r191, %r183, %r185;
	mov.b32 	%f273, %r209;
	add.f32 	%f274, %f272, %f273;
	mov.b32 	%r210, %f274;
	shfl.sync.bfly.b32 	%r212|%p61, %r210, %r194, %r183, %r185;
	mov.b32 	%f275, %r212;
	add.f32 	%f276, %f274, %f275;
	mov.b32 	%r213, %f276;
	shfl.sync.bfly.b32 	%r215|%p62, %r213, %r197, %r183, %r185;
	mov.b32 	%f277, %r215;
	add.f32 	%f278, %f276, %f277;
	st.local.f32 	[%rd2+16], %f278;

$L__BB54_19:
	bar.sync 	0;
	mov.b32 	%r216, %f337;
	shfl.sync.bfly.b32 	%r220|%p64, %r216, %r184, %r183, %r185;
	mov.b32 	%f279, %r220;
	add.f32 	%f280, %f337, %f279;
	mov.b32 	%r221, %f280;
	shfl.sync.bfly.b32 	%r223|%p65, %r221, %r188, %r183, %r185;
	mov.b32 	%f281, %r223;
	add.f32 	%f282, %f280, %f281;
	mov.b32 	%r224, %f282;
	shfl.sync.bfly.b32 	%r226|%p66, %r224, %r191, %r183, %r185;
	mov.b32 	%f283, %r226;
	add.f32 	%f284, %f282, %f283;
	mov.b32 	%r227, %f284;
	shfl.sync.bfly.b32 	%r229|%p67, %r227, %r194, %r183, %r185;
	mov.b32 	%f285, %r229;
	add.f32 	%f286, %f284, %f285;
	mov.b32 	%r230, %f286;
	shfl.sync.bfly.b32 	%r232|%p68, %r230, %r197, %r183, %r185;
	mov.b32 	%f287, %r232;
	add.f32 	%f288, %f286, %f287;
	st.local.f32 	[%rd2+20], %f288;
	st.shared.f32 	[%r10], %f288;
	bar.sync 	0;
	@%p1 bra 	$L__BB54_21;

	ld.shared.f32 	%f289, [%r4];
	mov.b32 	%r233, %f289;
	mov.u32 	%r234, 31;
	mov.u32 	%r235, 16;
	mov.u32 	%r236, -1;
	shfl.sync.bfly.b32 	%r237|%p69, %r233, %r235, %r234, %r236;
	mov.b32 	%f290, %r237;
	add.f32 	%f291, %f289, %f290;
	mov.b32 	%r238, %f291;
	mov.u32 	%r239, 8;
	shfl.sync.bfly.b32 	%r240|%p70, %r238, %r239, %r234, %r236;
	mov.b32 	%f292, %r240;
	add.f32 	%f293, %f291, %f292;
	mov.b32 	%r241, %f293;
	mov.u32 	%r242, 4;
	shfl.sync.bfly.b32 	%r243|%p71, %r241, %r242, %r234, %r236;
	mov.b32 	%f294, %r243;
	add.f32 	%f295, %f293, %f294;
	mov.b32 	%r244, %f295;
	mov.u32 	%r245, 2;
	shfl.sync.bfly.b32 	%r246|%p72, %r244, %r245, %r234, %r236;
	mov.b32 	%f296, %r246;
	add.f32 	%f297, %f295, %f296;
	mov.b32 	%r247, %f297;
	mov.u32 	%r248, 1;
	shfl.sync.bfly.b32 	%r249|%p73, %r247, %r248, %r234, %r236;
	mov.b32 	%f298, %r249;
	add.f32 	%f299, %f297, %f298;
	st.local.f32 	[%rd2+20], %f299;

$L__BB54_21:
	bar.sync 	0;
	mov.b32 	%r250, %f336;
	mov.u32 	%r251, 31;
	mov.u32 	%r252, 16;
	mov.u32 	%r253, -1;
	shfl.sync.bfly.b32 	%r254|%p75, %r250, %r252, %r251, %r253;
	mov.b32 	%f300, %r254;
	add.f32 	%f301, %f336, %f300;
	mov.b32 	%r255, %f301;
	mov.u32 	%r256, 8;
	shfl.sync.bfly.b32 	%r257|%p76, %r255, %r256, %r251, %r253;
	mov.b32 	%f302, %r257;
	add.f32 	%f303, %f301, %f302;
	mov.b32 	%r258, %f303;
	mov.u32 	%r259, 4;
	shfl.sync.bfly.b32 	%r260|%p77, %r258, %r259, %r251, %r253;
	mov.b32 	%f304, %r260;
	add.f32 	%f305, %f303, %f304;
	mov.b32 	%r261, %f305;
	mov.u32 	%r262, 2;
	shfl.sync.bfly.b32 	%r263|%p78, %r261, %r262, %r251, %r253;
	mov.b32 	%f306, %r263;
	add.f32 	%f307, %f305, %f306;
	mov.b32 	%r264, %f307;
	mov.u32 	%r265, 1;
	shfl.sync.bfly.b32 	%r266|%p79, %r264, %r265, %r251, %r253;
	mov.b32 	%f308, %r266;
	add.f32 	%f309, %f307, %f308;
	st.local.f32 	[%rd2+24], %f309;
	st.shared.f32 	[%r10], %f309;
	bar.sync 	0;
	@%p1 bra 	$L__BB54_23;

	ld.shared.f32 	%f310, [%r4];
	mov.b32 	%r267, %f310;
	shfl.sync.bfly.b32 	%r271|%p80, %r267, %r252, %r251, %r253;
	mov.b32 	%f311, %r271;
	add.f32 	%f312, %f310, %f311;
	mov.b32 	%r272, %f312;
	shfl.sync.bfly.b32 	%r274|%p81, %r272, %r256, %r251, %r253;
	mov.b32 	%f313, %r274;
	add.f32 	%f314, %f312, %f313;
	mov.b32 	%r275, %f314;
	shfl.sync.bfly.b32 	%r277|%p82, %r275, %r259, %r251, %r253;
	mov.b32 	%f315, %r277;
	add.f32 	%f316, %f314, %f315;
	mov.b32 	%r278, %f316;
	shfl.sync.bfly.b32 	%r280|%p83, %r278, %r262, %r251, %r253;
	mov.b32 	%f317, %r280;
	add.f32 	%f318, %f316, %f317;
	mov.b32 	%r281, %f318;
	shfl.sync.bfly.b32 	%r283|%p84, %r281, %r265, %r251, %r253;
	mov.b32 	%f319, %r283;
	add.f32 	%f320, %f318, %f319;
	st.local.f32 	[%rd2+24], %f320;

$L__BB54_23:
	bar.sync 	0;
	setp.gt.s32 	%p85, %r3, 6;
	@%p85 bra 	$L__BB54_25;

	mul.wide.s32 	%rd64, %r3, 4;
	add.s64 	%rd65, %rd2, %rd64;
	ld.local.f32 	%f321, [%rd65];
	mad.lo.s32 	%r284, %r3, %r13, %r2;
	cvt.s64.s32 	%rd66, %r284;
	mul.lo.s32 	%r285, %r1, %r14;
	cvt.s64.s32 	%rd67, %r285;
	add.s64 	%rd68, %rd67, %rd66;
	cvta.to.global.u64 	%rd69, %rd20;
	shl.b64 	%rd70, %rd68, 2;
	add.s64 	%rd71, %rd69, %rd70;
	st.global.f32 	[%rd71], %f321;

$L__BB54_25:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_8_bs_224
.visible .entry ggml_matvec_f32_ncols_8_bs_224(
	.param .u64 ggml_matvec_f32_ncols_8_bs_224_param_0,
	.param .u64 ggml_matvec_f32_ncols_8_bs_224_param_1,
	.param .u64 ggml_matvec_f32_ncols_8_bs_224_param_2,
	.param .u32 ggml_matvec_f32_ncols_8_bs_224_param_3,
	.param .u32 ggml_matvec_f32_ncols_8_bs_224_param_4,
	.param .u32 ggml_matvec_f32_ncols_8_bs_224_param_5,
	.param .u32 ggml_matvec_f32_ncols_8_bs_224_param_6,
	.param .u32 ggml_matvec_f32_ncols_8_bs_224_param_7,
	.param .u32 ggml_matvec_f32_ncols_8_bs_224_param_8,
	.param .u32 ggml_matvec_f32_ncols_8_bs_224_param_9,
	.param .u32 ggml_matvec_f32_ncols_8_bs_224_param_10,
	.param .u32 ggml_matvec_f32_ncols_8_bs_224_param_11
)
{
	.local .align 16 .b8 	__local_depot55[32];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<97>;
	.reg .f32 	%f<390>;
	.reg .b32 	%r<322>;
	.reg .b64 	%rd<78>;


	mov.u64 	%SPL, __local_depot55;
	ld.param.u64 	%rd22, [ggml_matvec_f32_ncols_8_bs_224_param_0];
	ld.param.u64 	%rd23, [ggml_matvec_f32_ncols_8_bs_224_param_1];
	ld.param.u64 	%rd21, [ggml_matvec_f32_ncols_8_bs_224_param_2];
	ld.param.u32 	%r11, [ggml_matvec_f32_ncols_8_bs_224_param_3];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_8_bs_224_param_5];
	ld.param.u32 	%r12, [ggml_matvec_f32_ncols_8_bs_224_param_6];
	ld.param.u32 	%r13, [ggml_matvec_f32_ncols_8_bs_224_param_7];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_8_bs_224_param_8];
	ld.param.u32 	%r17, [ggml_matvec_f32_ncols_8_bs_224_param_9];
	ld.param.u32 	%r18, [ggml_matvec_f32_ncols_8_bs_224_param_10];
	ld.param.u32 	%r14, [ggml_matvec_f32_ncols_8_bs_224_param_11];
	cvta.to.global.u64 	%rd77, %rd23;
	add.u64 	%rd2, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r19, %r1, %r16;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r20, %r2, %r15;
	mad.lo.s32 	%r21, %r19, %r17, %r20;
	cvt.s64.s32 	%rd3, %r21;
	cvta.to.global.u64 	%rd4, %rd22;
	mul.lo.s32 	%r22, %r1, %r18;
	cvt.s64.s32 	%rd5, %r22;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r23, %r3, 2;
	mov.u32 	%r24, data_mmv;
	add.s32 	%r4, %r24, %r23;
	@%p1 bra 	$L__BB55_2;

	mov.u32 	%r25, 0;
	st.shared.u32 	[%r4], %r25;

$L__BB55_2:
	bar.sync 	0;
	mov.f32 	%f382, 0f00000000;
	st.local.v4.f32 	[%rd2], {%f382, %f382, %f382, %f382};
	st.local.v4.f32 	[%rd2+16], {%f382, %f382, %f382, %f382};
	setp.ge.s32 	%p2, %r3, %r11;
	mov.f32 	%f383, %f382;
	mov.f32 	%f384, %f382;
	mov.f32 	%f385, %f382;
	mov.f32 	%f386, %f382;
	mov.f32 	%f387, %f382;
	mov.f32 	%f388, %f382;
	mov.f32 	%f389, %f382;
	@%p2 bra 	$L__BB55_9;

	not.b32 	%r26, %r3;
	add.s32 	%r5, %r26, %r11;
	shr.u32 	%r27, %r5, 5;
	mul.wide.u32 	%rd25, %r27, 613566757;
	shr.u64 	%rd26, %rd25, 32;
	and.b64  	%rd27, %rd26, 1;
	setp.eq.b64 	%p3, %rd27, 1;
	mov.pred 	%p4, 0;
	xor.pred  	%p5, %p3, %p4;
	mov.f32 	%f382, 0f00000000;
	mov.u32 	%r321, %r3;
	@%p5 bra 	$L__BB55_5;

	shl.b64 	%rd28, %rd5, 2;
	add.s64 	%rd29, %rd77, %rd28;
	shl.b64 	%rd30, %rd3, 2;
	add.s64 	%rd31, %rd4, %rd30;
	mul.wide.s32 	%rd32, %r3, 8;
	add.s64 	%rd33, %rd31, %rd32;
	ld.global.nc.v2.f32 	{%f57, %f58}, [%rd33];
	add.s64 	%rd34, %rd29, %rd32;
	ld.global.nc.v2.f32 	{%f61, %f62}, [%rd34];
	fma.rn.f32 	%f65, %f57, %f61, 0f00000000;
	fma.rn.f32 	%f389, %f58, %f62, %f65;
	mul.wide.s32 	%rd35, %r12, 8;
	add.s64 	%rd36, %rd34, %rd35;
	ld.global.nc.v2.f32 	{%f66, %f67}, [%rd36];
	fma.rn.f32 	%f70, %f57, %f66, 0f00000000;
	fma.rn.f32 	%f388, %f58, %f67, %f70;
	add.s32 	%r28, %r3, %r12;
	add.s32 	%r29, %r28, %r12;
	mul.wide.s32 	%rd37, %r29, 8;
	add.s64 	%rd38, %rd29, %rd37;
	ld.global.nc.v2.f32 	{%f71, %f72}, [%rd38];
	fma.rn.f32 	%f75, %f57, %f71, 0f00000000;
	fma.rn.f32 	%f387, %f58, %f72, %f75;
	add.s64 	%rd39, %rd38, %rd35;
	ld.global.nc.v2.f32 	{%f76, %f77}, [%rd39];
	fma.rn.f32 	%f80, %f57, %f76, 0f00000000;
	fma.rn.f32 	%f386, %f58, %f77, %f80;
	st.local.v4.f32 	[%rd2], {%f389, %f388, %f387, %f386};
	add.s64 	%rd40, %rd39, %rd35;
	ld.global.nc.v2.f32 	{%f81, %f82}, [%rd40];
	fma.rn.f32 	%f85, %f57, %f81, 0f00000000;
	fma.rn.f32 	%f385, %f58, %f82, %f85;
	add.s64 	%rd41, %rd40, %rd35;
	ld.global.nc.v2.f32 	{%f86, %f87}, [%rd41];
	fma.rn.f32 	%f90, %f57, %f86, 0f00000000;
	fma.rn.f32 	%f384, %f58, %f87, %f90;
	add.s64 	%rd42, %rd41, %rd35;
	ld.global.nc.v2.f32 	{%f91, %f92}, [%rd42];
	fma.rn.f32 	%f95, %f57, %f91, 0f00000000;
	fma.rn.f32 	%f383, %f58, %f92, %f95;
	add.s64 	%rd43, %rd42, %rd35;
	ld.global.nc.v2.f32 	{%f96, %f97}, [%rd43];
	fma.rn.f32 	%f100, %f57, %f96, 0f00000000;
	fma.rn.f32 	%f382, %f58, %f97, %f100;
	st.local.v4.f32 	[%rd2+16], {%f385, %f384, %f383, %f382};
	add.s32 	%r321, %r3, 224;

$L__BB55_5:
	setp.lt.u32 	%p6, %r5, 224;
	@%p6 bra 	$L__BB55_9;

	add.s32 	%r30, %r321, %r12;
	add.s32 	%r31, %r30, 224;
	mul.wide.s32 	%rd44, %r31, 8;
	shl.b64 	%rd45, %rd5, 2;
	add.s64 	%rd7, %rd44, %rd45;
	shl.b32 	%r32, %r12, 1;
	add.s32 	%r33, %r321, %r32;
	mad.lo.s32 	%r34, %r12, 3, %r321;
	shl.b32 	%r35, %r12, 2;
	add.s32 	%r36, %r321, %r35;
	mad.lo.s32 	%r37, %r12, 5, %r321;
	mad.lo.s32 	%r38, %r12, 6, %r321;
	mad.lo.s32 	%r39, %r12, 7, %r321;
	mul.wide.s32 	%rd46, %r33, 8;
	add.s64 	%rd8, %rd46, %rd45;
	mul.wide.s32 	%rd47, %r34, 8;
	add.s64 	%rd9, %rd47, %rd45;
	mul.wide.s32 	%rd48, %r36, 8;
	add.s64 	%rd10, %rd48, %rd45;
	mul.wide.s32 	%rd49, %r37, 8;
	add.s64 	%rd11, %rd49, %rd45;
	mul.wide.s32 	%rd50, %r38, 8;
	add.s64 	%rd12, %rd50, %rd45;
	mul.wide.s32 	%rd51, %r39, 8;
	add.s64 	%rd13, %rd51, %rd45;
	mul.wide.s32 	%rd52, %r321, 2;
	add.s64 	%rd53, %rd52, %rd3;
	shl.b64 	%rd54, %rd53, 2;
	add.s64 	%rd55, %rd4, %rd54;
	add.s64 	%rd76, %rd55, 1792;
	mul.wide.s32 	%rd56, %r321, 8;
	mul.wide.s32 	%rd57, %r12, 8;
	add.s64 	%rd58, %rd56, %rd57;
	add.s64 	%rd15, %rd58, %rd45;
	add.s64 	%rd16, %rd56, %rd45;

$L__BB55_7:
	ld.global.nc.v2.f32 	{%f101, %f102}, [%rd76+-1792];
	add.s64 	%rd59, %rd77, %rd16;
	ld.global.nc.v2.f32 	{%f105, %f106}, [%rd59];
	fma.rn.f32 	%f109, %f101, %f105, %f389;
	fma.rn.f32 	%f110, %f102, %f106, %f109;
	add.s64 	%rd60, %rd77, %rd15;
	ld.global.nc.v2.f32 	{%f111, %f112}, [%rd60];
	fma.rn.f32 	%f115, %f101, %f111, %f388;
	fma.rn.f32 	%f116, %f102, %f112, %f115;
	add.s64 	%rd61, %rd77, %rd8;
	ld.global.nc.v2.f32 	{%f117, %f118}, [%rd61];
	fma.rn.f32 	%f121, %f101, %f117, %f387;
	fma.rn.f32 	%f122, %f102, %f118, %f121;
	add.s64 	%rd62, %rd77, %rd9;
	ld.global.nc.v2.f32 	{%f123, %f124}, [%rd62];
	fma.rn.f32 	%f127, %f101, %f123, %f386;
	fma.rn.f32 	%f128, %f102, %f124, %f127;
	add.s64 	%rd63, %rd77, %rd10;
	ld.global.nc.v2.f32 	{%f129, %f130}, [%rd63];
	fma.rn.f32 	%f133, %f101, %f129, %f385;
	fma.rn.f32 	%f134, %f102, %f130, %f133;
	add.s64 	%rd64, %rd77, %rd11;
	ld.global.nc.v2.f32 	{%f135, %f136}, [%rd64];
	fma.rn.f32 	%f139, %f101, %f135, %f384;
	fma.rn.f32 	%f140, %f102, %f136, %f139;
	add.s64 	%rd65, %rd77, %rd12;
	ld.global.nc.v2.f32 	{%f141, %f142}, [%rd65];
	fma.rn.f32 	%f145, %f101, %f141, %f383;
	fma.rn.f32 	%f146, %f102, %f142, %f145;
	add.s64 	%rd66, %rd77, %rd13;
	ld.global.nc.v2.f32 	{%f147, %f148}, [%rd66];
	fma.rn.f32 	%f151, %f101, %f147, %f382;
	fma.rn.f32 	%f152, %f102, %f148, %f151;
	ld.global.nc.v2.f32 	{%f153, %f154}, [%rd76];
	ld.global.nc.v2.f32 	{%f157, %f158}, [%rd59+1792];
	fma.rn.f32 	%f161, %f153, %f157, %f110;
	fma.rn.f32 	%f389, %f154, %f158, %f161;
	add.s64 	%rd67, %rd77, %rd7;
	ld.global.nc.v2.f32 	{%f162, %f163}, [%rd67];
	fma.rn.f32 	%f166, %f153, %f162, %f116;
	fma.rn.f32 	%f388, %f154, %f163, %f166;
	ld.global.nc.v2.f32 	{%f167, %f168}, [%rd61+1792];
	fma.rn.f32 	%f171, %f153, %f167, %f122;
	fma.rn.f32 	%f387, %f154, %f168, %f171;
	ld.global.nc.v2.f32 	{%f172, %f173}, [%rd62+1792];
	fma.rn.f32 	%f176, %f153, %f172, %f128;
	fma.rn.f32 	%f386, %f154, %f173, %f176;
	ld.global.nc.v2.f32 	{%f177, %f178}, [%rd63+1792];
	fma.rn.f32 	%f181, %f153, %f177, %f134;
	fma.rn.f32 	%f385, %f154, %f178, %f181;
	ld.global.nc.v2.f32 	{%f182, %f183}, [%rd64+1792];
	fma.rn.f32 	%f186, %f153, %f182, %f140;
	fma.rn.f32 	%f384, %f154, %f183, %f186;
	ld.global.nc.v2.f32 	{%f187, %f188}, [%rd65+1792];
	fma.rn.f32 	%f191, %f153, %f187, %f146;
	fma.rn.f32 	%f383, %f154, %f188, %f191;
	ld.global.nc.v2.f32 	{%f192, %f193}, [%rd66+1792];
	fma.rn.f32 	%f196, %f153, %f192, %f152;
	fma.rn.f32 	%f382, %f154, %f193, %f196;
	add.s64 	%rd77, %rd77, 3584;
	add.s64 	%rd76, %rd76, 3584;
	add.s32 	%r321, %r321, 448;
	setp.lt.s32 	%p7, %r321, %r11;
	@%p7 bra 	$L__BB55_7;

	st.local.v4.f32 	[%rd2], {%f389, %f388, %f387, %f386};
	st.local.v4.f32 	[%rd2+16], {%f385, %f384, %f383, %f382};

$L__BB55_9:
	shr.s32 	%r40, %r3, 31;
	shr.u32 	%r41, %r40, 27;
	add.s32 	%r42, %r3, %r41;
	shr.s32 	%r43, %r42, 5;
	shl.b32 	%r44, %r43, 2;
	add.s32 	%r10, %r24, %r44;
	mov.u32 	%r46, 2;
	mov.b32 	%r47, %f389;
	mov.u32 	%r48, 31;
	mov.u32 	%r49, 16;
	mov.u32 	%r50, -1;
	shfl.sync.bfly.b32 	%r51|%p8, %r47, %r49, %r48, %r50;
	mov.b32 	%f197, %r51;
	add.f32 	%f198, %f389, %f197;
	mov.b32 	%r52, %f198;
	mov.u32 	%r53, 8;
	shfl.sync.bfly.b32 	%r54|%p9, %r52, %r53, %r48, %r50;
	mov.b32 	%f199, %r54;
	add.f32 	%f200, %f198, %f199;
	mov.b32 	%r55, %f200;
	mov.u32 	%r56, 4;
	shfl.sync.bfly.b32 	%r57|%p10, %r55, %r56, %r48, %r50;
	mov.b32 	%f201, %r57;
	add.f32 	%f202, %f200, %f201;
	mov.b32 	%r58, %f202;
	shfl.sync.bfly.b32 	%r59|%p11, %r58, %r46, %r48, %r50;
	mov.b32 	%f203, %r59;
	add.f32 	%f204, %f202, %f203;
	mov.b32 	%r60, %f204;
	mov.u32 	%r61, 1;
	shfl.sync.bfly.b32 	%r62|%p12, %r60, %r61, %r48, %r50;
	mov.b32 	%f205, %r62;
	add.f32 	%f206, %f204, %f205;
	st.local.f32 	[%rd2], %f206;
	st.shared.f32 	[%r10], %f206;
	bar.sync 	0;
	@%p1 bra 	$L__BB55_11;

	ld.shared.f32 	%f207, [%r4];
	mov.b32 	%r63, %f207;
	shfl.sync.bfly.b32 	%r67|%p14, %r63, %r49, %r48, %r50;
	mov.b32 	%f208, %r67;
	add.f32 	%f209, %f207, %f208;
	mov.b32 	%r68, %f209;
	shfl.sync.bfly.b32 	%r70|%p15, %r68, %r53, %r48, %r50;
	mov.b32 	%f210, %r70;
	add.f32 	%f211, %f209, %f210;
	mov.b32 	%r71, %f211;
	shfl.sync.bfly.b32 	%r73|%p16, %r71, %r56, %r48, %r50;
	mov.b32 	%f212, %r73;
	add.f32 	%f213, %f211, %f212;
	mov.b32 	%r74, %f213;
	shfl.sync.bfly.b32 	%r76|%p17, %r74, %r46, %r48, %r50;
	mov.b32 	%f214, %r76;
	add.f32 	%f215, %f213, %f214;
	mov.b32 	%r77, %f215;
	shfl.sync.bfly.b32 	%r79|%p18, %r77, %r61, %r48, %r50;
	mov.b32 	%f216, %r79;
	add.f32 	%f217, %f215, %f216;
	st.local.f32 	[%rd2], %f217;

$L__BB55_11:
	bar.sync 	0;
	mov.b32 	%r80, %f388;
	shfl.sync.bfly.b32 	%r84|%p20, %r80, %r49, %r48, %r50;
	mov.b32 	%f218, %r84;
	add.f32 	%f219, %f388, %f218;
	mov.b32 	%r85, %f219;
	shfl.sync.bfly.b32 	%r87|%p21, %r85, %r53, %r48, %r50;
	mov.b32 	%f220, %r87;
	add.f32 	%f221, %f219, %f220;
	mov.b32 	%r88, %f221;
	shfl.sync.bfly.b32 	%r90|%p22, %r88, %r56, %r48, %r50;
	mov.b32 	%f222, %r90;
	add.f32 	%f223, %f221, %f222;
	mov.b32 	%r91, %f223;
	shfl.sync.bfly.b32 	%r93|%p23, %r91, %r46, %r48, %r50;
	mov.b32 	%f224, %r93;
	add.f32 	%f225, %f223, %f224;
	mov.b32 	%r94, %f225;
	shfl.sync.bfly.b32 	%r96|%p24, %r94, %r61, %r48, %r50;
	mov.b32 	%f226, %r96;
	add.f32 	%f227, %f225, %f226;
	st.local.f32 	[%rd2+4], %f227;
	st.shared.f32 	[%r10], %f227;
	bar.sync 	0;
	@%p1 bra 	$L__BB55_13;

	ld.shared.f32 	%f228, [%r4];
	mov.b32 	%r97, %f228;
	mov.u32 	%r98, 31;
	mov.u32 	%r99, 16;
	mov.u32 	%r100, -1;
	shfl.sync.bfly.b32 	%r101|%p25, %r97, %r99, %r98, %r100;
	mov.b32 	%f229, %r101;
	add.f32 	%f230, %f228, %f229;
	mov.b32 	%r102, %f230;
	mov.u32 	%r103, 8;
	shfl.sync.bfly.b32 	%r104|%p26, %r102, %r103, %r98, %r100;
	mov.b32 	%f231, %r104;
	add.f32 	%f232, %f230, %f231;
	mov.b32 	%r105, %f232;
	mov.u32 	%r106, 4;
	shfl.sync.bfly.b32 	%r107|%p27, %r105, %r106, %r98, %r100;
	mov.b32 	%f233, %r107;
	add.f32 	%f234, %f232, %f233;
	mov.b32 	%r108, %f234;
	mov.u32 	%r109, 2;
	shfl.sync.bfly.b32 	%r110|%p28, %r108, %r109, %r98, %r100;
	mov.b32 	%f235, %r110;
	add.f32 	%f236, %f234, %f235;
	mov.b32 	%r111, %f236;
	mov.u32 	%r112, 1;
	shfl.sync.bfly.b32 	%r113|%p29, %r111, %r112, %r98, %r100;
	mov.b32 	%f237, %r113;
	add.f32 	%f238, %f236, %f237;
	st.local.f32 	[%rd2+4], %f238;

$L__BB55_13:
	bar.sync 	0;
	mov.b32 	%r114, %f387;
	mov.u32 	%r115, 31;
	mov.u32 	%r116, 16;
	mov.u32 	%r117, -1;
	shfl.sync.bfly.b32 	%r118|%p31, %r114, %r116, %r115, %r117;
	mov.b32 	%f239, %r118;
	add.f32 	%f240, %f387, %f239;
	mov.b32 	%r119, %f240;
	mov.u32 	%r120, 8;
	shfl.sync.bfly.b32 	%r121|%p32, %r119, %r120, %r115, %r117;
	mov.b32 	%f241, %r121;
	add.f32 	%f242, %f240, %f241;
	mov.b32 	%r122, %f242;
	mov.u32 	%r123, 4;
	shfl.sync.bfly.b32 	%r124|%p33, %r122, %r123, %r115, %r117;
	mov.b32 	%f243, %r124;
	add.f32 	%f244, %f242, %f243;
	mov.b32 	%r125, %f244;
	mov.u32 	%r126, 2;
	shfl.sync.bfly.b32 	%r127|%p34, %r125, %r126, %r115, %r117;
	mov.b32 	%f245, %r127;
	add.f32 	%f246, %f244, %f245;
	mov.b32 	%r128, %f246;
	mov.u32 	%r129, 1;
	shfl.sync.bfly.b32 	%r130|%p35, %r128, %r129, %r115, %r117;
	mov.b32 	%f247, %r130;
	add.f32 	%f248, %f246, %f247;
	st.local.f32 	[%rd2+8], %f248;
	st.shared.f32 	[%r10], %f248;
	bar.sync 	0;
	@%p1 bra 	$L__BB55_15;

	ld.shared.f32 	%f249, [%r4];
	mov.b32 	%r131, %f249;
	shfl.sync.bfly.b32 	%r135|%p36, %r131, %r116, %r115, %r117;
	mov.b32 	%f250, %r135;
	add.f32 	%f251, %f249, %f250;
	mov.b32 	%r136, %f251;
	shfl.sync.bfly.b32 	%r138|%p37, %r136, %r120, %r115, %r117;
	mov.b32 	%f252, %r138;
	add.f32 	%f253, %f251, %f252;
	mov.b32 	%r139, %f253;
	shfl.sync.bfly.b32 	%r141|%p38, %r139, %r123, %r115, %r117;
	mov.b32 	%f254, %r141;
	add.f32 	%f255, %f253, %f254;
	mov.b32 	%r142, %f255;
	shfl.sync.bfly.b32 	%r144|%p39, %r142, %r126, %r115, %r117;
	mov.b32 	%f256, %r144;
	add.f32 	%f257, %f255, %f256;
	mov.b32 	%r145, %f257;
	shfl.sync.bfly.b32 	%r147|%p40, %r145, %r129, %r115, %r117;
	mov.b32 	%f258, %r147;
	add.f32 	%f259, %f257, %f258;
	st.local.f32 	[%rd2+8], %f259;

$L__BB55_15:
	bar.sync 	0;
	mov.b32 	%r148, %f386;
	shfl.sync.bfly.b32 	%r152|%p42, %r148, %r116, %r115, %r117;
	mov.b32 	%f260, %r152;
	add.f32 	%f261, %f386, %f260;
	mov.b32 	%r153, %f261;
	shfl.sync.bfly.b32 	%r155|%p43, %r153, %r120, %r115, %r117;
	mov.b32 	%f262, %r155;
	add.f32 	%f263, %f261, %f262;
	mov.b32 	%r156, %f263;
	shfl.sync.bfly.b32 	%r158|%p44, %r156, %r123, %r115, %r117;
	mov.b32 	%f264, %r158;
	add.f32 	%f265, %f263, %f264;
	mov.b32 	%r159, %f265;
	shfl.sync.bfly.b32 	%r161|%p45, %r159, %r126, %r115, %r117;
	mov.b32 	%f266, %r161;
	add.f32 	%f267, %f265, %f266;
	mov.b32 	%r162, %f267;
	shfl.sync.bfly.b32 	%r164|%p46, %r162, %r129, %r115, %r117;
	mov.b32 	%f268, %r164;
	add.f32 	%f269, %f267, %f268;
	st.local.f32 	[%rd2+12], %f269;
	st.shared.f32 	[%r10], %f269;
	bar.sync 	0;
	@%p1 bra 	$L__BB55_17;

	ld.shared.f32 	%f270, [%r4];
	mov.b32 	%r165, %f270;
	mov.u32 	%r166, 31;
	mov.u32 	%r167, 16;
	mov.u32 	%r168, -1;
	shfl.sync.bfly.b32 	%r169|%p47, %r165, %r167, %r166, %r168;
	mov.b32 	%f271, %r169;
	add.f32 	%f272, %f270, %f271;
	mov.b32 	%r170, %f272;
	mov.u32 	%r171, 8;
	shfl.sync.bfly.b32 	%r172|%p48, %r170, %r171, %r166, %r168;
	mov.b32 	%f273, %r172;
	add.f32 	%f274, %f272, %f273;
	mov.b32 	%r173, %f274;
	mov.u32 	%r174, 4;
	shfl.sync.bfly.b32 	%r175|%p49, %r173, %r174, %r166, %r168;
	mov.b32 	%f275, %r175;
	add.f32 	%f276, %f274, %f275;
	mov.b32 	%r176, %f276;
	mov.u32 	%r177, 2;
	shfl.sync.bfly.b32 	%r178|%p50, %r176, %r177, %r166, %r168;
	mov.b32 	%f277, %r178;
	add.f32 	%f278, %f276, %f277;
	mov.b32 	%r179, %f278;
	mov.u32 	%r180, 1;
	shfl.sync.bfly.b32 	%r181|%p51, %r179, %r180, %r166, %r168;
	mov.b32 	%f279, %r181;
	add.f32 	%f280, %f278, %f279;
	st.local.f32 	[%rd2+12], %f280;

$L__BB55_17:
	bar.sync 	0;
	mov.b32 	%r182, %f385;
	mov.u32 	%r183, 31;
	mov.u32 	%r184, 16;
	mov.u32 	%r185, -1;
	shfl.sync.bfly.b32 	%r186|%p53, %r182, %r184, %r183, %r185;
	mov.b32 	%f281, %r186;
	add.f32 	%f282, %f385, %f281;
	mov.b32 	%r187, %f282;
	mov.u32 	%r188, 8;
	shfl.sync.bfly.b32 	%r189|%p54, %r187, %r188, %r183, %r185;
	mov.b32 	%f283, %r189;
	add.f32 	%f284, %f282, %f283;
	mov.b32 	%r190, %f284;
	mov.u32 	%r191, 4;
	shfl.sync.bfly.b32 	%r192|%p55, %r190, %r191, %r183, %r185;
	mov.b32 	%f285, %r192;
	add.f32 	%f286, %f284, %f285;
	mov.b32 	%r193, %f286;
	mov.u32 	%r194, 2;
	shfl.sync.bfly.b32 	%r195|%p56, %r193, %r194, %r183, %r185;
	mov.b32 	%f287, %r195;
	add.f32 	%f288, %f286, %f287;
	mov.b32 	%r196, %f288;
	mov.u32 	%r197, 1;
	shfl.sync.bfly.b32 	%r198|%p57, %r196, %r197, %r183, %r185;
	mov.b32 	%f289, %r198;
	add.f32 	%f290, %f288, %f289;
	st.local.f32 	[%rd2+16], %f290;
	st.shared.f32 	[%r10], %f290;
	bar.sync 	0;
	@%p1 bra 	$L__BB55_19;

	ld.shared.f32 	%f291, [%r4];
	mov.b32 	%r199, %f291;
	shfl.sync.bfly.b32 	%r203|%p58, %r199, %r184, %r183, %r185;
	mov.b32 	%f292, %r203;
	add.f32 	%f293, %f291, %f292;
	mov.b32 	%r204, %f293;
	shfl.sync.bfly.b32 	%r206|%p59, %r204, %r188, %r183, %r185;
	mov.b32 	%f294, %r206;
	add.f32 	%f295, %f293, %f294;
	mov.b32 	%r207, %f295;
	shfl.sync.bfly.b32 	%r209|%p60, %r207, %r191, %r183, %r185;
	mov.b32 	%f296, %r209;
	add.f32 	%f297, %f295, %f296;
	mov.b32 	%r210, %f297;
	shfl.sync.bfly.b32 	%r212|%p61, %r210, %r194, %r183, %r185;
	mov.b32 	%f298, %r212;
	add.f32 	%f299, %f297, %f298;
	mov.b32 	%r213, %f299;
	shfl.sync.bfly.b32 	%r215|%p62, %r213, %r197, %r183, %r185;
	mov.b32 	%f300, %r215;
	add.f32 	%f301, %f299, %f300;
	st.local.f32 	[%rd2+16], %f301;

$L__BB55_19:
	bar.sync 	0;
	mov.b32 	%r216, %f384;
	shfl.sync.bfly.b32 	%r220|%p64, %r216, %r184, %r183, %r185;
	mov.b32 	%f302, %r220;
	add.f32 	%f303, %f384, %f302;
	mov.b32 	%r221, %f303;
	shfl.sync.bfly.b32 	%r223|%p65, %r221, %r188, %r183, %r185;
	mov.b32 	%f304, %r223;
	add.f32 	%f305, %f303, %f304;
	mov.b32 	%r224, %f305;
	shfl.sync.bfly.b32 	%r226|%p66, %r224, %r191, %r183, %r185;
	mov.b32 	%f306, %r226;
	add.f32 	%f307, %f305, %f306;
	mov.b32 	%r227, %f307;
	shfl.sync.bfly.b32 	%r229|%p67, %r227, %r194, %r183, %r185;
	mov.b32 	%f308, %r229;
	add.f32 	%f309, %f307, %f308;
	mov.b32 	%r230, %f309;
	shfl.sync.bfly.b32 	%r232|%p68, %r230, %r197, %r183, %r185;
	mov.b32 	%f310, %r232;
	add.f32 	%f311, %f309, %f310;
	st.local.f32 	[%rd2+20], %f311;
	st.shared.f32 	[%r10], %f311;
	bar.sync 	0;
	@%p1 bra 	$L__BB55_21;

	ld.shared.f32 	%f312, [%r4];
	mov.b32 	%r233, %f312;
	mov.u32 	%r234, 31;
	mov.u32 	%r235, 16;
	mov.u32 	%r236, -1;
	shfl.sync.bfly.b32 	%r237|%p69, %r233, %r235, %r234, %r236;
	mov.b32 	%f313, %r237;
	add.f32 	%f314, %f312, %f313;
	mov.b32 	%r238, %f314;
	mov.u32 	%r239, 8;
	shfl.sync.bfly.b32 	%r240|%p70, %r238, %r239, %r234, %r236;
	mov.b32 	%f315, %r240;
	add.f32 	%f316, %f314, %f315;
	mov.b32 	%r241, %f316;
	mov.u32 	%r242, 4;
	shfl.sync.bfly.b32 	%r243|%p71, %r241, %r242, %r234, %r236;
	mov.b32 	%f317, %r243;
	add.f32 	%f318, %f316, %f317;
	mov.b32 	%r244, %f318;
	mov.u32 	%r245, 2;
	shfl.sync.bfly.b32 	%r246|%p72, %r244, %r245, %r234, %r236;
	mov.b32 	%f319, %r246;
	add.f32 	%f320, %f318, %f319;
	mov.b32 	%r247, %f320;
	mov.u32 	%r248, 1;
	shfl.sync.bfly.b32 	%r249|%p73, %r247, %r248, %r234, %r236;
	mov.b32 	%f321, %r249;
	add.f32 	%f322, %f320, %f321;
	st.local.f32 	[%rd2+20], %f322;

$L__BB55_21:
	bar.sync 	0;
	mov.b32 	%r250, %f383;
	mov.u32 	%r251, 31;
	mov.u32 	%r252, 16;
	mov.u32 	%r253, -1;
	shfl.sync.bfly.b32 	%r254|%p75, %r250, %r252, %r251, %r253;
	mov.b32 	%f323, %r254;
	add.f32 	%f324, %f383, %f323;
	mov.b32 	%r255, %f324;
	mov.u32 	%r256, 8;
	shfl.sync.bfly.b32 	%r257|%p76, %r255, %r256, %r251, %r253;
	mov.b32 	%f325, %r257;
	add.f32 	%f326, %f324, %f325;
	mov.b32 	%r258, %f326;
	mov.u32 	%r259, 4;
	shfl.sync.bfly.b32 	%r260|%p77, %r258, %r259, %r251, %r253;
	mov.b32 	%f327, %r260;
	add.f32 	%f328, %f326, %f327;
	mov.b32 	%r261, %f328;
	mov.u32 	%r262, 2;
	shfl.sync.bfly.b32 	%r263|%p78, %r261, %r262, %r251, %r253;
	mov.b32 	%f329, %r263;
	add.f32 	%f330, %f328, %f329;
	mov.b32 	%r264, %f330;
	mov.u32 	%r265, 1;
	shfl.sync.bfly.b32 	%r266|%p79, %r264, %r265, %r251, %r253;
	mov.b32 	%f331, %r266;
	add.f32 	%f332, %f330, %f331;
	st.local.f32 	[%rd2+24], %f332;
	st.shared.f32 	[%r10], %f332;
	bar.sync 	0;
	@%p1 bra 	$L__BB55_23;

	ld.shared.f32 	%f333, [%r4];
	mov.b32 	%r267, %f333;
	shfl.sync.bfly.b32 	%r271|%p80, %r267, %r252, %r251, %r253;
	mov.b32 	%f334, %r271;
	add.f32 	%f335, %f333, %f334;
	mov.b32 	%r272, %f335;
	shfl.sync.bfly.b32 	%r274|%p81, %r272, %r256, %r251, %r253;
	mov.b32 	%f336, %r274;
	add.f32 	%f337, %f335, %f336;
	mov.b32 	%r275, %f337;
	shfl.sync.bfly.b32 	%r277|%p82, %r275, %r259, %r251, %r253;
	mov.b32 	%f338, %r277;
	add.f32 	%f339, %f337, %f338;
	mov.b32 	%r278, %f339;
	shfl.sync.bfly.b32 	%r280|%p83, %r278, %r262, %r251, %r253;
	mov.b32 	%f340, %r280;
	add.f32 	%f341, %f339, %f340;
	mov.b32 	%r281, %f341;
	shfl.sync.bfly.b32 	%r283|%p84, %r281, %r265, %r251, %r253;
	mov.b32 	%f342, %r283;
	add.f32 	%f343, %f341, %f342;
	st.local.f32 	[%rd2+24], %f343;

$L__BB55_23:
	bar.sync 	0;
	mov.b32 	%r284, %f382;
	shfl.sync.bfly.b32 	%r288|%p86, %r284, %r252, %r251, %r253;
	mov.b32 	%f344, %r288;
	add.f32 	%f345, %f382, %f344;
	mov.b32 	%r289, %f345;
	shfl.sync.bfly.b32 	%r291|%p87, %r289, %r256, %r251, %r253;
	mov.b32 	%f346, %r291;
	add.f32 	%f347, %f345, %f346;
	mov.b32 	%r292, %f347;
	shfl.sync.bfly.b32 	%r294|%p88, %r292, %r259, %r251, %r253;
	mov.b32 	%f348, %r294;
	add.f32 	%f349, %f347, %f348;
	mov.b32 	%r295, %f349;
	shfl.sync.bfly.b32 	%r297|%p89, %r295, %r262, %r251, %r253;
	mov.b32 	%f350, %r297;
	add.f32 	%f351, %f349, %f350;
	mov.b32 	%r298, %f351;
	shfl.sync.bfly.b32 	%r300|%p90, %r298, %r265, %r251, %r253;
	mov.b32 	%f352, %r300;
	add.f32 	%f353, %f351, %f352;
	st.local.f32 	[%rd2+28], %f353;
	st.shared.f32 	[%r10], %f353;
	bar.sync 	0;
	@%p1 bra 	$L__BB55_25;

	ld.shared.f32 	%f354, [%r4];
	mov.b32 	%r301, %f354;
	mov.u32 	%r302, 31;
	mov.u32 	%r303, 16;
	mov.u32 	%r304, -1;
	shfl.sync.bfly.b32 	%r305|%p91, %r301, %r303, %r302, %r304;
	mov.b32 	%f355, %r305;
	add.f32 	%f356, %f354, %f355;
	mov.b32 	%r306, %f356;
	mov.u32 	%r307, 8;
	shfl.sync.bfly.b32 	%r308|%p92, %r306, %r307, %r302, %r304;
	mov.b32 	%f357, %r308;
	add.f32 	%f358, %f356, %f357;
	mov.b32 	%r309, %f358;
	mov.u32 	%r310, 4;
	shfl.sync.bfly.b32 	%r311|%p93, %r309, %r310, %r302, %r304;
	mov.b32 	%f359, %r311;
	add.f32 	%f360, %f358, %f359;
	mov.b32 	%r312, %f360;
	mov.u32 	%r313, 2;
	shfl.sync.bfly.b32 	%r314|%p94, %r312, %r313, %r302, %r304;
	mov.b32 	%f361, %r314;
	add.f32 	%f362, %f360, %f361;
	mov.b32 	%r315, %f362;
	mov.u32 	%r316, 1;
	shfl.sync.bfly.b32 	%r317|%p95, %r315, %r316, %r302, %r304;
	mov.b32 	%f363, %r317;
	add.f32 	%f364, %f362, %f363;
	st.local.f32 	[%rd2+28], %f364;

$L__BB55_25:
	bar.sync 	0;
	setp.gt.s32 	%p96, %r3, 7;
	@%p96 bra 	$L__BB55_27;

	mul.wide.s32 	%rd68, %r3, 4;
	add.s64 	%rd69, %rd2, %rd68;
	ld.local.f32 	%f365, [%rd69];
	mad.lo.s32 	%r318, %r3, %r13, %r2;
	cvt.s64.s32 	%rd70, %r318;
	mul.lo.s32 	%r319, %r1, %r14;
	cvt.s64.s32 	%rd71, %r319;
	add.s64 	%rd72, %rd71, %rd70;
	cvta.to.global.u64 	%rd73, %rd21;
	shl.b64 	%rd74, %rd72, 2;
	add.s64 	%rd75, %rd73, %rd74;
	st.global.f32 	[%rd75], %f365;

$L__BB55_27:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_1_bs_256
.visible .entry ggml_matvec_f32_ncols_1_bs_256(
	.param .u64 ggml_matvec_f32_ncols_1_bs_256_param_0,
	.param .u64 ggml_matvec_f32_ncols_1_bs_256_param_1,
	.param .u64 ggml_matvec_f32_ncols_1_bs_256_param_2,
	.param .u32 ggml_matvec_f32_ncols_1_bs_256_param_3,
	.param .u32 ggml_matvec_f32_ncols_1_bs_256_param_4,
	.param .u32 ggml_matvec_f32_ncols_1_bs_256_param_5,
	.param .u32 ggml_matvec_f32_ncols_1_bs_256_param_6,
	.param .u32 ggml_matvec_f32_ncols_1_bs_256_param_7,
	.param .u32 ggml_matvec_f32_ncols_1_bs_256_param_8,
	.param .u32 ggml_matvec_f32_ncols_1_bs_256_param_9,
	.param .u32 ggml_matvec_f32_ncols_1_bs_256_param_10,
	.param .u32 ggml_matvec_f32_ncols_1_bs_256_param_11
)
{
	.reg .pred 	%p<19>;
	.reg .f32 	%f<88>;
	.reg .b32 	%r<79>;
	.reg .b64 	%rd<42>;


	ld.param.u64 	%rd18, [ggml_matvec_f32_ncols_1_bs_256_param_0];
	ld.param.u64 	%rd19, [ggml_matvec_f32_ncols_1_bs_256_param_1];
	ld.param.u64 	%rd17, [ggml_matvec_f32_ncols_1_bs_256_param_2];
	ld.param.u32 	%r13, [ggml_matvec_f32_ncols_1_bs_256_param_3];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_1_bs_256_param_5];
	ld.param.u32 	%r14, [ggml_matvec_f32_ncols_1_bs_256_param_7];
	ld.param.u32 	%r17, [ggml_matvec_f32_ncols_1_bs_256_param_8];
	ld.param.u32 	%r18, [ggml_matvec_f32_ncols_1_bs_256_param_9];
	ld.param.u32 	%r19, [ggml_matvec_f32_ncols_1_bs_256_param_10];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_1_bs_256_param_11];
	cvta.to.global.u64 	%rd1, %rd19;
	cvta.to.global.u64 	%rd2, %rd18;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r20, %r1, %r17;
	mov.u32 	%r21, %ctaid.x;
	mul.lo.s32 	%r22, %r21, %r16;
	mad.lo.s32 	%r23, %r20, %r18, %r22;
	cvt.s64.s32 	%rd3, %r23;
	mul.lo.s32 	%r24, %r1, %r19;
	cvt.s64.s32 	%rd4, %r24;
	mov.u32 	%r2, %tid.x;
	setp.gt.s32 	%p1, %r2, 31;
	shl.b32 	%r25, %r2, 2;
	mov.u32 	%r26, data_mmv;
	add.s32 	%r3, %r26, %r25;
	@%p1 bra 	$L__BB56_2;

	mov.u32 	%r27, 0;
	st.shared.u32 	[%r3], %r27;

$L__BB56_2:
	bar.sync 	0;
	setp.ge.s32 	%p2, %r2, %r13;
	mov.f32 	%f86, 0f00000000;
	@%p2 bra 	$L__BB56_9;

	not.b32 	%r28, %r2;
	add.s32 	%r4, %r28, %r13;
	shr.u32 	%r29, %r4, 8;
	add.s32 	%r30, %r29, 1;
	and.b32  	%r76, %r30, 3;
	setp.eq.s32 	%p3, %r76, 0;
	mov.f32 	%f86, 0f00000000;
	mov.u32 	%r77, %r2;
	@%p3 bra 	$L__BB56_6;

	mul.wide.s32 	%rd20, %r2, 2;
	add.s64 	%rd21, %rd20, %rd4;
	shl.b64 	%rd22, %rd21, 2;
	add.s64 	%rd39, %rd1, %rd22;
	add.s64 	%rd23, %rd20, %rd3;
	shl.b64 	%rd24, %rd23, 2;
	add.s64 	%rd38, %rd2, %rd24;
	mov.f32 	%f86, 0f00000000;
	mov.u32 	%r77, %r2;

$L__BB56_5:
	.pragma "nounroll";
	ld.global.nc.v2.f32 	{%f15, %f16}, [%rd38];
	ld.global.nc.v2.f32 	{%f19, %f20}, [%rd39];
	fma.rn.f32 	%f23, %f15, %f19, %f86;
	fma.rn.f32 	%f86, %f16, %f20, %f23;
	add.s32 	%r77, %r77, 256;
	add.s64 	%rd39, %rd39, 2048;
	add.s64 	%rd38, %rd38, 2048;
	add.s32 	%r76, %r76, -1;
	setp.ne.s32 	%p4, %r76, 0;
	@%p4 bra 	$L__BB56_5;

$L__BB56_6:
	setp.lt.u32 	%p5, %r4, 768;
	@%p5 bra 	$L__BB56_9;

	mul.wide.s32 	%rd25, %r77, 2;
	add.s64 	%rd26, %rd25, %rd3;
	shl.b64 	%rd27, %rd26, 2;
	add.s64 	%rd28, %rd2, %rd27;
	add.s64 	%rd41, %rd28, 4096;
	add.s64 	%rd29, %rd25, %rd4;
	shl.b64 	%rd30, %rd29, 2;
	add.s64 	%rd31, %rd1, %rd30;
	add.s64 	%rd40, %rd31, 4096;

$L__BB56_8:
	ld.global.nc.v2.f32 	{%f24, %f25}, [%rd41+-4096];
	ld.global.nc.v2.f32 	{%f28, %f29}, [%rd40+-4096];
	fma.rn.f32 	%f32, %f24, %f28, %f86;
	fma.rn.f32 	%f33, %f25, %f29, %f32;
	ld.global.nc.v2.f32 	{%f34, %f35}, [%rd41+-2048];
	ld.global.nc.v2.f32 	{%f38, %f39}, [%rd40+-2048];
	fma.rn.f32 	%f42, %f34, %f38, %f33;
	fma.rn.f32 	%f43, %f35, %f39, %f42;
	ld.global.nc.v2.f32 	{%f44, %f45}, [%rd41];
	ld.global.nc.v2.f32 	{%f48, %f49}, [%rd40];
	fma.rn.f32 	%f52, %f44, %f48, %f43;
	fma.rn.f32 	%f53, %f45, %f49, %f52;
	ld.global.nc.v2.f32 	{%f54, %f55}, [%rd41+2048];
	ld.global.nc.v2.f32 	{%f58, %f59}, [%rd40+2048];
	fma.rn.f32 	%f62, %f54, %f58, %f53;
	fma.rn.f32 	%f86, %f55, %f59, %f62;
	add.s64 	%rd41, %rd41, 8192;
	add.s64 	%rd40, %rd40, 8192;
	add.s32 	%r77, %r77, 1024;
	setp.lt.s32 	%p6, %r77, %r13;
	@%p6 bra 	$L__BB56_8;

$L__BB56_9:
	mov.b32 	%r31, %f86;
	mov.u32 	%r32, 31;
	mov.u32 	%r33, 16;
	mov.u32 	%r34, -1;
	shfl.sync.bfly.b32 	%r35|%p7, %r31, %r33, %r32, %r34;
	mov.b32 	%f63, %r35;
	add.f32 	%f64, %f86, %f63;
	mov.b32 	%r36, %f64;
	mov.u32 	%r37, 8;
	shfl.sync.bfly.b32 	%r38|%p8, %r36, %r37, %r32, %r34;
	mov.b32 	%f65, %r38;
	add.f32 	%f66, %f64, %f65;
	mov.b32 	%r39, %f66;
	mov.u32 	%r40, 4;
	shfl.sync.bfly.b32 	%r41|%p9, %r39, %r40, %r32, %r34;
	mov.b32 	%f67, %r41;
	add.f32 	%f68, %f66, %f67;
	mov.b32 	%r42, %f68;
	mov.u32 	%r43, 2;
	shfl.sync.bfly.b32 	%r44|%p10, %r42, %r43, %r32, %r34;
	mov.b32 	%f69, %r44;
	add.f32 	%f70, %f68, %f69;
	mov.b32 	%r45, %f70;
	mov.u32 	%r46, 1;
	shfl.sync.bfly.b32 	%r47|%p11, %r45, %r46, %r32, %r34;
	mov.b32 	%f71, %r47;
	add.f32 	%f87, %f70, %f71;
	shr.s32 	%r48, %r2, 31;
	shr.u32 	%r49, %r48, 27;
	add.s32 	%r50, %r2, %r49;
	shr.s32 	%r51, %r50, 5;
	shl.b32 	%r52, %r51, 2;
	add.s32 	%r54, %r26, %r52;
	st.shared.f32 	[%r54], %f87;
	bar.sync 	0;
	@%p1 bra 	$L__BB56_11;

	ld.shared.f32 	%f72, [%r3];
	mov.b32 	%r55, %f72;
	shfl.sync.bfly.b32 	%r59|%p13, %r55, %r33, %r32, %r34;
	mov.b32 	%f73, %r59;
	add.f32 	%f74, %f72, %f73;
	mov.b32 	%r60, %f74;
	shfl.sync.bfly.b32 	%r62|%p14, %r60, %r37, %r32, %r34;
	mov.b32 	%f75, %r62;
	add.f32 	%f76, %f74, %f75;
	mov.b32 	%r63, %f76;
	shfl.sync.bfly.b32 	%r65|%p15, %r63, %r40, %r32, %r34;
	mov.b32 	%f77, %r65;
	add.f32 	%f78, %f76, %f77;
	mov.b32 	%r66, %f78;
	shfl.sync.bfly.b32 	%r68|%p16, %r66, %r43, %r32, %r34;
	mov.b32 	%f79, %r68;
	add.f32 	%f80, %f78, %f79;
	mov.b32 	%r69, %f80;
	shfl.sync.bfly.b32 	%r71|%p17, %r69, %r46, %r32, %r34;
	mov.b32 	%f81, %r71;
	add.f32 	%f87, %f80, %f81;

$L__BB56_11:
	bar.sync 	0;
	setp.gt.s32 	%p18, %r2, 0;
	@%p18 bra 	$L__BB56_13;

	mad.lo.s32 	%r73, %r2, %r14, %r21;
	cvt.s64.s32 	%rd32, %r73;
	mul.lo.s32 	%r74, %r1, %r15;
	cvt.s64.s32 	%rd33, %r74;
	add.s64 	%rd34, %rd33, %rd32;
	cvta.to.global.u64 	%rd35, %rd17;
	shl.b64 	%rd36, %rd34, 2;
	add.s64 	%rd37, %rd35, %rd36;
	st.global.f32 	[%rd37], %f87;

$L__BB56_13:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_2_bs_256
.visible .entry ggml_matvec_f32_ncols_2_bs_256(
	.param .u64 ggml_matvec_f32_ncols_2_bs_256_param_0,
	.param .u64 ggml_matvec_f32_ncols_2_bs_256_param_1,
	.param .u64 ggml_matvec_f32_ncols_2_bs_256_param_2,
	.param .u32 ggml_matvec_f32_ncols_2_bs_256_param_3,
	.param .u32 ggml_matvec_f32_ncols_2_bs_256_param_4,
	.param .u32 ggml_matvec_f32_ncols_2_bs_256_param_5,
	.param .u32 ggml_matvec_f32_ncols_2_bs_256_param_6,
	.param .u32 ggml_matvec_f32_ncols_2_bs_256_param_7,
	.param .u32 ggml_matvec_f32_ncols_2_bs_256_param_8,
	.param .u32 ggml_matvec_f32_ncols_2_bs_256_param_9,
	.param .u32 ggml_matvec_f32_ncols_2_bs_256_param_10,
	.param .u32 ggml_matvec_f32_ncols_2_bs_256_param_11
)
{
	.local .align 8 .b8 	__local_depot57[8];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<30>;
	.reg .f32 	%f<146>;
	.reg .b32 	%r<113>;
	.reg .b64 	%rd<64>;


	mov.u64 	%SPL, __local_depot57;
	ld.param.u64 	%rd27, [ggml_matvec_f32_ncols_2_bs_256_param_0];
	ld.param.u64 	%rd28, [ggml_matvec_f32_ncols_2_bs_256_param_1];
	ld.param.u64 	%rd26, [ggml_matvec_f32_ncols_2_bs_256_param_2];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_2_bs_256_param_3];
	ld.param.u32 	%r19, [ggml_matvec_f32_ncols_2_bs_256_param_5];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_2_bs_256_param_6];
	ld.param.u32 	%r17, [ggml_matvec_f32_ncols_2_bs_256_param_7];
	ld.param.u32 	%r20, [ggml_matvec_f32_ncols_2_bs_256_param_8];
	ld.param.u32 	%r21, [ggml_matvec_f32_ncols_2_bs_256_param_9];
	ld.param.u32 	%r22, [ggml_matvec_f32_ncols_2_bs_256_param_10];
	ld.param.u32 	%r18, [ggml_matvec_f32_ncols_2_bs_256_param_11];
	cvta.to.global.u64 	%rd1, %rd28;
	cvta.to.global.u64 	%rd2, %rd27;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r23, %r1, %r20;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r24, %r2, %r19;
	mad.lo.s32 	%r25, %r23, %r21, %r24;
	cvt.s64.s32 	%rd4, %r25;
	mul.lo.s32 	%r26, %r1, %r22;
	cvt.s64.s32 	%rd5, %r26;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r27, %r3, 2;
	mov.u32 	%r28, data_mmv;
	add.s32 	%r4, %r28, %r27;
	@%p1 bra 	$L__BB57_2;

	mov.u32 	%r29, 0;
	st.shared.u32 	[%r4], %r29;

$L__BB57_2:
	bar.sync 	0;
	mov.f32 	%f144, 0f00000000;
	st.local.v2.f32 	[%rd3], {%f144, %f144};
	setp.ge.s32 	%p2, %r3, %r15;
	mov.f32 	%f145, %f144;
	@%p2 bra 	$L__BB57_11;

	not.b32 	%r30, %r3;
	add.s32 	%r5, %r30, %r15;
	shr.u32 	%r31, %r5, 8;
	add.s32 	%r32, %r31, 1;
	and.b32  	%r110, %r32, 3;
	setp.eq.s32 	%p3, %r110, 0;
	mov.f32 	%f144, 0f00000000;
	mov.u32 	%r111, %r3;
	@%p3 bra 	$L__BB57_7;

	mul.wide.s32 	%rd30, %r16, 2;
	mul.wide.s32 	%rd31, %r3, 2;
	add.s64 	%rd32, %rd30, %rd31;
	add.s64 	%rd33, %rd32, %rd5;
	shl.b64 	%rd34, %rd33, 2;
	add.s64 	%rd60, %rd1, %rd34;
	add.s64 	%rd35, %rd31, %rd5;
	shl.b64 	%rd36, %rd35, 2;
	add.s64 	%rd59, %rd1, %rd36;
	add.s64 	%rd37, %rd31, %rd4;
	shl.b64 	%rd38, %rd37, 2;
	add.s64 	%rd58, %rd2, %rd38;
	mov.f32 	%f144, 0f00000000;
	mov.f32 	%f145, %f144;
	mov.u32 	%r111, %r3;

$L__BB57_5:
	.pragma "nounroll";
	ld.global.nc.v2.f32 	{%f19, %f20}, [%rd58];
	ld.global.nc.v2.f32 	{%f23, %f24}, [%rd59];
	fma.rn.f32 	%f27, %f19, %f23, %f145;
	fma.rn.f32 	%f145, %f20, %f24, %f27;
	ld.global.nc.v2.f32 	{%f28, %f29}, [%rd60];
	fma.rn.f32 	%f32, %f19, %f28, %f144;
	fma.rn.f32 	%f144, %f20, %f29, %f32;
	add.s32 	%r111, %r111, 256;
	add.s64 	%rd60, %rd60, 2048;
	add.s64 	%rd59, %rd59, 2048;
	add.s64 	%rd58, %rd58, 2048;
	add.s32 	%r110, %r110, -1;
	setp.ne.s32 	%p4, %r110, 0;
	@%p4 bra 	$L__BB57_5;

	st.local.v2.f32 	[%rd3], {%f145, %f144};

$L__BB57_7:
	setp.lt.u32 	%p5, %r5, 768;
	@%p5 bra 	$L__BB57_11;

	mul.wide.s32 	%rd39, %r111, 2;
	add.s64 	%rd40, %rd39, %rd4;
	shl.b64 	%rd41, %rd40, 2;
	add.s64 	%rd42, %rd2, %rd41;
	add.s64 	%rd63, %rd42, 4096;
	add.s64 	%rd43, %rd39, %rd5;
	shl.b64 	%rd44, %rd43, 2;
	add.s64 	%rd45, %rd1, %rd44;
	add.s64 	%rd62, %rd45, 6144;
	mul.wide.s32 	%rd46, %r16, 2;
	add.s64 	%rd47, %rd43, %rd46;
	shl.b64 	%rd48, %rd47, 2;
	add.s64 	%rd49, %rd1, %rd48;
	add.s64 	%rd61, %rd49, 4096;

$L__BB57_9:
	ld.global.nc.v2.f32 	{%f33, %f34}, [%rd63+-4096];
	ld.global.nc.v2.f32 	{%f37, %f38}, [%rd62+-6144];
	fma.rn.f32 	%f41, %f33, %f37, %f145;
	fma.rn.f32 	%f42, %f34, %f38, %f41;
	ld.global.nc.v2.f32 	{%f43, %f44}, [%rd61+-4096];
	fma.rn.f32 	%f47, %f33, %f43, %f144;
	fma.rn.f32 	%f48, %f34, %f44, %f47;
	ld.global.nc.v2.f32 	{%f49, %f50}, [%rd63+-2048];
	ld.global.nc.v2.f32 	{%f53, %f54}, [%rd62+-4096];
	fma.rn.f32 	%f57, %f49, %f53, %f42;
	fma.rn.f32 	%f58, %f50, %f54, %f57;
	ld.global.nc.v2.f32 	{%f59, %f60}, [%rd61+-2048];
	fma.rn.f32 	%f63, %f49, %f59, %f48;
	fma.rn.f32 	%f64, %f50, %f60, %f63;
	ld.global.nc.v2.f32 	{%f65, %f66}, [%rd63];
	ld.global.nc.v2.f32 	{%f69, %f70}, [%rd62+-2048];
	fma.rn.f32 	%f73, %f65, %f69, %f58;
	fma.rn.f32 	%f74, %f66, %f70, %f73;
	ld.global.nc.v2.f32 	{%f75, %f76}, [%rd61];
	fma.rn.f32 	%f79, %f65, %f75, %f64;
	fma.rn.f32 	%f80, %f66, %f76, %f79;
	ld.global.nc.v2.f32 	{%f81, %f82}, [%rd63+2048];
	ld.global.nc.v2.f32 	{%f85, %f86}, [%rd62];
	fma.rn.f32 	%f89, %f81, %f85, %f74;
	fma.rn.f32 	%f145, %f82, %f86, %f89;
	ld.global.nc.v2.f32 	{%f90, %f91}, [%rd61+2048];
	fma.rn.f32 	%f94, %f81, %f90, %f80;
	fma.rn.f32 	%f144, %f82, %f91, %f94;
	add.s64 	%rd63, %rd63, 8192;
	add.s64 	%rd62, %rd62, 8192;
	add.s64 	%rd61, %rd61, 8192;
	add.s32 	%r111, %r111, 1024;
	setp.lt.s32 	%p6, %r111, %r15;
	@%p6 bra 	$L__BB57_9;

	st.local.v2.f32 	[%rd3], {%f145, %f144};

$L__BB57_11:
	shr.s32 	%r33, %r3, 31;
	shr.u32 	%r34, %r33, 27;
	add.s32 	%r35, %r3, %r34;
	shr.s32 	%r36, %r35, 5;
	shl.b32 	%r37, %r36, 2;
	add.s32 	%r14, %r28, %r37;
	mov.u32 	%r39, 2;
	mov.b32 	%r40, %f145;
	mov.u32 	%r41, 31;
	mov.u32 	%r42, 16;
	mov.u32 	%r43, -1;
	shfl.sync.bfly.b32 	%r44|%p7, %r40, %r42, %r41, %r43;
	mov.b32 	%f95, %r44;
	add.f32 	%f96, %f145, %f95;
	mov.b32 	%r45, %f96;
	mov.u32 	%r46, 8;
	shfl.sync.bfly.b32 	%r47|%p8, %r45, %r46, %r41, %r43;
	mov.b32 	%f97, %r47;
	add.f32 	%f98, %f96, %f97;
	mov.b32 	%r48, %f98;
	mov.u32 	%r49, 4;
	shfl.sync.bfly.b32 	%r50|%p9, %r48, %r49, %r41, %r43;
	mov.b32 	%f99, %r50;
	add.f32 	%f100, %f98, %f99;
	mov.b32 	%r51, %f100;
	shfl.sync.bfly.b32 	%r52|%p10, %r51, %r39, %r41, %r43;
	mov.b32 	%f101, %r52;
	add.f32 	%f102, %f100, %f101;
	mov.b32 	%r53, %f102;
	mov.u32 	%r54, 1;
	shfl.sync.bfly.b32 	%r55|%p11, %r53, %r54, %r41, %r43;
	mov.b32 	%f103, %r55;
	add.f32 	%f104, %f102, %f103;
	st.local.f32 	[%rd3], %f104;
	st.shared.f32 	[%r14], %f104;
	bar.sync 	0;
	@%p1 bra 	$L__BB57_13;

	ld.shared.f32 	%f105, [%r4];
	mov.b32 	%r56, %f105;
	shfl.sync.bfly.b32 	%r60|%p13, %r56, %r42, %r41, %r43;
	mov.b32 	%f106, %r60;
	add.f32 	%f107, %f105, %f106;
	mov.b32 	%r61, %f107;
	shfl.sync.bfly.b32 	%r63|%p14, %r61, %r46, %r41, %r43;
	mov.b32 	%f108, %r63;
	add.f32 	%f109, %f107, %f108;
	mov.b32 	%r64, %f109;
	shfl.sync.bfly.b32 	%r66|%p15, %r64, %r49, %r41, %r43;
	mov.b32 	%f110, %r66;
	add.f32 	%f111, %f109, %f110;
	mov.b32 	%r67, %f111;
	shfl.sync.bfly.b32 	%r69|%p16, %r67, %r39, %r41, %r43;
	mov.b32 	%f112, %r69;
	add.f32 	%f113, %f111, %f112;
	mov.b32 	%r70, %f113;
	shfl.sync.bfly.b32 	%r72|%p17, %r70, %r54, %r41, %r43;
	mov.b32 	%f114, %r72;
	add.f32 	%f115, %f113, %f114;
	st.local.f32 	[%rd3], %f115;

$L__BB57_13:
	bar.sync 	0;
	mov.b32 	%r73, %f144;
	shfl.sync.bfly.b32 	%r77|%p19, %r73, %r42, %r41, %r43;
	mov.b32 	%f116, %r77;
	add.f32 	%f117, %f144, %f116;
	mov.b32 	%r78, %f117;
	shfl.sync.bfly.b32 	%r80|%p20, %r78, %r46, %r41, %r43;
	mov.b32 	%f118, %r80;
	add.f32 	%f119, %f117, %f118;
	mov.b32 	%r81, %f119;
	shfl.sync.bfly.b32 	%r83|%p21, %r81, %r49, %r41, %r43;
	mov.b32 	%f120, %r83;
	add.f32 	%f121, %f119, %f120;
	mov.b32 	%r84, %f121;
	shfl.sync.bfly.b32 	%r86|%p22, %r84, %r39, %r41, %r43;
	mov.b32 	%f122, %r86;
	add.f32 	%f123, %f121, %f122;
	mov.b32 	%r87, %f123;
	shfl.sync.bfly.b32 	%r89|%p23, %r87, %r54, %r41, %r43;
	mov.b32 	%f124, %r89;
	add.f32 	%f125, %f123, %f124;
	st.local.f32 	[%rd3+4], %f125;
	st.shared.f32 	[%r14], %f125;
	bar.sync 	0;
	@%p1 bra 	$L__BB57_15;

	ld.shared.f32 	%f126, [%r4];
	mov.b32 	%r90, %f126;
	mov.u32 	%r91, 31;
	mov.u32 	%r92, 16;
	mov.u32 	%r93, -1;
	shfl.sync.bfly.b32 	%r94|%p24, %r90, %r92, %r91, %r93;
	mov.b32 	%f127, %r94;
	add.f32 	%f128, %f126, %f127;
	mov.b32 	%r95, %f128;
	mov.u32 	%r96, 8;
	shfl.sync.bfly.b32 	%r97|%p25, %r95, %r96, %r91, %r93;
	mov.b32 	%f129, %r97;
	add.f32 	%f130, %f128, %f129;
	mov.b32 	%r98, %f130;
	mov.u32 	%r99, 4;
	shfl.sync.bfly.b32 	%r100|%p26, %r98, %r99, %r91, %r93;
	mov.b32 	%f131, %r100;
	add.f32 	%f132, %f130, %f131;
	mov.b32 	%r101, %f132;
	mov.u32 	%r102, 2;
	shfl.sync.bfly.b32 	%r103|%p27, %r101, %r102, %r91, %r93;
	mov.b32 	%f133, %r103;
	add.f32 	%f134, %f132, %f133;
	mov.b32 	%r104, %f134;
	mov.u32 	%r105, 1;
	shfl.sync.bfly.b32 	%r106|%p28, %r104, %r105, %r91, %r93;
	mov.b32 	%f135, %r106;
	add.f32 	%f136, %f134, %f135;
	st.local.f32 	[%rd3+4], %f136;

$L__BB57_15:
	bar.sync 	0;
	setp.gt.s32 	%p29, %r3, 1;
	@%p29 bra 	$L__BB57_17;

	mul.wide.s32 	%rd50, %r3, 4;
	add.s64 	%rd51, %rd3, %rd50;
	ld.local.f32 	%f137, [%rd51];
	mad.lo.s32 	%r107, %r3, %r17, %r2;
	cvt.s64.s32 	%rd52, %r107;
	mul.lo.s32 	%r108, %r1, %r18;
	cvt.s64.s32 	%rd53, %r108;
	add.s64 	%rd54, %rd53, %rd52;
	cvta.to.global.u64 	%rd55, %rd26;
	shl.b64 	%rd56, %rd54, 2;
	add.s64 	%rd57, %rd55, %rd56;
	st.global.f32 	[%rd57], %f137;

$L__BB57_17:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_3_bs_256
.visible .entry ggml_matvec_f32_ncols_3_bs_256(
	.param .u64 ggml_matvec_f32_ncols_3_bs_256_param_0,
	.param .u64 ggml_matvec_f32_ncols_3_bs_256_param_1,
	.param .u64 ggml_matvec_f32_ncols_3_bs_256_param_2,
	.param .u32 ggml_matvec_f32_ncols_3_bs_256_param_3,
	.param .u32 ggml_matvec_f32_ncols_3_bs_256_param_4,
	.param .u32 ggml_matvec_f32_ncols_3_bs_256_param_5,
	.param .u32 ggml_matvec_f32_ncols_3_bs_256_param_6,
	.param .u32 ggml_matvec_f32_ncols_3_bs_256_param_7,
	.param .u32 ggml_matvec_f32_ncols_3_bs_256_param_8,
	.param .u32 ggml_matvec_f32_ncols_3_bs_256_param_9,
	.param .u32 ggml_matvec_f32_ncols_3_bs_256_param_10,
	.param .u32 ggml_matvec_f32_ncols_3_bs_256_param_11
)
{
	.local .align 4 .b8 	__local_depot58[12];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<41>;
	.reg .f32 	%f<208>;
	.reg .b32 	%r<154>;
	.reg .b64 	%rd<72>;


	mov.u64 	%SPL, __local_depot58;
	ld.param.u64 	%rd29, [ggml_matvec_f32_ncols_3_bs_256_param_0];
	ld.param.u64 	%rd30, [ggml_matvec_f32_ncols_3_bs_256_param_1];
	ld.param.u64 	%rd28, [ggml_matvec_f32_ncols_3_bs_256_param_2];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_3_bs_256_param_3];
	ld.param.u32 	%r19, [ggml_matvec_f32_ncols_3_bs_256_param_5];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_3_bs_256_param_6];
	ld.param.u32 	%r17, [ggml_matvec_f32_ncols_3_bs_256_param_7];
	ld.param.u32 	%r20, [ggml_matvec_f32_ncols_3_bs_256_param_8];
	ld.param.u32 	%r21, [ggml_matvec_f32_ncols_3_bs_256_param_9];
	ld.param.u32 	%r22, [ggml_matvec_f32_ncols_3_bs_256_param_10];
	ld.param.u32 	%r18, [ggml_matvec_f32_ncols_3_bs_256_param_11];
	cvta.to.global.u64 	%rd71, %rd30;
	cvta.to.global.u64 	%rd2, %rd29;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r23, %r1, %r20;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r24, %r2, %r19;
	mad.lo.s32 	%r25, %r23, %r21, %r24;
	cvt.s64.s32 	%rd4, %r25;
	mul.lo.s32 	%r26, %r1, %r22;
	cvt.s64.s32 	%rd5, %r26;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r27, %r3, 2;
	mov.u32 	%r28, data_mmv;
	add.s32 	%r4, %r28, %r27;
	@%p1 bra 	$L__BB58_2;

	mov.u32 	%r29, 0;
	st.shared.u32 	[%r4], %r29;

$L__BB58_2:
	bar.sync 	0;
	mov.f32 	%f205, 0f00000000;
	mov.u32 	%r30, 0;
	st.local.u32 	[%rd3], %r30;
	st.local.u32 	[%rd3+4], %r30;
	st.local.u32 	[%rd3+8], %r30;
	setp.ge.s32 	%p2, %r3, %r15;
	mov.f32 	%f206, %f205;
	mov.f32 	%f207, %f205;
	@%p2 bra 	$L__BB58_11;

	not.b32 	%r31, %r3;
	add.s32 	%r5, %r31, %r15;
	shr.u32 	%r32, %r5, 8;
	add.s32 	%r33, %r32, 1;
	and.b32  	%r151, %r33, 3;
	setp.eq.s32 	%p3, %r151, 0;
	mov.f32 	%f205, 0f00000000;
	mov.u32 	%r152, %r3;
	@%p3 bra 	$L__BB58_7;

	shl.b32 	%r34, %r16, 1;
	add.s32 	%r35, %r3, %r34;
	mul.wide.s32 	%rd32, %r35, 2;
	add.s64 	%rd33, %rd32, %rd5;
	shl.b64 	%rd34, %rd33, 2;
	add.s64 	%rd69, %rd71, %rd34;
	mul.wide.s32 	%rd35, %r16, 2;
	mul.wide.s32 	%rd36, %r3, 2;
	add.s64 	%rd37, %rd35, %rd36;
	add.s64 	%rd38, %rd37, %rd5;
	shl.b64 	%rd39, %rd38, 2;
	add.s64 	%rd68, %rd71, %rd39;
	add.s64 	%rd40, %rd36, %rd5;
	shl.b64 	%rd41, %rd40, 2;
	add.s64 	%rd67, %rd71, %rd41;
	add.s64 	%rd42, %rd36, %rd4;
	shl.b64 	%rd43, %rd42, 2;
	add.s64 	%rd66, %rd2, %rd43;
	mov.f32 	%f205, 0f00000000;
	mov.f32 	%f206, %f205;
	mov.f32 	%f207, %f205;
	mov.u32 	%r152, %r3;

$L__BB58_5:
	.pragma "nounroll";
	ld.global.nc.v2.f32 	{%f28, %f29}, [%rd66];
	ld.global.nc.v2.f32 	{%f32, %f33}, [%rd67];
	fma.rn.f32 	%f36, %f28, %f32, %f207;
	fma.rn.f32 	%f207, %f29, %f33, %f36;
	ld.global.nc.v2.f32 	{%f37, %f38}, [%rd68];
	fma.rn.f32 	%f41, %f28, %f37, %f206;
	fma.rn.f32 	%f206, %f29, %f38, %f41;
	ld.global.nc.v2.f32 	{%f42, %f43}, [%rd69];
	fma.rn.f32 	%f46, %f28, %f42, %f205;
	fma.rn.f32 	%f205, %f29, %f43, %f46;
	add.s32 	%r152, %r152, 256;
	add.s64 	%rd69, %rd69, 2048;
	add.s64 	%rd68, %rd68, 2048;
	add.s64 	%rd67, %rd67, 2048;
	add.s64 	%rd66, %rd66, 2048;
	add.s32 	%r151, %r151, -1;
	setp.ne.s32 	%p4, %r151, 0;
	@%p4 bra 	$L__BB58_5;

	st.local.f32 	[%rd3], %f207;
	st.local.f32 	[%rd3+4], %f206;
	st.local.f32 	[%rd3+8], %f205;

$L__BB58_7:
	setp.lt.u32 	%p5, %r5, 768;
	@%p5 bra 	$L__BB58_11;

	add.s32 	%r36, %r152, %r16;
	shl.b32 	%r37, %r16, 1;
	add.s32 	%r38, %r152, %r37;
	add.s32 	%r39, %r36, 256;
	mul.wide.s32 	%rd44, %r39, 8;
	shl.b64 	%rd45, %rd5, 2;
	add.s64 	%rd19, %rd44, %rd45;
	mul.wide.s32 	%rd46, %r38, 8;
	add.s64 	%rd20, %rd46, %rd45;
	mul.wide.s32 	%rd47, %r152, 2;
	add.s64 	%rd48, %rd47, %rd4;
	shl.b64 	%rd49, %rd48, 2;
	add.s64 	%rd50, %rd2, %rd49;
	add.s64 	%rd70, %rd50, 4096;
	mul.wide.s32 	%rd51, %r152, 8;
	add.s64 	%rd22, %rd51, %rd45;
	mul.wide.s32 	%rd52, %r16, 8;
	add.s64 	%rd53, %rd51, %rd52;
	add.s64 	%rd23, %rd53, %rd45;

$L__BB58_9:
	ld.global.nc.v2.f32 	{%f47, %f48}, [%rd70+-4096];
	add.s64 	%rd54, %rd71, %rd22;
	ld.global.nc.v2.f32 	{%f51, %f52}, [%rd54];
	fma.rn.f32 	%f55, %f47, %f51, %f207;
	fma.rn.f32 	%f56, %f48, %f52, %f55;
	add.s64 	%rd55, %rd71, %rd23;
	ld.global.nc.v2.f32 	{%f57, %f58}, [%rd55];
	fma.rn.f32 	%f61, %f47, %f57, %f206;
	fma.rn.f32 	%f62, %f48, %f58, %f61;
	add.s64 	%rd56, %rd71, %rd20;
	ld.global.nc.v2.f32 	{%f63, %f64}, [%rd56];
	fma.rn.f32 	%f67, %f47, %f63, %f205;
	fma.rn.f32 	%f68, %f48, %f64, %f67;
	ld.global.nc.v2.f32 	{%f69, %f70}, [%rd70+-2048];
	ld.global.nc.v2.f32 	{%f73, %f74}, [%rd54+2048];
	fma.rn.f32 	%f77, %f69, %f73, %f56;
	fma.rn.f32 	%f78, %f70, %f74, %f77;
	add.s64 	%rd57, %rd71, %rd19;
	ld.global.nc.v2.f32 	{%f79, %f80}, [%rd57];
	fma.rn.f32 	%f83, %f69, %f79, %f62;
	fma.rn.f32 	%f84, %f70, %f80, %f83;
	ld.global.nc.v2.f32 	{%f85, %f86}, [%rd56+2048];
	fma.rn.f32 	%f89, %f69, %f85, %f68;
	fma.rn.f32 	%f90, %f70, %f86, %f89;
	ld.global.nc.v2.f32 	{%f91, %f92}, [%rd70];
	ld.global.nc.v2.f32 	{%f95, %f96}, [%rd54+4096];
	fma.rn.f32 	%f99, %f91, %f95, %f78;
	fma.rn.f32 	%f100, %f92, %f96, %f99;
	ld.global.nc.v2.f32 	{%f101, %f102}, [%rd57+2048];
	fma.rn.f32 	%f105, %f91, %f101, %f84;
	fma.rn.f32 	%f106, %f92, %f102, %f105;
	ld.global.nc.v2.f32 	{%f107, %f108}, [%rd56+4096];
	fma.rn.f32 	%f111, %f91, %f107, %f90;
	fma.rn.f32 	%f112, %f92, %f108, %f111;
	ld.global.nc.v2.f32 	{%f113, %f114}, [%rd70+2048];
	ld.global.nc.v2.f32 	{%f117, %f118}, [%rd54+6144];
	fma.rn.f32 	%f121, %f113, %f117, %f100;
	fma.rn.f32 	%f207, %f114, %f118, %f121;
	ld.global.nc.v2.f32 	{%f122, %f123}, [%rd57+4096];
	fma.rn.f32 	%f126, %f113, %f122, %f106;
	fma.rn.f32 	%f206, %f114, %f123, %f126;
	ld.global.nc.v2.f32 	{%f127, %f128}, [%rd56+6144];
	fma.rn.f32 	%f131, %f113, %f127, %f112;
	fma.rn.f32 	%f205, %f114, %f128, %f131;
	add.s64 	%rd71, %rd71, 8192;
	add.s64 	%rd70, %rd70, 8192;
	add.s32 	%r152, %r152, 1024;
	setp.lt.s32 	%p6, %r152, %r15;
	@%p6 bra 	$L__BB58_9;

	st.local.f32 	[%rd3], %f207;
	st.local.f32 	[%rd3+4], %f206;
	st.local.f32 	[%rd3+8], %f205;

$L__BB58_11:
	shr.s32 	%r40, %r3, 31;
	shr.u32 	%r41, %r40, 27;
	add.s32 	%r42, %r3, %r41;
	shr.s32 	%r43, %r42, 5;
	shl.b32 	%r44, %r43, 2;
	add.s32 	%r14, %r28, %r44;
	mov.u32 	%r46, 2;
	mov.b32 	%r47, %f207;
	mov.u32 	%r48, 31;
	mov.u32 	%r49, 16;
	mov.u32 	%r50, -1;
	shfl.sync.bfly.b32 	%r51|%p7, %r47, %r49, %r48, %r50;
	mov.b32 	%f132, %r51;
	add.f32 	%f133, %f207, %f132;
	mov.b32 	%r52, %f133;
	mov.u32 	%r53, 8;
	shfl.sync.bfly.b32 	%r54|%p8, %r52, %r53, %r48, %r50;
	mov.b32 	%f134, %r54;
	add.f32 	%f135, %f133, %f134;
	mov.b32 	%r55, %f135;
	mov.u32 	%r56, 4;
	shfl.sync.bfly.b32 	%r57|%p9, %r55, %r56, %r48, %r50;
	mov.b32 	%f136, %r57;
	add.f32 	%f137, %f135, %f136;
	mov.b32 	%r58, %f137;
	shfl.sync.bfly.b32 	%r59|%p10, %r58, %r46, %r48, %r50;
	mov.b32 	%f138, %r59;
	add.f32 	%f139, %f137, %f138;
	mov.b32 	%r60, %f139;
	mov.u32 	%r61, 1;
	shfl.sync.bfly.b32 	%r62|%p11, %r60, %r61, %r48, %r50;
	mov.b32 	%f140, %r62;
	add.f32 	%f141, %f139, %f140;
	st.local.f32 	[%rd3], %f141;
	st.shared.f32 	[%r14], %f141;
	bar.sync 	0;
	@%p1 bra 	$L__BB58_13;

	ld.shared.f32 	%f142, [%r4];
	mov.b32 	%r63, %f142;
	shfl.sync.bfly.b32 	%r67|%p13, %r63, %r49, %r48, %r50;
	mov.b32 	%f143, %r67;
	add.f32 	%f144, %f142, %f143;
	mov.b32 	%r68, %f144;
	shfl.sync.bfly.b32 	%r70|%p14, %r68, %r53, %r48, %r50;
	mov.b32 	%f145, %r70;
	add.f32 	%f146, %f144, %f145;
	mov.b32 	%r71, %f146;
	shfl.sync.bfly.b32 	%r73|%p15, %r71, %r56, %r48, %r50;
	mov.b32 	%f147, %r73;
	add.f32 	%f148, %f146, %f147;
	mov.b32 	%r74, %f148;
	shfl.sync.bfly.b32 	%r76|%p16, %r74, %r46, %r48, %r50;
	mov.b32 	%f149, %r76;
	add.f32 	%f150, %f148, %f149;
	mov.b32 	%r77, %f150;
	shfl.sync.bfly.b32 	%r79|%p17, %r77, %r61, %r48, %r50;
	mov.b32 	%f151, %r79;
	add.f32 	%f152, %f150, %f151;
	st.local.f32 	[%rd3], %f152;

$L__BB58_13:
	bar.sync 	0;
	mov.b32 	%r80, %f206;
	shfl.sync.bfly.b32 	%r84|%p19, %r80, %r49, %r48, %r50;
	mov.b32 	%f153, %r84;
	add.f32 	%f154, %f206, %f153;
	mov.b32 	%r85, %f154;
	shfl.sync.bfly.b32 	%r87|%p20, %r85, %r53, %r48, %r50;
	mov.b32 	%f155, %r87;
	add.f32 	%f156, %f154, %f155;
	mov.b32 	%r88, %f156;
	shfl.sync.bfly.b32 	%r90|%p21, %r88, %r56, %r48, %r50;
	mov.b32 	%f157, %r90;
	add.f32 	%f158, %f156, %f157;
	mov.b32 	%r91, %f158;
	shfl.sync.bfly.b32 	%r93|%p22, %r91, %r46, %r48, %r50;
	mov.b32 	%f159, %r93;
	add.f32 	%f160, %f158, %f159;
	mov.b32 	%r94, %f160;
	shfl.sync.bfly.b32 	%r96|%p23, %r94, %r61, %r48, %r50;
	mov.b32 	%f161, %r96;
	add.f32 	%f162, %f160, %f161;
	st.local.f32 	[%rd3+4], %f162;
	st.shared.f32 	[%r14], %f162;
	bar.sync 	0;
	@%p1 bra 	$L__BB58_15;

	ld.shared.f32 	%f163, [%r4];
	mov.b32 	%r97, %f163;
	mov.u32 	%r98, 31;
	mov.u32 	%r99, 16;
	mov.u32 	%r100, -1;
	shfl.sync.bfly.b32 	%r101|%p24, %r97, %r99, %r98, %r100;
	mov.b32 	%f164, %r101;
	add.f32 	%f165, %f163, %f164;
	mov.b32 	%r102, %f165;
	mov.u32 	%r103, 8;
	shfl.sync.bfly.b32 	%r104|%p25, %r102, %r103, %r98, %r100;
	mov.b32 	%f166, %r104;
	add.f32 	%f167, %f165, %f166;
	mov.b32 	%r105, %f167;
	mov.u32 	%r106, 4;
	shfl.sync.bfly.b32 	%r107|%p26, %r105, %r106, %r98, %r100;
	mov.b32 	%f168, %r107;
	add.f32 	%f169, %f167, %f168;
	mov.b32 	%r108, %f169;
	mov.u32 	%r109, 2;
	shfl.sync.bfly.b32 	%r110|%p27, %r108, %r109, %r98, %r100;
	mov.b32 	%f170, %r110;
	add.f32 	%f171, %f169, %f170;
	mov.b32 	%r111, %f171;
	mov.u32 	%r112, 1;
	shfl.sync.bfly.b32 	%r113|%p28, %r111, %r112, %r98, %r100;
	mov.b32 	%f172, %r113;
	add.f32 	%f173, %f171, %f172;
	st.local.f32 	[%rd3+4], %f173;

$L__BB58_15:
	bar.sync 	0;
	mov.b32 	%r114, %f205;
	mov.u32 	%r115, 31;
	mov.u32 	%r116, 16;
	mov.u32 	%r117, -1;
	shfl.sync.bfly.b32 	%r118|%p30, %r114, %r116, %r115, %r117;
	mov.b32 	%f174, %r118;
	add.f32 	%f175, %f205, %f174;
	mov.b32 	%r119, %f175;
	mov.u32 	%r120, 8;
	shfl.sync.bfly.b32 	%r121|%p31, %r119, %r120, %r115, %r117;
	mov.b32 	%f176, %r121;
	add.f32 	%f177, %f175, %f176;
	mov.b32 	%r122, %f177;
	mov.u32 	%r123, 4;
	shfl.sync.bfly.b32 	%r124|%p32, %r122, %r123, %r115, %r117;
	mov.b32 	%f178, %r124;
	add.f32 	%f179, %f177, %f178;
	mov.b32 	%r125, %f179;
	mov.u32 	%r126, 2;
	shfl.sync.bfly.b32 	%r127|%p33, %r125, %r126, %r115, %r117;
	mov.b32 	%f180, %r127;
	add.f32 	%f181, %f179, %f180;
	mov.b32 	%r128, %f181;
	mov.u32 	%r129, 1;
	shfl.sync.bfly.b32 	%r130|%p34, %r128, %r129, %r115, %r117;
	mov.b32 	%f182, %r130;
	add.f32 	%f183, %f181, %f182;
	st.local.f32 	[%rd3+8], %f183;
	st.shared.f32 	[%r14], %f183;
	bar.sync 	0;
	@%p1 bra 	$L__BB58_17;

	ld.shared.f32 	%f184, [%r4];
	mov.b32 	%r131, %f184;
	shfl.sync.bfly.b32 	%r135|%p35, %r131, %r116, %r115, %r117;
	mov.b32 	%f185, %r135;
	add.f32 	%f186, %f184, %f185;
	mov.b32 	%r136, %f186;
	shfl.sync.bfly.b32 	%r138|%p36, %r136, %r120, %r115, %r117;
	mov.b32 	%f187, %r138;
	add.f32 	%f188, %f186, %f187;
	mov.b32 	%r139, %f188;
	shfl.sync.bfly.b32 	%r141|%p37, %r139, %r123, %r115, %r117;
	mov.b32 	%f189, %r141;
	add.f32 	%f190, %f188, %f189;
	mov.b32 	%r142, %f190;
	shfl.sync.bfly.b32 	%r144|%p38, %r142, %r126, %r115, %r117;
	mov.b32 	%f191, %r144;
	add.f32 	%f192, %f190, %f191;
	mov.b32 	%r145, %f192;
	shfl.sync.bfly.b32 	%r147|%p39, %r145, %r129, %r115, %r117;
	mov.b32 	%f193, %r147;
	add.f32 	%f194, %f192, %f193;
	st.local.f32 	[%rd3+8], %f194;

$L__BB58_17:
	bar.sync 	0;
	setp.gt.s32 	%p40, %r3, 2;
	@%p40 bra 	$L__BB58_19;

	mul.wide.s32 	%rd58, %r3, 4;
	add.s64 	%rd59, %rd3, %rd58;
	ld.local.f32 	%f195, [%rd59];
	mad.lo.s32 	%r148, %r3, %r17, %r2;
	cvt.s64.s32 	%rd60, %r148;
	mul.lo.s32 	%r149, %r1, %r18;
	cvt.s64.s32 	%rd61, %r149;
	add.s64 	%rd62, %rd61, %rd60;
	cvta.to.global.u64 	%rd63, %rd28;
	shl.b64 	%rd64, %rd62, 2;
	add.s64 	%rd65, %rd63, %rd64;
	st.global.f32 	[%rd65], %f195;

$L__BB58_19:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_4_bs_256
.visible .entry ggml_matvec_f32_ncols_4_bs_256(
	.param .u64 ggml_matvec_f32_ncols_4_bs_256_param_0,
	.param .u64 ggml_matvec_f32_ncols_4_bs_256_param_1,
	.param .u64 ggml_matvec_f32_ncols_4_bs_256_param_2,
	.param .u32 ggml_matvec_f32_ncols_4_bs_256_param_3,
	.param .u32 ggml_matvec_f32_ncols_4_bs_256_param_4,
	.param .u32 ggml_matvec_f32_ncols_4_bs_256_param_5,
	.param .u32 ggml_matvec_f32_ncols_4_bs_256_param_6,
	.param .u32 ggml_matvec_f32_ncols_4_bs_256_param_7,
	.param .u32 ggml_matvec_f32_ncols_4_bs_256_param_8,
	.param .u32 ggml_matvec_f32_ncols_4_bs_256_param_9,
	.param .u32 ggml_matvec_f32_ncols_4_bs_256_param_10,
	.param .u32 ggml_matvec_f32_ncols_4_bs_256_param_11
)
{
	.local .align 16 .b8 	__local_depot59[16];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<52>;
	.reg .f32 	%f<270>;
	.reg .b32 	%r<189>;
	.reg .b64 	%rd<83>;


	mov.u64 	%SPL, __local_depot59;
	ld.param.u64 	%rd34, [ggml_matvec_f32_ncols_4_bs_256_param_0];
	ld.param.u64 	%rd35, [ggml_matvec_f32_ncols_4_bs_256_param_1];
	ld.param.u64 	%rd33, [ggml_matvec_f32_ncols_4_bs_256_param_2];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_4_bs_256_param_3];
	ld.param.u32 	%r19, [ggml_matvec_f32_ncols_4_bs_256_param_5];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_4_bs_256_param_6];
	ld.param.u32 	%r17, [ggml_matvec_f32_ncols_4_bs_256_param_7];
	ld.param.u32 	%r20, [ggml_matvec_f32_ncols_4_bs_256_param_8];
	ld.param.u32 	%r21, [ggml_matvec_f32_ncols_4_bs_256_param_9];
	ld.param.u32 	%r22, [ggml_matvec_f32_ncols_4_bs_256_param_10];
	ld.param.u32 	%r18, [ggml_matvec_f32_ncols_4_bs_256_param_11];
	cvta.to.global.u64 	%rd82, %rd35;
	cvta.to.global.u64 	%rd2, %rd34;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r23, %r1, %r20;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r24, %r2, %r19;
	mad.lo.s32 	%r25, %r23, %r21, %r24;
	cvt.s64.s32 	%rd4, %r25;
	mul.lo.s32 	%r26, %r1, %r22;
	cvt.s64.s32 	%rd5, %r26;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r27, %r3, 2;
	mov.u32 	%r28, data_mmv;
	add.s32 	%r4, %r28, %r27;
	@%p1 bra 	$L__BB59_2;

	mov.u32 	%r29, 0;
	st.shared.u32 	[%r4], %r29;

$L__BB59_2:
	bar.sync 	0;
	mov.f32 	%f266, 0f00000000;
	st.local.v4.f32 	[%rd3], {%f266, %f266, %f266, %f266};
	setp.ge.s32 	%p2, %r3, %r15;
	mov.f32 	%f267, %f266;
	mov.f32 	%f268, %f266;
	mov.f32 	%f269, %f266;
	@%p2 bra 	$L__BB59_11;

	not.b32 	%r30, %r3;
	add.s32 	%r5, %r30, %r15;
	shr.u32 	%r31, %r5, 8;
	add.s32 	%r32, %r31, 1;
	and.b32  	%r186, %r32, 3;
	setp.eq.s32 	%p3, %r186, 0;
	mov.f32 	%f266, 0f00000000;
	mov.u32 	%r187, %r3;
	@%p3 bra 	$L__BB59_7;

	shl.b32 	%r33, %r16, 1;
	add.s32 	%r34, %r3, %r33;
	mul.wide.s32 	%rd37, %r34, 2;
	add.s64 	%rd38, %rd37, %rd5;
	shl.b64 	%rd39, %rd38, 2;
	add.s64 	%rd80, %rd82, %rd39;
	mad.lo.s32 	%r35, %r16, 3, %r3;
	mul.wide.s32 	%rd40, %r35, 2;
	add.s64 	%rd41, %rd40, %rd5;
	shl.b64 	%rd42, %rd41, 2;
	add.s64 	%rd79, %rd82, %rd42;
	mul.wide.s32 	%rd43, %r16, 2;
	mul.wide.s32 	%rd44, %r3, 2;
	add.s64 	%rd45, %rd43, %rd44;
	add.s64 	%rd46, %rd45, %rd5;
	shl.b64 	%rd47, %rd46, 2;
	add.s64 	%rd78, %rd82, %rd47;
	add.s64 	%rd48, %rd44, %rd5;
	shl.b64 	%rd49, %rd48, 2;
	add.s64 	%rd77, %rd82, %rd49;
	add.s64 	%rd50, %rd44, %rd4;
	shl.b64 	%rd51, %rd50, 2;
	add.s64 	%rd76, %rd2, %rd51;
	mov.f32 	%f266, 0f00000000;
	mov.f32 	%f267, %f266;
	mov.f32 	%f268, %f266;
	mov.f32 	%f269, %f266;
	mov.u32 	%r187, %r3;

$L__BB59_5:
	.pragma "nounroll";
	ld.global.nc.v2.f32 	{%f37, %f38}, [%rd76];
	ld.global.nc.v2.f32 	{%f41, %f42}, [%rd77];
	fma.rn.f32 	%f45, %f37, %f41, %f269;
	fma.rn.f32 	%f269, %f38, %f42, %f45;
	ld.global.nc.v2.f32 	{%f46, %f47}, [%rd78];
	fma.rn.f32 	%f50, %f37, %f46, %f268;
	fma.rn.f32 	%f268, %f38, %f47, %f50;
	ld.global.nc.v2.f32 	{%f51, %f52}, [%rd80];
	fma.rn.f32 	%f55, %f37, %f51, %f267;
	fma.rn.f32 	%f267, %f38, %f52, %f55;
	ld.global.nc.v2.f32 	{%f56, %f57}, [%rd79];
	fma.rn.f32 	%f60, %f37, %f56, %f266;
	fma.rn.f32 	%f266, %f38, %f57, %f60;
	add.s32 	%r187, %r187, 256;
	add.s64 	%rd80, %rd80, 2048;
	add.s64 	%rd79, %rd79, 2048;
	add.s64 	%rd78, %rd78, 2048;
	add.s64 	%rd77, %rd77, 2048;
	add.s64 	%rd76, %rd76, 2048;
	add.s32 	%r186, %r186, -1;
	setp.ne.s32 	%p4, %r186, 0;
	@%p4 bra 	$L__BB59_5;

	st.local.v4.f32 	[%rd3], {%f269, %f268, %f267, %f266};

$L__BB59_7:
	setp.lt.u32 	%p5, %r5, 768;
	@%p5 bra 	$L__BB59_11;

	add.s32 	%r36, %r187, %r16;
	shl.b32 	%r37, %r16, 1;
	add.s32 	%r38, %r187, %r37;
	mad.lo.s32 	%r39, %r16, 3, %r187;
	add.s32 	%r40, %r36, 256;
	mul.wide.s32 	%rd52, %r40, 8;
	shl.b64 	%rd53, %rd5, 2;
	add.s64 	%rd23, %rd52, %rd53;
	mul.wide.s32 	%rd54, %r38, 8;
	add.s64 	%rd24, %rd54, %rd53;
	mul.wide.s32 	%rd55, %r39, 8;
	add.s64 	%rd25, %rd55, %rd53;
	mul.wide.s32 	%rd56, %r187, 2;
	add.s64 	%rd57, %rd56, %rd4;
	shl.b64 	%rd58, %rd57, 2;
	add.s64 	%rd59, %rd2, %rd58;
	add.s64 	%rd81, %rd59, 4096;
	mul.wide.s32 	%rd60, %r187, 8;
	add.s64 	%rd27, %rd60, %rd53;
	mul.wide.s32 	%rd61, %r16, 8;
	add.s64 	%rd62, %rd60, %rd61;
	add.s64 	%rd28, %rd62, %rd53;

$L__BB59_9:
	ld.global.nc.v2.f32 	{%f61, %f62}, [%rd81+-4096];
	add.s64 	%rd63, %rd82, %rd27;
	ld.global.nc.v2.f32 	{%f65, %f66}, [%rd63];
	fma.rn.f32 	%f69, %f61, %f65, %f269;
	fma.rn.f32 	%f70, %f62, %f66, %f69;
	add.s64 	%rd64, %rd82, %rd28;
	ld.global.nc.v2.f32 	{%f71, %f72}, [%rd64];
	fma.rn.f32 	%f75, %f61, %f71, %f268;
	fma.rn.f32 	%f76, %f62, %f72, %f75;
	add.s64 	%rd65, %rd82, %rd24;
	ld.global.nc.v2.f32 	{%f77, %f78}, [%rd65];
	fma.rn.f32 	%f81, %f61, %f77, %f267;
	fma.rn.f32 	%f82, %f62, %f78, %f81;
	add.s64 	%rd66, %rd82, %rd25;
	ld.global.nc.v2.f32 	{%f83, %f84}, [%rd66];
	fma.rn.f32 	%f87, %f61, %f83, %f266;
	fma.rn.f32 	%f88, %f62, %f84, %f87;
	ld.global.nc.v2.f32 	{%f89, %f90}, [%rd81+-2048];
	ld.global.nc.v2.f32 	{%f93, %f94}, [%rd63+2048];
	fma.rn.f32 	%f97, %f89, %f93, %f70;
	fma.rn.f32 	%f98, %f90, %f94, %f97;
	add.s64 	%rd67, %rd82, %rd23;
	ld.global.nc.v2.f32 	{%f99, %f100}, [%rd67];
	fma.rn.f32 	%f103, %f89, %f99, %f76;
	fma.rn.f32 	%f104, %f90, %f100, %f103;
	ld.global.nc.v2.f32 	{%f105, %f106}, [%rd65+2048];
	fma.rn.f32 	%f109, %f89, %f105, %f82;
	fma.rn.f32 	%f110, %f90, %f106, %f109;
	ld.global.nc.v2.f32 	{%f111, %f112}, [%rd66+2048];
	fma.rn.f32 	%f115, %f89, %f111, %f88;
	fma.rn.f32 	%f116, %f90, %f112, %f115;
	ld.global.nc.v2.f32 	{%f117, %f118}, [%rd81];
	ld.global.nc.v2.f32 	{%f121, %f122}, [%rd63+4096];
	fma.rn.f32 	%f125, %f117, %f121, %f98;
	fma.rn.f32 	%f126, %f118, %f122, %f125;
	ld.global.nc.v2.f32 	{%f127, %f128}, [%rd67+2048];
	fma.rn.f32 	%f131, %f117, %f127, %f104;
	fma.rn.f32 	%f132, %f118, %f128, %f131;
	ld.global.nc.v2.f32 	{%f133, %f134}, [%rd65+4096];
	fma.rn.f32 	%f137, %f117, %f133, %f110;
	fma.rn.f32 	%f138, %f118, %f134, %f137;
	ld.global.nc.v2.f32 	{%f139, %f140}, [%rd66+4096];
	fma.rn.f32 	%f143, %f117, %f139, %f116;
	fma.rn.f32 	%f144, %f118, %f140, %f143;
	ld.global.nc.v2.f32 	{%f145, %f146}, [%rd81+2048];
	ld.global.nc.v2.f32 	{%f149, %f150}, [%rd63+6144];
	fma.rn.f32 	%f153, %f145, %f149, %f126;
	fma.rn.f32 	%f269, %f146, %f150, %f153;
	ld.global.nc.v2.f32 	{%f154, %f155}, [%rd67+4096];
	fma.rn.f32 	%f158, %f145, %f154, %f132;
	fma.rn.f32 	%f268, %f146, %f155, %f158;
	ld.global.nc.v2.f32 	{%f159, %f160}, [%rd65+6144];
	fma.rn.f32 	%f163, %f145, %f159, %f138;
	fma.rn.f32 	%f267, %f146, %f160, %f163;
	ld.global.nc.v2.f32 	{%f164, %f165}, [%rd66+6144];
	fma.rn.f32 	%f168, %f145, %f164, %f144;
	fma.rn.f32 	%f266, %f146, %f165, %f168;
	add.s64 	%rd82, %rd82, 8192;
	add.s64 	%rd81, %rd81, 8192;
	add.s32 	%r187, %r187, 1024;
	setp.lt.s32 	%p6, %r187, %r15;
	@%p6 bra 	$L__BB59_9;

	st.local.v4.f32 	[%rd3], {%f269, %f268, %f267, %f266};

$L__BB59_11:
	shr.s32 	%r41, %r3, 31;
	shr.u32 	%r42, %r41, 27;
	add.s32 	%r43, %r3, %r42;
	shr.s32 	%r44, %r43, 5;
	shl.b32 	%r45, %r44, 2;
	add.s32 	%r14, %r28, %r45;
	mov.u32 	%r47, 2;
	mov.b32 	%r48, %f269;
	mov.u32 	%r49, 31;
	mov.u32 	%r50, 16;
	mov.u32 	%r51, -1;
	shfl.sync.bfly.b32 	%r52|%p7, %r48, %r50, %r49, %r51;
	mov.b32 	%f169, %r52;
	add.f32 	%f170, %f269, %f169;
	mov.b32 	%r53, %f170;
	mov.u32 	%r54, 8;
	shfl.sync.bfly.b32 	%r55|%p8, %r53, %r54, %r49, %r51;
	mov.b32 	%f171, %r55;
	add.f32 	%f172, %f170, %f171;
	mov.b32 	%r56, %f172;
	mov.u32 	%r57, 4;
	shfl.sync.bfly.b32 	%r58|%p9, %r56, %r57, %r49, %r51;
	mov.b32 	%f173, %r58;
	add.f32 	%f174, %f172, %f173;
	mov.b32 	%r59, %f174;
	shfl.sync.bfly.b32 	%r60|%p10, %r59, %r47, %r49, %r51;
	mov.b32 	%f175, %r60;
	add.f32 	%f176, %f174, %f175;
	mov.b32 	%r61, %f176;
	mov.u32 	%r62, 1;
	shfl.sync.bfly.b32 	%r63|%p11, %r61, %r62, %r49, %r51;
	mov.b32 	%f177, %r63;
	add.f32 	%f178, %f176, %f177;
	st.local.f32 	[%rd3], %f178;
	st.shared.f32 	[%r14], %f178;
	bar.sync 	0;
	@%p1 bra 	$L__BB59_13;

	ld.shared.f32 	%f179, [%r4];
	mov.b32 	%r64, %f179;
	shfl.sync.bfly.b32 	%r68|%p13, %r64, %r50, %r49, %r51;
	mov.b32 	%f180, %r68;
	add.f32 	%f181, %f179, %f180;
	mov.b32 	%r69, %f181;
	shfl.sync.bfly.b32 	%r71|%p14, %r69, %r54, %r49, %r51;
	mov.b32 	%f182, %r71;
	add.f32 	%f183, %f181, %f182;
	mov.b32 	%r72, %f183;
	shfl.sync.bfly.b32 	%r74|%p15, %r72, %r57, %r49, %r51;
	mov.b32 	%f184, %r74;
	add.f32 	%f185, %f183, %f184;
	mov.b32 	%r75, %f185;
	shfl.sync.bfly.b32 	%r77|%p16, %r75, %r47, %r49, %r51;
	mov.b32 	%f186, %r77;
	add.f32 	%f187, %f185, %f186;
	mov.b32 	%r78, %f187;
	shfl.sync.bfly.b32 	%r80|%p17, %r78, %r62, %r49, %r51;
	mov.b32 	%f188, %r80;
	add.f32 	%f189, %f187, %f188;
	st.local.f32 	[%rd3], %f189;

$L__BB59_13:
	bar.sync 	0;
	mov.b32 	%r81, %f268;
	shfl.sync.bfly.b32 	%r85|%p19, %r81, %r50, %r49, %r51;
	mov.b32 	%f190, %r85;
	add.f32 	%f191, %f268, %f190;
	mov.b32 	%r86, %f191;
	shfl.sync.bfly.b32 	%r88|%p20, %r86, %r54, %r49, %r51;
	mov.b32 	%f192, %r88;
	add.f32 	%f193, %f191, %f192;
	mov.b32 	%r89, %f193;
	shfl.sync.bfly.b32 	%r91|%p21, %r89, %r57, %r49, %r51;
	mov.b32 	%f194, %r91;
	add.f32 	%f195, %f193, %f194;
	mov.b32 	%r92, %f195;
	shfl.sync.bfly.b32 	%r94|%p22, %r92, %r47, %r49, %r51;
	mov.b32 	%f196, %r94;
	add.f32 	%f197, %f195, %f196;
	mov.b32 	%r95, %f197;
	shfl.sync.bfly.b32 	%r97|%p23, %r95, %r62, %r49, %r51;
	mov.b32 	%f198, %r97;
	add.f32 	%f199, %f197, %f198;
	st.local.f32 	[%rd3+4], %f199;
	st.shared.f32 	[%r14], %f199;
	bar.sync 	0;
	@%p1 bra 	$L__BB59_15;

	ld.shared.f32 	%f200, [%r4];
	mov.b32 	%r98, %f200;
	mov.u32 	%r99, 31;
	mov.u32 	%r100, 16;
	mov.u32 	%r101, -1;
	shfl.sync.bfly.b32 	%r102|%p24, %r98, %r100, %r99, %r101;
	mov.b32 	%f201, %r102;
	add.f32 	%f202, %f200, %f201;
	mov.b32 	%r103, %f202;
	mov.u32 	%r104, 8;
	shfl.sync.bfly.b32 	%r105|%p25, %r103, %r104, %r99, %r101;
	mov.b32 	%f203, %r105;
	add.f32 	%f204, %f202, %f203;
	mov.b32 	%r106, %f204;
	mov.u32 	%r107, 4;
	shfl.sync.bfly.b32 	%r108|%p26, %r106, %r107, %r99, %r101;
	mov.b32 	%f205, %r108;
	add.f32 	%f206, %f204, %f205;
	mov.b32 	%r109, %f206;
	mov.u32 	%r110, 2;
	shfl.sync.bfly.b32 	%r111|%p27, %r109, %r110, %r99, %r101;
	mov.b32 	%f207, %r111;
	add.f32 	%f208, %f206, %f207;
	mov.b32 	%r112, %f208;
	mov.u32 	%r113, 1;
	shfl.sync.bfly.b32 	%r114|%p28, %r112, %r113, %r99, %r101;
	mov.b32 	%f209, %r114;
	add.f32 	%f210, %f208, %f209;
	st.local.f32 	[%rd3+4], %f210;

$L__BB59_15:
	bar.sync 	0;
	mov.b32 	%r115, %f267;
	mov.u32 	%r116, 31;
	mov.u32 	%r117, 16;
	mov.u32 	%r118, -1;
	shfl.sync.bfly.b32 	%r119|%p30, %r115, %r117, %r116, %r118;
	mov.b32 	%f211, %r119;
	add.f32 	%f212, %f267, %f211;
	mov.b32 	%r120, %f212;
	mov.u32 	%r121, 8;
	shfl.sync.bfly.b32 	%r122|%p31, %r120, %r121, %r116, %r118;
	mov.b32 	%f213, %r122;
	add.f32 	%f214, %f212, %f213;
	mov.b32 	%r123, %f214;
	mov.u32 	%r124, 4;
	shfl.sync.bfly.b32 	%r125|%p32, %r123, %r124, %r116, %r118;
	mov.b32 	%f215, %r125;
	add.f32 	%f216, %f214, %f215;
	mov.b32 	%r126, %f216;
	mov.u32 	%r127, 2;
	shfl.sync.bfly.b32 	%r128|%p33, %r126, %r127, %r116, %r118;
	mov.b32 	%f217, %r128;
	add.f32 	%f218, %f216, %f217;
	mov.b32 	%r129, %f218;
	mov.u32 	%r130, 1;
	shfl.sync.bfly.b32 	%r131|%p34, %r129, %r130, %r116, %r118;
	mov.b32 	%f219, %r131;
	add.f32 	%f220, %f218, %f219;
	st.local.f32 	[%rd3+8], %f220;
	st.shared.f32 	[%r14], %f220;
	bar.sync 	0;
	@%p1 bra 	$L__BB59_17;

	ld.shared.f32 	%f221, [%r4];
	mov.b32 	%r132, %f221;
	shfl.sync.bfly.b32 	%r136|%p35, %r132, %r117, %r116, %r118;
	mov.b32 	%f222, %r136;
	add.f32 	%f223, %f221, %f222;
	mov.b32 	%r137, %f223;
	shfl.sync.bfly.b32 	%r139|%p36, %r137, %r121, %r116, %r118;
	mov.b32 	%f224, %r139;
	add.f32 	%f225, %f223, %f224;
	mov.b32 	%r140, %f225;
	shfl.sync.bfly.b32 	%r142|%p37, %r140, %r124, %r116, %r118;
	mov.b32 	%f226, %r142;
	add.f32 	%f227, %f225, %f226;
	mov.b32 	%r143, %f227;
	shfl.sync.bfly.b32 	%r145|%p38, %r143, %r127, %r116, %r118;
	mov.b32 	%f228, %r145;
	add.f32 	%f229, %f227, %f228;
	mov.b32 	%r146, %f229;
	shfl.sync.bfly.b32 	%r148|%p39, %r146, %r130, %r116, %r118;
	mov.b32 	%f230, %r148;
	add.f32 	%f231, %f229, %f230;
	st.local.f32 	[%rd3+8], %f231;

$L__BB59_17:
	bar.sync 	0;
	mov.b32 	%r149, %f266;
	shfl.sync.bfly.b32 	%r153|%p41, %r149, %r117, %r116, %r118;
	mov.b32 	%f232, %r153;
	add.f32 	%f233, %f266, %f232;
	mov.b32 	%r154, %f233;
	shfl.sync.bfly.b32 	%r156|%p42, %r154, %r121, %r116, %r118;
	mov.b32 	%f234, %r156;
	add.f32 	%f235, %f233, %f234;
	mov.b32 	%r157, %f235;
	shfl.sync.bfly.b32 	%r159|%p43, %r157, %r124, %r116, %r118;
	mov.b32 	%f236, %r159;
	add.f32 	%f237, %f235, %f236;
	mov.b32 	%r160, %f237;
	shfl.sync.bfly.b32 	%r162|%p44, %r160, %r127, %r116, %r118;
	mov.b32 	%f238, %r162;
	add.f32 	%f239, %f237, %f238;
	mov.b32 	%r163, %f239;
	shfl.sync.bfly.b32 	%r165|%p45, %r163, %r130, %r116, %r118;
	mov.b32 	%f240, %r165;
	add.f32 	%f241, %f239, %f240;
	st.local.f32 	[%rd3+12], %f241;
	st.shared.f32 	[%r14], %f241;
	bar.sync 	0;
	@%p1 bra 	$L__BB59_19;

	ld.shared.f32 	%f242, [%r4];
	mov.b32 	%r166, %f242;
	mov.u32 	%r167, 31;
	mov.u32 	%r168, 16;
	mov.u32 	%r169, -1;
	shfl.sync.bfly.b32 	%r170|%p46, %r166, %r168, %r167, %r169;
	mov.b32 	%f243, %r170;
	add.f32 	%f244, %f242, %f243;
	mov.b32 	%r171, %f244;
	mov.u32 	%r172, 8;
	shfl.sync.bfly.b32 	%r173|%p47, %r171, %r172, %r167, %r169;
	mov.b32 	%f245, %r173;
	add.f32 	%f246, %f244, %f245;
	mov.b32 	%r174, %f246;
	mov.u32 	%r175, 4;
	shfl.sync.bfly.b32 	%r176|%p48, %r174, %r175, %r167, %r169;
	mov.b32 	%f247, %r176;
	add.f32 	%f248, %f246, %f247;
	mov.b32 	%r177, %f248;
	mov.u32 	%r178, 2;
	shfl.sync.bfly.b32 	%r179|%p49, %r177, %r178, %r167, %r169;
	mov.b32 	%f249, %r179;
	add.f32 	%f250, %f248, %f249;
	mov.b32 	%r180, %f250;
	mov.u32 	%r181, 1;
	shfl.sync.bfly.b32 	%r182|%p50, %r180, %r181, %r167, %r169;
	mov.b32 	%f251, %r182;
	add.f32 	%f252, %f250, %f251;
	st.local.f32 	[%rd3+12], %f252;

$L__BB59_19:
	bar.sync 	0;
	setp.gt.s32 	%p51, %r3, 3;
	@%p51 bra 	$L__BB59_21;

	mul.wide.s32 	%rd68, %r3, 4;
	add.s64 	%rd69, %rd3, %rd68;
	ld.local.f32 	%f253, [%rd69];
	mad.lo.s32 	%r183, %r3, %r17, %r2;
	cvt.s64.s32 	%rd70, %r183;
	mul.lo.s32 	%r184, %r1, %r18;
	cvt.s64.s32 	%rd71, %r184;
	add.s64 	%rd72, %rd71, %rd70;
	cvta.to.global.u64 	%rd73, %rd33;
	shl.b64 	%rd74, %rd72, 2;
	add.s64 	%rd75, %rd73, %rd74;
	st.global.f32 	[%rd75], %f253;

$L__BB59_21:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_5_bs_256
.visible .entry ggml_matvec_f32_ncols_5_bs_256(
	.param .u64 ggml_matvec_f32_ncols_5_bs_256_param_0,
	.param .u64 ggml_matvec_f32_ncols_5_bs_256_param_1,
	.param .u64 ggml_matvec_f32_ncols_5_bs_256_param_2,
	.param .u32 ggml_matvec_f32_ncols_5_bs_256_param_3,
	.param .u32 ggml_matvec_f32_ncols_5_bs_256_param_4,
	.param .u32 ggml_matvec_f32_ncols_5_bs_256_param_5,
	.param .u32 ggml_matvec_f32_ncols_5_bs_256_param_6,
	.param .u32 ggml_matvec_f32_ncols_5_bs_256_param_7,
	.param .u32 ggml_matvec_f32_ncols_5_bs_256_param_8,
	.param .u32 ggml_matvec_f32_ncols_5_bs_256_param_9,
	.param .u32 ggml_matvec_f32_ncols_5_bs_256_param_10,
	.param .u32 ggml_matvec_f32_ncols_5_bs_256_param_11
)
{
	.local .align 4 .b8 	__local_depot60[20];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<63>;
	.reg .f32 	%f<332>;
	.reg .b32 	%r<225>;
	.reg .b64 	%rd<74>;


	mov.u64 	%SPL, __local_depot60;
	ld.param.u64 	%rd28, [ggml_matvec_f32_ncols_5_bs_256_param_0];
	ld.param.u64 	%rd29, [ggml_matvec_f32_ncols_5_bs_256_param_1];
	ld.param.u64 	%rd27, [ggml_matvec_f32_ncols_5_bs_256_param_2];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_5_bs_256_param_3];
	ld.param.u32 	%r19, [ggml_matvec_f32_ncols_5_bs_256_param_5];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_5_bs_256_param_6];
	ld.param.u32 	%r17, [ggml_matvec_f32_ncols_5_bs_256_param_7];
	ld.param.u32 	%r20, [ggml_matvec_f32_ncols_5_bs_256_param_8];
	ld.param.u32 	%r21, [ggml_matvec_f32_ncols_5_bs_256_param_9];
	ld.param.u32 	%r22, [ggml_matvec_f32_ncols_5_bs_256_param_10];
	ld.param.u32 	%r18, [ggml_matvec_f32_ncols_5_bs_256_param_11];
	cvta.to.global.u64 	%rd73, %rd29;
	cvta.to.global.u64 	%rd2, %rd28;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r23, %r1, %r20;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r24, %r2, %r19;
	mad.lo.s32 	%r25, %r23, %r21, %r24;
	cvt.s64.s32 	%rd4, %r25;
	mul.lo.s32 	%r26, %r1, %r22;
	cvt.s64.s32 	%rd5, %r26;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r27, %r3, 2;
	mov.u32 	%r28, data_mmv;
	add.s32 	%r4, %r28, %r27;
	@%p1 bra 	$L__BB60_2;

	mov.u32 	%r29, 0;
	st.shared.u32 	[%r4], %r29;

$L__BB60_2:
	bar.sync 	0;
	mov.f32 	%f327, 0f00000000;
	mov.u32 	%r30, 0;
	st.local.u32 	[%rd3], %r30;
	st.local.u32 	[%rd3+4], %r30;
	st.local.u32 	[%rd3+8], %r30;
	st.local.u32 	[%rd3+12], %r30;
	st.local.u32 	[%rd3+16], %r30;
	setp.ge.s32 	%p2, %r3, %r15;
	mov.f32 	%f328, %f327;
	mov.f32 	%f329, %f327;
	mov.f32 	%f330, %f327;
	mov.f32 	%f331, %f327;
	@%p2 bra 	$L__BB60_11;

	not.b32 	%r31, %r3;
	add.s32 	%r5, %r31, %r15;
	shr.u32 	%r32, %r5, 8;
	add.s32 	%r33, %r32, 1;
	and.b32  	%r222, %r33, 3;
	setp.eq.s32 	%p3, %r222, 0;
	mov.f32 	%f327, 0f00000000;
	mov.u32 	%r223, %r3;
	@%p3 bra 	$L__BB60_7;

	shl.b32 	%r34, %r16, 1;
	mad.lo.s32 	%r35, %r16, 3, %r3;
	mul.wide.s32 	%rd31, %r35, 8;
	shl.b64 	%rd32, %rd5, 2;
	add.s64 	%rd7, %rd31, %rd32;
	mul.wide.s32 	%rd33, %r3, 8;
	mul.wide.s32 	%rd34, %r16, 8;
	add.s64 	%rd35, %rd33, %rd34;
	add.s64 	%rd8, %rd35, %rd32;
	add.s64 	%rd9, %rd33, %rd32;
	mul.wide.s32 	%rd36, %r3, 2;
	add.s64 	%rd37, %rd36, %rd4;
	shl.b64 	%rd38, %rd37, 2;
	add.s64 	%rd70, %rd2, %rd38;
	mul.wide.s32 	%rd11, %r34, 8;
	mov.f32 	%f327, 0f00000000;
	mov.u64 	%rd71, %rd73;
	mov.f32 	%f328, %f327;
	mov.f32 	%f329, %f327;
	mov.f32 	%f330, %f327;
	mov.f32 	%f331, %f327;
	mov.u32 	%r223, %r3;

$L__BB60_5:
	.pragma "nounroll";
	ld.global.nc.v2.f32 	{%f46, %f47}, [%rd70];
	add.s64 	%rd39, %rd71, %rd9;
	ld.global.nc.v2.f32 	{%f50, %f51}, [%rd39];
	fma.rn.f32 	%f54, %f46, %f50, %f331;
	fma.rn.f32 	%f331, %f47, %f51, %f54;
	add.s64 	%rd40, %rd71, %rd8;
	ld.global.nc.v2.f32 	{%f55, %f56}, [%rd40];
	fma.rn.f32 	%f59, %f46, %f55, %f330;
	fma.rn.f32 	%f330, %f47, %f56, %f59;
	add.s64 	%rd41, %rd39, %rd11;
	ld.global.nc.v2.f32 	{%f60, %f61}, [%rd41];
	fma.rn.f32 	%f64, %f46, %f60, %f329;
	fma.rn.f32 	%f329, %f47, %f61, %f64;
	add.s64 	%rd42, %rd71, %rd7;
	ld.global.nc.v2.f32 	{%f65, %f66}, [%rd42];
	fma.rn.f32 	%f69, %f46, %f65, %f328;
	fma.rn.f32 	%f328, %f47, %f66, %f69;
	add.s64 	%rd43, %rd41, %rd11;
	ld.global.nc.v2.f32 	{%f70, %f71}, [%rd43];
	fma.rn.f32 	%f74, %f46, %f70, %f327;
	fma.rn.f32 	%f327, %f47, %f71, %f74;
	add.s32 	%r223, %r223, 256;
	add.s64 	%rd71, %rd71, 2048;
	add.s64 	%rd70, %rd70, 2048;
	add.s32 	%r222, %r222, -1;
	setp.ne.s32 	%p4, %r222, 0;
	@%p4 bra 	$L__BB60_5;

	st.local.f32 	[%rd3], %f331;
	st.local.f32 	[%rd3+4], %f330;
	st.local.f32 	[%rd3+8], %f329;
	st.local.f32 	[%rd3+12], %f328;
	st.local.f32 	[%rd3+16], %f327;

$L__BB60_7:
	setp.lt.u32 	%p5, %r5, 768;
	@%p5 bra 	$L__BB60_11;

	add.s32 	%r36, %r223, %r16;
	shl.b32 	%r37, %r16, 1;
	add.s32 	%r38, %r223, %r37;
	mad.lo.s32 	%r39, %r16, 3, %r223;
	shl.b32 	%r40, %r16, 2;
	add.s32 	%r41, %r223, %r40;
	add.s32 	%r42, %r36, 256;
	mul.wide.s32 	%rd44, %r42, 8;
	shl.b64 	%rd45, %rd5, 2;
	add.s64 	%rd16, %rd44, %rd45;
	mul.wide.s32 	%rd46, %r38, 8;
	add.s64 	%rd17, %rd46, %rd45;
	mul.wide.s32 	%rd47, %r39, 8;
	add.s64 	%rd18, %rd47, %rd45;
	mul.wide.s32 	%rd48, %r41, 8;
	add.s64 	%rd19, %rd48, %rd45;
	mul.wide.s32 	%rd49, %r223, 2;
	add.s64 	%rd50, %rd49, %rd4;
	shl.b64 	%rd51, %rd50, 2;
	add.s64 	%rd52, %rd2, %rd51;
	add.s64 	%rd72, %rd52, 4096;
	mul.wide.s32 	%rd53, %r223, 8;
	add.s64 	%rd21, %rd53, %rd45;
	mul.wide.s32 	%rd54, %r16, 8;
	add.s64 	%rd55, %rd53, %rd54;
	add.s64 	%rd22, %rd55, %rd45;

$L__BB60_9:
	ld.global.nc.v2.f32 	{%f75, %f76}, [%rd72+-4096];
	add.s64 	%rd56, %rd73, %rd21;
	ld.global.nc.v2.f32 	{%f79, %f80}, [%rd56];
	fma.rn.f32 	%f83, %f75, %f79, %f331;
	fma.rn.f32 	%f84, %f76, %f80, %f83;
	add.s64 	%rd57, %rd73, %rd22;
	ld.global.nc.v2.f32 	{%f85, %f86}, [%rd57];
	fma.rn.f32 	%f89, %f75, %f85, %f330;
	fma.rn.f32 	%f90, %f76, %f86, %f89;
	add.s64 	%rd58, %rd73, %rd17;
	ld.global.nc.v2.f32 	{%f91, %f92}, [%rd58];
	fma.rn.f32 	%f95, %f75, %f91, %f329;
	fma.rn.f32 	%f96, %f76, %f92, %f95;
	add.s64 	%rd59, %rd73, %rd18;
	ld.global.nc.v2.f32 	{%f97, %f98}, [%rd59];
	fma.rn.f32 	%f101, %f75, %f97, %f328;
	fma.rn.f32 	%f102, %f76, %f98, %f101;
	add.s64 	%rd60, %rd73, %rd19;
	ld.global.nc.v2.f32 	{%f103, %f104}, [%rd60];
	fma.rn.f32 	%f107, %f75, %f103, %f327;
	fma.rn.f32 	%f108, %f76, %f104, %f107;
	ld.global.nc.v2.f32 	{%f109, %f110}, [%rd72+-2048];
	ld.global.nc.v2.f32 	{%f113, %f114}, [%rd56+2048];
	fma.rn.f32 	%f117, %f109, %f113, %f84;
	fma.rn.f32 	%f118, %f110, %f114, %f117;
	add.s64 	%rd61, %rd73, %rd16;
	ld.global.nc.v2.f32 	{%f119, %f120}, [%rd61];
	fma.rn.f32 	%f123, %f109, %f119, %f90;
	fma.rn.f32 	%f124, %f110, %f120, %f123;
	ld.global.nc.v2.f32 	{%f125, %f126}, [%rd58+2048];
	fma.rn.f32 	%f129, %f109, %f125, %f96;
	fma.rn.f32 	%f130, %f110, %f126, %f129;
	ld.global.nc.v2.f32 	{%f131, %f132}, [%rd59+2048];
	fma.rn.f32 	%f135, %f109, %f131, %f102;
	fma.rn.f32 	%f136, %f110, %f132, %f135;
	ld.global.nc.v2.f32 	{%f137, %f138}, [%rd60+2048];
	fma.rn.f32 	%f141, %f109, %f137, %f108;
	fma.rn.f32 	%f142, %f110, %f138, %f141;
	ld.global.nc.v2.f32 	{%f143, %f144}, [%rd72];
	ld.global.nc.v2.f32 	{%f147, %f148}, [%rd56+4096];
	fma.rn.f32 	%f151, %f143, %f147, %f118;
	fma.rn.f32 	%f152, %f144, %f148, %f151;
	ld.global.nc.v2.f32 	{%f153, %f154}, [%rd61+2048];
	fma.rn.f32 	%f157, %f143, %f153, %f124;
	fma.rn.f32 	%f158, %f144, %f154, %f157;
	ld.global.nc.v2.f32 	{%f159, %f160}, [%rd58+4096];
	fma.rn.f32 	%f163, %f143, %f159, %f130;
	fma.rn.f32 	%f164, %f144, %f160, %f163;
	ld.global.nc.v2.f32 	{%f165, %f166}, [%rd59+4096];
	fma.rn.f32 	%f169, %f143, %f165, %f136;
	fma.rn.f32 	%f170, %f144, %f166, %f169;
	ld.global.nc.v2.f32 	{%f171, %f172}, [%rd60+4096];
	fma.rn.f32 	%f175, %f143, %f171, %f142;
	fma.rn.f32 	%f176, %f144, %f172, %f175;
	ld.global.nc.v2.f32 	{%f177, %f178}, [%rd72+2048];
	ld.global.nc.v2.f32 	{%f181, %f182}, [%rd56+6144];
	fma.rn.f32 	%f185, %f177, %f181, %f152;
	fma.rn.f32 	%f331, %f178, %f182, %f185;
	ld.global.nc.v2.f32 	{%f186, %f187}, [%rd61+4096];
	fma.rn.f32 	%f190, %f177, %f186, %f158;
	fma.rn.f32 	%f330, %f178, %f187, %f190;
	ld.global.nc.v2.f32 	{%f191, %f192}, [%rd58+6144];
	fma.rn.f32 	%f195, %f177, %f191, %f164;
	fma.rn.f32 	%f329, %f178, %f192, %f195;
	ld.global.nc.v2.f32 	{%f196, %f197}, [%rd59+6144];
	fma.rn.f32 	%f200, %f177, %f196, %f170;
	fma.rn.f32 	%f328, %f178, %f197, %f200;
	ld.global.nc.v2.f32 	{%f201, %f202}, [%rd60+6144];
	fma.rn.f32 	%f205, %f177, %f201, %f176;
	fma.rn.f32 	%f327, %f178, %f202, %f205;
	add.s64 	%rd73, %rd73, 8192;
	add.s64 	%rd72, %rd72, 8192;
	add.s32 	%r223, %r223, 1024;
	setp.lt.s32 	%p6, %r223, %r15;
	@%p6 bra 	$L__BB60_9;

	st.local.f32 	[%rd3], %f331;
	st.local.f32 	[%rd3+4], %f330;
	st.local.f32 	[%rd3+8], %f329;
	st.local.f32 	[%rd3+12], %f328;
	st.local.f32 	[%rd3+16], %f327;

$L__BB60_11:
	shr.s32 	%r43, %r3, 31;
	shr.u32 	%r44, %r43, 27;
	add.s32 	%r45, %r3, %r44;
	shr.s32 	%r46, %r45, 5;
	shl.b32 	%r47, %r46, 2;
	add.s32 	%r14, %r28, %r47;
	mov.u32 	%r49, 2;
	mov.b32 	%r50, %f331;
	mov.u32 	%r51, 31;
	mov.u32 	%r52, 16;
	mov.u32 	%r53, -1;
	shfl.sync.bfly.b32 	%r54|%p7, %r50, %r52, %r51, %r53;
	mov.b32 	%f206, %r54;
	add.f32 	%f207, %f331, %f206;
	mov.b32 	%r55, %f207;
	mov.u32 	%r56, 8;
	shfl.sync.bfly.b32 	%r57|%p8, %r55, %r56, %r51, %r53;
	mov.b32 	%f208, %r57;
	add.f32 	%f209, %f207, %f208;
	mov.b32 	%r58, %f209;
	mov.u32 	%r59, 4;
	shfl.sync.bfly.b32 	%r60|%p9, %r58, %r59, %r51, %r53;
	mov.b32 	%f210, %r60;
	add.f32 	%f211, %f209, %f210;
	mov.b32 	%r61, %f211;
	shfl.sync.bfly.b32 	%r62|%p10, %r61, %r49, %r51, %r53;
	mov.b32 	%f212, %r62;
	add.f32 	%f213, %f211, %f212;
	mov.b32 	%r63, %f213;
	mov.u32 	%r64, 1;
	shfl.sync.bfly.b32 	%r65|%p11, %r63, %r64, %r51, %r53;
	mov.b32 	%f214, %r65;
	add.f32 	%f215, %f213, %f214;
	st.local.f32 	[%rd3], %f215;
	st.shared.f32 	[%r14], %f215;
	bar.sync 	0;
	@%p1 bra 	$L__BB60_13;

	ld.shared.f32 	%f216, [%r4];
	mov.b32 	%r66, %f216;
	shfl.sync.bfly.b32 	%r70|%p13, %r66, %r52, %r51, %r53;
	mov.b32 	%f217, %r70;
	add.f32 	%f218, %f216, %f217;
	mov.b32 	%r71, %f218;
	shfl.sync.bfly.b32 	%r73|%p14, %r71, %r56, %r51, %r53;
	mov.b32 	%f219, %r73;
	add.f32 	%f220, %f218, %f219;
	mov.b32 	%r74, %f220;
	shfl.sync.bfly.b32 	%r76|%p15, %r74, %r59, %r51, %r53;
	mov.b32 	%f221, %r76;
	add.f32 	%f222, %f220, %f221;
	mov.b32 	%r77, %f222;
	shfl.sync.bfly.b32 	%r79|%p16, %r77, %r49, %r51, %r53;
	mov.b32 	%f223, %r79;
	add.f32 	%f224, %f222, %f223;
	mov.b32 	%r80, %f224;
	shfl.sync.bfly.b32 	%r82|%p17, %r80, %r64, %r51, %r53;
	mov.b32 	%f225, %r82;
	add.f32 	%f226, %f224, %f225;
	st.local.f32 	[%rd3], %f226;

$L__BB60_13:
	bar.sync 	0;
	mov.b32 	%r83, %f330;
	shfl.sync.bfly.b32 	%r87|%p19, %r83, %r52, %r51, %r53;
	mov.b32 	%f227, %r87;
	add.f32 	%f228, %f330, %f227;
	mov.b32 	%r88, %f228;
	shfl.sync.bfly.b32 	%r90|%p20, %r88, %r56, %r51, %r53;
	mov.b32 	%f229, %r90;
	add.f32 	%f230, %f228, %f229;
	mov.b32 	%r91, %f230;
	shfl.sync.bfly.b32 	%r93|%p21, %r91, %r59, %r51, %r53;
	mov.b32 	%f231, %r93;
	add.f32 	%f232, %f230, %f231;
	mov.b32 	%r94, %f232;
	shfl.sync.bfly.b32 	%r96|%p22, %r94, %r49, %r51, %r53;
	mov.b32 	%f233, %r96;
	add.f32 	%f234, %f232, %f233;
	mov.b32 	%r97, %f234;
	shfl.sync.bfly.b32 	%r99|%p23, %r97, %r64, %r51, %r53;
	mov.b32 	%f235, %r99;
	add.f32 	%f236, %f234, %f235;
	st.local.f32 	[%rd3+4], %f236;
	st.shared.f32 	[%r14], %f236;
	bar.sync 	0;
	@%p1 bra 	$L__BB60_15;

	ld.shared.f32 	%f237, [%r4];
	mov.b32 	%r100, %f237;
	mov.u32 	%r101, 31;
	mov.u32 	%r102, 16;
	mov.u32 	%r103, -1;
	shfl.sync.bfly.b32 	%r104|%p24, %r100, %r102, %r101, %r103;
	mov.b32 	%f238, %r104;
	add.f32 	%f239, %f237, %f238;
	mov.b32 	%r105, %f239;
	mov.u32 	%r106, 8;
	shfl.sync.bfly.b32 	%r107|%p25, %r105, %r106, %r101, %r103;
	mov.b32 	%f240, %r107;
	add.f32 	%f241, %f239, %f240;
	mov.b32 	%r108, %f241;
	mov.u32 	%r109, 4;
	shfl.sync.bfly.b32 	%r110|%p26, %r108, %r109, %r101, %r103;
	mov.b32 	%f242, %r110;
	add.f32 	%f243, %f241, %f242;
	mov.b32 	%r111, %f243;
	mov.u32 	%r112, 2;
	shfl.sync.bfly.b32 	%r113|%p27, %r111, %r112, %r101, %r103;
	mov.b32 	%f244, %r113;
	add.f32 	%f245, %f243, %f244;
	mov.b32 	%r114, %f245;
	mov.u32 	%r115, 1;
	shfl.sync.bfly.b32 	%r116|%p28, %r114, %r115, %r101, %r103;
	mov.b32 	%f246, %r116;
	add.f32 	%f247, %f245, %f246;
	st.local.f32 	[%rd3+4], %f247;

$L__BB60_15:
	bar.sync 	0;
	mov.b32 	%r117, %f329;
	mov.u32 	%r118, 31;
	mov.u32 	%r119, 16;
	mov.u32 	%r120, -1;
	shfl.sync.bfly.b32 	%r121|%p30, %r117, %r119, %r118, %r120;
	mov.b32 	%f248, %r121;
	add.f32 	%f249, %f329, %f248;
	mov.b32 	%r122, %f249;
	mov.u32 	%r123, 8;
	shfl.sync.bfly.b32 	%r124|%p31, %r122, %r123, %r118, %r120;
	mov.b32 	%f250, %r124;
	add.f32 	%f251, %f249, %f250;
	mov.b32 	%r125, %f251;
	mov.u32 	%r126, 4;
	shfl.sync.bfly.b32 	%r127|%p32, %r125, %r126, %r118, %r120;
	mov.b32 	%f252, %r127;
	add.f32 	%f253, %f251, %f252;
	mov.b32 	%r128, %f253;
	mov.u32 	%r129, 2;
	shfl.sync.bfly.b32 	%r130|%p33, %r128, %r129, %r118, %r120;
	mov.b32 	%f254, %r130;
	add.f32 	%f255, %f253, %f254;
	mov.b32 	%r131, %f255;
	mov.u32 	%r132, 1;
	shfl.sync.bfly.b32 	%r133|%p34, %r131, %r132, %r118, %r120;
	mov.b32 	%f256, %r133;
	add.f32 	%f257, %f255, %f256;
	st.local.f32 	[%rd3+8], %f257;
	st.shared.f32 	[%r14], %f257;
	bar.sync 	0;
	@%p1 bra 	$L__BB60_17;

	ld.shared.f32 	%f258, [%r4];
	mov.b32 	%r134, %f258;
	shfl.sync.bfly.b32 	%r138|%p35, %r134, %r119, %r118, %r120;
	mov.b32 	%f259, %r138;
	add.f32 	%f260, %f258, %f259;
	mov.b32 	%r139, %f260;
	shfl.sync.bfly.b32 	%r141|%p36, %r139, %r123, %r118, %r120;
	mov.b32 	%f261, %r141;
	add.f32 	%f262, %f260, %f261;
	mov.b32 	%r142, %f262;
	shfl.sync.bfly.b32 	%r144|%p37, %r142, %r126, %r118, %r120;
	mov.b32 	%f263, %r144;
	add.f32 	%f264, %f262, %f263;
	mov.b32 	%r145, %f264;
	shfl.sync.bfly.b32 	%r147|%p38, %r145, %r129, %r118, %r120;
	mov.b32 	%f265, %r147;
	add.f32 	%f266, %f264, %f265;
	mov.b32 	%r148, %f266;
	shfl.sync.bfly.b32 	%r150|%p39, %r148, %r132, %r118, %r120;
	mov.b32 	%f267, %r150;
	add.f32 	%f268, %f266, %f267;
	st.local.f32 	[%rd3+8], %f268;

$L__BB60_17:
	bar.sync 	0;
	mov.b32 	%r151, %f328;
	shfl.sync.bfly.b32 	%r155|%p41, %r151, %r119, %r118, %r120;
	mov.b32 	%f269, %r155;
	add.f32 	%f270, %f328, %f269;
	mov.b32 	%r156, %f270;
	shfl.sync.bfly.b32 	%r158|%p42, %r156, %r123, %r118, %r120;
	mov.b32 	%f271, %r158;
	add.f32 	%f272, %f270, %f271;
	mov.b32 	%r159, %f272;
	shfl.sync.bfly.b32 	%r161|%p43, %r159, %r126, %r118, %r120;
	mov.b32 	%f273, %r161;
	add.f32 	%f274, %f272, %f273;
	mov.b32 	%r162, %f274;
	shfl.sync.bfly.b32 	%r164|%p44, %r162, %r129, %r118, %r120;
	mov.b32 	%f275, %r164;
	add.f32 	%f276, %f274, %f275;
	mov.b32 	%r165, %f276;
	shfl.sync.bfly.b32 	%r167|%p45, %r165, %r132, %r118, %r120;
	mov.b32 	%f277, %r167;
	add.f32 	%f278, %f276, %f277;
	st.local.f32 	[%rd3+12], %f278;
	st.shared.f32 	[%r14], %f278;
	bar.sync 	0;
	@%p1 bra 	$L__BB60_19;

	ld.shared.f32 	%f279, [%r4];
	mov.b32 	%r168, %f279;
	mov.u32 	%r169, 31;
	mov.u32 	%r170, 16;
	mov.u32 	%r171, -1;
	shfl.sync.bfly.b32 	%r172|%p46, %r168, %r170, %r169, %r171;
	mov.b32 	%f280, %r172;
	add.f32 	%f281, %f279, %f280;
	mov.b32 	%r173, %f281;
	mov.u32 	%r174, 8;
	shfl.sync.bfly.b32 	%r175|%p47, %r173, %r174, %r169, %r171;
	mov.b32 	%f282, %r175;
	add.f32 	%f283, %f281, %f282;
	mov.b32 	%r176, %f283;
	mov.u32 	%r177, 4;
	shfl.sync.bfly.b32 	%r178|%p48, %r176, %r177, %r169, %r171;
	mov.b32 	%f284, %r178;
	add.f32 	%f285, %f283, %f284;
	mov.b32 	%r179, %f285;
	mov.u32 	%r180, 2;
	shfl.sync.bfly.b32 	%r181|%p49, %r179, %r180, %r169, %r171;
	mov.b32 	%f286, %r181;
	add.f32 	%f287, %f285, %f286;
	mov.b32 	%r182, %f287;
	mov.u32 	%r183, 1;
	shfl.sync.bfly.b32 	%r184|%p50, %r182, %r183, %r169, %r171;
	mov.b32 	%f288, %r184;
	add.f32 	%f289, %f287, %f288;
	st.local.f32 	[%rd3+12], %f289;

$L__BB60_19:
	bar.sync 	0;
	mov.b32 	%r185, %f327;
	mov.u32 	%r186, 31;
	mov.u32 	%r187, 16;
	mov.u32 	%r188, -1;
	shfl.sync.bfly.b32 	%r189|%p52, %r185, %r187, %r186, %r188;
	mov.b32 	%f290, %r189;
	add.f32 	%f291, %f327, %f290;
	mov.b32 	%r190, %f291;
	mov.u32 	%r191, 8;
	shfl.sync.bfly.b32 	%r192|%p53, %r190, %r191, %r186, %r188;
	mov.b32 	%f292, %r192;
	add.f32 	%f293, %f291, %f292;
	mov.b32 	%r193, %f293;
	mov.u32 	%r194, 4;
	shfl.sync.bfly.b32 	%r195|%p54, %r193, %r194, %r186, %r188;
	mov.b32 	%f294, %r195;
	add.f32 	%f295, %f293, %f294;
	mov.b32 	%r196, %f295;
	mov.u32 	%r197, 2;
	shfl.sync.bfly.b32 	%r198|%p55, %r196, %r197, %r186, %r188;
	mov.b32 	%f296, %r198;
	add.f32 	%f297, %f295, %f296;
	mov.b32 	%r199, %f297;
	mov.u32 	%r200, 1;
	shfl.sync.bfly.b32 	%r201|%p56, %r199, %r200, %r186, %r188;
	mov.b32 	%f298, %r201;
	add.f32 	%f299, %f297, %f298;
	st.local.f32 	[%rd3+16], %f299;
	st.shared.f32 	[%r14], %f299;
	bar.sync 	0;
	@%p1 bra 	$L__BB60_21;

	ld.shared.f32 	%f300, [%r4];
	mov.b32 	%r202, %f300;
	shfl.sync.bfly.b32 	%r206|%p57, %r202, %r187, %r186, %r188;
	mov.b32 	%f301, %r206;
	add.f32 	%f302, %f300, %f301;
	mov.b32 	%r207, %f302;
	shfl.sync.bfly.b32 	%r209|%p58, %r207, %r191, %r186, %r188;
	mov.b32 	%f303, %r209;
	add.f32 	%f304, %f302, %f303;
	mov.b32 	%r210, %f304;
	shfl.sync.bfly.b32 	%r212|%p59, %r210, %r194, %r186, %r188;
	mov.b32 	%f305, %r212;
	add.f32 	%f306, %f304, %f305;
	mov.b32 	%r213, %f306;
	shfl.sync.bfly.b32 	%r215|%p60, %r213, %r197, %r186, %r188;
	mov.b32 	%f307, %r215;
	add.f32 	%f308, %f306, %f307;
	mov.b32 	%r216, %f308;
	shfl.sync.bfly.b32 	%r218|%p61, %r216, %r200, %r186, %r188;
	mov.b32 	%f309, %r218;
	add.f32 	%f310, %f308, %f309;
	st.local.f32 	[%rd3+16], %f310;

$L__BB60_21:
	bar.sync 	0;
	setp.gt.s32 	%p62, %r3, 4;
	@%p62 bra 	$L__BB60_23;

	mul.wide.s32 	%rd62, %r3, 4;
	add.s64 	%rd63, %rd3, %rd62;
	ld.local.f32 	%f311, [%rd63];
	mad.lo.s32 	%r219, %r3, %r17, %r2;
	cvt.s64.s32 	%rd64, %r219;
	mul.lo.s32 	%r220, %r1, %r18;
	cvt.s64.s32 	%rd65, %r220;
	add.s64 	%rd66, %rd65, %rd64;
	cvta.to.global.u64 	%rd67, %rd27;
	shl.b64 	%rd68, %rd66, 2;
	add.s64 	%rd69, %rd67, %rd68;
	st.global.f32 	[%rd69], %f311;

$L__BB60_23:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_6_bs_256
.visible .entry ggml_matvec_f32_ncols_6_bs_256(
	.param .u64 ggml_matvec_f32_ncols_6_bs_256_param_0,
	.param .u64 ggml_matvec_f32_ncols_6_bs_256_param_1,
	.param .u64 ggml_matvec_f32_ncols_6_bs_256_param_2,
	.param .u32 ggml_matvec_f32_ncols_6_bs_256_param_3,
	.param .u32 ggml_matvec_f32_ncols_6_bs_256_param_4,
	.param .u32 ggml_matvec_f32_ncols_6_bs_256_param_5,
	.param .u32 ggml_matvec_f32_ncols_6_bs_256_param_6,
	.param .u32 ggml_matvec_f32_ncols_6_bs_256_param_7,
	.param .u32 ggml_matvec_f32_ncols_6_bs_256_param_8,
	.param .u32 ggml_matvec_f32_ncols_6_bs_256_param_9,
	.param .u32 ggml_matvec_f32_ncols_6_bs_256_param_10,
	.param .u32 ggml_matvec_f32_ncols_6_bs_256_param_11
)
{
	.local .align 8 .b8 	__local_depot61[24];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<73>;
	.reg .f32 	%f<296>;
	.reg .b32 	%r<253>;
	.reg .b64 	%rd<67>;


	mov.u64 	%SPL, __local_depot61;
	ld.param.u64 	%rd20, [ggml_matvec_f32_ncols_6_bs_256_param_0];
	ld.param.u64 	%rd21, [ggml_matvec_f32_ncols_6_bs_256_param_1];
	ld.param.u64 	%rd19, [ggml_matvec_f32_ncols_6_bs_256_param_2];
	ld.param.u32 	%r11, [ggml_matvec_f32_ncols_6_bs_256_param_3];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_6_bs_256_param_5];
	ld.param.u32 	%r12, [ggml_matvec_f32_ncols_6_bs_256_param_6];
	ld.param.u32 	%r13, [ggml_matvec_f32_ncols_6_bs_256_param_7];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_6_bs_256_param_8];
	ld.param.u32 	%r17, [ggml_matvec_f32_ncols_6_bs_256_param_9];
	ld.param.u32 	%r18, [ggml_matvec_f32_ncols_6_bs_256_param_10];
	ld.param.u32 	%r14, [ggml_matvec_f32_ncols_6_bs_256_param_11];
	cvta.to.global.u64 	%rd66, %rd21;
	add.u64 	%rd2, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r19, %r1, %r16;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r20, %r2, %r15;
	mad.lo.s32 	%r21, %r19, %r17, %r20;
	cvt.s64.s32 	%rd3, %r21;
	cvta.to.global.u64 	%rd4, %rd20;
	mul.lo.s32 	%r22, %r1, %r18;
	cvt.s64.s32 	%rd5, %r22;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r23, %r3, 2;
	mov.u32 	%r24, data_mmv;
	add.s32 	%r4, %r24, %r23;
	@%p1 bra 	$L__BB61_2;

	mov.u32 	%r25, 0;
	st.shared.u32 	[%r4], %r25;

$L__BB61_2:
	bar.sync 	0;
	mov.f32 	%f290, 0f00000000;
	st.local.v2.f32 	[%rd2], {%f290, %f290};
	st.local.v2.f32 	[%rd2+8], {%f290, %f290};
	st.local.v2.f32 	[%rd2+16], {%f290, %f290};
	setp.ge.s32 	%p2, %r3, %r11;
	mov.f32 	%f291, %f290;
	mov.f32 	%f292, %f290;
	mov.f32 	%f293, %f290;
	mov.f32 	%f294, %f290;
	mov.f32 	%f295, %f290;
	@%p2 bra 	$L__BB61_9;

	not.b32 	%r26, %r3;
	add.s32 	%r5, %r26, %r11;
	and.b32  	%r27, %r5, 256;
	setp.ne.s32 	%p3, %r27, 0;
	mov.f32 	%f290, 0f00000000;
	mov.u32 	%r252, %r3;
	@%p3 bra 	$L__BB61_5;

	shl.b64 	%rd23, %rd5, 2;
	add.s64 	%rd24, %rd66, %rd23;
	shl.b64 	%rd25, %rd3, 2;
	add.s64 	%rd26, %rd4, %rd25;
	mul.wide.s32 	%rd27, %r3, 8;
	add.s64 	%rd28, %rd26, %rd27;
	ld.global.nc.v2.f32 	{%f43, %f44}, [%rd28];
	add.s64 	%rd29, %rd24, %rd27;
	ld.global.nc.v2.f32 	{%f47, %f48}, [%rd29];
	fma.rn.f32 	%f51, %f43, %f47, 0f00000000;
	fma.rn.f32 	%f295, %f44, %f48, %f51;
	mul.wide.s32 	%rd30, %r12, 8;
	add.s64 	%rd31, %rd29, %rd30;
	ld.global.nc.v2.f32 	{%f52, %f53}, [%rd31];
	fma.rn.f32 	%f56, %f43, %f52, 0f00000000;
	fma.rn.f32 	%f294, %f44, %f53, %f56;
	st.local.v2.f32 	[%rd2], {%f295, %f294};
	add.s32 	%r28, %r3, %r12;
	add.s32 	%r29, %r28, %r12;
	mul.wide.s32 	%rd32, %r29, 8;
	add.s64 	%rd33, %rd24, %rd32;
	ld.global.nc.v2.f32 	{%f57, %f58}, [%rd33];
	fma.rn.f32 	%f61, %f43, %f57, 0f00000000;
	fma.rn.f32 	%f293, %f44, %f58, %f61;
	add.s64 	%rd34, %rd33, %rd30;
	ld.global.nc.v2.f32 	{%f62, %f63}, [%rd34];
	fma.rn.f32 	%f66, %f43, %f62, 0f00000000;
	fma.rn.f32 	%f292, %f44, %f63, %f66;
	st.local.v2.f32 	[%rd2+8], {%f293, %f292};
	add.s64 	%rd35, %rd34, %rd30;
	ld.global.nc.v2.f32 	{%f67, %f68}, [%rd35];
	fma.rn.f32 	%f71, %f43, %f67, 0f00000000;
	fma.rn.f32 	%f291, %f44, %f68, %f71;
	add.s64 	%rd36, %rd35, %rd30;
	ld.global.nc.v2.f32 	{%f72, %f73}, [%rd36];
	fma.rn.f32 	%f76, %f43, %f72, 0f00000000;
	fma.rn.f32 	%f290, %f44, %f73, %f76;
	st.local.v2.f32 	[%rd2+16], {%f291, %f290};
	add.s32 	%r252, %r3, 256;

$L__BB61_5:
	and.b32  	%r30, %r5, -256;
	setp.eq.s32 	%p4, %r30, 0;
	@%p4 bra 	$L__BB61_9;

	add.s32 	%r31, %r252, %r12;
	add.s32 	%r32, %r31, 256;
	mul.wide.s32 	%rd37, %r32, 8;
	shl.b64 	%rd38, %rd5, 2;
	add.s64 	%rd7, %rd37, %rd38;
	shl.b32 	%r33, %r12, 1;
	add.s32 	%r34, %r252, %r33;
	mad.lo.s32 	%r35, %r12, 3, %r252;
	shl.b32 	%r36, %r12, 2;
	add.s32 	%r37, %r252, %r36;
	mad.lo.s32 	%r38, %r12, 5, %r252;
	mul.wide.s32 	%rd39, %r34, 8;
	add.s64 	%rd8, %rd39, %rd38;
	mul.wide.s32 	%rd40, %r35, 8;
	add.s64 	%rd9, %rd40, %rd38;
	mul.wide.s32 	%rd41, %r37, 8;
	add.s64 	%rd10, %rd41, %rd38;
	mul.wide.s32 	%rd42, %r38, 8;
	add.s64 	%rd11, %rd42, %rd38;
	mul.wide.s32 	%rd43, %r252, 2;
	add.s64 	%rd44, %rd43, %rd3;
	shl.b64 	%rd45, %rd44, 2;
	add.s64 	%rd46, %rd4, %rd45;
	add.s64 	%rd65, %rd46, 2048;
	mul.wide.s32 	%rd47, %r252, 8;
	mul.wide.s32 	%rd48, %r12, 8;
	add.s64 	%rd49, %rd47, %rd48;
	add.s64 	%rd13, %rd49, %rd38;
	add.s64 	%rd14, %rd47, %rd38;

$L__BB61_7:
	ld.global.nc.v2.f32 	{%f77, %f78}, [%rd65+-2048];
	add.s64 	%rd50, %rd66, %rd14;
	ld.global.nc.v2.f32 	{%f81, %f82}, [%rd50];
	fma.rn.f32 	%f85, %f77, %f81, %f295;
	fma.rn.f32 	%f86, %f78, %f82, %f85;
	add.s64 	%rd51, %rd66, %rd13;
	ld.global.nc.v2.f32 	{%f87, %f88}, [%rd51];
	fma.rn.f32 	%f91, %f77, %f87, %f294;
	fma.rn.f32 	%f92, %f78, %f88, %f91;
	add.s64 	%rd52, %rd66, %rd8;
	ld.global.nc.v2.f32 	{%f93, %f94}, [%rd52];
	fma.rn.f32 	%f97, %f77, %f93, %f293;
	fma.rn.f32 	%f98, %f78, %f94, %f97;
	add.s64 	%rd53, %rd66, %rd9;
	ld.global.nc.v2.f32 	{%f99, %f100}, [%rd53];
	fma.rn.f32 	%f103, %f77, %f99, %f292;
	fma.rn.f32 	%f104, %f78, %f100, %f103;
	add.s64 	%rd54, %rd66, %rd10;
	ld.global.nc.v2.f32 	{%f105, %f106}, [%rd54];
	fma.rn.f32 	%f109, %f77, %f105, %f291;
	fma.rn.f32 	%f110, %f78, %f106, %f109;
	add.s64 	%rd55, %rd66, %rd11;
	ld.global.nc.v2.f32 	{%f111, %f112}, [%rd55];
	fma.rn.f32 	%f115, %f77, %f111, %f290;
	fma.rn.f32 	%f116, %f78, %f112, %f115;
	ld.global.nc.v2.f32 	{%f117, %f118}, [%rd65];
	ld.global.nc.v2.f32 	{%f121, %f122}, [%rd50+2048];
	fma.rn.f32 	%f125, %f117, %f121, %f86;
	fma.rn.f32 	%f295, %f118, %f122, %f125;
	add.s64 	%rd56, %rd66, %rd7;
	ld.global.nc.v2.f32 	{%f126, %f127}, [%rd56];
	fma.rn.f32 	%f130, %f117, %f126, %f92;
	fma.rn.f32 	%f294, %f118, %f127, %f130;
	ld.global.nc.v2.f32 	{%f131, %f132}, [%rd52+2048];
	fma.rn.f32 	%f135, %f117, %f131, %f98;
	fma.rn.f32 	%f293, %f118, %f132, %f135;
	ld.global.nc.v2.f32 	{%f136, %f137}, [%rd53+2048];
	fma.rn.f32 	%f140, %f117, %f136, %f104;
	fma.rn.f32 	%f292, %f118, %f137, %f140;
	ld.global.nc.v2.f32 	{%f141, %f142}, [%rd54+2048];
	fma.rn.f32 	%f145, %f117, %f141, %f110;
	fma.rn.f32 	%f291, %f118, %f142, %f145;
	ld.global.nc.v2.f32 	{%f146, %f147}, [%rd55+2048];
	fma.rn.f32 	%f150, %f117, %f146, %f116;
	fma.rn.f32 	%f290, %f118, %f147, %f150;
	add.s64 	%rd66, %rd66, 4096;
	add.s64 	%rd65, %rd65, 4096;
	add.s32 	%r252, %r252, 512;
	setp.lt.s32 	%p5, %r252, %r11;
	@%p5 bra 	$L__BB61_7;

	st.local.v2.f32 	[%rd2], {%f295, %f294};
	st.local.v2.f32 	[%rd2+8], {%f293, %f292};
	st.local.v2.f32 	[%rd2+16], {%f291, %f290};

$L__BB61_9:
	shr.s32 	%r39, %r3, 31;
	shr.u32 	%r40, %r39, 27;
	add.s32 	%r41, %r3, %r40;
	shr.s32 	%r42, %r41, 5;
	shl.b32 	%r43, %r42, 2;
	add.s32 	%r10, %r24, %r43;
	mov.u32 	%r45, 2;
	mov.b32 	%r46, %f295;
	mov.u32 	%r47, 31;
	mov.u32 	%r48, 16;
	mov.u32 	%r49, -1;
	shfl.sync.bfly.b32 	%r50|%p6, %r46, %r48, %r47, %r49;
	mov.b32 	%f151, %r50;
	add.f32 	%f152, %f295, %f151;
	mov.b32 	%r51, %f152;
	mov.u32 	%r52, 8;
	shfl.sync.bfly.b32 	%r53|%p7, %r51, %r52, %r47, %r49;
	mov.b32 	%f153, %r53;
	add.f32 	%f154, %f152, %f153;
	mov.b32 	%r54, %f154;
	mov.u32 	%r55, 4;
	shfl.sync.bfly.b32 	%r56|%p8, %r54, %r55, %r47, %r49;
	mov.b32 	%f155, %r56;
	add.f32 	%f156, %f154, %f155;
	mov.b32 	%r57, %f156;
	shfl.sync.bfly.b32 	%r58|%p9, %r57, %r45, %r47, %r49;
	mov.b32 	%f157, %r58;
	add.f32 	%f158, %f156, %f157;
	mov.b32 	%r59, %f158;
	mov.u32 	%r60, 1;
	shfl.sync.bfly.b32 	%r61|%p10, %r59, %r60, %r47, %r49;
	mov.b32 	%f159, %r61;
	add.f32 	%f160, %f158, %f159;
	st.local.f32 	[%rd2], %f160;
	st.shared.f32 	[%r10], %f160;
	bar.sync 	0;
	@%p1 bra 	$L__BB61_11;

	ld.shared.f32 	%f161, [%r4];
	mov.b32 	%r62, %f161;
	shfl.sync.bfly.b32 	%r66|%p12, %r62, %r48, %r47, %r49;
	mov.b32 	%f162, %r66;
	add.f32 	%f163, %f161, %f162;
	mov.b32 	%r67, %f163;
	shfl.sync.bfly.b32 	%r69|%p13, %r67, %r52, %r47, %r49;
	mov.b32 	%f164, %r69;
	add.f32 	%f165, %f163, %f164;
	mov.b32 	%r70, %f165;
	shfl.sync.bfly.b32 	%r72|%p14, %r70, %r55, %r47, %r49;
	mov.b32 	%f166, %r72;
	add.f32 	%f167, %f165, %f166;
	mov.b32 	%r73, %f167;
	shfl.sync.bfly.b32 	%r75|%p15, %r73, %r45, %r47, %r49;
	mov.b32 	%f168, %r75;
	add.f32 	%f169, %f167, %f168;
	mov.b32 	%r76, %f169;
	shfl.sync.bfly.b32 	%r78|%p16, %r76, %r60, %r47, %r49;
	mov.b32 	%f170, %r78;
	add.f32 	%f171, %f169, %f170;
	st.local.f32 	[%rd2], %f171;

$L__BB61_11:
	bar.sync 	0;
	mov.b32 	%r79, %f294;
	shfl.sync.bfly.b32 	%r83|%p18, %r79, %r48, %r47, %r49;
	mov.b32 	%f172, %r83;
	add.f32 	%f173, %f294, %f172;
	mov.b32 	%r84, %f173;
	shfl.sync.bfly.b32 	%r86|%p19, %r84, %r52, %r47, %r49;
	mov.b32 	%f174, %r86;
	add.f32 	%f175, %f173, %f174;
	mov.b32 	%r87, %f175;
	shfl.sync.bfly.b32 	%r89|%p20, %r87, %r55, %r47, %r49;
	mov.b32 	%f176, %r89;
	add.f32 	%f177, %f175, %f176;
	mov.b32 	%r90, %f177;
	shfl.sync.bfly.b32 	%r92|%p21, %r90, %r45, %r47, %r49;
	mov.b32 	%f178, %r92;
	add.f32 	%f179, %f177, %f178;
	mov.b32 	%r93, %f179;
	shfl.sync.bfly.b32 	%r95|%p22, %r93, %r60, %r47, %r49;
	mov.b32 	%f180, %r95;
	add.f32 	%f181, %f179, %f180;
	st.local.f32 	[%rd2+4], %f181;
	st.shared.f32 	[%r10], %f181;
	bar.sync 	0;
	@%p1 bra 	$L__BB61_13;

	ld.shared.f32 	%f182, [%r4];
	mov.b32 	%r96, %f182;
	mov.u32 	%r97, 31;
	mov.u32 	%r98, 16;
	mov.u32 	%r99, -1;
	shfl.sync.bfly.b32 	%r100|%p23, %r96, %r98, %r97, %r99;
	mov.b32 	%f183, %r100;
	add.f32 	%f184, %f182, %f183;
	mov.b32 	%r101, %f184;
	mov.u32 	%r102, 8;
	shfl.sync.bfly.b32 	%r103|%p24, %r101, %r102, %r97, %r99;
	mov.b32 	%f185, %r103;
	add.f32 	%f186, %f184, %f185;
	mov.b32 	%r104, %f186;
	mov.u32 	%r105, 4;
	shfl.sync.bfly.b32 	%r106|%p25, %r104, %r105, %r97, %r99;
	mov.b32 	%f187, %r106;
	add.f32 	%f188, %f186, %f187;
	mov.b32 	%r107, %f188;
	mov.u32 	%r108, 2;
	shfl.sync.bfly.b32 	%r109|%p26, %r107, %r108, %r97, %r99;
	mov.b32 	%f189, %r109;
	add.f32 	%f190, %f188, %f189;
	mov.b32 	%r110, %f190;
	mov.u32 	%r111, 1;
	shfl.sync.bfly.b32 	%r112|%p27, %r110, %r111, %r97, %r99;
	mov.b32 	%f191, %r112;
	add.f32 	%f192, %f190, %f191;
	st.local.f32 	[%rd2+4], %f192;

$L__BB61_13:
	bar.sync 	0;
	mov.b32 	%r113, %f293;
	mov.u32 	%r114, 31;
	mov.u32 	%r115, 16;
	mov.u32 	%r116, -1;
	shfl.sync.bfly.b32 	%r117|%p29, %r113, %r115, %r114, %r116;
	mov.b32 	%f193, %r117;
	add.f32 	%f194, %f293, %f193;
	mov.b32 	%r118, %f194;
	mov.u32 	%r119, 8;
	shfl.sync.bfly.b32 	%r120|%p30, %r118, %r119, %r114, %r116;
	mov.b32 	%f195, %r120;
	add.f32 	%f196, %f194, %f195;
	mov.b32 	%r121, %f196;
	mov.u32 	%r122, 4;
	shfl.sync.bfly.b32 	%r123|%p31, %r121, %r122, %r114, %r116;
	mov.b32 	%f197, %r123;
	add.f32 	%f198, %f196, %f197;
	mov.b32 	%r124, %f198;
	mov.u32 	%r125, 2;
	shfl.sync.bfly.b32 	%r126|%p32, %r124, %r125, %r114, %r116;
	mov.b32 	%f199, %r126;
	add.f32 	%f200, %f198, %f199;
	mov.b32 	%r127, %f200;
	mov.u32 	%r128, 1;
	shfl.sync.bfly.b32 	%r129|%p33, %r127, %r128, %r114, %r116;
	mov.b32 	%f201, %r129;
	add.f32 	%f202, %f200, %f201;
	st.local.f32 	[%rd2+8], %f202;
	st.shared.f32 	[%r10], %f202;
	bar.sync 	0;
	@%p1 bra 	$L__BB61_15;

	ld.shared.f32 	%f203, [%r4];
	mov.b32 	%r130, %f203;
	shfl.sync.bfly.b32 	%r134|%p34, %r130, %r115, %r114, %r116;
	mov.b32 	%f204, %r134;
	add.f32 	%f205, %f203, %f204;
	mov.b32 	%r135, %f205;
	shfl.sync.bfly.b32 	%r137|%p35, %r135, %r119, %r114, %r116;
	mov.b32 	%f206, %r137;
	add.f32 	%f207, %f205, %f206;
	mov.b32 	%r138, %f207;
	shfl.sync.bfly.b32 	%r140|%p36, %r138, %r122, %r114, %r116;
	mov.b32 	%f208, %r140;
	add.f32 	%f209, %f207, %f208;
	mov.b32 	%r141, %f209;
	shfl.sync.bfly.b32 	%r143|%p37, %r141, %r125, %r114, %r116;
	mov.b32 	%f210, %r143;
	add.f32 	%f211, %f209, %f210;
	mov.b32 	%r144, %f211;
	shfl.sync.bfly.b32 	%r146|%p38, %r144, %r128, %r114, %r116;
	mov.b32 	%f212, %r146;
	add.f32 	%f213, %f211, %f212;
	st.local.f32 	[%rd2+8], %f213;

$L__BB61_15:
	bar.sync 	0;
	mov.b32 	%r147, %f292;
	shfl.sync.bfly.b32 	%r151|%p40, %r147, %r115, %r114, %r116;
	mov.b32 	%f214, %r151;
	add.f32 	%f215, %f292, %f214;
	mov.b32 	%r152, %f215;
	shfl.sync.bfly.b32 	%r154|%p41, %r152, %r119, %r114, %r116;
	mov.b32 	%f216, %r154;
	add.f32 	%f217, %f215, %f216;
	mov.b32 	%r155, %f217;
	shfl.sync.bfly.b32 	%r157|%p42, %r155, %r122, %r114, %r116;
	mov.b32 	%f218, %r157;
	add.f32 	%f219, %f217, %f218;
	mov.b32 	%r158, %f219;
	shfl.sync.bfly.b32 	%r160|%p43, %r158, %r125, %r114, %r116;
	mov.b32 	%f220, %r160;
	add.f32 	%f221, %f219, %f220;
	mov.b32 	%r161, %f221;
	shfl.sync.bfly.b32 	%r163|%p44, %r161, %r128, %r114, %r116;
	mov.b32 	%f222, %r163;
	add.f32 	%f223, %f221, %f222;
	st.local.f32 	[%rd2+12], %f223;
	st.shared.f32 	[%r10], %f223;
	bar.sync 	0;
	@%p1 bra 	$L__BB61_17;

	ld.shared.f32 	%f224, [%r4];
	mov.b32 	%r164, %f224;
	mov.u32 	%r165, 31;
	mov.u32 	%r166, 16;
	mov.u32 	%r167, -1;
	shfl.sync.bfly.b32 	%r168|%p45, %r164, %r166, %r165, %r167;
	mov.b32 	%f225, %r168;
	add.f32 	%f226, %f224, %f225;
	mov.b32 	%r169, %f226;
	mov.u32 	%r170, 8;
	shfl.sync.bfly.b32 	%r171|%p46, %r169, %r170, %r165, %r167;
	mov.b32 	%f227, %r171;
	add.f32 	%f228, %f226, %f227;
	mov.b32 	%r172, %f228;
	mov.u32 	%r173, 4;
	shfl.sync.bfly.b32 	%r174|%p47, %r172, %r173, %r165, %r167;
	mov.b32 	%f229, %r174;
	add.f32 	%f230, %f228, %f229;
	mov.b32 	%r175, %f230;
	mov.u32 	%r176, 2;
	shfl.sync.bfly.b32 	%r177|%p48, %r175, %r176, %r165, %r167;
	mov.b32 	%f231, %r177;
	add.f32 	%f232, %f230, %f231;
	mov.b32 	%r178, %f232;
	mov.u32 	%r179, 1;
	shfl.sync.bfly.b32 	%r180|%p49, %r178, %r179, %r165, %r167;
	mov.b32 	%f233, %r180;
	add.f32 	%f234, %f232, %f233;
	st.local.f32 	[%rd2+12], %f234;

$L__BB61_17:
	bar.sync 	0;
	mov.b32 	%r181, %f291;
	mov.u32 	%r182, 31;
	mov.u32 	%r183, 16;
	mov.u32 	%r184, -1;
	shfl.sync.bfly.b32 	%r185|%p51, %r181, %r183, %r182, %r184;
	mov.b32 	%f235, %r185;
	add.f32 	%f236, %f291, %f235;
	mov.b32 	%r186, %f236;
	mov.u32 	%r187, 8;
	shfl.sync.bfly.b32 	%r188|%p52, %r186, %r187, %r182, %r184;
	mov.b32 	%f237, %r188;
	add.f32 	%f238, %f236, %f237;
	mov.b32 	%r189, %f238;
	mov.u32 	%r190, 4;
	shfl.sync.bfly.b32 	%r191|%p53, %r189, %r190, %r182, %r184;
	mov.b32 	%f239, %r191;
	add.f32 	%f240, %f238, %f239;
	mov.b32 	%r192, %f240;
	mov.u32 	%r193, 2;
	shfl.sync.bfly.b32 	%r194|%p54, %r192, %r193, %r182, %r184;
	mov.b32 	%f241, %r194;
	add.f32 	%f242, %f240, %f241;
	mov.b32 	%r195, %f242;
	mov.u32 	%r196, 1;
	shfl.sync.bfly.b32 	%r197|%p55, %r195, %r196, %r182, %r184;
	mov.b32 	%f243, %r197;
	add.f32 	%f244, %f242, %f243;
	st.local.f32 	[%rd2+16], %f244;
	st.shared.f32 	[%r10], %f244;
	bar.sync 	0;
	@%p1 bra 	$L__BB61_19;

	ld.shared.f32 	%f245, [%r4];
	mov.b32 	%r198, %f245;
	shfl.sync.bfly.b32 	%r202|%p56, %r198, %r183, %r182, %r184;
	mov.b32 	%f246, %r202;
	add.f32 	%f247, %f245, %f246;
	mov.b32 	%r203, %f247;
	shfl.sync.bfly.b32 	%r205|%p57, %r203, %r187, %r182, %r184;
	mov.b32 	%f248, %r205;
	add.f32 	%f249, %f247, %f248;
	mov.b32 	%r206, %f249;
	shfl.sync.bfly.b32 	%r208|%p58, %r206, %r190, %r182, %r184;
	mov.b32 	%f250, %r208;
	add.f32 	%f251, %f249, %f250;
	mov.b32 	%r209, %f251;
	shfl.sync.bfly.b32 	%r211|%p59, %r209, %r193, %r182, %r184;
	mov.b32 	%f252, %r211;
	add.f32 	%f253, %f251, %f252;
	mov.b32 	%r212, %f253;
	shfl.sync.bfly.b32 	%r214|%p60, %r212, %r196, %r182, %r184;
	mov.b32 	%f254, %r214;
	add.f32 	%f255, %f253, %f254;
	st.local.f32 	[%rd2+16], %f255;

$L__BB61_19:
	bar.sync 	0;
	mov.b32 	%r215, %f290;
	shfl.sync.bfly.b32 	%r219|%p62, %r215, %r183, %r182, %r184;
	mov.b32 	%f256, %r219;
	add.f32 	%f257, %f290, %f256;
	mov.b32 	%r220, %f257;
	shfl.sync.bfly.b32 	%r222|%p63, %r220, %r187, %r182, %r184;
	mov.b32 	%f258, %r222;
	add.f32 	%f259, %f257, %f258;
	mov.b32 	%r223, %f259;
	shfl.sync.bfly.b32 	%r225|%p64, %r223, %r190, %r182, %r184;
	mov.b32 	%f260, %r225;
	add.f32 	%f261, %f259, %f260;
	mov.b32 	%r226, %f261;
	shfl.sync.bfly.b32 	%r228|%p65, %r226, %r193, %r182, %r184;
	mov.b32 	%f262, %r228;
	add.f32 	%f263, %f261, %f262;
	mov.b32 	%r229, %f263;
	shfl.sync.bfly.b32 	%r231|%p66, %r229, %r196, %r182, %r184;
	mov.b32 	%f264, %r231;
	add.f32 	%f265, %f263, %f264;
	st.local.f32 	[%rd2+20], %f265;
	st.shared.f32 	[%r10], %f265;
	bar.sync 	0;
	@%p1 bra 	$L__BB61_21;

	ld.shared.f32 	%f266, [%r4];
	mov.b32 	%r232, %f266;
	mov.u32 	%r233, 31;
	mov.u32 	%r234, 16;
	mov.u32 	%r235, -1;
	shfl.sync.bfly.b32 	%r236|%p67, %r232, %r234, %r233, %r235;
	mov.b32 	%f267, %r236;
	add.f32 	%f268, %f266, %f267;
	mov.b32 	%r237, %f268;
	mov.u32 	%r238, 8;
	shfl.sync.bfly.b32 	%r239|%p68, %r237, %r238, %r233, %r235;
	mov.b32 	%f269, %r239;
	add.f32 	%f270, %f268, %f269;
	mov.b32 	%r240, %f270;
	mov.u32 	%r241, 4;
	shfl.sync.bfly.b32 	%r242|%p69, %r240, %r241, %r233, %r235;
	mov.b32 	%f271, %r242;
	add.f32 	%f272, %f270, %f271;
	mov.b32 	%r243, %f272;
	mov.u32 	%r244, 2;
	shfl.sync.bfly.b32 	%r245|%p70, %r243, %r244, %r233, %r235;
	mov.b32 	%f273, %r245;
	add.f32 	%f274, %f272, %f273;
	mov.b32 	%r246, %f274;
	mov.u32 	%r247, 1;
	shfl.sync.bfly.b32 	%r248|%p71, %r246, %r247, %r233, %r235;
	mov.b32 	%f275, %r248;
	add.f32 	%f276, %f274, %f275;
	st.local.f32 	[%rd2+20], %f276;

$L__BB61_21:
	bar.sync 	0;
	setp.gt.s32 	%p72, %r3, 5;
	@%p72 bra 	$L__BB61_23;

	mul.wide.s32 	%rd57, %r3, 4;
	add.s64 	%rd58, %rd2, %rd57;
	ld.local.f32 	%f277, [%rd58];
	mad.lo.s32 	%r249, %r3, %r13, %r2;
	cvt.s64.s32 	%rd59, %r249;
	mul.lo.s32 	%r250, %r1, %r14;
	cvt.s64.s32 	%rd60, %r250;
	add.s64 	%rd61, %rd60, %rd59;
	cvta.to.global.u64 	%rd62, %rd19;
	shl.b64 	%rd63, %rd61, 2;
	add.s64 	%rd64, %rd62, %rd63;
	st.global.f32 	[%rd64], %f277;

$L__BB61_23:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_7_bs_256
.visible .entry ggml_matvec_f32_ncols_7_bs_256(
	.param .u64 ggml_matvec_f32_ncols_7_bs_256_param_0,
	.param .u64 ggml_matvec_f32_ncols_7_bs_256_param_1,
	.param .u64 ggml_matvec_f32_ncols_7_bs_256_param_2,
	.param .u32 ggml_matvec_f32_ncols_7_bs_256_param_3,
	.param .u32 ggml_matvec_f32_ncols_7_bs_256_param_4,
	.param .u32 ggml_matvec_f32_ncols_7_bs_256_param_5,
	.param .u32 ggml_matvec_f32_ncols_7_bs_256_param_6,
	.param .u32 ggml_matvec_f32_ncols_7_bs_256_param_7,
	.param .u32 ggml_matvec_f32_ncols_7_bs_256_param_8,
	.param .u32 ggml_matvec_f32_ncols_7_bs_256_param_9,
	.param .u32 ggml_matvec_f32_ncols_7_bs_256_param_10,
	.param .u32 ggml_matvec_f32_ncols_7_bs_256_param_11
)
{
	.local .align 4 .b8 	__local_depot62[28];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<84>;
	.reg .f32 	%f<343>;
	.reg .b32 	%r<289>;
	.reg .b64 	%rd<71>;


	mov.u64 	%SPL, __local_depot62;
	ld.param.u64 	%rd21, [ggml_matvec_f32_ncols_7_bs_256_param_0];
	ld.param.u64 	%rd22, [ggml_matvec_f32_ncols_7_bs_256_param_1];
	ld.param.u64 	%rd20, [ggml_matvec_f32_ncols_7_bs_256_param_2];
	ld.param.u32 	%r11, [ggml_matvec_f32_ncols_7_bs_256_param_3];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_7_bs_256_param_5];
	ld.param.u32 	%r12, [ggml_matvec_f32_ncols_7_bs_256_param_6];
	ld.param.u32 	%r13, [ggml_matvec_f32_ncols_7_bs_256_param_7];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_7_bs_256_param_8];
	ld.param.u32 	%r17, [ggml_matvec_f32_ncols_7_bs_256_param_9];
	ld.param.u32 	%r18, [ggml_matvec_f32_ncols_7_bs_256_param_10];
	ld.param.u32 	%r14, [ggml_matvec_f32_ncols_7_bs_256_param_11];
	cvta.to.global.u64 	%rd70, %rd22;
	add.u64 	%rd2, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r19, %r1, %r16;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r20, %r2, %r15;
	mad.lo.s32 	%r21, %r19, %r17, %r20;
	cvt.s64.s32 	%rd3, %r21;
	cvta.to.global.u64 	%rd4, %rd21;
	mul.lo.s32 	%r22, %r1, %r18;
	cvt.s64.s32 	%rd5, %r22;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r23, %r3, 2;
	mov.u32 	%r24, data_mmv;
	add.s32 	%r4, %r24, %r23;
	@%p1 bra 	$L__BB62_2;

	mov.u32 	%r25, 0;
	st.shared.u32 	[%r4], %r25;

$L__BB62_2:
	bar.sync 	0;
	mov.f32 	%f336, 0f00000000;
	mov.u32 	%r26, 0;
	st.local.u32 	[%rd2], %r26;
	st.local.u32 	[%rd2+4], %r26;
	st.local.u32 	[%rd2+8], %r26;
	st.local.u32 	[%rd2+12], %r26;
	st.local.u32 	[%rd2+16], %r26;
	st.local.u32 	[%rd2+20], %r26;
	st.local.u32 	[%rd2+24], %r26;
	setp.ge.s32 	%p2, %r3, %r11;
	mov.f32 	%f337, %f336;
	mov.f32 	%f338, %f336;
	mov.f32 	%f339, %f336;
	mov.f32 	%f340, %f336;
	mov.f32 	%f341, %f336;
	mov.f32 	%f342, %f336;
	@%p2 bra 	$L__BB62_9;

	not.b32 	%r27, %r3;
	add.s32 	%r5, %r27, %r11;
	and.b32  	%r28, %r5, 256;
	setp.ne.s32 	%p3, %r28, 0;
	mov.f32 	%f336, 0f00000000;
	mov.u32 	%r288, %r3;
	@%p3 bra 	$L__BB62_5;

	shl.b64 	%rd24, %rd5, 2;
	add.s64 	%rd25, %rd70, %rd24;
	shl.b64 	%rd26, %rd3, 2;
	add.s64 	%rd27, %rd4, %rd26;
	mul.wide.s32 	%rd28, %r3, 8;
	add.s64 	%rd29, %rd27, %rd28;
	ld.global.nc.v2.f32 	{%f50, %f51}, [%rd29];
	add.s64 	%rd30, %rd25, %rd28;
	ld.global.nc.v2.f32 	{%f54, %f55}, [%rd30];
	fma.rn.f32 	%f58, %f50, %f54, 0f00000000;
	fma.rn.f32 	%f342, %f51, %f55, %f58;
	st.local.f32 	[%rd2], %f342;
	mul.wide.s32 	%rd31, %r12, 8;
	add.s64 	%rd32, %rd30, %rd31;
	ld.global.nc.v2.f32 	{%f59, %f60}, [%rd32];
	fma.rn.f32 	%f63, %f50, %f59, 0f00000000;
	fma.rn.f32 	%f341, %f51, %f60, %f63;
	st.local.f32 	[%rd2+4], %f341;
	add.s32 	%r29, %r3, %r12;
	add.s32 	%r30, %r29, %r12;
	mul.wide.s32 	%rd33, %r30, 8;
	add.s64 	%rd34, %rd25, %rd33;
	ld.global.nc.v2.f32 	{%f64, %f65}, [%rd34];
	fma.rn.f32 	%f68, %f50, %f64, 0f00000000;
	fma.rn.f32 	%f340, %f51, %f65, %f68;
	st.local.f32 	[%rd2+8], %f340;
	add.s64 	%rd35, %rd34, %rd31;
	ld.global.nc.v2.f32 	{%f69, %f70}, [%rd35];
	fma.rn.f32 	%f73, %f50, %f69, 0f00000000;
	fma.rn.f32 	%f339, %f51, %f70, %f73;
	st.local.f32 	[%rd2+12], %f339;
	add.s64 	%rd36, %rd35, %rd31;
	ld.global.nc.v2.f32 	{%f74, %f75}, [%rd36];
	fma.rn.f32 	%f78, %f50, %f74, 0f00000000;
	fma.rn.f32 	%f338, %f51, %f75, %f78;
	st.local.f32 	[%rd2+16], %f338;
	add.s64 	%rd37, %rd36, %rd31;
	ld.global.nc.v2.f32 	{%f79, %f80}, [%rd37];
	fma.rn.f32 	%f83, %f50, %f79, 0f00000000;
	fma.rn.f32 	%f337, %f51, %f80, %f83;
	st.local.f32 	[%rd2+20], %f337;
	add.s64 	%rd38, %rd37, %rd31;
	ld.global.nc.v2.f32 	{%f84, %f85}, [%rd38];
	fma.rn.f32 	%f88, %f50, %f84, 0f00000000;
	fma.rn.f32 	%f336, %f51, %f85, %f88;
	st.local.f32 	[%rd2+24], %f336;
	add.s32 	%r288, %r3, 256;

$L__BB62_5:
	and.b32  	%r31, %r5, -256;
	setp.eq.s32 	%p4, %r31, 0;
	@%p4 bra 	$L__BB62_9;

	add.s32 	%r32, %r288, %r12;
	add.s32 	%r33, %r32, 256;
	mul.wide.s32 	%rd39, %r33, 8;
	shl.b64 	%rd40, %rd5, 2;
	add.s64 	%rd7, %rd39, %rd40;
	shl.b32 	%r34, %r12, 1;
	add.s32 	%r35, %r288, %r34;
	mad.lo.s32 	%r36, %r12, 3, %r288;
	shl.b32 	%r37, %r12, 2;
	add.s32 	%r38, %r288, %r37;
	mad.lo.s32 	%r39, %r12, 5, %r288;
	mad.lo.s32 	%r40, %r12, 6, %r288;
	mul.wide.s32 	%rd41, %r35, 8;
	add.s64 	%rd8, %rd41, %rd40;
	mul.wide.s32 	%rd42, %r36, 8;
	add.s64 	%rd9, %rd42, %rd40;
	mul.wide.s32 	%rd43, %r38, 8;
	add.s64 	%rd10, %rd43, %rd40;
	mul.wide.s32 	%rd44, %r39, 8;
	add.s64 	%rd11, %rd44, %rd40;
	mul.wide.s32 	%rd45, %r40, 8;
	add.s64 	%rd12, %rd45, %rd40;
	mul.wide.s32 	%rd46, %r288, 2;
	add.s64 	%rd47, %rd46, %rd3;
	shl.b64 	%rd48, %rd47, 2;
	add.s64 	%rd49, %rd4, %rd48;
	add.s64 	%rd69, %rd49, 2048;
	mul.wide.s32 	%rd50, %r288, 8;
	mul.wide.s32 	%rd51, %r12, 8;
	add.s64 	%rd52, %rd50, %rd51;
	add.s64 	%rd14, %rd52, %rd40;
	add.s64 	%rd15, %rd50, %rd40;

$L__BB62_7:
	ld.global.nc.v2.f32 	{%f89, %f90}, [%rd69+-2048];
	add.s64 	%rd53, %rd70, %rd15;
	ld.global.nc.v2.f32 	{%f93, %f94}, [%rd53];
	fma.rn.f32 	%f97, %f89, %f93, %f342;
	fma.rn.f32 	%f98, %f90, %f94, %f97;
	add.s64 	%rd54, %rd70, %rd14;
	ld.global.nc.v2.f32 	{%f99, %f100}, [%rd54];
	fma.rn.f32 	%f103, %f89, %f99, %f341;
	fma.rn.f32 	%f104, %f90, %f100, %f103;
	add.s64 	%rd55, %rd70, %rd8;
	ld.global.nc.v2.f32 	{%f105, %f106}, [%rd55];
	fma.rn.f32 	%f109, %f89, %f105, %f340;
	fma.rn.f32 	%f110, %f90, %f106, %f109;
	add.s64 	%rd56, %rd70, %rd9;
	ld.global.nc.v2.f32 	{%f111, %f112}, [%rd56];
	fma.rn.f32 	%f115, %f89, %f111, %f339;
	fma.rn.f32 	%f116, %f90, %f112, %f115;
	add.s64 	%rd57, %rd70, %rd10;
	ld.global.nc.v2.f32 	{%f117, %f118}, [%rd57];
	fma.rn.f32 	%f121, %f89, %f117, %f338;
	fma.rn.f32 	%f122, %f90, %f118, %f121;
	add.s64 	%rd58, %rd70, %rd11;
	ld.global.nc.v2.f32 	{%f123, %f124}, [%rd58];
	fma.rn.f32 	%f127, %f89, %f123, %f337;
	fma.rn.f32 	%f128, %f90, %f124, %f127;
	add.s64 	%rd59, %rd70, %rd12;
	ld.global.nc.v2.f32 	{%f129, %f130}, [%rd59];
	fma.rn.f32 	%f133, %f89, %f129, %f336;
	fma.rn.f32 	%f134, %f90, %f130, %f133;
	ld.global.nc.v2.f32 	{%f135, %f136}, [%rd69];
	ld.global.nc.v2.f32 	{%f139, %f140}, [%rd53+2048];
	fma.rn.f32 	%f143, %f135, %f139, %f98;
	fma.rn.f32 	%f342, %f136, %f140, %f143;
	add.s64 	%rd60, %rd70, %rd7;
	ld.global.nc.v2.f32 	{%f144, %f145}, [%rd60];
	fma.rn.f32 	%f148, %f135, %f144, %f104;
	fma.rn.f32 	%f341, %f136, %f145, %f148;
	ld.global.nc.v2.f32 	{%f149, %f150}, [%rd55+2048];
	fma.rn.f32 	%f153, %f135, %f149, %f110;
	fma.rn.f32 	%f340, %f136, %f150, %f153;
	ld.global.nc.v2.f32 	{%f154, %f155}, [%rd56+2048];
	fma.rn.f32 	%f158, %f135, %f154, %f116;
	fma.rn.f32 	%f339, %f136, %f155, %f158;
	ld.global.nc.v2.f32 	{%f159, %f160}, [%rd57+2048];
	fma.rn.f32 	%f163, %f135, %f159, %f122;
	fma.rn.f32 	%f338, %f136, %f160, %f163;
	ld.global.nc.v2.f32 	{%f164, %f165}, [%rd58+2048];
	fma.rn.f32 	%f168, %f135, %f164, %f128;
	fma.rn.f32 	%f337, %f136, %f165, %f168;
	ld.global.nc.v2.f32 	{%f169, %f170}, [%rd59+2048];
	fma.rn.f32 	%f173, %f135, %f169, %f134;
	fma.rn.f32 	%f336, %f136, %f170, %f173;
	add.s64 	%rd70, %rd70, 4096;
	add.s64 	%rd69, %rd69, 4096;
	add.s32 	%r288, %r288, 512;
	setp.lt.s32 	%p5, %r288, %r11;
	@%p5 bra 	$L__BB62_7;

	st.local.f32 	[%rd2], %f342;
	st.local.f32 	[%rd2+4], %f341;
	st.local.f32 	[%rd2+8], %f340;
	st.local.f32 	[%rd2+12], %f339;
	st.local.f32 	[%rd2+16], %f338;
	st.local.f32 	[%rd2+20], %f337;
	st.local.f32 	[%rd2+24], %f336;

$L__BB62_9:
	shr.s32 	%r41, %r3, 31;
	shr.u32 	%r42, %r41, 27;
	add.s32 	%r43, %r3, %r42;
	shr.s32 	%r44, %r43, 5;
	shl.b32 	%r45, %r44, 2;
	add.s32 	%r10, %r24, %r45;
	mov.u32 	%r47, 2;
	mov.b32 	%r48, %f342;
	mov.u32 	%r49, 31;
	mov.u32 	%r50, 16;
	mov.u32 	%r51, -1;
	shfl.sync.bfly.b32 	%r52|%p6, %r48, %r50, %r49, %r51;
	mov.b32 	%f174, %r52;
	add.f32 	%f175, %f342, %f174;
	mov.b32 	%r53, %f175;
	mov.u32 	%r54, 8;
	shfl.sync.bfly.b32 	%r55|%p7, %r53, %r54, %r49, %r51;
	mov.b32 	%f176, %r55;
	add.f32 	%f177, %f175, %f176;
	mov.b32 	%r56, %f177;
	mov.u32 	%r57, 4;
	shfl.sync.bfly.b32 	%r58|%p8, %r56, %r57, %r49, %r51;
	mov.b32 	%f178, %r58;
	add.f32 	%f179, %f177, %f178;
	mov.b32 	%r59, %f179;
	shfl.sync.bfly.b32 	%r60|%p9, %r59, %r47, %r49, %r51;
	mov.b32 	%f180, %r60;
	add.f32 	%f181, %f179, %f180;
	mov.b32 	%r61, %f181;
	mov.u32 	%r62, 1;
	shfl.sync.bfly.b32 	%r63|%p10, %r61, %r62, %r49, %r51;
	mov.b32 	%f182, %r63;
	add.f32 	%f183, %f181, %f182;
	st.local.f32 	[%rd2], %f183;
	st.shared.f32 	[%r10], %f183;
	bar.sync 	0;
	@%p1 bra 	$L__BB62_11;

	ld.shared.f32 	%f184, [%r4];
	mov.b32 	%r64, %f184;
	shfl.sync.bfly.b32 	%r68|%p12, %r64, %r50, %r49, %r51;
	mov.b32 	%f185, %r68;
	add.f32 	%f186, %f184, %f185;
	mov.b32 	%r69, %f186;
	shfl.sync.bfly.b32 	%r71|%p13, %r69, %r54, %r49, %r51;
	mov.b32 	%f187, %r71;
	add.f32 	%f188, %f186, %f187;
	mov.b32 	%r72, %f188;
	shfl.sync.bfly.b32 	%r74|%p14, %r72, %r57, %r49, %r51;
	mov.b32 	%f189, %r74;
	add.f32 	%f190, %f188, %f189;
	mov.b32 	%r75, %f190;
	shfl.sync.bfly.b32 	%r77|%p15, %r75, %r47, %r49, %r51;
	mov.b32 	%f191, %r77;
	add.f32 	%f192, %f190, %f191;
	mov.b32 	%r78, %f192;
	shfl.sync.bfly.b32 	%r80|%p16, %r78, %r62, %r49, %r51;
	mov.b32 	%f193, %r80;
	add.f32 	%f194, %f192, %f193;
	st.local.f32 	[%rd2], %f194;

$L__BB62_11:
	bar.sync 	0;
	mov.b32 	%r81, %f341;
	shfl.sync.bfly.b32 	%r85|%p18, %r81, %r50, %r49, %r51;
	mov.b32 	%f195, %r85;
	add.f32 	%f196, %f341, %f195;
	mov.b32 	%r86, %f196;
	shfl.sync.bfly.b32 	%r88|%p19, %r86, %r54, %r49, %r51;
	mov.b32 	%f197, %r88;
	add.f32 	%f198, %f196, %f197;
	mov.b32 	%r89, %f198;
	shfl.sync.bfly.b32 	%r91|%p20, %r89, %r57, %r49, %r51;
	mov.b32 	%f199, %r91;
	add.f32 	%f200, %f198, %f199;
	mov.b32 	%r92, %f200;
	shfl.sync.bfly.b32 	%r94|%p21, %r92, %r47, %r49, %r51;
	mov.b32 	%f201, %r94;
	add.f32 	%f202, %f200, %f201;
	mov.b32 	%r95, %f202;
	shfl.sync.bfly.b32 	%r97|%p22, %r95, %r62, %r49, %r51;
	mov.b32 	%f203, %r97;
	add.f32 	%f204, %f202, %f203;
	st.local.f32 	[%rd2+4], %f204;
	st.shared.f32 	[%r10], %f204;
	bar.sync 	0;
	@%p1 bra 	$L__BB62_13;

	ld.shared.f32 	%f205, [%r4];
	mov.b32 	%r98, %f205;
	mov.u32 	%r99, 31;
	mov.u32 	%r100, 16;
	mov.u32 	%r101, -1;
	shfl.sync.bfly.b32 	%r102|%p23, %r98, %r100, %r99, %r101;
	mov.b32 	%f206, %r102;
	add.f32 	%f207, %f205, %f206;
	mov.b32 	%r103, %f207;
	mov.u32 	%r104, 8;
	shfl.sync.bfly.b32 	%r105|%p24, %r103, %r104, %r99, %r101;
	mov.b32 	%f208, %r105;
	add.f32 	%f209, %f207, %f208;
	mov.b32 	%r106, %f209;
	mov.u32 	%r107, 4;
	shfl.sync.bfly.b32 	%r108|%p25, %r106, %r107, %r99, %r101;
	mov.b32 	%f210, %r108;
	add.f32 	%f211, %f209, %f210;
	mov.b32 	%r109, %f211;
	mov.u32 	%r110, 2;
	shfl.sync.bfly.b32 	%r111|%p26, %r109, %r110, %r99, %r101;
	mov.b32 	%f212, %r111;
	add.f32 	%f213, %f211, %f212;
	mov.b32 	%r112, %f213;
	mov.u32 	%r113, 1;
	shfl.sync.bfly.b32 	%r114|%p27, %r112, %r113, %r99, %r101;
	mov.b32 	%f214, %r114;
	add.f32 	%f215, %f213, %f214;
	st.local.f32 	[%rd2+4], %f215;

$L__BB62_13:
	bar.sync 	0;
	mov.b32 	%r115, %f340;
	mov.u32 	%r116, 31;
	mov.u32 	%r117, 16;
	mov.u32 	%r118, -1;
	shfl.sync.bfly.b32 	%r119|%p29, %r115, %r117, %r116, %r118;
	mov.b32 	%f216, %r119;
	add.f32 	%f217, %f340, %f216;
	mov.b32 	%r120, %f217;
	mov.u32 	%r121, 8;
	shfl.sync.bfly.b32 	%r122|%p30, %r120, %r121, %r116, %r118;
	mov.b32 	%f218, %r122;
	add.f32 	%f219, %f217, %f218;
	mov.b32 	%r123, %f219;
	mov.u32 	%r124, 4;
	shfl.sync.bfly.b32 	%r125|%p31, %r123, %r124, %r116, %r118;
	mov.b32 	%f220, %r125;
	add.f32 	%f221, %f219, %f220;
	mov.b32 	%r126, %f221;
	mov.u32 	%r127, 2;
	shfl.sync.bfly.b32 	%r128|%p32, %r126, %r127, %r116, %r118;
	mov.b32 	%f222, %r128;
	add.f32 	%f223, %f221, %f222;
	mov.b32 	%r129, %f223;
	mov.u32 	%r130, 1;
	shfl.sync.bfly.b32 	%r131|%p33, %r129, %r130, %r116, %r118;
	mov.b32 	%f224, %r131;
	add.f32 	%f225, %f223, %f224;
	st.local.f32 	[%rd2+8], %f225;
	st.shared.f32 	[%r10], %f225;
	bar.sync 	0;
	@%p1 bra 	$L__BB62_15;

	ld.shared.f32 	%f226, [%r4];
	mov.b32 	%r132, %f226;
	shfl.sync.bfly.b32 	%r136|%p34, %r132, %r117, %r116, %r118;
	mov.b32 	%f227, %r136;
	add.f32 	%f228, %f226, %f227;
	mov.b32 	%r137, %f228;
	shfl.sync.bfly.b32 	%r139|%p35, %r137, %r121, %r116, %r118;
	mov.b32 	%f229, %r139;
	add.f32 	%f230, %f228, %f229;
	mov.b32 	%r140, %f230;
	shfl.sync.bfly.b32 	%r142|%p36, %r140, %r124, %r116, %r118;
	mov.b32 	%f231, %r142;
	add.f32 	%f232, %f230, %f231;
	mov.b32 	%r143, %f232;
	shfl.sync.bfly.b32 	%r145|%p37, %r143, %r127, %r116, %r118;
	mov.b32 	%f233, %r145;
	add.f32 	%f234, %f232, %f233;
	mov.b32 	%r146, %f234;
	shfl.sync.bfly.b32 	%r148|%p38, %r146, %r130, %r116, %r118;
	mov.b32 	%f235, %r148;
	add.f32 	%f236, %f234, %f235;
	st.local.f32 	[%rd2+8], %f236;

$L__BB62_15:
	bar.sync 	0;
	mov.b32 	%r149, %f339;
	shfl.sync.bfly.b32 	%r153|%p40, %r149, %r117, %r116, %r118;
	mov.b32 	%f237, %r153;
	add.f32 	%f238, %f339, %f237;
	mov.b32 	%r154, %f238;
	shfl.sync.bfly.b32 	%r156|%p41, %r154, %r121, %r116, %r118;
	mov.b32 	%f239, %r156;
	add.f32 	%f240, %f238, %f239;
	mov.b32 	%r157, %f240;
	shfl.sync.bfly.b32 	%r159|%p42, %r157, %r124, %r116, %r118;
	mov.b32 	%f241, %r159;
	add.f32 	%f242, %f240, %f241;
	mov.b32 	%r160, %f242;
	shfl.sync.bfly.b32 	%r162|%p43, %r160, %r127, %r116, %r118;
	mov.b32 	%f243, %r162;
	add.f32 	%f244, %f242, %f243;
	mov.b32 	%r163, %f244;
	shfl.sync.bfly.b32 	%r165|%p44, %r163, %r130, %r116, %r118;
	mov.b32 	%f245, %r165;
	add.f32 	%f246, %f244, %f245;
	st.local.f32 	[%rd2+12], %f246;
	st.shared.f32 	[%r10], %f246;
	bar.sync 	0;
	@%p1 bra 	$L__BB62_17;

	ld.shared.f32 	%f247, [%r4];
	mov.b32 	%r166, %f247;
	mov.u32 	%r167, 31;
	mov.u32 	%r168, 16;
	mov.u32 	%r169, -1;
	shfl.sync.bfly.b32 	%r170|%p45, %r166, %r168, %r167, %r169;
	mov.b32 	%f248, %r170;
	add.f32 	%f249, %f247, %f248;
	mov.b32 	%r171, %f249;
	mov.u32 	%r172, 8;
	shfl.sync.bfly.b32 	%r173|%p46, %r171, %r172, %r167, %r169;
	mov.b32 	%f250, %r173;
	add.f32 	%f251, %f249, %f250;
	mov.b32 	%r174, %f251;
	mov.u32 	%r175, 4;
	shfl.sync.bfly.b32 	%r176|%p47, %r174, %r175, %r167, %r169;
	mov.b32 	%f252, %r176;
	add.f32 	%f253, %f251, %f252;
	mov.b32 	%r177, %f253;
	mov.u32 	%r178, 2;
	shfl.sync.bfly.b32 	%r179|%p48, %r177, %r178, %r167, %r169;
	mov.b32 	%f254, %r179;
	add.f32 	%f255, %f253, %f254;
	mov.b32 	%r180, %f255;
	mov.u32 	%r181, 1;
	shfl.sync.bfly.b32 	%r182|%p49, %r180, %r181, %r167, %r169;
	mov.b32 	%f256, %r182;
	add.f32 	%f257, %f255, %f256;
	st.local.f32 	[%rd2+12], %f257;

$L__BB62_17:
	bar.sync 	0;
	mov.b32 	%r183, %f338;
	mov.u32 	%r184, 31;
	mov.u32 	%r185, 16;
	mov.u32 	%r186, -1;
	shfl.sync.bfly.b32 	%r187|%p51, %r183, %r185, %r184, %r186;
	mov.b32 	%f258, %r187;
	add.f32 	%f259, %f338, %f258;
	mov.b32 	%r188, %f259;
	mov.u32 	%r189, 8;
	shfl.sync.bfly.b32 	%r190|%p52, %r188, %r189, %r184, %r186;
	mov.b32 	%f260, %r190;
	add.f32 	%f261, %f259, %f260;
	mov.b32 	%r191, %f261;
	mov.u32 	%r192, 4;
	shfl.sync.bfly.b32 	%r193|%p53, %r191, %r192, %r184, %r186;
	mov.b32 	%f262, %r193;
	add.f32 	%f263, %f261, %f262;
	mov.b32 	%r194, %f263;
	mov.u32 	%r195, 2;
	shfl.sync.bfly.b32 	%r196|%p54, %r194, %r195, %r184, %r186;
	mov.b32 	%f264, %r196;
	add.f32 	%f265, %f263, %f264;
	mov.b32 	%r197, %f265;
	mov.u32 	%r198, 1;
	shfl.sync.bfly.b32 	%r199|%p55, %r197, %r198, %r184, %r186;
	mov.b32 	%f266, %r199;
	add.f32 	%f267, %f265, %f266;
	st.local.f32 	[%rd2+16], %f267;
	st.shared.f32 	[%r10], %f267;
	bar.sync 	0;
	@%p1 bra 	$L__BB62_19;

	ld.shared.f32 	%f268, [%r4];
	mov.b32 	%r200, %f268;
	shfl.sync.bfly.b32 	%r204|%p56, %r200, %r185, %r184, %r186;
	mov.b32 	%f269, %r204;
	add.f32 	%f270, %f268, %f269;
	mov.b32 	%r205, %f270;
	shfl.sync.bfly.b32 	%r207|%p57, %r205, %r189, %r184, %r186;
	mov.b32 	%f271, %r207;
	add.f32 	%f272, %f270, %f271;
	mov.b32 	%r208, %f272;
	shfl.sync.bfly.b32 	%r210|%p58, %r208, %r192, %r184, %r186;
	mov.b32 	%f273, %r210;
	add.f32 	%f274, %f272, %f273;
	mov.b32 	%r211, %f274;
	shfl.sync.bfly.b32 	%r213|%p59, %r211, %r195, %r184, %r186;
	mov.b32 	%f275, %r213;
	add.f32 	%f276, %f274, %f275;
	mov.b32 	%r214, %f276;
	shfl.sync.bfly.b32 	%r216|%p60, %r214, %r198, %r184, %r186;
	mov.b32 	%f277, %r216;
	add.f32 	%f278, %f276, %f277;
	st.local.f32 	[%rd2+16], %f278;

$L__BB62_19:
	bar.sync 	0;
	mov.b32 	%r217, %f337;
	shfl.sync.bfly.b32 	%r221|%p62, %r217, %r185, %r184, %r186;
	mov.b32 	%f279, %r221;
	add.f32 	%f280, %f337, %f279;
	mov.b32 	%r222, %f280;
	shfl.sync.bfly.b32 	%r224|%p63, %r222, %r189, %r184, %r186;
	mov.b32 	%f281, %r224;
	add.f32 	%f282, %f280, %f281;
	mov.b32 	%r225, %f282;
	shfl.sync.bfly.b32 	%r227|%p64, %r225, %r192, %r184, %r186;
	mov.b32 	%f283, %r227;
	add.f32 	%f284, %f282, %f283;
	mov.b32 	%r228, %f284;
	shfl.sync.bfly.b32 	%r230|%p65, %r228, %r195, %r184, %r186;
	mov.b32 	%f285, %r230;
	add.f32 	%f286, %f284, %f285;
	mov.b32 	%r231, %f286;
	shfl.sync.bfly.b32 	%r233|%p66, %r231, %r198, %r184, %r186;
	mov.b32 	%f287, %r233;
	add.f32 	%f288, %f286, %f287;
	st.local.f32 	[%rd2+20], %f288;
	st.shared.f32 	[%r10], %f288;
	bar.sync 	0;
	@%p1 bra 	$L__BB62_21;

	ld.shared.f32 	%f289, [%r4];
	mov.b32 	%r234, %f289;
	mov.u32 	%r235, 31;
	mov.u32 	%r236, 16;
	mov.u32 	%r237, -1;
	shfl.sync.bfly.b32 	%r238|%p67, %r234, %r236, %r235, %r237;
	mov.b32 	%f290, %r238;
	add.f32 	%f291, %f289, %f290;
	mov.b32 	%r239, %f291;
	mov.u32 	%r240, 8;
	shfl.sync.bfly.b32 	%r241|%p68, %r239, %r240, %r235, %r237;
	mov.b32 	%f292, %r241;
	add.f32 	%f293, %f291, %f292;
	mov.b32 	%r242, %f293;
	mov.u32 	%r243, 4;
	shfl.sync.bfly.b32 	%r244|%p69, %r242, %r243, %r235, %r237;
	mov.b32 	%f294, %r244;
	add.f32 	%f295, %f293, %f294;
	mov.b32 	%r245, %f295;
	mov.u32 	%r246, 2;
	shfl.sync.bfly.b32 	%r247|%p70, %r245, %r246, %r235, %r237;
	mov.b32 	%f296, %r247;
	add.f32 	%f297, %f295, %f296;
	mov.b32 	%r248, %f297;
	mov.u32 	%r249, 1;
	shfl.sync.bfly.b32 	%r250|%p71, %r248, %r249, %r235, %r237;
	mov.b32 	%f298, %r250;
	add.f32 	%f299, %f297, %f298;
	st.local.f32 	[%rd2+20], %f299;

$L__BB62_21:
	bar.sync 	0;
	mov.b32 	%r251, %f336;
	mov.u32 	%r252, 31;
	mov.u32 	%r253, 16;
	mov.u32 	%r254, -1;
	shfl.sync.bfly.b32 	%r255|%p73, %r251, %r253, %r252, %r254;
	mov.b32 	%f300, %r255;
	add.f32 	%f301, %f336, %f300;
	mov.b32 	%r256, %f301;
	mov.u32 	%r257, 8;
	shfl.sync.bfly.b32 	%r258|%p74, %r256, %r257, %r252, %r254;
	mov.b32 	%f302, %r258;
	add.f32 	%f303, %f301, %f302;
	mov.b32 	%r259, %f303;
	mov.u32 	%r260, 4;
	shfl.sync.bfly.b32 	%r261|%p75, %r259, %r260, %r252, %r254;
	mov.b32 	%f304, %r261;
	add.f32 	%f305, %f303, %f304;
	mov.b32 	%r262, %f305;
	mov.u32 	%r263, 2;
	shfl.sync.bfly.b32 	%r264|%p76, %r262, %r263, %r252, %r254;
	mov.b32 	%f306, %r264;
	add.f32 	%f307, %f305, %f306;
	mov.b32 	%r265, %f307;
	mov.u32 	%r266, 1;
	shfl.sync.bfly.b32 	%r267|%p77, %r265, %r266, %r252, %r254;
	mov.b32 	%f308, %r267;
	add.f32 	%f309, %f307, %f308;
	st.local.f32 	[%rd2+24], %f309;
	st.shared.f32 	[%r10], %f309;
	bar.sync 	0;
	@%p1 bra 	$L__BB62_23;

	ld.shared.f32 	%f310, [%r4];
	mov.b32 	%r268, %f310;
	shfl.sync.bfly.b32 	%r272|%p78, %r268, %r253, %r252, %r254;
	mov.b32 	%f311, %r272;
	add.f32 	%f312, %f310, %f311;
	mov.b32 	%r273, %f312;
	shfl.sync.bfly.b32 	%r275|%p79, %r273, %r257, %r252, %r254;
	mov.b32 	%f313, %r275;
	add.f32 	%f314, %f312, %f313;
	mov.b32 	%r276, %f314;
	shfl.sync.bfly.b32 	%r278|%p80, %r276, %r260, %r252, %r254;
	mov.b32 	%f315, %r278;
	add.f32 	%f316, %f314, %f315;
	mov.b32 	%r279, %f316;
	shfl.sync.bfly.b32 	%r281|%p81, %r279, %r263, %r252, %r254;
	mov.b32 	%f317, %r281;
	add.f32 	%f318, %f316, %f317;
	mov.b32 	%r282, %f318;
	shfl.sync.bfly.b32 	%r284|%p82, %r282, %r266, %r252, %r254;
	mov.b32 	%f319, %r284;
	add.f32 	%f320, %f318, %f319;
	st.local.f32 	[%rd2+24], %f320;

$L__BB62_23:
	bar.sync 	0;
	setp.gt.s32 	%p83, %r3, 6;
	@%p83 bra 	$L__BB62_25;

	mul.wide.s32 	%rd61, %r3, 4;
	add.s64 	%rd62, %rd2, %rd61;
	ld.local.f32 	%f321, [%rd62];
	mad.lo.s32 	%r285, %r3, %r13, %r2;
	cvt.s64.s32 	%rd63, %r285;
	mul.lo.s32 	%r286, %r1, %r14;
	cvt.s64.s32 	%rd64, %r286;
	add.s64 	%rd65, %rd64, %rd63;
	cvta.to.global.u64 	%rd66, %rd20;
	shl.b64 	%rd67, %rd65, 2;
	add.s64 	%rd68, %rd66, %rd67;
	st.global.f32 	[%rd68], %f321;

$L__BB62_25:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_8_bs_256
.visible .entry ggml_matvec_f32_ncols_8_bs_256(
	.param .u64 ggml_matvec_f32_ncols_8_bs_256_param_0,
	.param .u64 ggml_matvec_f32_ncols_8_bs_256_param_1,
	.param .u64 ggml_matvec_f32_ncols_8_bs_256_param_2,
	.param .u32 ggml_matvec_f32_ncols_8_bs_256_param_3,
	.param .u32 ggml_matvec_f32_ncols_8_bs_256_param_4,
	.param .u32 ggml_matvec_f32_ncols_8_bs_256_param_5,
	.param .u32 ggml_matvec_f32_ncols_8_bs_256_param_6,
	.param .u32 ggml_matvec_f32_ncols_8_bs_256_param_7,
	.param .u32 ggml_matvec_f32_ncols_8_bs_256_param_8,
	.param .u32 ggml_matvec_f32_ncols_8_bs_256_param_9,
	.param .u32 ggml_matvec_f32_ncols_8_bs_256_param_10,
	.param .u32 ggml_matvec_f32_ncols_8_bs_256_param_11
)
{
	.local .align 16 .b8 	__local_depot63[32];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<95>;
	.reg .f32 	%f<390>;
	.reg .b32 	%r<323>;
	.reg .b64 	%rd<75>;


	mov.u64 	%SPL, __local_depot63;
	ld.param.u64 	%rd22, [ggml_matvec_f32_ncols_8_bs_256_param_0];
	ld.param.u64 	%rd23, [ggml_matvec_f32_ncols_8_bs_256_param_1];
	ld.param.u64 	%rd21, [ggml_matvec_f32_ncols_8_bs_256_param_2];
	ld.param.u32 	%r11, [ggml_matvec_f32_ncols_8_bs_256_param_3];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_8_bs_256_param_5];
	ld.param.u32 	%r12, [ggml_matvec_f32_ncols_8_bs_256_param_6];
	ld.param.u32 	%r13, [ggml_matvec_f32_ncols_8_bs_256_param_7];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_8_bs_256_param_8];
	ld.param.u32 	%r17, [ggml_matvec_f32_ncols_8_bs_256_param_9];
	ld.param.u32 	%r18, [ggml_matvec_f32_ncols_8_bs_256_param_10];
	ld.param.u32 	%r14, [ggml_matvec_f32_ncols_8_bs_256_param_11];
	cvta.to.global.u64 	%rd74, %rd23;
	add.u64 	%rd2, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r19, %r1, %r16;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r20, %r2, %r15;
	mad.lo.s32 	%r21, %r19, %r17, %r20;
	cvt.s64.s32 	%rd3, %r21;
	cvta.to.global.u64 	%rd4, %rd22;
	mul.lo.s32 	%r22, %r1, %r18;
	cvt.s64.s32 	%rd5, %r22;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r23, %r3, 2;
	mov.u32 	%r24, data_mmv;
	add.s32 	%r4, %r24, %r23;
	@%p1 bra 	$L__BB63_2;

	mov.u32 	%r25, 0;
	st.shared.u32 	[%r4], %r25;

$L__BB63_2:
	bar.sync 	0;
	mov.f32 	%f382, 0f00000000;
	st.local.v4.f32 	[%rd2], {%f382, %f382, %f382, %f382};
	st.local.v4.f32 	[%rd2+16], {%f382, %f382, %f382, %f382};
	setp.ge.s32 	%p2, %r3, %r11;
	mov.f32 	%f383, %f382;
	mov.f32 	%f384, %f382;
	mov.f32 	%f385, %f382;
	mov.f32 	%f386, %f382;
	mov.f32 	%f387, %f382;
	mov.f32 	%f388, %f382;
	mov.f32 	%f389, %f382;
	@%p2 bra 	$L__BB63_9;

	not.b32 	%r26, %r3;
	add.s32 	%r5, %r26, %r11;
	and.b32  	%r27, %r5, 256;
	setp.ne.s32 	%p3, %r27, 0;
	mov.f32 	%f382, 0f00000000;
	mov.u32 	%r322, %r3;
	@%p3 bra 	$L__BB63_5;

	shl.b64 	%rd25, %rd5, 2;
	add.s64 	%rd26, %rd74, %rd25;
	shl.b64 	%rd27, %rd3, 2;
	add.s64 	%rd28, %rd4, %rd27;
	mul.wide.s32 	%rd29, %r3, 8;
	add.s64 	%rd30, %rd28, %rd29;
	ld.global.nc.v2.f32 	{%f57, %f58}, [%rd30];
	add.s64 	%rd31, %rd26, %rd29;
	ld.global.nc.v2.f32 	{%f61, %f62}, [%rd31];
	fma.rn.f32 	%f65, %f57, %f61, 0f00000000;
	fma.rn.f32 	%f389, %f58, %f62, %f65;
	mul.wide.s32 	%rd32, %r12, 8;
	add.s64 	%rd33, %rd31, %rd32;
	ld.global.nc.v2.f32 	{%f66, %f67}, [%rd33];
	fma.rn.f32 	%f70, %f57, %f66, 0f00000000;
	fma.rn.f32 	%f388, %f58, %f67, %f70;
	add.s32 	%r28, %r3, %r12;
	add.s32 	%r29, %r28, %r12;
	mul.wide.s32 	%rd34, %r29, 8;
	add.s64 	%rd35, %rd26, %rd34;
	ld.global.nc.v2.f32 	{%f71, %f72}, [%rd35];
	fma.rn.f32 	%f75, %f57, %f71, 0f00000000;
	fma.rn.f32 	%f387, %f58, %f72, %f75;
	add.s64 	%rd36, %rd35, %rd32;
	ld.global.nc.v2.f32 	{%f76, %f77}, [%rd36];
	fma.rn.f32 	%f80, %f57, %f76, 0f00000000;
	fma.rn.f32 	%f386, %f58, %f77, %f80;
	st.local.v4.f32 	[%rd2], {%f389, %f388, %f387, %f386};
	add.s64 	%rd37, %rd36, %rd32;
	ld.global.nc.v2.f32 	{%f81, %f82}, [%rd37];
	fma.rn.f32 	%f85, %f57, %f81, 0f00000000;
	fma.rn.f32 	%f385, %f58, %f82, %f85;
	add.s64 	%rd38, %rd37, %rd32;
	ld.global.nc.v2.f32 	{%f86, %f87}, [%rd38];
	fma.rn.f32 	%f90, %f57, %f86, 0f00000000;
	fma.rn.f32 	%f384, %f58, %f87, %f90;
	add.s64 	%rd39, %rd38, %rd32;
	ld.global.nc.v2.f32 	{%f91, %f92}, [%rd39];
	fma.rn.f32 	%f95, %f57, %f91, 0f00000000;
	fma.rn.f32 	%f383, %f58, %f92, %f95;
	add.s64 	%rd40, %rd39, %rd32;
	ld.global.nc.v2.f32 	{%f96, %f97}, [%rd40];
	fma.rn.f32 	%f100, %f57, %f96, 0f00000000;
	fma.rn.f32 	%f382, %f58, %f97, %f100;
	st.local.v4.f32 	[%rd2+16], {%f385, %f384, %f383, %f382};
	add.s32 	%r322, %r3, 256;

$L__BB63_5:
	and.b32  	%r30, %r5, -256;
	setp.eq.s32 	%p4, %r30, 0;
	@%p4 bra 	$L__BB63_9;

	add.s32 	%r31, %r322, %r12;
	add.s32 	%r32, %r31, 256;
	mul.wide.s32 	%rd41, %r32, 8;
	shl.b64 	%rd42, %rd5, 2;
	add.s64 	%rd7, %rd41, %rd42;
	shl.b32 	%r33, %r12, 1;
	add.s32 	%r34, %r322, %r33;
	mad.lo.s32 	%r35, %r12, 3, %r322;
	shl.b32 	%r36, %r12, 2;
	add.s32 	%r37, %r322, %r36;
	mad.lo.s32 	%r38, %r12, 5, %r322;
	mad.lo.s32 	%r39, %r12, 6, %r322;
	mad.lo.s32 	%r40, %r12, 7, %r322;
	mul.wide.s32 	%rd43, %r34, 8;
	add.s64 	%rd8, %rd43, %rd42;
	mul.wide.s32 	%rd44, %r35, 8;
	add.s64 	%rd9, %rd44, %rd42;
	mul.wide.s32 	%rd45, %r37, 8;
	add.s64 	%rd10, %rd45, %rd42;
	mul.wide.s32 	%rd46, %r38, 8;
	add.s64 	%rd11, %rd46, %rd42;
	mul.wide.s32 	%rd47, %r39, 8;
	add.s64 	%rd12, %rd47, %rd42;
	mul.wide.s32 	%rd48, %r40, 8;
	add.s64 	%rd13, %rd48, %rd42;
	mul.wide.s32 	%rd49, %r322, 2;
	add.s64 	%rd50, %rd49, %rd3;
	shl.b64 	%rd51, %rd50, 2;
	add.s64 	%rd52, %rd4, %rd51;
	add.s64 	%rd73, %rd52, 2048;
	mul.wide.s32 	%rd53, %r322, 8;
	mul.wide.s32 	%rd54, %r12, 8;
	add.s64 	%rd55, %rd53, %rd54;
	add.s64 	%rd15, %rd55, %rd42;
	add.s64 	%rd16, %rd53, %rd42;

$L__BB63_7:
	ld.global.nc.v2.f32 	{%f101, %f102}, [%rd73+-2048];
	add.s64 	%rd56, %rd74, %rd16;
	ld.global.nc.v2.f32 	{%f105, %f106}, [%rd56];
	fma.rn.f32 	%f109, %f101, %f105, %f389;
	fma.rn.f32 	%f110, %f102, %f106, %f109;
	add.s64 	%rd57, %rd74, %rd15;
	ld.global.nc.v2.f32 	{%f111, %f112}, [%rd57];
	fma.rn.f32 	%f115, %f101, %f111, %f388;
	fma.rn.f32 	%f116, %f102, %f112, %f115;
	add.s64 	%rd58, %rd74, %rd8;
	ld.global.nc.v2.f32 	{%f117, %f118}, [%rd58];
	fma.rn.f32 	%f121, %f101, %f117, %f387;
	fma.rn.f32 	%f122, %f102, %f118, %f121;
	add.s64 	%rd59, %rd74, %rd9;
	ld.global.nc.v2.f32 	{%f123, %f124}, [%rd59];
	fma.rn.f32 	%f127, %f101, %f123, %f386;
	fma.rn.f32 	%f128, %f102, %f124, %f127;
	add.s64 	%rd60, %rd74, %rd10;
	ld.global.nc.v2.f32 	{%f129, %f130}, [%rd60];
	fma.rn.f32 	%f133, %f101, %f129, %f385;
	fma.rn.f32 	%f134, %f102, %f130, %f133;
	add.s64 	%rd61, %rd74, %rd11;
	ld.global.nc.v2.f32 	{%f135, %f136}, [%rd61];
	fma.rn.f32 	%f139, %f101, %f135, %f384;
	fma.rn.f32 	%f140, %f102, %f136, %f139;
	add.s64 	%rd62, %rd74, %rd12;
	ld.global.nc.v2.f32 	{%f141, %f142}, [%rd62];
	fma.rn.f32 	%f145, %f101, %f141, %f383;
	fma.rn.f32 	%f146, %f102, %f142, %f145;
	add.s64 	%rd63, %rd74, %rd13;
	ld.global.nc.v2.f32 	{%f147, %f148}, [%rd63];
	fma.rn.f32 	%f151, %f101, %f147, %f382;
	fma.rn.f32 	%f152, %f102, %f148, %f151;
	ld.global.nc.v2.f32 	{%f153, %f154}, [%rd73];
	ld.global.nc.v2.f32 	{%f157, %f158}, [%rd56+2048];
	fma.rn.f32 	%f161, %f153, %f157, %f110;
	fma.rn.f32 	%f389, %f154, %f158, %f161;
	add.s64 	%rd64, %rd74, %rd7;
	ld.global.nc.v2.f32 	{%f162, %f163}, [%rd64];
	fma.rn.f32 	%f166, %f153, %f162, %f116;
	fma.rn.f32 	%f388, %f154, %f163, %f166;
	ld.global.nc.v2.f32 	{%f167, %f168}, [%rd58+2048];
	fma.rn.f32 	%f171, %f153, %f167, %f122;
	fma.rn.f32 	%f387, %f154, %f168, %f171;
	ld.global.nc.v2.f32 	{%f172, %f173}, [%rd59+2048];
	fma.rn.f32 	%f176, %f153, %f172, %f128;
	fma.rn.f32 	%f386, %f154, %f173, %f176;
	ld.global.nc.v2.f32 	{%f177, %f178}, [%rd60+2048];
	fma.rn.f32 	%f181, %f153, %f177, %f134;
	fma.rn.f32 	%f385, %f154, %f178, %f181;
	ld.global.nc.v2.f32 	{%f182, %f183}, [%rd61+2048];
	fma.rn.f32 	%f186, %f153, %f182, %f140;
	fma.rn.f32 	%f384, %f154, %f183, %f186;
	ld.global.nc.v2.f32 	{%f187, %f188}, [%rd62+2048];
	fma.rn.f32 	%f191, %f153, %f187, %f146;
	fma.rn.f32 	%f383, %f154, %f188, %f191;
	ld.global.nc.v2.f32 	{%f192, %f193}, [%rd63+2048];
	fma.rn.f32 	%f196, %f153, %f192, %f152;
	fma.rn.f32 	%f382, %f154, %f193, %f196;
	add.s64 	%rd74, %rd74, 4096;
	add.s64 	%rd73, %rd73, 4096;
	add.s32 	%r322, %r322, 512;
	setp.lt.s32 	%p5, %r322, %r11;
	@%p5 bra 	$L__BB63_7;

	st.local.v4.f32 	[%rd2], {%f389, %f388, %f387, %f386};
	st.local.v4.f32 	[%rd2+16], {%f385, %f384, %f383, %f382};

$L__BB63_9:
	shr.s32 	%r41, %r3, 31;
	shr.u32 	%r42, %r41, 27;
	add.s32 	%r43, %r3, %r42;
	shr.s32 	%r44, %r43, 5;
	shl.b32 	%r45, %r44, 2;
	add.s32 	%r10, %r24, %r45;
	mov.u32 	%r47, 2;
	mov.b32 	%r48, %f389;
	mov.u32 	%r49, 31;
	mov.u32 	%r50, 16;
	mov.u32 	%r51, -1;
	shfl.sync.bfly.b32 	%r52|%p6, %r48, %r50, %r49, %r51;
	mov.b32 	%f197, %r52;
	add.f32 	%f198, %f389, %f197;
	mov.b32 	%r53, %f198;
	mov.u32 	%r54, 8;
	shfl.sync.bfly.b32 	%r55|%p7, %r53, %r54, %r49, %r51;
	mov.b32 	%f199, %r55;
	add.f32 	%f200, %f198, %f199;
	mov.b32 	%r56, %f200;
	mov.u32 	%r57, 4;
	shfl.sync.bfly.b32 	%r58|%p8, %r56, %r57, %r49, %r51;
	mov.b32 	%f201, %r58;
	add.f32 	%f202, %f200, %f201;
	mov.b32 	%r59, %f202;
	shfl.sync.bfly.b32 	%r60|%p9, %r59, %r47, %r49, %r51;
	mov.b32 	%f203, %r60;
	add.f32 	%f204, %f202, %f203;
	mov.b32 	%r61, %f204;
	mov.u32 	%r62, 1;
	shfl.sync.bfly.b32 	%r63|%p10, %r61, %r62, %r49, %r51;
	mov.b32 	%f205, %r63;
	add.f32 	%f206, %f204, %f205;
	st.local.f32 	[%rd2], %f206;
	st.shared.f32 	[%r10], %f206;
	bar.sync 	0;
	@%p1 bra 	$L__BB63_11;

	ld.shared.f32 	%f207, [%r4];
	mov.b32 	%r64, %f207;
	shfl.sync.bfly.b32 	%r68|%p12, %r64, %r50, %r49, %r51;
	mov.b32 	%f208, %r68;
	add.f32 	%f209, %f207, %f208;
	mov.b32 	%r69, %f209;
	shfl.sync.bfly.b32 	%r71|%p13, %r69, %r54, %r49, %r51;
	mov.b32 	%f210, %r71;
	add.f32 	%f211, %f209, %f210;
	mov.b32 	%r72, %f211;
	shfl.sync.bfly.b32 	%r74|%p14, %r72, %r57, %r49, %r51;
	mov.b32 	%f212, %r74;
	add.f32 	%f213, %f211, %f212;
	mov.b32 	%r75, %f213;
	shfl.sync.bfly.b32 	%r77|%p15, %r75, %r47, %r49, %r51;
	mov.b32 	%f214, %r77;
	add.f32 	%f215, %f213, %f214;
	mov.b32 	%r78, %f215;
	shfl.sync.bfly.b32 	%r80|%p16, %r78, %r62, %r49, %r51;
	mov.b32 	%f216, %r80;
	add.f32 	%f217, %f215, %f216;
	st.local.f32 	[%rd2], %f217;

$L__BB63_11:
	bar.sync 	0;
	mov.b32 	%r81, %f388;
	shfl.sync.bfly.b32 	%r85|%p18, %r81, %r50, %r49, %r51;
	mov.b32 	%f218, %r85;
	add.f32 	%f219, %f388, %f218;
	mov.b32 	%r86, %f219;
	shfl.sync.bfly.b32 	%r88|%p19, %r86, %r54, %r49, %r51;
	mov.b32 	%f220, %r88;
	add.f32 	%f221, %f219, %f220;
	mov.b32 	%r89, %f221;
	shfl.sync.bfly.b32 	%r91|%p20, %r89, %r57, %r49, %r51;
	mov.b32 	%f222, %r91;
	add.f32 	%f223, %f221, %f222;
	mov.b32 	%r92, %f223;
	shfl.sync.bfly.b32 	%r94|%p21, %r92, %r47, %r49, %r51;
	mov.b32 	%f224, %r94;
	add.f32 	%f225, %f223, %f224;
	mov.b32 	%r95, %f225;
	shfl.sync.bfly.b32 	%r97|%p22, %r95, %r62, %r49, %r51;
	mov.b32 	%f226, %r97;
	add.f32 	%f227, %f225, %f226;
	st.local.f32 	[%rd2+4], %f227;
	st.shared.f32 	[%r10], %f227;
	bar.sync 	0;
	@%p1 bra 	$L__BB63_13;

	ld.shared.f32 	%f228, [%r4];
	mov.b32 	%r98, %f228;
	mov.u32 	%r99, 31;
	mov.u32 	%r100, 16;
	mov.u32 	%r101, -1;
	shfl.sync.bfly.b32 	%r102|%p23, %r98, %r100, %r99, %r101;
	mov.b32 	%f229, %r102;
	add.f32 	%f230, %f228, %f229;
	mov.b32 	%r103, %f230;
	mov.u32 	%r104, 8;
	shfl.sync.bfly.b32 	%r105|%p24, %r103, %r104, %r99, %r101;
	mov.b32 	%f231, %r105;
	add.f32 	%f232, %f230, %f231;
	mov.b32 	%r106, %f232;
	mov.u32 	%r107, 4;
	shfl.sync.bfly.b32 	%r108|%p25, %r106, %r107, %r99, %r101;
	mov.b32 	%f233, %r108;
	add.f32 	%f234, %f232, %f233;
	mov.b32 	%r109, %f234;
	mov.u32 	%r110, 2;
	shfl.sync.bfly.b32 	%r111|%p26, %r109, %r110, %r99, %r101;
	mov.b32 	%f235, %r111;
	add.f32 	%f236, %f234, %f235;
	mov.b32 	%r112, %f236;
	mov.u32 	%r113, 1;
	shfl.sync.bfly.b32 	%r114|%p27, %r112, %r113, %r99, %r101;
	mov.b32 	%f237, %r114;
	add.f32 	%f238, %f236, %f237;
	st.local.f32 	[%rd2+4], %f238;

$L__BB63_13:
	bar.sync 	0;
	mov.b32 	%r115, %f387;
	mov.u32 	%r116, 31;
	mov.u32 	%r117, 16;
	mov.u32 	%r118, -1;
	shfl.sync.bfly.b32 	%r119|%p29, %r115, %r117, %r116, %r118;
	mov.b32 	%f239, %r119;
	add.f32 	%f240, %f387, %f239;
	mov.b32 	%r120, %f240;
	mov.u32 	%r121, 8;
	shfl.sync.bfly.b32 	%r122|%p30, %r120, %r121, %r116, %r118;
	mov.b32 	%f241, %r122;
	add.f32 	%f242, %f240, %f241;
	mov.b32 	%r123, %f242;
	mov.u32 	%r124, 4;
	shfl.sync.bfly.b32 	%r125|%p31, %r123, %r124, %r116, %r118;
	mov.b32 	%f243, %r125;
	add.f32 	%f244, %f242, %f243;
	mov.b32 	%r126, %f244;
	mov.u32 	%r127, 2;
	shfl.sync.bfly.b32 	%r128|%p32, %r126, %r127, %r116, %r118;
	mov.b32 	%f245, %r128;
	add.f32 	%f246, %f244, %f245;
	mov.b32 	%r129, %f246;
	mov.u32 	%r130, 1;
	shfl.sync.bfly.b32 	%r131|%p33, %r129, %r130, %r116, %r118;
	mov.b32 	%f247, %r131;
	add.f32 	%f248, %f246, %f247;
	st.local.f32 	[%rd2+8], %f248;
	st.shared.f32 	[%r10], %f248;
	bar.sync 	0;
	@%p1 bra 	$L__BB63_15;

	ld.shared.f32 	%f249, [%r4];
	mov.b32 	%r132, %f249;
	shfl.sync.bfly.b32 	%r136|%p34, %r132, %r117, %r116, %r118;
	mov.b32 	%f250, %r136;
	add.f32 	%f251, %f249, %f250;
	mov.b32 	%r137, %f251;
	shfl.sync.bfly.b32 	%r139|%p35, %r137, %r121, %r116, %r118;
	mov.b32 	%f252, %r139;
	add.f32 	%f253, %f251, %f252;
	mov.b32 	%r140, %f253;
	shfl.sync.bfly.b32 	%r142|%p36, %r140, %r124, %r116, %r118;
	mov.b32 	%f254, %r142;
	add.f32 	%f255, %f253, %f254;
	mov.b32 	%r143, %f255;
	shfl.sync.bfly.b32 	%r145|%p37, %r143, %r127, %r116, %r118;
	mov.b32 	%f256, %r145;
	add.f32 	%f257, %f255, %f256;
	mov.b32 	%r146, %f257;
	shfl.sync.bfly.b32 	%r148|%p38, %r146, %r130, %r116, %r118;
	mov.b32 	%f258, %r148;
	add.f32 	%f259, %f257, %f258;
	st.local.f32 	[%rd2+8], %f259;

$L__BB63_15:
	bar.sync 	0;
	mov.b32 	%r149, %f386;
	shfl.sync.bfly.b32 	%r153|%p40, %r149, %r117, %r116, %r118;
	mov.b32 	%f260, %r153;
	add.f32 	%f261, %f386, %f260;
	mov.b32 	%r154, %f261;
	shfl.sync.bfly.b32 	%r156|%p41, %r154, %r121, %r116, %r118;
	mov.b32 	%f262, %r156;
	add.f32 	%f263, %f261, %f262;
	mov.b32 	%r157, %f263;
	shfl.sync.bfly.b32 	%r159|%p42, %r157, %r124, %r116, %r118;
	mov.b32 	%f264, %r159;
	add.f32 	%f265, %f263, %f264;
	mov.b32 	%r160, %f265;
	shfl.sync.bfly.b32 	%r162|%p43, %r160, %r127, %r116, %r118;
	mov.b32 	%f266, %r162;
	add.f32 	%f267, %f265, %f266;
	mov.b32 	%r163, %f267;
	shfl.sync.bfly.b32 	%r165|%p44, %r163, %r130, %r116, %r118;
	mov.b32 	%f268, %r165;
	add.f32 	%f269, %f267, %f268;
	st.local.f32 	[%rd2+12], %f269;
	st.shared.f32 	[%r10], %f269;
	bar.sync 	0;
	@%p1 bra 	$L__BB63_17;

	ld.shared.f32 	%f270, [%r4];
	mov.b32 	%r166, %f270;
	mov.u32 	%r167, 31;
	mov.u32 	%r168, 16;
	mov.u32 	%r169, -1;
	shfl.sync.bfly.b32 	%r170|%p45, %r166, %r168, %r167, %r169;
	mov.b32 	%f271, %r170;
	add.f32 	%f272, %f270, %f271;
	mov.b32 	%r171, %f272;
	mov.u32 	%r172, 8;
	shfl.sync.bfly.b32 	%r173|%p46, %r171, %r172, %r167, %r169;
	mov.b32 	%f273, %r173;
	add.f32 	%f274, %f272, %f273;
	mov.b32 	%r174, %f274;
	mov.u32 	%r175, 4;
	shfl.sync.bfly.b32 	%r176|%p47, %r174, %r175, %r167, %r169;
	mov.b32 	%f275, %r176;
	add.f32 	%f276, %f274, %f275;
	mov.b32 	%r177, %f276;
	mov.u32 	%r178, 2;
	shfl.sync.bfly.b32 	%r179|%p48, %r177, %r178, %r167, %r169;
	mov.b32 	%f277, %r179;
	add.f32 	%f278, %f276, %f277;
	mov.b32 	%r180, %f278;
	mov.u32 	%r181, 1;
	shfl.sync.bfly.b32 	%r182|%p49, %r180, %r181, %r167, %r169;
	mov.b32 	%f279, %r182;
	add.f32 	%f280, %f278, %f279;
	st.local.f32 	[%rd2+12], %f280;

$L__BB63_17:
	bar.sync 	0;
	mov.b32 	%r183, %f385;
	mov.u32 	%r184, 31;
	mov.u32 	%r185, 16;
	mov.u32 	%r186, -1;
	shfl.sync.bfly.b32 	%r187|%p51, %r183, %r185, %r184, %r186;
	mov.b32 	%f281, %r187;
	add.f32 	%f282, %f385, %f281;
	mov.b32 	%r188, %f282;
	mov.u32 	%r189, 8;
	shfl.sync.bfly.b32 	%r190|%p52, %r188, %r189, %r184, %r186;
	mov.b32 	%f283, %r190;
	add.f32 	%f284, %f282, %f283;
	mov.b32 	%r191, %f284;
	mov.u32 	%r192, 4;
	shfl.sync.bfly.b32 	%r193|%p53, %r191, %r192, %r184, %r186;
	mov.b32 	%f285, %r193;
	add.f32 	%f286, %f284, %f285;
	mov.b32 	%r194, %f286;
	mov.u32 	%r195, 2;
	shfl.sync.bfly.b32 	%r196|%p54, %r194, %r195, %r184, %r186;
	mov.b32 	%f287, %r196;
	add.f32 	%f288, %f286, %f287;
	mov.b32 	%r197, %f288;
	mov.u32 	%r198, 1;
	shfl.sync.bfly.b32 	%r199|%p55, %r197, %r198, %r184, %r186;
	mov.b32 	%f289, %r199;
	add.f32 	%f290, %f288, %f289;
	st.local.f32 	[%rd2+16], %f290;
	st.shared.f32 	[%r10], %f290;
	bar.sync 	0;
	@%p1 bra 	$L__BB63_19;

	ld.shared.f32 	%f291, [%r4];
	mov.b32 	%r200, %f291;
	shfl.sync.bfly.b32 	%r204|%p56, %r200, %r185, %r184, %r186;
	mov.b32 	%f292, %r204;
	add.f32 	%f293, %f291, %f292;
	mov.b32 	%r205, %f293;
	shfl.sync.bfly.b32 	%r207|%p57, %r205, %r189, %r184, %r186;
	mov.b32 	%f294, %r207;
	add.f32 	%f295, %f293, %f294;
	mov.b32 	%r208, %f295;
	shfl.sync.bfly.b32 	%r210|%p58, %r208, %r192, %r184, %r186;
	mov.b32 	%f296, %r210;
	add.f32 	%f297, %f295, %f296;
	mov.b32 	%r211, %f297;
	shfl.sync.bfly.b32 	%r213|%p59, %r211, %r195, %r184, %r186;
	mov.b32 	%f298, %r213;
	add.f32 	%f299, %f297, %f298;
	mov.b32 	%r214, %f299;
	shfl.sync.bfly.b32 	%r216|%p60, %r214, %r198, %r184, %r186;
	mov.b32 	%f300, %r216;
	add.f32 	%f301, %f299, %f300;
	st.local.f32 	[%rd2+16], %f301;

$L__BB63_19:
	bar.sync 	0;
	mov.b32 	%r217, %f384;
	shfl.sync.bfly.b32 	%r221|%p62, %r217, %r185, %r184, %r186;
	mov.b32 	%f302, %r221;
	add.f32 	%f303, %f384, %f302;
	mov.b32 	%r222, %f303;
	shfl.sync.bfly.b32 	%r224|%p63, %r222, %r189, %r184, %r186;
	mov.b32 	%f304, %r224;
	add.f32 	%f305, %f303, %f304;
	mov.b32 	%r225, %f305;
	shfl.sync.bfly.b32 	%r227|%p64, %r225, %r192, %r184, %r186;
	mov.b32 	%f306, %r227;
	add.f32 	%f307, %f305, %f306;
	mov.b32 	%r228, %f307;
	shfl.sync.bfly.b32 	%r230|%p65, %r228, %r195, %r184, %r186;
	mov.b32 	%f308, %r230;
	add.f32 	%f309, %f307, %f308;
	mov.b32 	%r231, %f309;
	shfl.sync.bfly.b32 	%r233|%p66, %r231, %r198, %r184, %r186;
	mov.b32 	%f310, %r233;
	add.f32 	%f311, %f309, %f310;
	st.local.f32 	[%rd2+20], %f311;
	st.shared.f32 	[%r10], %f311;
	bar.sync 	0;
	@%p1 bra 	$L__BB63_21;

	ld.shared.f32 	%f312, [%r4];
	mov.b32 	%r234, %f312;
	mov.u32 	%r235, 31;
	mov.u32 	%r236, 16;
	mov.u32 	%r237, -1;
	shfl.sync.bfly.b32 	%r238|%p67, %r234, %r236, %r235, %r237;
	mov.b32 	%f313, %r238;
	add.f32 	%f314, %f312, %f313;
	mov.b32 	%r239, %f314;
	mov.u32 	%r240, 8;
	shfl.sync.bfly.b32 	%r241|%p68, %r239, %r240, %r235, %r237;
	mov.b32 	%f315, %r241;
	add.f32 	%f316, %f314, %f315;
	mov.b32 	%r242, %f316;
	mov.u32 	%r243, 4;
	shfl.sync.bfly.b32 	%r244|%p69, %r242, %r243, %r235, %r237;
	mov.b32 	%f317, %r244;
	add.f32 	%f318, %f316, %f317;
	mov.b32 	%r245, %f318;
	mov.u32 	%r246, 2;
	shfl.sync.bfly.b32 	%r247|%p70, %r245, %r246, %r235, %r237;
	mov.b32 	%f319, %r247;
	add.f32 	%f320, %f318, %f319;
	mov.b32 	%r248, %f320;
	mov.u32 	%r249, 1;
	shfl.sync.bfly.b32 	%r250|%p71, %r248, %r249, %r235, %r237;
	mov.b32 	%f321, %r250;
	add.f32 	%f322, %f320, %f321;
	st.local.f32 	[%rd2+20], %f322;

$L__BB63_21:
	bar.sync 	0;
	mov.b32 	%r251, %f383;
	mov.u32 	%r252, 31;
	mov.u32 	%r253, 16;
	mov.u32 	%r254, -1;
	shfl.sync.bfly.b32 	%r255|%p73, %r251, %r253, %r252, %r254;
	mov.b32 	%f323, %r255;
	add.f32 	%f324, %f383, %f323;
	mov.b32 	%r256, %f324;
	mov.u32 	%r257, 8;
	shfl.sync.bfly.b32 	%r258|%p74, %r256, %r257, %r252, %r254;
	mov.b32 	%f325, %r258;
	add.f32 	%f326, %f324, %f325;
	mov.b32 	%r259, %f326;
	mov.u32 	%r260, 4;
	shfl.sync.bfly.b32 	%r261|%p75, %r259, %r260, %r252, %r254;
	mov.b32 	%f327, %r261;
	add.f32 	%f328, %f326, %f327;
	mov.b32 	%r262, %f328;
	mov.u32 	%r263, 2;
	shfl.sync.bfly.b32 	%r264|%p76, %r262, %r263, %r252, %r254;
	mov.b32 	%f329, %r264;
	add.f32 	%f330, %f328, %f329;
	mov.b32 	%r265, %f330;
	mov.u32 	%r266, 1;
	shfl.sync.bfly.b32 	%r267|%p77, %r265, %r266, %r252, %r254;
	mov.b32 	%f331, %r267;
	add.f32 	%f332, %f330, %f331;
	st.local.f32 	[%rd2+24], %f332;
	st.shared.f32 	[%r10], %f332;
	bar.sync 	0;
	@%p1 bra 	$L__BB63_23;

	ld.shared.f32 	%f333, [%r4];
	mov.b32 	%r268, %f333;
	shfl.sync.bfly.b32 	%r272|%p78, %r268, %r253, %r252, %r254;
	mov.b32 	%f334, %r272;
	add.f32 	%f335, %f333, %f334;
	mov.b32 	%r273, %f335;
	shfl.sync.bfly.b32 	%r275|%p79, %r273, %r257, %r252, %r254;
	mov.b32 	%f336, %r275;
	add.f32 	%f337, %f335, %f336;
	mov.b32 	%r276, %f337;
	shfl.sync.bfly.b32 	%r278|%p80, %r276, %r260, %r252, %r254;
	mov.b32 	%f338, %r278;
	add.f32 	%f339, %f337, %f338;
	mov.b32 	%r279, %f339;
	shfl.sync.bfly.b32 	%r281|%p81, %r279, %r263, %r252, %r254;
	mov.b32 	%f340, %r281;
	add.f32 	%f341, %f339, %f340;
	mov.b32 	%r282, %f341;
	shfl.sync.bfly.b32 	%r284|%p82, %r282, %r266, %r252, %r254;
	mov.b32 	%f342, %r284;
	add.f32 	%f343, %f341, %f342;
	st.local.f32 	[%rd2+24], %f343;

$L__BB63_23:
	bar.sync 	0;
	mov.b32 	%r285, %f382;
	shfl.sync.bfly.b32 	%r289|%p84, %r285, %r253, %r252, %r254;
	mov.b32 	%f344, %r289;
	add.f32 	%f345, %f382, %f344;
	mov.b32 	%r290, %f345;
	shfl.sync.bfly.b32 	%r292|%p85, %r290, %r257, %r252, %r254;
	mov.b32 	%f346, %r292;
	add.f32 	%f347, %f345, %f346;
	mov.b32 	%r293, %f347;
	shfl.sync.bfly.b32 	%r295|%p86, %r293, %r260, %r252, %r254;
	mov.b32 	%f348, %r295;
	add.f32 	%f349, %f347, %f348;
	mov.b32 	%r296, %f349;
	shfl.sync.bfly.b32 	%r298|%p87, %r296, %r263, %r252, %r254;
	mov.b32 	%f350, %r298;
	add.f32 	%f351, %f349, %f350;
	mov.b32 	%r299, %f351;
	shfl.sync.bfly.b32 	%r301|%p88, %r299, %r266, %r252, %r254;
	mov.b32 	%f352, %r301;
	add.f32 	%f353, %f351, %f352;
	st.local.f32 	[%rd2+28], %f353;
	st.shared.f32 	[%r10], %f353;
	bar.sync 	0;
	@%p1 bra 	$L__BB63_25;

	ld.shared.f32 	%f354, [%r4];
	mov.b32 	%r302, %f354;
	mov.u32 	%r303, 31;
	mov.u32 	%r304, 16;
	mov.u32 	%r305, -1;
	shfl.sync.bfly.b32 	%r306|%p89, %r302, %r304, %r303, %r305;
	mov.b32 	%f355, %r306;
	add.f32 	%f356, %f354, %f355;
	mov.b32 	%r307, %f356;
	mov.u32 	%r308, 8;
	shfl.sync.bfly.b32 	%r309|%p90, %r307, %r308, %r303, %r305;
	mov.b32 	%f357, %r309;
	add.f32 	%f358, %f356, %f357;
	mov.b32 	%r310, %f358;
	mov.u32 	%r311, 4;
	shfl.sync.bfly.b32 	%r312|%p91, %r310, %r311, %r303, %r305;
	mov.b32 	%f359, %r312;
	add.f32 	%f360, %f358, %f359;
	mov.b32 	%r313, %f360;
	mov.u32 	%r314, 2;
	shfl.sync.bfly.b32 	%r315|%p92, %r313, %r314, %r303, %r305;
	mov.b32 	%f361, %r315;
	add.f32 	%f362, %f360, %f361;
	mov.b32 	%r316, %f362;
	mov.u32 	%r317, 1;
	shfl.sync.bfly.b32 	%r318|%p93, %r316, %r317, %r303, %r305;
	mov.b32 	%f363, %r318;
	add.f32 	%f364, %f362, %f363;
	st.local.f32 	[%rd2+28], %f364;

$L__BB63_25:
	bar.sync 	0;
	setp.gt.s32 	%p94, %r3, 7;
	@%p94 bra 	$L__BB63_27;

	mul.wide.s32 	%rd65, %r3, 4;
	add.s64 	%rd66, %rd2, %rd65;
	ld.local.f32 	%f365, [%rd66];
	mad.lo.s32 	%r319, %r3, %r13, %r2;
	cvt.s64.s32 	%rd67, %r319;
	mul.lo.s32 	%r320, %r1, %r14;
	cvt.s64.s32 	%rd68, %r320;
	add.s64 	%rd69, %rd68, %rd67;
	cvta.to.global.u64 	%rd70, %rd21;
	shl.b64 	%rd71, %rd69, 2;
	add.s64 	%rd72, %rd70, %rd71;
	st.global.f32 	[%rd72], %f365;

$L__BB63_27:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_1_bs_32
.visible .entry ggml_matvec_f16_ncols_1_bs_32(
	.param .u64 ggml_matvec_f16_ncols_1_bs_32_param_0,
	.param .u64 ggml_matvec_f16_ncols_1_bs_32_param_1,
	.param .u64 ggml_matvec_f16_ncols_1_bs_32_param_2,
	.param .u32 ggml_matvec_f16_ncols_1_bs_32_param_3,
	.param .u32 ggml_matvec_f16_ncols_1_bs_32_param_4,
	.param .u32 ggml_matvec_f16_ncols_1_bs_32_param_5,
	.param .u32 ggml_matvec_f16_ncols_1_bs_32_param_6,
	.param .u32 ggml_matvec_f16_ncols_1_bs_32_param_7,
	.param .u32 ggml_matvec_f16_ncols_1_bs_32_param_8,
	.param .u32 ggml_matvec_f16_ncols_1_bs_32_param_9,
	.param .u32 ggml_matvec_f16_ncols_1_bs_32_param_10,
	.param .u32 ggml_matvec_f16_ncols_1_bs_32_param_11
)
{
	.reg .pred 	%p<12>;
	.reg .b16 	%rs<31>;
	.reg .f32 	%f<17>;
	.reg .b32 	%r<94>;
	.reg .b64 	%rd<45>;


	ld.param.u64 	%rd14, [ggml_matvec_f16_ncols_1_bs_32_param_0];
	ld.param.u64 	%rd15, [ggml_matvec_f16_ncols_1_bs_32_param_1];
	ld.param.u64 	%rd16, [ggml_matvec_f16_ncols_1_bs_32_param_2];
	ld.param.u32 	%r11, [ggml_matvec_f16_ncols_1_bs_32_param_3];
	ld.param.u32 	%r15, [ggml_matvec_f16_ncols_1_bs_32_param_5];
	ld.param.u32 	%r12, [ggml_matvec_f16_ncols_1_bs_32_param_7];
	ld.param.u32 	%r16, [ggml_matvec_f16_ncols_1_bs_32_param_8];
	ld.param.u32 	%r17, [ggml_matvec_f16_ncols_1_bs_32_param_9];
	ld.param.u32 	%r13, [ggml_matvec_f16_ncols_1_bs_32_param_10];
	ld.param.u32 	%r14, [ggml_matvec_f16_ncols_1_bs_32_param_11];
	mov.u32 	%r18, %ctaid.x;
	mov.u32 	%r19, %ctaid.y;
	div.s32 	%r20, %r19, %r16;
	mul.lo.s32 	%r21, %r18, %r15;
	mad.lo.s32 	%r22, %r20, %r17, %r21;
	cvt.s64.s32 	%rd1, %r22;
	mov.f32 	%f3, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs30, %f3;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs29, %f3;}

	// end inline asm
	mov.u32 	%r23, %tid.x;
	setp.ge.s32 	%p1, %r23, %r11;
	@%p1 bra 	$L__BB64_7;

	not.b32 	%r24, %r23;
	add.s32 	%r25, %r24, %r11;
	shr.u32 	%r26, %r25, 5;
	add.s32 	%r27, %r26, 1;
	and.b32  	%r91, %r27, 3;
	setp.eq.s32 	%p2, %r91, 0;
	mov.u32 	%r92, %r23;
	@%p2 bra 	$L__BB64_4;

	mov.u32 	%r92, %tid.x;
	mul.wide.s32 	%rd17, %r92, 2;
	mul.lo.s32 	%r29, %r19, %r13;
	cvt.s64.s32 	%rd18, %r29;
	add.s64 	%rd19, %rd17, %rd18;
	cvta.to.global.u64 	%rd20, %rd15;
	shl.b64 	%rd21, %rd19, 1;
	add.s64 	%rd42, %rd20, %rd21;
	add.s64 	%rd22, %rd17, %rd1;
	cvta.to.global.u64 	%rd23, %rd14;
	shl.b64 	%rd24, %rd22, 1;
	add.s64 	%rd41, %rd23, %rd24;

$L__BB64_3:
	.pragma "nounroll";
	ld.global.nc.u32 	%r31, [%rd41];
	ld.global.nc.u32 	%r32, [%rd42];
	// begin inline asm
	{mul.f16x2 %r30,%r31,%r32;
}
	// end inline asm
	mov.b32 	%r34, {%rs30, %rs29};
	// begin inline asm
	{add.f16x2 %r33,%r34,%r30;
}
	// end inline asm
	mov.b32 	{%rs30, %rs29}, %r33;
	add.s32 	%r92, %r92, 32;
	add.s64 	%rd42, %rd42, 128;
	add.s64 	%rd41, %rd41, 128;
	add.s32 	%r91, %r91, -1;
	setp.ne.s32 	%p3, %r91, 0;
	@%p3 bra 	$L__BB64_3;

$L__BB64_4:
	setp.lt.u32 	%p4, %r25, 96;
	@%p4 bra 	$L__BB64_7;

	mul.wide.s32 	%rd25, %r92, 2;
	cvta.to.global.u64 	%rd26, %rd14;
	add.s64 	%rd27, %rd25, %rd1;
	shl.b64 	%rd28, %rd27, 1;
	add.s64 	%rd29, %rd26, %rd28;
	add.s64 	%rd44, %rd29, 256;
	mul.lo.s32 	%r40, %r19, %r13;
	cvt.s64.s32 	%rd30, %r40;
	cvta.to.global.u64 	%rd31, %rd15;
	add.s64 	%rd32, %rd25, %rd30;
	shl.b64 	%rd33, %rd32, 1;
	add.s64 	%rd34, %rd31, %rd33;
	add.s64 	%rd43, %rd34, 256;

$L__BB64_6:
	ld.global.nc.u32 	%r42, [%rd44+-256];
	ld.global.nc.u32 	%r43, [%rd43+-256];
	// begin inline asm
	{mul.f16x2 %r41,%r42,%r43;
}
	// end inline asm
	mov.b32 	%r45, {%rs30, %rs29};
	// begin inline asm
	{add.f16x2 %r44,%r45,%r41;
}
	// end inline asm
	ld.global.nc.u32 	%r48, [%rd44+-128];
	ld.global.nc.u32 	%r49, [%rd43+-128];
	// begin inline asm
	{mul.f16x2 %r47,%r48,%r49;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r50,%r44,%r47;
}
	// end inline asm
	ld.global.nc.u32 	%r54, [%rd44];
	ld.global.nc.u32 	%r55, [%rd43];
	// begin inline asm
	{mul.f16x2 %r53,%r54,%r55;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r56,%r50,%r53;
}
	// end inline asm
	ld.global.nc.u32 	%r60, [%rd44+128];
	ld.global.nc.u32 	%r61, [%rd43+128];
	// begin inline asm
	{mul.f16x2 %r59,%r60,%r61;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r62,%r56,%r59;
}
	// end inline asm
	mov.b32 	{%rs30, %rs29}, %r62;
	add.s64 	%rd44, %rd44, 512;
	add.s64 	%rd43, %rd43, 512;
	add.s32 	%r92, %r92, 128;
	setp.lt.s32 	%p5, %r92, %r11;
	@%p5 bra 	$L__BB64_6;

$L__BB64_7:
	mov.u32 	%r67, 1;
	mov.b32 	%r66, {%rs30, %rs29};
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r66;
  cvt.f32.f16 %f4, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r66;
  cvt.f32.f16 %f5, high;}

	// end inline asm
	add.f32 	%f6, %f4, %f5;
	mov.b32 	%r68, %f6;
	mov.u32 	%r69, 31;
	mov.u32 	%r70, 16;
	mov.u32 	%r71, -1;
	shfl.sync.bfly.b32 	%r72|%p6, %r68, %r70, %r69, %r71;
	mov.b32 	%f7, %r72;
	add.f32 	%f8, %f6, %f7;
	mov.b32 	%r73, %f8;
	mov.u32 	%r74, 8;
	shfl.sync.bfly.b32 	%r75|%p7, %r73, %r74, %r69, %r71;
	mov.b32 	%f9, %r75;
	add.f32 	%f10, %f8, %f9;
	mov.b32 	%r76, %f10;
	mov.u32 	%r77, 4;
	shfl.sync.bfly.b32 	%r78|%p8, %r76, %r77, %r69, %r71;
	mov.b32 	%f11, %r78;
	add.f32 	%f12, %f10, %f11;
	mov.b32 	%r79, %f12;
	mov.u32 	%r80, 2;
	shfl.sync.bfly.b32 	%r81|%p9, %r79, %r80, %r69, %r71;
	mov.b32 	%f13, %r81;
	add.f32 	%f14, %f12, %f13;
	mov.b32 	%r82, %f14;
	shfl.sync.bfly.b32 	%r83|%p10, %r82, %r67, %r69, %r71;
	mov.b32 	%f15, %r83;
	add.f32 	%f1, %f14, %f15;
	setp.gt.s32 	%p11, %r23, 0;
	@%p11 bra 	$L__BB64_9;

	mad.lo.s32 	%r87, %r23, %r12, %r18;
	cvt.s64.s32 	%rd35, %r87;
	mul.lo.s32 	%r89, %r19, %r14;
	cvt.s64.s32 	%rd36, %r89;
	add.s64 	%rd37, %rd36, %rd35;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs20, %f1;}

	// end inline asm
	cvta.to.global.u64 	%rd38, %rd16;
	shl.b64 	%rd39, %rd37, 1;
	add.s64 	%rd40, %rd38, %rd39;
	st.global.u16 	[%rd40], %rs20;

$L__BB64_9:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_2_bs_32
.visible .entry ggml_matvec_f16_ncols_2_bs_32(
	.param .u64 ggml_matvec_f16_ncols_2_bs_32_param_0,
	.param .u64 ggml_matvec_f16_ncols_2_bs_32_param_1,
	.param .u64 ggml_matvec_f16_ncols_2_bs_32_param_2,
	.param .u32 ggml_matvec_f16_ncols_2_bs_32_param_3,
	.param .u32 ggml_matvec_f16_ncols_2_bs_32_param_4,
	.param .u32 ggml_matvec_f16_ncols_2_bs_32_param_5,
	.param .u32 ggml_matvec_f16_ncols_2_bs_32_param_6,
	.param .u32 ggml_matvec_f16_ncols_2_bs_32_param_7,
	.param .u32 ggml_matvec_f16_ncols_2_bs_32_param_8,
	.param .u32 ggml_matvec_f16_ncols_2_bs_32_param_9,
	.param .u32 ggml_matvec_f16_ncols_2_bs_32_param_10,
	.param .u32 ggml_matvec_f16_ncols_2_bs_32_param_11
)
{
	.local .align 8 .b8 	__local_depot65[8];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<17>;
	.reg .b16 	%rs<4>;
	.reg .f32 	%f<30>;
	.reg .b32 	%r<161>;
	.reg .b64 	%rd<69>;


	mov.u64 	%SPL, __local_depot65;
	ld.param.u64 	%rd20, [ggml_matvec_f16_ncols_2_bs_32_param_0];
	ld.param.u64 	%rd21, [ggml_matvec_f16_ncols_2_bs_32_param_1];
	ld.param.u64 	%rd22, [ggml_matvec_f16_ncols_2_bs_32_param_2];
	ld.param.u32 	%r24, [ggml_matvec_f16_ncols_2_bs_32_param_3];
	ld.param.u32 	%r30, [ggml_matvec_f16_ncols_2_bs_32_param_5];
	ld.param.u32 	%r25, [ggml_matvec_f16_ncols_2_bs_32_param_6];
	ld.param.u32 	%r26, [ggml_matvec_f16_ncols_2_bs_32_param_7];
	ld.param.u32 	%r31, [ggml_matvec_f16_ncols_2_bs_32_param_8];
	ld.param.u32 	%r32, [ggml_matvec_f16_ncols_2_bs_32_param_9];
	ld.param.u32 	%r27, [ggml_matvec_f16_ncols_2_bs_32_param_10];
	ld.param.u32 	%r28, [ggml_matvec_f16_ncols_2_bs_32_param_11];
	add.u64 	%rd24, %SPL, 0;
	mov.u32 	%r33, %ctaid.y;
	div.s32 	%r34, %r33, %r31;
	mov.u32 	%r35, %ctaid.x;
	mul.lo.s32 	%r36, %r35, %r30;
	mad.lo.s32 	%r37, %r34, %r32, %r36;
	cvt.s64.s32 	%rd1, %r37;
	mov.f32 	%f2, 0f00000000;
	st.local.v2.f32 	[%rd24], {%f2, %f2};
	mov.u32 	%r159, 0;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs2, %f2;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f2;}

	// end inline asm
	mov.b32 	%r160, {%rs1, %rs2};
	mov.u32 	%r38, %tid.x;
	setp.ge.s32 	%p1, %r38, %r24;
	@%p1 bra 	$L__BB65_7;

	not.b32 	%r40, %r38;
	add.s32 	%r41, %r40, %r24;
	shr.u32 	%r42, %r41, 5;
	add.s32 	%r43, %r42, 1;
	and.b32  	%r152, %r43, 3;
	setp.eq.s32 	%p2, %r152, 0;
	mov.u32 	%r159, 0;
	mov.u32 	%r155, %r38;
	@%p2 bra 	$L__BB65_4;

	mul.wide.s32 	%rd25, %r25, 2;
	mov.u32 	%r155, %tid.x;
	mul.wide.s32 	%rd26, %r155, 2;
	add.s64 	%rd27, %rd25, %rd26;
	mul.lo.s32 	%r46, %r33, %r27;
	cvt.s64.s32 	%rd28, %r46;
	add.s64 	%rd29, %rd27, %rd28;
	cvta.to.global.u64 	%rd30, %rd21;
	shl.b64 	%rd31, %rd29, 1;
	add.s64 	%rd65, %rd30, %rd31;
	add.s64 	%rd32, %rd26, %rd28;
	shl.b64 	%rd33, %rd32, 1;
	add.s64 	%rd64, %rd30, %rd33;
	add.s64 	%rd34, %rd26, %rd1;
	cvta.to.global.u64 	%rd35, %rd20;
	shl.b64 	%rd36, %rd34, 1;
	add.s64 	%rd63, %rd35, %rd36;
	mov.u32 	%r159, 0;

$L__BB65_3:
	.pragma "nounroll";
	ld.global.nc.u32 	%r48, [%rd63];
	ld.global.nc.u32 	%r49, [%rd64];
	// begin inline asm
	{mul.f16x2 %r47,%r48,%r49;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r160,%r160,%r47;
}
	// end inline asm
	ld.global.nc.u32 	%r55, [%rd65];
	// begin inline asm
	{mul.f16x2 %r53,%r48,%r55;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r159,%r159,%r53;
}
	// end inline asm
	add.s32 	%r155, %r155, 32;
	add.s64 	%rd65, %rd65, 128;
	add.s64 	%rd64, %rd64, 128;
	add.s64 	%rd63, %rd63, 128;
	add.s32 	%r152, %r152, -1;
	setp.ne.s32 	%p3, %r152, 0;
	@%p3 bra 	$L__BB65_3;

$L__BB65_4:
	setp.lt.u32 	%p4, %r41, 96;
	@%p4 bra 	$L__BB65_7;

	mul.wide.s32 	%rd37, %r155, 2;
	cvta.to.global.u64 	%rd38, %rd20;
	add.s64 	%rd39, %rd37, %rd1;
	shl.b64 	%rd40, %rd39, 1;
	add.s64 	%rd41, %rd38, %rd40;
	add.s64 	%rd68, %rd41, 256;
	mul.lo.s32 	%r63, %r33, %r27;
	cvt.s64.s32 	%rd42, %r63;
	cvta.to.global.u64 	%rd43, %rd21;
	add.s64 	%rd44, %rd37, %rd42;
	shl.b64 	%rd45, %rd44, 1;
	add.s64 	%rd46, %rd43, %rd45;
	add.s64 	%rd67, %rd46, 384;
	mul.wide.s32 	%rd47, %r25, 2;
	add.s64 	%rd48, %rd44, %rd47;
	shl.b64 	%rd49, %rd48, 1;
	add.s64 	%rd50, %rd43, %rd49;
	add.s64 	%rd66, %rd50, 256;

$L__BB65_6:
	ld.global.nc.u32 	%r65, [%rd68+-256];
	ld.global.nc.u32 	%r66, [%rd67+-384];
	// begin inline asm
	{mul.f16x2 %r64,%r65,%r66;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r67,%r160,%r64;
}
	// end inline asm
	ld.global.nc.u32 	%r72, [%rd66+-256];
	// begin inline asm
	{mul.f16x2 %r70,%r65,%r72;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r73,%r159,%r70;
}
	// end inline asm
	ld.global.nc.u32 	%r77, [%rd68+-128];
	ld.global.nc.u32 	%r78, [%rd67+-256];
	// begin inline asm
	{mul.f16x2 %r76,%r77,%r78;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r79,%r67,%r76;
}
	// end inline asm
	ld.global.nc.u32 	%r84, [%rd66+-128];
	// begin inline asm
	{mul.f16x2 %r82,%r77,%r84;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r85,%r73,%r82;
}
	// end inline asm
	ld.global.nc.u32 	%r89, [%rd68];
	ld.global.nc.u32 	%r90, [%rd67+-128];
	// begin inline asm
	{mul.f16x2 %r88,%r89,%r90;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r91,%r79,%r88;
}
	// end inline asm
	ld.global.nc.u32 	%r96, [%rd66];
	// begin inline asm
	{mul.f16x2 %r94,%r89,%r96;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r97,%r85,%r94;
}
	// end inline asm
	ld.global.nc.u32 	%r101, [%rd68+128];
	ld.global.nc.u32 	%r102, [%rd67];
	// begin inline asm
	{mul.f16x2 %r100,%r101,%r102;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r160,%r91,%r100;
}
	// end inline asm
	ld.global.nc.u32 	%r108, [%rd66+128];
	// begin inline asm
	{mul.f16x2 %r106,%r101,%r108;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r159,%r97,%r106;
}
	// end inline asm
	add.s64 	%rd68, %rd68, 512;
	add.s64 	%rd67, %rd67, 512;
	add.s64 	%rd66, %rd66, 512;
	add.s32 	%r155, %r155, 128;
	setp.lt.s32 	%p5, %r155, %r24;
	@%p5 bra 	$L__BB65_6;

$L__BB65_7:
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r160;
  cvt.f32.f16 %f3, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r160;
  cvt.f32.f16 %f4, high;}

	// end inline asm
	add.f32 	%f7, %f3, %f4;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r159;
  cvt.f32.f16 %f5, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r159;
  cvt.f32.f16 %f6, high;}

	// end inline asm
	add.f32 	%f8, %f5, %f6;
	mov.b32 	%r116, %f7;
	mov.u32 	%r117, 31;
	mov.u32 	%r118, 16;
	mov.u32 	%r119, -1;
	shfl.sync.bfly.b32 	%r120|%p6, %r116, %r118, %r117, %r119;
	mov.b32 	%f9, %r120;
	add.f32 	%f10, %f7, %f9;
	mov.b32 	%r121, %f10;
	mov.u32 	%r122, 8;
	shfl.sync.bfly.b32 	%r123|%p7, %r121, %r122, %r117, %r119;
	mov.b32 	%f11, %r123;
	add.f32 	%f12, %f10, %f11;
	mov.b32 	%r124, %f12;
	mov.u32 	%r125, 4;
	shfl.sync.bfly.b32 	%r126|%p8, %r124, %r125, %r117, %r119;
	mov.b32 	%f13, %r126;
	add.f32 	%f14, %f12, %f13;
	mov.b32 	%r127, %f14;
	mov.u32 	%r128, 2;
	shfl.sync.bfly.b32 	%r129|%p9, %r127, %r128, %r117, %r119;
	mov.b32 	%f15, %r129;
	add.f32 	%f16, %f14, %f15;
	mov.b32 	%r130, %f16;
	mov.u32 	%r131, 1;
	shfl.sync.bfly.b32 	%r132|%p10, %r130, %r131, %r117, %r119;
	mov.b32 	%f17, %r132;
	add.f32 	%f18, %f16, %f17;
	st.local.f32 	[%rd24], %f18;
	mov.b32 	%r133, %f8;
	shfl.sync.bfly.b32 	%r134|%p11, %r133, %r118, %r117, %r119;
	mov.b32 	%f19, %r134;
	add.f32 	%f20, %f8, %f19;
	mov.b32 	%r135, %f20;
	shfl.sync.bfly.b32 	%r136|%p12, %r135, %r122, %r117, %r119;
	mov.b32 	%f21, %r136;
	add.f32 	%f22, %f20, %f21;
	mov.b32 	%r137, %f22;
	shfl.sync.bfly.b32 	%r138|%p13, %r137, %r125, %r117, %r119;
	mov.b32 	%f23, %r138;
	add.f32 	%f24, %f22, %f23;
	mov.b32 	%r139, %f24;
	shfl.sync.bfly.b32 	%r140|%p14, %r139, %r128, %r117, %r119;
	mov.b32 	%f25, %r140;
	add.f32 	%f26, %f24, %f25;
	mov.b32 	%r141, %f26;
	shfl.sync.bfly.b32 	%r142|%p15, %r141, %r131, %r117, %r119;
	mov.b32 	%f27, %r142;
	add.f32 	%f28, %f26, %f27;
	st.local.f32 	[%rd24+4], %f28;
	setp.gt.s32 	%p16, %r38, 1;
	@%p16 bra 	$L__BB65_9;

	mad.lo.s32 	%r146, %r38, %r26, %r35;
	cvt.s64.s32 	%rd53, %r146;
	mul.lo.s32 	%r148, %r33, %r28;
	cvt.s64.s32 	%rd54, %r148;
	add.s64 	%rd55, %rd54, %rd53;
	mul.wide.s32 	%rd58, %r38, 4;
	add.s64 	%rd59, %rd24, %rd58;
	ld.local.f32 	%f29, [%rd59];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs3, %f29;}

	// end inline asm
	cvta.to.global.u64 	%rd60, %rd22;
	shl.b64 	%rd61, %rd55, 1;
	add.s64 	%rd62, %rd60, %rd61;
	st.global.u16 	[%rd62], %rs3;

$L__BB65_9:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_3_bs_32
.visible .entry ggml_matvec_f16_ncols_3_bs_32(
	.param .u64 ggml_matvec_f16_ncols_3_bs_32_param_0,
	.param .u64 ggml_matvec_f16_ncols_3_bs_32_param_1,
	.param .u64 ggml_matvec_f16_ncols_3_bs_32_param_2,
	.param .u32 ggml_matvec_f16_ncols_3_bs_32_param_3,
	.param .u32 ggml_matvec_f16_ncols_3_bs_32_param_4,
	.param .u32 ggml_matvec_f16_ncols_3_bs_32_param_5,
	.param .u32 ggml_matvec_f16_ncols_3_bs_32_param_6,
	.param .u32 ggml_matvec_f16_ncols_3_bs_32_param_7,
	.param .u32 ggml_matvec_f16_ncols_3_bs_32_param_8,
	.param .u32 ggml_matvec_f16_ncols_3_bs_32_param_9,
	.param .u32 ggml_matvec_f16_ncols_3_bs_32_param_10,
	.param .u32 ggml_matvec_f16_ncols_3_bs_32_param_11
)
{
	.local .align 4 .b8 	__local_depot66[12];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<22>;
	.reg .b16 	%rs<4>;
	.reg .f32 	%f<43>;
	.reg .b32 	%r<222>;
	.reg .b64 	%rd<77>;


	mov.u64 	%SPL, __local_depot66;
	ld.param.u64 	%rd24, [ggml_matvec_f16_ncols_3_bs_32_param_0];
	ld.param.u64 	%rd25, [ggml_matvec_f16_ncols_3_bs_32_param_1];
	ld.param.u64 	%rd26, [ggml_matvec_f16_ncols_3_bs_32_param_2];
	ld.param.u32 	%r30, [ggml_matvec_f16_ncols_3_bs_32_param_3];
	ld.param.u32 	%r37, [ggml_matvec_f16_ncols_3_bs_32_param_5];
	ld.param.u32 	%r31, [ggml_matvec_f16_ncols_3_bs_32_param_6];
	ld.param.u32 	%r32, [ggml_matvec_f16_ncols_3_bs_32_param_7];
	ld.param.u32 	%r38, [ggml_matvec_f16_ncols_3_bs_32_param_8];
	ld.param.u32 	%r39, [ggml_matvec_f16_ncols_3_bs_32_param_9];
	ld.param.u32 	%r33, [ggml_matvec_f16_ncols_3_bs_32_param_10];
	ld.param.u32 	%r34, [ggml_matvec_f16_ncols_3_bs_32_param_11];
	add.u64 	%rd28, %SPL, 0;
	mov.u32 	%r40, %ctaid.y;
	div.s32 	%r41, %r40, %r38;
	mov.u32 	%r42, %ctaid.x;
	mul.lo.s32 	%r43, %r42, %r37;
	mad.lo.s32 	%r44, %r41, %r39, %r43;
	cvt.s64.s32 	%rd1, %r44;
	mov.f32 	%f2, 0f00000000;
	mov.u32 	%r219, 0;
	st.local.u32 	[%rd28], %r219;
	st.local.u32 	[%rd28+4], %r219;
	st.local.u32 	[%rd28+8], %r219;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs2, %f2;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f2;}

	// end inline asm
	mov.b32 	%r221, {%rs1, %rs2};
	mov.u32 	%r45, %tid.x;
	setp.ge.s32 	%p1, %r45, %r30;
	mov.u32 	%r220, %r219;
	@%p1 bra 	$L__BB66_7;

	not.b32 	%r48, %r45;
	add.s32 	%r49, %r48, %r30;
	shr.u32 	%r50, %r49, 5;
	add.s32 	%r51, %r50, 1;
	and.b32  	%r210, %r51, 3;
	setp.eq.s32 	%p2, %r210, 0;
	mov.u32 	%r219, 0;
	mov.u32 	%r214, %r45;
	@%p2 bra 	$L__BB66_4;

	shl.b32 	%r54, %r31, 1;
	mov.u32 	%r214, %tid.x;
	add.s32 	%r55, %r214, %r54;
	mul.wide.s32 	%rd29, %r55, 2;
	mul.lo.s32 	%r57, %r40, %r33;
	cvt.s64.s32 	%rd30, %r57;
	add.s64 	%rd31, %rd29, %rd30;
	cvta.to.global.u64 	%rd32, %rd25;
	shl.b64 	%rd33, %rd31, 1;
	add.s64 	%rd74, %rd32, %rd33;
	mul.wide.s32 	%rd34, %r31, 2;
	mul.wide.s32 	%rd35, %r214, 2;
	add.s64 	%rd36, %rd34, %rd35;
	add.s64 	%rd37, %rd36, %rd30;
	shl.b64 	%rd38, %rd37, 1;
	add.s64 	%rd73, %rd32, %rd38;
	add.s64 	%rd39, %rd35, %rd30;
	shl.b64 	%rd40, %rd39, 1;
	add.s64 	%rd72, %rd32, %rd40;
	add.s64 	%rd41, %rd35, %rd1;
	cvta.to.global.u64 	%rd42, %rd24;
	shl.b64 	%rd43, %rd41, 1;
	add.s64 	%rd71, %rd42, %rd43;
	mov.u32 	%r219, 0;
	mov.u32 	%r220, %r219;

$L__BB66_3:
	.pragma "nounroll";
	ld.global.nc.u32 	%r59, [%rd71];
	ld.global.nc.u32 	%r60, [%rd72];
	// begin inline asm
	{mul.f16x2 %r58,%r59,%r60;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r221,%r221,%r58;
}
	// end inline asm
	ld.global.nc.u32 	%r66, [%rd73];
	// begin inline asm
	{mul.f16x2 %r64,%r59,%r66;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r220,%r220,%r64;
}
	// end inline asm
	ld.global.nc.u32 	%r72, [%rd74];
	// begin inline asm
	{mul.f16x2 %r70,%r59,%r72;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r219,%r219,%r70;
}
	// end inline asm
	add.s32 	%r214, %r214, 32;
	add.s64 	%rd74, %rd74, 128;
	add.s64 	%rd73, %rd73, 128;
	add.s64 	%rd72, %rd72, 128;
	add.s64 	%rd71, %rd71, 128;
	add.s32 	%r210, %r210, -1;
	setp.ne.s32 	%p3, %r210, 0;
	@%p3 bra 	$L__BB66_3;

$L__BB66_4:
	setp.lt.u32 	%p4, %r49, 96;
	@%p4 bra 	$L__BB66_7;

	add.s32 	%r79, %r214, %r31;
	mul.lo.s32 	%r81, %r40, %r33;
	mul.wide.s32 	%rd44, %r81, 2;
	shl.b32 	%r82, %r31, 1;
	add.s32 	%r83, %r214, %r82;
	add.s32 	%r84, %r79, 32;
	mul.wide.s32 	%rd45, %r84, 4;
	add.s64 	%rd14, %rd45, %rd44;
	mul.wide.s32 	%rd46, %r83, 4;
	add.s64 	%rd15, %rd46, %rd44;
	mul.wide.s32 	%rd47, %r214, 2;
	cvta.to.global.u64 	%rd48, %rd24;
	add.s64 	%rd49, %rd47, %rd1;
	shl.b64 	%rd50, %rd49, 1;
	add.s64 	%rd51, %rd48, %rd50;
	add.s64 	%rd75, %rd51, 256;
	mul.wide.s32 	%rd52, %r214, 4;
	add.s64 	%rd17, %rd52, %rd44;
	mul.wide.s32 	%rd53, %r31, 4;
	add.s64 	%rd54, %rd52, %rd53;
	add.s64 	%rd18, %rd54, %rd44;
	cvta.to.global.u64 	%rd76, %rd25;

$L__BB66_6:
	ld.global.nc.u32 	%r86, [%rd75+-256];
	add.s64 	%rd55, %rd76, %rd17;
	ld.global.nc.u32 	%r87, [%rd55];
	// begin inline asm
	{mul.f16x2 %r85,%r86,%r87;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r88,%r221,%r85;
}
	// end inline asm
	add.s64 	%rd56, %rd76, %rd18;
	ld.global.nc.u32 	%r93, [%rd56];
	// begin inline asm
	{mul.f16x2 %r91,%r86,%r93;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r94,%r220,%r91;
}
	// end inline asm
	add.s64 	%rd57, %rd76, %rd15;
	ld.global.nc.u32 	%r99, [%rd57];
	// begin inline asm
	{mul.f16x2 %r97,%r86,%r99;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r100,%r219,%r97;
}
	// end inline asm
	ld.global.nc.u32 	%r104, [%rd75+-128];
	ld.global.nc.u32 	%r105, [%rd55+128];
	// begin inline asm
	{mul.f16x2 %r103,%r104,%r105;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r106,%r88,%r103;
}
	// end inline asm
	add.s64 	%rd58, %rd76, %rd14;
	ld.global.nc.u32 	%r111, [%rd58];
	// begin inline asm
	{mul.f16x2 %r109,%r104,%r111;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r112,%r94,%r109;
}
	// end inline asm
	ld.global.nc.u32 	%r117, [%rd57+128];
	// begin inline asm
	{mul.f16x2 %r115,%r104,%r117;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r118,%r100,%r115;
}
	// end inline asm
	ld.global.nc.u32 	%r122, [%rd75];
	ld.global.nc.u32 	%r123, [%rd55+256];
	// begin inline asm
	{mul.f16x2 %r121,%r122,%r123;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r124,%r106,%r121;
}
	// end inline asm
	ld.global.nc.u32 	%r129, [%rd58+128];
	// begin inline asm
	{mul.f16x2 %r127,%r122,%r129;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r130,%r112,%r127;
}
	// end inline asm
	ld.global.nc.u32 	%r135, [%rd57+256];
	// begin inline asm
	{mul.f16x2 %r133,%r122,%r135;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r136,%r118,%r133;
}
	// end inline asm
	ld.global.nc.u32 	%r140, [%rd75+128];
	ld.global.nc.u32 	%r141, [%rd55+384];
	// begin inline asm
	{mul.f16x2 %r139,%r140,%r141;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r221,%r124,%r139;
}
	// end inline asm
	ld.global.nc.u32 	%r147, [%rd58+256];
	// begin inline asm
	{mul.f16x2 %r145,%r140,%r147;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r220,%r130,%r145;
}
	// end inline asm
	ld.global.nc.u32 	%r153, [%rd57+384];
	// begin inline asm
	{mul.f16x2 %r151,%r140,%r153;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r219,%r136,%r151;
}
	// end inline asm
	add.s64 	%rd76, %rd76, 512;
	add.s64 	%rd75, %rd75, 512;
	add.s32 	%r214, %r214, 128;
	setp.lt.s32 	%p5, %r214, %r30;
	@%p5 bra 	$L__BB66_6;

$L__BB66_7:
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r221;
  cvt.f32.f16 %f3, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r221;
  cvt.f32.f16 %f4, high;}

	// end inline asm
	add.f32 	%f9, %f3, %f4;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r220;
  cvt.f32.f16 %f5, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r220;
  cvt.f32.f16 %f6, high;}

	// end inline asm
	add.f32 	%f10, %f5, %f6;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r219;
  cvt.f32.f16 %f7, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r219;
  cvt.f32.f16 %f8, high;}

	// end inline asm
	add.f32 	%f11, %f7, %f8;
	mov.b32 	%r163, %f9;
	mov.u32 	%r164, 31;
	mov.u32 	%r165, 16;
	mov.u32 	%r166, -1;
	shfl.sync.bfly.b32 	%r167|%p6, %r163, %r165, %r164, %r166;
	mov.b32 	%f12, %r167;
	add.f32 	%f13, %f9, %f12;
	mov.b32 	%r168, %f13;
	mov.u32 	%r169, 8;
	shfl.sync.bfly.b32 	%r170|%p7, %r168, %r169, %r164, %r166;
	mov.b32 	%f14, %r170;
	add.f32 	%f15, %f13, %f14;
	mov.b32 	%r171, %f15;
	mov.u32 	%r172, 4;
	shfl.sync.bfly.b32 	%r173|%p8, %r171, %r172, %r164, %r166;
	mov.b32 	%f16, %r173;
	add.f32 	%f17, %f15, %f16;
	mov.b32 	%r174, %f17;
	mov.u32 	%r175, 2;
	shfl.sync.bfly.b32 	%r176|%p9, %r174, %r175, %r164, %r166;
	mov.b32 	%f18, %r176;
	add.f32 	%f19, %f17, %f18;
	mov.b32 	%r177, %f19;
	mov.u32 	%r178, 1;
	shfl.sync.bfly.b32 	%r179|%p10, %r177, %r178, %r164, %r166;
	mov.b32 	%f20, %r179;
	add.f32 	%f21, %f19, %f20;
	st.local.f32 	[%rd28], %f21;
	mov.b32 	%r180, %f10;
	shfl.sync.bfly.b32 	%r181|%p11, %r180, %r165, %r164, %r166;
	mov.b32 	%f22, %r181;
	add.f32 	%f23, %f10, %f22;
	mov.b32 	%r182, %f23;
	shfl.sync.bfly.b32 	%r183|%p12, %r182, %r169, %r164, %r166;
	mov.b32 	%f24, %r183;
	add.f32 	%f25, %f23, %f24;
	mov.b32 	%r184, %f25;
	shfl.sync.bfly.b32 	%r185|%p13, %r184, %r172, %r164, %r166;
	mov.b32 	%f26, %r185;
	add.f32 	%f27, %f25, %f26;
	mov.b32 	%r186, %f27;
	shfl.sync.bfly.b32 	%r187|%p14, %r186, %r175, %r164, %r166;
	mov.b32 	%f28, %r187;
	add.f32 	%f29, %f27, %f28;
	mov.b32 	%r188, %f29;
	shfl.sync.bfly.b32 	%r189|%p15, %r188, %r178, %r164, %r166;
	mov.b32 	%f30, %r189;
	add.f32 	%f31, %f29, %f30;
	st.local.f32 	[%rd28+4], %f31;
	mov.b32 	%r190, %f11;
	shfl.sync.bfly.b32 	%r191|%p16, %r190, %r165, %r164, %r166;
	mov.b32 	%f32, %r191;
	add.f32 	%f33, %f11, %f32;
	mov.b32 	%r192, %f33;
	shfl.sync.bfly.b32 	%r193|%p17, %r192, %r169, %r164, %r166;
	mov.b32 	%f34, %r193;
	add.f32 	%f35, %f33, %f34;
	mov.b32 	%r194, %f35;
	shfl.sync.bfly.b32 	%r195|%p18, %r194, %r172, %r164, %r166;
	mov.b32 	%f36, %r195;
	add.f32 	%f37, %f35, %f36;
	mov.b32 	%r196, %f37;
	shfl.sync.bfly.b32 	%r197|%p19, %r196, %r175, %r164, %r166;
	mov.b32 	%f38, %r197;
	add.f32 	%f39, %f37, %f38;
	mov.b32 	%r198, %f39;
	shfl.sync.bfly.b32 	%r199|%p20, %r198, %r178, %r164, %r166;
	mov.b32 	%f40, %r199;
	add.f32 	%f41, %f39, %f40;
	st.local.f32 	[%rd28+8], %f41;
	setp.gt.s32 	%p21, %r45, 2;
	@%p21 bra 	$L__BB66_9;

	mad.lo.s32 	%r203, %r45, %r32, %r42;
	cvt.s64.s32 	%rd61, %r203;
	mul.lo.s32 	%r205, %r40, %r34;
	cvt.s64.s32 	%rd62, %r205;
	add.s64 	%rd63, %rd62, %rd61;
	mul.wide.s32 	%rd66, %r45, 4;
	add.s64 	%rd67, %rd28, %rd66;
	ld.local.f32 	%f42, [%rd67];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs3, %f42;}

	// end inline asm
	cvta.to.global.u64 	%rd68, %rd26;
	shl.b64 	%rd69, %rd63, 1;
	add.s64 	%rd70, %rd68, %rd69;
	st.global.u16 	[%rd70], %rs3;

$L__BB66_9:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_4_bs_32
.visible .entry ggml_matvec_f16_ncols_4_bs_32(
	.param .u64 ggml_matvec_f16_ncols_4_bs_32_param_0,
	.param .u64 ggml_matvec_f16_ncols_4_bs_32_param_1,
	.param .u64 ggml_matvec_f16_ncols_4_bs_32_param_2,
	.param .u32 ggml_matvec_f16_ncols_4_bs_32_param_3,
	.param .u32 ggml_matvec_f16_ncols_4_bs_32_param_4,
	.param .u32 ggml_matvec_f16_ncols_4_bs_32_param_5,
	.param .u32 ggml_matvec_f16_ncols_4_bs_32_param_6,
	.param .u32 ggml_matvec_f16_ncols_4_bs_32_param_7,
	.param .u32 ggml_matvec_f16_ncols_4_bs_32_param_8,
	.param .u32 ggml_matvec_f16_ncols_4_bs_32_param_9,
	.param .u32 ggml_matvec_f16_ncols_4_bs_32_param_10,
	.param .u32 ggml_matvec_f16_ncols_4_bs_32_param_11
)
{
	.local .align 16 .b8 	__local_depot67[16];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<27>;
	.reg .b16 	%rs<4>;
	.reg .f32 	%f<56>;
	.reg .b32 	%r<279>;
	.reg .b64 	%rd<87>;


	mov.u64 	%SPL, __local_depot67;
	ld.param.u64 	%rd28, [ggml_matvec_f16_ncols_4_bs_32_param_0];
	ld.param.u64 	%rd29, [ggml_matvec_f16_ncols_4_bs_32_param_1];
	ld.param.u64 	%rd30, [ggml_matvec_f16_ncols_4_bs_32_param_2];
	ld.param.u32 	%r36, [ggml_matvec_f16_ncols_4_bs_32_param_3];
	ld.param.u32 	%r44, [ggml_matvec_f16_ncols_4_bs_32_param_5];
	ld.param.u32 	%r37, [ggml_matvec_f16_ncols_4_bs_32_param_6];
	ld.param.u32 	%r38, [ggml_matvec_f16_ncols_4_bs_32_param_7];
	ld.param.u32 	%r45, [ggml_matvec_f16_ncols_4_bs_32_param_8];
	ld.param.u32 	%r46, [ggml_matvec_f16_ncols_4_bs_32_param_9];
	ld.param.u32 	%r39, [ggml_matvec_f16_ncols_4_bs_32_param_10];
	ld.param.u32 	%r40, [ggml_matvec_f16_ncols_4_bs_32_param_11];
	add.u64 	%rd32, %SPL, 0;
	mov.u32 	%r47, %ctaid.y;
	div.s32 	%r48, %r47, %r45;
	mov.u32 	%r49, %ctaid.x;
	mul.lo.s32 	%r50, %r49, %r44;
	mad.lo.s32 	%r51, %r48, %r46, %r50;
	cvt.s64.s32 	%rd1, %r51;
	mov.f32 	%f2, 0f00000000;
	st.local.v4.f32 	[%rd32], {%f2, %f2, %f2, %f2};
	mov.u32 	%r275, 0;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs2, %f2;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f2;}

	// end inline asm
	mov.b32 	%r278, {%rs1, %rs2};
	mov.u32 	%r52, %tid.x;
	setp.ge.s32 	%p1, %r52, %r36;
	mov.u32 	%r276, %r275;
	mov.u32 	%r277, %r275;
	@%p1 bra 	$L__BB67_7;

	not.b32 	%r56, %r52;
	add.s32 	%r57, %r56, %r36;
	shr.u32 	%r58, %r57, 5;
	add.s32 	%r59, %r58, 1;
	and.b32  	%r264, %r59, 3;
	setp.eq.s32 	%p2, %r264, 0;
	mov.u32 	%r275, 0;
	mov.u32 	%r269, %r52;
	@%p2 bra 	$L__BB67_4;

	shl.b32 	%r63, %r37, 1;
	mov.u32 	%r269, %tid.x;
	add.s32 	%r64, %r269, %r63;
	mul.wide.s32 	%rd33, %r64, 2;
	mul.lo.s32 	%r66, %r47, %r39;
	cvt.s64.s32 	%rd34, %r66;
	add.s64 	%rd35, %rd33, %rd34;
	cvta.to.global.u64 	%rd36, %rd29;
	shl.b64 	%rd37, %rd35, 1;
	add.s64 	%rd84, %rd36, %rd37;
	mad.lo.s32 	%r67, %r37, 3, %r269;
	mul.wide.s32 	%rd38, %r67, 2;
	add.s64 	%rd39, %rd38, %rd34;
	shl.b64 	%rd40, %rd39, 1;
	add.s64 	%rd83, %rd36, %rd40;
	mul.wide.s32 	%rd41, %r37, 2;
	mul.wide.s32 	%rd42, %r269, 2;
	add.s64 	%rd43, %rd41, %rd42;
	add.s64 	%rd44, %rd43, %rd34;
	shl.b64 	%rd45, %rd44, 1;
	add.s64 	%rd82, %rd36, %rd45;
	add.s64 	%rd46, %rd42, %rd34;
	shl.b64 	%rd47, %rd46, 1;
	add.s64 	%rd81, %rd36, %rd47;
	add.s64 	%rd48, %rd42, %rd1;
	cvta.to.global.u64 	%rd49, %rd28;
	shl.b64 	%rd50, %rd48, 1;
	add.s64 	%rd80, %rd49, %rd50;
	mov.u32 	%r275, 0;
	mov.u32 	%r276, %r275;
	mov.u32 	%r277, %r275;

$L__BB67_3:
	.pragma "nounroll";
	ld.global.nc.u32 	%r69, [%rd80];
	ld.global.nc.u32 	%r70, [%rd81];
	// begin inline asm
	{mul.f16x2 %r68,%r69,%r70;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r278,%r278,%r68;
}
	// end inline asm
	ld.global.nc.u32 	%r76, [%rd82];
	// begin inline asm
	{mul.f16x2 %r74,%r69,%r76;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r277,%r277,%r74;
}
	// end inline asm
	ld.global.nc.u32 	%r82, [%rd84];
	// begin inline asm
	{mul.f16x2 %r80,%r69,%r82;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r276,%r276,%r80;
}
	// end inline asm
	ld.global.nc.u32 	%r88, [%rd83];
	// begin inline asm
	{mul.f16x2 %r86,%r69,%r88;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r275,%r275,%r86;
}
	// end inline asm
	add.s32 	%r269, %r269, 32;
	add.s64 	%rd84, %rd84, 128;
	add.s64 	%rd83, %rd83, 128;
	add.s64 	%rd82, %rd82, 128;
	add.s64 	%rd81, %rd81, 128;
	add.s64 	%rd80, %rd80, 128;
	add.s32 	%r264, %r264, -1;
	setp.ne.s32 	%p3, %r264, 0;
	@%p3 bra 	$L__BB67_3;

$L__BB67_4:
	setp.lt.u32 	%p4, %r57, 96;
	@%p4 bra 	$L__BB67_7;

	add.s32 	%r95, %r269, %r37;
	mul.lo.s32 	%r97, %r47, %r39;
	mul.wide.s32 	%rd51, %r97, 2;
	shl.b32 	%r98, %r37, 1;
	add.s32 	%r99, %r269, %r98;
	mad.lo.s32 	%r100, %r37, 3, %r269;
	add.s32 	%r101, %r95, 32;
	mul.wide.s32 	%rd52, %r101, 4;
	add.s64 	%rd17, %rd52, %rd51;
	mul.wide.s32 	%rd53, %r99, 4;
	add.s64 	%rd18, %rd53, %rd51;
	mul.wide.s32 	%rd54, %r100, 4;
	add.s64 	%rd19, %rd54, %rd51;
	mul.wide.s32 	%rd55, %r269, 2;
	cvta.to.global.u64 	%rd56, %rd28;
	add.s64 	%rd57, %rd55, %rd1;
	shl.b64 	%rd58, %rd57, 1;
	add.s64 	%rd59, %rd56, %rd58;
	add.s64 	%rd85, %rd59, 256;
	mul.wide.s32 	%rd60, %r269, 4;
	add.s64 	%rd21, %rd60, %rd51;
	mul.wide.s32 	%rd61, %r37, 4;
	add.s64 	%rd62, %rd60, %rd61;
	add.s64 	%rd22, %rd62, %rd51;
	cvta.to.global.u64 	%rd86, %rd29;

$L__BB67_6:
	ld.global.nc.u32 	%r103, [%rd85+-256];
	add.s64 	%rd63, %rd86, %rd21;
	ld.global.nc.u32 	%r104, [%rd63];
	// begin inline asm
	{mul.f16x2 %r102,%r103,%r104;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r105,%r278,%r102;
}
	// end inline asm
	add.s64 	%rd64, %rd86, %rd22;
	ld.global.nc.u32 	%r110, [%rd64];
	// begin inline asm
	{mul.f16x2 %r108,%r103,%r110;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r111,%r277,%r108;
}
	// end inline asm
	add.s64 	%rd65, %rd86, %rd18;
	ld.global.nc.u32 	%r116, [%rd65];
	// begin inline asm
	{mul.f16x2 %r114,%r103,%r116;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r117,%r276,%r114;
}
	// end inline asm
	add.s64 	%rd66, %rd86, %rd19;
	ld.global.nc.u32 	%r122, [%rd66];
	// begin inline asm
	{mul.f16x2 %r120,%r103,%r122;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r123,%r275,%r120;
}
	// end inline asm
	ld.global.nc.u32 	%r127, [%rd85+-128];
	ld.global.nc.u32 	%r128, [%rd63+128];
	// begin inline asm
	{mul.f16x2 %r126,%r127,%r128;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r129,%r105,%r126;
}
	// end inline asm
	add.s64 	%rd67, %rd86, %rd17;
	ld.global.nc.u32 	%r134, [%rd67];
	// begin inline asm
	{mul.f16x2 %r132,%r127,%r134;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r135,%r111,%r132;
}
	// end inline asm
	ld.global.nc.u32 	%r140, [%rd65+128];
	// begin inline asm
	{mul.f16x2 %r138,%r127,%r140;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r141,%r117,%r138;
}
	// end inline asm
	ld.global.nc.u32 	%r146, [%rd66+128];
	// begin inline asm
	{mul.f16x2 %r144,%r127,%r146;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r147,%r123,%r144;
}
	// end inline asm
	ld.global.nc.u32 	%r151, [%rd85];
	ld.global.nc.u32 	%r152, [%rd63+256];
	// begin inline asm
	{mul.f16x2 %r150,%r151,%r152;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r153,%r129,%r150;
}
	// end inline asm
	ld.global.nc.u32 	%r158, [%rd67+128];
	// begin inline asm
	{mul.f16x2 %r156,%r151,%r158;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r159,%r135,%r156;
}
	// end inline asm
	ld.global.nc.u32 	%r164, [%rd65+256];
	// begin inline asm
	{mul.f16x2 %r162,%r151,%r164;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r165,%r141,%r162;
}
	// end inline asm
	ld.global.nc.u32 	%r170, [%rd66+256];
	// begin inline asm
	{mul.f16x2 %r168,%r151,%r170;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r171,%r147,%r168;
}
	// end inline asm
	ld.global.nc.u32 	%r175, [%rd85+128];
	ld.global.nc.u32 	%r176, [%rd63+384];
	// begin inline asm
	{mul.f16x2 %r174,%r175,%r176;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r278,%r153,%r174;
}
	// end inline asm
	ld.global.nc.u32 	%r182, [%rd67+256];
	// begin inline asm
	{mul.f16x2 %r180,%r175,%r182;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r277,%r159,%r180;
}
	// end inline asm
	ld.global.nc.u32 	%r188, [%rd65+384];
	// begin inline asm
	{mul.f16x2 %r186,%r175,%r188;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r276,%r165,%r186;
}
	// end inline asm
	ld.global.nc.u32 	%r194, [%rd66+384];
	// begin inline asm
	{mul.f16x2 %r192,%r175,%r194;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r275,%r171,%r192;
}
	// end inline asm
	add.s64 	%rd86, %rd86, 512;
	add.s64 	%rd85, %rd85, 512;
	add.s32 	%r269, %r269, 128;
	setp.lt.s32 	%p5, %r269, %r36;
	@%p5 bra 	$L__BB67_6;

$L__BB67_7:
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r278;
  cvt.f32.f16 %f3, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r278;
  cvt.f32.f16 %f4, high;}

	// end inline asm
	add.f32 	%f11, %f3, %f4;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r277;
  cvt.f32.f16 %f5, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r277;
  cvt.f32.f16 %f6, high;}

	// end inline asm
	add.f32 	%f12, %f5, %f6;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r276;
  cvt.f32.f16 %f7, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r276;
  cvt.f32.f16 %f8, high;}

	// end inline asm
	add.f32 	%f13, %f7, %f8;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r275;
  cvt.f32.f16 %f9, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r275;
  cvt.f32.f16 %f10, high;}

	// end inline asm
	add.f32 	%f14, %f9, %f10;
	mov.b32 	%r206, %f11;
	mov.u32 	%r207, 31;
	mov.u32 	%r208, 16;
	mov.u32 	%r209, -1;
	shfl.sync.bfly.b32 	%r210|%p6, %r206, %r208, %r207, %r209;
	mov.b32 	%f15, %r210;
	add.f32 	%f16, %f11, %f15;
	mov.b32 	%r211, %f16;
	mov.u32 	%r212, 8;
	shfl.sync.bfly.b32 	%r213|%p7, %r211, %r212, %r207, %r209;
	mov.b32 	%f17, %r213;
	add.f32 	%f18, %f16, %f17;
	mov.b32 	%r214, %f18;
	mov.u32 	%r215, 4;
	shfl.sync.bfly.b32 	%r216|%p8, %r214, %r215, %r207, %r209;
	mov.b32 	%f19, %r216;
	add.f32 	%f20, %f18, %f19;
	mov.b32 	%r217, %f20;
	mov.u32 	%r218, 2;
	shfl.sync.bfly.b32 	%r219|%p9, %r217, %r218, %r207, %r209;
	mov.b32 	%f21, %r219;
	add.f32 	%f22, %f20, %f21;
	mov.b32 	%r220, %f22;
	mov.u32 	%r221, 1;
	shfl.sync.bfly.b32 	%r222|%p10, %r220, %r221, %r207, %r209;
	mov.b32 	%f23, %r222;
	add.f32 	%f24, %f22, %f23;
	st.local.f32 	[%rd32], %f24;
	mov.b32 	%r223, %f12;
	shfl.sync.bfly.b32 	%r224|%p11, %r223, %r208, %r207, %r209;
	mov.b32 	%f25, %r224;
	add.f32 	%f26, %f12, %f25;
	mov.b32 	%r225, %f26;
	shfl.sync.bfly.b32 	%r226|%p12, %r225, %r212, %r207, %r209;
	mov.b32 	%f27, %r226;
	add.f32 	%f28, %f26, %f27;
	mov.b32 	%r227, %f28;
	shfl.sync.bfly.b32 	%r228|%p13, %r227, %r215, %r207, %r209;
	mov.b32 	%f29, %r228;
	add.f32 	%f30, %f28, %f29;
	mov.b32 	%r229, %f30;
	shfl.sync.bfly.b32 	%r230|%p14, %r229, %r218, %r207, %r209;
	mov.b32 	%f31, %r230;
	add.f32 	%f32, %f30, %f31;
	mov.b32 	%r231, %f32;
	shfl.sync.bfly.b32 	%r232|%p15, %r231, %r221, %r207, %r209;
	mov.b32 	%f33, %r232;
	add.f32 	%f34, %f32, %f33;
	st.local.f32 	[%rd32+4], %f34;
	mov.b32 	%r233, %f13;
	shfl.sync.bfly.b32 	%r234|%p16, %r233, %r208, %r207, %r209;
	mov.b32 	%f35, %r234;
	add.f32 	%f36, %f13, %f35;
	mov.b32 	%r235, %f36;
	shfl.sync.bfly.b32 	%r236|%p17, %r235, %r212, %r207, %r209;
	mov.b32 	%f37, %r236;
	add.f32 	%f38, %f36, %f37;
	mov.b32 	%r237, %f38;
	shfl.sync.bfly.b32 	%r238|%p18, %r237, %r215, %r207, %r209;
	mov.b32 	%f39, %r238;
	add.f32 	%f40, %f38, %f39;
	mov.b32 	%r239, %f40;
	shfl.sync.bfly.b32 	%r240|%p19, %r239, %r218, %r207, %r209;
	mov.b32 	%f41, %r240;
	add.f32 	%f42, %f40, %f41;
	mov.b32 	%r241, %f42;
	shfl.sync.bfly.b32 	%r242|%p20, %r241, %r221, %r207, %r209;
	mov.b32 	%f43, %r242;
	add.f32 	%f44, %f42, %f43;
	st.local.f32 	[%rd32+8], %f44;
	mov.b32 	%r243, %f14;
	shfl.sync.bfly.b32 	%r244|%p21, %r243, %r208, %r207, %r209;
	mov.b32 	%f45, %r244;
	add.f32 	%f46, %f14, %f45;
	mov.b32 	%r245, %f46;
	shfl.sync.bfly.b32 	%r246|%p22, %r245, %r212, %r207, %r209;
	mov.b32 	%f47, %r246;
	add.f32 	%f48, %f46, %f47;
	mov.b32 	%r247, %f48;
	shfl.sync.bfly.b32 	%r248|%p23, %r247, %r215, %r207, %r209;
	mov.b32 	%f49, %r248;
	add.f32 	%f50, %f48, %f49;
	mov.b32 	%r249, %f50;
	shfl.sync.bfly.b32 	%r250|%p24, %r249, %r218, %r207, %r209;
	mov.b32 	%f51, %r250;
	add.f32 	%f52, %f50, %f51;
	mov.b32 	%r251, %f52;
	shfl.sync.bfly.b32 	%r252|%p25, %r251, %r221, %r207, %r209;
	mov.b32 	%f53, %r252;
	add.f32 	%f54, %f52, %f53;
	st.local.f32 	[%rd32+12], %f54;
	setp.gt.s32 	%p26, %r52, 3;
	@%p26 bra 	$L__BB67_9;

	mad.lo.s32 	%r256, %r52, %r38, %r49;
	cvt.s64.s32 	%rd70, %r256;
	mul.lo.s32 	%r258, %r47, %r40;
	cvt.s64.s32 	%rd71, %r258;
	add.s64 	%rd72, %rd71, %rd70;
	mul.wide.s32 	%rd75, %r52, 4;
	add.s64 	%rd76, %rd32, %rd75;
	ld.local.f32 	%f55, [%rd76];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs3, %f55;}

	// end inline asm
	cvta.to.global.u64 	%rd77, %rd30;
	shl.b64 	%rd78, %rd72, 1;
	add.s64 	%rd79, %rd77, %rd78;
	st.global.u16 	[%rd79], %rs3;

$L__BB67_9:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_5_bs_32
.visible .entry ggml_matvec_f16_ncols_5_bs_32(
	.param .u64 ggml_matvec_f16_ncols_5_bs_32_param_0,
	.param .u64 ggml_matvec_f16_ncols_5_bs_32_param_1,
	.param .u64 ggml_matvec_f16_ncols_5_bs_32_param_2,
	.param .u32 ggml_matvec_f16_ncols_5_bs_32_param_3,
	.param .u32 ggml_matvec_f16_ncols_5_bs_32_param_4,
	.param .u32 ggml_matvec_f16_ncols_5_bs_32_param_5,
	.param .u32 ggml_matvec_f16_ncols_5_bs_32_param_6,
	.param .u32 ggml_matvec_f16_ncols_5_bs_32_param_7,
	.param .u32 ggml_matvec_f16_ncols_5_bs_32_param_8,
	.param .u32 ggml_matvec_f16_ncols_5_bs_32_param_9,
	.param .u32 ggml_matvec_f16_ncols_5_bs_32_param_10,
	.param .u32 ggml_matvec_f16_ncols_5_bs_32_param_11
)
{
	.local .align 4 .b8 	__local_depot68[20];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<32>;
	.reg .b16 	%rs<4>;
	.reg .f32 	%f<69>;
	.reg .b32 	%r<323>;
	.reg .b64 	%rd<74>;


	mov.u64 	%SPL, __local_depot68;
	ld.param.u64 	%rd28, [ggml_matvec_f16_ncols_5_bs_32_param_0];
	ld.param.u64 	%rd29, [ggml_matvec_f16_ncols_5_bs_32_param_1];
	ld.param.u64 	%rd27, [ggml_matvec_f16_ncols_5_bs_32_param_2];
	ld.param.u32 	%r44, [ggml_matvec_f16_ncols_5_bs_32_param_3];
	ld.param.u32 	%r52, [ggml_matvec_f16_ncols_5_bs_32_param_5];
	ld.param.u32 	%r45, [ggml_matvec_f16_ncols_5_bs_32_param_6];
	ld.param.u32 	%r46, [ggml_matvec_f16_ncols_5_bs_32_param_7];
	ld.param.u32 	%r53, [ggml_matvec_f16_ncols_5_bs_32_param_8];
	ld.param.u32 	%r54, [ggml_matvec_f16_ncols_5_bs_32_param_9];
	ld.param.u32 	%r55, [ggml_matvec_f16_ncols_5_bs_32_param_10];
	ld.param.u32 	%r47, [ggml_matvec_f16_ncols_5_bs_32_param_11];
	cvta.to.global.u64 	%rd73, %rd29;
	cvta.to.global.u64 	%rd2, %rd28;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r56, %r1, %r53;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r57, %r2, %r52;
	mad.lo.s32 	%r58, %r56, %r54, %r57;
	cvt.s64.s32 	%rd4, %r58;
	mul.lo.s32 	%r59, %r1, %r55;
	cvt.s64.s32 	%rd5, %r59;
	mov.f32 	%f2, 0f00000000;
	mov.u32 	%r318, 0;
	st.local.u32 	[%rd3], %r318;
	st.local.u32 	[%rd3+4], %r318;
	st.local.u32 	[%rd3+8], %r318;
	st.local.u32 	[%rd3+12], %r318;
	st.local.u32 	[%rd3+16], %r318;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs2, %f2;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f2;}

	// end inline asm
	mov.b32 	%r322, {%rs1, %rs2};
	mov.u32 	%r4, %tid.x;
	setp.ge.s32 	%p1, %r4, %r44;
	mov.u32 	%r319, %r318;
	mov.u32 	%r320, %r318;
	mov.u32 	%r321, %r318;
	@%p1 bra 	$L__BB68_7;

	not.b32 	%r64, %r4;
	add.s32 	%r5, %r64, %r44;
	shr.u32 	%r65, %r5, 5;
	add.s32 	%r66, %r65, 1;
	and.b32  	%r305, %r66, 3;
	setp.eq.s32 	%p2, %r305, 0;
	mov.u32 	%r318, 0;
	mov.u32 	%r311, %r4;
	@%p2 bra 	$L__BB68_4;

	shl.b32 	%r71, %r45, 1;
	mad.lo.s32 	%r72, %r45, 3, %r4;
	mul.wide.s32 	%rd31, %r72, 4;
	shl.b64 	%rd32, %rd5, 1;
	add.s64 	%rd7, %rd31, %rd32;
	mul.wide.s32 	%rd33, %r4, 4;
	mul.wide.s32 	%rd34, %r45, 4;
	add.s64 	%rd35, %rd33, %rd34;
	add.s64 	%rd8, %rd35, %rd32;
	add.s64 	%rd9, %rd33, %rd32;
	mul.wide.s32 	%rd36, %r4, 2;
	add.s64 	%rd37, %rd36, %rd4;
	shl.b64 	%rd38, %rd37, 1;
	add.s64 	%rd70, %rd2, %rd38;
	mul.wide.s32 	%rd11, %r71, 4;
	mov.u32 	%r318, 0;
	mov.u64 	%rd71, %rd73;
	mov.u32 	%r319, %r318;
	mov.u32 	%r320, %r318;
	mov.u32 	%r321, %r318;
	mov.u32 	%r311, %r4;

$L__BB68_3:
	.pragma "nounroll";
	ld.global.nc.u32 	%r74, [%rd70];
	add.s64 	%rd39, %rd71, %rd9;
	ld.global.nc.u32 	%r75, [%rd39];
	// begin inline asm
	{mul.f16x2 %r73,%r74,%r75;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r322,%r322,%r73;
}
	// end inline asm
	add.s64 	%rd40, %rd71, %rd8;
	ld.global.nc.u32 	%r81, [%rd40];
	// begin inline asm
	{mul.f16x2 %r79,%r74,%r81;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r321,%r321,%r79;
}
	// end inline asm
	add.s64 	%rd41, %rd39, %rd11;
	ld.global.nc.u32 	%r87, [%rd41];
	// begin inline asm
	{mul.f16x2 %r85,%r74,%r87;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r320,%r320,%r85;
}
	// end inline asm
	add.s64 	%rd42, %rd71, %rd7;
	ld.global.nc.u32 	%r93, [%rd42];
	// begin inline asm
	{mul.f16x2 %r91,%r74,%r93;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r319,%r319,%r91;
}
	// end inline asm
	add.s64 	%rd43, %rd41, %rd11;
	ld.global.nc.u32 	%r99, [%rd43];
	// begin inline asm
	{mul.f16x2 %r97,%r74,%r99;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r318,%r318,%r97;
}
	// end inline asm
	add.s32 	%r311, %r311, 32;
	add.s64 	%rd71, %rd71, 128;
	add.s64 	%rd70, %rd70, 128;
	add.s32 	%r305, %r305, -1;
	setp.ne.s32 	%p3, %r305, 0;
	@%p3 bra 	$L__BB68_3;

$L__BB68_4:
	setp.lt.u32 	%p4, %r5, 96;
	@%p4 bra 	$L__BB68_7;

	add.s32 	%r103, %r311, %r45;
	shl.b32 	%r104, %r45, 1;
	add.s32 	%r105, %r311, %r104;
	mad.lo.s32 	%r106, %r45, 3, %r311;
	shl.b32 	%r107, %r45, 2;
	add.s32 	%r108, %r311, %r107;
	add.s32 	%r109, %r103, 32;
	mul.wide.s32 	%rd44, %r109, 4;
	shl.b64 	%rd45, %rd5, 1;
	add.s64 	%rd16, %rd44, %rd45;
	mul.wide.s32 	%rd46, %r105, 4;
	add.s64 	%rd17, %rd46, %rd45;
	mul.wide.s32 	%rd47, %r106, 4;
	add.s64 	%rd18, %rd47, %rd45;
	mul.wide.s32 	%rd48, %r108, 4;
	add.s64 	%rd19, %rd48, %rd45;
	mul.wide.s32 	%rd49, %r311, 2;
	add.s64 	%rd50, %rd49, %rd4;
	shl.b64 	%rd51, %rd50, 1;
	add.s64 	%rd52, %rd2, %rd51;
	add.s64 	%rd72, %rd52, 256;
	mul.wide.s32 	%rd53, %r311, 4;
	add.s64 	%rd21, %rd53, %rd45;
	mul.wide.s32 	%rd54, %r45, 4;
	add.s64 	%rd55, %rd53, %rd54;
	add.s64 	%rd22, %rd55, %rd45;

$L__BB68_6:
	ld.global.nc.u32 	%r111, [%rd72+-256];
	add.s64 	%rd56, %rd73, %rd21;
	ld.global.nc.u32 	%r112, [%rd56];
	// begin inline asm
	{mul.f16x2 %r110,%r111,%r112;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r113,%r322,%r110;
}
	// end inline asm
	add.s64 	%rd57, %rd73, %rd22;
	ld.global.nc.u32 	%r118, [%rd57];
	// begin inline asm
	{mul.f16x2 %r116,%r111,%r118;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r119,%r321,%r116;
}
	// end inline asm
	add.s64 	%rd58, %rd73, %rd17;
	ld.global.nc.u32 	%r124, [%rd58];
	// begin inline asm
	{mul.f16x2 %r122,%r111,%r124;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r125,%r320,%r122;
}
	// end inline asm
	add.s64 	%rd59, %rd73, %rd18;
	ld.global.nc.u32 	%r130, [%rd59];
	// begin inline asm
	{mul.f16x2 %r128,%r111,%r130;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r131,%r319,%r128;
}
	// end inline asm
	add.s64 	%rd60, %rd73, %rd19;
	ld.global.nc.u32 	%r136, [%rd60];
	// begin inline asm
	{mul.f16x2 %r134,%r111,%r136;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r137,%r318,%r134;
}
	// end inline asm
	ld.global.nc.u32 	%r141, [%rd72+-128];
	ld.global.nc.u32 	%r142, [%rd56+128];
	// begin inline asm
	{mul.f16x2 %r140,%r141,%r142;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r143,%r113,%r140;
}
	// end inline asm
	add.s64 	%rd61, %rd73, %rd16;
	ld.global.nc.u32 	%r148, [%rd61];
	// begin inline asm
	{mul.f16x2 %r146,%r141,%r148;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r149,%r119,%r146;
}
	// end inline asm
	ld.global.nc.u32 	%r154, [%rd58+128];
	// begin inline asm
	{mul.f16x2 %r152,%r141,%r154;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r155,%r125,%r152;
}
	// end inline asm
	ld.global.nc.u32 	%r160, [%rd59+128];
	// begin inline asm
	{mul.f16x2 %r158,%r141,%r160;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r161,%r131,%r158;
}
	// end inline asm
	ld.global.nc.u32 	%r166, [%rd60+128];
	// begin inline asm
	{mul.f16x2 %r164,%r141,%r166;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r167,%r137,%r164;
}
	// end inline asm
	ld.global.nc.u32 	%r171, [%rd72];
	ld.global.nc.u32 	%r172, [%rd56+256];
	// begin inline asm
	{mul.f16x2 %r170,%r171,%r172;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r173,%r143,%r170;
}
	// end inline asm
	ld.global.nc.u32 	%r178, [%rd61+128];
	// begin inline asm
	{mul.f16x2 %r176,%r171,%r178;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r179,%r149,%r176;
}
	// end inline asm
	ld.global.nc.u32 	%r184, [%rd58+256];
	// begin inline asm
	{mul.f16x2 %r182,%r171,%r184;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r185,%r155,%r182;
}
	// end inline asm
	ld.global.nc.u32 	%r190, [%rd59+256];
	// begin inline asm
	{mul.f16x2 %r188,%r171,%r190;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r191,%r161,%r188;
}
	// end inline asm
	ld.global.nc.u32 	%r196, [%rd60+256];
	// begin inline asm
	{mul.f16x2 %r194,%r171,%r196;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r197,%r167,%r194;
}
	// end inline asm
	ld.global.nc.u32 	%r201, [%rd72+128];
	ld.global.nc.u32 	%r202, [%rd56+384];
	// begin inline asm
	{mul.f16x2 %r200,%r201,%r202;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r322,%r173,%r200;
}
	// end inline asm
	ld.global.nc.u32 	%r208, [%rd61+256];
	// begin inline asm
	{mul.f16x2 %r206,%r201,%r208;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r321,%r179,%r206;
}
	// end inline asm
	ld.global.nc.u32 	%r214, [%rd58+384];
	// begin inline asm
	{mul.f16x2 %r212,%r201,%r214;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r320,%r185,%r212;
}
	// end inline asm
	ld.global.nc.u32 	%r220, [%rd59+384];
	// begin inline asm
	{mul.f16x2 %r218,%r201,%r220;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r319,%r191,%r218;
}
	// end inline asm
	ld.global.nc.u32 	%r226, [%rd60+384];
	// begin inline asm
	{mul.f16x2 %r224,%r201,%r226;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r318,%r197,%r224;
}
	// end inline asm
	add.s64 	%rd73, %rd73, 512;
	add.s64 	%rd72, %rd72, 512;
	add.s32 	%r311, %r311, 128;
	setp.lt.s32 	%p5, %r311, %r44;
	@%p5 bra 	$L__BB68_6;

$L__BB68_7:
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r322;
  cvt.f32.f16 %f3, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r322;
  cvt.f32.f16 %f4, high;}

	// end inline asm
	add.f32 	%f13, %f3, %f4;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r321;
  cvt.f32.f16 %f5, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r321;
  cvt.f32.f16 %f6, high;}

	// end inline asm
	add.f32 	%f14, %f5, %f6;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r320;
  cvt.f32.f16 %f7, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r320;
  cvt.f32.f16 %f8, high;}

	// end inline asm
	add.f32 	%f15, %f7, %f8;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r319;
  cvt.f32.f16 %f9, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r319;
  cvt.f32.f16 %f10, high;}

	// end inline asm
	add.f32 	%f16, %f9, %f10;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r318;
  cvt.f32.f16 %f11, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r318;
  cvt.f32.f16 %f12, high;}

	// end inline asm
	add.f32 	%f17, %f11, %f12;
	mov.b32 	%r240, %f13;
	mov.u32 	%r241, 31;
	mov.u32 	%r242, 16;
	mov.u32 	%r243, -1;
	shfl.sync.bfly.b32 	%r244|%p6, %r240, %r242, %r241, %r243;
	mov.b32 	%f18, %r244;
	add.f32 	%f19, %f13, %f18;
	mov.b32 	%r245, %f19;
	mov.u32 	%r246, 8;
	shfl.sync.bfly.b32 	%r247|%p7, %r245, %r246, %r241, %r243;
	mov.b32 	%f20, %r247;
	add.f32 	%f21, %f19, %f20;
	mov.b32 	%r248, %f21;
	mov.u32 	%r249, 4;
	shfl.sync.bfly.b32 	%r250|%p8, %r248, %r249, %r241, %r243;
	mov.b32 	%f22, %r250;
	add.f32 	%f23, %f21, %f22;
	mov.b32 	%r251, %f23;
	mov.u32 	%r252, 2;
	shfl.sync.bfly.b32 	%r253|%p9, %r251, %r252, %r241, %r243;
	mov.b32 	%f24, %r253;
	add.f32 	%f25, %f23, %f24;
	mov.b32 	%r254, %f25;
	mov.u32 	%r255, 1;
	shfl.sync.bfly.b32 	%r256|%p10, %r254, %r255, %r241, %r243;
	mov.b32 	%f26, %r256;
	add.f32 	%f27, %f25, %f26;
	st.local.f32 	[%rd3], %f27;
	mov.b32 	%r257, %f14;
	shfl.sync.bfly.b32 	%r258|%p11, %r257, %r242, %r241, %r243;
	mov.b32 	%f28, %r258;
	add.f32 	%f29, %f14, %f28;
	mov.b32 	%r259, %f29;
	shfl.sync.bfly.b32 	%r260|%p12, %r259, %r246, %r241, %r243;
	mov.b32 	%f30, %r260;
	add.f32 	%f31, %f29, %f30;
	mov.b32 	%r261, %f31;
	shfl.sync.bfly.b32 	%r262|%p13, %r261, %r249, %r241, %r243;
	mov.b32 	%f32, %r262;
	add.f32 	%f33, %f31, %f32;
	mov.b32 	%r263, %f33;
	shfl.sync.bfly.b32 	%r264|%p14, %r263, %r252, %r241, %r243;
	mov.b32 	%f34, %r264;
	add.f32 	%f35, %f33, %f34;
	mov.b32 	%r265, %f35;
	shfl.sync.bfly.b32 	%r266|%p15, %r265, %r255, %r241, %r243;
	mov.b32 	%f36, %r266;
	add.f32 	%f37, %f35, %f36;
	st.local.f32 	[%rd3+4], %f37;
	mov.b32 	%r267, %f15;
	shfl.sync.bfly.b32 	%r268|%p16, %r267, %r242, %r241, %r243;
	mov.b32 	%f38, %r268;
	add.f32 	%f39, %f15, %f38;
	mov.b32 	%r269, %f39;
	shfl.sync.bfly.b32 	%r270|%p17, %r269, %r246, %r241, %r243;
	mov.b32 	%f40, %r270;
	add.f32 	%f41, %f39, %f40;
	mov.b32 	%r271, %f41;
	shfl.sync.bfly.b32 	%r272|%p18, %r271, %r249, %r241, %r243;
	mov.b32 	%f42, %r272;
	add.f32 	%f43, %f41, %f42;
	mov.b32 	%r273, %f43;
	shfl.sync.bfly.b32 	%r274|%p19, %r273, %r252, %r241, %r243;
	mov.b32 	%f44, %r274;
	add.f32 	%f45, %f43, %f44;
	mov.b32 	%r275, %f45;
	shfl.sync.bfly.b32 	%r276|%p20, %r275, %r255, %r241, %r243;
	mov.b32 	%f46, %r276;
	add.f32 	%f47, %f45, %f46;
	st.local.f32 	[%rd3+8], %f47;
	mov.b32 	%r277, %f16;
	shfl.sync.bfly.b32 	%r278|%p21, %r277, %r242, %r241, %r243;
	mov.b32 	%f48, %r278;
	add.f32 	%f49, %f16, %f48;
	mov.b32 	%r279, %f49;
	shfl.sync.bfly.b32 	%r280|%p22, %r279, %r246, %r241, %r243;
	mov.b32 	%f50, %r280;
	add.f32 	%f51, %f49, %f50;
	mov.b32 	%r281, %f51;
	shfl.sync.bfly.b32 	%r282|%p23, %r281, %r249, %r241, %r243;
	mov.b32 	%f52, %r282;
	add.f32 	%f53, %f51, %f52;
	mov.b32 	%r283, %f53;
	shfl.sync.bfly.b32 	%r284|%p24, %r283, %r252, %r241, %r243;
	mov.b32 	%f54, %r284;
	add.f32 	%f55, %f53, %f54;
	mov.b32 	%r285, %f55;
	shfl.sync.bfly.b32 	%r286|%p25, %r285, %r255, %r241, %r243;
	mov.b32 	%f56, %r286;
	add.f32 	%f57, %f55, %f56;
	st.local.f32 	[%rd3+12], %f57;
	mov.b32 	%r287, %f17;
	shfl.sync.bfly.b32 	%r288|%p26, %r287, %r242, %r241, %r243;
	mov.b32 	%f58, %r288;
	add.f32 	%f59, %f17, %f58;
	mov.b32 	%r289, %f59;
	shfl.sync.bfly.b32 	%r290|%p27, %r289, %r246, %r241, %r243;
	mov.b32 	%f60, %r290;
	add.f32 	%f61, %f59, %f60;
	mov.b32 	%r291, %f61;
	shfl.sync.bfly.b32 	%r292|%p28, %r291, %r249, %r241, %r243;
	mov.b32 	%f62, %r292;
	add.f32 	%f63, %f61, %f62;
	mov.b32 	%r293, %f63;
	shfl.sync.bfly.b32 	%r294|%p29, %r293, %r252, %r241, %r243;
	mov.b32 	%f64, %r294;
	add.f32 	%f65, %f63, %f64;
	mov.b32 	%r295, %f65;
	shfl.sync.bfly.b32 	%r296|%p30, %r295, %r255, %r241, %r243;
	mov.b32 	%f66, %r296;
	add.f32 	%f67, %f65, %f66;
	st.local.f32 	[%rd3+16], %f67;
	setp.gt.s32 	%p31, %r4, 4;
	@%p31 bra 	$L__BB68_9;

	mad.lo.s32 	%r297, %r4, %r46, %r2;
	cvt.s64.s32 	%rd62, %r297;
	mul.lo.s32 	%r298, %r1, %r47;
	cvt.s64.s32 	%rd63, %r298;
	add.s64 	%rd64, %rd63, %rd62;
	mul.wide.s32 	%rd65, %r4, 4;
	add.s64 	%rd66, %rd3, %rd65;
	ld.local.f32 	%f68, [%rd66];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs3, %f68;}

	// end inline asm
	cvta.to.global.u64 	%rd67, %rd27;
	shl.b64 	%rd68, %rd64, 1;
	add.s64 	%rd69, %rd67, %rd68;
	st.global.u16 	[%rd69], %rs3;

$L__BB68_9:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_6_bs_32
.visible .entry ggml_matvec_f16_ncols_6_bs_32(
	.param .u64 ggml_matvec_f16_ncols_6_bs_32_param_0,
	.param .u64 ggml_matvec_f16_ncols_6_bs_32_param_1,
	.param .u64 ggml_matvec_f16_ncols_6_bs_32_param_2,
	.param .u32 ggml_matvec_f16_ncols_6_bs_32_param_3,
	.param .u32 ggml_matvec_f16_ncols_6_bs_32_param_4,
	.param .u32 ggml_matvec_f16_ncols_6_bs_32_param_5,
	.param .u32 ggml_matvec_f16_ncols_6_bs_32_param_6,
	.param .u32 ggml_matvec_f16_ncols_6_bs_32_param_7,
	.param .u32 ggml_matvec_f16_ncols_6_bs_32_param_8,
	.param .u32 ggml_matvec_f16_ncols_6_bs_32_param_9,
	.param .u32 ggml_matvec_f16_ncols_6_bs_32_param_10,
	.param .u32 ggml_matvec_f16_ncols_6_bs_32_param_11
)
{
	.local .align 8 .b8 	__local_depot69[24];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<37>;
	.reg .b16 	%rs<4>;
	.reg .f32 	%f<82>;
	.reg .b32 	%r<379>;
	.reg .b64 	%rd<77>;


	mov.u64 	%SPL, __local_depot69;
	ld.param.u64 	%rd29, [ggml_matvec_f16_ncols_6_bs_32_param_0];
	ld.param.u64 	%rd30, [ggml_matvec_f16_ncols_6_bs_32_param_1];
	ld.param.u64 	%rd28, [ggml_matvec_f16_ncols_6_bs_32_param_2];
	ld.param.u32 	%r50, [ggml_matvec_f16_ncols_6_bs_32_param_3];
	ld.param.u32 	%r59, [ggml_matvec_f16_ncols_6_bs_32_param_5];
	ld.param.u32 	%r51, [ggml_matvec_f16_ncols_6_bs_32_param_6];
	ld.param.u32 	%r52, [ggml_matvec_f16_ncols_6_bs_32_param_7];
	ld.param.u32 	%r60, [ggml_matvec_f16_ncols_6_bs_32_param_8];
	ld.param.u32 	%r61, [ggml_matvec_f16_ncols_6_bs_32_param_9];
	ld.param.u32 	%r62, [ggml_matvec_f16_ncols_6_bs_32_param_10];
	ld.param.u32 	%r53, [ggml_matvec_f16_ncols_6_bs_32_param_11];
	cvta.to.global.u64 	%rd76, %rd30;
	cvta.to.global.u64 	%rd2, %rd29;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r63, %r1, %r60;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r64, %r2, %r59;
	mad.lo.s32 	%r65, %r63, %r61, %r64;
	cvt.s64.s32 	%rd4, %r65;
	mul.lo.s32 	%r66, %r1, %r62;
	cvt.s64.s32 	%rd5, %r66;
	mov.f32 	%f2, 0f00000000;
	st.local.v2.f32 	[%rd3], {%f2, %f2};
	st.local.v2.f32 	[%rd3+8], {%f2, %f2};
	st.local.v2.f32 	[%rd3+16], {%f2, %f2};
	mov.u32 	%r373, 0;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs2, %f2;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f2;}

	// end inline asm
	mov.b32 	%r378, {%rs1, %rs2};
	mov.u32 	%r4, %tid.x;
	setp.ge.s32 	%p1, %r4, %r50;
	mov.u32 	%r374, %r373;
	mov.u32 	%r375, %r373;
	mov.u32 	%r376, %r373;
	mov.u32 	%r377, %r373;
	@%p1 bra 	$L__BB69_7;

	not.b32 	%r72, %r4;
	add.s32 	%r5, %r72, %r50;
	shr.u32 	%r73, %r5, 5;
	add.s32 	%r74, %r73, 1;
	and.b32  	%r358, %r74, 3;
	setp.eq.s32 	%p2, %r358, 0;
	mov.u32 	%r373, 0;
	mov.u32 	%r365, %r4;
	@%p2 bra 	$L__BB69_4;

	shl.b32 	%r80, %r51, 1;
	add.s32 	%r81, %r4, %r80;
	mul.wide.s32 	%rd32, %r81, 4;
	shl.b64 	%rd33, %rd5, 1;
	add.s64 	%rd7, %rd32, %rd33;
	mul.wide.s32 	%rd34, %r4, 4;
	mul.wide.s32 	%rd8, %r51, 4;
	add.s64 	%rd35, %rd34, %rd8;
	add.s64 	%rd9, %rd35, %rd33;
	add.s64 	%rd10, %rd34, %rd33;
	mul.wide.s32 	%rd36, %r4, 2;
	add.s64 	%rd37, %rd36, %rd4;
	shl.b64 	%rd38, %rd37, 1;
	add.s64 	%rd73, %rd2, %rd38;
	mov.u32 	%r373, 0;
	mov.u64 	%rd74, %rd76;
	mov.u32 	%r374, %r373;
	mov.u32 	%r375, %r373;
	mov.u32 	%r376, %r373;
	mov.u32 	%r377, %r373;
	mov.u32 	%r365, %r4;

$L__BB69_3:
	.pragma "nounroll";
	ld.global.nc.u32 	%r83, [%rd73];
	add.s64 	%rd39, %rd74, %rd10;
	ld.global.nc.u32 	%r84, [%rd39];
	// begin inline asm
	{mul.f16x2 %r82,%r83,%r84;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r378,%r378,%r82;
}
	// end inline asm
	add.s64 	%rd40, %rd74, %rd9;
	ld.global.nc.u32 	%r90, [%rd40];
	// begin inline asm
	{mul.f16x2 %r88,%r83,%r90;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r377,%r377,%r88;
}
	// end inline asm
	add.s64 	%rd41, %rd74, %rd7;
	ld.global.nc.u32 	%r96, [%rd41];
	// begin inline asm
	{mul.f16x2 %r94,%r83,%r96;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r376,%r376,%r94;
}
	// end inline asm
	add.s64 	%rd42, %rd41, %rd8;
	ld.global.nc.u32 	%r102, [%rd42];
	// begin inline asm
	{mul.f16x2 %r100,%r83,%r102;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r375,%r375,%r100;
}
	// end inline asm
	add.s64 	%rd43, %rd42, %rd8;
	ld.global.nc.u32 	%r108, [%rd43];
	// begin inline asm
	{mul.f16x2 %r106,%r83,%r108;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r374,%r374,%r106;
}
	// end inline asm
	add.s64 	%rd44, %rd43, %rd8;
	ld.global.nc.u32 	%r114, [%rd44];
	// begin inline asm
	{mul.f16x2 %r112,%r83,%r114;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r373,%r373,%r112;
}
	// end inline asm
	add.s32 	%r365, %r365, 32;
	add.s64 	%rd74, %rd74, 128;
	add.s64 	%rd73, %rd73, 128;
	add.s32 	%r358, %r358, -1;
	setp.ne.s32 	%p3, %r358, 0;
	@%p3 bra 	$L__BB69_3;

$L__BB69_4:
	setp.lt.u32 	%p4, %r5, 96;
	@%p4 bra 	$L__BB69_7;

	add.s32 	%r118, %r365, %r51;
	shl.b32 	%r119, %r51, 1;
	add.s32 	%r120, %r365, %r119;
	mad.lo.s32 	%r121, %r51, 3, %r365;
	shl.b32 	%r122, %r51, 2;
	add.s32 	%r123, %r365, %r122;
	mad.lo.s32 	%r124, %r51, 5, %r365;
	add.s32 	%r125, %r118, 32;
	mul.wide.s32 	%rd45, %r125, 4;
	shl.b64 	%rd46, %rd5, 1;
	add.s64 	%rd16, %rd45, %rd46;
	mul.wide.s32 	%rd47, %r120, 4;
	add.s64 	%rd17, %rd47, %rd46;
	mul.wide.s32 	%rd48, %r121, 4;
	add.s64 	%rd18, %rd48, %rd46;
	mul.wide.s32 	%rd49, %r123, 4;
	add.s64 	%rd19, %rd49, %rd46;
	mul.wide.s32 	%rd50, %r124, 4;
	add.s64 	%rd20, %rd50, %rd46;
	mul.wide.s32 	%rd51, %r365, 2;
	add.s64 	%rd52, %rd51, %rd4;
	shl.b64 	%rd53, %rd52, 1;
	add.s64 	%rd54, %rd2, %rd53;
	add.s64 	%rd75, %rd54, 256;
	mul.wide.s32 	%rd55, %r365, 4;
	add.s64 	%rd22, %rd55, %rd46;
	mul.wide.s32 	%rd56, %r51, 4;
	add.s64 	%rd57, %rd55, %rd56;
	add.s64 	%rd23, %rd57, %rd46;

$L__BB69_6:
	ld.global.nc.u32 	%r127, [%rd75+-256];
	add.s64 	%rd58, %rd76, %rd22;
	ld.global.nc.u32 	%r128, [%rd58];
	// begin inline asm
	{mul.f16x2 %r126,%r127,%r128;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r129,%r378,%r126;
}
	// end inline asm
	add.s64 	%rd59, %rd76, %rd23;
	ld.global.nc.u32 	%r134, [%rd59];
	// begin inline asm
	{mul.f16x2 %r132,%r127,%r134;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r135,%r377,%r132;
}
	// end inline asm
	add.s64 	%rd60, %rd76, %rd17;
	ld.global.nc.u32 	%r140, [%rd60];
	// begin inline asm
	{mul.f16x2 %r138,%r127,%r140;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r141,%r376,%r138;
}
	// end inline asm
	add.s64 	%rd61, %rd76, %rd18;
	ld.global.nc.u32 	%r146, [%rd61];
	// begin inline asm
	{mul.f16x2 %r144,%r127,%r146;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r147,%r375,%r144;
}
	// end inline asm
	add.s64 	%rd62, %rd76, %rd19;
	ld.global.nc.u32 	%r152, [%rd62];
	// begin inline asm
	{mul.f16x2 %r150,%r127,%r152;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r153,%r374,%r150;
}
	// end inline asm
	add.s64 	%rd63, %rd76, %rd20;
	ld.global.nc.u32 	%r158, [%rd63];
	// begin inline asm
	{mul.f16x2 %r156,%r127,%r158;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r159,%r373,%r156;
}
	// end inline asm
	ld.global.nc.u32 	%r163, [%rd75+-128];
	ld.global.nc.u32 	%r164, [%rd58+128];
	// begin inline asm
	{mul.f16x2 %r162,%r163,%r164;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r165,%r129,%r162;
}
	// end inline asm
	add.s64 	%rd64, %rd76, %rd16;
	ld.global.nc.u32 	%r170, [%rd64];
	// begin inline asm
	{mul.f16x2 %r168,%r163,%r170;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r171,%r135,%r168;
}
	// end inline asm
	ld.global.nc.u32 	%r176, [%rd60+128];
	// begin inline asm
	{mul.f16x2 %r174,%r163,%r176;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r177,%r141,%r174;
}
	// end inline asm
	ld.global.nc.u32 	%r182, [%rd61+128];
	// begin inline asm
	{mul.f16x2 %r180,%r163,%r182;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r183,%r147,%r180;
}
	// end inline asm
	ld.global.nc.u32 	%r188, [%rd62+128];
	// begin inline asm
	{mul.f16x2 %r186,%r163,%r188;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r189,%r153,%r186;
}
	// end inline asm
	ld.global.nc.u32 	%r194, [%rd63+128];
	// begin inline asm
	{mul.f16x2 %r192,%r163,%r194;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r195,%r159,%r192;
}
	// end inline asm
	ld.global.nc.u32 	%r199, [%rd75];
	ld.global.nc.u32 	%r200, [%rd58+256];
	// begin inline asm
	{mul.f16x2 %r198,%r199,%r200;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r201,%r165,%r198;
}
	// end inline asm
	ld.global.nc.u32 	%r206, [%rd64+128];
	// begin inline asm
	{mul.f16x2 %r204,%r199,%r206;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r207,%r171,%r204;
}
	// end inline asm
	ld.global.nc.u32 	%r212, [%rd60+256];
	// begin inline asm
	{mul.f16x2 %r210,%r199,%r212;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r213,%r177,%r210;
}
	// end inline asm
	ld.global.nc.u32 	%r218, [%rd61+256];
	// begin inline asm
	{mul.f16x2 %r216,%r199,%r218;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r219,%r183,%r216;
}
	// end inline asm
	ld.global.nc.u32 	%r224, [%rd62+256];
	// begin inline asm
	{mul.f16x2 %r222,%r199,%r224;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r225,%r189,%r222;
}
	// end inline asm
	ld.global.nc.u32 	%r230, [%rd63+256];
	// begin inline asm
	{mul.f16x2 %r228,%r199,%r230;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r231,%r195,%r228;
}
	// end inline asm
	ld.global.nc.u32 	%r235, [%rd75+128];
	ld.global.nc.u32 	%r236, [%rd58+384];
	// begin inline asm
	{mul.f16x2 %r234,%r235,%r236;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r378,%r201,%r234;
}
	// end inline asm
	ld.global.nc.u32 	%r242, [%rd64+256];
	// begin inline asm
	{mul.f16x2 %r240,%r235,%r242;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r377,%r207,%r240;
}
	// end inline asm
	ld.global.nc.u32 	%r248, [%rd60+384];
	// begin inline asm
	{mul.f16x2 %r246,%r235,%r248;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r376,%r213,%r246;
}
	// end inline asm
	ld.global.nc.u32 	%r254, [%rd61+384];
	// begin inline asm
	{mul.f16x2 %r252,%r235,%r254;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r375,%r219,%r252;
}
	// end inline asm
	ld.global.nc.u32 	%r260, [%rd62+384];
	// begin inline asm
	{mul.f16x2 %r258,%r235,%r260;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r374,%r225,%r258;
}
	// end inline asm
	ld.global.nc.u32 	%r266, [%rd63+384];
	// begin inline asm
	{mul.f16x2 %r264,%r235,%r266;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r373,%r231,%r264;
}
	// end inline asm
	add.s64 	%rd76, %rd76, 512;
	add.s64 	%rd75, %rd75, 512;
	add.s32 	%r365, %r365, 128;
	setp.lt.s32 	%p5, %r365, %r50;
	@%p5 bra 	$L__BB69_6;

$L__BB69_7:
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r378;
  cvt.f32.f16 %f3, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r378;
  cvt.f32.f16 %f4, high;}

	// end inline asm
	add.f32 	%f15, %f3, %f4;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r377;
  cvt.f32.f16 %f5, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r377;
  cvt.f32.f16 %f6, high;}

	// end inline asm
	add.f32 	%f16, %f5, %f6;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r376;
  cvt.f32.f16 %f7, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r376;
  cvt.f32.f16 %f8, high;}

	// end inline asm
	add.f32 	%f17, %f7, %f8;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r375;
  cvt.f32.f16 %f9, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r375;
  cvt.f32.f16 %f10, high;}

	// end inline asm
	add.f32 	%f18, %f9, %f10;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r374;
  cvt.f32.f16 %f11, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r374;
  cvt.f32.f16 %f12, high;}

	// end inline asm
	add.f32 	%f19, %f11, %f12;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r373;
  cvt.f32.f16 %f13, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r373;
  cvt.f32.f16 %f14, high;}

	// end inline asm
	add.f32 	%f20, %f13, %f14;
	mov.b32 	%r282, %f15;
	mov.u32 	%r283, 31;
	mov.u32 	%r284, 16;
	mov.u32 	%r285, -1;
	shfl.sync.bfly.b32 	%r286|%p6, %r282, %r284, %r283, %r285;
	mov.b32 	%f21, %r286;
	add.f32 	%f22, %f15, %f21;
	mov.b32 	%r287, %f22;
	mov.u32 	%r288, 8;
	shfl.sync.bfly.b32 	%r289|%p7, %r287, %r288, %r283, %r285;
	mov.b32 	%f23, %r289;
	add.f32 	%f24, %f22, %f23;
	mov.b32 	%r290, %f24;
	mov.u32 	%r291, 4;
	shfl.sync.bfly.b32 	%r292|%p8, %r290, %r291, %r283, %r285;
	mov.b32 	%f25, %r292;
	add.f32 	%f26, %f24, %f25;
	mov.b32 	%r293, %f26;
	mov.u32 	%r294, 2;
	shfl.sync.bfly.b32 	%r295|%p9, %r293, %r294, %r283, %r285;
	mov.b32 	%f27, %r295;
	add.f32 	%f28, %f26, %f27;
	mov.b32 	%r296, %f28;
	mov.u32 	%r297, 1;
	shfl.sync.bfly.b32 	%r298|%p10, %r296, %r297, %r283, %r285;
	mov.b32 	%f29, %r298;
	add.f32 	%f30, %f28, %f29;
	st.local.f32 	[%rd3], %f30;
	mov.b32 	%r299, %f16;
	shfl.sync.bfly.b32 	%r300|%p11, %r299, %r284, %r283, %r285;
	mov.b32 	%f31, %r300;
	add.f32 	%f32, %f16, %f31;
	mov.b32 	%r301, %f32;
	shfl.sync.bfly.b32 	%r302|%p12, %r301, %r288, %r283, %r285;
	mov.b32 	%f33, %r302;
	add.f32 	%f34, %f32, %f33;
	mov.b32 	%r303, %f34;
	shfl.sync.bfly.b32 	%r304|%p13, %r303, %r291, %r283, %r285;
	mov.b32 	%f35, %r304;
	add.f32 	%f36, %f34, %f35;
	mov.b32 	%r305, %f36;
	shfl.sync.bfly.b32 	%r306|%p14, %r305, %r294, %r283, %r285;
	mov.b32 	%f37, %r306;
	add.f32 	%f38, %f36, %f37;
	mov.b32 	%r307, %f38;
	shfl.sync.bfly.b32 	%r308|%p15, %r307, %r297, %r283, %r285;
	mov.b32 	%f39, %r308;
	add.f32 	%f40, %f38, %f39;
	st.local.f32 	[%rd3+4], %f40;
	mov.b32 	%r309, %f17;
	shfl.sync.bfly.b32 	%r310|%p16, %r309, %r284, %r283, %r285;
	mov.b32 	%f41, %r310;
	add.f32 	%f42, %f17, %f41;
	mov.b32 	%r311, %f42;
	shfl.sync.bfly.b32 	%r312|%p17, %r311, %r288, %r283, %r285;
	mov.b32 	%f43, %r312;
	add.f32 	%f44, %f42, %f43;
	mov.b32 	%r313, %f44;
	shfl.sync.bfly.b32 	%r314|%p18, %r313, %r291, %r283, %r285;
	mov.b32 	%f45, %r314;
	add.f32 	%f46, %f44, %f45;
	mov.b32 	%r315, %f46;
	shfl.sync.bfly.b32 	%r316|%p19, %r315, %r294, %r283, %r285;
	mov.b32 	%f47, %r316;
	add.f32 	%f48, %f46, %f47;
	mov.b32 	%r317, %f48;
	shfl.sync.bfly.b32 	%r318|%p20, %r317, %r297, %r283, %r285;
	mov.b32 	%f49, %r318;
	add.f32 	%f50, %f48, %f49;
	st.local.f32 	[%rd3+8], %f50;
	mov.b32 	%r319, %f18;
	shfl.sync.bfly.b32 	%r320|%p21, %r319, %r284, %r283, %r285;
	mov.b32 	%f51, %r320;
	add.f32 	%f52, %f18, %f51;
	mov.b32 	%r321, %f52;
	shfl.sync.bfly.b32 	%r322|%p22, %r321, %r288, %r283, %r285;
	mov.b32 	%f53, %r322;
	add.f32 	%f54, %f52, %f53;
	mov.b32 	%r323, %f54;
	shfl.sync.bfly.b32 	%r324|%p23, %r323, %r291, %r283, %r285;
	mov.b32 	%f55, %r324;
	add.f32 	%f56, %f54, %f55;
	mov.b32 	%r325, %f56;
	shfl.sync.bfly.b32 	%r326|%p24, %r325, %r294, %r283, %r285;
	mov.b32 	%f57, %r326;
	add.f32 	%f58, %f56, %f57;
	mov.b32 	%r327, %f58;
	shfl.sync.bfly.b32 	%r328|%p25, %r327, %r297, %r283, %r285;
	mov.b32 	%f59, %r328;
	add.f32 	%f60, %f58, %f59;
	st.local.f32 	[%rd3+12], %f60;
	mov.b32 	%r329, %f19;
	shfl.sync.bfly.b32 	%r330|%p26, %r329, %r284, %r283, %r285;
	mov.b32 	%f61, %r330;
	add.f32 	%f62, %f19, %f61;
	mov.b32 	%r331, %f62;
	shfl.sync.bfly.b32 	%r332|%p27, %r331, %r288, %r283, %r285;
	mov.b32 	%f63, %r332;
	add.f32 	%f64, %f62, %f63;
	mov.b32 	%r333, %f64;
	shfl.sync.bfly.b32 	%r334|%p28, %r333, %r291, %r283, %r285;
	mov.b32 	%f65, %r334;
	add.f32 	%f66, %f64, %f65;
	mov.b32 	%r335, %f66;
	shfl.sync.bfly.b32 	%r336|%p29, %r335, %r294, %r283, %r285;
	mov.b32 	%f67, %r336;
	add.f32 	%f68, %f66, %f67;
	mov.b32 	%r337, %f68;
	shfl.sync.bfly.b32 	%r338|%p30, %r337, %r297, %r283, %r285;
	mov.b32 	%f69, %r338;
	add.f32 	%f70, %f68, %f69;
	st.local.f32 	[%rd3+16], %f70;
	mov.b32 	%r339, %f20;
	shfl.sync.bfly.b32 	%r340|%p31, %r339, %r284, %r283, %r285;
	mov.b32 	%f71, %r340;
	add.f32 	%f72, %f20, %f71;
	mov.b32 	%r341, %f72;
	shfl.sync.bfly.b32 	%r342|%p32, %r341, %r288, %r283, %r285;
	mov.b32 	%f73, %r342;
	add.f32 	%f74, %f72, %f73;
	mov.b32 	%r343, %f74;
	shfl.sync.bfly.b32 	%r344|%p33, %r343, %r291, %r283, %r285;
	mov.b32 	%f75, %r344;
	add.f32 	%f76, %f74, %f75;
	mov.b32 	%r345, %f76;
	shfl.sync.bfly.b32 	%r346|%p34, %r345, %r294, %r283, %r285;
	mov.b32 	%f77, %r346;
	add.f32 	%f78, %f76, %f77;
	mov.b32 	%r347, %f78;
	shfl.sync.bfly.b32 	%r348|%p35, %r347, %r297, %r283, %r285;
	mov.b32 	%f79, %r348;
	add.f32 	%f80, %f78, %f79;
	st.local.f32 	[%rd3+20], %f80;
	setp.gt.s32 	%p36, %r4, 5;
	@%p36 bra 	$L__BB69_9;

	mad.lo.s32 	%r349, %r4, %r52, %r2;
	cvt.s64.s32 	%rd65, %r349;
	mul.lo.s32 	%r350, %r1, %r53;
	cvt.s64.s32 	%rd66, %r350;
	add.s64 	%rd67, %rd66, %rd65;
	mul.wide.s32 	%rd68, %r4, 4;
	add.s64 	%rd69, %rd3, %rd68;
	ld.local.f32 	%f81, [%rd69];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs3, %f81;}

	// end inline asm
	cvta.to.global.u64 	%rd70, %rd28;
	shl.b64 	%rd71, %rd67, 1;
	add.s64 	%rd72, %rd70, %rd71;
	st.global.u16 	[%rd72], %rs3;

$L__BB69_9:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_7_bs_32
.visible .entry ggml_matvec_f16_ncols_7_bs_32(
	.param .u64 ggml_matvec_f16_ncols_7_bs_32_param_0,
	.param .u64 ggml_matvec_f16_ncols_7_bs_32_param_1,
	.param .u64 ggml_matvec_f16_ncols_7_bs_32_param_2,
	.param .u32 ggml_matvec_f16_ncols_7_bs_32_param_3,
	.param .u32 ggml_matvec_f16_ncols_7_bs_32_param_4,
	.param .u32 ggml_matvec_f16_ncols_7_bs_32_param_5,
	.param .u32 ggml_matvec_f16_ncols_7_bs_32_param_6,
	.param .u32 ggml_matvec_f16_ncols_7_bs_32_param_7,
	.param .u32 ggml_matvec_f16_ncols_7_bs_32_param_8,
	.param .u32 ggml_matvec_f16_ncols_7_bs_32_param_9,
	.param .u32 ggml_matvec_f16_ncols_7_bs_32_param_10,
	.param .u32 ggml_matvec_f16_ncols_7_bs_32_param_11
)
{
	.local .align 4 .b8 	__local_depot70[28];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<42>;
	.reg .b16 	%rs<4>;
	.reg .f32 	%f<95>;
	.reg .b32 	%r<435>;
	.reg .b64 	%rd<81>;


	mov.u64 	%SPL, __local_depot70;
	ld.param.u64 	%rd30, [ggml_matvec_f16_ncols_7_bs_32_param_0];
	ld.param.u64 	%rd31, [ggml_matvec_f16_ncols_7_bs_32_param_1];
	ld.param.u64 	%rd29, [ggml_matvec_f16_ncols_7_bs_32_param_2];
	ld.param.u32 	%r56, [ggml_matvec_f16_ncols_7_bs_32_param_3];
	ld.param.u32 	%r66, [ggml_matvec_f16_ncols_7_bs_32_param_5];
	ld.param.u32 	%r57, [ggml_matvec_f16_ncols_7_bs_32_param_6];
	ld.param.u32 	%r58, [ggml_matvec_f16_ncols_7_bs_32_param_7];
	ld.param.u32 	%r67, [ggml_matvec_f16_ncols_7_bs_32_param_8];
	ld.param.u32 	%r68, [ggml_matvec_f16_ncols_7_bs_32_param_9];
	ld.param.u32 	%r69, [ggml_matvec_f16_ncols_7_bs_32_param_10];
	ld.param.u32 	%r59, [ggml_matvec_f16_ncols_7_bs_32_param_11];
	cvta.to.global.u64 	%rd80, %rd31;
	cvta.to.global.u64 	%rd2, %rd30;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r70, %r1, %r67;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r71, %r2, %r66;
	mad.lo.s32 	%r72, %r70, %r68, %r71;
	cvt.s64.s32 	%rd4, %r72;
	mul.lo.s32 	%r73, %r1, %r69;
	cvt.s64.s32 	%rd5, %r73;
	mov.f32 	%f2, 0f00000000;
	mov.u32 	%r428, 0;
	st.local.u32 	[%rd3], %r428;
	st.local.u32 	[%rd3+4], %r428;
	st.local.u32 	[%rd3+8], %r428;
	st.local.u32 	[%rd3+12], %r428;
	st.local.u32 	[%rd3+16], %r428;
	st.local.u32 	[%rd3+20], %r428;
	st.local.u32 	[%rd3+24], %r428;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs2, %f2;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f2;}

	// end inline asm
	mov.b32 	%r434, {%rs1, %rs2};
	mov.u32 	%r4, %tid.x;
	setp.ge.s32 	%p1, %r4, %r56;
	mov.u32 	%r429, %r428;
	mov.u32 	%r430, %r428;
	mov.u32 	%r431, %r428;
	mov.u32 	%r432, %r428;
	mov.u32 	%r433, %r428;
	@%p1 bra 	$L__BB70_7;

	not.b32 	%r80, %r4;
	add.s32 	%r5, %r80, %r56;
	shr.u32 	%r81, %r5, 5;
	add.s32 	%r82, %r81, 1;
	and.b32  	%r411, %r82, 3;
	setp.eq.s32 	%p2, %r411, 0;
	mov.u32 	%r428, 0;
	mov.u32 	%r419, %r4;
	@%p2 bra 	$L__BB70_4;

	shl.b32 	%r89, %r57, 1;
	add.s32 	%r90, %r4, %r89;
	mul.wide.s32 	%rd33, %r90, 4;
	shl.b64 	%rd34, %rd5, 1;
	add.s64 	%rd7, %rd33, %rd34;
	mul.wide.s32 	%rd35, %r4, 4;
	mul.wide.s32 	%rd8, %r57, 4;
	add.s64 	%rd36, %rd35, %rd8;
	add.s64 	%rd9, %rd36, %rd34;
	add.s64 	%rd10, %rd35, %rd34;
	mul.wide.s32 	%rd37, %r4, 2;
	add.s64 	%rd38, %rd37, %rd4;
	shl.b64 	%rd39, %rd38, 1;
	add.s64 	%rd77, %rd2, %rd39;
	mov.u32 	%r428, 0;
	mov.u64 	%rd78, %rd80;
	mov.u32 	%r429, %r428;
	mov.u32 	%r430, %r428;
	mov.u32 	%r431, %r428;
	mov.u32 	%r432, %r428;
	mov.u32 	%r433, %r428;
	mov.u32 	%r419, %r4;

$L__BB70_3:
	.pragma "nounroll";
	ld.global.nc.u32 	%r92, [%rd77];
	add.s64 	%rd40, %rd78, %rd10;
	ld.global.nc.u32 	%r93, [%rd40];
	// begin inline asm
	{mul.f16x2 %r91,%r92,%r93;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r434,%r434,%r91;
}
	// end inline asm
	add.s64 	%rd41, %rd78, %rd9;
	ld.global.nc.u32 	%r99, [%rd41];
	// begin inline asm
	{mul.f16x2 %r97,%r92,%r99;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r433,%r433,%r97;
}
	// end inline asm
	add.s64 	%rd42, %rd78, %rd7;
	ld.global.nc.u32 	%r105, [%rd42];
	// begin inline asm
	{mul.f16x2 %r103,%r92,%r105;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r432,%r432,%r103;
}
	// end inline asm
	add.s64 	%rd43, %rd42, %rd8;
	ld.global.nc.u32 	%r111, [%rd43];
	// begin inline asm
	{mul.f16x2 %r109,%r92,%r111;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r431,%r431,%r109;
}
	// end inline asm
	add.s64 	%rd44, %rd43, %rd8;
	ld.global.nc.u32 	%r117, [%rd44];
	// begin inline asm
	{mul.f16x2 %r115,%r92,%r117;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r430,%r430,%r115;
}
	// end inline asm
	add.s64 	%rd45, %rd44, %rd8;
	ld.global.nc.u32 	%r123, [%rd45];
	// begin inline asm
	{mul.f16x2 %r121,%r92,%r123;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r429,%r429,%r121;
}
	// end inline asm
	add.s64 	%rd46, %rd45, %rd8;
	ld.global.nc.u32 	%r129, [%rd46];
	// begin inline asm
	{mul.f16x2 %r127,%r92,%r129;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r428,%r428,%r127;
}
	// end inline asm
	add.s32 	%r419, %r419, 32;
	add.s64 	%rd78, %rd78, 128;
	add.s64 	%rd77, %rd77, 128;
	add.s32 	%r411, %r411, -1;
	setp.ne.s32 	%p3, %r411, 0;
	@%p3 bra 	$L__BB70_3;

$L__BB70_4:
	setp.lt.u32 	%p4, %r5, 96;
	@%p4 bra 	$L__BB70_7;

	add.s32 	%r133, %r419, %r57;
	shl.b32 	%r134, %r57, 1;
	add.s32 	%r135, %r419, %r134;
	mad.lo.s32 	%r136, %r57, 3, %r419;
	shl.b32 	%r137, %r57, 2;
	add.s32 	%r138, %r419, %r137;
	mad.lo.s32 	%r139, %r57, 5, %r419;
	mad.lo.s32 	%r140, %r57, 6, %r419;
	add.s32 	%r141, %r133, 32;
	mul.wide.s32 	%rd47, %r141, 4;
	shl.b64 	%rd48, %rd5, 1;
	add.s64 	%rd16, %rd47, %rd48;
	mul.wide.s32 	%rd49, %r135, 4;
	add.s64 	%rd17, %rd49, %rd48;
	mul.wide.s32 	%rd50, %r136, 4;
	add.s64 	%rd18, %rd50, %rd48;
	mul.wide.s32 	%rd51, %r138, 4;
	add.s64 	%rd19, %rd51, %rd48;
	mul.wide.s32 	%rd52, %r139, 4;
	add.s64 	%rd20, %rd52, %rd48;
	mul.wide.s32 	%rd53, %r140, 4;
	add.s64 	%rd21, %rd53, %rd48;
	mul.wide.s32 	%rd54, %r419, 2;
	add.s64 	%rd55, %rd54, %rd4;
	shl.b64 	%rd56, %rd55, 1;
	add.s64 	%rd57, %rd2, %rd56;
	add.s64 	%rd79, %rd57, 256;
	mul.wide.s32 	%rd58, %r419, 4;
	add.s64 	%rd23, %rd58, %rd48;
	mul.wide.s32 	%rd59, %r57, 4;
	add.s64 	%rd60, %rd58, %rd59;
	add.s64 	%rd24, %rd60, %rd48;

$L__BB70_6:
	ld.global.nc.u32 	%r143, [%rd79+-256];
	add.s64 	%rd61, %rd80, %rd23;
	ld.global.nc.u32 	%r144, [%rd61];
	// begin inline asm
	{mul.f16x2 %r142,%r143,%r144;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r145,%r434,%r142;
}
	// end inline asm
	add.s64 	%rd62, %rd80, %rd24;
	ld.global.nc.u32 	%r150, [%rd62];
	// begin inline asm
	{mul.f16x2 %r148,%r143,%r150;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r151,%r433,%r148;
}
	// end inline asm
	add.s64 	%rd63, %rd80, %rd17;
	ld.global.nc.u32 	%r156, [%rd63];
	// begin inline asm
	{mul.f16x2 %r154,%r143,%r156;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r157,%r432,%r154;
}
	// end inline asm
	add.s64 	%rd64, %rd80, %rd18;
	ld.global.nc.u32 	%r162, [%rd64];
	// begin inline asm
	{mul.f16x2 %r160,%r143,%r162;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r163,%r431,%r160;
}
	// end inline asm
	add.s64 	%rd65, %rd80, %rd19;
	ld.global.nc.u32 	%r168, [%rd65];
	// begin inline asm
	{mul.f16x2 %r166,%r143,%r168;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r169,%r430,%r166;
}
	// end inline asm
	add.s64 	%rd66, %rd80, %rd20;
	ld.global.nc.u32 	%r174, [%rd66];
	// begin inline asm
	{mul.f16x2 %r172,%r143,%r174;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r175,%r429,%r172;
}
	// end inline asm
	add.s64 	%rd67, %rd80, %rd21;
	ld.global.nc.u32 	%r180, [%rd67];
	// begin inline asm
	{mul.f16x2 %r178,%r143,%r180;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r181,%r428,%r178;
}
	// end inline asm
	ld.global.nc.u32 	%r185, [%rd79+-128];
	ld.global.nc.u32 	%r186, [%rd61+128];
	// begin inline asm
	{mul.f16x2 %r184,%r185,%r186;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r187,%r145,%r184;
}
	// end inline asm
	add.s64 	%rd68, %rd80, %rd16;
	ld.global.nc.u32 	%r192, [%rd68];
	// begin inline asm
	{mul.f16x2 %r190,%r185,%r192;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r193,%r151,%r190;
}
	// end inline asm
	ld.global.nc.u32 	%r198, [%rd63+128];
	// begin inline asm
	{mul.f16x2 %r196,%r185,%r198;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r199,%r157,%r196;
}
	// end inline asm
	ld.global.nc.u32 	%r204, [%rd64+128];
	// begin inline asm
	{mul.f16x2 %r202,%r185,%r204;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r205,%r163,%r202;
}
	// end inline asm
	ld.global.nc.u32 	%r210, [%rd65+128];
	// begin inline asm
	{mul.f16x2 %r208,%r185,%r210;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r211,%r169,%r208;
}
	// end inline asm
	ld.global.nc.u32 	%r216, [%rd66+128];
	// begin inline asm
	{mul.f16x2 %r214,%r185,%r216;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r217,%r175,%r214;
}
	// end inline asm
	ld.global.nc.u32 	%r222, [%rd67+128];
	// begin inline asm
	{mul.f16x2 %r220,%r185,%r222;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r223,%r181,%r220;
}
	// end inline asm
	ld.global.nc.u32 	%r227, [%rd79];
	ld.global.nc.u32 	%r228, [%rd61+256];
	// begin inline asm
	{mul.f16x2 %r226,%r227,%r228;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r229,%r187,%r226;
}
	// end inline asm
	ld.global.nc.u32 	%r234, [%rd68+128];
	// begin inline asm
	{mul.f16x2 %r232,%r227,%r234;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r235,%r193,%r232;
}
	// end inline asm
	ld.global.nc.u32 	%r240, [%rd63+256];
	// begin inline asm
	{mul.f16x2 %r238,%r227,%r240;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r241,%r199,%r238;
}
	// end inline asm
	ld.global.nc.u32 	%r246, [%rd64+256];
	// begin inline asm
	{mul.f16x2 %r244,%r227,%r246;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r247,%r205,%r244;
}
	// end inline asm
	ld.global.nc.u32 	%r252, [%rd65+256];
	// begin inline asm
	{mul.f16x2 %r250,%r227,%r252;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r253,%r211,%r250;
}
	// end inline asm
	ld.global.nc.u32 	%r258, [%rd66+256];
	// begin inline asm
	{mul.f16x2 %r256,%r227,%r258;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r259,%r217,%r256;
}
	// end inline asm
	ld.global.nc.u32 	%r264, [%rd67+256];
	// begin inline asm
	{mul.f16x2 %r262,%r227,%r264;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r265,%r223,%r262;
}
	// end inline asm
	ld.global.nc.u32 	%r269, [%rd79+128];
	ld.global.nc.u32 	%r270, [%rd61+384];
	// begin inline asm
	{mul.f16x2 %r268,%r269,%r270;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r434,%r229,%r268;
}
	// end inline asm
	ld.global.nc.u32 	%r276, [%rd68+256];
	// begin inline asm
	{mul.f16x2 %r274,%r269,%r276;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r433,%r235,%r274;
}
	// end inline asm
	ld.global.nc.u32 	%r282, [%rd63+384];
	// begin inline asm
	{mul.f16x2 %r280,%r269,%r282;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r432,%r241,%r280;
}
	// end inline asm
	ld.global.nc.u32 	%r288, [%rd64+384];
	// begin inline asm
	{mul.f16x2 %r286,%r269,%r288;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r431,%r247,%r286;
}
	// end inline asm
	ld.global.nc.u32 	%r294, [%rd65+384];
	// begin inline asm
	{mul.f16x2 %r292,%r269,%r294;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r430,%r253,%r292;
}
	// end inline asm
	ld.global.nc.u32 	%r300, [%rd66+384];
	// begin inline asm
	{mul.f16x2 %r298,%r269,%r300;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r429,%r259,%r298;
}
	// end inline asm
	ld.global.nc.u32 	%r306, [%rd67+384];
	// begin inline asm
	{mul.f16x2 %r304,%r269,%r306;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r428,%r265,%r304;
}
	// end inline asm
	add.s64 	%rd80, %rd80, 512;
	add.s64 	%rd79, %rd79, 512;
	add.s32 	%r419, %r419, 128;
	setp.lt.s32 	%p5, %r419, %r56;
	@%p5 bra 	$L__BB70_6;

$L__BB70_7:
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r434;
  cvt.f32.f16 %f3, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r434;
  cvt.f32.f16 %f4, high;}

	// end inline asm
	add.f32 	%f17, %f3, %f4;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r433;
  cvt.f32.f16 %f5, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r433;
  cvt.f32.f16 %f6, high;}

	// end inline asm
	add.f32 	%f18, %f5, %f6;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r432;
  cvt.f32.f16 %f7, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r432;
  cvt.f32.f16 %f8, high;}

	// end inline asm
	add.f32 	%f19, %f7, %f8;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r431;
  cvt.f32.f16 %f9, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r431;
  cvt.f32.f16 %f10, high;}

	// end inline asm
	add.f32 	%f20, %f9, %f10;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r430;
  cvt.f32.f16 %f11, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r430;
  cvt.f32.f16 %f12, high;}

	// end inline asm
	add.f32 	%f21, %f11, %f12;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r429;
  cvt.f32.f16 %f13, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r429;
  cvt.f32.f16 %f14, high;}

	// end inline asm
	add.f32 	%f22, %f13, %f14;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r428;
  cvt.f32.f16 %f15, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r428;
  cvt.f32.f16 %f16, high;}

	// end inline asm
	add.f32 	%f23, %f15, %f16;
	mov.b32 	%r324, %f17;
	mov.u32 	%r325, 31;
	mov.u32 	%r326, 16;
	mov.u32 	%r327, -1;
	shfl.sync.bfly.b32 	%r328|%p6, %r324, %r326, %r325, %r327;
	mov.b32 	%f24, %r328;
	add.f32 	%f25, %f17, %f24;
	mov.b32 	%r329, %f25;
	mov.u32 	%r330, 8;
	shfl.sync.bfly.b32 	%r331|%p7, %r329, %r330, %r325, %r327;
	mov.b32 	%f26, %r331;
	add.f32 	%f27, %f25, %f26;
	mov.b32 	%r332, %f27;
	mov.u32 	%r333, 4;
	shfl.sync.bfly.b32 	%r334|%p8, %r332, %r333, %r325, %r327;
	mov.b32 	%f28, %r334;
	add.f32 	%f29, %f27, %f28;
	mov.b32 	%r335, %f29;
	mov.u32 	%r336, 2;
	shfl.sync.bfly.b32 	%r337|%p9, %r335, %r336, %r325, %r327;
	mov.b32 	%f30, %r337;
	add.f32 	%f31, %f29, %f30;
	mov.b32 	%r338, %f31;
	mov.u32 	%r339, 1;
	shfl.sync.bfly.b32 	%r340|%p10, %r338, %r339, %r325, %r327;
	mov.b32 	%f32, %r340;
	add.f32 	%f33, %f31, %f32;
	st.local.f32 	[%rd3], %f33;
	mov.b32 	%r341, %f18;
	shfl.sync.bfly.b32 	%r342|%p11, %r341, %r326, %r325, %r327;
	mov.b32 	%f34, %r342;
	add.f32 	%f35, %f18, %f34;
	mov.b32 	%r343, %f35;
	shfl.sync.bfly.b32 	%r344|%p12, %r343, %r330, %r325, %r327;
	mov.b32 	%f36, %r344;
	add.f32 	%f37, %f35, %f36;
	mov.b32 	%r345, %f37;
	shfl.sync.bfly.b32 	%r346|%p13, %r345, %r333, %r325, %r327;
	mov.b32 	%f38, %r346;
	add.f32 	%f39, %f37, %f38;
	mov.b32 	%r347, %f39;
	shfl.sync.bfly.b32 	%r348|%p14, %r347, %r336, %r325, %r327;
	mov.b32 	%f40, %r348;
	add.f32 	%f41, %f39, %f40;
	mov.b32 	%r349, %f41;
	shfl.sync.bfly.b32 	%r350|%p15, %r349, %r339, %r325, %r327;
	mov.b32 	%f42, %r350;
	add.f32 	%f43, %f41, %f42;
	st.local.f32 	[%rd3+4], %f43;
	mov.b32 	%r351, %f19;
	shfl.sync.bfly.b32 	%r352|%p16, %r351, %r326, %r325, %r327;
	mov.b32 	%f44, %r352;
	add.f32 	%f45, %f19, %f44;
	mov.b32 	%r353, %f45;
	shfl.sync.bfly.b32 	%r354|%p17, %r353, %r330, %r325, %r327;
	mov.b32 	%f46, %r354;
	add.f32 	%f47, %f45, %f46;
	mov.b32 	%r355, %f47;
	shfl.sync.bfly.b32 	%r356|%p18, %r355, %r333, %r325, %r327;
	mov.b32 	%f48, %r356;
	add.f32 	%f49, %f47, %f48;
	mov.b32 	%r357, %f49;
	shfl.sync.bfly.b32 	%r358|%p19, %r357, %r336, %r325, %r327;
	mov.b32 	%f50, %r358;
	add.f32 	%f51, %f49, %f50;
	mov.b32 	%r359, %f51;
	shfl.sync.bfly.b32 	%r360|%p20, %r359, %r339, %r325, %r327;
	mov.b32 	%f52, %r360;
	add.f32 	%f53, %f51, %f52;
	st.local.f32 	[%rd3+8], %f53;
	mov.b32 	%r361, %f20;
	shfl.sync.bfly.b32 	%r362|%p21, %r361, %r326, %r325, %r327;
	mov.b32 	%f54, %r362;
	add.f32 	%f55, %f20, %f54;
	mov.b32 	%r363, %f55;
	shfl.sync.bfly.b32 	%r364|%p22, %r363, %r330, %r325, %r327;
	mov.b32 	%f56, %r364;
	add.f32 	%f57, %f55, %f56;
	mov.b32 	%r365, %f57;
	shfl.sync.bfly.b32 	%r366|%p23, %r365, %r333, %r325, %r327;
	mov.b32 	%f58, %r366;
	add.f32 	%f59, %f57, %f58;
	mov.b32 	%r367, %f59;
	shfl.sync.bfly.b32 	%r368|%p24, %r367, %r336, %r325, %r327;
	mov.b32 	%f60, %r368;
	add.f32 	%f61, %f59, %f60;
	mov.b32 	%r369, %f61;
	shfl.sync.bfly.b32 	%r370|%p25, %r369, %r339, %r325, %r327;
	mov.b32 	%f62, %r370;
	add.f32 	%f63, %f61, %f62;
	st.local.f32 	[%rd3+12], %f63;
	mov.b32 	%r371, %f21;
	shfl.sync.bfly.b32 	%r372|%p26, %r371, %r326, %r325, %r327;
	mov.b32 	%f64, %r372;
	add.f32 	%f65, %f21, %f64;
	mov.b32 	%r373, %f65;
	shfl.sync.bfly.b32 	%r374|%p27, %r373, %r330, %r325, %r327;
	mov.b32 	%f66, %r374;
	add.f32 	%f67, %f65, %f66;
	mov.b32 	%r375, %f67;
	shfl.sync.bfly.b32 	%r376|%p28, %r375, %r333, %r325, %r327;
	mov.b32 	%f68, %r376;
	add.f32 	%f69, %f67, %f68;
	mov.b32 	%r377, %f69;
	shfl.sync.bfly.b32 	%r378|%p29, %r377, %r336, %r325, %r327;
	mov.b32 	%f70, %r378;
	add.f32 	%f71, %f69, %f70;
	mov.b32 	%r379, %f71;
	shfl.sync.bfly.b32 	%r380|%p30, %r379, %r339, %r325, %r327;
	mov.b32 	%f72, %r380;
	add.f32 	%f73, %f71, %f72;
	st.local.f32 	[%rd3+16], %f73;
	mov.b32 	%r381, %f22;
	shfl.sync.bfly.b32 	%r382|%p31, %r381, %r326, %r325, %r327;
	mov.b32 	%f74, %r382;
	add.f32 	%f75, %f22, %f74;
	mov.b32 	%r383, %f75;
	shfl.sync.bfly.b32 	%r384|%p32, %r383, %r330, %r325, %r327;
	mov.b32 	%f76, %r384;
	add.f32 	%f77, %f75, %f76;
	mov.b32 	%r385, %f77;
	shfl.sync.bfly.b32 	%r386|%p33, %r385, %r333, %r325, %r327;
	mov.b32 	%f78, %r386;
	add.f32 	%f79, %f77, %f78;
	mov.b32 	%r387, %f79;
	shfl.sync.bfly.b32 	%r388|%p34, %r387, %r336, %r325, %r327;
	mov.b32 	%f80, %r388;
	add.f32 	%f81, %f79, %f80;
	mov.b32 	%r389, %f81;
	shfl.sync.bfly.b32 	%r390|%p35, %r389, %r339, %r325, %r327;
	mov.b32 	%f82, %r390;
	add.f32 	%f83, %f81, %f82;
	st.local.f32 	[%rd3+20], %f83;
	mov.b32 	%r391, %f23;
	shfl.sync.bfly.b32 	%r392|%p36, %r391, %r326, %r325, %r327;
	mov.b32 	%f84, %r392;
	add.f32 	%f85, %f23, %f84;
	mov.b32 	%r393, %f85;
	shfl.sync.bfly.b32 	%r394|%p37, %r393, %r330, %r325, %r327;
	mov.b32 	%f86, %r394;
	add.f32 	%f87, %f85, %f86;
	mov.b32 	%r395, %f87;
	shfl.sync.bfly.b32 	%r396|%p38, %r395, %r333, %r325, %r327;
	mov.b32 	%f88, %r396;
	add.f32 	%f89, %f87, %f88;
	mov.b32 	%r397, %f89;
	shfl.sync.bfly.b32 	%r398|%p39, %r397, %r336, %r325, %r327;
	mov.b32 	%f90, %r398;
	add.f32 	%f91, %f89, %f90;
	mov.b32 	%r399, %f91;
	shfl.sync.bfly.b32 	%r400|%p40, %r399, %r339, %r325, %r327;
	mov.b32 	%f92, %r400;
	add.f32 	%f93, %f91, %f92;
	st.local.f32 	[%rd3+24], %f93;
	setp.gt.s32 	%p41, %r4, 6;
	@%p41 bra 	$L__BB70_9;

	mad.lo.s32 	%r401, %r4, %r58, %r2;
	cvt.s64.s32 	%rd69, %r401;
	mul.lo.s32 	%r402, %r1, %r59;
	cvt.s64.s32 	%rd70, %r402;
	add.s64 	%rd71, %rd70, %rd69;
	mul.wide.s32 	%rd72, %r4, 4;
	add.s64 	%rd73, %rd3, %rd72;
	ld.local.f32 	%f94, [%rd73];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs3, %f94;}

	// end inline asm
	cvta.to.global.u64 	%rd74, %rd29;
	shl.b64 	%rd75, %rd71, 1;
	add.s64 	%rd76, %rd74, %rd75;
	st.global.u16 	[%rd76], %rs3;

$L__BB70_9:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_8_bs_32
.visible .entry ggml_matvec_f16_ncols_8_bs_32(
	.param .u64 ggml_matvec_f16_ncols_8_bs_32_param_0,
	.param .u64 ggml_matvec_f16_ncols_8_bs_32_param_1,
	.param .u64 ggml_matvec_f16_ncols_8_bs_32_param_2,
	.param .u32 ggml_matvec_f16_ncols_8_bs_32_param_3,
	.param .u32 ggml_matvec_f16_ncols_8_bs_32_param_4,
	.param .u32 ggml_matvec_f16_ncols_8_bs_32_param_5,
	.param .u32 ggml_matvec_f16_ncols_8_bs_32_param_6,
	.param .u32 ggml_matvec_f16_ncols_8_bs_32_param_7,
	.param .u32 ggml_matvec_f16_ncols_8_bs_32_param_8,
	.param .u32 ggml_matvec_f16_ncols_8_bs_32_param_9,
	.param .u32 ggml_matvec_f16_ncols_8_bs_32_param_10,
	.param .u32 ggml_matvec_f16_ncols_8_bs_32_param_11
)
{
	.local .align 16 .b8 	__local_depot71[32];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<47>;
	.reg .b16 	%rs<4>;
	.reg .f32 	%f<108>;
	.reg .b32 	%r<491>;
	.reg .b64 	%rd<84>;


	mov.u64 	%SPL, __local_depot71;
	ld.param.u64 	%rd30, [ggml_matvec_f16_ncols_8_bs_32_param_0];
	ld.param.u64 	%rd31, [ggml_matvec_f16_ncols_8_bs_32_param_1];
	ld.param.u64 	%rd29, [ggml_matvec_f16_ncols_8_bs_32_param_2];
	ld.param.u32 	%r62, [ggml_matvec_f16_ncols_8_bs_32_param_3];
	ld.param.u32 	%r73, [ggml_matvec_f16_ncols_8_bs_32_param_5];
	ld.param.u32 	%r63, [ggml_matvec_f16_ncols_8_bs_32_param_6];
	ld.param.u32 	%r64, [ggml_matvec_f16_ncols_8_bs_32_param_7];
	ld.param.u32 	%r74, [ggml_matvec_f16_ncols_8_bs_32_param_8];
	ld.param.u32 	%r75, [ggml_matvec_f16_ncols_8_bs_32_param_9];
	ld.param.u32 	%r76, [ggml_matvec_f16_ncols_8_bs_32_param_10];
	ld.param.u32 	%r65, [ggml_matvec_f16_ncols_8_bs_32_param_11];
	cvta.to.global.u64 	%rd83, %rd31;
	cvta.to.global.u64 	%rd2, %rd30;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r77, %r1, %r74;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r78, %r2, %r73;
	mad.lo.s32 	%r79, %r77, %r75, %r78;
	cvt.s64.s32 	%rd4, %r79;
	mul.lo.s32 	%r80, %r1, %r76;
	cvt.s64.s32 	%rd5, %r80;
	mov.f32 	%f2, 0f00000000;
	st.local.v4.f32 	[%rd3], {%f2, %f2, %f2, %f2};
	st.local.v4.f32 	[%rd3+16], {%f2, %f2, %f2, %f2};
	mov.u32 	%r483, 0;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs2, %f2;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f2;}

	// end inline asm
	mov.b32 	%r490, {%rs1, %rs2};
	mov.u32 	%r4, %tid.x;
	setp.ge.s32 	%p1, %r4, %r62;
	mov.u32 	%r484, %r483;
	mov.u32 	%r485, %r483;
	mov.u32 	%r486, %r483;
	mov.u32 	%r487, %r483;
	mov.u32 	%r488, %r483;
	mov.u32 	%r489, %r483;
	@%p1 bra 	$L__BB71_7;

	not.b32 	%r88, %r4;
	add.s32 	%r5, %r88, %r62;
	shr.u32 	%r89, %r5, 5;
	add.s32 	%r90, %r89, 1;
	and.b32  	%r464, %r90, 3;
	setp.eq.s32 	%p2, %r464, 0;
	mov.u32 	%r483, 0;
	mov.u32 	%r473, %r4;
	@%p2 bra 	$L__BB71_4;

	shl.b32 	%r98, %r63, 1;
	add.s32 	%r99, %r4, %r98;
	mul.wide.s32 	%rd33, %r99, 4;
	shl.b64 	%rd34, %rd5, 1;
	add.s64 	%rd6, %rd33, %rd34;
	mul.wide.s32 	%rd35, %r4, 4;
	mul.wide.s32 	%rd7, %r63, 4;
	add.s64 	%rd36, %rd35, %rd7;
	add.s64 	%rd8, %rd36, %rd34;
	add.s64 	%rd9, %rd35, %rd34;
	mul.wide.s32 	%rd37, %r4, 2;
	add.s64 	%rd38, %rd37, %rd4;
	shl.b64 	%rd39, %rd38, 1;
	add.s64 	%rd80, %rd2, %rd39;
	mov.u32 	%r483, 0;
	mov.u64 	%rd81, %rd83;
	mov.u32 	%r484, %r483;
	mov.u32 	%r485, %r483;
	mov.u32 	%r486, %r483;
	mov.u32 	%r487, %r483;
	mov.u32 	%r488, %r483;
	mov.u32 	%r489, %r483;
	mov.u32 	%r473, %r4;

$L__BB71_3:
	.pragma "nounroll";
	ld.global.nc.u32 	%r101, [%rd80];
	add.s64 	%rd40, %rd81, %rd9;
	ld.global.nc.u32 	%r102, [%rd40];
	// begin inline asm
	{mul.f16x2 %r100,%r101,%r102;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r490,%r490,%r100;
}
	// end inline asm
	add.s64 	%rd41, %rd81, %rd8;
	ld.global.nc.u32 	%r108, [%rd41];
	// begin inline asm
	{mul.f16x2 %r106,%r101,%r108;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r489,%r489,%r106;
}
	// end inline asm
	add.s64 	%rd42, %rd81, %rd6;
	ld.global.nc.u32 	%r114, [%rd42];
	// begin inline asm
	{mul.f16x2 %r112,%r101,%r114;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r488,%r488,%r112;
}
	// end inline asm
	add.s64 	%rd43, %rd42, %rd7;
	ld.global.nc.u32 	%r120, [%rd43];
	// begin inline asm
	{mul.f16x2 %r118,%r101,%r120;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r487,%r487,%r118;
}
	// end inline asm
	add.s64 	%rd44, %rd43, %rd7;
	ld.global.nc.u32 	%r126, [%rd44];
	// begin inline asm
	{mul.f16x2 %r124,%r101,%r126;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r486,%r486,%r124;
}
	// end inline asm
	add.s64 	%rd45, %rd44, %rd7;
	ld.global.nc.u32 	%r132, [%rd45];
	// begin inline asm
	{mul.f16x2 %r130,%r101,%r132;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r485,%r485,%r130;
}
	// end inline asm
	add.s64 	%rd46, %rd45, %rd7;
	ld.global.nc.u32 	%r138, [%rd46];
	// begin inline asm
	{mul.f16x2 %r136,%r101,%r138;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r484,%r484,%r136;
}
	// end inline asm
	add.s64 	%rd47, %rd46, %rd7;
	ld.global.nc.u32 	%r144, [%rd47];
	// begin inline asm
	{mul.f16x2 %r142,%r101,%r144;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r483,%r483,%r142;
}
	// end inline asm
	add.s32 	%r473, %r473, 32;
	add.s64 	%rd81, %rd81, 128;
	add.s64 	%rd80, %rd80, 128;
	add.s32 	%r464, %r464, -1;
	setp.ne.s32 	%p3, %r464, 0;
	@%p3 bra 	$L__BB71_3;

$L__BB71_4:
	setp.lt.u32 	%p4, %r5, 96;
	@%p4 bra 	$L__BB71_7;

	add.s32 	%r148, %r473, %r63;
	shl.b32 	%r149, %r63, 1;
	add.s32 	%r150, %r473, %r149;
	mad.lo.s32 	%r151, %r63, 3, %r473;
	shl.b32 	%r152, %r63, 2;
	add.s32 	%r153, %r473, %r152;
	mad.lo.s32 	%r154, %r63, 5, %r473;
	mad.lo.s32 	%r155, %r63, 6, %r473;
	mad.lo.s32 	%r156, %r63, 7, %r473;
	add.s32 	%r157, %r148, 32;
	mul.wide.s32 	%rd48, %r157, 4;
	shl.b64 	%rd49, %rd5, 1;
	add.s64 	%rd15, %rd48, %rd49;
	mul.wide.s32 	%rd50, %r150, 4;
	add.s64 	%rd16, %rd50, %rd49;
	mul.wide.s32 	%rd51, %r151, 4;
	add.s64 	%rd17, %rd51, %rd49;
	mul.wide.s32 	%rd52, %r153, 4;
	add.s64 	%rd18, %rd52, %rd49;
	mul.wide.s32 	%rd53, %r154, 4;
	add.s64 	%rd19, %rd53, %rd49;
	mul.wide.s32 	%rd54, %r155, 4;
	add.s64 	%rd20, %rd54, %rd49;
	mul.wide.s32 	%rd55, %r156, 4;
	add.s64 	%rd21, %rd55, %rd49;
	mul.wide.s32 	%rd56, %r473, 2;
	add.s64 	%rd57, %rd56, %rd4;
	shl.b64 	%rd58, %rd57, 1;
	add.s64 	%rd59, %rd2, %rd58;
	add.s64 	%rd82, %rd59, 256;
	mul.wide.s32 	%rd60, %r473, 4;
	add.s64 	%rd23, %rd60, %rd49;
	mul.wide.s32 	%rd61, %r63, 4;
	add.s64 	%rd62, %rd60, %rd61;
	add.s64 	%rd24, %rd62, %rd49;

$L__BB71_6:
	ld.global.nc.u32 	%r159, [%rd82+-256];
	add.s64 	%rd63, %rd83, %rd23;
	ld.global.nc.u32 	%r160, [%rd63];
	// begin inline asm
	{mul.f16x2 %r158,%r159,%r160;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r161,%r490,%r158;
}
	// end inline asm
	add.s64 	%rd64, %rd83, %rd24;
	ld.global.nc.u32 	%r166, [%rd64];
	// begin inline asm
	{mul.f16x2 %r164,%r159,%r166;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r167,%r489,%r164;
}
	// end inline asm
	add.s64 	%rd65, %rd83, %rd16;
	ld.global.nc.u32 	%r172, [%rd65];
	// begin inline asm
	{mul.f16x2 %r170,%r159,%r172;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r173,%r488,%r170;
}
	// end inline asm
	add.s64 	%rd66, %rd83, %rd17;
	ld.global.nc.u32 	%r178, [%rd66];
	// begin inline asm
	{mul.f16x2 %r176,%r159,%r178;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r179,%r487,%r176;
}
	// end inline asm
	add.s64 	%rd67, %rd83, %rd18;
	ld.global.nc.u32 	%r184, [%rd67];
	// begin inline asm
	{mul.f16x2 %r182,%r159,%r184;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r185,%r486,%r182;
}
	// end inline asm
	add.s64 	%rd68, %rd83, %rd19;
	ld.global.nc.u32 	%r190, [%rd68];
	// begin inline asm
	{mul.f16x2 %r188,%r159,%r190;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r191,%r485,%r188;
}
	// end inline asm
	add.s64 	%rd69, %rd83, %rd20;
	ld.global.nc.u32 	%r196, [%rd69];
	// begin inline asm
	{mul.f16x2 %r194,%r159,%r196;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r197,%r484,%r194;
}
	// end inline asm
	add.s64 	%rd70, %rd83, %rd21;
	ld.global.nc.u32 	%r202, [%rd70];
	// begin inline asm
	{mul.f16x2 %r200,%r159,%r202;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r203,%r483,%r200;
}
	// end inline asm
	ld.global.nc.u32 	%r207, [%rd82+-128];
	ld.global.nc.u32 	%r208, [%rd63+128];
	// begin inline asm
	{mul.f16x2 %r206,%r207,%r208;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r209,%r161,%r206;
}
	// end inline asm
	add.s64 	%rd71, %rd83, %rd15;
	ld.global.nc.u32 	%r214, [%rd71];
	// begin inline asm
	{mul.f16x2 %r212,%r207,%r214;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r215,%r167,%r212;
}
	// end inline asm
	ld.global.nc.u32 	%r220, [%rd65+128];
	// begin inline asm
	{mul.f16x2 %r218,%r207,%r220;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r221,%r173,%r218;
}
	// end inline asm
	ld.global.nc.u32 	%r226, [%rd66+128];
	// begin inline asm
	{mul.f16x2 %r224,%r207,%r226;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r227,%r179,%r224;
}
	// end inline asm
	ld.global.nc.u32 	%r232, [%rd67+128];
	// begin inline asm
	{mul.f16x2 %r230,%r207,%r232;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r233,%r185,%r230;
}
	// end inline asm
	ld.global.nc.u32 	%r238, [%rd68+128];
	// begin inline asm
	{mul.f16x2 %r236,%r207,%r238;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r239,%r191,%r236;
}
	// end inline asm
	ld.global.nc.u32 	%r244, [%rd69+128];
	// begin inline asm
	{mul.f16x2 %r242,%r207,%r244;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r245,%r197,%r242;
}
	// end inline asm
	ld.global.nc.u32 	%r250, [%rd70+128];
	// begin inline asm
	{mul.f16x2 %r248,%r207,%r250;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r251,%r203,%r248;
}
	// end inline asm
	ld.global.nc.u32 	%r255, [%rd82];
	ld.global.nc.u32 	%r256, [%rd63+256];
	// begin inline asm
	{mul.f16x2 %r254,%r255,%r256;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r257,%r209,%r254;
}
	// end inline asm
	ld.global.nc.u32 	%r262, [%rd71+128];
	// begin inline asm
	{mul.f16x2 %r260,%r255,%r262;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r263,%r215,%r260;
}
	// end inline asm
	ld.global.nc.u32 	%r268, [%rd65+256];
	// begin inline asm
	{mul.f16x2 %r266,%r255,%r268;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r269,%r221,%r266;
}
	// end inline asm
	ld.global.nc.u32 	%r274, [%rd66+256];
	// begin inline asm
	{mul.f16x2 %r272,%r255,%r274;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r275,%r227,%r272;
}
	// end inline asm
	ld.global.nc.u32 	%r280, [%rd67+256];
	// begin inline asm
	{mul.f16x2 %r278,%r255,%r280;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r281,%r233,%r278;
}
	// end inline asm
	ld.global.nc.u32 	%r286, [%rd68+256];
	// begin inline asm
	{mul.f16x2 %r284,%r255,%r286;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r287,%r239,%r284;
}
	// end inline asm
	ld.global.nc.u32 	%r292, [%rd69+256];
	// begin inline asm
	{mul.f16x2 %r290,%r255,%r292;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r293,%r245,%r290;
}
	// end inline asm
	ld.global.nc.u32 	%r298, [%rd70+256];
	// begin inline asm
	{mul.f16x2 %r296,%r255,%r298;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r299,%r251,%r296;
}
	// end inline asm
	ld.global.nc.u32 	%r303, [%rd82+128];
	ld.global.nc.u32 	%r304, [%rd63+384];
	// begin inline asm
	{mul.f16x2 %r302,%r303,%r304;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r490,%r257,%r302;
}
	// end inline asm
	ld.global.nc.u32 	%r310, [%rd71+256];
	// begin inline asm
	{mul.f16x2 %r308,%r303,%r310;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r489,%r263,%r308;
}
	// end inline asm
	ld.global.nc.u32 	%r316, [%rd65+384];
	// begin inline asm
	{mul.f16x2 %r314,%r303,%r316;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r488,%r269,%r314;
}
	// end inline asm
	ld.global.nc.u32 	%r322, [%rd66+384];
	// begin inline asm
	{mul.f16x2 %r320,%r303,%r322;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r487,%r275,%r320;
}
	// end inline asm
	ld.global.nc.u32 	%r328, [%rd67+384];
	// begin inline asm
	{mul.f16x2 %r326,%r303,%r328;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r486,%r281,%r326;
}
	// end inline asm
	ld.global.nc.u32 	%r334, [%rd68+384];
	// begin inline asm
	{mul.f16x2 %r332,%r303,%r334;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r485,%r287,%r332;
}
	// end inline asm
	ld.global.nc.u32 	%r340, [%rd69+384];
	// begin inline asm
	{mul.f16x2 %r338,%r303,%r340;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r484,%r293,%r338;
}
	// end inline asm
	ld.global.nc.u32 	%r346, [%rd70+384];
	// begin inline asm
	{mul.f16x2 %r344,%r303,%r346;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r483,%r299,%r344;
}
	// end inline asm
	add.s64 	%rd83, %rd83, 512;
	add.s64 	%rd82, %rd82, 512;
	add.s32 	%r473, %r473, 128;
	setp.lt.s32 	%p5, %r473, %r62;
	@%p5 bra 	$L__BB71_6;

$L__BB71_7:
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r490;
  cvt.f32.f16 %f3, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r490;
  cvt.f32.f16 %f4, high;}

	// end inline asm
	add.f32 	%f19, %f3, %f4;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r489;
  cvt.f32.f16 %f5, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r489;
  cvt.f32.f16 %f6, high;}

	// end inline asm
	add.f32 	%f20, %f5, %f6;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r488;
  cvt.f32.f16 %f7, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r488;
  cvt.f32.f16 %f8, high;}

	// end inline asm
	add.f32 	%f21, %f7, %f8;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r487;
  cvt.f32.f16 %f9, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r487;
  cvt.f32.f16 %f10, high;}

	// end inline asm
	add.f32 	%f22, %f9, %f10;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r486;
  cvt.f32.f16 %f11, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r486;
  cvt.f32.f16 %f12, high;}

	// end inline asm
	add.f32 	%f23, %f11, %f12;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r485;
  cvt.f32.f16 %f13, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r485;
  cvt.f32.f16 %f14, high;}

	// end inline asm
	add.f32 	%f24, %f13, %f14;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r484;
  cvt.f32.f16 %f15, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r484;
  cvt.f32.f16 %f16, high;}

	// end inline asm
	add.f32 	%f25, %f15, %f16;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r483;
  cvt.f32.f16 %f17, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r483;
  cvt.f32.f16 %f18, high;}

	// end inline asm
	add.f32 	%f26, %f17, %f18;
	mov.b32 	%r366, %f19;
	mov.u32 	%r367, 31;
	mov.u32 	%r368, 16;
	mov.u32 	%r369, -1;
	shfl.sync.bfly.b32 	%r370|%p6, %r366, %r368, %r367, %r369;
	mov.b32 	%f27, %r370;
	add.f32 	%f28, %f19, %f27;
	mov.b32 	%r371, %f28;
	mov.u32 	%r372, 8;
	shfl.sync.bfly.b32 	%r373|%p7, %r371, %r372, %r367, %r369;
	mov.b32 	%f29, %r373;
	add.f32 	%f30, %f28, %f29;
	mov.b32 	%r374, %f30;
	mov.u32 	%r375, 4;
	shfl.sync.bfly.b32 	%r376|%p8, %r374, %r375, %r367, %r369;
	mov.b32 	%f31, %r376;
	add.f32 	%f32, %f30, %f31;
	mov.b32 	%r377, %f32;
	mov.u32 	%r378, 2;
	shfl.sync.bfly.b32 	%r379|%p9, %r377, %r378, %r367, %r369;
	mov.b32 	%f33, %r379;
	add.f32 	%f34, %f32, %f33;
	mov.b32 	%r380, %f34;
	mov.u32 	%r381, 1;
	shfl.sync.bfly.b32 	%r382|%p10, %r380, %r381, %r367, %r369;
	mov.b32 	%f35, %r382;
	add.f32 	%f36, %f34, %f35;
	st.local.f32 	[%rd3], %f36;
	mov.b32 	%r383, %f20;
	shfl.sync.bfly.b32 	%r384|%p11, %r383, %r368, %r367, %r369;
	mov.b32 	%f37, %r384;
	add.f32 	%f38, %f20, %f37;
	mov.b32 	%r385, %f38;
	shfl.sync.bfly.b32 	%r386|%p12, %r385, %r372, %r367, %r369;
	mov.b32 	%f39, %r386;
	add.f32 	%f40, %f38, %f39;
	mov.b32 	%r387, %f40;
	shfl.sync.bfly.b32 	%r388|%p13, %r387, %r375, %r367, %r369;
	mov.b32 	%f41, %r388;
	add.f32 	%f42, %f40, %f41;
	mov.b32 	%r389, %f42;
	shfl.sync.bfly.b32 	%r390|%p14, %r389, %r378, %r367, %r369;
	mov.b32 	%f43, %r390;
	add.f32 	%f44, %f42, %f43;
	mov.b32 	%r391, %f44;
	shfl.sync.bfly.b32 	%r392|%p15, %r391, %r381, %r367, %r369;
	mov.b32 	%f45, %r392;
	add.f32 	%f46, %f44, %f45;
	st.local.f32 	[%rd3+4], %f46;
	mov.b32 	%r393, %f21;
	shfl.sync.bfly.b32 	%r394|%p16, %r393, %r368, %r367, %r369;
	mov.b32 	%f47, %r394;
	add.f32 	%f48, %f21, %f47;
	mov.b32 	%r395, %f48;
	shfl.sync.bfly.b32 	%r396|%p17, %r395, %r372, %r367, %r369;
	mov.b32 	%f49, %r396;
	add.f32 	%f50, %f48, %f49;
	mov.b32 	%r397, %f50;
	shfl.sync.bfly.b32 	%r398|%p18, %r397, %r375, %r367, %r369;
	mov.b32 	%f51, %r398;
	add.f32 	%f52, %f50, %f51;
	mov.b32 	%r399, %f52;
	shfl.sync.bfly.b32 	%r400|%p19, %r399, %r378, %r367, %r369;
	mov.b32 	%f53, %r400;
	add.f32 	%f54, %f52, %f53;
	mov.b32 	%r401, %f54;
	shfl.sync.bfly.b32 	%r402|%p20, %r401, %r381, %r367, %r369;
	mov.b32 	%f55, %r402;
	add.f32 	%f56, %f54, %f55;
	st.local.f32 	[%rd3+8], %f56;
	mov.b32 	%r403, %f22;
	shfl.sync.bfly.b32 	%r404|%p21, %r403, %r368, %r367, %r369;
	mov.b32 	%f57, %r404;
	add.f32 	%f58, %f22, %f57;
	mov.b32 	%r405, %f58;
	shfl.sync.bfly.b32 	%r406|%p22, %r405, %r372, %r367, %r369;
	mov.b32 	%f59, %r406;
	add.f32 	%f60, %f58, %f59;
	mov.b32 	%r407, %f60;
	shfl.sync.bfly.b32 	%r408|%p23, %r407, %r375, %r367, %r369;
	mov.b32 	%f61, %r408;
	add.f32 	%f62, %f60, %f61;
	mov.b32 	%r409, %f62;
	shfl.sync.bfly.b32 	%r410|%p24, %r409, %r378, %r367, %r369;
	mov.b32 	%f63, %r410;
	add.f32 	%f64, %f62, %f63;
	mov.b32 	%r411, %f64;
	shfl.sync.bfly.b32 	%r412|%p25, %r411, %r381, %r367, %r369;
	mov.b32 	%f65, %r412;
	add.f32 	%f66, %f64, %f65;
	st.local.f32 	[%rd3+12], %f66;
	mov.b32 	%r413, %f23;
	shfl.sync.bfly.b32 	%r414|%p26, %r413, %r368, %r367, %r369;
	mov.b32 	%f67, %r414;
	add.f32 	%f68, %f23, %f67;
	mov.b32 	%r415, %f68;
	shfl.sync.bfly.b32 	%r416|%p27, %r415, %r372, %r367, %r369;
	mov.b32 	%f69, %r416;
	add.f32 	%f70, %f68, %f69;
	mov.b32 	%r417, %f70;
	shfl.sync.bfly.b32 	%r418|%p28, %r417, %r375, %r367, %r369;
	mov.b32 	%f71, %r418;
	add.f32 	%f72, %f70, %f71;
	mov.b32 	%r419, %f72;
	shfl.sync.bfly.b32 	%r420|%p29, %r419, %r378, %r367, %r369;
	mov.b32 	%f73, %r420;
	add.f32 	%f74, %f72, %f73;
	mov.b32 	%r421, %f74;
	shfl.sync.bfly.b32 	%r422|%p30, %r421, %r381, %r367, %r369;
	mov.b32 	%f75, %r422;
	add.f32 	%f76, %f74, %f75;
	st.local.f32 	[%rd3+16], %f76;
	mov.b32 	%r423, %f24;
	shfl.sync.bfly.b32 	%r424|%p31, %r423, %r368, %r367, %r369;
	mov.b32 	%f77, %r424;
	add.f32 	%f78, %f24, %f77;
	mov.b32 	%r425, %f78;
	shfl.sync.bfly.b32 	%r426|%p32, %r425, %r372, %r367, %r369;
	mov.b32 	%f79, %r426;
	add.f32 	%f80, %f78, %f79;
	mov.b32 	%r427, %f80;
	shfl.sync.bfly.b32 	%r428|%p33, %r427, %r375, %r367, %r369;
	mov.b32 	%f81, %r428;
	add.f32 	%f82, %f80, %f81;
	mov.b32 	%r429, %f82;
	shfl.sync.bfly.b32 	%r430|%p34, %r429, %r378, %r367, %r369;
	mov.b32 	%f83, %r430;
	add.f32 	%f84, %f82, %f83;
	mov.b32 	%r431, %f84;
	shfl.sync.bfly.b32 	%r432|%p35, %r431, %r381, %r367, %r369;
	mov.b32 	%f85, %r432;
	add.f32 	%f86, %f84, %f85;
	st.local.f32 	[%rd3+20], %f86;
	mov.b32 	%r433, %f25;
	shfl.sync.bfly.b32 	%r434|%p36, %r433, %r368, %r367, %r369;
	mov.b32 	%f87, %r434;
	add.f32 	%f88, %f25, %f87;
	mov.b32 	%r435, %f88;
	shfl.sync.bfly.b32 	%r436|%p37, %r435, %r372, %r367, %r369;
	mov.b32 	%f89, %r436;
	add.f32 	%f90, %f88, %f89;
	mov.b32 	%r437, %f90;
	shfl.sync.bfly.b32 	%r438|%p38, %r437, %r375, %r367, %r369;
	mov.b32 	%f91, %r438;
	add.f32 	%f92, %f90, %f91;
	mov.b32 	%r439, %f92;
	shfl.sync.bfly.b32 	%r440|%p39, %r439, %r378, %r367, %r369;
	mov.b32 	%f93, %r440;
	add.f32 	%f94, %f92, %f93;
	mov.b32 	%r441, %f94;
	shfl.sync.bfly.b32 	%r442|%p40, %r441, %r381, %r367, %r369;
	mov.b32 	%f95, %r442;
	add.f32 	%f96, %f94, %f95;
	st.local.f32 	[%rd3+24], %f96;
	mov.b32 	%r443, %f26;
	shfl.sync.bfly.b32 	%r444|%p41, %r443, %r368, %r367, %r369;
	mov.b32 	%f97, %r444;
	add.f32 	%f98, %f26, %f97;
	mov.b32 	%r445, %f98;
	shfl.sync.bfly.b32 	%r446|%p42, %r445, %r372, %r367, %r369;
	mov.b32 	%f99, %r446;
	add.f32 	%f100, %f98, %f99;
	mov.b32 	%r447, %f100;
	shfl.sync.bfly.b32 	%r448|%p43, %r447, %r375, %r367, %r369;
	mov.b32 	%f101, %r448;
	add.f32 	%f102, %f100, %f101;
	mov.b32 	%r449, %f102;
	shfl.sync.bfly.b32 	%r450|%p44, %r449, %r378, %r367, %r369;
	mov.b32 	%f103, %r450;
	add.f32 	%f104, %f102, %f103;
	mov.b32 	%r451, %f104;
	shfl.sync.bfly.b32 	%r452|%p45, %r451, %r381, %r367, %r369;
	mov.b32 	%f105, %r452;
	add.f32 	%f106, %f104, %f105;
	st.local.f32 	[%rd3+28], %f106;
	setp.gt.s32 	%p46, %r4, 7;
	@%p46 bra 	$L__BB71_9;

	mad.lo.s32 	%r453, %r4, %r64, %r2;
	cvt.s64.s32 	%rd72, %r453;
	mul.lo.s32 	%r454, %r1, %r65;
	cvt.s64.s32 	%rd73, %r454;
	add.s64 	%rd74, %rd73, %rd72;
	mul.wide.s32 	%rd75, %r4, 4;
	add.s64 	%rd76, %rd3, %rd75;
	ld.local.f32 	%f107, [%rd76];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs3, %f107;}

	// end inline asm
	cvta.to.global.u64 	%rd77, %rd29;
	shl.b64 	%rd78, %rd74, 1;
	add.s64 	%rd79, %rd77, %rd78;
	st.global.u16 	[%rd79], %rs3;

$L__BB71_9:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_1_bs_64
.visible .entry ggml_matvec_f16_ncols_1_bs_64(
	.param .u64 ggml_matvec_f16_ncols_1_bs_64_param_0,
	.param .u64 ggml_matvec_f16_ncols_1_bs_64_param_1,
	.param .u64 ggml_matvec_f16_ncols_1_bs_64_param_2,
	.param .u32 ggml_matvec_f16_ncols_1_bs_64_param_3,
	.param .u32 ggml_matvec_f16_ncols_1_bs_64_param_4,
	.param .u32 ggml_matvec_f16_ncols_1_bs_64_param_5,
	.param .u32 ggml_matvec_f16_ncols_1_bs_64_param_6,
	.param .u32 ggml_matvec_f16_ncols_1_bs_64_param_7,
	.param .u32 ggml_matvec_f16_ncols_1_bs_64_param_8,
	.param .u32 ggml_matvec_f16_ncols_1_bs_64_param_9,
	.param .u32 ggml_matvec_f16_ncols_1_bs_64_param_10,
	.param .u32 ggml_matvec_f16_ncols_1_bs_64_param_11
)
{
	.reg .pred 	%p<19>;
	.reg .b16 	%rs<31>;
	.reg .f32 	%f<30>;
	.reg .b32 	%r<127>;
	.reg .b64 	%rd<45>;


	ld.param.u64 	%rd14, [ggml_matvec_f16_ncols_1_bs_64_param_0];
	ld.param.u64 	%rd15, [ggml_matvec_f16_ncols_1_bs_64_param_1];
	ld.param.u64 	%rd16, [ggml_matvec_f16_ncols_1_bs_64_param_2];
	ld.param.u32 	%r13, [ggml_matvec_f16_ncols_1_bs_64_param_3];
	ld.param.u32 	%r17, [ggml_matvec_f16_ncols_1_bs_64_param_5];
	ld.param.u32 	%r14, [ggml_matvec_f16_ncols_1_bs_64_param_7];
	ld.param.u32 	%r18, [ggml_matvec_f16_ncols_1_bs_64_param_8];
	ld.param.u32 	%r19, [ggml_matvec_f16_ncols_1_bs_64_param_9];
	ld.param.u32 	%r15, [ggml_matvec_f16_ncols_1_bs_64_param_10];
	ld.param.u32 	%r16, [ggml_matvec_f16_ncols_1_bs_64_param_11];
	mov.u32 	%r20, %ctaid.x;
	mov.u32 	%r21, %ctaid.y;
	div.s32 	%r22, %r21, %r18;
	mul.lo.s32 	%r23, %r20, %r17;
	mad.lo.s32 	%r24, %r22, %r19, %r23;
	cvt.s64.s32 	%rd1, %r24;
	mov.u32 	%r1, %tid.x;
	setp.gt.s32 	%p1, %r1, 31;
	shl.b32 	%r25, %r1, 2;
	mov.u32 	%r26, data_mmv;
	add.s32 	%r2, %r26, %r25;
	@%p1 bra 	$L__BB72_2;

	mov.u32 	%r27, 0;
	st.shared.u32 	[%r2], %r27;

$L__BB72_2:
	bar.sync 	0;
	mov.f32 	%f5, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs30, %f5;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs29, %f5;}

	// end inline asm
	setp.ge.s32 	%p2, %r1, %r13;
	@%p2 bra 	$L__BB72_9;

	not.b32 	%r28, %r1;
	add.s32 	%r29, %r28, %r13;
	shr.u32 	%r30, %r29, 6;
	add.s32 	%r31, %r30, 1;
	and.b32  	%r124, %r31, 3;
	setp.eq.s32 	%p3, %r124, 0;
	mov.u32 	%r125, %r1;
	@%p3 bra 	$L__BB72_6;

	mov.u32 	%r125, %tid.x;
	mul.wide.s32 	%rd17, %r125, 2;
	mul.lo.s32 	%r33, %r21, %r15;
	cvt.s64.s32 	%rd18, %r33;
	add.s64 	%rd19, %rd17, %rd18;
	cvta.to.global.u64 	%rd20, %rd15;
	shl.b64 	%rd21, %rd19, 1;
	add.s64 	%rd42, %rd20, %rd21;
	add.s64 	%rd22, %rd17, %rd1;
	cvta.to.global.u64 	%rd23, %rd14;
	shl.b64 	%rd24, %rd22, 1;
	add.s64 	%rd41, %rd23, %rd24;

$L__BB72_5:
	.pragma "nounroll";
	ld.global.nc.u32 	%r35, [%rd41];
	ld.global.nc.u32 	%r36, [%rd42];
	// begin inline asm
	{mul.f16x2 %r34,%r35,%r36;
}
	// end inline asm
	mov.b32 	%r38, {%rs30, %rs29};
	// begin inline asm
	{add.f16x2 %r37,%r38,%r34;
}
	// end inline asm
	mov.b32 	{%rs30, %rs29}, %r37;
	add.s32 	%r125, %r125, 64;
	add.s64 	%rd42, %rd42, 256;
	add.s64 	%rd41, %rd41, 256;
	add.s32 	%r124, %r124, -1;
	setp.ne.s32 	%p4, %r124, 0;
	@%p4 bra 	$L__BB72_5;

$L__BB72_6:
	setp.lt.u32 	%p5, %r29, 192;
	@%p5 bra 	$L__BB72_9;

	mul.wide.s32 	%rd25, %r125, 2;
	cvta.to.global.u64 	%rd26, %rd14;
	add.s64 	%rd27, %rd25, %rd1;
	shl.b64 	%rd28, %rd27, 1;
	add.s64 	%rd29, %rd26, %rd28;
	add.s64 	%rd44, %rd29, 512;
	mul.lo.s32 	%r44, %r21, %r15;
	cvt.s64.s32 	%rd30, %r44;
	cvta.to.global.u64 	%rd31, %rd15;
	add.s64 	%rd32, %rd25, %rd30;
	shl.b64 	%rd33, %rd32, 1;
	add.s64 	%rd34, %rd31, %rd33;
	add.s64 	%rd43, %rd34, 512;

$L__BB72_8:
	ld.global.nc.u32 	%r46, [%rd44+-512];
	ld.global.nc.u32 	%r47, [%rd43+-512];
	// begin inline asm
	{mul.f16x2 %r45,%r46,%r47;
}
	// end inline asm
	mov.b32 	%r49, {%rs30, %rs29};
	// begin inline asm
	{add.f16x2 %r48,%r49,%r45;
}
	// end inline asm
	ld.global.nc.u32 	%r52, [%rd44+-256];
	ld.global.nc.u32 	%r53, [%rd43+-256];
	// begin inline asm
	{mul.f16x2 %r51,%r52,%r53;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r54,%r48,%r51;
}
	// end inline asm
	ld.global.nc.u32 	%r58, [%rd44];
	ld.global.nc.u32 	%r59, [%rd43];
	// begin inline asm
	{mul.f16x2 %r57,%r58,%r59;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r60,%r54,%r57;
}
	// end inline asm
	ld.global.nc.u32 	%r64, [%rd44+256];
	ld.global.nc.u32 	%r65, [%rd43+256];
	// begin inline asm
	{mul.f16x2 %r63,%r64,%r65;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r66,%r60,%r63;
}
	// end inline asm
	mov.b32 	{%rs30, %rs29}, %r66;
	add.s64 	%rd44, %rd44, 1024;
	add.s64 	%rd43, %rd43, 1024;
	add.s32 	%r125, %r125, 256;
	setp.lt.s32 	%p6, %r125, %r13;
	@%p6 bra 	$L__BB72_8;

$L__BB72_9:
	mov.u32 	%r72, 1;
	mov.b32 	%r70, {%rs30, %rs29};
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r70;
  cvt.f32.f16 %f6, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r70;
  cvt.f32.f16 %f7, high;}

	// end inline asm
	add.f32 	%f8, %f6, %f7;
	mov.b32 	%r73, %f8;
	mov.u32 	%r74, 31;
	mov.u32 	%r75, 16;
	mov.u32 	%r76, -1;
	shfl.sync.bfly.b32 	%r77|%p8, %r73, %r75, %r74, %r76;
	mov.b32 	%f9, %r77;
	add.f32 	%f10, %f8, %f9;
	mov.b32 	%r78, %f10;
	mov.u32 	%r79, 8;
	shfl.sync.bfly.b32 	%r80|%p9, %r78, %r79, %r74, %r76;
	mov.b32 	%f11, %r80;
	add.f32 	%f12, %f10, %f11;
	mov.b32 	%r81, %f12;
	mov.u32 	%r82, 4;
	shfl.sync.bfly.b32 	%r83|%p10, %r81, %r82, %r74, %r76;
	mov.b32 	%f13, %r83;
	add.f32 	%f14, %f12, %f13;
	mov.b32 	%r84, %f14;
	mov.u32 	%r85, 2;
	shfl.sync.bfly.b32 	%r86|%p11, %r84, %r85, %r74, %r76;
	mov.b32 	%f15, %r86;
	add.f32 	%f16, %f14, %f15;
	mov.b32 	%r87, %f16;
	shfl.sync.bfly.b32 	%r88|%p12, %r87, %r72, %r74, %r76;
	mov.b32 	%f17, %r88;
	add.f32 	%f29, %f16, %f17;
	shr.s32 	%r89, %r1, 31;
	shr.u32 	%r90, %r89, 27;
	add.s32 	%r91, %r1, %r90;
	shr.s32 	%r92, %r91, 5;
	shl.b32 	%r93, %r92, 2;
	add.s32 	%r95, %r26, %r93;
	st.shared.f32 	[%r95], %f29;
	bar.sync 	0;
	@%p1 bra 	$L__BB72_11;

	ld.shared.f32 	%f18, [%r2];
	mov.b32 	%r101, %f18;
	shfl.sync.bfly.b32 	%r105|%p13, %r101, %r75, %r74, %r76;
	mov.b32 	%f19, %r105;
	add.f32 	%f20, %f18, %f19;
	mov.b32 	%r106, %f20;
	shfl.sync.bfly.b32 	%r108|%p14, %r106, %r79, %r74, %r76;
	mov.b32 	%f21, %r108;
	add.f32 	%f22, %f20, %f21;
	mov.b32 	%r109, %f22;
	shfl.sync.bfly.b32 	%r111|%p15, %r109, %r82, %r74, %r76;
	mov.b32 	%f23, %r111;
	add.f32 	%f24, %f22, %f23;
	mov.b32 	%r112, %f24;
	shfl.sync.bfly.b32 	%r113|%p16, %r112, %r85, %r74, %r76;
	mov.b32 	%f25, %r113;
	add.f32 	%f26, %f24, %f25;
	mov.b32 	%r114, %f26;
	shfl.sync.bfly.b32 	%r116|%p17, %r114, %r72, %r74, %r76;
	mov.b32 	%f27, %r116;
	add.f32 	%f29, %f26, %f27;

$L__BB72_11:
	bar.sync 	0;
	setp.gt.s32 	%p18, %r1, 0;
	@%p18 bra 	$L__BB72_13;

	mad.lo.s32 	%r120, %r1, %r14, %r20;
	cvt.s64.s32 	%rd35, %r120;
	mul.lo.s32 	%r122, %r21, %r16;
	cvt.s64.s32 	%rd36, %r122;
	add.s64 	%rd37, %rd36, %rd35;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs20, %f29;}

	// end inline asm
	cvta.to.global.u64 	%rd38, %rd16;
	shl.b64 	%rd39, %rd37, 1;
	add.s64 	%rd40, %rd38, %rd39;
	st.global.u16 	[%rd40], %rs20;

$L__BB72_13:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_2_bs_64
.visible .entry ggml_matvec_f16_ncols_2_bs_64(
	.param .u64 ggml_matvec_f16_ncols_2_bs_64_param_0,
	.param .u64 ggml_matvec_f16_ncols_2_bs_64_param_1,
	.param .u64 ggml_matvec_f16_ncols_2_bs_64_param_2,
	.param .u32 ggml_matvec_f16_ncols_2_bs_64_param_3,
	.param .u32 ggml_matvec_f16_ncols_2_bs_64_param_4,
	.param .u32 ggml_matvec_f16_ncols_2_bs_64_param_5,
	.param .u32 ggml_matvec_f16_ncols_2_bs_64_param_6,
	.param .u32 ggml_matvec_f16_ncols_2_bs_64_param_7,
	.param .u32 ggml_matvec_f16_ncols_2_bs_64_param_8,
	.param .u32 ggml_matvec_f16_ncols_2_bs_64_param_9,
	.param .u32 ggml_matvec_f16_ncols_2_bs_64_param_10,
	.param .u32 ggml_matvec_f16_ncols_2_bs_64_param_11
)
{
	.local .align 8 .b8 	__local_depot73[8];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<30>;
	.reg .b16 	%rs<4>;
	.reg .f32 	%f<52>;
	.reg .b32 	%r<201>;
	.reg .b64 	%rd<64>;


	mov.u64 	%SPL, __local_depot73;
	ld.param.u64 	%rd27, [ggml_matvec_f16_ncols_2_bs_64_param_0];
	ld.param.u64 	%rd28, [ggml_matvec_f16_ncols_2_bs_64_param_1];
	ld.param.u64 	%rd26, [ggml_matvec_f16_ncols_2_bs_64_param_2];
	ld.param.u32 	%r28, [ggml_matvec_f16_ncols_2_bs_64_param_3];
	ld.param.u32 	%r32, [ggml_matvec_f16_ncols_2_bs_64_param_5];
	ld.param.u32 	%r29, [ggml_matvec_f16_ncols_2_bs_64_param_6];
	ld.param.u32 	%r30, [ggml_matvec_f16_ncols_2_bs_64_param_7];
	ld.param.u32 	%r33, [ggml_matvec_f16_ncols_2_bs_64_param_8];
	ld.param.u32 	%r34, [ggml_matvec_f16_ncols_2_bs_64_param_9];
	ld.param.u32 	%r35, [ggml_matvec_f16_ncols_2_bs_64_param_10];
	ld.param.u32 	%r31, [ggml_matvec_f16_ncols_2_bs_64_param_11];
	cvta.to.global.u64 	%rd1, %rd28;
	cvta.to.global.u64 	%rd2, %rd27;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r36, %r1, %r33;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r37, %r2, %r32;
	mad.lo.s32 	%r38, %r36, %r34, %r37;
	cvt.s64.s32 	%rd4, %r38;
	mul.lo.s32 	%r39, %r1, %r35;
	cvt.s64.s32 	%rd5, %r39;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r40, %r3, 2;
	mov.u32 	%r41, data_mmv;
	add.s32 	%r4, %r41, %r40;
	@%p1 bra 	$L__BB73_2;

	mov.u32 	%r42, 0;
	st.shared.u32 	[%r4], %r42;

$L__BB73_2:
	bar.sync 	0;
	mov.f32 	%f3, 0f00000000;
	st.local.v2.f32 	[%rd3], {%f3, %f3};
	mov.u32 	%r199, 0;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs2, %f3;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f3;}

	// end inline asm
	mov.b32 	%r200, {%rs1, %rs2};
	setp.ge.s32 	%p2, %r3, %r28;
	@%p2 bra 	$L__BB73_9;

	not.b32 	%r45, %r3;
	add.s32 	%r6, %r45, %r28;
	shr.u32 	%r46, %r6, 6;
	add.s32 	%r47, %r46, 1;
	and.b32  	%r192, %r47, 3;
	setp.eq.s32 	%p3, %r192, 0;
	mov.u32 	%r199, 0;
	mov.u32 	%r195, %r3;
	@%p3 bra 	$L__BB73_6;

	mul.wide.s32 	%rd30, %r29, 2;
	mul.wide.s32 	%rd31, %r3, 2;
	add.s64 	%rd32, %rd30, %rd31;
	add.s64 	%rd33, %rd32, %rd5;
	shl.b64 	%rd34, %rd33, 1;
	add.s64 	%rd60, %rd1, %rd34;
	add.s64 	%rd35, %rd31, %rd5;
	shl.b64 	%rd36, %rd35, 1;
	add.s64 	%rd59, %rd1, %rd36;
	add.s64 	%rd37, %rd31, %rd4;
	shl.b64 	%rd38, %rd37, 1;
	add.s64 	%rd58, %rd2, %rd38;
	mov.u32 	%r199, 0;
	mov.u32 	%r195, %r3;

$L__BB73_5:
	.pragma "nounroll";
	ld.global.nc.u32 	%r50, [%rd58];
	ld.global.nc.u32 	%r51, [%rd59];
	// begin inline asm
	{mul.f16x2 %r49,%r50,%r51;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r200,%r200,%r49;
}
	// end inline asm
	ld.global.nc.u32 	%r57, [%rd60];
	// begin inline asm
	{mul.f16x2 %r55,%r50,%r57;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r199,%r199,%r55;
}
	// end inline asm
	add.s32 	%r195, %r195, 64;
	add.s64 	%rd60, %rd60, 256;
	add.s64 	%rd59, %rd59, 256;
	add.s64 	%rd58, %rd58, 256;
	add.s32 	%r192, %r192, -1;
	setp.ne.s32 	%p4, %r192, 0;
	@%p4 bra 	$L__BB73_5;

$L__BB73_6:
	setp.lt.u32 	%p5, %r6, 192;
	@%p5 bra 	$L__BB73_9;

	mul.wide.s32 	%rd39, %r195, 2;
	add.s64 	%rd40, %rd39, %rd4;
	shl.b64 	%rd41, %rd40, 1;
	add.s64 	%rd42, %rd2, %rd41;
	add.s64 	%rd63, %rd42, 512;
	add.s64 	%rd43, %rd39, %rd5;
	shl.b64 	%rd44, %rd43, 1;
	add.s64 	%rd45, %rd1, %rd44;
	add.s64 	%rd62, %rd45, 768;
	mul.wide.s32 	%rd46, %r29, 2;
	add.s64 	%rd47, %rd43, %rd46;
	shl.b64 	%rd48, %rd47, 1;
	add.s64 	%rd49, %rd1, %rd48;
	add.s64 	%rd61, %rd49, 512;

$L__BB73_8:
	ld.global.nc.u32 	%r62, [%rd63+-512];
	ld.global.nc.u32 	%r63, [%rd62+-768];
	// begin inline asm
	{mul.f16x2 %r61,%r62,%r63;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r64,%r200,%r61;
}
	// end inline asm
	ld.global.nc.u32 	%r69, [%rd61+-512];
	// begin inline asm
	{mul.f16x2 %r67,%r62,%r69;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r70,%r199,%r67;
}
	// end inline asm
	ld.global.nc.u32 	%r74, [%rd63+-256];
	ld.global.nc.u32 	%r75, [%rd62+-512];
	// begin inline asm
	{mul.f16x2 %r73,%r74,%r75;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r76,%r64,%r73;
}
	// end inline asm
	ld.global.nc.u32 	%r81, [%rd61+-256];
	// begin inline asm
	{mul.f16x2 %r79,%r74,%r81;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r82,%r70,%r79;
}
	// end inline asm
	ld.global.nc.u32 	%r86, [%rd63];
	ld.global.nc.u32 	%r87, [%rd62+-256];
	// begin inline asm
	{mul.f16x2 %r85,%r86,%r87;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r88,%r76,%r85;
}
	// end inline asm
	ld.global.nc.u32 	%r93, [%rd61];
	// begin inline asm
	{mul.f16x2 %r91,%r86,%r93;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r94,%r82,%r91;
}
	// end inline asm
	ld.global.nc.u32 	%r98, [%rd63+256];
	ld.global.nc.u32 	%r99, [%rd62];
	// begin inline asm
	{mul.f16x2 %r97,%r98,%r99;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r200,%r88,%r97;
}
	// end inline asm
	ld.global.nc.u32 	%r105, [%rd61+256];
	// begin inline asm
	{mul.f16x2 %r103,%r98,%r105;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r199,%r94,%r103;
}
	// end inline asm
	add.s64 	%rd63, %rd63, 1024;
	add.s64 	%rd62, %rd62, 1024;
	add.s64 	%rd61, %rd61, 1024;
	add.s32 	%r195, %r195, 256;
	setp.lt.s32 	%p6, %r195, %r28;
	@%p6 bra 	$L__BB73_8;

$L__BB73_9:
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r200;
  cvt.f32.f16 %f4, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r200;
  cvt.f32.f16 %f5, high;}

	// end inline asm
	add.f32 	%f8, %f4, %f5;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r199;
  cvt.f32.f16 %f6, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r199;
  cvt.f32.f16 %f7, high;}

	// end inline asm
	add.f32 	%f1, %f6, %f7;
	st.local.f32 	[%rd3+4], %f1;
	shr.s32 	%r113, %r3, 31;
	shr.u32 	%r114, %r113, 27;
	add.s32 	%r115, %r3, %r114;
	shr.s32 	%r116, %r115, 5;
	shl.b32 	%r117, %r116, 2;
	add.s32 	%r27, %r41, %r117;
	mov.u32 	%r119, 2;
	mov.b32 	%r120, %f8;
	mov.u32 	%r121, 31;
	mov.u32 	%r122, 16;
	mov.u32 	%r123, -1;
	shfl.sync.bfly.b32 	%r124|%p7, %r120, %r122, %r121, %r123;
	mov.b32 	%f9, %r124;
	add.f32 	%f10, %f8, %f9;
	mov.b32 	%r125, %f10;
	mov.u32 	%r126, 8;
	shfl.sync.bfly.b32 	%r127|%p8, %r125, %r126, %r121, %r123;
	mov.b32 	%f11, %r127;
	add.f32 	%f12, %f10, %f11;
	mov.b32 	%r128, %f12;
	mov.u32 	%r129, 4;
	shfl.sync.bfly.b32 	%r130|%p9, %r128, %r129, %r121, %r123;
	mov.b32 	%f13, %r130;
	add.f32 	%f14, %f12, %f13;
	mov.b32 	%r131, %f14;
	shfl.sync.bfly.b32 	%r132|%p10, %r131, %r119, %r121, %r123;
	mov.b32 	%f15, %r132;
	add.f32 	%f16, %f14, %f15;
	mov.b32 	%r133, %f16;
	mov.u32 	%r134, 1;
	shfl.sync.bfly.b32 	%r135|%p11, %r133, %r134, %r121, %r123;
	mov.b32 	%f17, %r135;
	add.f32 	%f18, %f16, %f17;
	st.local.f32 	[%rd3], %f18;
	st.shared.f32 	[%r27], %f18;
	bar.sync 	0;
	@%p1 bra 	$L__BB73_11;

	ld.shared.f32 	%f19, [%r4];
	mov.b32 	%r136, %f19;
	shfl.sync.bfly.b32 	%r140|%p13, %r136, %r122, %r121, %r123;
	mov.b32 	%f20, %r140;
	add.f32 	%f21, %f19, %f20;
	mov.b32 	%r141, %f21;
	shfl.sync.bfly.b32 	%r143|%p14, %r141, %r126, %r121, %r123;
	mov.b32 	%f22, %r143;
	add.f32 	%f23, %f21, %f22;
	mov.b32 	%r144, %f23;
	shfl.sync.bfly.b32 	%r146|%p15, %r144, %r129, %r121, %r123;
	mov.b32 	%f24, %r146;
	add.f32 	%f25, %f23, %f24;
	mov.b32 	%r147, %f25;
	shfl.sync.bfly.b32 	%r149|%p16, %r147, %r119, %r121, %r123;
	mov.b32 	%f26, %r149;
	add.f32 	%f27, %f25, %f26;
	mov.b32 	%r150, %f27;
	shfl.sync.bfly.b32 	%r152|%p17, %r150, %r134, %r121, %r123;
	mov.b32 	%f28, %r152;
	add.f32 	%f29, %f27, %f28;
	st.local.f32 	[%rd3], %f29;

$L__BB73_11:
	bar.sync 	0;
	mov.b32 	%r153, %f1;
	shfl.sync.bfly.b32 	%r157|%p19, %r153, %r122, %r121, %r123;
	mov.b32 	%f30, %r157;
	add.f32 	%f31, %f1, %f30;
	mov.b32 	%r158, %f31;
	shfl.sync.bfly.b32 	%r160|%p20, %r158, %r126, %r121, %r123;
	mov.b32 	%f32, %r160;
	add.f32 	%f33, %f31, %f32;
	mov.b32 	%r161, %f33;
	shfl.sync.bfly.b32 	%r163|%p21, %r161, %r129, %r121, %r123;
	mov.b32 	%f34, %r163;
	add.f32 	%f35, %f33, %f34;
	mov.b32 	%r164, %f35;
	shfl.sync.bfly.b32 	%r166|%p22, %r164, %r119, %r121, %r123;
	mov.b32 	%f36, %r166;
	add.f32 	%f37, %f35, %f36;
	mov.b32 	%r167, %f37;
	shfl.sync.bfly.b32 	%r169|%p23, %r167, %r134, %r121, %r123;
	mov.b32 	%f38, %r169;
	add.f32 	%f39, %f37, %f38;
	st.local.f32 	[%rd3+4], %f39;
	st.shared.f32 	[%r27], %f39;
	bar.sync 	0;
	@%p1 bra 	$L__BB73_13;

	ld.shared.f32 	%f40, [%r4];
	mov.b32 	%r170, %f40;
	mov.u32 	%r171, 31;
	mov.u32 	%r172, 16;
	mov.u32 	%r173, -1;
	shfl.sync.bfly.b32 	%r174|%p24, %r170, %r172, %r171, %r173;
	mov.b32 	%f41, %r174;
	add.f32 	%f42, %f40, %f41;
	mov.b32 	%r175, %f42;
	mov.u32 	%r176, 8;
	shfl.sync.bfly.b32 	%r177|%p25, %r175, %r176, %r171, %r173;
	mov.b32 	%f43, %r177;
	add.f32 	%f44, %f42, %f43;
	mov.b32 	%r178, %f44;
	mov.u32 	%r179, 4;
	shfl.sync.bfly.b32 	%r180|%p26, %r178, %r179, %r171, %r173;
	mov.b32 	%f45, %r180;
	add.f32 	%f46, %f44, %f45;
	mov.b32 	%r181, %f46;
	mov.u32 	%r182, 2;
	shfl.sync.bfly.b32 	%r183|%p27, %r181, %r182, %r171, %r173;
	mov.b32 	%f47, %r183;
	add.f32 	%f48, %f46, %f47;
	mov.b32 	%r184, %f48;
	mov.u32 	%r185, 1;
	shfl.sync.bfly.b32 	%r186|%p28, %r184, %r185, %r171, %r173;
	mov.b32 	%f49, %r186;
	add.f32 	%f50, %f48, %f49;
	st.local.f32 	[%rd3+4], %f50;

$L__BB73_13:
	bar.sync 	0;
	setp.gt.s32 	%p29, %r3, 1;
	@%p29 bra 	$L__BB73_15;

	mad.lo.s32 	%r187, %r3, %r30, %r2;
	cvt.s64.s32 	%rd50, %r187;
	mul.lo.s32 	%r188, %r1, %r31;
	cvt.s64.s32 	%rd51, %r188;
	add.s64 	%rd52, %rd51, %rd50;
	mul.wide.s32 	%rd53, %r3, 4;
	add.s64 	%rd54, %rd3, %rd53;
	ld.local.f32 	%f51, [%rd54];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs3, %f51;}

	// end inline asm
	cvta.to.global.u64 	%rd55, %rd26;
	shl.b64 	%rd56, %rd52, 1;
	add.s64 	%rd57, %rd55, %rd56;
	st.global.u16 	[%rd57], %rs3;

$L__BB73_15:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_3_bs_64
.visible .entry ggml_matvec_f16_ncols_3_bs_64(
	.param .u64 ggml_matvec_f16_ncols_3_bs_64_param_0,
	.param .u64 ggml_matvec_f16_ncols_3_bs_64_param_1,
	.param .u64 ggml_matvec_f16_ncols_3_bs_64_param_2,
	.param .u32 ggml_matvec_f16_ncols_3_bs_64_param_3,
	.param .u32 ggml_matvec_f16_ncols_3_bs_64_param_4,
	.param .u32 ggml_matvec_f16_ncols_3_bs_64_param_5,
	.param .u32 ggml_matvec_f16_ncols_3_bs_64_param_6,
	.param .u32 ggml_matvec_f16_ncols_3_bs_64_param_7,
	.param .u32 ggml_matvec_f16_ncols_3_bs_64_param_8,
	.param .u32 ggml_matvec_f16_ncols_3_bs_64_param_9,
	.param .u32 ggml_matvec_f16_ncols_3_bs_64_param_10,
	.param .u32 ggml_matvec_f16_ncols_3_bs_64_param_11
)
{
	.local .align 4 .b8 	__local_depot74[12];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<41>;
	.reg .b16 	%rs<4>;
	.reg .f32 	%f<76>;
	.reg .b32 	%r<286>;
	.reg .b64 	%rd<72>;


	mov.u64 	%SPL, __local_depot74;
	ld.param.u64 	%rd29, [ggml_matvec_f16_ncols_3_bs_64_param_0];
	ld.param.u64 	%rd30, [ggml_matvec_f16_ncols_3_bs_64_param_1];
	ld.param.u64 	%rd28, [ggml_matvec_f16_ncols_3_bs_64_param_2];
	ld.param.u32 	%r34, [ggml_matvec_f16_ncols_3_bs_64_param_3];
	ld.param.u32 	%r38, [ggml_matvec_f16_ncols_3_bs_64_param_5];
	ld.param.u32 	%r35, [ggml_matvec_f16_ncols_3_bs_64_param_6];
	ld.param.u32 	%r36, [ggml_matvec_f16_ncols_3_bs_64_param_7];
	ld.param.u32 	%r39, [ggml_matvec_f16_ncols_3_bs_64_param_8];
	ld.param.u32 	%r40, [ggml_matvec_f16_ncols_3_bs_64_param_9];
	ld.param.u32 	%r41, [ggml_matvec_f16_ncols_3_bs_64_param_10];
	ld.param.u32 	%r37, [ggml_matvec_f16_ncols_3_bs_64_param_11];
	cvta.to.global.u64 	%rd71, %rd30;
	cvta.to.global.u64 	%rd2, %rd29;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r42, %r1, %r39;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r43, %r2, %r38;
	mad.lo.s32 	%r44, %r42, %r40, %r43;
	cvt.s64.s32 	%rd4, %r44;
	mul.lo.s32 	%r45, %r1, %r41;
	cvt.s64.s32 	%rd5, %r45;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r46, %r3, 2;
	mov.u32 	%r47, data_mmv;
	add.s32 	%r4, %r47, %r46;
	@%p1 bra 	$L__BB74_2;

	mov.u32 	%r48, 0;
	st.shared.u32 	[%r4], %r48;

$L__BB74_2:
	bar.sync 	0;
	mov.f32 	%f4, 0f00000000;
	mov.u32 	%r283, 0;
	st.local.u32 	[%rd3], %r283;
	st.local.u32 	[%rd3+4], %r283;
	st.local.u32 	[%rd3+8], %r283;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs2, %f4;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f4;}

	// end inline asm
	mov.b32 	%r285, {%rs1, %rs2};
	setp.ge.s32 	%p2, %r3, %r34;
	mov.u32 	%r284, %r283;
	@%p2 bra 	$L__BB74_9;

	not.b32 	%r53, %r3;
	add.s32 	%r6, %r53, %r34;
	shr.u32 	%r54, %r6, 6;
	add.s32 	%r55, %r54, 1;
	and.b32  	%r274, %r55, 3;
	setp.eq.s32 	%p3, %r274, 0;
	mov.u32 	%r283, 0;
	mov.u32 	%r278, %r3;
	@%p3 bra 	$L__BB74_6;

	shl.b32 	%r58, %r35, 1;
	add.s32 	%r59, %r3, %r58;
	mul.wide.s32 	%rd32, %r59, 2;
	add.s64 	%rd33, %rd32, %rd5;
	shl.b64 	%rd34, %rd33, 1;
	add.s64 	%rd69, %rd71, %rd34;
	mul.wide.s32 	%rd35, %r35, 2;
	mul.wide.s32 	%rd36, %r3, 2;
	add.s64 	%rd37, %rd35, %rd36;
	add.s64 	%rd38, %rd37, %rd5;
	shl.b64 	%rd39, %rd38, 1;
	add.s64 	%rd68, %rd71, %rd39;
	add.s64 	%rd40, %rd36, %rd5;
	shl.b64 	%rd41, %rd40, 1;
	add.s64 	%rd67, %rd71, %rd41;
	add.s64 	%rd42, %rd36, %rd4;
	shl.b64 	%rd43, %rd42, 1;
	add.s64 	%rd66, %rd2, %rd43;
	mov.u32 	%r283, 0;
	mov.u32 	%r284, %r283;
	mov.u32 	%r278, %r3;

$L__BB74_5:
	.pragma "nounroll";
	ld.global.nc.u32 	%r61, [%rd66];
	ld.global.nc.u32 	%r62, [%rd67];
	// begin inline asm
	{mul.f16x2 %r60,%r61,%r62;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r285,%r285,%r60;
}
	// end inline asm
	ld.global.nc.u32 	%r68, [%rd68];
	// begin inline asm
	{mul.f16x2 %r66,%r61,%r68;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r284,%r284,%r66;
}
	// end inline asm
	ld.global.nc.u32 	%r74, [%rd69];
	// begin inline asm
	{mul.f16x2 %r72,%r61,%r74;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r283,%r283,%r72;
}
	// end inline asm
	add.s32 	%r278, %r278, 64;
	add.s64 	%rd69, %rd69, 256;
	add.s64 	%rd68, %rd68, 256;
	add.s64 	%rd67, %rd67, 256;
	add.s64 	%rd66, %rd66, 256;
	add.s32 	%r274, %r274, -1;
	setp.ne.s32 	%p4, %r274, 0;
	@%p4 bra 	$L__BB74_5;

$L__BB74_6:
	setp.lt.u32 	%p5, %r6, 192;
	@%p5 bra 	$L__BB74_9;

	add.s32 	%r78, %r278, %r35;
	shl.b32 	%r79, %r35, 1;
	add.s32 	%r80, %r278, %r79;
	add.s32 	%r81, %r78, 64;
	mul.wide.s32 	%rd44, %r81, 4;
	shl.b64 	%rd45, %rd5, 1;
	add.s64 	%rd19, %rd44, %rd45;
	mul.wide.s32 	%rd46, %r80, 4;
	add.s64 	%rd20, %rd46, %rd45;
	mul.wide.s32 	%rd47, %r278, 2;
	add.s64 	%rd48, %rd47, %rd4;
	shl.b64 	%rd49, %rd48, 1;
	add.s64 	%rd50, %rd2, %rd49;
	add.s64 	%rd70, %rd50, 512;
	mul.wide.s32 	%rd51, %r278, 4;
	add.s64 	%rd22, %rd51, %rd45;
	mul.wide.s32 	%rd52, %r35, 4;
	add.s64 	%rd53, %rd51, %rd52;
	add.s64 	%rd23, %rd53, %rd45;

$L__BB74_8:
	ld.global.nc.u32 	%r83, [%rd70+-512];
	add.s64 	%rd54, %rd71, %rd22;
	ld.global.nc.u32 	%r84, [%rd54];
	// begin inline asm
	{mul.f16x2 %r82,%r83,%r84;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r85,%r285,%r82;
}
	// end inline asm
	add.s64 	%rd55, %rd71, %rd23;
	ld.global.nc.u32 	%r90, [%rd55];
	// begin inline asm
	{mul.f16x2 %r88,%r83,%r90;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r91,%r284,%r88;
}
	// end inline asm
	add.s64 	%rd56, %rd71, %rd20;
	ld.global.nc.u32 	%r96, [%rd56];
	// begin inline asm
	{mul.f16x2 %r94,%r83,%r96;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r97,%r283,%r94;
}
	// end inline asm
	ld.global.nc.u32 	%r101, [%rd70+-256];
	ld.global.nc.u32 	%r102, [%rd54+256];
	// begin inline asm
	{mul.f16x2 %r100,%r101,%r102;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r103,%r85,%r100;
}
	// end inline asm
	add.s64 	%rd57, %rd71, %rd19;
	ld.global.nc.u32 	%r108, [%rd57];
	// begin inline asm
	{mul.f16x2 %r106,%r101,%r108;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r109,%r91,%r106;
}
	// end inline asm
	ld.global.nc.u32 	%r114, [%rd56+256];
	// begin inline asm
	{mul.f16x2 %r112,%r101,%r114;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r115,%r97,%r112;
}
	// end inline asm
	ld.global.nc.u32 	%r119, [%rd70];
	ld.global.nc.u32 	%r120, [%rd54+512];
	// begin inline asm
	{mul.f16x2 %r118,%r119,%r120;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r121,%r103,%r118;
}
	// end inline asm
	ld.global.nc.u32 	%r126, [%rd57+256];
	// begin inline asm
	{mul.f16x2 %r124,%r119,%r126;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r127,%r109,%r124;
}
	// end inline asm
	ld.global.nc.u32 	%r132, [%rd56+512];
	// begin inline asm
	{mul.f16x2 %r130,%r119,%r132;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r133,%r115,%r130;
}
	// end inline asm
	ld.global.nc.u32 	%r137, [%rd70+256];
	ld.global.nc.u32 	%r138, [%rd54+768];
	// begin inline asm
	{mul.f16x2 %r136,%r137,%r138;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r285,%r121,%r136;
}
	// end inline asm
	ld.global.nc.u32 	%r144, [%rd57+512];
	// begin inline asm
	{mul.f16x2 %r142,%r137,%r144;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r284,%r127,%r142;
}
	// end inline asm
	ld.global.nc.u32 	%r150, [%rd56+768];
	// begin inline asm
	{mul.f16x2 %r148,%r137,%r150;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r283,%r133,%r148;
}
	// end inline asm
	add.s64 	%rd71, %rd71, 1024;
	add.s64 	%rd70, %rd70, 1024;
	add.s32 	%r278, %r278, 256;
	setp.lt.s32 	%p6, %r278, %r34;
	@%p6 bra 	$L__BB74_8;

$L__BB74_9:
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r285;
  cvt.f32.f16 %f5, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r285;
  cvt.f32.f16 %f6, high;}

	// end inline asm
	add.f32 	%f11, %f5, %f6;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r284;
  cvt.f32.f16 %f7, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r284;
  cvt.f32.f16 %f8, high;}

	// end inline asm
	add.f32 	%f1, %f7, %f8;
	st.local.f32 	[%rd3+4], %f1;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r283;
  cvt.f32.f16 %f9, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r283;
  cvt.f32.f16 %f10, high;}

	// end inline asm
	add.f32 	%f2, %f9, %f10;
	st.local.f32 	[%rd3+8], %f2;
	shr.s32 	%r160, %r3, 31;
	shr.u32 	%r161, %r160, 27;
	add.s32 	%r162, %r3, %r161;
	shr.s32 	%r163, %r162, 5;
	shl.b32 	%r164, %r163, 2;
	add.s32 	%r33, %r47, %r164;
	mov.u32 	%r166, 2;
	mov.b32 	%r167, %f11;
	mov.u32 	%r168, 31;
	mov.u32 	%r169, 16;
	mov.u32 	%r170, -1;
	shfl.sync.bfly.b32 	%r171|%p7, %r167, %r169, %r168, %r170;
	mov.b32 	%f12, %r171;
	add.f32 	%f13, %f11, %f12;
	mov.b32 	%r172, %f13;
	mov.u32 	%r173, 8;
	shfl.sync.bfly.b32 	%r174|%p8, %r172, %r173, %r168, %r170;
	mov.b32 	%f14, %r174;
	add.f32 	%f15, %f13, %f14;
	mov.b32 	%r175, %f15;
	mov.u32 	%r176, 4;
	shfl.sync.bfly.b32 	%r177|%p9, %r175, %r176, %r168, %r170;
	mov.b32 	%f16, %r177;
	add.f32 	%f17, %f15, %f16;
	mov.b32 	%r178, %f17;
	shfl.sync.bfly.b32 	%r179|%p10, %r178, %r166, %r168, %r170;
	mov.b32 	%f18, %r179;
	add.f32 	%f19, %f17, %f18;
	mov.b32 	%r180, %f19;
	mov.u32 	%r181, 1;
	shfl.sync.bfly.b32 	%r182|%p11, %r180, %r181, %r168, %r170;
	mov.b32 	%f20, %r182;
	add.f32 	%f21, %f19, %f20;
	st.local.f32 	[%rd3], %f21;
	st.shared.f32 	[%r33], %f21;
	bar.sync 	0;
	@%p1 bra 	$L__BB74_11;

	ld.shared.f32 	%f22, [%r4];
	mov.b32 	%r183, %f22;
	shfl.sync.bfly.b32 	%r187|%p13, %r183, %r169, %r168, %r170;
	mov.b32 	%f23, %r187;
	add.f32 	%f24, %f22, %f23;
	mov.b32 	%r188, %f24;
	shfl.sync.bfly.b32 	%r190|%p14, %r188, %r173, %r168, %r170;
	mov.b32 	%f25, %r190;
	add.f32 	%f26, %f24, %f25;
	mov.b32 	%r191, %f26;
	shfl.sync.bfly.b32 	%r193|%p15, %r191, %r176, %r168, %r170;
	mov.b32 	%f27, %r193;
	add.f32 	%f28, %f26, %f27;
	mov.b32 	%r194, %f28;
	shfl.sync.bfly.b32 	%r196|%p16, %r194, %r166, %r168, %r170;
	mov.b32 	%f29, %r196;
	add.f32 	%f30, %f28, %f29;
	mov.b32 	%r197, %f30;
	shfl.sync.bfly.b32 	%r199|%p17, %r197, %r181, %r168, %r170;
	mov.b32 	%f31, %r199;
	add.f32 	%f32, %f30, %f31;
	st.local.f32 	[%rd3], %f32;

$L__BB74_11:
	bar.sync 	0;
	mov.b32 	%r200, %f1;
	shfl.sync.bfly.b32 	%r204|%p19, %r200, %r169, %r168, %r170;
	mov.b32 	%f33, %r204;
	add.f32 	%f34, %f1, %f33;
	mov.b32 	%r205, %f34;
	shfl.sync.bfly.b32 	%r207|%p20, %r205, %r173, %r168, %r170;
	mov.b32 	%f35, %r207;
	add.f32 	%f36, %f34, %f35;
	mov.b32 	%r208, %f36;
	shfl.sync.bfly.b32 	%r210|%p21, %r208, %r176, %r168, %r170;
	mov.b32 	%f37, %r210;
	add.f32 	%f38, %f36, %f37;
	mov.b32 	%r211, %f38;
	shfl.sync.bfly.b32 	%r213|%p22, %r211, %r166, %r168, %r170;
	mov.b32 	%f39, %r213;
	add.f32 	%f40, %f38, %f39;
	mov.b32 	%r214, %f40;
	shfl.sync.bfly.b32 	%r216|%p23, %r214, %r181, %r168, %r170;
	mov.b32 	%f41, %r216;
	add.f32 	%f42, %f40, %f41;
	st.local.f32 	[%rd3+4], %f42;
	st.shared.f32 	[%r33], %f42;
	bar.sync 	0;
	@%p1 bra 	$L__BB74_13;

	ld.shared.f32 	%f43, [%r4];
	mov.b32 	%r217, %f43;
	mov.u32 	%r218, 31;
	mov.u32 	%r219, 16;
	mov.u32 	%r220, -1;
	shfl.sync.bfly.b32 	%r221|%p24, %r217, %r219, %r218, %r220;
	mov.b32 	%f44, %r221;
	add.f32 	%f45, %f43, %f44;
	mov.b32 	%r222, %f45;
	mov.u32 	%r223, 8;
	shfl.sync.bfly.b32 	%r224|%p25, %r222, %r223, %r218, %r220;
	mov.b32 	%f46, %r224;
	add.f32 	%f47, %f45, %f46;
	mov.b32 	%r225, %f47;
	mov.u32 	%r226, 4;
	shfl.sync.bfly.b32 	%r227|%p26, %r225, %r226, %r218, %r220;
	mov.b32 	%f48, %r227;
	add.f32 	%f49, %f47, %f48;
	mov.b32 	%r228, %f49;
	mov.u32 	%r229, 2;
	shfl.sync.bfly.b32 	%r230|%p27, %r228, %r229, %r218, %r220;
	mov.b32 	%f50, %r230;
	add.f32 	%f51, %f49, %f50;
	mov.b32 	%r231, %f51;
	mov.u32 	%r232, 1;
	shfl.sync.bfly.b32 	%r233|%p28, %r231, %r232, %r218, %r220;
	mov.b32 	%f52, %r233;
	add.f32 	%f53, %f51, %f52;
	st.local.f32 	[%rd3+4], %f53;

$L__BB74_13:
	bar.sync 	0;
	mov.b32 	%r234, %f2;
	mov.u32 	%r235, 31;
	mov.u32 	%r236, 16;
	mov.u32 	%r237, -1;
	shfl.sync.bfly.b32 	%r238|%p30, %r234, %r236, %r235, %r237;
	mov.b32 	%f54, %r238;
	add.f32 	%f55, %f2, %f54;
	mov.b32 	%r239, %f55;
	mov.u32 	%r240, 8;
	shfl.sync.bfly.b32 	%r241|%p31, %r239, %r240, %r235, %r237;
	mov.b32 	%f56, %r241;
	add.f32 	%f57, %f55, %f56;
	mov.b32 	%r242, %f57;
	mov.u32 	%r243, 4;
	shfl.sync.bfly.b32 	%r244|%p32, %r242, %r243, %r235, %r237;
	mov.b32 	%f58, %r244;
	add.f32 	%f59, %f57, %f58;
	mov.b32 	%r245, %f59;
	mov.u32 	%r246, 2;
	shfl.sync.bfly.b32 	%r247|%p33, %r245, %r246, %r235, %r237;
	mov.b32 	%f60, %r247;
	add.f32 	%f61, %f59, %f60;
	mov.b32 	%r248, %f61;
	mov.u32 	%r249, 1;
	shfl.sync.bfly.b32 	%r250|%p34, %r248, %r249, %r235, %r237;
	mov.b32 	%f62, %r250;
	add.f32 	%f63, %f61, %f62;
	st.local.f32 	[%rd3+8], %f63;
	st.shared.f32 	[%r33], %f63;
	bar.sync 	0;
	@%p1 bra 	$L__BB74_15;

	ld.shared.f32 	%f64, [%r4];
	mov.b32 	%r251, %f64;
	shfl.sync.bfly.b32 	%r255|%p35, %r251, %r236, %r235, %r237;
	mov.b32 	%f65, %r255;
	add.f32 	%f66, %f64, %f65;
	mov.b32 	%r256, %f66;
	shfl.sync.bfly.b32 	%r258|%p36, %r256, %r240, %r235, %r237;
	mov.b32 	%f67, %r258;
	add.f32 	%f68, %f66, %f67;
	mov.b32 	%r259, %f68;
	shfl.sync.bfly.b32 	%r261|%p37, %r259, %r243, %r235, %r237;
	mov.b32 	%f69, %r261;
	add.f32 	%f70, %f68, %f69;
	mov.b32 	%r262, %f70;
	shfl.sync.bfly.b32 	%r264|%p38, %r262, %r246, %r235, %r237;
	mov.b32 	%f71, %r264;
	add.f32 	%f72, %f70, %f71;
	mov.b32 	%r265, %f72;
	shfl.sync.bfly.b32 	%r267|%p39, %r265, %r249, %r235, %r237;
	mov.b32 	%f73, %r267;
	add.f32 	%f74, %f72, %f73;
	st.local.f32 	[%rd3+8], %f74;

$L__BB74_15:
	bar.sync 	0;
	setp.gt.s32 	%p40, %r3, 2;
	@%p40 bra 	$L__BB74_17;

	mad.lo.s32 	%r268, %r3, %r36, %r2;
	cvt.s64.s32 	%rd58, %r268;
	mul.lo.s32 	%r269, %r1, %r37;
	cvt.s64.s32 	%rd59, %r269;
	add.s64 	%rd60, %rd59, %rd58;
	mul.wide.s32 	%rd61, %r3, 4;
	add.s64 	%rd62, %rd3, %rd61;
	ld.local.f32 	%f75, [%rd62];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs3, %f75;}

	// end inline asm
	cvta.to.global.u64 	%rd63, %rd28;
	shl.b64 	%rd64, %rd60, 1;
	add.s64 	%rd65, %rd63, %rd64;
	st.global.u16 	[%rd65], %rs3;

$L__BB74_17:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_4_bs_64
.visible .entry ggml_matvec_f16_ncols_4_bs_64(
	.param .u64 ggml_matvec_f16_ncols_4_bs_64_param_0,
	.param .u64 ggml_matvec_f16_ncols_4_bs_64_param_1,
	.param .u64 ggml_matvec_f16_ncols_4_bs_64_param_2,
	.param .u32 ggml_matvec_f16_ncols_4_bs_64_param_3,
	.param .u32 ggml_matvec_f16_ncols_4_bs_64_param_4,
	.param .u32 ggml_matvec_f16_ncols_4_bs_64_param_5,
	.param .u32 ggml_matvec_f16_ncols_4_bs_64_param_6,
	.param .u32 ggml_matvec_f16_ncols_4_bs_64_param_7,
	.param .u32 ggml_matvec_f16_ncols_4_bs_64_param_8,
	.param .u32 ggml_matvec_f16_ncols_4_bs_64_param_9,
	.param .u32 ggml_matvec_f16_ncols_4_bs_64_param_10,
	.param .u32 ggml_matvec_f16_ncols_4_bs_64_param_11
)
{
	.local .align 16 .b8 	__local_depot75[16];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<52>;
	.reg .b16 	%rs<4>;
	.reg .f32 	%f<100>;
	.reg .b32 	%r<367>;
	.reg .b64 	%rd<83>;


	mov.u64 	%SPL, __local_depot75;
	ld.param.u64 	%rd34, [ggml_matvec_f16_ncols_4_bs_64_param_0];
	ld.param.u64 	%rd35, [ggml_matvec_f16_ncols_4_bs_64_param_1];
	ld.param.u64 	%rd33, [ggml_matvec_f16_ncols_4_bs_64_param_2];
	ld.param.u32 	%r40, [ggml_matvec_f16_ncols_4_bs_64_param_3];
	ld.param.u32 	%r44, [ggml_matvec_f16_ncols_4_bs_64_param_5];
	ld.param.u32 	%r41, [ggml_matvec_f16_ncols_4_bs_64_param_6];
	ld.param.u32 	%r42, [ggml_matvec_f16_ncols_4_bs_64_param_7];
	ld.param.u32 	%r45, [ggml_matvec_f16_ncols_4_bs_64_param_8];
	ld.param.u32 	%r46, [ggml_matvec_f16_ncols_4_bs_64_param_9];
	ld.param.u32 	%r47, [ggml_matvec_f16_ncols_4_bs_64_param_10];
	ld.param.u32 	%r43, [ggml_matvec_f16_ncols_4_bs_64_param_11];
	cvta.to.global.u64 	%rd82, %rd35;
	cvta.to.global.u64 	%rd2, %rd34;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r48, %r1, %r45;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r49, %r2, %r44;
	mad.lo.s32 	%r50, %r48, %r46, %r49;
	cvt.s64.s32 	%rd4, %r50;
	mul.lo.s32 	%r51, %r1, %r47;
	cvt.s64.s32 	%rd5, %r51;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r52, %r3, 2;
	mov.u32 	%r53, data_mmv;
	add.s32 	%r4, %r53, %r52;
	@%p1 bra 	$L__BB75_2;

	mov.u32 	%r54, 0;
	st.shared.u32 	[%r4], %r54;

$L__BB75_2:
	bar.sync 	0;
	mov.f32 	%f5, 0f00000000;
	st.local.v4.f32 	[%rd3], {%f5, %f5, %f5, %f5};
	mov.u32 	%r363, 0;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs2, %f5;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f5;}

	// end inline asm
	mov.b32 	%r366, {%rs1, %rs2};
	setp.ge.s32 	%p2, %r3, %r40;
	mov.u32 	%r364, %r363;
	mov.u32 	%r365, %r363;
	@%p2 bra 	$L__BB75_9;

	not.b32 	%r61, %r3;
	add.s32 	%r6, %r61, %r40;
	shr.u32 	%r62, %r6, 6;
	add.s32 	%r63, %r62, 1;
	and.b32  	%r352, %r63, 3;
	setp.eq.s32 	%p3, %r352, 0;
	mov.u32 	%r363, 0;
	mov.u32 	%r357, %r3;
	@%p3 bra 	$L__BB75_6;

	shl.b32 	%r67, %r41, 1;
	add.s32 	%r68, %r3, %r67;
	mul.wide.s32 	%rd37, %r68, 2;
	add.s64 	%rd38, %rd37, %rd5;
	shl.b64 	%rd39, %rd38, 1;
	add.s64 	%rd80, %rd82, %rd39;
	mad.lo.s32 	%r69, %r41, 3, %r3;
	mul.wide.s32 	%rd40, %r69, 2;
	add.s64 	%rd41, %rd40, %rd5;
	shl.b64 	%rd42, %rd41, 1;
	add.s64 	%rd79, %rd82, %rd42;
	mul.wide.s32 	%rd43, %r41, 2;
	mul.wide.s32 	%rd44, %r3, 2;
	add.s64 	%rd45, %rd43, %rd44;
	add.s64 	%rd46, %rd45, %rd5;
	shl.b64 	%rd47, %rd46, 1;
	add.s64 	%rd78, %rd82, %rd47;
	add.s64 	%rd48, %rd44, %rd5;
	shl.b64 	%rd49, %rd48, 1;
	add.s64 	%rd77, %rd82, %rd49;
	add.s64 	%rd50, %rd44, %rd4;
	shl.b64 	%rd51, %rd50, 1;
	add.s64 	%rd76, %rd2, %rd51;
	mov.u32 	%r363, 0;
	mov.u32 	%r364, %r363;
	mov.u32 	%r365, %r363;
	mov.u32 	%r357, %r3;

$L__BB75_5:
	.pragma "nounroll";
	ld.global.nc.u32 	%r71, [%rd76];
	ld.global.nc.u32 	%r72, [%rd77];
	// begin inline asm
	{mul.f16x2 %r70,%r71,%r72;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r366,%r366,%r70;
}
	// end inline asm
	ld.global.nc.u32 	%r78, [%rd78];
	// begin inline asm
	{mul.f16x2 %r76,%r71,%r78;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r365,%r365,%r76;
}
	// end inline asm
	ld.global.nc.u32 	%r84, [%rd80];
	// begin inline asm
	{mul.f16x2 %r82,%r71,%r84;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r364,%r364,%r82;
}
	// end inline asm
	ld.global.nc.u32 	%r90, [%rd79];
	// begin inline asm
	{mul.f16x2 %r88,%r71,%r90;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r363,%r363,%r88;
}
	// end inline asm
	add.s32 	%r357, %r357, 64;
	add.s64 	%rd80, %rd80, 256;
	add.s64 	%rd79, %rd79, 256;
	add.s64 	%rd78, %rd78, 256;
	add.s64 	%rd77, %rd77, 256;
	add.s64 	%rd76, %rd76, 256;
	add.s32 	%r352, %r352, -1;
	setp.ne.s32 	%p4, %r352, 0;
	@%p4 bra 	$L__BB75_5;

$L__BB75_6:
	setp.lt.u32 	%p5, %r6, 192;
	@%p5 bra 	$L__BB75_9;

	add.s32 	%r94, %r357, %r41;
	shl.b32 	%r95, %r41, 1;
	add.s32 	%r96, %r357, %r95;
	mad.lo.s32 	%r97, %r41, 3, %r357;
	add.s32 	%r98, %r94, 64;
	mul.wide.s32 	%rd52, %r98, 4;
	shl.b64 	%rd53, %rd5, 1;
	add.s64 	%rd23, %rd52, %rd53;
	mul.wide.s32 	%rd54, %r96, 4;
	add.s64 	%rd24, %rd54, %rd53;
	mul.wide.s32 	%rd55, %r97, 4;
	add.s64 	%rd25, %rd55, %rd53;
	mul.wide.s32 	%rd56, %r357, 2;
	add.s64 	%rd57, %rd56, %rd4;
	shl.b64 	%rd58, %rd57, 1;
	add.s64 	%rd59, %rd2, %rd58;
	add.s64 	%rd81, %rd59, 512;
	mul.wide.s32 	%rd60, %r357, 4;
	add.s64 	%rd27, %rd60, %rd53;
	mul.wide.s32 	%rd61, %r41, 4;
	add.s64 	%rd62, %rd60, %rd61;
	add.s64 	%rd28, %rd62, %rd53;

$L__BB75_8:
	ld.global.nc.u32 	%r100, [%rd81+-512];
	add.s64 	%rd63, %rd82, %rd27;
	ld.global.nc.u32 	%r101, [%rd63];
	// begin inline asm
	{mul.f16x2 %r99,%r100,%r101;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r102,%r366,%r99;
}
	// end inline asm
	add.s64 	%rd64, %rd82, %rd28;
	ld.global.nc.u32 	%r107, [%rd64];
	// begin inline asm
	{mul.f16x2 %r105,%r100,%r107;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r108,%r365,%r105;
}
	// end inline asm
	add.s64 	%rd65, %rd82, %rd24;
	ld.global.nc.u32 	%r113, [%rd65];
	// begin inline asm
	{mul.f16x2 %r111,%r100,%r113;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r114,%r364,%r111;
}
	// end inline asm
	add.s64 	%rd66, %rd82, %rd25;
	ld.global.nc.u32 	%r119, [%rd66];
	// begin inline asm
	{mul.f16x2 %r117,%r100,%r119;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r120,%r363,%r117;
}
	// end inline asm
	ld.global.nc.u32 	%r124, [%rd81+-256];
	ld.global.nc.u32 	%r125, [%rd63+256];
	// begin inline asm
	{mul.f16x2 %r123,%r124,%r125;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r126,%r102,%r123;
}
	// end inline asm
	add.s64 	%rd67, %rd82, %rd23;
	ld.global.nc.u32 	%r131, [%rd67];
	// begin inline asm
	{mul.f16x2 %r129,%r124,%r131;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r132,%r108,%r129;
}
	// end inline asm
	ld.global.nc.u32 	%r137, [%rd65+256];
	// begin inline asm
	{mul.f16x2 %r135,%r124,%r137;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r138,%r114,%r135;
}
	// end inline asm
	ld.global.nc.u32 	%r143, [%rd66+256];
	// begin inline asm
	{mul.f16x2 %r141,%r124,%r143;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r144,%r120,%r141;
}
	// end inline asm
	ld.global.nc.u32 	%r148, [%rd81];
	ld.global.nc.u32 	%r149, [%rd63+512];
	// begin inline asm
	{mul.f16x2 %r147,%r148,%r149;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r150,%r126,%r147;
}
	// end inline asm
	ld.global.nc.u32 	%r155, [%rd67+256];
	// begin inline asm
	{mul.f16x2 %r153,%r148,%r155;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r156,%r132,%r153;
}
	// end inline asm
	ld.global.nc.u32 	%r161, [%rd65+512];
	// begin inline asm
	{mul.f16x2 %r159,%r148,%r161;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r162,%r138,%r159;
}
	// end inline asm
	ld.global.nc.u32 	%r167, [%rd66+512];
	// begin inline asm
	{mul.f16x2 %r165,%r148,%r167;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r168,%r144,%r165;
}
	// end inline asm
	ld.global.nc.u32 	%r172, [%rd81+256];
	ld.global.nc.u32 	%r173, [%rd63+768];
	// begin inline asm
	{mul.f16x2 %r171,%r172,%r173;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r366,%r150,%r171;
}
	// end inline asm
	ld.global.nc.u32 	%r179, [%rd67+512];
	// begin inline asm
	{mul.f16x2 %r177,%r172,%r179;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r365,%r156,%r177;
}
	// end inline asm
	ld.global.nc.u32 	%r185, [%rd65+768];
	// begin inline asm
	{mul.f16x2 %r183,%r172,%r185;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r364,%r162,%r183;
}
	// end inline asm
	ld.global.nc.u32 	%r191, [%rd66+768];
	// begin inline asm
	{mul.f16x2 %r189,%r172,%r191;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r363,%r168,%r189;
}
	// end inline asm
	add.s64 	%rd82, %rd82, 1024;
	add.s64 	%rd81, %rd81, 1024;
	add.s32 	%r357, %r357, 256;
	setp.lt.s32 	%p6, %r357, %r40;
	@%p6 bra 	$L__BB75_8;

$L__BB75_9:
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r366;
  cvt.f32.f16 %f6, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r366;
  cvt.f32.f16 %f7, high;}

	// end inline asm
	add.f32 	%f14, %f6, %f7;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r365;
  cvt.f32.f16 %f8, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r365;
  cvt.f32.f16 %f9, high;}

	// end inline asm
	add.f32 	%f1, %f8, %f9;
	st.local.f32 	[%rd3+4], %f1;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r364;
  cvt.f32.f16 %f10, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r364;
  cvt.f32.f16 %f11, high;}

	// end inline asm
	add.f32 	%f2, %f10, %f11;
	st.local.f32 	[%rd3+8], %f2;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r363;
  cvt.f32.f16 %f12, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r363;
  cvt.f32.f16 %f13, high;}

	// end inline asm
	add.f32 	%f3, %f12, %f13;
	st.local.f32 	[%rd3+12], %f3;
	shr.s32 	%r203, %r3, 31;
	shr.u32 	%r204, %r203, 27;
	add.s32 	%r205, %r3, %r204;
	shr.s32 	%r206, %r205, 5;
	shl.b32 	%r207, %r206, 2;
	add.s32 	%r39, %r53, %r207;
	mov.u32 	%r209, 2;
	mov.b32 	%r210, %f14;
	mov.u32 	%r211, 31;
	mov.u32 	%r212, 16;
	mov.u32 	%r213, -1;
	shfl.sync.bfly.b32 	%r214|%p7, %r210, %r212, %r211, %r213;
	mov.b32 	%f15, %r214;
	add.f32 	%f16, %f14, %f15;
	mov.b32 	%r215, %f16;
	mov.u32 	%r216, 8;
	shfl.sync.bfly.b32 	%r217|%p8, %r215, %r216, %r211, %r213;
	mov.b32 	%f17, %r217;
	add.f32 	%f18, %f16, %f17;
	mov.b32 	%r218, %f18;
	mov.u32 	%r219, 4;
	shfl.sync.bfly.b32 	%r220|%p9, %r218, %r219, %r211, %r213;
	mov.b32 	%f19, %r220;
	add.f32 	%f20, %f18, %f19;
	mov.b32 	%r221, %f20;
	shfl.sync.bfly.b32 	%r222|%p10, %r221, %r209, %r211, %r213;
	mov.b32 	%f21, %r222;
	add.f32 	%f22, %f20, %f21;
	mov.b32 	%r223, %f22;
	mov.u32 	%r224, 1;
	shfl.sync.bfly.b32 	%r225|%p11, %r223, %r224, %r211, %r213;
	mov.b32 	%f23, %r225;
	add.f32 	%f24, %f22, %f23;
	st.local.f32 	[%rd3], %f24;
	st.shared.f32 	[%r39], %f24;
	bar.sync 	0;
	@%p1 bra 	$L__BB75_11;

	ld.shared.f32 	%f25, [%r4];
	mov.b32 	%r226, %f25;
	shfl.sync.bfly.b32 	%r230|%p13, %r226, %r212, %r211, %r213;
	mov.b32 	%f26, %r230;
	add.f32 	%f27, %f25, %f26;
	mov.b32 	%r231, %f27;
	shfl.sync.bfly.b32 	%r233|%p14, %r231, %r216, %r211, %r213;
	mov.b32 	%f28, %r233;
	add.f32 	%f29, %f27, %f28;
	mov.b32 	%r234, %f29;
	shfl.sync.bfly.b32 	%r236|%p15, %r234, %r219, %r211, %r213;
	mov.b32 	%f30, %r236;
	add.f32 	%f31, %f29, %f30;
	mov.b32 	%r237, %f31;
	shfl.sync.bfly.b32 	%r239|%p16, %r237, %r209, %r211, %r213;
	mov.b32 	%f32, %r239;
	add.f32 	%f33, %f31, %f32;
	mov.b32 	%r240, %f33;
	shfl.sync.bfly.b32 	%r242|%p17, %r240, %r224, %r211, %r213;
	mov.b32 	%f34, %r242;
	add.f32 	%f35, %f33, %f34;
	st.local.f32 	[%rd3], %f35;

$L__BB75_11:
	bar.sync 	0;
	mov.b32 	%r243, %f1;
	shfl.sync.bfly.b32 	%r247|%p19, %r243, %r212, %r211, %r213;
	mov.b32 	%f36, %r247;
	add.f32 	%f37, %f1, %f36;
	mov.b32 	%r248, %f37;
	shfl.sync.bfly.b32 	%r250|%p20, %r248, %r216, %r211, %r213;
	mov.b32 	%f38, %r250;
	add.f32 	%f39, %f37, %f38;
	mov.b32 	%r251, %f39;
	shfl.sync.bfly.b32 	%r253|%p21, %r251, %r219, %r211, %r213;
	mov.b32 	%f40, %r253;
	add.f32 	%f41, %f39, %f40;
	mov.b32 	%r254, %f41;
	shfl.sync.bfly.b32 	%r256|%p22, %r254, %r209, %r211, %r213;
	mov.b32 	%f42, %r256;
	add.f32 	%f43, %f41, %f42;
	mov.b32 	%r257, %f43;
	shfl.sync.bfly.b32 	%r259|%p23, %r257, %r224, %r211, %r213;
	mov.b32 	%f44, %r259;
	add.f32 	%f45, %f43, %f44;
	st.local.f32 	[%rd3+4], %f45;
	st.shared.f32 	[%r39], %f45;
	bar.sync 	0;
	@%p1 bra 	$L__BB75_13;

	ld.shared.f32 	%f46, [%r4];
	mov.b32 	%r260, %f46;
	mov.u32 	%r261, 31;
	mov.u32 	%r262, 16;
	mov.u32 	%r263, -1;
	shfl.sync.bfly.b32 	%r264|%p24, %r260, %r262, %r261, %r263;
	mov.b32 	%f47, %r264;
	add.f32 	%f48, %f46, %f47;
	mov.b32 	%r265, %f48;
	mov.u32 	%r266, 8;
	shfl.sync.bfly.b32 	%r267|%p25, %r265, %r266, %r261, %r263;
	mov.b32 	%f49, %r267;
	add.f32 	%f50, %f48, %f49;
	mov.b32 	%r268, %f50;
	mov.u32 	%r269, 4;
	shfl.sync.bfly.b32 	%r270|%p26, %r268, %r269, %r261, %r263;
	mov.b32 	%f51, %r270;
	add.f32 	%f52, %f50, %f51;
	mov.b32 	%r271, %f52;
	mov.u32 	%r272, 2;
	shfl.sync.bfly.b32 	%r273|%p27, %r271, %r272, %r261, %r263;
	mov.b32 	%f53, %r273;
	add.f32 	%f54, %f52, %f53;
	mov.b32 	%r274, %f54;
	mov.u32 	%r275, 1;
	shfl.sync.bfly.b32 	%r276|%p28, %r274, %r275, %r261, %r263;
	mov.b32 	%f55, %r276;
	add.f32 	%f56, %f54, %f55;
	st.local.f32 	[%rd3+4], %f56;

$L__BB75_13:
	bar.sync 	0;
	mov.b32 	%r277, %f2;
	mov.u32 	%r278, 31;
	mov.u32 	%r279, 16;
	mov.u32 	%r280, -1;
	shfl.sync.bfly.b32 	%r281|%p30, %r277, %r279, %r278, %r280;
	mov.b32 	%f57, %r281;
	add.f32 	%f58, %f2, %f57;
	mov.b32 	%r282, %f58;
	mov.u32 	%r283, 8;
	shfl.sync.bfly.b32 	%r284|%p31, %r282, %r283, %r278, %r280;
	mov.b32 	%f59, %r284;
	add.f32 	%f60, %f58, %f59;
	mov.b32 	%r285, %f60;
	mov.u32 	%r286, 4;
	shfl.sync.bfly.b32 	%r287|%p32, %r285, %r286, %r278, %r280;
	mov.b32 	%f61, %r287;
	add.f32 	%f62, %f60, %f61;
	mov.b32 	%r288, %f62;
	mov.u32 	%r289, 2;
	shfl.sync.bfly.b32 	%r290|%p33, %r288, %r289, %r278, %r280;
	mov.b32 	%f63, %r290;
	add.f32 	%f64, %f62, %f63;
	mov.b32 	%r291, %f64;
	mov.u32 	%r292, 1;
	shfl.sync.bfly.b32 	%r293|%p34, %r291, %r292, %r278, %r280;
	mov.b32 	%f65, %r293;
	add.f32 	%f66, %f64, %f65;
	st.local.f32 	[%rd3+8], %f66;
	st.shared.f32 	[%r39], %f66;
	bar.sync 	0;
	@%p1 bra 	$L__BB75_15;

	ld.shared.f32 	%f67, [%r4];
	mov.b32 	%r294, %f67;
	shfl.sync.bfly.b32 	%r298|%p35, %r294, %r279, %r278, %r280;
	mov.b32 	%f68, %r298;
	add.f32 	%f69, %f67, %f68;
	mov.b32 	%r299, %f69;
	shfl.sync.bfly.b32 	%r301|%p36, %r299, %r283, %r278, %r280;
	mov.b32 	%f70, %r301;
	add.f32 	%f71, %f69, %f70;
	mov.b32 	%r302, %f71;
	shfl.sync.bfly.b32 	%r304|%p37, %r302, %r286, %r278, %r280;
	mov.b32 	%f72, %r304;
	add.f32 	%f73, %f71, %f72;
	mov.b32 	%r305, %f73;
	shfl.sync.bfly.b32 	%r307|%p38, %r305, %r289, %r278, %r280;
	mov.b32 	%f74, %r307;
	add.f32 	%f75, %f73, %f74;
	mov.b32 	%r308, %f75;
	shfl.sync.bfly.b32 	%r310|%p39, %r308, %r292, %r278, %r280;
	mov.b32 	%f76, %r310;
	add.f32 	%f77, %f75, %f76;
	st.local.f32 	[%rd3+8], %f77;

$L__BB75_15:
	bar.sync 	0;
	mov.b32 	%r311, %f3;
	shfl.sync.bfly.b32 	%r315|%p41, %r311, %r279, %r278, %r280;
	mov.b32 	%f78, %r315;
	add.f32 	%f79, %f3, %f78;
	mov.b32 	%r316, %f79;
	shfl.sync.bfly.b32 	%r318|%p42, %r316, %r283, %r278, %r280;
	mov.b32 	%f80, %r318;
	add.f32 	%f81, %f79, %f80;
	mov.b32 	%r319, %f81;
	shfl.sync.bfly.b32 	%r321|%p43, %r319, %r286, %r278, %r280;
	mov.b32 	%f82, %r321;
	add.f32 	%f83, %f81, %f82;
	mov.b32 	%r322, %f83;
	shfl.sync.bfly.b32 	%r324|%p44, %r322, %r289, %r278, %r280;
	mov.b32 	%f84, %r324;
	add.f32 	%f85, %f83, %f84;
	mov.b32 	%r325, %f85;
	shfl.sync.bfly.b32 	%r327|%p45, %r325, %r292, %r278, %r280;
	mov.b32 	%f86, %r327;
	add.f32 	%f87, %f85, %f86;
	st.local.f32 	[%rd3+12], %f87;
	st.shared.f32 	[%r39], %f87;
	bar.sync 	0;
	@%p1 bra 	$L__BB75_17;

	ld.shared.f32 	%f88, [%r4];
	mov.b32 	%r328, %f88;
	mov.u32 	%r329, 31;
	mov.u32 	%r330, 16;
	mov.u32 	%r331, -1;
	shfl.sync.bfly.b32 	%r332|%p46, %r328, %r330, %r329, %r331;
	mov.b32 	%f89, %r332;
	add.f32 	%f90, %f88, %f89;
	mov.b32 	%r333, %f90;
	mov.u32 	%r334, 8;
	shfl.sync.bfly.b32 	%r335|%p47, %r333, %r334, %r329, %r331;
	mov.b32 	%f91, %r335;
	add.f32 	%f92, %f90, %f91;
	mov.b32 	%r336, %f92;
	mov.u32 	%r337, 4;
	shfl.sync.bfly.b32 	%r338|%p48, %r336, %r337, %r329, %r331;
	mov.b32 	%f93, %r338;
	add.f32 	%f94, %f92, %f93;
	mov.b32 	%r339, %f94;
	mov.u32 	%r340, 2;
	shfl.sync.bfly.b32 	%r341|%p49, %r339, %r340, %r329, %r331;
	mov.b32 	%f95, %r341;
	add.f32 	%f96, %f94, %f95;
	mov.b32 	%r342, %f96;
	mov.u32 	%r343, 1;
	shfl.sync.bfly.b32 	%r344|%p50, %r342, %r343, %r329, %r331;
	mov.b32 	%f97, %r344;
	add.f32 	%f98, %f96, %f97;
	st.local.f32 	[%rd3+12], %f98;

$L__BB75_17:
	bar.sync 	0;
	setp.gt.s32 	%p51, %r3, 3;
	@%p51 bra 	$L__BB75_19;

	mad.lo.s32 	%r345, %r3, %r42, %r2;
	cvt.s64.s32 	%rd68, %r345;
	mul.lo.s32 	%r346, %r1, %r43;
	cvt.s64.s32 	%rd69, %r346;
	add.s64 	%rd70, %rd69, %rd68;
	mul.wide.s32 	%rd71, %r3, 4;
	add.s64 	%rd72, %rd3, %rd71;
	ld.local.f32 	%f99, [%rd72];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs3, %f99;}

	// end inline asm
	cvta.to.global.u64 	%rd73, %rd33;
	shl.b64 	%rd74, %rd70, 1;
	add.s64 	%rd75, %rd73, %rd74;
	st.global.u16 	[%rd75], %rs3;

$L__BB75_19:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_5_bs_64
.visible .entry ggml_matvec_f16_ncols_5_bs_64(
	.param .u64 ggml_matvec_f16_ncols_5_bs_64_param_0,
	.param .u64 ggml_matvec_f16_ncols_5_bs_64_param_1,
	.param .u64 ggml_matvec_f16_ncols_5_bs_64_param_2,
	.param .u32 ggml_matvec_f16_ncols_5_bs_64_param_3,
	.param .u32 ggml_matvec_f16_ncols_5_bs_64_param_4,
	.param .u32 ggml_matvec_f16_ncols_5_bs_64_param_5,
	.param .u32 ggml_matvec_f16_ncols_5_bs_64_param_6,
	.param .u32 ggml_matvec_f16_ncols_5_bs_64_param_7,
	.param .u32 ggml_matvec_f16_ncols_5_bs_64_param_8,
	.param .u32 ggml_matvec_f16_ncols_5_bs_64_param_9,
	.param .u32 ggml_matvec_f16_ncols_5_bs_64_param_10,
	.param .u32 ggml_matvec_f16_ncols_5_bs_64_param_11
)
{
	.local .align 4 .b8 	__local_depot76[20];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<63>;
	.reg .b16 	%rs<4>;
	.reg .f32 	%f<124>;
	.reg .b32 	%r<447>;
	.reg .b64 	%rd<74>;


	mov.u64 	%SPL, __local_depot76;
	ld.param.u64 	%rd28, [ggml_matvec_f16_ncols_5_bs_64_param_0];
	ld.param.u64 	%rd29, [ggml_matvec_f16_ncols_5_bs_64_param_1];
	ld.param.u64 	%rd27, [ggml_matvec_f16_ncols_5_bs_64_param_2];
	ld.param.u32 	%r46, [ggml_matvec_f16_ncols_5_bs_64_param_3];
	ld.param.u32 	%r50, [ggml_matvec_f16_ncols_5_bs_64_param_5];
	ld.param.u32 	%r47, [ggml_matvec_f16_ncols_5_bs_64_param_6];
	ld.param.u32 	%r48, [ggml_matvec_f16_ncols_5_bs_64_param_7];
	ld.param.u32 	%r51, [ggml_matvec_f16_ncols_5_bs_64_param_8];
	ld.param.u32 	%r52, [ggml_matvec_f16_ncols_5_bs_64_param_9];
	ld.param.u32 	%r53, [ggml_matvec_f16_ncols_5_bs_64_param_10];
	ld.param.u32 	%r49, [ggml_matvec_f16_ncols_5_bs_64_param_11];
	cvta.to.global.u64 	%rd73, %rd29;
	cvta.to.global.u64 	%rd2, %rd28;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r54, %r1, %r51;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r55, %r2, %r50;
	mad.lo.s32 	%r56, %r54, %r52, %r55;
	cvt.s64.s32 	%rd4, %r56;
	mul.lo.s32 	%r57, %r1, %r53;
	cvt.s64.s32 	%rd5, %r57;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r58, %r3, 2;
	mov.u32 	%r59, data_mmv;
	add.s32 	%r4, %r59, %r58;
	@%p1 bra 	$L__BB76_2;

	mov.u32 	%r60, 0;
	st.shared.u32 	[%r4], %r60;

$L__BB76_2:
	bar.sync 	0;
	mov.f32 	%f6, 0f00000000;
	mov.u32 	%r442, 0;
	st.local.u32 	[%rd3], %r442;
	st.local.u32 	[%rd3+4], %r442;
	st.local.u32 	[%rd3+8], %r442;
	st.local.u32 	[%rd3+12], %r442;
	st.local.u32 	[%rd3+16], %r442;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs2, %f6;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f6;}

	// end inline asm
	mov.b32 	%r446, {%rs1, %rs2};
	setp.ge.s32 	%p2, %r3, %r46;
	mov.u32 	%r443, %r442;
	mov.u32 	%r444, %r442;
	mov.u32 	%r445, %r442;
	@%p2 bra 	$L__BB76_9;

	not.b32 	%r69, %r3;
	add.s32 	%r6, %r69, %r46;
	shr.u32 	%r70, %r6, 6;
	add.s32 	%r71, %r70, 1;
	and.b32  	%r429, %r71, 3;
	setp.eq.s32 	%p3, %r429, 0;
	mov.u32 	%r442, 0;
	mov.u32 	%r435, %r3;
	@%p3 bra 	$L__BB76_6;

	shl.b32 	%r76, %r47, 1;
	mad.lo.s32 	%r77, %r47, 3, %r3;
	mul.wide.s32 	%rd31, %r77, 4;
	shl.b64 	%rd32, %rd5, 1;
	add.s64 	%rd7, %rd31, %rd32;
	mul.wide.s32 	%rd33, %r3, 4;
	mul.wide.s32 	%rd34, %r47, 4;
	add.s64 	%rd35, %rd33, %rd34;
	add.s64 	%rd8, %rd35, %rd32;
	add.s64 	%rd9, %rd33, %rd32;
	mul.wide.s32 	%rd36, %r3, 2;
	add.s64 	%rd37, %rd36, %rd4;
	shl.b64 	%rd38, %rd37, 1;
	add.s64 	%rd70, %rd2, %rd38;
	mul.wide.s32 	%rd11, %r76, 4;
	mov.u32 	%r442, 0;
	mov.u64 	%rd71, %rd73;
	mov.u32 	%r443, %r442;
	mov.u32 	%r444, %r442;
	mov.u32 	%r445, %r442;
	mov.u32 	%r435, %r3;

$L__BB76_5:
	.pragma "nounroll";
	ld.global.nc.u32 	%r79, [%rd70];
	add.s64 	%rd39, %rd71, %rd9;
	ld.global.nc.u32 	%r80, [%rd39];
	// begin inline asm
	{mul.f16x2 %r78,%r79,%r80;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r446,%r446,%r78;
}
	// end inline asm
	add.s64 	%rd40, %rd71, %rd8;
	ld.global.nc.u32 	%r86, [%rd40];
	// begin inline asm
	{mul.f16x2 %r84,%r79,%r86;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r445,%r445,%r84;
}
	// end inline asm
	add.s64 	%rd41, %rd39, %rd11;
	ld.global.nc.u32 	%r92, [%rd41];
	// begin inline asm
	{mul.f16x2 %r90,%r79,%r92;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r444,%r444,%r90;
}
	// end inline asm
	add.s64 	%rd42, %rd71, %rd7;
	ld.global.nc.u32 	%r98, [%rd42];
	// begin inline asm
	{mul.f16x2 %r96,%r79,%r98;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r443,%r443,%r96;
}
	// end inline asm
	add.s64 	%rd43, %rd41, %rd11;
	ld.global.nc.u32 	%r104, [%rd43];
	// begin inline asm
	{mul.f16x2 %r102,%r79,%r104;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r442,%r442,%r102;
}
	// end inline asm
	add.s32 	%r435, %r435, 64;
	add.s64 	%rd71, %rd71, 256;
	add.s64 	%rd70, %rd70, 256;
	add.s32 	%r429, %r429, -1;
	setp.ne.s32 	%p4, %r429, 0;
	@%p4 bra 	$L__BB76_5;

$L__BB76_6:
	setp.lt.u32 	%p5, %r6, 192;
	@%p5 bra 	$L__BB76_9;

	add.s32 	%r108, %r435, %r47;
	shl.b32 	%r109, %r47, 1;
	add.s32 	%r110, %r435, %r109;
	mad.lo.s32 	%r111, %r47, 3, %r435;
	shl.b32 	%r112, %r47, 2;
	add.s32 	%r113, %r435, %r112;
	add.s32 	%r114, %r108, 64;
	mul.wide.s32 	%rd44, %r114, 4;
	shl.b64 	%rd45, %rd5, 1;
	add.s64 	%rd16, %rd44, %rd45;
	mul.wide.s32 	%rd46, %r110, 4;
	add.s64 	%rd17, %rd46, %rd45;
	mul.wide.s32 	%rd47, %r111, 4;
	add.s64 	%rd18, %rd47, %rd45;
	mul.wide.s32 	%rd48, %r113, 4;
	add.s64 	%rd19, %rd48, %rd45;
	mul.wide.s32 	%rd49, %r435, 2;
	add.s64 	%rd50, %rd49, %rd4;
	shl.b64 	%rd51, %rd50, 1;
	add.s64 	%rd52, %rd2, %rd51;
	add.s64 	%rd72, %rd52, 512;
	mul.wide.s32 	%rd53, %r435, 4;
	add.s64 	%rd21, %rd53, %rd45;
	mul.wide.s32 	%rd54, %r47, 4;
	add.s64 	%rd55, %rd53, %rd54;
	add.s64 	%rd22, %rd55, %rd45;

$L__BB76_8:
	ld.global.nc.u32 	%r116, [%rd72+-512];
	add.s64 	%rd56, %rd73, %rd21;
	ld.global.nc.u32 	%r117, [%rd56];
	// begin inline asm
	{mul.f16x2 %r115,%r116,%r117;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r118,%r446,%r115;
}
	// end inline asm
	add.s64 	%rd57, %rd73, %rd22;
	ld.global.nc.u32 	%r123, [%rd57];
	// begin inline asm
	{mul.f16x2 %r121,%r116,%r123;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r124,%r445,%r121;
}
	// end inline asm
	add.s64 	%rd58, %rd73, %rd17;
	ld.global.nc.u32 	%r129, [%rd58];
	// begin inline asm
	{mul.f16x2 %r127,%r116,%r129;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r130,%r444,%r127;
}
	// end inline asm
	add.s64 	%rd59, %rd73, %rd18;
	ld.global.nc.u32 	%r135, [%rd59];
	// begin inline asm
	{mul.f16x2 %r133,%r116,%r135;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r136,%r443,%r133;
}
	// end inline asm
	add.s64 	%rd60, %rd73, %rd19;
	ld.global.nc.u32 	%r141, [%rd60];
	// begin inline asm
	{mul.f16x2 %r139,%r116,%r141;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r142,%r442,%r139;
}
	// end inline asm
	ld.global.nc.u32 	%r146, [%rd72+-256];
	ld.global.nc.u32 	%r147, [%rd56+256];
	// begin inline asm
	{mul.f16x2 %r145,%r146,%r147;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r148,%r118,%r145;
}
	// end inline asm
	add.s64 	%rd61, %rd73, %rd16;
	ld.global.nc.u32 	%r153, [%rd61];
	// begin inline asm
	{mul.f16x2 %r151,%r146,%r153;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r154,%r124,%r151;
}
	// end inline asm
	ld.global.nc.u32 	%r159, [%rd58+256];
	// begin inline asm
	{mul.f16x2 %r157,%r146,%r159;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r160,%r130,%r157;
}
	// end inline asm
	ld.global.nc.u32 	%r165, [%rd59+256];
	// begin inline asm
	{mul.f16x2 %r163,%r146,%r165;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r166,%r136,%r163;
}
	// end inline asm
	ld.global.nc.u32 	%r171, [%rd60+256];
	// begin inline asm
	{mul.f16x2 %r169,%r146,%r171;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r172,%r142,%r169;
}
	// end inline asm
	ld.global.nc.u32 	%r176, [%rd72];
	ld.global.nc.u32 	%r177, [%rd56+512];
	// begin inline asm
	{mul.f16x2 %r175,%r176,%r177;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r178,%r148,%r175;
}
	// end inline asm
	ld.global.nc.u32 	%r183, [%rd61+256];
	// begin inline asm
	{mul.f16x2 %r181,%r176,%r183;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r184,%r154,%r181;
}
	// end inline asm
	ld.global.nc.u32 	%r189, [%rd58+512];
	// begin inline asm
	{mul.f16x2 %r187,%r176,%r189;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r190,%r160,%r187;
}
	// end inline asm
	ld.global.nc.u32 	%r195, [%rd59+512];
	// begin inline asm
	{mul.f16x2 %r193,%r176,%r195;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r196,%r166,%r193;
}
	// end inline asm
	ld.global.nc.u32 	%r201, [%rd60+512];
	// begin inline asm
	{mul.f16x2 %r199,%r176,%r201;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r202,%r172,%r199;
}
	// end inline asm
	ld.global.nc.u32 	%r206, [%rd72+256];
	ld.global.nc.u32 	%r207, [%rd56+768];
	// begin inline asm
	{mul.f16x2 %r205,%r206,%r207;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r446,%r178,%r205;
}
	// end inline asm
	ld.global.nc.u32 	%r213, [%rd61+512];
	// begin inline asm
	{mul.f16x2 %r211,%r206,%r213;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r445,%r184,%r211;
}
	// end inline asm
	ld.global.nc.u32 	%r219, [%rd58+768];
	// begin inline asm
	{mul.f16x2 %r217,%r206,%r219;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r444,%r190,%r217;
}
	// end inline asm
	ld.global.nc.u32 	%r225, [%rd59+768];
	// begin inline asm
	{mul.f16x2 %r223,%r206,%r225;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r443,%r196,%r223;
}
	// end inline asm
	ld.global.nc.u32 	%r231, [%rd60+768];
	// begin inline asm
	{mul.f16x2 %r229,%r206,%r231;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r442,%r202,%r229;
}
	// end inline asm
	add.s64 	%rd73, %rd73, 1024;
	add.s64 	%rd72, %rd72, 1024;
	add.s32 	%r435, %r435, 256;
	setp.lt.s32 	%p6, %r435, %r46;
	@%p6 bra 	$L__BB76_8;

$L__BB76_9:
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r446;
  cvt.f32.f16 %f7, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r446;
  cvt.f32.f16 %f8, high;}

	// end inline asm
	add.f32 	%f17, %f7, %f8;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r445;
  cvt.f32.f16 %f9, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r445;
  cvt.f32.f16 %f10, high;}

	// end inline asm
	add.f32 	%f1, %f9, %f10;
	st.local.f32 	[%rd3+4], %f1;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r444;
  cvt.f32.f16 %f11, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r444;
  cvt.f32.f16 %f12, high;}

	// end inline asm
	add.f32 	%f2, %f11, %f12;
	st.local.f32 	[%rd3+8], %f2;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r443;
  cvt.f32.f16 %f13, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r443;
  cvt.f32.f16 %f14, high;}

	// end inline asm
	add.f32 	%f3, %f13, %f14;
	st.local.f32 	[%rd3+12], %f3;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r442;
  cvt.f32.f16 %f15, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r442;
  cvt.f32.f16 %f16, high;}

	// end inline asm
	add.f32 	%f4, %f15, %f16;
	st.local.f32 	[%rd3+16], %f4;
	shr.s32 	%r245, %r3, 31;
	shr.u32 	%r246, %r245, 27;
	add.s32 	%r247, %r3, %r246;
	shr.s32 	%r248, %r247, 5;
	shl.b32 	%r249, %r248, 2;
	add.s32 	%r45, %r59, %r249;
	mov.u32 	%r251, 2;
	mov.b32 	%r252, %f17;
	mov.u32 	%r253, 31;
	mov.u32 	%r254, 16;
	mov.u32 	%r255, -1;
	shfl.sync.bfly.b32 	%r256|%p7, %r252, %r254, %r253, %r255;
	mov.b32 	%f18, %r256;
	add.f32 	%f19, %f17, %f18;
	mov.b32 	%r257, %f19;
	mov.u32 	%r258, 8;
	shfl.sync.bfly.b32 	%r259|%p8, %r257, %r258, %r253, %r255;
	mov.b32 	%f20, %r259;
	add.f32 	%f21, %f19, %f20;
	mov.b32 	%r260, %f21;
	mov.u32 	%r261, 4;
	shfl.sync.bfly.b32 	%r262|%p9, %r260, %r261, %r253, %r255;
	mov.b32 	%f22, %r262;
	add.f32 	%f23, %f21, %f22;
	mov.b32 	%r263, %f23;
	shfl.sync.bfly.b32 	%r264|%p10, %r263, %r251, %r253, %r255;
	mov.b32 	%f24, %r264;
	add.f32 	%f25, %f23, %f24;
	mov.b32 	%r265, %f25;
	mov.u32 	%r266, 1;
	shfl.sync.bfly.b32 	%r267|%p11, %r265, %r266, %r253, %r255;
	mov.b32 	%f26, %r267;
	add.f32 	%f27, %f25, %f26;
	st.local.f32 	[%rd3], %f27;
	st.shared.f32 	[%r45], %f27;
	bar.sync 	0;
	@%p1 bra 	$L__BB76_11;

	ld.shared.f32 	%f28, [%r4];
	mov.b32 	%r268, %f28;
	shfl.sync.bfly.b32 	%r272|%p13, %r268, %r254, %r253, %r255;
	mov.b32 	%f29, %r272;
	add.f32 	%f30, %f28, %f29;
	mov.b32 	%r273, %f30;
	shfl.sync.bfly.b32 	%r275|%p14, %r273, %r258, %r253, %r255;
	mov.b32 	%f31, %r275;
	add.f32 	%f32, %f30, %f31;
	mov.b32 	%r276, %f32;
	shfl.sync.bfly.b32 	%r278|%p15, %r276, %r261, %r253, %r255;
	mov.b32 	%f33, %r278;
	add.f32 	%f34, %f32, %f33;
	mov.b32 	%r279, %f34;
	shfl.sync.bfly.b32 	%r281|%p16, %r279, %r251, %r253, %r255;
	mov.b32 	%f35, %r281;
	add.f32 	%f36, %f34, %f35;
	mov.b32 	%r282, %f36;
	shfl.sync.bfly.b32 	%r284|%p17, %r282, %r266, %r253, %r255;
	mov.b32 	%f37, %r284;
	add.f32 	%f38, %f36, %f37;
	st.local.f32 	[%rd3], %f38;

$L__BB76_11:
	bar.sync 	0;
	mov.b32 	%r285, %f1;
	shfl.sync.bfly.b32 	%r289|%p19, %r285, %r254, %r253, %r255;
	mov.b32 	%f39, %r289;
	add.f32 	%f40, %f1, %f39;
	mov.b32 	%r290, %f40;
	shfl.sync.bfly.b32 	%r292|%p20, %r290, %r258, %r253, %r255;
	mov.b32 	%f41, %r292;
	add.f32 	%f42, %f40, %f41;
	mov.b32 	%r293, %f42;
	shfl.sync.bfly.b32 	%r295|%p21, %r293, %r261, %r253, %r255;
	mov.b32 	%f43, %r295;
	add.f32 	%f44, %f42, %f43;
	mov.b32 	%r296, %f44;
	shfl.sync.bfly.b32 	%r298|%p22, %r296, %r251, %r253, %r255;
	mov.b32 	%f45, %r298;
	add.f32 	%f46, %f44, %f45;
	mov.b32 	%r299, %f46;
	shfl.sync.bfly.b32 	%r301|%p23, %r299, %r266, %r253, %r255;
	mov.b32 	%f47, %r301;
	add.f32 	%f48, %f46, %f47;
	st.local.f32 	[%rd3+4], %f48;
	st.shared.f32 	[%r45], %f48;
	bar.sync 	0;
	@%p1 bra 	$L__BB76_13;

	ld.shared.f32 	%f49, [%r4];
	mov.b32 	%r302, %f49;
	mov.u32 	%r303, 31;
	mov.u32 	%r304, 16;
	mov.u32 	%r305, -1;
	shfl.sync.bfly.b32 	%r306|%p24, %r302, %r304, %r303, %r305;
	mov.b32 	%f50, %r306;
	add.f32 	%f51, %f49, %f50;
	mov.b32 	%r307, %f51;
	mov.u32 	%r308, 8;
	shfl.sync.bfly.b32 	%r309|%p25, %r307, %r308, %r303, %r305;
	mov.b32 	%f52, %r309;
	add.f32 	%f53, %f51, %f52;
	mov.b32 	%r310, %f53;
	mov.u32 	%r311, 4;
	shfl.sync.bfly.b32 	%r312|%p26, %r310, %r311, %r303, %r305;
	mov.b32 	%f54, %r312;
	add.f32 	%f55, %f53, %f54;
	mov.b32 	%r313, %f55;
	mov.u32 	%r314, 2;
	shfl.sync.bfly.b32 	%r315|%p27, %r313, %r314, %r303, %r305;
	mov.b32 	%f56, %r315;
	add.f32 	%f57, %f55, %f56;
	mov.b32 	%r316, %f57;
	mov.u32 	%r317, 1;
	shfl.sync.bfly.b32 	%r318|%p28, %r316, %r317, %r303, %r305;
	mov.b32 	%f58, %r318;
	add.f32 	%f59, %f57, %f58;
	st.local.f32 	[%rd3+4], %f59;

$L__BB76_13:
	bar.sync 	0;
	mov.b32 	%r319, %f2;
	mov.u32 	%r320, 31;
	mov.u32 	%r321, 16;
	mov.u32 	%r322, -1;
	shfl.sync.bfly.b32 	%r323|%p30, %r319, %r321, %r320, %r322;
	mov.b32 	%f60, %r323;
	add.f32 	%f61, %f2, %f60;
	mov.b32 	%r324, %f61;
	mov.u32 	%r325, 8;
	shfl.sync.bfly.b32 	%r326|%p31, %r324, %r325, %r320, %r322;
	mov.b32 	%f62, %r326;
	add.f32 	%f63, %f61, %f62;
	mov.b32 	%r327, %f63;
	mov.u32 	%r328, 4;
	shfl.sync.bfly.b32 	%r329|%p32, %r327, %r328, %r320, %r322;
	mov.b32 	%f64, %r329;
	add.f32 	%f65, %f63, %f64;
	mov.b32 	%r330, %f65;
	mov.u32 	%r331, 2;
	shfl.sync.bfly.b32 	%r332|%p33, %r330, %r331, %r320, %r322;
	mov.b32 	%f66, %r332;
	add.f32 	%f67, %f65, %f66;
	mov.b32 	%r333, %f67;
	mov.u32 	%r334, 1;
	shfl.sync.bfly.b32 	%r335|%p34, %r333, %r334, %r320, %r322;
	mov.b32 	%f68, %r335;
	add.f32 	%f69, %f67, %f68;
	st.local.f32 	[%rd3+8], %f69;
	st.shared.f32 	[%r45], %f69;
	bar.sync 	0;
	@%p1 bra 	$L__BB76_15;

	ld.shared.f32 	%f70, [%r4];
	mov.b32 	%r336, %f70;
	shfl.sync.bfly.b32 	%r340|%p35, %r336, %r321, %r320, %r322;
	mov.b32 	%f71, %r340;
	add.f32 	%f72, %f70, %f71;
	mov.b32 	%r341, %f72;
	shfl.sync.bfly.b32 	%r343|%p36, %r341, %r325, %r320, %r322;
	mov.b32 	%f73, %r343;
	add.f32 	%f74, %f72, %f73;
	mov.b32 	%r344, %f74;
	shfl.sync.bfly.b32 	%r346|%p37, %r344, %r328, %r320, %r322;
	mov.b32 	%f75, %r346;
	add.f32 	%f76, %f74, %f75;
	mov.b32 	%r347, %f76;
	shfl.sync.bfly.b32 	%r349|%p38, %r347, %r331, %r320, %r322;
	mov.b32 	%f77, %r349;
	add.f32 	%f78, %f76, %f77;
	mov.b32 	%r350, %f78;
	shfl.sync.bfly.b32 	%r352|%p39, %r350, %r334, %r320, %r322;
	mov.b32 	%f79, %r352;
	add.f32 	%f80, %f78, %f79;
	st.local.f32 	[%rd3+8], %f80;

$L__BB76_15:
	bar.sync 	0;
	mov.b32 	%r353, %f3;
	shfl.sync.bfly.b32 	%r357|%p41, %r353, %r321, %r320, %r322;
	mov.b32 	%f81, %r357;
	add.f32 	%f82, %f3, %f81;
	mov.b32 	%r358, %f82;
	shfl.sync.bfly.b32 	%r360|%p42, %r358, %r325, %r320, %r322;
	mov.b32 	%f83, %r360;
	add.f32 	%f84, %f82, %f83;
	mov.b32 	%r361, %f84;
	shfl.sync.bfly.b32 	%r363|%p43, %r361, %r328, %r320, %r322;
	mov.b32 	%f85, %r363;
	add.f32 	%f86, %f84, %f85;
	mov.b32 	%r364, %f86;
	shfl.sync.bfly.b32 	%r366|%p44, %r364, %r331, %r320, %r322;
	mov.b32 	%f87, %r366;
	add.f32 	%f88, %f86, %f87;
	mov.b32 	%r367, %f88;
	shfl.sync.bfly.b32 	%r369|%p45, %r367, %r334, %r320, %r322;
	mov.b32 	%f89, %r369;
	add.f32 	%f90, %f88, %f89;
	st.local.f32 	[%rd3+12], %f90;
	st.shared.f32 	[%r45], %f90;
	bar.sync 	0;
	@%p1 bra 	$L__BB76_17;

	ld.shared.f32 	%f91, [%r4];
	mov.b32 	%r370, %f91;
	mov.u32 	%r371, 31;
	mov.u32 	%r372, 16;
	mov.u32 	%r373, -1;
	shfl.sync.bfly.b32 	%r374|%p46, %r370, %r372, %r371, %r373;
	mov.b32 	%f92, %r374;
	add.f32 	%f93, %f91, %f92;
	mov.b32 	%r375, %f93;
	mov.u32 	%r376, 8;
	shfl.sync.bfly.b32 	%r377|%p47, %r375, %r376, %r371, %r373;
	mov.b32 	%f94, %r377;
	add.f32 	%f95, %f93, %f94;
	mov.b32 	%r378, %f95;
	mov.u32 	%r379, 4;
	shfl.sync.bfly.b32 	%r380|%p48, %r378, %r379, %r371, %r373;
	mov.b32 	%f96, %r380;
	add.f32 	%f97, %f95, %f96;
	mov.b32 	%r381, %f97;
	mov.u32 	%r382, 2;
	shfl.sync.bfly.b32 	%r383|%p49, %r381, %r382, %r371, %r373;
	mov.b32 	%f98, %r383;
	add.f32 	%f99, %f97, %f98;
	mov.b32 	%r384, %f99;
	mov.u32 	%r385, 1;
	shfl.sync.bfly.b32 	%r386|%p50, %r384, %r385, %r371, %r373;
	mov.b32 	%f100, %r386;
	add.f32 	%f101, %f99, %f100;
	st.local.f32 	[%rd3+12], %f101;

$L__BB76_17:
	bar.sync 	0;
	mov.b32 	%r387, %f4;
	mov.u32 	%r388, 31;
	mov.u32 	%r389, 16;
	mov.u32 	%r390, -1;
	shfl.sync.bfly.b32 	%r391|%p52, %r387, %r389, %r388, %r390;
	mov.b32 	%f102, %r391;
	add.f32 	%f103, %f4, %f102;
	mov.b32 	%r392, %f103;
	mov.u32 	%r393, 8;
	shfl.sync.bfly.b32 	%r394|%p53, %r392, %r393, %r388, %r390;
	mov.b32 	%f104, %r394;
	add.f32 	%f105, %f103, %f104;
	mov.b32 	%r395, %f105;
	mov.u32 	%r396, 4;
	shfl.sync.bfly.b32 	%r397|%p54, %r395, %r396, %r388, %r390;
	mov.b32 	%f106, %r397;
	add.f32 	%f107, %f105, %f106;
	mov.b32 	%r398, %f107;
	mov.u32 	%r399, 2;
	shfl.sync.bfly.b32 	%r400|%p55, %r398, %r399, %r388, %r390;
	mov.b32 	%f108, %r400;
	add.f32 	%f109, %f107, %f108;
	mov.b32 	%r401, %f109;
	mov.u32 	%r402, 1;
	shfl.sync.bfly.b32 	%r403|%p56, %r401, %r402, %r388, %r390;
	mov.b32 	%f110, %r403;
	add.f32 	%f111, %f109, %f110;
	st.local.f32 	[%rd3+16], %f111;
	st.shared.f32 	[%r45], %f111;
	bar.sync 	0;
	@%p1 bra 	$L__BB76_19;

	ld.shared.f32 	%f112, [%r4];
	mov.b32 	%r404, %f112;
	shfl.sync.bfly.b32 	%r408|%p57, %r404, %r389, %r388, %r390;
	mov.b32 	%f113, %r408;
	add.f32 	%f114, %f112, %f113;
	mov.b32 	%r409, %f114;
	shfl.sync.bfly.b32 	%r411|%p58, %r409, %r393, %r388, %r390;
	mov.b32 	%f115, %r411;
	add.f32 	%f116, %f114, %f115;
	mov.b32 	%r412, %f116;
	shfl.sync.bfly.b32 	%r414|%p59, %r412, %r396, %r388, %r390;
	mov.b32 	%f117, %r414;
	add.f32 	%f118, %f116, %f117;
	mov.b32 	%r415, %f118;
	shfl.sync.bfly.b32 	%r417|%p60, %r415, %r399, %r388, %r390;
	mov.b32 	%f119, %r417;
	add.f32 	%f120, %f118, %f119;
	mov.b32 	%r418, %f120;
	shfl.sync.bfly.b32 	%r420|%p61, %r418, %r402, %r388, %r390;
	mov.b32 	%f121, %r420;
	add.f32 	%f122, %f120, %f121;
	st.local.f32 	[%rd3+16], %f122;

$L__BB76_19:
	bar.sync 	0;
	setp.gt.s32 	%p62, %r3, 4;
	@%p62 bra 	$L__BB76_21;

	mad.lo.s32 	%r421, %r3, %r48, %r2;
	cvt.s64.s32 	%rd62, %r421;
	mul.lo.s32 	%r422, %r1, %r49;
	cvt.s64.s32 	%rd63, %r422;
	add.s64 	%rd64, %rd63, %rd62;
	mul.wide.s32 	%rd65, %r3, 4;
	add.s64 	%rd66, %rd3, %rd65;
	ld.local.f32 	%f123, [%rd66];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs3, %f123;}

	// end inline asm
	cvta.to.global.u64 	%rd67, %rd27;
	shl.b64 	%rd68, %rd64, 1;
	add.s64 	%rd69, %rd67, %rd68;
	st.global.u16 	[%rd69], %rs3;

$L__BB76_21:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_6_bs_64
.visible .entry ggml_matvec_f16_ncols_6_bs_64(
	.param .u64 ggml_matvec_f16_ncols_6_bs_64_param_0,
	.param .u64 ggml_matvec_f16_ncols_6_bs_64_param_1,
	.param .u64 ggml_matvec_f16_ncols_6_bs_64_param_2,
	.param .u32 ggml_matvec_f16_ncols_6_bs_64_param_3,
	.param .u32 ggml_matvec_f16_ncols_6_bs_64_param_4,
	.param .u32 ggml_matvec_f16_ncols_6_bs_64_param_5,
	.param .u32 ggml_matvec_f16_ncols_6_bs_64_param_6,
	.param .u32 ggml_matvec_f16_ncols_6_bs_64_param_7,
	.param .u32 ggml_matvec_f16_ncols_6_bs_64_param_8,
	.param .u32 ggml_matvec_f16_ncols_6_bs_64_param_9,
	.param .u32 ggml_matvec_f16_ncols_6_bs_64_param_10,
	.param .u32 ggml_matvec_f16_ncols_6_bs_64_param_11
)
{
	.local .align 8 .b8 	__local_depot77[24];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<74>;
	.reg .b16 	%rs<4>;
	.reg .f32 	%f<148>;
	.reg .b32 	%r<527>;
	.reg .b64 	%rd<77>;


	mov.u64 	%SPL, __local_depot77;
	ld.param.u64 	%rd29, [ggml_matvec_f16_ncols_6_bs_64_param_0];
	ld.param.u64 	%rd30, [ggml_matvec_f16_ncols_6_bs_64_param_1];
	ld.param.u64 	%rd28, [ggml_matvec_f16_ncols_6_bs_64_param_2];
	ld.param.u32 	%r52, [ggml_matvec_f16_ncols_6_bs_64_param_3];
	ld.param.u32 	%r56, [ggml_matvec_f16_ncols_6_bs_64_param_5];
	ld.param.u32 	%r53, [ggml_matvec_f16_ncols_6_bs_64_param_6];
	ld.param.u32 	%r54, [ggml_matvec_f16_ncols_6_bs_64_param_7];
	ld.param.u32 	%r57, [ggml_matvec_f16_ncols_6_bs_64_param_8];
	ld.param.u32 	%r58, [ggml_matvec_f16_ncols_6_bs_64_param_9];
	ld.param.u32 	%r59, [ggml_matvec_f16_ncols_6_bs_64_param_10];
	ld.param.u32 	%r55, [ggml_matvec_f16_ncols_6_bs_64_param_11];
	cvta.to.global.u64 	%rd76, %rd30;
	cvta.to.global.u64 	%rd2, %rd29;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r60, %r1, %r57;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r61, %r2, %r56;
	mad.lo.s32 	%r62, %r60, %r58, %r61;
	cvt.s64.s32 	%rd4, %r62;
	mul.lo.s32 	%r63, %r1, %r59;
	cvt.s64.s32 	%rd5, %r63;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r64, %r3, 2;
	mov.u32 	%r65, data_mmv;
	add.s32 	%r4, %r65, %r64;
	@%p1 bra 	$L__BB77_2;

	mov.u32 	%r66, 0;
	st.shared.u32 	[%r4], %r66;

$L__BB77_2:
	bar.sync 	0;
	mov.f32 	%f7, 0f00000000;
	st.local.v2.f32 	[%rd3], {%f7, %f7};
	st.local.v2.f32 	[%rd3+8], {%f7, %f7};
	st.local.v2.f32 	[%rd3+16], {%f7, %f7};
	mov.u32 	%r521, 0;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs2, %f7;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f7;}

	// end inline asm
	mov.b32 	%r526, {%rs1, %rs2};
	setp.ge.s32 	%p2, %r3, %r52;
	mov.u32 	%r522, %r521;
	mov.u32 	%r523, %r521;
	mov.u32 	%r524, %r521;
	mov.u32 	%r525, %r521;
	@%p2 bra 	$L__BB77_9;

	not.b32 	%r77, %r3;
	add.s32 	%r6, %r77, %r52;
	shr.u32 	%r78, %r6, 6;
	add.s32 	%r79, %r78, 1;
	and.b32  	%r506, %r79, 3;
	setp.eq.s32 	%p3, %r506, 0;
	mov.u32 	%r521, 0;
	mov.u32 	%r513, %r3;
	@%p3 bra 	$L__BB77_6;

	shl.b32 	%r85, %r53, 1;
	add.s32 	%r86, %r3, %r85;
	mul.wide.s32 	%rd32, %r86, 4;
	shl.b64 	%rd33, %rd5, 1;
	add.s64 	%rd7, %rd32, %rd33;
	mul.wide.s32 	%rd34, %r3, 4;
	mul.wide.s32 	%rd8, %r53, 4;
	add.s64 	%rd35, %rd34, %rd8;
	add.s64 	%rd9, %rd35, %rd33;
	add.s64 	%rd10, %rd34, %rd33;
	mul.wide.s32 	%rd36, %r3, 2;
	add.s64 	%rd37, %rd36, %rd4;
	shl.b64 	%rd38, %rd37, 1;
	add.s64 	%rd73, %rd2, %rd38;
	mov.u32 	%r521, 0;
	mov.u64 	%rd74, %rd76;
	mov.u32 	%r522, %r521;
	mov.u32 	%r523, %r521;
	mov.u32 	%r524, %r521;
	mov.u32 	%r525, %r521;
	mov.u32 	%r513, %r3;

$L__BB77_5:
	.pragma "nounroll";
	ld.global.nc.u32 	%r88, [%rd73];
	add.s64 	%rd39, %rd74, %rd10;
	ld.global.nc.u32 	%r89, [%rd39];
	// begin inline asm
	{mul.f16x2 %r87,%r88,%r89;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r526,%r526,%r87;
}
	// end inline asm
	add.s64 	%rd40, %rd74, %rd9;
	ld.global.nc.u32 	%r95, [%rd40];
	// begin inline asm
	{mul.f16x2 %r93,%r88,%r95;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r525,%r525,%r93;
}
	// end inline asm
	add.s64 	%rd41, %rd74, %rd7;
	ld.global.nc.u32 	%r101, [%rd41];
	// begin inline asm
	{mul.f16x2 %r99,%r88,%r101;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r524,%r524,%r99;
}
	// end inline asm
	add.s64 	%rd42, %rd41, %rd8;
	ld.global.nc.u32 	%r107, [%rd42];
	// begin inline asm
	{mul.f16x2 %r105,%r88,%r107;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r523,%r523,%r105;
}
	// end inline asm
	add.s64 	%rd43, %rd42, %rd8;
	ld.global.nc.u32 	%r113, [%rd43];
	// begin inline asm
	{mul.f16x2 %r111,%r88,%r113;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r522,%r522,%r111;
}
	// end inline asm
	add.s64 	%rd44, %rd43, %rd8;
	ld.global.nc.u32 	%r119, [%rd44];
	// begin inline asm
	{mul.f16x2 %r117,%r88,%r119;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r521,%r521,%r117;
}
	// end inline asm
	add.s32 	%r513, %r513, 64;
	add.s64 	%rd74, %rd74, 256;
	add.s64 	%rd73, %rd73, 256;
	add.s32 	%r506, %r506, -1;
	setp.ne.s32 	%p4, %r506, 0;
	@%p4 bra 	$L__BB77_5;

$L__BB77_6:
	setp.lt.u32 	%p5, %r6, 192;
	@%p5 bra 	$L__BB77_9;

	add.s32 	%r123, %r513, %r53;
	shl.b32 	%r124, %r53, 1;
	add.s32 	%r125, %r513, %r124;
	mad.lo.s32 	%r126, %r53, 3, %r513;
	shl.b32 	%r127, %r53, 2;
	add.s32 	%r128, %r513, %r127;
	mad.lo.s32 	%r129, %r53, 5, %r513;
	add.s32 	%r130, %r123, 64;
	mul.wide.s32 	%rd45, %r130, 4;
	shl.b64 	%rd46, %rd5, 1;
	add.s64 	%rd16, %rd45, %rd46;
	mul.wide.s32 	%rd47, %r125, 4;
	add.s64 	%rd17, %rd47, %rd46;
	mul.wide.s32 	%rd48, %r126, 4;
	add.s64 	%rd18, %rd48, %rd46;
	mul.wide.s32 	%rd49, %r128, 4;
	add.s64 	%rd19, %rd49, %rd46;
	mul.wide.s32 	%rd50, %r129, 4;
	add.s64 	%rd20, %rd50, %rd46;
	mul.wide.s32 	%rd51, %r513, 2;
	add.s64 	%rd52, %rd51, %rd4;
	shl.b64 	%rd53, %rd52, 1;
	add.s64 	%rd54, %rd2, %rd53;
	add.s64 	%rd75, %rd54, 512;
	mul.wide.s32 	%rd55, %r513, 4;
	add.s64 	%rd22, %rd55, %rd46;
	mul.wide.s32 	%rd56, %r53, 4;
	add.s64 	%rd57, %rd55, %rd56;
	add.s64 	%rd23, %rd57, %rd46;

$L__BB77_8:
	ld.global.nc.u32 	%r132, [%rd75+-512];
	add.s64 	%rd58, %rd76, %rd22;
	ld.global.nc.u32 	%r133, [%rd58];
	// begin inline asm
	{mul.f16x2 %r131,%r132,%r133;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r134,%r526,%r131;
}
	// end inline asm
	add.s64 	%rd59, %rd76, %rd23;
	ld.global.nc.u32 	%r139, [%rd59];
	// begin inline asm
	{mul.f16x2 %r137,%r132,%r139;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r140,%r525,%r137;
}
	// end inline asm
	add.s64 	%rd60, %rd76, %rd17;
	ld.global.nc.u32 	%r145, [%rd60];
	// begin inline asm
	{mul.f16x2 %r143,%r132,%r145;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r146,%r524,%r143;
}
	// end inline asm
	add.s64 	%rd61, %rd76, %rd18;
	ld.global.nc.u32 	%r151, [%rd61];
	// begin inline asm
	{mul.f16x2 %r149,%r132,%r151;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r152,%r523,%r149;
}
	// end inline asm
	add.s64 	%rd62, %rd76, %rd19;
	ld.global.nc.u32 	%r157, [%rd62];
	// begin inline asm
	{mul.f16x2 %r155,%r132,%r157;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r158,%r522,%r155;
}
	// end inline asm
	add.s64 	%rd63, %rd76, %rd20;
	ld.global.nc.u32 	%r163, [%rd63];
	// begin inline asm
	{mul.f16x2 %r161,%r132,%r163;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r164,%r521,%r161;
}
	// end inline asm
	ld.global.nc.u32 	%r168, [%rd75+-256];
	ld.global.nc.u32 	%r169, [%rd58+256];
	// begin inline asm
	{mul.f16x2 %r167,%r168,%r169;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r170,%r134,%r167;
}
	// end inline asm
	add.s64 	%rd64, %rd76, %rd16;
	ld.global.nc.u32 	%r175, [%rd64];
	// begin inline asm
	{mul.f16x2 %r173,%r168,%r175;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r176,%r140,%r173;
}
	// end inline asm
	ld.global.nc.u32 	%r181, [%rd60+256];
	// begin inline asm
	{mul.f16x2 %r179,%r168,%r181;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r182,%r146,%r179;
}
	// end inline asm
	ld.global.nc.u32 	%r187, [%rd61+256];
	// begin inline asm
	{mul.f16x2 %r185,%r168,%r187;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r188,%r152,%r185;
}
	// end inline asm
	ld.global.nc.u32 	%r193, [%rd62+256];
	// begin inline asm
	{mul.f16x2 %r191,%r168,%r193;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r194,%r158,%r191;
}
	// end inline asm
	ld.global.nc.u32 	%r199, [%rd63+256];
	// begin inline asm
	{mul.f16x2 %r197,%r168,%r199;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r200,%r164,%r197;
}
	// end inline asm
	ld.global.nc.u32 	%r204, [%rd75];
	ld.global.nc.u32 	%r205, [%rd58+512];
	// begin inline asm
	{mul.f16x2 %r203,%r204,%r205;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r206,%r170,%r203;
}
	// end inline asm
	ld.global.nc.u32 	%r211, [%rd64+256];
	// begin inline asm
	{mul.f16x2 %r209,%r204,%r211;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r212,%r176,%r209;
}
	// end inline asm
	ld.global.nc.u32 	%r217, [%rd60+512];
	// begin inline asm
	{mul.f16x2 %r215,%r204,%r217;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r218,%r182,%r215;
}
	// end inline asm
	ld.global.nc.u32 	%r223, [%rd61+512];
	// begin inline asm
	{mul.f16x2 %r221,%r204,%r223;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r224,%r188,%r221;
}
	// end inline asm
	ld.global.nc.u32 	%r229, [%rd62+512];
	// begin inline asm
	{mul.f16x2 %r227,%r204,%r229;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r230,%r194,%r227;
}
	// end inline asm
	ld.global.nc.u32 	%r235, [%rd63+512];
	// begin inline asm
	{mul.f16x2 %r233,%r204,%r235;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r236,%r200,%r233;
}
	// end inline asm
	ld.global.nc.u32 	%r240, [%rd75+256];
	ld.global.nc.u32 	%r241, [%rd58+768];
	// begin inline asm
	{mul.f16x2 %r239,%r240,%r241;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r526,%r206,%r239;
}
	// end inline asm
	ld.global.nc.u32 	%r247, [%rd64+512];
	// begin inline asm
	{mul.f16x2 %r245,%r240,%r247;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r525,%r212,%r245;
}
	// end inline asm
	ld.global.nc.u32 	%r253, [%rd60+768];
	// begin inline asm
	{mul.f16x2 %r251,%r240,%r253;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r524,%r218,%r251;
}
	// end inline asm
	ld.global.nc.u32 	%r259, [%rd61+768];
	// begin inline asm
	{mul.f16x2 %r257,%r240,%r259;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r523,%r224,%r257;
}
	// end inline asm
	ld.global.nc.u32 	%r265, [%rd62+768];
	// begin inline asm
	{mul.f16x2 %r263,%r240,%r265;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r522,%r230,%r263;
}
	// end inline asm
	ld.global.nc.u32 	%r271, [%rd63+768];
	// begin inline asm
	{mul.f16x2 %r269,%r240,%r271;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r521,%r236,%r269;
}
	// end inline asm
	add.s64 	%rd76, %rd76, 1024;
	add.s64 	%rd75, %rd75, 1024;
	add.s32 	%r513, %r513, 256;
	setp.lt.s32 	%p6, %r513, %r52;
	@%p6 bra 	$L__BB77_8;

$L__BB77_9:
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r526;
  cvt.f32.f16 %f8, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r526;
  cvt.f32.f16 %f9, high;}

	// end inline asm
	add.f32 	%f20, %f8, %f9;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r525;
  cvt.f32.f16 %f10, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r525;
  cvt.f32.f16 %f11, high;}

	// end inline asm
	add.f32 	%f1, %f10, %f11;
	st.local.f32 	[%rd3+4], %f1;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r524;
  cvt.f32.f16 %f12, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r524;
  cvt.f32.f16 %f13, high;}

	// end inline asm
	add.f32 	%f2, %f12, %f13;
	st.local.f32 	[%rd3+8], %f2;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r523;
  cvt.f32.f16 %f14, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r523;
  cvt.f32.f16 %f15, high;}

	// end inline asm
	add.f32 	%f3, %f14, %f15;
	st.local.f32 	[%rd3+12], %f3;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r522;
  cvt.f32.f16 %f16, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r522;
  cvt.f32.f16 %f17, high;}

	// end inline asm
	add.f32 	%f4, %f16, %f17;
	st.local.f32 	[%rd3+16], %f4;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r521;
  cvt.f32.f16 %f18, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r521;
  cvt.f32.f16 %f19, high;}

	// end inline asm
	add.f32 	%f5, %f18, %f19;
	st.local.f32 	[%rd3+20], %f5;
	shr.s32 	%r287, %r3, 31;
	shr.u32 	%r288, %r287, 27;
	add.s32 	%r289, %r3, %r288;
	shr.s32 	%r290, %r289, 5;
	shl.b32 	%r291, %r290, 2;
	add.s32 	%r51, %r65, %r291;
	mov.u32 	%r293, 2;
	mov.b32 	%r294, %f20;
	mov.u32 	%r295, 31;
	mov.u32 	%r296, 16;
	mov.u32 	%r297, -1;
	shfl.sync.bfly.b32 	%r298|%p7, %r294, %r296, %r295, %r297;
	mov.b32 	%f21, %r298;
	add.f32 	%f22, %f20, %f21;
	mov.b32 	%r299, %f22;
	mov.u32 	%r300, 8;
	shfl.sync.bfly.b32 	%r301|%p8, %r299, %r300, %r295, %r297;
	mov.b32 	%f23, %r301;
	add.f32 	%f24, %f22, %f23;
	mov.b32 	%r302, %f24;
	mov.u32 	%r303, 4;
	shfl.sync.bfly.b32 	%r304|%p9, %r302, %r303, %r295, %r297;
	mov.b32 	%f25, %r304;
	add.f32 	%f26, %f24, %f25;
	mov.b32 	%r305, %f26;
	shfl.sync.bfly.b32 	%r306|%p10, %r305, %r293, %r295, %r297;
	mov.b32 	%f27, %r306;
	add.f32 	%f28, %f26, %f27;
	mov.b32 	%r307, %f28;
	mov.u32 	%r308, 1;
	shfl.sync.bfly.b32 	%r309|%p11, %r307, %r308, %r295, %r297;
	mov.b32 	%f29, %r309;
	add.f32 	%f30, %f28, %f29;
	st.local.f32 	[%rd3], %f30;
	st.shared.f32 	[%r51], %f30;
	bar.sync 	0;
	@%p1 bra 	$L__BB77_11;

	ld.shared.f32 	%f31, [%r4];
	mov.b32 	%r310, %f31;
	shfl.sync.bfly.b32 	%r314|%p13, %r310, %r296, %r295, %r297;
	mov.b32 	%f32, %r314;
	add.f32 	%f33, %f31, %f32;
	mov.b32 	%r315, %f33;
	shfl.sync.bfly.b32 	%r317|%p14, %r315, %r300, %r295, %r297;
	mov.b32 	%f34, %r317;
	add.f32 	%f35, %f33, %f34;
	mov.b32 	%r318, %f35;
	shfl.sync.bfly.b32 	%r320|%p15, %r318, %r303, %r295, %r297;
	mov.b32 	%f36, %r320;
	add.f32 	%f37, %f35, %f36;
	mov.b32 	%r321, %f37;
	shfl.sync.bfly.b32 	%r323|%p16, %r321, %r293, %r295, %r297;
	mov.b32 	%f38, %r323;
	add.f32 	%f39, %f37, %f38;
	mov.b32 	%r324, %f39;
	shfl.sync.bfly.b32 	%r326|%p17, %r324, %r308, %r295, %r297;
	mov.b32 	%f40, %r326;
	add.f32 	%f41, %f39, %f40;
	st.local.f32 	[%rd3], %f41;

$L__BB77_11:
	bar.sync 	0;
	mov.b32 	%r327, %f1;
	shfl.sync.bfly.b32 	%r331|%p19, %r327, %r296, %r295, %r297;
	mov.b32 	%f42, %r331;
	add.f32 	%f43, %f1, %f42;
	mov.b32 	%r332, %f43;
	shfl.sync.bfly.b32 	%r334|%p20, %r332, %r300, %r295, %r297;
	mov.b32 	%f44, %r334;
	add.f32 	%f45, %f43, %f44;
	mov.b32 	%r335, %f45;
	shfl.sync.bfly.b32 	%r337|%p21, %r335, %r303, %r295, %r297;
	mov.b32 	%f46, %r337;
	add.f32 	%f47, %f45, %f46;
	mov.b32 	%r338, %f47;
	shfl.sync.bfly.b32 	%r340|%p22, %r338, %r293, %r295, %r297;
	mov.b32 	%f48, %r340;
	add.f32 	%f49, %f47, %f48;
	mov.b32 	%r341, %f49;
	shfl.sync.bfly.b32 	%r343|%p23, %r341, %r308, %r295, %r297;
	mov.b32 	%f50, %r343;
	add.f32 	%f51, %f49, %f50;
	st.local.f32 	[%rd3+4], %f51;
	st.shared.f32 	[%r51], %f51;
	bar.sync 	0;
	@%p1 bra 	$L__BB77_13;

	ld.shared.f32 	%f52, [%r4];
	mov.b32 	%r344, %f52;
	mov.u32 	%r345, 31;
	mov.u32 	%r346, 16;
	mov.u32 	%r347, -1;
	shfl.sync.bfly.b32 	%r348|%p24, %r344, %r346, %r345, %r347;
	mov.b32 	%f53, %r348;
	add.f32 	%f54, %f52, %f53;
	mov.b32 	%r349, %f54;
	mov.u32 	%r350, 8;
	shfl.sync.bfly.b32 	%r351|%p25, %r349, %r350, %r345, %r347;
	mov.b32 	%f55, %r351;
	add.f32 	%f56, %f54, %f55;
	mov.b32 	%r352, %f56;
	mov.u32 	%r353, 4;
	shfl.sync.bfly.b32 	%r354|%p26, %r352, %r353, %r345, %r347;
	mov.b32 	%f57, %r354;
	add.f32 	%f58, %f56, %f57;
	mov.b32 	%r355, %f58;
	mov.u32 	%r356, 2;
	shfl.sync.bfly.b32 	%r357|%p27, %r355, %r356, %r345, %r347;
	mov.b32 	%f59, %r357;
	add.f32 	%f60, %f58, %f59;
	mov.b32 	%r358, %f60;
	mov.u32 	%r359, 1;
	shfl.sync.bfly.b32 	%r360|%p28, %r358, %r359, %r345, %r347;
	mov.b32 	%f61, %r360;
	add.f32 	%f62, %f60, %f61;
	st.local.f32 	[%rd3+4], %f62;

$L__BB77_13:
	bar.sync 	0;
	mov.b32 	%r361, %f2;
	mov.u32 	%r362, 31;
	mov.u32 	%r363, 16;
	mov.u32 	%r364, -1;
	shfl.sync.bfly.b32 	%r365|%p30, %r361, %r363, %r362, %r364;
	mov.b32 	%f63, %r365;
	add.f32 	%f64, %f2, %f63;
	mov.b32 	%r366, %f64;
	mov.u32 	%r367, 8;
	shfl.sync.bfly.b32 	%r368|%p31, %r366, %r367, %r362, %r364;
	mov.b32 	%f65, %r368;
	add.f32 	%f66, %f64, %f65;
	mov.b32 	%r369, %f66;
	mov.u32 	%r370, 4;
	shfl.sync.bfly.b32 	%r371|%p32, %r369, %r370, %r362, %r364;
	mov.b32 	%f67, %r371;
	add.f32 	%f68, %f66, %f67;
	mov.b32 	%r372, %f68;
	mov.u32 	%r373, 2;
	shfl.sync.bfly.b32 	%r374|%p33, %r372, %r373, %r362, %r364;
	mov.b32 	%f69, %r374;
	add.f32 	%f70, %f68, %f69;
	mov.b32 	%r375, %f70;
	mov.u32 	%r376, 1;
	shfl.sync.bfly.b32 	%r377|%p34, %r375, %r376, %r362, %r364;
	mov.b32 	%f71, %r377;
	add.f32 	%f72, %f70, %f71;
	st.local.f32 	[%rd3+8], %f72;
	st.shared.f32 	[%r51], %f72;
	bar.sync 	0;
	@%p1 bra 	$L__BB77_15;

	ld.shared.f32 	%f73, [%r4];
	mov.b32 	%r378, %f73;
	shfl.sync.bfly.b32 	%r382|%p35, %r378, %r363, %r362, %r364;
	mov.b32 	%f74, %r382;
	add.f32 	%f75, %f73, %f74;
	mov.b32 	%r383, %f75;
	shfl.sync.bfly.b32 	%r385|%p36, %r383, %r367, %r362, %r364;
	mov.b32 	%f76, %r385;
	add.f32 	%f77, %f75, %f76;
	mov.b32 	%r386, %f77;
	shfl.sync.bfly.b32 	%r388|%p37, %r386, %r370, %r362, %r364;
	mov.b32 	%f78, %r388;
	add.f32 	%f79, %f77, %f78;
	mov.b32 	%r389, %f79;
	shfl.sync.bfly.b32 	%r391|%p38, %r389, %r373, %r362, %r364;
	mov.b32 	%f80, %r391;
	add.f32 	%f81, %f79, %f80;
	mov.b32 	%r392, %f81;
	shfl.sync.bfly.b32 	%r394|%p39, %r392, %r376, %r362, %r364;
	mov.b32 	%f82, %r394;
	add.f32 	%f83, %f81, %f82;
	st.local.f32 	[%rd3+8], %f83;

$L__BB77_15:
	bar.sync 	0;
	mov.b32 	%r395, %f3;
	shfl.sync.bfly.b32 	%r399|%p41, %r395, %r363, %r362, %r364;
	mov.b32 	%f84, %r399;
	add.f32 	%f85, %f3, %f84;
	mov.b32 	%r400, %f85;
	shfl.sync.bfly.b32 	%r402|%p42, %r400, %r367, %r362, %r364;
	mov.b32 	%f86, %r402;
	add.f32 	%f87, %f85, %f86;
	mov.b32 	%r403, %f87;
	shfl.sync.bfly.b32 	%r405|%p43, %r403, %r370, %r362, %r364;
	mov.b32 	%f88, %r405;
	add.f32 	%f89, %f87, %f88;
	mov.b32 	%r406, %f89;
	shfl.sync.bfly.b32 	%r408|%p44, %r406, %r373, %r362, %r364;
	mov.b32 	%f90, %r408;
	add.f32 	%f91, %f89, %f90;
	mov.b32 	%r409, %f91;
	shfl.sync.bfly.b32 	%r411|%p45, %r409, %r376, %r362, %r364;
	mov.b32 	%f92, %r411;
	add.f32 	%f93, %f91, %f92;
	st.local.f32 	[%rd3+12], %f93;
	st.shared.f32 	[%r51], %f93;
	bar.sync 	0;
	@%p1 bra 	$L__BB77_17;

	ld.shared.f32 	%f94, [%r4];
	mov.b32 	%r412, %f94;
	mov.u32 	%r413, 31;
	mov.u32 	%r414, 16;
	mov.u32 	%r415, -1;
	shfl.sync.bfly.b32 	%r416|%p46, %r412, %r414, %r413, %r415;
	mov.b32 	%f95, %r416;
	add.f32 	%f96, %f94, %f95;
	mov.b32 	%r417, %f96;
	mov.u32 	%r418, 8;
	shfl.sync.bfly.b32 	%r419|%p47, %r417, %r418, %r413, %r415;
	mov.b32 	%f97, %r419;
	add.f32 	%f98, %f96, %f97;
	mov.b32 	%r420, %f98;
	mov.u32 	%r421, 4;
	shfl.sync.bfly.b32 	%r422|%p48, %r420, %r421, %r413, %r415;
	mov.b32 	%f99, %r422;
	add.f32 	%f100, %f98, %f99;
	mov.b32 	%r423, %f100;
	mov.u32 	%r424, 2;
	shfl.sync.bfly.b32 	%r425|%p49, %r423, %r424, %r413, %r415;
	mov.b32 	%f101, %r425;
	add.f32 	%f102, %f100, %f101;
	mov.b32 	%r426, %f102;
	mov.u32 	%r427, 1;
	shfl.sync.bfly.b32 	%r428|%p50, %r426, %r427, %r413, %r415;
	mov.b32 	%f103, %r428;
	add.f32 	%f104, %f102, %f103;
	st.local.f32 	[%rd3+12], %f104;

$L__BB77_17:
	bar.sync 	0;
	mov.b32 	%r429, %f4;
	mov.u32 	%r430, 31;
	mov.u32 	%r431, 16;
	mov.u32 	%r432, -1;
	shfl.sync.bfly.b32 	%r433|%p52, %r429, %r431, %r430, %r432;
	mov.b32 	%f105, %r433;
	add.f32 	%f106, %f4, %f105;
	mov.b32 	%r434, %f106;
	mov.u32 	%r435, 8;
	shfl.sync.bfly.b32 	%r436|%p53, %r434, %r435, %r430, %r432;
	mov.b32 	%f107, %r436;
	add.f32 	%f108, %f106, %f107;
	mov.b32 	%r437, %f108;
	mov.u32 	%r438, 4;
	shfl.sync.bfly.b32 	%r439|%p54, %r437, %r438, %r430, %r432;
	mov.b32 	%f109, %r439;
	add.f32 	%f110, %f108, %f109;
	mov.b32 	%r440, %f110;
	mov.u32 	%r441, 2;
	shfl.sync.bfly.b32 	%r442|%p55, %r440, %r441, %r430, %r432;
	mov.b32 	%f111, %r442;
	add.f32 	%f112, %f110, %f111;
	mov.b32 	%r443, %f112;
	mov.u32 	%r444, 1;
	shfl.sync.bfly.b32 	%r445|%p56, %r443, %r444, %r430, %r432;
	mov.b32 	%f113, %r445;
	add.f32 	%f114, %f112, %f113;
	st.local.f32 	[%rd3+16], %f114;
	st.shared.f32 	[%r51], %f114;
	bar.sync 	0;
	@%p1 bra 	$L__BB77_19;

	ld.shared.f32 	%f115, [%r4];
	mov.b32 	%r446, %f115;
	shfl.sync.bfly.b32 	%r450|%p57, %r446, %r431, %r430, %r432;
	mov.b32 	%f116, %r450;
	add.f32 	%f117, %f115, %f116;
	mov.b32 	%r451, %f117;
	shfl.sync.bfly.b32 	%r453|%p58, %r451, %r435, %r430, %r432;
	mov.b32 	%f118, %r453;
	add.f32 	%f119, %f117, %f118;
	mov.b32 	%r454, %f119;
	shfl.sync.bfly.b32 	%r456|%p59, %r454, %r438, %r430, %r432;
	mov.b32 	%f120, %r456;
	add.f32 	%f121, %f119, %f120;
	mov.b32 	%r457, %f121;
	shfl.sync.bfly.b32 	%r459|%p60, %r457, %r441, %r430, %r432;
	mov.b32 	%f122, %r459;
	add.f32 	%f123, %f121, %f122;
	mov.b32 	%r460, %f123;
	shfl.sync.bfly.b32 	%r462|%p61, %r460, %r444, %r430, %r432;
	mov.b32 	%f124, %r462;
	add.f32 	%f125, %f123, %f124;
	st.local.f32 	[%rd3+16], %f125;

$L__BB77_19:
	bar.sync 	0;
	mov.b32 	%r463, %f5;
	shfl.sync.bfly.b32 	%r467|%p63, %r463, %r431, %r430, %r432;
	mov.b32 	%f126, %r467;
	add.f32 	%f127, %f5, %f126;
	mov.b32 	%r468, %f127;
	shfl.sync.bfly.b32 	%r470|%p64, %r468, %r435, %r430, %r432;
	mov.b32 	%f128, %r470;
	add.f32 	%f129, %f127, %f128;
	mov.b32 	%r471, %f129;
	shfl.sync.bfly.b32 	%r473|%p65, %r471, %r438, %r430, %r432;
	mov.b32 	%f130, %r473;
	add.f32 	%f131, %f129, %f130;
	mov.b32 	%r474, %f131;
	shfl.sync.bfly.b32 	%r476|%p66, %r474, %r441, %r430, %r432;
	mov.b32 	%f132, %r476;
	add.f32 	%f133, %f131, %f132;
	mov.b32 	%r477, %f133;
	shfl.sync.bfly.b32 	%r479|%p67, %r477, %r444, %r430, %r432;
	mov.b32 	%f134, %r479;
	add.f32 	%f135, %f133, %f134;
	st.local.f32 	[%rd3+20], %f135;
	st.shared.f32 	[%r51], %f135;
	bar.sync 	0;
	@%p1 bra 	$L__BB77_21;

	ld.shared.f32 	%f136, [%r4];
	mov.b32 	%r480, %f136;
	mov.u32 	%r481, 31;
	mov.u32 	%r482, 16;
	mov.u32 	%r483, -1;
	shfl.sync.bfly.b32 	%r484|%p68, %r480, %r482, %r481, %r483;
	mov.b32 	%f137, %r484;
	add.f32 	%f138, %f136, %f137;
	mov.b32 	%r485, %f138;
	mov.u32 	%r486, 8;
	shfl.sync.bfly.b32 	%r487|%p69, %r485, %r486, %r481, %r483;
	mov.b32 	%f139, %r487;
	add.f32 	%f140, %f138, %f139;
	mov.b32 	%r488, %f140;
	mov.u32 	%r489, 4;
	shfl.sync.bfly.b32 	%r490|%p70, %r488, %r489, %r481, %r483;
	mov.b32 	%f141, %r490;
	add.f32 	%f142, %f140, %f141;
	mov.b32 	%r491, %f142;
	mov.u32 	%r492, 2;
	shfl.sync.bfly.b32 	%r493|%p71, %r491, %r492, %r481, %r483;
	mov.b32 	%f143, %r493;
	add.f32 	%f144, %f142, %f143;
	mov.b32 	%r494, %f144;
	mov.u32 	%r495, 1;
	shfl.sync.bfly.b32 	%r496|%p72, %r494, %r495, %r481, %r483;
	mov.b32 	%f145, %r496;
	add.f32 	%f146, %f144, %f145;
	st.local.f32 	[%rd3+20], %f146;

$L__BB77_21:
	bar.sync 	0;
	setp.gt.s32 	%p73, %r3, 5;
	@%p73 bra 	$L__BB77_23;

	mad.lo.s32 	%r497, %r3, %r54, %r2;
	cvt.s64.s32 	%rd65, %r497;
	mul.lo.s32 	%r498, %r1, %r55;
	cvt.s64.s32 	%rd66, %r498;
	add.s64 	%rd67, %rd66, %rd65;
	mul.wide.s32 	%rd68, %r3, 4;
	add.s64 	%rd69, %rd3, %rd68;
	ld.local.f32 	%f147, [%rd69];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs3, %f147;}

	// end inline asm
	cvta.to.global.u64 	%rd70, %rd28;
	shl.b64 	%rd71, %rd67, 1;
	add.s64 	%rd72, %rd70, %rd71;
	st.global.u16 	[%rd72], %rs3;

$L__BB77_23:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_7_bs_64
.visible .entry ggml_matvec_f16_ncols_7_bs_64(
	.param .u64 ggml_matvec_f16_ncols_7_bs_64_param_0,
	.param .u64 ggml_matvec_f16_ncols_7_bs_64_param_1,
	.param .u64 ggml_matvec_f16_ncols_7_bs_64_param_2,
	.param .u32 ggml_matvec_f16_ncols_7_bs_64_param_3,
	.param .u32 ggml_matvec_f16_ncols_7_bs_64_param_4,
	.param .u32 ggml_matvec_f16_ncols_7_bs_64_param_5,
	.param .u32 ggml_matvec_f16_ncols_7_bs_64_param_6,
	.param .u32 ggml_matvec_f16_ncols_7_bs_64_param_7,
	.param .u32 ggml_matvec_f16_ncols_7_bs_64_param_8,
	.param .u32 ggml_matvec_f16_ncols_7_bs_64_param_9,
	.param .u32 ggml_matvec_f16_ncols_7_bs_64_param_10,
	.param .u32 ggml_matvec_f16_ncols_7_bs_64_param_11
)
{
	.local .align 4 .b8 	__local_depot78[28];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<85>;
	.reg .b16 	%rs<4>;
	.reg .f32 	%f<172>;
	.reg .b32 	%r<607>;
	.reg .b64 	%rd<81>;


	mov.u64 	%SPL, __local_depot78;
	ld.param.u64 	%rd30, [ggml_matvec_f16_ncols_7_bs_64_param_0];
	ld.param.u64 	%rd31, [ggml_matvec_f16_ncols_7_bs_64_param_1];
	ld.param.u64 	%rd29, [ggml_matvec_f16_ncols_7_bs_64_param_2];
	ld.param.u32 	%r58, [ggml_matvec_f16_ncols_7_bs_64_param_3];
	ld.param.u32 	%r62, [ggml_matvec_f16_ncols_7_bs_64_param_5];
	ld.param.u32 	%r59, [ggml_matvec_f16_ncols_7_bs_64_param_6];
	ld.param.u32 	%r60, [ggml_matvec_f16_ncols_7_bs_64_param_7];
	ld.param.u32 	%r63, [ggml_matvec_f16_ncols_7_bs_64_param_8];
	ld.param.u32 	%r64, [ggml_matvec_f16_ncols_7_bs_64_param_9];
	ld.param.u32 	%r65, [ggml_matvec_f16_ncols_7_bs_64_param_10];
	ld.param.u32 	%r61, [ggml_matvec_f16_ncols_7_bs_64_param_11];
	cvta.to.global.u64 	%rd80, %rd31;
	cvta.to.global.u64 	%rd2, %rd30;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r66, %r1, %r63;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r67, %r2, %r62;
	mad.lo.s32 	%r68, %r66, %r64, %r67;
	cvt.s64.s32 	%rd4, %r68;
	mul.lo.s32 	%r69, %r1, %r65;
	cvt.s64.s32 	%rd5, %r69;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r70, %r3, 2;
	mov.u32 	%r71, data_mmv;
	add.s32 	%r4, %r71, %r70;
	@%p1 bra 	$L__BB78_2;

	mov.u32 	%r72, 0;
	st.shared.u32 	[%r4], %r72;

$L__BB78_2:
	bar.sync 	0;
	mov.f32 	%f8, 0f00000000;
	mov.u32 	%r600, 0;
	st.local.u32 	[%rd3], %r600;
	st.local.u32 	[%rd3+4], %r600;
	st.local.u32 	[%rd3+8], %r600;
	st.local.u32 	[%rd3+12], %r600;
	st.local.u32 	[%rd3+16], %r600;
	st.local.u32 	[%rd3+20], %r600;
	st.local.u32 	[%rd3+24], %r600;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs2, %f8;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f8;}

	// end inline asm
	mov.b32 	%r606, {%rs1, %rs2};
	setp.ge.s32 	%p2, %r3, %r58;
	mov.u32 	%r601, %r600;
	mov.u32 	%r602, %r600;
	mov.u32 	%r603, %r600;
	mov.u32 	%r604, %r600;
	mov.u32 	%r605, %r600;
	@%p2 bra 	$L__BB78_9;

	not.b32 	%r85, %r3;
	add.s32 	%r6, %r85, %r58;
	shr.u32 	%r86, %r6, 6;
	add.s32 	%r87, %r86, 1;
	and.b32  	%r583, %r87, 3;
	setp.eq.s32 	%p3, %r583, 0;
	mov.u32 	%r600, 0;
	mov.u32 	%r591, %r3;
	@%p3 bra 	$L__BB78_6;

	shl.b32 	%r94, %r59, 1;
	add.s32 	%r95, %r3, %r94;
	mul.wide.s32 	%rd33, %r95, 4;
	shl.b64 	%rd34, %rd5, 1;
	add.s64 	%rd7, %rd33, %rd34;
	mul.wide.s32 	%rd35, %r3, 4;
	mul.wide.s32 	%rd8, %r59, 4;
	add.s64 	%rd36, %rd35, %rd8;
	add.s64 	%rd9, %rd36, %rd34;
	add.s64 	%rd10, %rd35, %rd34;
	mul.wide.s32 	%rd37, %r3, 2;
	add.s64 	%rd38, %rd37, %rd4;
	shl.b64 	%rd39, %rd38, 1;
	add.s64 	%rd77, %rd2, %rd39;
	mov.u32 	%r600, 0;
	mov.u64 	%rd78, %rd80;
	mov.u32 	%r601, %r600;
	mov.u32 	%r602, %r600;
	mov.u32 	%r603, %r600;
	mov.u32 	%r604, %r600;
	mov.u32 	%r605, %r600;
	mov.u32 	%r591, %r3;

$L__BB78_5:
	.pragma "nounroll";
	ld.global.nc.u32 	%r97, [%rd77];
	add.s64 	%rd40, %rd78, %rd10;
	ld.global.nc.u32 	%r98, [%rd40];
	// begin inline asm
	{mul.f16x2 %r96,%r97,%r98;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r606,%r606,%r96;
}
	// end inline asm
	add.s64 	%rd41, %rd78, %rd9;
	ld.global.nc.u32 	%r104, [%rd41];
	// begin inline asm
	{mul.f16x2 %r102,%r97,%r104;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r605,%r605,%r102;
}
	// end inline asm
	add.s64 	%rd42, %rd78, %rd7;
	ld.global.nc.u32 	%r110, [%rd42];
	// begin inline asm
	{mul.f16x2 %r108,%r97,%r110;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r604,%r604,%r108;
}
	// end inline asm
	add.s64 	%rd43, %rd42, %rd8;
	ld.global.nc.u32 	%r116, [%rd43];
	// begin inline asm
	{mul.f16x2 %r114,%r97,%r116;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r603,%r603,%r114;
}
	// end inline asm
	add.s64 	%rd44, %rd43, %rd8;
	ld.global.nc.u32 	%r122, [%rd44];
	// begin inline asm
	{mul.f16x2 %r120,%r97,%r122;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r602,%r602,%r120;
}
	// end inline asm
	add.s64 	%rd45, %rd44, %rd8;
	ld.global.nc.u32 	%r128, [%rd45];
	// begin inline asm
	{mul.f16x2 %r126,%r97,%r128;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r601,%r601,%r126;
}
	// end inline asm
	add.s64 	%rd46, %rd45, %rd8;
	ld.global.nc.u32 	%r134, [%rd46];
	// begin inline asm
	{mul.f16x2 %r132,%r97,%r134;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r600,%r600,%r132;
}
	// end inline asm
	add.s32 	%r591, %r591, 64;
	add.s64 	%rd78, %rd78, 256;
	add.s64 	%rd77, %rd77, 256;
	add.s32 	%r583, %r583, -1;
	setp.ne.s32 	%p4, %r583, 0;
	@%p4 bra 	$L__BB78_5;

$L__BB78_6:
	setp.lt.u32 	%p5, %r6, 192;
	@%p5 bra 	$L__BB78_9;

	add.s32 	%r138, %r591, %r59;
	shl.b32 	%r139, %r59, 1;
	add.s32 	%r140, %r591, %r139;
	mad.lo.s32 	%r141, %r59, 3, %r591;
	shl.b32 	%r142, %r59, 2;
	add.s32 	%r143, %r591, %r142;
	mad.lo.s32 	%r144, %r59, 5, %r591;
	mad.lo.s32 	%r145, %r59, 6, %r591;
	add.s32 	%r146, %r138, 64;
	mul.wide.s32 	%rd47, %r146, 4;
	shl.b64 	%rd48, %rd5, 1;
	add.s64 	%rd16, %rd47, %rd48;
	mul.wide.s32 	%rd49, %r140, 4;
	add.s64 	%rd17, %rd49, %rd48;
	mul.wide.s32 	%rd50, %r141, 4;
	add.s64 	%rd18, %rd50, %rd48;
	mul.wide.s32 	%rd51, %r143, 4;
	add.s64 	%rd19, %rd51, %rd48;
	mul.wide.s32 	%rd52, %r144, 4;
	add.s64 	%rd20, %rd52, %rd48;
	mul.wide.s32 	%rd53, %r145, 4;
	add.s64 	%rd21, %rd53, %rd48;
	mul.wide.s32 	%rd54, %r591, 2;
	add.s64 	%rd55, %rd54, %rd4;
	shl.b64 	%rd56, %rd55, 1;
	add.s64 	%rd57, %rd2, %rd56;
	add.s64 	%rd79, %rd57, 512;
	mul.wide.s32 	%rd58, %r591, 4;
	add.s64 	%rd23, %rd58, %rd48;
	mul.wide.s32 	%rd59, %r59, 4;
	add.s64 	%rd60, %rd58, %rd59;
	add.s64 	%rd24, %rd60, %rd48;

$L__BB78_8:
	ld.global.nc.u32 	%r148, [%rd79+-512];
	add.s64 	%rd61, %rd80, %rd23;
	ld.global.nc.u32 	%r149, [%rd61];
	// begin inline asm
	{mul.f16x2 %r147,%r148,%r149;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r150,%r606,%r147;
}
	// end inline asm
	add.s64 	%rd62, %rd80, %rd24;
	ld.global.nc.u32 	%r155, [%rd62];
	// begin inline asm
	{mul.f16x2 %r153,%r148,%r155;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r156,%r605,%r153;
}
	// end inline asm
	add.s64 	%rd63, %rd80, %rd17;
	ld.global.nc.u32 	%r161, [%rd63];
	// begin inline asm
	{mul.f16x2 %r159,%r148,%r161;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r162,%r604,%r159;
}
	// end inline asm
	add.s64 	%rd64, %rd80, %rd18;
	ld.global.nc.u32 	%r167, [%rd64];
	// begin inline asm
	{mul.f16x2 %r165,%r148,%r167;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r168,%r603,%r165;
}
	// end inline asm
	add.s64 	%rd65, %rd80, %rd19;
	ld.global.nc.u32 	%r173, [%rd65];
	// begin inline asm
	{mul.f16x2 %r171,%r148,%r173;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r174,%r602,%r171;
}
	// end inline asm
	add.s64 	%rd66, %rd80, %rd20;
	ld.global.nc.u32 	%r179, [%rd66];
	// begin inline asm
	{mul.f16x2 %r177,%r148,%r179;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r180,%r601,%r177;
}
	// end inline asm
	add.s64 	%rd67, %rd80, %rd21;
	ld.global.nc.u32 	%r185, [%rd67];
	// begin inline asm
	{mul.f16x2 %r183,%r148,%r185;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r186,%r600,%r183;
}
	// end inline asm
	ld.global.nc.u32 	%r190, [%rd79+-256];
	ld.global.nc.u32 	%r191, [%rd61+256];
	// begin inline asm
	{mul.f16x2 %r189,%r190,%r191;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r192,%r150,%r189;
}
	// end inline asm
	add.s64 	%rd68, %rd80, %rd16;
	ld.global.nc.u32 	%r197, [%rd68];
	// begin inline asm
	{mul.f16x2 %r195,%r190,%r197;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r198,%r156,%r195;
}
	// end inline asm
	ld.global.nc.u32 	%r203, [%rd63+256];
	// begin inline asm
	{mul.f16x2 %r201,%r190,%r203;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r204,%r162,%r201;
}
	// end inline asm
	ld.global.nc.u32 	%r209, [%rd64+256];
	// begin inline asm
	{mul.f16x2 %r207,%r190,%r209;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r210,%r168,%r207;
}
	// end inline asm
	ld.global.nc.u32 	%r215, [%rd65+256];
	// begin inline asm
	{mul.f16x2 %r213,%r190,%r215;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r216,%r174,%r213;
}
	// end inline asm
	ld.global.nc.u32 	%r221, [%rd66+256];
	// begin inline asm
	{mul.f16x2 %r219,%r190,%r221;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r222,%r180,%r219;
}
	// end inline asm
	ld.global.nc.u32 	%r227, [%rd67+256];
	// begin inline asm
	{mul.f16x2 %r225,%r190,%r227;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r228,%r186,%r225;
}
	// end inline asm
	ld.global.nc.u32 	%r232, [%rd79];
	ld.global.nc.u32 	%r233, [%rd61+512];
	// begin inline asm
	{mul.f16x2 %r231,%r232,%r233;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r234,%r192,%r231;
}
	// end inline asm
	ld.global.nc.u32 	%r239, [%rd68+256];
	// begin inline asm
	{mul.f16x2 %r237,%r232,%r239;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r240,%r198,%r237;
}
	// end inline asm
	ld.global.nc.u32 	%r245, [%rd63+512];
	// begin inline asm
	{mul.f16x2 %r243,%r232,%r245;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r246,%r204,%r243;
}
	// end inline asm
	ld.global.nc.u32 	%r251, [%rd64+512];
	// begin inline asm
	{mul.f16x2 %r249,%r232,%r251;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r252,%r210,%r249;
}
	// end inline asm
	ld.global.nc.u32 	%r257, [%rd65+512];
	// begin inline asm
	{mul.f16x2 %r255,%r232,%r257;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r258,%r216,%r255;
}
	// end inline asm
	ld.global.nc.u32 	%r263, [%rd66+512];
	// begin inline asm
	{mul.f16x2 %r261,%r232,%r263;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r264,%r222,%r261;
}
	// end inline asm
	ld.global.nc.u32 	%r269, [%rd67+512];
	// begin inline asm
	{mul.f16x2 %r267,%r232,%r269;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r270,%r228,%r267;
}
	// end inline asm
	ld.global.nc.u32 	%r274, [%rd79+256];
	ld.global.nc.u32 	%r275, [%rd61+768];
	// begin inline asm
	{mul.f16x2 %r273,%r274,%r275;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r606,%r234,%r273;
}
	// end inline asm
	ld.global.nc.u32 	%r281, [%rd68+512];
	// begin inline asm
	{mul.f16x2 %r279,%r274,%r281;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r605,%r240,%r279;
}
	// end inline asm
	ld.global.nc.u32 	%r287, [%rd63+768];
	// begin inline asm
	{mul.f16x2 %r285,%r274,%r287;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r604,%r246,%r285;
}
	// end inline asm
	ld.global.nc.u32 	%r293, [%rd64+768];
	// begin inline asm
	{mul.f16x2 %r291,%r274,%r293;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r603,%r252,%r291;
}
	// end inline asm
	ld.global.nc.u32 	%r299, [%rd65+768];
	// begin inline asm
	{mul.f16x2 %r297,%r274,%r299;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r602,%r258,%r297;
}
	// end inline asm
	ld.global.nc.u32 	%r305, [%rd66+768];
	// begin inline asm
	{mul.f16x2 %r303,%r274,%r305;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r601,%r264,%r303;
}
	// end inline asm
	ld.global.nc.u32 	%r311, [%rd67+768];
	// begin inline asm
	{mul.f16x2 %r309,%r274,%r311;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r600,%r270,%r309;
}
	// end inline asm
	add.s64 	%rd80, %rd80, 1024;
	add.s64 	%rd79, %rd79, 1024;
	add.s32 	%r591, %r591, 256;
	setp.lt.s32 	%p6, %r591, %r58;
	@%p6 bra 	$L__BB78_8;

$L__BB78_9:
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r606;
  cvt.f32.f16 %f9, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r606;
  cvt.f32.f16 %f10, high;}

	// end inline asm
	add.f32 	%f23, %f9, %f10;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r605;
  cvt.f32.f16 %f11, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r605;
  cvt.f32.f16 %f12, high;}

	// end inline asm
	add.f32 	%f1, %f11, %f12;
	st.local.f32 	[%rd3+4], %f1;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r604;
  cvt.f32.f16 %f13, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r604;
  cvt.f32.f16 %f14, high;}

	// end inline asm
	add.f32 	%f2, %f13, %f14;
	st.local.f32 	[%rd3+8], %f2;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r603;
  cvt.f32.f16 %f15, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r603;
  cvt.f32.f16 %f16, high;}

	// end inline asm
	add.f32 	%f3, %f15, %f16;
	st.local.f32 	[%rd3+12], %f3;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r602;
  cvt.f32.f16 %f17, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r602;
  cvt.f32.f16 %f18, high;}

	// end inline asm
	add.f32 	%f4, %f17, %f18;
	st.local.f32 	[%rd3+16], %f4;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r601;
  cvt.f32.f16 %f19, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r601;
  cvt.f32.f16 %f20, high;}

	// end inline asm
	add.f32 	%f5, %f19, %f20;
	st.local.f32 	[%rd3+20], %f5;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r600;
  cvt.f32.f16 %f21, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r600;
  cvt.f32.f16 %f22, high;}

	// end inline asm
	add.f32 	%f6, %f21, %f22;
	st.local.f32 	[%rd3+24], %f6;
	shr.s32 	%r329, %r3, 31;
	shr.u32 	%r330, %r329, 27;
	add.s32 	%r331, %r3, %r330;
	shr.s32 	%r332, %r331, 5;
	shl.b32 	%r333, %r332, 2;
	add.s32 	%r57, %r71, %r333;
	mov.u32 	%r335, 2;
	mov.b32 	%r336, %f23;
	mov.u32 	%r337, 31;
	mov.u32 	%r338, 16;
	mov.u32 	%r339, -1;
	shfl.sync.bfly.b32 	%r340|%p7, %r336, %r338, %r337, %r339;
	mov.b32 	%f24, %r340;
	add.f32 	%f25, %f23, %f24;
	mov.b32 	%r341, %f25;
	mov.u32 	%r342, 8;
	shfl.sync.bfly.b32 	%r343|%p8, %r341, %r342, %r337, %r339;
	mov.b32 	%f26, %r343;
	add.f32 	%f27, %f25, %f26;
	mov.b32 	%r344, %f27;
	mov.u32 	%r345, 4;
	shfl.sync.bfly.b32 	%r346|%p9, %r344, %r345, %r337, %r339;
	mov.b32 	%f28, %r346;
	add.f32 	%f29, %f27, %f28;
	mov.b32 	%r347, %f29;
	shfl.sync.bfly.b32 	%r348|%p10, %r347, %r335, %r337, %r339;
	mov.b32 	%f30, %r348;
	add.f32 	%f31, %f29, %f30;
	mov.b32 	%r349, %f31;
	mov.u32 	%r350, 1;
	shfl.sync.bfly.b32 	%r351|%p11, %r349, %r350, %r337, %r339;
	mov.b32 	%f32, %r351;
	add.f32 	%f33, %f31, %f32;
	st.local.f32 	[%rd3], %f33;
	st.shared.f32 	[%r57], %f33;
	bar.sync 	0;
	@%p1 bra 	$L__BB78_11;

	ld.shared.f32 	%f34, [%r4];
	mov.b32 	%r352, %f34;
	shfl.sync.bfly.b32 	%r356|%p13, %r352, %r338, %r337, %r339;
	mov.b32 	%f35, %r356;
	add.f32 	%f36, %f34, %f35;
	mov.b32 	%r357, %f36;
	shfl.sync.bfly.b32 	%r359|%p14, %r357, %r342, %r337, %r339;
	mov.b32 	%f37, %r359;
	add.f32 	%f38, %f36, %f37;
	mov.b32 	%r360, %f38;
	shfl.sync.bfly.b32 	%r362|%p15, %r360, %r345, %r337, %r339;
	mov.b32 	%f39, %r362;
	add.f32 	%f40, %f38, %f39;
	mov.b32 	%r363, %f40;
	shfl.sync.bfly.b32 	%r365|%p16, %r363, %r335, %r337, %r339;
	mov.b32 	%f41, %r365;
	add.f32 	%f42, %f40, %f41;
	mov.b32 	%r366, %f42;
	shfl.sync.bfly.b32 	%r368|%p17, %r366, %r350, %r337, %r339;
	mov.b32 	%f43, %r368;
	add.f32 	%f44, %f42, %f43;
	st.local.f32 	[%rd3], %f44;

$L__BB78_11:
	bar.sync 	0;
	mov.b32 	%r369, %f1;
	shfl.sync.bfly.b32 	%r373|%p19, %r369, %r338, %r337, %r339;
	mov.b32 	%f45, %r373;
	add.f32 	%f46, %f1, %f45;
	mov.b32 	%r374, %f46;
	shfl.sync.bfly.b32 	%r376|%p20, %r374, %r342, %r337, %r339;
	mov.b32 	%f47, %r376;
	add.f32 	%f48, %f46, %f47;
	mov.b32 	%r377, %f48;
	shfl.sync.bfly.b32 	%r379|%p21, %r377, %r345, %r337, %r339;
	mov.b32 	%f49, %r379;
	add.f32 	%f50, %f48, %f49;
	mov.b32 	%r380, %f50;
	shfl.sync.bfly.b32 	%r382|%p22, %r380, %r335, %r337, %r339;
	mov.b32 	%f51, %r382;
	add.f32 	%f52, %f50, %f51;
	mov.b32 	%r383, %f52;
	shfl.sync.bfly.b32 	%r385|%p23, %r383, %r350, %r337, %r339;
	mov.b32 	%f53, %r385;
	add.f32 	%f54, %f52, %f53;
	st.local.f32 	[%rd3+4], %f54;
	st.shared.f32 	[%r57], %f54;
	bar.sync 	0;
	@%p1 bra 	$L__BB78_13;

	ld.shared.f32 	%f55, [%r4];
	mov.b32 	%r386, %f55;
	mov.u32 	%r387, 31;
	mov.u32 	%r388, 16;
	mov.u32 	%r389, -1;
	shfl.sync.bfly.b32 	%r390|%p24, %r386, %r388, %r387, %r389;
	mov.b32 	%f56, %r390;
	add.f32 	%f57, %f55, %f56;
	mov.b32 	%r391, %f57;
	mov.u32 	%r392, 8;
	shfl.sync.bfly.b32 	%r393|%p25, %r391, %r392, %r387, %r389;
	mov.b32 	%f58, %r393;
	add.f32 	%f59, %f57, %f58;
	mov.b32 	%r394, %f59;
	mov.u32 	%r395, 4;
	shfl.sync.bfly.b32 	%r396|%p26, %r394, %r395, %r387, %r389;
	mov.b32 	%f60, %r396;
	add.f32 	%f61, %f59, %f60;
	mov.b32 	%r397, %f61;
	mov.u32 	%r398, 2;
	shfl.sync.bfly.b32 	%r399|%p27, %r397, %r398, %r387, %r389;
	mov.b32 	%f62, %r399;
	add.f32 	%f63, %f61, %f62;
	mov.b32 	%r400, %f63;
	mov.u32 	%r401, 1;
	shfl.sync.bfly.b32 	%r402|%p28, %r400, %r401, %r387, %r389;
	mov.b32 	%f64, %r402;
	add.f32 	%f65, %f63, %f64;
	st.local.f32 	[%rd3+4], %f65;

$L__BB78_13:
	bar.sync 	0;
	mov.b32 	%r403, %f2;
	mov.u32 	%r404, 31;
	mov.u32 	%r405, 16;
	mov.u32 	%r406, -1;
	shfl.sync.bfly.b32 	%r407|%p30, %r403, %r405, %r404, %r406;
	mov.b32 	%f66, %r407;
	add.f32 	%f67, %f2, %f66;
	mov.b32 	%r408, %f67;
	mov.u32 	%r409, 8;
	shfl.sync.bfly.b32 	%r410|%p31, %r408, %r409, %r404, %r406;
	mov.b32 	%f68, %r410;
	add.f32 	%f69, %f67, %f68;
	mov.b32 	%r411, %f69;
	mov.u32 	%r412, 4;
	shfl.sync.bfly.b32 	%r413|%p32, %r411, %r412, %r404, %r406;
	mov.b32 	%f70, %r413;
	add.f32 	%f71, %f69, %f70;
	mov.b32 	%r414, %f71;
	mov.u32 	%r415, 2;
	shfl.sync.bfly.b32 	%r416|%p33, %r414, %r415, %r404, %r406;
	mov.b32 	%f72, %r416;
	add.f32 	%f73, %f71, %f72;
	mov.b32 	%r417, %f73;
	mov.u32 	%r418, 1;
	shfl.sync.bfly.b32 	%r419|%p34, %r417, %r418, %r404, %r406;
	mov.b32 	%f74, %r419;
	add.f32 	%f75, %f73, %f74;
	st.local.f32 	[%rd3+8], %f75;
	st.shared.f32 	[%r57], %f75;
	bar.sync 	0;
	@%p1 bra 	$L__BB78_15;

	ld.shared.f32 	%f76, [%r4];
	mov.b32 	%r420, %f76;
	shfl.sync.bfly.b32 	%r424|%p35, %r420, %r405, %r404, %r406;
	mov.b32 	%f77, %r424;
	add.f32 	%f78, %f76, %f77;
	mov.b32 	%r425, %f78;
	shfl.sync.bfly.b32 	%r427|%p36, %r425, %r409, %r404, %r406;
	mov.b32 	%f79, %r427;
	add.f32 	%f80, %f78, %f79;
	mov.b32 	%r428, %f80;
	shfl.sync.bfly.b32 	%r430|%p37, %r428, %r412, %r404, %r406;
	mov.b32 	%f81, %r430;
	add.f32 	%f82, %f80, %f81;
	mov.b32 	%r431, %f82;
	shfl.sync.bfly.b32 	%r433|%p38, %r431, %r415, %r404, %r406;
	mov.b32 	%f83, %r433;
	add.f32 	%f84, %f82, %f83;
	mov.b32 	%r434, %f84;
	shfl.sync.bfly.b32 	%r436|%p39, %r434, %r418, %r404, %r406;
	mov.b32 	%f85, %r436;
	add.f32 	%f86, %f84, %f85;
	st.local.f32 	[%rd3+8], %f86;

$L__BB78_15:
	bar.sync 	0;
	mov.b32 	%r437, %f3;
	shfl.sync.bfly.b32 	%r441|%p41, %r437, %r405, %r404, %r406;
	mov.b32 	%f87, %r441;
	add.f32 	%f88, %f3, %f87;
	mov.b32 	%r442, %f88;
	shfl.sync.bfly.b32 	%r444|%p42, %r442, %r409, %r404, %r406;
	mov.b32 	%f89, %r444;
	add.f32 	%f90, %f88, %f89;
	mov.b32 	%r445, %f90;
	shfl.sync.bfly.b32 	%r447|%p43, %r445, %r412, %r404, %r406;
	mov.b32 	%f91, %r447;
	add.f32 	%f92, %f90, %f91;
	mov.b32 	%r448, %f92;
	shfl.sync.bfly.b32 	%r450|%p44, %r448, %r415, %r404, %r406;
	mov.b32 	%f93, %r450;
	add.f32 	%f94, %f92, %f93;
	mov.b32 	%r451, %f94;
	shfl.sync.bfly.b32 	%r453|%p45, %r451, %r418, %r404, %r406;
	mov.b32 	%f95, %r453;
	add.f32 	%f96, %f94, %f95;
	st.local.f32 	[%rd3+12], %f96;
	st.shared.f32 	[%r57], %f96;
	bar.sync 	0;
	@%p1 bra 	$L__BB78_17;

	ld.shared.f32 	%f97, [%r4];
	mov.b32 	%r454, %f97;
	mov.u32 	%r455, 31;
	mov.u32 	%r456, 16;
	mov.u32 	%r457, -1;
	shfl.sync.bfly.b32 	%r458|%p46, %r454, %r456, %r455, %r457;
	mov.b32 	%f98, %r458;
	add.f32 	%f99, %f97, %f98;
	mov.b32 	%r459, %f99;
	mov.u32 	%r460, 8;
	shfl.sync.bfly.b32 	%r461|%p47, %r459, %r460, %r455, %r457;
	mov.b32 	%f100, %r461;
	add.f32 	%f101, %f99, %f100;
	mov.b32 	%r462, %f101;
	mov.u32 	%r463, 4;
	shfl.sync.bfly.b32 	%r464|%p48, %r462, %r463, %r455, %r457;
	mov.b32 	%f102, %r464;
	add.f32 	%f103, %f101, %f102;
	mov.b32 	%r465, %f103;
	mov.u32 	%r466, 2;
	shfl.sync.bfly.b32 	%r467|%p49, %r465, %r466, %r455, %r457;
	mov.b32 	%f104, %r467;
	add.f32 	%f105, %f103, %f104;
	mov.b32 	%r468, %f105;
	mov.u32 	%r469, 1;
	shfl.sync.bfly.b32 	%r470|%p50, %r468, %r469, %r455, %r457;
	mov.b32 	%f106, %r470;
	add.f32 	%f107, %f105, %f106;
	st.local.f32 	[%rd3+12], %f107;

$L__BB78_17:
	bar.sync 	0;
	mov.b32 	%r471, %f4;
	mov.u32 	%r472, 31;
	mov.u32 	%r473, 16;
	mov.u32 	%r474, -1;
	shfl.sync.bfly.b32 	%r475|%p52, %r471, %r473, %r472, %r474;
	mov.b32 	%f108, %r475;
	add.f32 	%f109, %f4, %f108;
	mov.b32 	%r476, %f109;
	mov.u32 	%r477, 8;
	shfl.sync.bfly.b32 	%r478|%p53, %r476, %r477, %r472, %r474;
	mov.b32 	%f110, %r478;
	add.f32 	%f111, %f109, %f110;
	mov.b32 	%r479, %f111;
	mov.u32 	%r480, 4;
	shfl.sync.bfly.b32 	%r481|%p54, %r479, %r480, %r472, %r474;
	mov.b32 	%f112, %r481;
	add.f32 	%f113, %f111, %f112;
	mov.b32 	%r482, %f113;
	mov.u32 	%r483, 2;
	shfl.sync.bfly.b32 	%r484|%p55, %r482, %r483, %r472, %r474;
	mov.b32 	%f114, %r484;
	add.f32 	%f115, %f113, %f114;
	mov.b32 	%r485, %f115;
	mov.u32 	%r486, 1;
	shfl.sync.bfly.b32 	%r487|%p56, %r485, %r486, %r472, %r474;
	mov.b32 	%f116, %r487;
	add.f32 	%f117, %f115, %f116;
	st.local.f32 	[%rd3+16], %f117;
	st.shared.f32 	[%r57], %f117;
	bar.sync 	0;
	@%p1 bra 	$L__BB78_19;

	ld.shared.f32 	%f118, [%r4];
	mov.b32 	%r488, %f118;
	shfl.sync.bfly.b32 	%r492|%p57, %r488, %r473, %r472, %r474;
	mov.b32 	%f119, %r492;
	add.f32 	%f120, %f118, %f119;
	mov.b32 	%r493, %f120;
	shfl.sync.bfly.b32 	%r495|%p58, %r493, %r477, %r472, %r474;
	mov.b32 	%f121, %r495;
	add.f32 	%f122, %f120, %f121;
	mov.b32 	%r496, %f122;
	shfl.sync.bfly.b32 	%r498|%p59, %r496, %r480, %r472, %r474;
	mov.b32 	%f123, %r498;
	add.f32 	%f124, %f122, %f123;
	mov.b32 	%r499, %f124;
	shfl.sync.bfly.b32 	%r501|%p60, %r499, %r483, %r472, %r474;
	mov.b32 	%f125, %r501;
	add.f32 	%f126, %f124, %f125;
	mov.b32 	%r502, %f126;
	shfl.sync.bfly.b32 	%r504|%p61, %r502, %r486, %r472, %r474;
	mov.b32 	%f127, %r504;
	add.f32 	%f128, %f126, %f127;
	st.local.f32 	[%rd3+16], %f128;

$L__BB78_19:
	bar.sync 	0;
	mov.b32 	%r505, %f5;
	shfl.sync.bfly.b32 	%r509|%p63, %r505, %r473, %r472, %r474;
	mov.b32 	%f129, %r509;
	add.f32 	%f130, %f5, %f129;
	mov.b32 	%r510, %f130;
	shfl.sync.bfly.b32 	%r512|%p64, %r510, %r477, %r472, %r474;
	mov.b32 	%f131, %r512;
	add.f32 	%f132, %f130, %f131;
	mov.b32 	%r513, %f132;
	shfl.sync.bfly.b32 	%r515|%p65, %r513, %r480, %r472, %r474;
	mov.b32 	%f133, %r515;
	add.f32 	%f134, %f132, %f133;
	mov.b32 	%r516, %f134;
	shfl.sync.bfly.b32 	%r518|%p66, %r516, %r483, %r472, %r474;
	mov.b32 	%f135, %r518;
	add.f32 	%f136, %f134, %f135;
	mov.b32 	%r519, %f136;
	shfl.sync.bfly.b32 	%r521|%p67, %r519, %r486, %r472, %r474;
	mov.b32 	%f137, %r521;
	add.f32 	%f138, %f136, %f137;
	st.local.f32 	[%rd3+20], %f138;
	st.shared.f32 	[%r57], %f138;
	bar.sync 	0;
	@%p1 bra 	$L__BB78_21;

	ld.shared.f32 	%f139, [%r4];
	mov.b32 	%r522, %f139;
	mov.u32 	%r523, 31;
	mov.u32 	%r524, 16;
	mov.u32 	%r525, -1;
	shfl.sync.bfly.b32 	%r526|%p68, %r522, %r524, %r523, %r525;
	mov.b32 	%f140, %r526;
	add.f32 	%f141, %f139, %f140;
	mov.b32 	%r527, %f141;
	mov.u32 	%r528, 8;
	shfl.sync.bfly.b32 	%r529|%p69, %r527, %r528, %r523, %r525;
	mov.b32 	%f142, %r529;
	add.f32 	%f143, %f141, %f142;
	mov.b32 	%r530, %f143;
	mov.u32 	%r531, 4;
	shfl.sync.bfly.b32 	%r532|%p70, %r530, %r531, %r523, %r525;
	mov.b32 	%f144, %r532;
	add.f32 	%f145, %f143, %f144;
	mov.b32 	%r533, %f145;
	mov.u32 	%r534, 2;
	shfl.sync.bfly.b32 	%r535|%p71, %r533, %r534, %r523, %r525;
	mov.b32 	%f146, %r535;
	add.f32 	%f147, %f145, %f146;
	mov.b32 	%r536, %f147;
	mov.u32 	%r537, 1;
	shfl.sync.bfly.b32 	%r538|%p72, %r536, %r537, %r523, %r525;
	mov.b32 	%f148, %r538;
	add.f32 	%f149, %f147, %f148;
	st.local.f32 	[%rd3+20], %f149;

$L__BB78_21:
	bar.sync 	0;
	mov.b32 	%r539, %f6;
	mov.u32 	%r540, 31;
	mov.u32 	%r541, 16;
	mov.u32 	%r542, -1;
	shfl.sync.bfly.b32 	%r543|%p74, %r539, %r541, %r540, %r542;
	mov.b32 	%f150, %r543;
	add.f32 	%f151, %f6, %f150;
	mov.b32 	%r544, %f151;
	mov.u32 	%r545, 8;
	shfl.sync.bfly.b32 	%r546|%p75, %r544, %r545, %r540, %r542;
	mov.b32 	%f152, %r546;
	add.f32 	%f153, %f151, %f152;
	mov.b32 	%r547, %f153;
	mov.u32 	%r548, 4;
	shfl.sync.bfly.b32 	%r549|%p76, %r547, %r548, %r540, %r542;
	mov.b32 	%f154, %r549;
	add.f32 	%f155, %f153, %f154;
	mov.b32 	%r550, %f155;
	mov.u32 	%r551, 2;
	shfl.sync.bfly.b32 	%r552|%p77, %r550, %r551, %r540, %r542;
	mov.b32 	%f156, %r552;
	add.f32 	%f157, %f155, %f156;
	mov.b32 	%r553, %f157;
	mov.u32 	%r554, 1;
	shfl.sync.bfly.b32 	%r555|%p78, %r553, %r554, %r540, %r542;
	mov.b32 	%f158, %r555;
	add.f32 	%f159, %f157, %f158;
	st.local.f32 	[%rd3+24], %f159;
	st.shared.f32 	[%r57], %f159;
	bar.sync 	0;
	@%p1 bra 	$L__BB78_23;

	ld.shared.f32 	%f160, [%r4];
	mov.b32 	%r556, %f160;
	shfl.sync.bfly.b32 	%r560|%p79, %r556, %r541, %r540, %r542;
	mov.b32 	%f161, %r560;
	add.f32 	%f162, %f160, %f161;
	mov.b32 	%r561, %f162;
	shfl.sync.bfly.b32 	%r563|%p80, %r561, %r545, %r540, %r542;
	mov.b32 	%f163, %r563;
	add.f32 	%f164, %f162, %f163;
	mov.b32 	%r564, %f164;
	shfl.sync.bfly.b32 	%r566|%p81, %r564, %r548, %r540, %r542;
	mov.b32 	%f165, %r566;
	add.f32 	%f166, %f164, %f165;
	mov.b32 	%r567, %f166;
	shfl.sync.bfly.b32 	%r569|%p82, %r567, %r551, %r540, %r542;
	mov.b32 	%f167, %r569;
	add.f32 	%f168, %f166, %f167;
	mov.b32 	%r570, %f168;
	shfl.sync.bfly.b32 	%r572|%p83, %r570, %r554, %r540, %r542;
	mov.b32 	%f169, %r572;
	add.f32 	%f170, %f168, %f169;
	st.local.f32 	[%rd3+24], %f170;

$L__BB78_23:
	bar.sync 	0;
	setp.gt.s32 	%p84, %r3, 6;
	@%p84 bra 	$L__BB78_25;

	mad.lo.s32 	%r573, %r3, %r60, %r2;
	cvt.s64.s32 	%rd69, %r573;
	mul.lo.s32 	%r574, %r1, %r61;
	cvt.s64.s32 	%rd70, %r574;
	add.s64 	%rd71, %rd70, %rd69;
	mul.wide.s32 	%rd72, %r3, 4;
	add.s64 	%rd73, %rd3, %rd72;
	ld.local.f32 	%f171, [%rd73];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs3, %f171;}

	// end inline asm
	cvta.to.global.u64 	%rd74, %rd29;
	shl.b64 	%rd75, %rd71, 1;
	add.s64 	%rd76, %rd74, %rd75;
	st.global.u16 	[%rd76], %rs3;

$L__BB78_25:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_8_bs_64
.visible .entry ggml_matvec_f16_ncols_8_bs_64(
	.param .u64 ggml_matvec_f16_ncols_8_bs_64_param_0,
	.param .u64 ggml_matvec_f16_ncols_8_bs_64_param_1,
	.param .u64 ggml_matvec_f16_ncols_8_bs_64_param_2,
	.param .u32 ggml_matvec_f16_ncols_8_bs_64_param_3,
	.param .u32 ggml_matvec_f16_ncols_8_bs_64_param_4,
	.param .u32 ggml_matvec_f16_ncols_8_bs_64_param_5,
	.param .u32 ggml_matvec_f16_ncols_8_bs_64_param_6,
	.param .u32 ggml_matvec_f16_ncols_8_bs_64_param_7,
	.param .u32 ggml_matvec_f16_ncols_8_bs_64_param_8,
	.param .u32 ggml_matvec_f16_ncols_8_bs_64_param_9,
	.param .u32 ggml_matvec_f16_ncols_8_bs_64_param_10,
	.param .u32 ggml_matvec_f16_ncols_8_bs_64_param_11
)
{
	.local .align 16 .b8 	__local_depot79[32];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<96>;
	.reg .b16 	%rs<4>;
	.reg .f32 	%f<196>;
	.reg .b32 	%r<687>;
	.reg .b64 	%rd<85>;


	mov.u64 	%SPL, __local_depot79;
	ld.param.u64 	%rd31, [ggml_matvec_f16_ncols_8_bs_64_param_0];
	ld.param.u64 	%rd32, [ggml_matvec_f16_ncols_8_bs_64_param_1];
	ld.param.u64 	%rd30, [ggml_matvec_f16_ncols_8_bs_64_param_2];
	ld.param.u32 	%r64, [ggml_matvec_f16_ncols_8_bs_64_param_3];
	ld.param.u32 	%r68, [ggml_matvec_f16_ncols_8_bs_64_param_5];
	ld.param.u32 	%r65, [ggml_matvec_f16_ncols_8_bs_64_param_6];
	ld.param.u32 	%r66, [ggml_matvec_f16_ncols_8_bs_64_param_7];
	ld.param.u32 	%r69, [ggml_matvec_f16_ncols_8_bs_64_param_8];
	ld.param.u32 	%r70, [ggml_matvec_f16_ncols_8_bs_64_param_9];
	ld.param.u32 	%r71, [ggml_matvec_f16_ncols_8_bs_64_param_10];
	ld.param.u32 	%r67, [ggml_matvec_f16_ncols_8_bs_64_param_11];
	cvta.to.global.u64 	%rd84, %rd32;
	cvta.to.global.u64 	%rd2, %rd31;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r72, %r1, %r69;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r73, %r2, %r68;
	mad.lo.s32 	%r74, %r72, %r70, %r73;
	cvt.s64.s32 	%rd4, %r74;
	mul.lo.s32 	%r75, %r1, %r71;
	cvt.s64.s32 	%rd5, %r75;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r76, %r3, 2;
	mov.u32 	%r77, data_mmv;
	add.s32 	%r4, %r77, %r76;
	@%p1 bra 	$L__BB79_2;

	mov.u32 	%r78, 0;
	st.shared.u32 	[%r4], %r78;

$L__BB79_2:
	bar.sync 	0;
	mov.f32 	%f9, 0f00000000;
	st.local.v4.f32 	[%rd3], {%f9, %f9, %f9, %f9};
	st.local.v4.f32 	[%rd3+16], {%f9, %f9, %f9, %f9};
	mov.u32 	%r679, 0;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs2, %f9;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f9;}

	// end inline asm
	mov.b32 	%r686, {%rs1, %rs2};
	setp.ge.s32 	%p2, %r3, %r64;
	mov.u32 	%r680, %r679;
	mov.u32 	%r681, %r679;
	mov.u32 	%r682, %r679;
	mov.u32 	%r683, %r679;
	mov.u32 	%r684, %r679;
	mov.u32 	%r685, %r679;
	@%p2 bra 	$L__BB79_9;

	not.b32 	%r93, %r3;
	add.s32 	%r6, %r93, %r64;
	shr.u32 	%r94, %r6, 6;
	add.s32 	%r95, %r94, 1;
	and.b32  	%r660, %r95, 3;
	setp.eq.s32 	%p3, %r660, 0;
	mov.u32 	%r679, 0;
	mov.u32 	%r669, %r3;
	@%p3 bra 	$L__BB79_6;

	shl.b32 	%r103, %r65, 1;
	add.s32 	%r104, %r3, %r103;
	mul.wide.s32 	%rd34, %r104, 4;
	shl.b64 	%rd35, %rd5, 1;
	add.s64 	%rd7, %rd34, %rd35;
	mul.wide.s32 	%rd36, %r3, 4;
	mul.wide.s32 	%rd8, %r65, 4;
	add.s64 	%rd37, %rd36, %rd8;
	add.s64 	%rd9, %rd37, %rd35;
	add.s64 	%rd10, %rd36, %rd35;
	mul.wide.s32 	%rd38, %r3, 2;
	add.s64 	%rd39, %rd38, %rd4;
	shl.b64 	%rd40, %rd39, 1;
	add.s64 	%rd81, %rd2, %rd40;
	mov.u32 	%r679, 0;
	mov.u64 	%rd82, %rd84;
	mov.u32 	%r680, %r679;
	mov.u32 	%r681, %r679;
	mov.u32 	%r682, %r679;
	mov.u32 	%r683, %r679;
	mov.u32 	%r684, %r679;
	mov.u32 	%r685, %r679;
	mov.u32 	%r669, %r3;

$L__BB79_5:
	.pragma "nounroll";
	ld.global.nc.u32 	%r106, [%rd81];
	add.s64 	%rd41, %rd82, %rd10;
	ld.global.nc.u32 	%r107, [%rd41];
	// begin inline asm
	{mul.f16x2 %r105,%r106,%r107;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r686,%r686,%r105;
}
	// end inline asm
	add.s64 	%rd42, %rd82, %rd9;
	ld.global.nc.u32 	%r113, [%rd42];
	// begin inline asm
	{mul.f16x2 %r111,%r106,%r113;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r685,%r685,%r111;
}
	// end inline asm
	add.s64 	%rd43, %rd82, %rd7;
	ld.global.nc.u32 	%r119, [%rd43];
	// begin inline asm
	{mul.f16x2 %r117,%r106,%r119;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r684,%r684,%r117;
}
	// end inline asm
	add.s64 	%rd44, %rd43, %rd8;
	ld.global.nc.u32 	%r125, [%rd44];
	// begin inline asm
	{mul.f16x2 %r123,%r106,%r125;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r683,%r683,%r123;
}
	// end inline asm
	add.s64 	%rd45, %rd44, %rd8;
	ld.global.nc.u32 	%r131, [%rd45];
	// begin inline asm
	{mul.f16x2 %r129,%r106,%r131;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r682,%r682,%r129;
}
	// end inline asm
	add.s64 	%rd46, %rd45, %rd8;
	ld.global.nc.u32 	%r137, [%rd46];
	// begin inline asm
	{mul.f16x2 %r135,%r106,%r137;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r681,%r681,%r135;
}
	// end inline asm
	add.s64 	%rd47, %rd46, %rd8;
	ld.global.nc.u32 	%r143, [%rd47];
	// begin inline asm
	{mul.f16x2 %r141,%r106,%r143;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r680,%r680,%r141;
}
	// end inline asm
	add.s64 	%rd48, %rd47, %rd8;
	ld.global.nc.u32 	%r149, [%rd48];
	// begin inline asm
	{mul.f16x2 %r147,%r106,%r149;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r679,%r679,%r147;
}
	// end inline asm
	add.s32 	%r669, %r669, 64;
	add.s64 	%rd82, %rd82, 256;
	add.s64 	%rd81, %rd81, 256;
	add.s32 	%r660, %r660, -1;
	setp.ne.s32 	%p4, %r660, 0;
	@%p4 bra 	$L__BB79_5;

$L__BB79_6:
	setp.lt.u32 	%p5, %r6, 192;
	@%p5 bra 	$L__BB79_9;

	add.s32 	%r153, %r669, %r65;
	shl.b32 	%r154, %r65, 1;
	add.s32 	%r155, %r669, %r154;
	mad.lo.s32 	%r156, %r65, 3, %r669;
	shl.b32 	%r157, %r65, 2;
	add.s32 	%r158, %r669, %r157;
	mad.lo.s32 	%r159, %r65, 5, %r669;
	mad.lo.s32 	%r160, %r65, 6, %r669;
	mad.lo.s32 	%r161, %r65, 7, %r669;
	add.s32 	%r162, %r153, 64;
	mul.wide.s32 	%rd49, %r162, 4;
	shl.b64 	%rd50, %rd5, 1;
	add.s64 	%rd16, %rd49, %rd50;
	mul.wide.s32 	%rd51, %r155, 4;
	add.s64 	%rd17, %rd51, %rd50;
	mul.wide.s32 	%rd52, %r156, 4;
	add.s64 	%rd18, %rd52, %rd50;
	mul.wide.s32 	%rd53, %r158, 4;
	add.s64 	%rd19, %rd53, %rd50;
	mul.wide.s32 	%rd54, %r159, 4;
	add.s64 	%rd20, %rd54, %rd50;
	mul.wide.s32 	%rd55, %r160, 4;
	add.s64 	%rd21, %rd55, %rd50;
	mul.wide.s32 	%rd56, %r161, 4;
	add.s64 	%rd22, %rd56, %rd50;
	mul.wide.s32 	%rd57, %r669, 2;
	add.s64 	%rd58, %rd57, %rd4;
	shl.b64 	%rd59, %rd58, 1;
	add.s64 	%rd60, %rd2, %rd59;
	add.s64 	%rd83, %rd60, 512;
	mul.wide.s32 	%rd61, %r669, 4;
	add.s64 	%rd24, %rd61, %rd50;
	mul.wide.s32 	%rd62, %r65, 4;
	add.s64 	%rd63, %rd61, %rd62;
	add.s64 	%rd25, %rd63, %rd50;

$L__BB79_8:
	ld.global.nc.u32 	%r164, [%rd83+-512];
	add.s64 	%rd64, %rd84, %rd24;
	ld.global.nc.u32 	%r165, [%rd64];
	// begin inline asm
	{mul.f16x2 %r163,%r164,%r165;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r166,%r686,%r163;
}
	// end inline asm
	add.s64 	%rd65, %rd84, %rd25;
	ld.global.nc.u32 	%r171, [%rd65];
	// begin inline asm
	{mul.f16x2 %r169,%r164,%r171;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r172,%r685,%r169;
}
	// end inline asm
	add.s64 	%rd66, %rd84, %rd17;
	ld.global.nc.u32 	%r177, [%rd66];
	// begin inline asm
	{mul.f16x2 %r175,%r164,%r177;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r178,%r684,%r175;
}
	// end inline asm
	add.s64 	%rd67, %rd84, %rd18;
	ld.global.nc.u32 	%r183, [%rd67];
	// begin inline asm
	{mul.f16x2 %r181,%r164,%r183;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r184,%r683,%r181;
}
	// end inline asm
	add.s64 	%rd68, %rd84, %rd19;
	ld.global.nc.u32 	%r189, [%rd68];
	// begin inline asm
	{mul.f16x2 %r187,%r164,%r189;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r190,%r682,%r187;
}
	// end inline asm
	add.s64 	%rd69, %rd84, %rd20;
	ld.global.nc.u32 	%r195, [%rd69];
	// begin inline asm
	{mul.f16x2 %r193,%r164,%r195;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r196,%r681,%r193;
}
	// end inline asm
	add.s64 	%rd70, %rd84, %rd21;
	ld.global.nc.u32 	%r201, [%rd70];
	// begin inline asm
	{mul.f16x2 %r199,%r164,%r201;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r202,%r680,%r199;
}
	// end inline asm
	add.s64 	%rd71, %rd84, %rd22;
	ld.global.nc.u32 	%r207, [%rd71];
	// begin inline asm
	{mul.f16x2 %r205,%r164,%r207;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r208,%r679,%r205;
}
	// end inline asm
	ld.global.nc.u32 	%r212, [%rd83+-256];
	ld.global.nc.u32 	%r213, [%rd64+256];
	// begin inline asm
	{mul.f16x2 %r211,%r212,%r213;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r214,%r166,%r211;
}
	// end inline asm
	add.s64 	%rd72, %rd84, %rd16;
	ld.global.nc.u32 	%r219, [%rd72];
	// begin inline asm
	{mul.f16x2 %r217,%r212,%r219;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r220,%r172,%r217;
}
	// end inline asm
	ld.global.nc.u32 	%r225, [%rd66+256];
	// begin inline asm
	{mul.f16x2 %r223,%r212,%r225;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r226,%r178,%r223;
}
	// end inline asm
	ld.global.nc.u32 	%r231, [%rd67+256];
	// begin inline asm
	{mul.f16x2 %r229,%r212,%r231;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r232,%r184,%r229;
}
	// end inline asm
	ld.global.nc.u32 	%r237, [%rd68+256];
	// begin inline asm
	{mul.f16x2 %r235,%r212,%r237;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r238,%r190,%r235;
}
	// end inline asm
	ld.global.nc.u32 	%r243, [%rd69+256];
	// begin inline asm
	{mul.f16x2 %r241,%r212,%r243;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r244,%r196,%r241;
}
	// end inline asm
	ld.global.nc.u32 	%r249, [%rd70+256];
	// begin inline asm
	{mul.f16x2 %r247,%r212,%r249;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r250,%r202,%r247;
}
	// end inline asm
	ld.global.nc.u32 	%r255, [%rd71+256];
	// begin inline asm
	{mul.f16x2 %r253,%r212,%r255;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r256,%r208,%r253;
}
	// end inline asm
	ld.global.nc.u32 	%r260, [%rd83];
	ld.global.nc.u32 	%r261, [%rd64+512];
	// begin inline asm
	{mul.f16x2 %r259,%r260,%r261;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r262,%r214,%r259;
}
	// end inline asm
	ld.global.nc.u32 	%r267, [%rd72+256];
	// begin inline asm
	{mul.f16x2 %r265,%r260,%r267;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r268,%r220,%r265;
}
	// end inline asm
	ld.global.nc.u32 	%r273, [%rd66+512];
	// begin inline asm
	{mul.f16x2 %r271,%r260,%r273;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r274,%r226,%r271;
}
	// end inline asm
	ld.global.nc.u32 	%r279, [%rd67+512];
	// begin inline asm
	{mul.f16x2 %r277,%r260,%r279;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r280,%r232,%r277;
}
	// end inline asm
	ld.global.nc.u32 	%r285, [%rd68+512];
	// begin inline asm
	{mul.f16x2 %r283,%r260,%r285;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r286,%r238,%r283;
}
	// end inline asm
	ld.global.nc.u32 	%r291, [%rd69+512];
	// begin inline asm
	{mul.f16x2 %r289,%r260,%r291;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r292,%r244,%r289;
}
	// end inline asm
	ld.global.nc.u32 	%r297, [%rd70+512];
	// begin inline asm
	{mul.f16x2 %r295,%r260,%r297;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r298,%r250,%r295;
}
	// end inline asm
	ld.global.nc.u32 	%r303, [%rd71+512];
	// begin inline asm
	{mul.f16x2 %r301,%r260,%r303;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r304,%r256,%r301;
}
	// end inline asm
	ld.global.nc.u32 	%r308, [%rd83+256];
	ld.global.nc.u32 	%r309, [%rd64+768];
	// begin inline asm
	{mul.f16x2 %r307,%r308,%r309;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r686,%r262,%r307;
}
	// end inline asm
	ld.global.nc.u32 	%r315, [%rd72+512];
	// begin inline asm
	{mul.f16x2 %r313,%r308,%r315;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r685,%r268,%r313;
}
	// end inline asm
	ld.global.nc.u32 	%r321, [%rd66+768];
	// begin inline asm
	{mul.f16x2 %r319,%r308,%r321;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r684,%r274,%r319;
}
	// end inline asm
	ld.global.nc.u32 	%r327, [%rd67+768];
	// begin inline asm
	{mul.f16x2 %r325,%r308,%r327;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r683,%r280,%r325;
}
	// end inline asm
	ld.global.nc.u32 	%r333, [%rd68+768];
	// begin inline asm
	{mul.f16x2 %r331,%r308,%r333;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r682,%r286,%r331;
}
	// end inline asm
	ld.global.nc.u32 	%r339, [%rd69+768];
	// begin inline asm
	{mul.f16x2 %r337,%r308,%r339;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r681,%r292,%r337;
}
	// end inline asm
	ld.global.nc.u32 	%r345, [%rd70+768];
	// begin inline asm
	{mul.f16x2 %r343,%r308,%r345;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r680,%r298,%r343;
}
	// end inline asm
	ld.global.nc.u32 	%r351, [%rd71+768];
	// begin inline asm
	{mul.f16x2 %r349,%r308,%r351;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r679,%r304,%r349;
}
	// end inline asm
	add.s64 	%rd84, %rd84, 1024;
	add.s64 	%rd83, %rd83, 1024;
	add.s32 	%r669, %r669, 256;
	setp.lt.s32 	%p6, %r669, %r64;
	@%p6 bra 	$L__BB79_8;

$L__BB79_9:
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r686;
  cvt.f32.f16 %f10, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r686;
  cvt.f32.f16 %f11, high;}

	// end inline asm
	add.f32 	%f26, %f10, %f11;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r685;
  cvt.f32.f16 %f12, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r685;
  cvt.f32.f16 %f13, high;}

	// end inline asm
	add.f32 	%f1, %f12, %f13;
	st.local.f32 	[%rd3+4], %f1;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r684;
  cvt.f32.f16 %f14, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r684;
  cvt.f32.f16 %f15, high;}

	// end inline asm
	add.f32 	%f2, %f14, %f15;
	st.local.f32 	[%rd3+8], %f2;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r683;
  cvt.f32.f16 %f16, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r683;
  cvt.f32.f16 %f17, high;}

	// end inline asm
	add.f32 	%f3, %f16, %f17;
	st.local.f32 	[%rd3+12], %f3;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r682;
  cvt.f32.f16 %f18, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r682;
  cvt.f32.f16 %f19, high;}

	// end inline asm
	add.f32 	%f4, %f18, %f19;
	st.local.f32 	[%rd3+16], %f4;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r681;
  cvt.f32.f16 %f20, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r681;
  cvt.f32.f16 %f21, high;}

	// end inline asm
	add.f32 	%f5, %f20, %f21;
	st.local.f32 	[%rd3+20], %f5;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r680;
  cvt.f32.f16 %f22, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r680;
  cvt.f32.f16 %f23, high;}

	// end inline asm
	add.f32 	%f6, %f22, %f23;
	st.local.f32 	[%rd3+24], %f6;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r679;
  cvt.f32.f16 %f24, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r679;
  cvt.f32.f16 %f25, high;}

	// end inline asm
	add.f32 	%f7, %f24, %f25;
	st.local.f32 	[%rd3+28], %f7;
	shr.s32 	%r371, %r3, 31;
	shr.u32 	%r372, %r371, 27;
	add.s32 	%r373, %r3, %r372;
	shr.s32 	%r374, %r373, 5;
	shl.b32 	%r375, %r374, 2;
	add.s32 	%r63, %r77, %r375;
	mov.u32 	%r377, 2;
	mov.b32 	%r378, %f26;
	mov.u32 	%r379, 31;
	mov.u32 	%r380, 16;
	mov.u32 	%r381, -1;
	shfl.sync.bfly.b32 	%r382|%p7, %r378, %r380, %r379, %r381;
	mov.b32 	%f27, %r382;
	add.f32 	%f28, %f26, %f27;
	mov.b32 	%r383, %f28;
	mov.u32 	%r384, 8;
	shfl.sync.bfly.b32 	%r385|%p8, %r383, %r384, %r379, %r381;
	mov.b32 	%f29, %r385;
	add.f32 	%f30, %f28, %f29;
	mov.b32 	%r386, %f30;
	mov.u32 	%r387, 4;
	shfl.sync.bfly.b32 	%r388|%p9, %r386, %r387, %r379, %r381;
	mov.b32 	%f31, %r388;
	add.f32 	%f32, %f30, %f31;
	mov.b32 	%r389, %f32;
	shfl.sync.bfly.b32 	%r390|%p10, %r389, %r377, %r379, %r381;
	mov.b32 	%f33, %r390;
	add.f32 	%f34, %f32, %f33;
	mov.b32 	%r391, %f34;
	mov.u32 	%r392, 1;
	shfl.sync.bfly.b32 	%r393|%p11, %r391, %r392, %r379, %r381;
	mov.b32 	%f35, %r393;
	add.f32 	%f36, %f34, %f35;
	st.local.f32 	[%rd3], %f36;
	st.shared.f32 	[%r63], %f36;
	bar.sync 	0;
	@%p1 bra 	$L__BB79_11;

	ld.shared.f32 	%f37, [%r4];
	mov.b32 	%r394, %f37;
	shfl.sync.bfly.b32 	%r398|%p13, %r394, %r380, %r379, %r381;
	mov.b32 	%f38, %r398;
	add.f32 	%f39, %f37, %f38;
	mov.b32 	%r399, %f39;
	shfl.sync.bfly.b32 	%r401|%p14, %r399, %r384, %r379, %r381;
	mov.b32 	%f40, %r401;
	add.f32 	%f41, %f39, %f40;
	mov.b32 	%r402, %f41;
	shfl.sync.bfly.b32 	%r404|%p15, %r402, %r387, %r379, %r381;
	mov.b32 	%f42, %r404;
	add.f32 	%f43, %f41, %f42;
	mov.b32 	%r405, %f43;
	shfl.sync.bfly.b32 	%r407|%p16, %r405, %r377, %r379, %r381;
	mov.b32 	%f44, %r407;
	add.f32 	%f45, %f43, %f44;
	mov.b32 	%r408, %f45;
	shfl.sync.bfly.b32 	%r410|%p17, %r408, %r392, %r379, %r381;
	mov.b32 	%f46, %r410;
	add.f32 	%f47, %f45, %f46;
	st.local.f32 	[%rd3], %f47;

$L__BB79_11:
	bar.sync 	0;
	mov.b32 	%r411, %f1;
	shfl.sync.bfly.b32 	%r415|%p19, %r411, %r380, %r379, %r381;
	mov.b32 	%f48, %r415;
	add.f32 	%f49, %f1, %f48;
	mov.b32 	%r416, %f49;
	shfl.sync.bfly.b32 	%r418|%p20, %r416, %r384, %r379, %r381;
	mov.b32 	%f50, %r418;
	add.f32 	%f51, %f49, %f50;
	mov.b32 	%r419, %f51;
	shfl.sync.bfly.b32 	%r421|%p21, %r419, %r387, %r379, %r381;
	mov.b32 	%f52, %r421;
	add.f32 	%f53, %f51, %f52;
	mov.b32 	%r422, %f53;
	shfl.sync.bfly.b32 	%r424|%p22, %r422, %r377, %r379, %r381;
	mov.b32 	%f54, %r424;
	add.f32 	%f55, %f53, %f54;
	mov.b32 	%r425, %f55;
	shfl.sync.bfly.b32 	%r427|%p23, %r425, %r392, %r379, %r381;
	mov.b32 	%f56, %r427;
	add.f32 	%f57, %f55, %f56;
	st.local.f32 	[%rd3+4], %f57;
	st.shared.f32 	[%r63], %f57;
	bar.sync 	0;
	@%p1 bra 	$L__BB79_13;

	ld.shared.f32 	%f58, [%r4];
	mov.b32 	%r428, %f58;
	mov.u32 	%r429, 31;
	mov.u32 	%r430, 16;
	mov.u32 	%r431, -1;
	shfl.sync.bfly.b32 	%r432|%p24, %r428, %r430, %r429, %r431;
	mov.b32 	%f59, %r432;
	add.f32 	%f60, %f58, %f59;
	mov.b32 	%r433, %f60;
	mov.u32 	%r434, 8;
	shfl.sync.bfly.b32 	%r435|%p25, %r433, %r434, %r429, %r431;
	mov.b32 	%f61, %r435;
	add.f32 	%f62, %f60, %f61;
	mov.b32 	%r436, %f62;
	mov.u32 	%r437, 4;
	shfl.sync.bfly.b32 	%r438|%p26, %r436, %r437, %r429, %r431;
	mov.b32 	%f63, %r438;
	add.f32 	%f64, %f62, %f63;
	mov.b32 	%r439, %f64;
	mov.u32 	%r440, 2;
	shfl.sync.bfly.b32 	%r441|%p27, %r439, %r440, %r429, %r431;
	mov.b32 	%f65, %r441;
	add.f32 	%f66, %f64, %f65;
	mov.b32 	%r442, %f66;
	mov.u32 	%r443, 1;
	shfl.sync.bfly.b32 	%r444|%p28, %r442, %r443, %r429, %r431;
	mov.b32 	%f67, %r444;
	add.f32 	%f68, %f66, %f67;
	st.local.f32 	[%rd3+4], %f68;

$L__BB79_13:
	bar.sync 	0;
	mov.b32 	%r445, %f2;
	mov.u32 	%r446, 31;
	mov.u32 	%r447, 16;
	mov.u32 	%r448, -1;
	shfl.sync.bfly.b32 	%r449|%p30, %r445, %r447, %r446, %r448;
	mov.b32 	%f69, %r449;
	add.f32 	%f70, %f2, %f69;
	mov.b32 	%r450, %f70;
	mov.u32 	%r451, 8;
	shfl.sync.bfly.b32 	%r452|%p31, %r450, %r451, %r446, %r448;
	mov.b32 	%f71, %r452;
	add.f32 	%f72, %f70, %f71;
	mov.b32 	%r453, %f72;
	mov.u32 	%r454, 4;
	shfl.sync.bfly.b32 	%r455|%p32, %r453, %r454, %r446, %r448;
	mov.b32 	%f73, %r455;
	add.f32 	%f74, %f72, %f73;
	mov.b32 	%r456, %f74;
	mov.u32 	%r457, 2;
	shfl.sync.bfly.b32 	%r458|%p33, %r456, %r457, %r446, %r448;
	mov.b32 	%f75, %r458;
	add.f32 	%f76, %f74, %f75;
	mov.b32 	%r459, %f76;
	mov.u32 	%r460, 1;
	shfl.sync.bfly.b32 	%r461|%p34, %r459, %r460, %r446, %r448;
	mov.b32 	%f77, %r461;
	add.f32 	%f78, %f76, %f77;
	st.local.f32 	[%rd3+8], %f78;
	st.shared.f32 	[%r63], %f78;
	bar.sync 	0;
	@%p1 bra 	$L__BB79_15;

	ld.shared.f32 	%f79, [%r4];
	mov.b32 	%r462, %f79;
	shfl.sync.bfly.b32 	%r466|%p35, %r462, %r447, %r446, %r448;
	mov.b32 	%f80, %r466;
	add.f32 	%f81, %f79, %f80;
	mov.b32 	%r467, %f81;
	shfl.sync.bfly.b32 	%r469|%p36, %r467, %r451, %r446, %r448;
	mov.b32 	%f82, %r469;
	add.f32 	%f83, %f81, %f82;
	mov.b32 	%r470, %f83;
	shfl.sync.bfly.b32 	%r472|%p37, %r470, %r454, %r446, %r448;
	mov.b32 	%f84, %r472;
	add.f32 	%f85, %f83, %f84;
	mov.b32 	%r473, %f85;
	shfl.sync.bfly.b32 	%r475|%p38, %r473, %r457, %r446, %r448;
	mov.b32 	%f86, %r475;
	add.f32 	%f87, %f85, %f86;
	mov.b32 	%r476, %f87;
	shfl.sync.bfly.b32 	%r478|%p39, %r476, %r460, %r446, %r448;
	mov.b32 	%f88, %r478;
	add.f32 	%f89, %f87, %f88;
	st.local.f32 	[%rd3+8], %f89;

$L__BB79_15:
	bar.sync 	0;
	mov.b32 	%r479, %f3;
	shfl.sync.bfly.b32 	%r483|%p41, %r479, %r447, %r446, %r448;
	mov.b32 	%f90, %r483;
	add.f32 	%f91, %f3, %f90;
	mov.b32 	%r484, %f91;
	shfl.sync.bfly.b32 	%r486|%p42, %r484, %r451, %r446, %r448;
	mov.b32 	%f92, %r486;
	add.f32 	%f93, %f91, %f92;
	mov.b32 	%r487, %f93;
	shfl.sync.bfly.b32 	%r489|%p43, %r487, %r454, %r446, %r448;
	mov.b32 	%f94, %r489;
	add.f32 	%f95, %f93, %f94;
	mov.b32 	%r490, %f95;
	shfl.sync.bfly.b32 	%r492|%p44, %r490, %r457, %r446, %r448;
	mov.b32 	%f96, %r492;
	add.f32 	%f97, %f95, %f96;
	mov.b32 	%r493, %f97;
	shfl.sync.bfly.b32 	%r495|%p45, %r493, %r460, %r446, %r448;
	mov.b32 	%f98, %r495;
	add.f32 	%f99, %f97, %f98;
	st.local.f32 	[%rd3+12], %f99;
	st.shared.f32 	[%r63], %f99;
	bar.sync 	0;
	@%p1 bra 	$L__BB79_17;

	ld.shared.f32 	%f100, [%r4];
	mov.b32 	%r496, %f100;
	mov.u32 	%r497, 31;
	mov.u32 	%r498, 16;
	mov.u32 	%r499, -1;
	shfl.sync.bfly.b32 	%r500|%p46, %r496, %r498, %r497, %r499;
	mov.b32 	%f101, %r500;
	add.f32 	%f102, %f100, %f101;
	mov.b32 	%r501, %f102;
	mov.u32 	%r502, 8;
	shfl.sync.bfly.b32 	%r503|%p47, %r501, %r502, %r497, %r499;
	mov.b32 	%f103, %r503;
	add.f32 	%f104, %f102, %f103;
	mov.b32 	%r504, %f104;
	mov.u32 	%r505, 4;
	shfl.sync.bfly.b32 	%r506|%p48, %r504, %r505, %r497, %r499;
	mov.b32 	%f105, %r506;
	add.f32 	%f106, %f104, %f105;
	mov.b32 	%r507, %f106;
	mov.u32 	%r508, 2;
	shfl.sync.bfly.b32 	%r509|%p49, %r507, %r508, %r497, %r499;
	mov.b32 	%f107, %r509;
	add.f32 	%f108, %f106, %f107;
	mov.b32 	%r510, %f108;
	mov.u32 	%r511, 1;
	shfl.sync.bfly.b32 	%r512|%p50, %r510, %r511, %r497, %r499;
	mov.b32 	%f109, %r512;
	add.f32 	%f110, %f108, %f109;
	st.local.f32 	[%rd3+12], %f110;

$L__BB79_17:
	bar.sync 	0;
	mov.b32 	%r513, %f4;
	mov.u32 	%r514, 31;
	mov.u32 	%r515, 16;
	mov.u32 	%r516, -1;
	shfl.sync.bfly.b32 	%r517|%p52, %r513, %r515, %r514, %r516;
	mov.b32 	%f111, %r517;
	add.f32 	%f112, %f4, %f111;
	mov.b32 	%r518, %f112;
	mov.u32 	%r519, 8;
	shfl.sync.bfly.b32 	%r520|%p53, %r518, %r519, %r514, %r516;
	mov.b32 	%f113, %r520;
	add.f32 	%f114, %f112, %f113;
	mov.b32 	%r521, %f114;
	mov.u32 	%r522, 4;
	shfl.sync.bfly.b32 	%r523|%p54, %r521, %r522, %r514, %r516;
	mov.b32 	%f115, %r523;
	add.f32 	%f116, %f114, %f115;
	mov.b32 	%r524, %f116;
	mov.u32 	%r525, 2;
	shfl.sync.bfly.b32 	%r526|%p55, %r524, %r525, %r514, %r516;
	mov.b32 	%f117, %r526;
	add.f32 	%f118, %f116, %f117;
	mov.b32 	%r527, %f118;
	mov.u32 	%r528, 1;
	shfl.sync.bfly.b32 	%r529|%p56, %r527, %r528, %r514, %r516;
	mov.b32 	%f119, %r529;
	add.f32 	%f120, %f118, %f119;
	st.local.f32 	[%rd3+16], %f120;
	st.shared.f32 	[%r63], %f120;
	bar.sync 	0;
	@%p1 bra 	$L__BB79_19;

	ld.shared.f32 	%f121, [%r4];
	mov.b32 	%r530, %f121;
	shfl.sync.bfly.b32 	%r534|%p57, %r530, %r515, %r514, %r516;
	mov.b32 	%f122, %r534;
	add.f32 	%f123, %f121, %f122;
	mov.b32 	%r535, %f123;
	shfl.sync.bfly.b32 	%r537|%p58, %r535, %r519, %r514, %r516;
	mov.b32 	%f124, %r537;
	add.f32 	%f125, %f123, %f124;
	mov.b32 	%r538, %f125;
	shfl.sync.bfly.b32 	%r540|%p59, %r538, %r522, %r514, %r516;
	mov.b32 	%f126, %r540;
	add.f32 	%f127, %f125, %f126;
	mov.b32 	%r541, %f127;
	shfl.sync.bfly.b32 	%r543|%p60, %r541, %r525, %r514, %r516;
	mov.b32 	%f128, %r543;
	add.f32 	%f129, %f127, %f128;
	mov.b32 	%r544, %f129;
	shfl.sync.bfly.b32 	%r546|%p61, %r544, %r528, %r514, %r516;
	mov.b32 	%f130, %r546;
	add.f32 	%f131, %f129, %f130;
	st.local.f32 	[%rd3+16], %f131;

$L__BB79_19:
	bar.sync 	0;
	mov.b32 	%r547, %f5;
	shfl.sync.bfly.b32 	%r551|%p63, %r547, %r515, %r514, %r516;
	mov.b32 	%f132, %r551;
	add.f32 	%f133, %f5, %f132;
	mov.b32 	%r552, %f133;
	shfl.sync.bfly.b32 	%r554|%p64, %r552, %r519, %r514, %r516;
	mov.b32 	%f134, %r554;
	add.f32 	%f135, %f133, %f134;
	mov.b32 	%r555, %f135;
	shfl.sync.bfly.b32 	%r557|%p65, %r555, %r522, %r514, %r516;
	mov.b32 	%f136, %r557;
	add.f32 	%f137, %f135, %f136;
	mov.b32 	%r558, %f137;
	shfl.sync.bfly.b32 	%r560|%p66, %r558, %r525, %r514, %r516;
	mov.b32 	%f138, %r560;
	add.f32 	%f139, %f137, %f138;
	mov.b32 	%r561, %f139;
	shfl.sync.bfly.b32 	%r563|%p67, %r561, %r528, %r514, %r516;
	mov.b32 	%f140, %r563;
	add.f32 	%f141, %f139, %f140;
	st.local.f32 	[%rd3+20], %f141;
	st.shared.f32 	[%r63], %f141;
	bar.sync 	0;
	@%p1 bra 	$L__BB79_21;

	ld.shared.f32 	%f142, [%r4];
	mov.b32 	%r564, %f142;
	mov.u32 	%r565, 31;
	mov.u32 	%r566, 16;
	mov.u32 	%r567, -1;
	shfl.sync.bfly.b32 	%r568|%p68, %r564, %r566, %r565, %r567;
	mov.b32 	%f143, %r568;
	add.f32 	%f144, %f142, %f143;
	mov.b32 	%r569, %f144;
	mov.u32 	%r570, 8;
	shfl.sync.bfly.b32 	%r571|%p69, %r569, %r570, %r565, %r567;
	mov.b32 	%f145, %r571;
	add.f32 	%f146, %f144, %f145;
	mov.b32 	%r572, %f146;
	mov.u32 	%r573, 4;
	shfl.sync.bfly.b32 	%r574|%p70, %r572, %r573, %r565, %r567;
	mov.b32 	%f147, %r574;
	add.f32 	%f148, %f146, %f147;
	mov.b32 	%r575, %f148;
	mov.u32 	%r576, 2;
	shfl.sync.bfly.b32 	%r577|%p71, %r575, %r576, %r565, %r567;
	mov.b32 	%f149, %r577;
	add.f32 	%f150, %f148, %f149;
	mov.b32 	%r578, %f150;
	mov.u32 	%r579, 1;
	shfl.sync.bfly.b32 	%r580|%p72, %r578, %r579, %r565, %r567;
	mov.b32 	%f151, %r580;
	add.f32 	%f152, %f150, %f151;
	st.local.f32 	[%rd3+20], %f152;

$L__BB79_21:
	bar.sync 	0;
	mov.b32 	%r581, %f6;
	mov.u32 	%r582, 31;
	mov.u32 	%r583, 16;
	mov.u32 	%r584, -1;
	shfl.sync.bfly.b32 	%r585|%p74, %r581, %r583, %r582, %r584;
	mov.b32 	%f153, %r585;
	add.f32 	%f154, %f6, %f153;
	mov.b32 	%r586, %f154;
	mov.u32 	%r587, 8;
	shfl.sync.bfly.b32 	%r588|%p75, %r586, %r587, %r582, %r584;
	mov.b32 	%f155, %r588;
	add.f32 	%f156, %f154, %f155;
	mov.b32 	%r589, %f156;
	mov.u32 	%r590, 4;
	shfl.sync.bfly.b32 	%r591|%p76, %r589, %r590, %r582, %r584;
	mov.b32 	%f157, %r591;
	add.f32 	%f158, %f156, %f157;
	mov.b32 	%r592, %f158;
	mov.u32 	%r593, 2;
	shfl.sync.bfly.b32 	%r594|%p77, %r592, %r593, %r582, %r584;
	mov.b32 	%f159, %r594;
	add.f32 	%f160, %f158, %f159;
	mov.b32 	%r595, %f160;
	mov.u32 	%r596, 1;
	shfl.sync.bfly.b32 	%r597|%p78, %r595, %r596, %r582, %r584;
	mov.b32 	%f161, %r597;
	add.f32 	%f162, %f160, %f161;
	st.local.f32 	[%rd3+24], %f162;
	st.shared.f32 	[%r63], %f162;
	bar.sync 	0;
	@%p1 bra 	$L__BB79_23;

	ld.shared.f32 	%f163, [%r4];
	mov.b32 	%r598, %f163;
	shfl.sync.bfly.b32 	%r602|%p79, %r598, %r583, %r582, %r584;
	mov.b32 	%f164, %r602;
	add.f32 	%f165, %f163, %f164;
	mov.b32 	%r603, %f165;
	shfl.sync.bfly.b32 	%r605|%p80, %r603, %r587, %r582, %r584;
	mov.b32 	%f166, %r605;
	add.f32 	%f167, %f165, %f166;
	mov.b32 	%r606, %f167;
	shfl.sync.bfly.b32 	%r608|%p81, %r606, %r590, %r582, %r584;
	mov.b32 	%f168, %r608;
	add.f32 	%f169, %f167, %f168;
	mov.b32 	%r609, %f169;
	shfl.sync.bfly.b32 	%r611|%p82, %r609, %r593, %r582, %r584;
	mov.b32 	%f170, %r611;
	add.f32 	%f171, %f169, %f170;
	mov.b32 	%r612, %f171;
	shfl.sync.bfly.b32 	%r614|%p83, %r612, %r596, %r582, %r584;
	mov.b32 	%f172, %r614;
	add.f32 	%f173, %f171, %f172;
	st.local.f32 	[%rd3+24], %f173;

$L__BB79_23:
	bar.sync 	0;
	mov.b32 	%r615, %f7;
	shfl.sync.bfly.b32 	%r619|%p85, %r615, %r583, %r582, %r584;
	mov.b32 	%f174, %r619;
	add.f32 	%f175, %f7, %f174;
	mov.b32 	%r620, %f175;
	shfl.sync.bfly.b32 	%r622|%p86, %r620, %r587, %r582, %r584;
	mov.b32 	%f176, %r622;
	add.f32 	%f177, %f175, %f176;
	mov.b32 	%r623, %f177;
	shfl.sync.bfly.b32 	%r625|%p87, %r623, %r590, %r582, %r584;
	mov.b32 	%f178, %r625;
	add.f32 	%f179, %f177, %f178;
	mov.b32 	%r626, %f179;
	shfl.sync.bfly.b32 	%r628|%p88, %r626, %r593, %r582, %r584;
	mov.b32 	%f180, %r628;
	add.f32 	%f181, %f179, %f180;
	mov.b32 	%r629, %f181;
	shfl.sync.bfly.b32 	%r631|%p89, %r629, %r596, %r582, %r584;
	mov.b32 	%f182, %r631;
	add.f32 	%f183, %f181, %f182;
	st.local.f32 	[%rd3+28], %f183;
	st.shared.f32 	[%r63], %f183;
	bar.sync 	0;
	@%p1 bra 	$L__BB79_25;

	ld.shared.f32 	%f184, [%r4];
	mov.b32 	%r632, %f184;
	mov.u32 	%r633, 31;
	mov.u32 	%r634, 16;
	mov.u32 	%r635, -1;
	shfl.sync.bfly.b32 	%r636|%p90, %r632, %r634, %r633, %r635;
	mov.b32 	%f185, %r636;
	add.f32 	%f186, %f184, %f185;
	mov.b32 	%r637, %f186;
	mov.u32 	%r638, 8;
	shfl.sync.bfly.b32 	%r639|%p91, %r637, %r638, %r633, %r635;
	mov.b32 	%f187, %r639;
	add.f32 	%f188, %f186, %f187;
	mov.b32 	%r640, %f188;
	mov.u32 	%r641, 4;
	shfl.sync.bfly.b32 	%r642|%p92, %r640, %r641, %r633, %r635;
	mov.b32 	%f189, %r642;
	add.f32 	%f190, %f188, %f189;
	mov.b32 	%r643, %f190;
	mov.u32 	%r644, 2;
	shfl.sync.bfly.b32 	%r645|%p93, %r643, %r644, %r633, %r635;
	mov.b32 	%f191, %r645;
	add.f32 	%f192, %f190, %f191;
	mov.b32 	%r646, %f192;
	mov.u32 	%r647, 1;
	shfl.sync.bfly.b32 	%r648|%p94, %r646, %r647, %r633, %r635;
	mov.b32 	%f193, %r648;
	add.f32 	%f194, %f192, %f193;
	st.local.f32 	[%rd3+28], %f194;

$L__BB79_25:
	bar.sync 	0;
	setp.gt.s32 	%p95, %r3, 7;
	@%p95 bra 	$L__BB79_27;

	mad.lo.s32 	%r649, %r3, %r66, %r2;
	cvt.s64.s32 	%rd73, %r649;
	mul.lo.s32 	%r650, %r1, %r67;
	cvt.s64.s32 	%rd74, %r650;
	add.s64 	%rd75, %rd74, %rd73;
	mul.wide.s32 	%rd76, %r3, 4;
	add.s64 	%rd77, %rd3, %rd76;
	ld.local.f32 	%f195, [%rd77];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs3, %f195;}

	// end inline asm
	cvta.to.global.u64 	%rd78, %rd30;
	shl.b64 	%rd79, %rd75, 1;
	add.s64 	%rd80, %rd78, %rd79;
	st.global.u16 	[%rd80], %rs3;

$L__BB79_27:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_1_bs_96
.visible .entry ggml_matvec_f16_ncols_1_bs_96(
	.param .u64 ggml_matvec_f16_ncols_1_bs_96_param_0,
	.param .u64 ggml_matvec_f16_ncols_1_bs_96_param_1,
	.param .u64 ggml_matvec_f16_ncols_1_bs_96_param_2,
	.param .u32 ggml_matvec_f16_ncols_1_bs_96_param_3,
	.param .u32 ggml_matvec_f16_ncols_1_bs_96_param_4,
	.param .u32 ggml_matvec_f16_ncols_1_bs_96_param_5,
	.param .u32 ggml_matvec_f16_ncols_1_bs_96_param_6,
	.param .u32 ggml_matvec_f16_ncols_1_bs_96_param_7,
	.param .u32 ggml_matvec_f16_ncols_1_bs_96_param_8,
	.param .u32 ggml_matvec_f16_ncols_1_bs_96_param_9,
	.param .u32 ggml_matvec_f16_ncols_1_bs_96_param_10,
	.param .u32 ggml_matvec_f16_ncols_1_bs_96_param_11
)
{
	.reg .pred 	%p<19>;
	.reg .b16 	%rs<31>;
	.reg .f32 	%f<30>;
	.reg .b32 	%r<127>;
	.reg .b64 	%rd<47>;


	ld.param.u64 	%rd14, [ggml_matvec_f16_ncols_1_bs_96_param_0];
	ld.param.u64 	%rd15, [ggml_matvec_f16_ncols_1_bs_96_param_1];
	ld.param.u64 	%rd16, [ggml_matvec_f16_ncols_1_bs_96_param_2];
	ld.param.u32 	%r13, [ggml_matvec_f16_ncols_1_bs_96_param_3];
	ld.param.u32 	%r17, [ggml_matvec_f16_ncols_1_bs_96_param_5];
	ld.param.u32 	%r14, [ggml_matvec_f16_ncols_1_bs_96_param_7];
	ld.param.u32 	%r18, [ggml_matvec_f16_ncols_1_bs_96_param_8];
	ld.param.u32 	%r19, [ggml_matvec_f16_ncols_1_bs_96_param_9];
	ld.param.u32 	%r15, [ggml_matvec_f16_ncols_1_bs_96_param_10];
	ld.param.u32 	%r16, [ggml_matvec_f16_ncols_1_bs_96_param_11];
	mov.u32 	%r20, %ctaid.x;
	mov.u32 	%r21, %ctaid.y;
	div.s32 	%r22, %r21, %r18;
	mul.lo.s32 	%r23, %r20, %r17;
	mad.lo.s32 	%r24, %r22, %r19, %r23;
	cvt.s64.s32 	%rd1, %r24;
	mov.u32 	%r1, %tid.x;
	setp.gt.s32 	%p1, %r1, 31;
	shl.b32 	%r25, %r1, 2;
	mov.u32 	%r26, data_mmv;
	add.s32 	%r2, %r26, %r25;
	@%p1 bra 	$L__BB80_2;

	mov.u32 	%r27, 0;
	st.shared.u32 	[%r2], %r27;

$L__BB80_2:
	bar.sync 	0;
	mov.f32 	%f5, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs30, %f5;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs29, %f5;}

	// end inline asm
	setp.ge.s32 	%p2, %r1, %r13;
	@%p2 bra 	$L__BB80_9;

	not.b32 	%r28, %r1;
	add.s32 	%r29, %r28, %r13;
	mul.wide.u32 	%rd17, %r29, -1431655765;
	shr.u64 	%rd18, %rd17, 38;
	cvt.u32.u64 	%r30, %rd18;
	add.s32 	%r31, %r30, 1;
	and.b32  	%r124, %r31, 3;
	setp.eq.s32 	%p3, %r124, 0;
	mov.u32 	%r125, %r1;
	@%p3 bra 	$L__BB80_6;

	mov.u32 	%r125, %tid.x;
	mul.wide.s32 	%rd19, %r125, 2;
	mul.lo.s32 	%r33, %r21, %r15;
	cvt.s64.s32 	%rd20, %r33;
	add.s64 	%rd21, %rd19, %rd20;
	cvta.to.global.u64 	%rd22, %rd15;
	shl.b64 	%rd23, %rd21, 1;
	add.s64 	%rd44, %rd22, %rd23;
	add.s64 	%rd24, %rd19, %rd1;
	cvta.to.global.u64 	%rd25, %rd14;
	shl.b64 	%rd26, %rd24, 1;
	add.s64 	%rd43, %rd25, %rd26;

$L__BB80_5:
	.pragma "nounroll";
	ld.global.nc.u32 	%r35, [%rd43];
	ld.global.nc.u32 	%r36, [%rd44];
	// begin inline asm
	{mul.f16x2 %r34,%r35,%r36;
}
	// end inline asm
	mov.b32 	%r38, {%rs30, %rs29};
	// begin inline asm
	{add.f16x2 %r37,%r38,%r34;
}
	// end inline asm
	mov.b32 	{%rs30, %rs29}, %r37;
	add.s32 	%r125, %r125, 96;
	add.s64 	%rd44, %rd44, 384;
	add.s64 	%rd43, %rd43, 384;
	add.s32 	%r124, %r124, -1;
	setp.ne.s32 	%p4, %r124, 0;
	@%p4 bra 	$L__BB80_5;

$L__BB80_6:
	setp.lt.u32 	%p5, %r29, 288;
	@%p5 bra 	$L__BB80_9;

	mul.wide.s32 	%rd27, %r125, 2;
	cvta.to.global.u64 	%rd28, %rd14;
	add.s64 	%rd29, %rd27, %rd1;
	shl.b64 	%rd30, %rd29, 1;
	add.s64 	%rd31, %rd28, %rd30;
	add.s64 	%rd46, %rd31, 768;
	mul.lo.s32 	%r44, %r21, %r15;
	cvt.s64.s32 	%rd32, %r44;
	cvta.to.global.u64 	%rd33, %rd15;
	add.s64 	%rd34, %rd27, %rd32;
	shl.b64 	%rd35, %rd34, 1;
	add.s64 	%rd36, %rd33, %rd35;
	add.s64 	%rd45, %rd36, 768;

$L__BB80_8:
	ld.global.nc.u32 	%r46, [%rd46+-768];
	ld.global.nc.u32 	%r47, [%rd45+-768];
	// begin inline asm
	{mul.f16x2 %r45,%r46,%r47;
}
	// end inline asm
	mov.b32 	%r49, {%rs30, %rs29};
	// begin inline asm
	{add.f16x2 %r48,%r49,%r45;
}
	// end inline asm
	ld.global.nc.u32 	%r52, [%rd46+-384];
	ld.global.nc.u32 	%r53, [%rd45+-384];
	// begin inline asm
	{mul.f16x2 %r51,%r52,%r53;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r54,%r48,%r51;
}
	// end inline asm
	ld.global.nc.u32 	%r58, [%rd46];
	ld.global.nc.u32 	%r59, [%rd45];
	// begin inline asm
	{mul.f16x2 %r57,%r58,%r59;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r60,%r54,%r57;
}
	// end inline asm
	ld.global.nc.u32 	%r64, [%rd46+384];
	ld.global.nc.u32 	%r65, [%rd45+384];
	// begin inline asm
	{mul.f16x2 %r63,%r64,%r65;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r66,%r60,%r63;
}
	// end inline asm
	mov.b32 	{%rs30, %rs29}, %r66;
	add.s64 	%rd46, %rd46, 1536;
	add.s64 	%rd45, %rd45, 1536;
	add.s32 	%r125, %r125, 384;
	setp.lt.s32 	%p6, %r125, %r13;
	@%p6 bra 	$L__BB80_8;

$L__BB80_9:
	mov.u32 	%r72, 1;
	mov.b32 	%r70, {%rs30, %rs29};
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r70;
  cvt.f32.f16 %f6, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r70;
  cvt.f32.f16 %f7, high;}

	// end inline asm
	add.f32 	%f8, %f6, %f7;
	mov.b32 	%r73, %f8;
	mov.u32 	%r74, 31;
	mov.u32 	%r75, 16;
	mov.u32 	%r76, -1;
	shfl.sync.bfly.b32 	%r77|%p8, %r73, %r75, %r74, %r76;
	mov.b32 	%f9, %r77;
	add.f32 	%f10, %f8, %f9;
	mov.b32 	%r78, %f10;
	mov.u32 	%r79, 8;
	shfl.sync.bfly.b32 	%r80|%p9, %r78, %r79, %r74, %r76;
	mov.b32 	%f11, %r80;
	add.f32 	%f12, %f10, %f11;
	mov.b32 	%r81, %f12;
	mov.u32 	%r82, 4;
	shfl.sync.bfly.b32 	%r83|%p10, %r81, %r82, %r74, %r76;
	mov.b32 	%f13, %r83;
	add.f32 	%f14, %f12, %f13;
	mov.b32 	%r84, %f14;
	mov.u32 	%r85, 2;
	shfl.sync.bfly.b32 	%r86|%p11, %r84, %r85, %r74, %r76;
	mov.b32 	%f15, %r86;
	add.f32 	%f16, %f14, %f15;
	mov.b32 	%r87, %f16;
	shfl.sync.bfly.b32 	%r88|%p12, %r87, %r72, %r74, %r76;
	mov.b32 	%f17, %r88;
	add.f32 	%f29, %f16, %f17;
	shr.s32 	%r89, %r1, 31;
	shr.u32 	%r90, %r89, 27;
	add.s32 	%r91, %r1, %r90;
	shr.s32 	%r92, %r91, 5;
	shl.b32 	%r93, %r92, 2;
	add.s32 	%r95, %r26, %r93;
	st.shared.f32 	[%r95], %f29;
	bar.sync 	0;
	@%p1 bra 	$L__BB80_11;

	ld.shared.f32 	%f18, [%r2];
	mov.b32 	%r101, %f18;
	shfl.sync.bfly.b32 	%r105|%p13, %r101, %r75, %r74, %r76;
	mov.b32 	%f19, %r105;
	add.f32 	%f20, %f18, %f19;
	mov.b32 	%r106, %f20;
	shfl.sync.bfly.b32 	%r108|%p14, %r106, %r79, %r74, %r76;
	mov.b32 	%f21, %r108;
	add.f32 	%f22, %f20, %f21;
	mov.b32 	%r109, %f22;
	shfl.sync.bfly.b32 	%r111|%p15, %r109, %r82, %r74, %r76;
	mov.b32 	%f23, %r111;
	add.f32 	%f24, %f22, %f23;
	mov.b32 	%r112, %f24;
	shfl.sync.bfly.b32 	%r113|%p16, %r112, %r85, %r74, %r76;
	mov.b32 	%f25, %r113;
	add.f32 	%f26, %f24, %f25;
	mov.b32 	%r114, %f26;
	shfl.sync.bfly.b32 	%r116|%p17, %r114, %r72, %r74, %r76;
	mov.b32 	%f27, %r116;
	add.f32 	%f29, %f26, %f27;

$L__BB80_11:
	bar.sync 	0;
	setp.gt.s32 	%p18, %r1, 0;
	@%p18 bra 	$L__BB80_13;

	mad.lo.s32 	%r120, %r1, %r14, %r20;
	cvt.s64.s32 	%rd37, %r120;
	mul.lo.s32 	%r122, %r21, %r16;
	cvt.s64.s32 	%rd38, %r122;
	add.s64 	%rd39, %rd38, %rd37;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs20, %f29;}

	// end inline asm
	cvta.to.global.u64 	%rd40, %rd16;
	shl.b64 	%rd41, %rd39, 1;
	add.s64 	%rd42, %rd40, %rd41;
	st.global.u16 	[%rd42], %rs20;

$L__BB80_13:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_2_bs_96
.visible .entry ggml_matvec_f16_ncols_2_bs_96(
	.param .u64 ggml_matvec_f16_ncols_2_bs_96_param_0,
	.param .u64 ggml_matvec_f16_ncols_2_bs_96_param_1,
	.param .u64 ggml_matvec_f16_ncols_2_bs_96_param_2,
	.param .u32 ggml_matvec_f16_ncols_2_bs_96_param_3,
	.param .u32 ggml_matvec_f16_ncols_2_bs_96_param_4,
	.param .u32 ggml_matvec_f16_ncols_2_bs_96_param_5,
	.param .u32 ggml_matvec_f16_ncols_2_bs_96_param_6,
	.param .u32 ggml_matvec_f16_ncols_2_bs_96_param_7,
	.param .u32 ggml_matvec_f16_ncols_2_bs_96_param_8,
	.param .u32 ggml_matvec_f16_ncols_2_bs_96_param_9,
	.param .u32 ggml_matvec_f16_ncols_2_bs_96_param_10,
	.param .u32 ggml_matvec_f16_ncols_2_bs_96_param_11
)
{
	.local .align 8 .b8 	__local_depot81[8];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<30>;
	.reg .b16 	%rs<4>;
	.reg .f32 	%f<52>;
	.reg .b32 	%r<201>;
	.reg .b64 	%rd<66>;


	mov.u64 	%SPL, __local_depot81;
	ld.param.u64 	%rd27, [ggml_matvec_f16_ncols_2_bs_96_param_0];
	ld.param.u64 	%rd28, [ggml_matvec_f16_ncols_2_bs_96_param_1];
	ld.param.u64 	%rd26, [ggml_matvec_f16_ncols_2_bs_96_param_2];
	ld.param.u32 	%r28, [ggml_matvec_f16_ncols_2_bs_96_param_3];
	ld.param.u32 	%r32, [ggml_matvec_f16_ncols_2_bs_96_param_5];
	ld.param.u32 	%r29, [ggml_matvec_f16_ncols_2_bs_96_param_6];
	ld.param.u32 	%r30, [ggml_matvec_f16_ncols_2_bs_96_param_7];
	ld.param.u32 	%r33, [ggml_matvec_f16_ncols_2_bs_96_param_8];
	ld.param.u32 	%r34, [ggml_matvec_f16_ncols_2_bs_96_param_9];
	ld.param.u32 	%r35, [ggml_matvec_f16_ncols_2_bs_96_param_10];
	ld.param.u32 	%r31, [ggml_matvec_f16_ncols_2_bs_96_param_11];
	cvta.to.global.u64 	%rd1, %rd28;
	cvta.to.global.u64 	%rd2, %rd27;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r36, %r1, %r33;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r37, %r2, %r32;
	mad.lo.s32 	%r38, %r36, %r34, %r37;
	cvt.s64.s32 	%rd4, %r38;
	mul.lo.s32 	%r39, %r1, %r35;
	cvt.s64.s32 	%rd5, %r39;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r40, %r3, 2;
	mov.u32 	%r41, data_mmv;
	add.s32 	%r4, %r41, %r40;
	@%p1 bra 	$L__BB81_2;

	mov.u32 	%r42, 0;
	st.shared.u32 	[%r4], %r42;

$L__BB81_2:
	bar.sync 	0;
	mov.f32 	%f3, 0f00000000;
	st.local.v2.f32 	[%rd3], {%f3, %f3};
	mov.u32 	%r199, 0;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs2, %f3;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f3;}

	// end inline asm
	mov.b32 	%r200, {%rs1, %rs2};
	setp.ge.s32 	%p2, %r3, %r28;
	@%p2 bra 	$L__BB81_9;

	not.b32 	%r45, %r3;
	add.s32 	%r6, %r45, %r28;
	mul.wide.u32 	%rd30, %r6, -1431655765;
	shr.u64 	%rd31, %rd30, 38;
	cvt.u32.u64 	%r46, %rd31;
	add.s32 	%r47, %r46, 1;
	and.b32  	%r192, %r47, 3;
	setp.eq.s32 	%p3, %r192, 0;
	mov.u32 	%r199, 0;
	mov.u32 	%r195, %r3;
	@%p3 bra 	$L__BB81_6;

	mul.wide.s32 	%rd32, %r29, 2;
	mul.wide.s32 	%rd33, %r3, 2;
	add.s64 	%rd34, %rd32, %rd33;
	add.s64 	%rd35, %rd34, %rd5;
	shl.b64 	%rd36, %rd35, 1;
	add.s64 	%rd62, %rd1, %rd36;
	add.s64 	%rd37, %rd33, %rd5;
	shl.b64 	%rd38, %rd37, 1;
	add.s64 	%rd61, %rd1, %rd38;
	add.s64 	%rd39, %rd33, %rd4;
	shl.b64 	%rd40, %rd39, 1;
	add.s64 	%rd60, %rd2, %rd40;
	mov.u32 	%r199, 0;
	mov.u32 	%r195, %r3;

$L__BB81_5:
	.pragma "nounroll";
	ld.global.nc.u32 	%r50, [%rd60];
	ld.global.nc.u32 	%r51, [%rd61];
	// begin inline asm
	{mul.f16x2 %r49,%r50,%r51;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r200,%r200,%r49;
}
	// end inline asm
	ld.global.nc.u32 	%r57, [%rd62];
	// begin inline asm
	{mul.f16x2 %r55,%r50,%r57;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r199,%r199,%r55;
}
	// end inline asm
	add.s32 	%r195, %r195, 96;
	add.s64 	%rd62, %rd62, 384;
	add.s64 	%rd61, %rd61, 384;
	add.s64 	%rd60, %rd60, 384;
	add.s32 	%r192, %r192, -1;
	setp.ne.s32 	%p4, %r192, 0;
	@%p4 bra 	$L__BB81_5;

$L__BB81_6:
	setp.lt.u32 	%p5, %r6, 288;
	@%p5 bra 	$L__BB81_9;

	mul.wide.s32 	%rd41, %r195, 2;
	add.s64 	%rd42, %rd41, %rd4;
	shl.b64 	%rd43, %rd42, 1;
	add.s64 	%rd44, %rd2, %rd43;
	add.s64 	%rd65, %rd44, 768;
	add.s64 	%rd45, %rd41, %rd5;
	shl.b64 	%rd46, %rd45, 1;
	add.s64 	%rd47, %rd1, %rd46;
	add.s64 	%rd64, %rd47, 1152;
	mul.wide.s32 	%rd48, %r29, 2;
	add.s64 	%rd49, %rd45, %rd48;
	shl.b64 	%rd50, %rd49, 1;
	add.s64 	%rd51, %rd1, %rd50;
	add.s64 	%rd63, %rd51, 768;

$L__BB81_8:
	ld.global.nc.u32 	%r62, [%rd65+-768];
	ld.global.nc.u32 	%r63, [%rd64+-1152];
	// begin inline asm
	{mul.f16x2 %r61,%r62,%r63;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r64,%r200,%r61;
}
	// end inline asm
	ld.global.nc.u32 	%r69, [%rd63+-768];
	// begin inline asm
	{mul.f16x2 %r67,%r62,%r69;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r70,%r199,%r67;
}
	// end inline asm
	ld.global.nc.u32 	%r74, [%rd65+-384];
	ld.global.nc.u32 	%r75, [%rd64+-768];
	// begin inline asm
	{mul.f16x2 %r73,%r74,%r75;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r76,%r64,%r73;
}
	// end inline asm
	ld.global.nc.u32 	%r81, [%rd63+-384];
	// begin inline asm
	{mul.f16x2 %r79,%r74,%r81;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r82,%r70,%r79;
}
	// end inline asm
	ld.global.nc.u32 	%r86, [%rd65];
	ld.global.nc.u32 	%r87, [%rd64+-384];
	// begin inline asm
	{mul.f16x2 %r85,%r86,%r87;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r88,%r76,%r85;
}
	// end inline asm
	ld.global.nc.u32 	%r93, [%rd63];
	// begin inline asm
	{mul.f16x2 %r91,%r86,%r93;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r94,%r82,%r91;
}
	// end inline asm
	ld.global.nc.u32 	%r98, [%rd65+384];
	ld.global.nc.u32 	%r99, [%rd64];
	// begin inline asm
	{mul.f16x2 %r97,%r98,%r99;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r200,%r88,%r97;
}
	// end inline asm
	ld.global.nc.u32 	%r105, [%rd63+384];
	// begin inline asm
	{mul.f16x2 %r103,%r98,%r105;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r199,%r94,%r103;
}
	// end inline asm
	add.s64 	%rd65, %rd65, 1536;
	add.s64 	%rd64, %rd64, 1536;
	add.s64 	%rd63, %rd63, 1536;
	add.s32 	%r195, %r195, 384;
	setp.lt.s32 	%p6, %r195, %r28;
	@%p6 bra 	$L__BB81_8;

$L__BB81_9:
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r200;
  cvt.f32.f16 %f4, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r200;
  cvt.f32.f16 %f5, high;}

	// end inline asm
	add.f32 	%f8, %f4, %f5;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r199;
  cvt.f32.f16 %f6, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r199;
  cvt.f32.f16 %f7, high;}

	// end inline asm
	add.f32 	%f1, %f6, %f7;
	st.local.f32 	[%rd3+4], %f1;
	shr.s32 	%r113, %r3, 31;
	shr.u32 	%r114, %r113, 27;
	add.s32 	%r115, %r3, %r114;
	shr.s32 	%r116, %r115, 5;
	shl.b32 	%r117, %r116, 2;
	add.s32 	%r27, %r41, %r117;
	mov.u32 	%r119, 2;
	mov.b32 	%r120, %f8;
	mov.u32 	%r121, 31;
	mov.u32 	%r122, 16;
	mov.u32 	%r123, -1;
	shfl.sync.bfly.b32 	%r124|%p7, %r120, %r122, %r121, %r123;
	mov.b32 	%f9, %r124;
	add.f32 	%f10, %f8, %f9;
	mov.b32 	%r125, %f10;
	mov.u32 	%r126, 8;
	shfl.sync.bfly.b32 	%r127|%p8, %r125, %r126, %r121, %r123;
	mov.b32 	%f11, %r127;
	add.f32 	%f12, %f10, %f11;
	mov.b32 	%r128, %f12;
	mov.u32 	%r129, 4;
	shfl.sync.bfly.b32 	%r130|%p9, %r128, %r129, %r121, %r123;
	mov.b32 	%f13, %r130;
	add.f32 	%f14, %f12, %f13;
	mov.b32 	%r131, %f14;
	shfl.sync.bfly.b32 	%r132|%p10, %r131, %r119, %r121, %r123;
	mov.b32 	%f15, %r132;
	add.f32 	%f16, %f14, %f15;
	mov.b32 	%r133, %f16;
	mov.u32 	%r134, 1;
	shfl.sync.bfly.b32 	%r135|%p11, %r133, %r134, %r121, %r123;
	mov.b32 	%f17, %r135;
	add.f32 	%f18, %f16, %f17;
	st.local.f32 	[%rd3], %f18;
	st.shared.f32 	[%r27], %f18;
	bar.sync 	0;
	@%p1 bra 	$L__BB81_11;

	ld.shared.f32 	%f19, [%r4];
	mov.b32 	%r136, %f19;
	shfl.sync.bfly.b32 	%r140|%p13, %r136, %r122, %r121, %r123;
	mov.b32 	%f20, %r140;
	add.f32 	%f21, %f19, %f20;
	mov.b32 	%r141, %f21;
	shfl.sync.bfly.b32 	%r143|%p14, %r141, %r126, %r121, %r123;
	mov.b32 	%f22, %r143;
	add.f32 	%f23, %f21, %f22;
	mov.b32 	%r144, %f23;
	shfl.sync.bfly.b32 	%r146|%p15, %r144, %r129, %r121, %r123;
	mov.b32 	%f24, %r146;
	add.f32 	%f25, %f23, %f24;
	mov.b32 	%r147, %f25;
	shfl.sync.bfly.b32 	%r149|%p16, %r147, %r119, %r121, %r123;
	mov.b32 	%f26, %r149;
	add.f32 	%f27, %f25, %f26;
	mov.b32 	%r150, %f27;
	shfl.sync.bfly.b32 	%r152|%p17, %r150, %r134, %r121, %r123;
	mov.b32 	%f28, %r152;
	add.f32 	%f29, %f27, %f28;
	st.local.f32 	[%rd3], %f29;

$L__BB81_11:
	bar.sync 	0;
	mov.b32 	%r153, %f1;
	shfl.sync.bfly.b32 	%r157|%p19, %r153, %r122, %r121, %r123;
	mov.b32 	%f30, %r157;
	add.f32 	%f31, %f1, %f30;
	mov.b32 	%r158, %f31;
	shfl.sync.bfly.b32 	%r160|%p20, %r158, %r126, %r121, %r123;
	mov.b32 	%f32, %r160;
	add.f32 	%f33, %f31, %f32;
	mov.b32 	%r161, %f33;
	shfl.sync.bfly.b32 	%r163|%p21, %r161, %r129, %r121, %r123;
	mov.b32 	%f34, %r163;
	add.f32 	%f35, %f33, %f34;
	mov.b32 	%r164, %f35;
	shfl.sync.bfly.b32 	%r166|%p22, %r164, %r119, %r121, %r123;
	mov.b32 	%f36, %r166;
	add.f32 	%f37, %f35, %f36;
	mov.b32 	%r167, %f37;
	shfl.sync.bfly.b32 	%r169|%p23, %r167, %r134, %r121, %r123;
	mov.b32 	%f38, %r169;
	add.f32 	%f39, %f37, %f38;
	st.local.f32 	[%rd3+4], %f39;
	st.shared.f32 	[%r27], %f39;
	bar.sync 	0;
	@%p1 bra 	$L__BB81_13;

	ld.shared.f32 	%f40, [%r4];
	mov.b32 	%r170, %f40;
	mov.u32 	%r171, 31;
	mov.u32 	%r172, 16;
	mov.u32 	%r173, -1;
	shfl.sync.bfly.b32 	%r174|%p24, %r170, %r172, %r171, %r173;
	mov.b32 	%f41, %r174;
	add.f32 	%f42, %f40, %f41;
	mov.b32 	%r175, %f42;
	mov.u32 	%r176, 8;
	shfl.sync.bfly.b32 	%r177|%p25, %r175, %r176, %r171, %r173;
	mov.b32 	%f43, %r177;
	add.f32 	%f44, %f42, %f43;
	mov.b32 	%r178, %f44;
	mov.u32 	%r179, 4;
	shfl.sync.bfly.b32 	%r180|%p26, %r178, %r179, %r171, %r173;
	mov.b32 	%f45, %r180;
	add.f32 	%f46, %f44, %f45;
	mov.b32 	%r181, %f46;
	mov.u32 	%r182, 2;
	shfl.sync.bfly.b32 	%r183|%p27, %r181, %r182, %r171, %r173;
	mov.b32 	%f47, %r183;
	add.f32 	%f48, %f46, %f47;
	mov.b32 	%r184, %f48;
	mov.u32 	%r185, 1;
	shfl.sync.bfly.b32 	%r186|%p28, %r184, %r185, %r171, %r173;
	mov.b32 	%f49, %r186;
	add.f32 	%f50, %f48, %f49;
	st.local.f32 	[%rd3+4], %f50;

$L__BB81_13:
	bar.sync 	0;
	setp.gt.s32 	%p29, %r3, 1;
	@%p29 bra 	$L__BB81_15;

	mad.lo.s32 	%r187, %r3, %r30, %r2;
	cvt.s64.s32 	%rd52, %r187;
	mul.lo.s32 	%r188, %r1, %r31;
	cvt.s64.s32 	%rd53, %r188;
	add.s64 	%rd54, %rd53, %rd52;
	mul.wide.s32 	%rd55, %r3, 4;
	add.s64 	%rd56, %rd3, %rd55;
	ld.local.f32 	%f51, [%rd56];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs3, %f51;}

	// end inline asm
	cvta.to.global.u64 	%rd57, %rd26;
	shl.b64 	%rd58, %rd54, 1;
	add.s64 	%rd59, %rd57, %rd58;
	st.global.u16 	[%rd59], %rs3;

$L__BB81_15:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_3_bs_96
.visible .entry ggml_matvec_f16_ncols_3_bs_96(
	.param .u64 ggml_matvec_f16_ncols_3_bs_96_param_0,
	.param .u64 ggml_matvec_f16_ncols_3_bs_96_param_1,
	.param .u64 ggml_matvec_f16_ncols_3_bs_96_param_2,
	.param .u32 ggml_matvec_f16_ncols_3_bs_96_param_3,
	.param .u32 ggml_matvec_f16_ncols_3_bs_96_param_4,
	.param .u32 ggml_matvec_f16_ncols_3_bs_96_param_5,
	.param .u32 ggml_matvec_f16_ncols_3_bs_96_param_6,
	.param .u32 ggml_matvec_f16_ncols_3_bs_96_param_7,
	.param .u32 ggml_matvec_f16_ncols_3_bs_96_param_8,
	.param .u32 ggml_matvec_f16_ncols_3_bs_96_param_9,
	.param .u32 ggml_matvec_f16_ncols_3_bs_96_param_10,
	.param .u32 ggml_matvec_f16_ncols_3_bs_96_param_11
)
{
	.local .align 4 .b8 	__local_depot82[12];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<41>;
	.reg .b16 	%rs<4>;
	.reg .f32 	%f<76>;
	.reg .b32 	%r<286>;
	.reg .b64 	%rd<74>;


	mov.u64 	%SPL, __local_depot82;
	ld.param.u64 	%rd29, [ggml_matvec_f16_ncols_3_bs_96_param_0];
	ld.param.u64 	%rd30, [ggml_matvec_f16_ncols_3_bs_96_param_1];
	ld.param.u64 	%rd28, [ggml_matvec_f16_ncols_3_bs_96_param_2];
	ld.param.u32 	%r34, [ggml_matvec_f16_ncols_3_bs_96_param_3];
	ld.param.u32 	%r38, [ggml_matvec_f16_ncols_3_bs_96_param_5];
	ld.param.u32 	%r35, [ggml_matvec_f16_ncols_3_bs_96_param_6];
	ld.param.u32 	%r36, [ggml_matvec_f16_ncols_3_bs_96_param_7];
	ld.param.u32 	%r39, [ggml_matvec_f16_ncols_3_bs_96_param_8];
	ld.param.u32 	%r40, [ggml_matvec_f16_ncols_3_bs_96_param_9];
	ld.param.u32 	%r41, [ggml_matvec_f16_ncols_3_bs_96_param_10];
	ld.param.u32 	%r37, [ggml_matvec_f16_ncols_3_bs_96_param_11];
	cvta.to.global.u64 	%rd73, %rd30;
	cvta.to.global.u64 	%rd2, %rd29;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r42, %r1, %r39;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r43, %r2, %r38;
	mad.lo.s32 	%r44, %r42, %r40, %r43;
	cvt.s64.s32 	%rd4, %r44;
	mul.lo.s32 	%r45, %r1, %r41;
	cvt.s64.s32 	%rd5, %r45;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r46, %r3, 2;
	mov.u32 	%r47, data_mmv;
	add.s32 	%r4, %r47, %r46;
	@%p1 bra 	$L__BB82_2;

	mov.u32 	%r48, 0;
	st.shared.u32 	[%r4], %r48;

$L__BB82_2:
	bar.sync 	0;
	mov.f32 	%f4, 0f00000000;
	mov.u32 	%r283, 0;
	st.local.u32 	[%rd3], %r283;
	st.local.u32 	[%rd3+4], %r283;
	st.local.u32 	[%rd3+8], %r283;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs2, %f4;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f4;}

	// end inline asm
	mov.b32 	%r285, {%rs1, %rs2};
	setp.ge.s32 	%p2, %r3, %r34;
	mov.u32 	%r284, %r283;
	@%p2 bra 	$L__BB82_9;

	not.b32 	%r53, %r3;
	add.s32 	%r6, %r53, %r34;
	mul.wide.u32 	%rd32, %r6, -1431655765;
	shr.u64 	%rd33, %rd32, 38;
	cvt.u32.u64 	%r54, %rd33;
	add.s32 	%r55, %r54, 1;
	and.b32  	%r274, %r55, 3;
	setp.eq.s32 	%p3, %r274, 0;
	mov.u32 	%r283, 0;
	mov.u32 	%r278, %r3;
	@%p3 bra 	$L__BB82_6;

	shl.b32 	%r58, %r35, 1;
	add.s32 	%r59, %r3, %r58;
	mul.wide.s32 	%rd34, %r59, 2;
	add.s64 	%rd35, %rd34, %rd5;
	shl.b64 	%rd36, %rd35, 1;
	add.s64 	%rd71, %rd73, %rd36;
	mul.wide.s32 	%rd37, %r35, 2;
	mul.wide.s32 	%rd38, %r3, 2;
	add.s64 	%rd39, %rd37, %rd38;
	add.s64 	%rd40, %rd39, %rd5;
	shl.b64 	%rd41, %rd40, 1;
	add.s64 	%rd70, %rd73, %rd41;
	add.s64 	%rd42, %rd38, %rd5;
	shl.b64 	%rd43, %rd42, 1;
	add.s64 	%rd69, %rd73, %rd43;
	add.s64 	%rd44, %rd38, %rd4;
	shl.b64 	%rd45, %rd44, 1;
	add.s64 	%rd68, %rd2, %rd45;
	mov.u32 	%r283, 0;
	mov.u32 	%r284, %r283;
	mov.u32 	%r278, %r3;

$L__BB82_5:
	.pragma "nounroll";
	ld.global.nc.u32 	%r61, [%rd68];
	ld.global.nc.u32 	%r62, [%rd69];
	// begin inline asm
	{mul.f16x2 %r60,%r61,%r62;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r285,%r285,%r60;
}
	// end inline asm
	ld.global.nc.u32 	%r68, [%rd70];
	// begin inline asm
	{mul.f16x2 %r66,%r61,%r68;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r284,%r284,%r66;
}
	// end inline asm
	ld.global.nc.u32 	%r74, [%rd71];
	// begin inline asm
	{mul.f16x2 %r72,%r61,%r74;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r283,%r283,%r72;
}
	// end inline asm
	add.s32 	%r278, %r278, 96;
	add.s64 	%rd71, %rd71, 384;
	add.s64 	%rd70, %rd70, 384;
	add.s64 	%rd69, %rd69, 384;
	add.s64 	%rd68, %rd68, 384;
	add.s32 	%r274, %r274, -1;
	setp.ne.s32 	%p4, %r274, 0;
	@%p4 bra 	$L__BB82_5;

$L__BB82_6:
	setp.lt.u32 	%p5, %r6, 288;
	@%p5 bra 	$L__BB82_9;

	add.s32 	%r78, %r278, %r35;
	shl.b32 	%r79, %r35, 1;
	add.s32 	%r80, %r278, %r79;
	add.s32 	%r81, %r78, 96;
	mul.wide.s32 	%rd46, %r81, 4;
	shl.b64 	%rd47, %rd5, 1;
	add.s64 	%rd19, %rd46, %rd47;
	mul.wide.s32 	%rd48, %r80, 4;
	add.s64 	%rd20, %rd48, %rd47;
	mul.wide.s32 	%rd49, %r278, 2;
	add.s64 	%rd50, %rd49, %rd4;
	shl.b64 	%rd51, %rd50, 1;
	add.s64 	%rd52, %rd2, %rd51;
	add.s64 	%rd72, %rd52, 768;
	mul.wide.s32 	%rd53, %r278, 4;
	add.s64 	%rd22, %rd53, %rd47;
	mul.wide.s32 	%rd54, %r35, 4;
	add.s64 	%rd55, %rd53, %rd54;
	add.s64 	%rd23, %rd55, %rd47;

$L__BB82_8:
	ld.global.nc.u32 	%r83, [%rd72+-768];
	add.s64 	%rd56, %rd73, %rd22;
	ld.global.nc.u32 	%r84, [%rd56];
	// begin inline asm
	{mul.f16x2 %r82,%r83,%r84;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r85,%r285,%r82;
}
	// end inline asm
	add.s64 	%rd57, %rd73, %rd23;
	ld.global.nc.u32 	%r90, [%rd57];
	// begin inline asm
	{mul.f16x2 %r88,%r83,%r90;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r91,%r284,%r88;
}
	// end inline asm
	add.s64 	%rd58, %rd73, %rd20;
	ld.global.nc.u32 	%r96, [%rd58];
	// begin inline asm
	{mul.f16x2 %r94,%r83,%r96;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r97,%r283,%r94;
}
	// end inline asm
	ld.global.nc.u32 	%r101, [%rd72+-384];
	ld.global.nc.u32 	%r102, [%rd56+384];
	// begin inline asm
	{mul.f16x2 %r100,%r101,%r102;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r103,%r85,%r100;
}
	// end inline asm
	add.s64 	%rd59, %rd73, %rd19;
	ld.global.nc.u32 	%r108, [%rd59];
	// begin inline asm
	{mul.f16x2 %r106,%r101,%r108;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r109,%r91,%r106;
}
	// end inline asm
	ld.global.nc.u32 	%r114, [%rd58+384];
	// begin inline asm
	{mul.f16x2 %r112,%r101,%r114;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r115,%r97,%r112;
}
	// end inline asm
	ld.global.nc.u32 	%r119, [%rd72];
	ld.global.nc.u32 	%r120, [%rd56+768];
	// begin inline asm
	{mul.f16x2 %r118,%r119,%r120;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r121,%r103,%r118;
}
	// end inline asm
	ld.global.nc.u32 	%r126, [%rd59+384];
	// begin inline asm
	{mul.f16x2 %r124,%r119,%r126;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r127,%r109,%r124;
}
	// end inline asm
	ld.global.nc.u32 	%r132, [%rd58+768];
	// begin inline asm
	{mul.f16x2 %r130,%r119,%r132;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r133,%r115,%r130;
}
	// end inline asm
	ld.global.nc.u32 	%r137, [%rd72+384];
	ld.global.nc.u32 	%r138, [%rd56+1152];
	// begin inline asm
	{mul.f16x2 %r136,%r137,%r138;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r285,%r121,%r136;
}
	// end inline asm
	ld.global.nc.u32 	%r144, [%rd59+768];
	// begin inline asm
	{mul.f16x2 %r142,%r137,%r144;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r284,%r127,%r142;
}
	// end inline asm
	ld.global.nc.u32 	%r150, [%rd58+1152];
	// begin inline asm
	{mul.f16x2 %r148,%r137,%r150;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r283,%r133,%r148;
}
	// end inline asm
	add.s64 	%rd73, %rd73, 1536;
	add.s64 	%rd72, %rd72, 1536;
	add.s32 	%r278, %r278, 384;
	setp.lt.s32 	%p6, %r278, %r34;
	@%p6 bra 	$L__BB82_8;

$L__BB82_9:
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r285;
  cvt.f32.f16 %f5, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r285;
  cvt.f32.f16 %f6, high;}

	// end inline asm
	add.f32 	%f11, %f5, %f6;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r284;
  cvt.f32.f16 %f7, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r284;
  cvt.f32.f16 %f8, high;}

	// end inline asm
	add.f32 	%f1, %f7, %f8;
	st.local.f32 	[%rd3+4], %f1;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r283;
  cvt.f32.f16 %f9, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r283;
  cvt.f32.f16 %f10, high;}

	// end inline asm
	add.f32 	%f2, %f9, %f10;
	st.local.f32 	[%rd3+8], %f2;
	shr.s32 	%r160, %r3, 31;
	shr.u32 	%r161, %r160, 27;
	add.s32 	%r162, %r3, %r161;
	shr.s32 	%r163, %r162, 5;
	shl.b32 	%r164, %r163, 2;
	add.s32 	%r33, %r47, %r164;
	mov.u32 	%r166, 2;
	mov.b32 	%r167, %f11;
	mov.u32 	%r168, 31;
	mov.u32 	%r169, 16;
	mov.u32 	%r170, -1;
	shfl.sync.bfly.b32 	%r171|%p7, %r167, %r169, %r168, %r170;
	mov.b32 	%f12, %r171;
	add.f32 	%f13, %f11, %f12;
	mov.b32 	%r172, %f13;
	mov.u32 	%r173, 8;
	shfl.sync.bfly.b32 	%r174|%p8, %r172, %r173, %r168, %r170;
	mov.b32 	%f14, %r174;
	add.f32 	%f15, %f13, %f14;
	mov.b32 	%r175, %f15;
	mov.u32 	%r176, 4;
	shfl.sync.bfly.b32 	%r177|%p9, %r175, %r176, %r168, %r170;
	mov.b32 	%f16, %r177;
	add.f32 	%f17, %f15, %f16;
	mov.b32 	%r178, %f17;
	shfl.sync.bfly.b32 	%r179|%p10, %r178, %r166, %r168, %r170;
	mov.b32 	%f18, %r179;
	add.f32 	%f19, %f17, %f18;
	mov.b32 	%r180, %f19;
	mov.u32 	%r181, 1;
	shfl.sync.bfly.b32 	%r182|%p11, %r180, %r181, %r168, %r170;
	mov.b32 	%f20, %r182;
	add.f32 	%f21, %f19, %f20;
	st.local.f32 	[%rd3], %f21;
	st.shared.f32 	[%r33], %f21;
	bar.sync 	0;
	@%p1 bra 	$L__BB82_11;

	ld.shared.f32 	%f22, [%r4];
	mov.b32 	%r183, %f22;
	shfl.sync.bfly.b32 	%r187|%p13, %r183, %r169, %r168, %r170;
	mov.b32 	%f23, %r187;
	add.f32 	%f24, %f22, %f23;
	mov.b32 	%r188, %f24;
	shfl.sync.bfly.b32 	%r190|%p14, %r188, %r173, %r168, %r170;
	mov.b32 	%f25, %r190;
	add.f32 	%f26, %f24, %f25;
	mov.b32 	%r191, %f26;
	shfl.sync.bfly.b32 	%r193|%p15, %r191, %r176, %r168, %r170;
	mov.b32 	%f27, %r193;
	add.f32 	%f28, %f26, %f27;
	mov.b32 	%r194, %f28;
	shfl.sync.bfly.b32 	%r196|%p16, %r194, %r166, %r168, %r170;
	mov.b32 	%f29, %r196;
	add.f32 	%f30, %f28, %f29;
	mov.b32 	%r197, %f30;
	shfl.sync.bfly.b32 	%r199|%p17, %r197, %r181, %r168, %r170;
	mov.b32 	%f31, %r199;
	add.f32 	%f32, %f30, %f31;
	st.local.f32 	[%rd3], %f32;

$L__BB82_11:
	bar.sync 	0;
	mov.b32 	%r200, %f1;
	shfl.sync.bfly.b32 	%r204|%p19, %r200, %r169, %r168, %r170;
	mov.b32 	%f33, %r204;
	add.f32 	%f34, %f1, %f33;
	mov.b32 	%r205, %f34;
	shfl.sync.bfly.b32 	%r207|%p20, %r205, %r173, %r168, %r170;
	mov.b32 	%f35, %r207;
	add.f32 	%f36, %f34, %f35;
	mov.b32 	%r208, %f36;
	shfl.sync.bfly.b32 	%r210|%p21, %r208, %r176, %r168, %r170;
	mov.b32 	%f37, %r210;
	add.f32 	%f38, %f36, %f37;
	mov.b32 	%r211, %f38;
	shfl.sync.bfly.b32 	%r213|%p22, %r211, %r166, %r168, %r170;
	mov.b32 	%f39, %r213;
	add.f32 	%f40, %f38, %f39;
	mov.b32 	%r214, %f40;
	shfl.sync.bfly.b32 	%r216|%p23, %r214, %r181, %r168, %r170;
	mov.b32 	%f41, %r216;
	add.f32 	%f42, %f40, %f41;
	st.local.f32 	[%rd3+4], %f42;
	st.shared.f32 	[%r33], %f42;
	bar.sync 	0;
	@%p1 bra 	$L__BB82_13;

	ld.shared.f32 	%f43, [%r4];
	mov.b32 	%r217, %f43;
	mov.u32 	%r218, 31;
	mov.u32 	%r219, 16;
	mov.u32 	%r220, -1;
	shfl.sync.bfly.b32 	%r221|%p24, %r217, %r219, %r218, %r220;
	mov.b32 	%f44, %r221;
	add.f32 	%f45, %f43, %f44;
	mov.b32 	%r222, %f45;
	mov.u32 	%r223, 8;
	shfl.sync.bfly.b32 	%r224|%p25, %r222, %r223, %r218, %r220;
	mov.b32 	%f46, %r224;
	add.f32 	%f47, %f45, %f46;
	mov.b32 	%r225, %f47;
	mov.u32 	%r226, 4;
	shfl.sync.bfly.b32 	%r227|%p26, %r225, %r226, %r218, %r220;
	mov.b32 	%f48, %r227;
	add.f32 	%f49, %f47, %f48;
	mov.b32 	%r228, %f49;
	mov.u32 	%r229, 2;
	shfl.sync.bfly.b32 	%r230|%p27, %r228, %r229, %r218, %r220;
	mov.b32 	%f50, %r230;
	add.f32 	%f51, %f49, %f50;
	mov.b32 	%r231, %f51;
	mov.u32 	%r232, 1;
	shfl.sync.bfly.b32 	%r233|%p28, %r231, %r232, %r218, %r220;
	mov.b32 	%f52, %r233;
	add.f32 	%f53, %f51, %f52;
	st.local.f32 	[%rd3+4], %f53;

$L__BB82_13:
	bar.sync 	0;
	mov.b32 	%r234, %f2;
	mov.u32 	%r235, 31;
	mov.u32 	%r236, 16;
	mov.u32 	%r237, -1;
	shfl.sync.bfly.b32 	%r238|%p30, %r234, %r236, %r235, %r237;
	mov.b32 	%f54, %r238;
	add.f32 	%f55, %f2, %f54;
	mov.b32 	%r239, %f55;
	mov.u32 	%r240, 8;
	shfl.sync.bfly.b32 	%r241|%p31, %r239, %r240, %r235, %r237;
	mov.b32 	%f56, %r241;
	add.f32 	%f57, %f55, %f56;
	mov.b32 	%r242, %f57;
	mov.u32 	%r243, 4;
	shfl.sync.bfly.b32 	%r244|%p32, %r242, %r243, %r235, %r237;
	mov.b32 	%f58, %r244;
	add.f32 	%f59, %f57, %f58;
	mov.b32 	%r245, %f59;
	mov.u32 	%r246, 2;
	shfl.sync.bfly.b32 	%r247|%p33, %r245, %r246, %r235, %r237;
	mov.b32 	%f60, %r247;
	add.f32 	%f61, %f59, %f60;
	mov.b32 	%r248, %f61;
	mov.u32 	%r249, 1;
	shfl.sync.bfly.b32 	%r250|%p34, %r248, %r249, %r235, %r237;
	mov.b32 	%f62, %r250;
	add.f32 	%f63, %f61, %f62;
	st.local.f32 	[%rd3+8], %f63;
	st.shared.f32 	[%r33], %f63;
	bar.sync 	0;
	@%p1 bra 	$L__BB82_15;

	ld.shared.f32 	%f64, [%r4];
	mov.b32 	%r251, %f64;
	shfl.sync.bfly.b32 	%r255|%p35, %r251, %r236, %r235, %r237;
	mov.b32 	%f65, %r255;
	add.f32 	%f66, %f64, %f65;
	mov.b32 	%r256, %f66;
	shfl.sync.bfly.b32 	%r258|%p36, %r256, %r240, %r235, %r237;
	mov.b32 	%f67, %r258;
	add.f32 	%f68, %f66, %f67;
	mov.b32 	%r259, %f68;
	shfl.sync.bfly.b32 	%r261|%p37, %r259, %r243, %r235, %r237;
	mov.b32 	%f69, %r261;
	add.f32 	%f70, %f68, %f69;
	mov.b32 	%r262, %f70;
	shfl.sync.bfly.b32 	%r264|%p38, %r262, %r246, %r235, %r237;
	mov.b32 	%f71, %r264;
	add.f32 	%f72, %f70, %f71;
	mov.b32 	%r265, %f72;
	shfl.sync.bfly.b32 	%r267|%p39, %r265, %r249, %r235, %r237;
	mov.b32 	%f73, %r267;
	add.f32 	%f74, %f72, %f73;
	st.local.f32 	[%rd3+8], %f74;

$L__BB82_15:
	bar.sync 	0;
	setp.gt.s32 	%p40, %r3, 2;
	@%p40 bra 	$L__BB82_17;

	mad.lo.s32 	%r268, %r3, %r36, %r2;
	cvt.s64.s32 	%rd60, %r268;
	mul.lo.s32 	%r269, %r1, %r37;
	cvt.s64.s32 	%rd61, %r269;
	add.s64 	%rd62, %rd61, %rd60;
	mul.wide.s32 	%rd63, %r3, 4;
	add.s64 	%rd64, %rd3, %rd63;
	ld.local.f32 	%f75, [%rd64];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs3, %f75;}

	// end inline asm
	cvta.to.global.u64 	%rd65, %rd28;
	shl.b64 	%rd66, %rd62, 1;
	add.s64 	%rd67, %rd65, %rd66;
	st.global.u16 	[%rd67], %rs3;

$L__BB82_17:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_4_bs_96
.visible .entry ggml_matvec_f16_ncols_4_bs_96(
	.param .u64 ggml_matvec_f16_ncols_4_bs_96_param_0,
	.param .u64 ggml_matvec_f16_ncols_4_bs_96_param_1,
	.param .u64 ggml_matvec_f16_ncols_4_bs_96_param_2,
	.param .u32 ggml_matvec_f16_ncols_4_bs_96_param_3,
	.param .u32 ggml_matvec_f16_ncols_4_bs_96_param_4,
	.param .u32 ggml_matvec_f16_ncols_4_bs_96_param_5,
	.param .u32 ggml_matvec_f16_ncols_4_bs_96_param_6,
	.param .u32 ggml_matvec_f16_ncols_4_bs_96_param_7,
	.param .u32 ggml_matvec_f16_ncols_4_bs_96_param_8,
	.param .u32 ggml_matvec_f16_ncols_4_bs_96_param_9,
	.param .u32 ggml_matvec_f16_ncols_4_bs_96_param_10,
	.param .u32 ggml_matvec_f16_ncols_4_bs_96_param_11
)
{
	.local .align 16 .b8 	__local_depot83[16];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<52>;
	.reg .b16 	%rs<4>;
	.reg .f32 	%f<100>;
	.reg .b32 	%r<367>;
	.reg .b64 	%rd<85>;


	mov.u64 	%SPL, __local_depot83;
	ld.param.u64 	%rd34, [ggml_matvec_f16_ncols_4_bs_96_param_0];
	ld.param.u64 	%rd35, [ggml_matvec_f16_ncols_4_bs_96_param_1];
	ld.param.u64 	%rd33, [ggml_matvec_f16_ncols_4_bs_96_param_2];
	ld.param.u32 	%r40, [ggml_matvec_f16_ncols_4_bs_96_param_3];
	ld.param.u32 	%r44, [ggml_matvec_f16_ncols_4_bs_96_param_5];
	ld.param.u32 	%r41, [ggml_matvec_f16_ncols_4_bs_96_param_6];
	ld.param.u32 	%r42, [ggml_matvec_f16_ncols_4_bs_96_param_7];
	ld.param.u32 	%r45, [ggml_matvec_f16_ncols_4_bs_96_param_8];
	ld.param.u32 	%r46, [ggml_matvec_f16_ncols_4_bs_96_param_9];
	ld.param.u32 	%r47, [ggml_matvec_f16_ncols_4_bs_96_param_10];
	ld.param.u32 	%r43, [ggml_matvec_f16_ncols_4_bs_96_param_11];
	cvta.to.global.u64 	%rd84, %rd35;
	cvta.to.global.u64 	%rd2, %rd34;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r48, %r1, %r45;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r49, %r2, %r44;
	mad.lo.s32 	%r50, %r48, %r46, %r49;
	cvt.s64.s32 	%rd4, %r50;
	mul.lo.s32 	%r51, %r1, %r47;
	cvt.s64.s32 	%rd5, %r51;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r52, %r3, 2;
	mov.u32 	%r53, data_mmv;
	add.s32 	%r4, %r53, %r52;
	@%p1 bra 	$L__BB83_2;

	mov.u32 	%r54, 0;
	st.shared.u32 	[%r4], %r54;

$L__BB83_2:
	bar.sync 	0;
	mov.f32 	%f5, 0f00000000;
	st.local.v4.f32 	[%rd3], {%f5, %f5, %f5, %f5};
	mov.u32 	%r363, 0;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs2, %f5;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f5;}

	// end inline asm
	mov.b32 	%r366, {%rs1, %rs2};
	setp.ge.s32 	%p2, %r3, %r40;
	mov.u32 	%r364, %r363;
	mov.u32 	%r365, %r363;
	@%p2 bra 	$L__BB83_9;

	not.b32 	%r61, %r3;
	add.s32 	%r6, %r61, %r40;
	mul.wide.u32 	%rd37, %r6, -1431655765;
	shr.u64 	%rd38, %rd37, 38;
	cvt.u32.u64 	%r62, %rd38;
	add.s32 	%r63, %r62, 1;
	and.b32  	%r352, %r63, 3;
	setp.eq.s32 	%p3, %r352, 0;
	mov.u32 	%r363, 0;
	mov.u32 	%r357, %r3;
	@%p3 bra 	$L__BB83_6;

	shl.b32 	%r67, %r41, 1;
	add.s32 	%r68, %r3, %r67;
	mul.wide.s32 	%rd39, %r68, 2;
	add.s64 	%rd40, %rd39, %rd5;
	shl.b64 	%rd41, %rd40, 1;
	add.s64 	%rd82, %rd84, %rd41;
	mad.lo.s32 	%r69, %r41, 3, %r3;
	mul.wide.s32 	%rd42, %r69, 2;
	add.s64 	%rd43, %rd42, %rd5;
	shl.b64 	%rd44, %rd43, 1;
	add.s64 	%rd81, %rd84, %rd44;
	mul.wide.s32 	%rd45, %r41, 2;
	mul.wide.s32 	%rd46, %r3, 2;
	add.s64 	%rd47, %rd45, %rd46;
	add.s64 	%rd48, %rd47, %rd5;
	shl.b64 	%rd49, %rd48, 1;
	add.s64 	%rd80, %rd84, %rd49;
	add.s64 	%rd50, %rd46, %rd5;
	shl.b64 	%rd51, %rd50, 1;
	add.s64 	%rd79, %rd84, %rd51;
	add.s64 	%rd52, %rd46, %rd4;
	shl.b64 	%rd53, %rd52, 1;
	add.s64 	%rd78, %rd2, %rd53;
	mov.u32 	%r363, 0;
	mov.u32 	%r364, %r363;
	mov.u32 	%r365, %r363;
	mov.u32 	%r357, %r3;

$L__BB83_5:
	.pragma "nounroll";
	ld.global.nc.u32 	%r71, [%rd78];
	ld.global.nc.u32 	%r72, [%rd79];
	// begin inline asm
	{mul.f16x2 %r70,%r71,%r72;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r366,%r366,%r70;
}
	// end inline asm
	ld.global.nc.u32 	%r78, [%rd80];
	// begin inline asm
	{mul.f16x2 %r76,%r71,%r78;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r365,%r365,%r76;
}
	// end inline asm
	ld.global.nc.u32 	%r84, [%rd82];
	// begin inline asm
	{mul.f16x2 %r82,%r71,%r84;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r364,%r364,%r82;
}
	// end inline asm
	ld.global.nc.u32 	%r90, [%rd81];
	// begin inline asm
	{mul.f16x2 %r88,%r71,%r90;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r363,%r363,%r88;
}
	// end inline asm
	add.s32 	%r357, %r357, 96;
	add.s64 	%rd82, %rd82, 384;
	add.s64 	%rd81, %rd81, 384;
	add.s64 	%rd80, %rd80, 384;
	add.s64 	%rd79, %rd79, 384;
	add.s64 	%rd78, %rd78, 384;
	add.s32 	%r352, %r352, -1;
	setp.ne.s32 	%p4, %r352, 0;
	@%p4 bra 	$L__BB83_5;

$L__BB83_6:
	setp.lt.u32 	%p5, %r6, 288;
	@%p5 bra 	$L__BB83_9;

	add.s32 	%r94, %r357, %r41;
	shl.b32 	%r95, %r41, 1;
	add.s32 	%r96, %r357, %r95;
	mad.lo.s32 	%r97, %r41, 3, %r357;
	add.s32 	%r98, %r94, 96;
	mul.wide.s32 	%rd54, %r98, 4;
	shl.b64 	%rd55, %rd5, 1;
	add.s64 	%rd23, %rd54, %rd55;
	mul.wide.s32 	%rd56, %r96, 4;
	add.s64 	%rd24, %rd56, %rd55;
	mul.wide.s32 	%rd57, %r97, 4;
	add.s64 	%rd25, %rd57, %rd55;
	mul.wide.s32 	%rd58, %r357, 2;
	add.s64 	%rd59, %rd58, %rd4;
	shl.b64 	%rd60, %rd59, 1;
	add.s64 	%rd61, %rd2, %rd60;
	add.s64 	%rd83, %rd61, 768;
	mul.wide.s32 	%rd62, %r357, 4;
	add.s64 	%rd27, %rd62, %rd55;
	mul.wide.s32 	%rd63, %r41, 4;
	add.s64 	%rd64, %rd62, %rd63;
	add.s64 	%rd28, %rd64, %rd55;

$L__BB83_8:
	ld.global.nc.u32 	%r100, [%rd83+-768];
	add.s64 	%rd65, %rd84, %rd27;
	ld.global.nc.u32 	%r101, [%rd65];
	// begin inline asm
	{mul.f16x2 %r99,%r100,%r101;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r102,%r366,%r99;
}
	// end inline asm
	add.s64 	%rd66, %rd84, %rd28;
	ld.global.nc.u32 	%r107, [%rd66];
	// begin inline asm
	{mul.f16x2 %r105,%r100,%r107;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r108,%r365,%r105;
}
	// end inline asm
	add.s64 	%rd67, %rd84, %rd24;
	ld.global.nc.u32 	%r113, [%rd67];
	// begin inline asm
	{mul.f16x2 %r111,%r100,%r113;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r114,%r364,%r111;
}
	// end inline asm
	add.s64 	%rd68, %rd84, %rd25;
	ld.global.nc.u32 	%r119, [%rd68];
	// begin inline asm
	{mul.f16x2 %r117,%r100,%r119;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r120,%r363,%r117;
}
	// end inline asm
	ld.global.nc.u32 	%r124, [%rd83+-384];
	ld.global.nc.u32 	%r125, [%rd65+384];
	// begin inline asm
	{mul.f16x2 %r123,%r124,%r125;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r126,%r102,%r123;
}
	// end inline asm
	add.s64 	%rd69, %rd84, %rd23;
	ld.global.nc.u32 	%r131, [%rd69];
	// begin inline asm
	{mul.f16x2 %r129,%r124,%r131;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r132,%r108,%r129;
}
	// end inline asm
	ld.global.nc.u32 	%r137, [%rd67+384];
	// begin inline asm
	{mul.f16x2 %r135,%r124,%r137;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r138,%r114,%r135;
}
	// end inline asm
	ld.global.nc.u32 	%r143, [%rd68+384];
	// begin inline asm
	{mul.f16x2 %r141,%r124,%r143;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r144,%r120,%r141;
}
	// end inline asm
	ld.global.nc.u32 	%r148, [%rd83];
	ld.global.nc.u32 	%r149, [%rd65+768];
	// begin inline asm
	{mul.f16x2 %r147,%r148,%r149;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r150,%r126,%r147;
}
	// end inline asm
	ld.global.nc.u32 	%r155, [%rd69+384];
	// begin inline asm
	{mul.f16x2 %r153,%r148,%r155;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r156,%r132,%r153;
}
	// end inline asm
	ld.global.nc.u32 	%r161, [%rd67+768];
	// begin inline asm
	{mul.f16x2 %r159,%r148,%r161;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r162,%r138,%r159;
}
	// end inline asm
	ld.global.nc.u32 	%r167, [%rd68+768];
	// begin inline asm
	{mul.f16x2 %r165,%r148,%r167;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r168,%r144,%r165;
}
	// end inline asm
	ld.global.nc.u32 	%r172, [%rd83+384];
	ld.global.nc.u32 	%r173, [%rd65+1152];
	// begin inline asm
	{mul.f16x2 %r171,%r172,%r173;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r366,%r150,%r171;
}
	// end inline asm
	ld.global.nc.u32 	%r179, [%rd69+768];
	// begin inline asm
	{mul.f16x2 %r177,%r172,%r179;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r365,%r156,%r177;
}
	// end inline asm
	ld.global.nc.u32 	%r185, [%rd67+1152];
	// begin inline asm
	{mul.f16x2 %r183,%r172,%r185;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r364,%r162,%r183;
}
	// end inline asm
	ld.global.nc.u32 	%r191, [%rd68+1152];
	// begin inline asm
	{mul.f16x2 %r189,%r172,%r191;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r363,%r168,%r189;
}
	// end inline asm
	add.s64 	%rd84, %rd84, 1536;
	add.s64 	%rd83, %rd83, 1536;
	add.s32 	%r357, %r357, 384;
	setp.lt.s32 	%p6, %r357, %r40;
	@%p6 bra 	$L__BB83_8;

$L__BB83_9:
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r366;
  cvt.f32.f16 %f6, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r366;
  cvt.f32.f16 %f7, high;}

	// end inline asm
	add.f32 	%f14, %f6, %f7;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r365;
  cvt.f32.f16 %f8, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r365;
  cvt.f32.f16 %f9, high;}

	// end inline asm
	add.f32 	%f1, %f8, %f9;
	st.local.f32 	[%rd3+4], %f1;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r364;
  cvt.f32.f16 %f10, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r364;
  cvt.f32.f16 %f11, high;}

	// end inline asm
	add.f32 	%f2, %f10, %f11;
	st.local.f32 	[%rd3+8], %f2;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r363;
  cvt.f32.f16 %f12, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r363;
  cvt.f32.f16 %f13, high;}

	// end inline asm
	add.f32 	%f3, %f12, %f13;
	st.local.f32 	[%rd3+12], %f3;
	shr.s32 	%r203, %r3, 31;
	shr.u32 	%r204, %r203, 27;
	add.s32 	%r205, %r3, %r204;
	shr.s32 	%r206, %r205, 5;
	shl.b32 	%r207, %r206, 2;
	add.s32 	%r39, %r53, %r207;
	mov.u32 	%r209, 2;
	mov.b32 	%r210, %f14;
	mov.u32 	%r211, 31;
	mov.u32 	%r212, 16;
	mov.u32 	%r213, -1;
	shfl.sync.bfly.b32 	%r214|%p7, %r210, %r212, %r211, %r213;
	mov.b32 	%f15, %r214;
	add.f32 	%f16, %f14, %f15;
	mov.b32 	%r215, %f16;
	mov.u32 	%r216, 8;
	shfl.sync.bfly.b32 	%r217|%p8, %r215, %r216, %r211, %r213;
	mov.b32 	%f17, %r217;
	add.f32 	%f18, %f16, %f17;
	mov.b32 	%r218, %f18;
	mov.u32 	%r219, 4;
	shfl.sync.bfly.b32 	%r220|%p9, %r218, %r219, %r211, %r213;
	mov.b32 	%f19, %r220;
	add.f32 	%f20, %f18, %f19;
	mov.b32 	%r221, %f20;
	shfl.sync.bfly.b32 	%r222|%p10, %r221, %r209, %r211, %r213;
	mov.b32 	%f21, %r222;
	add.f32 	%f22, %f20, %f21;
	mov.b32 	%r223, %f22;
	mov.u32 	%r224, 1;
	shfl.sync.bfly.b32 	%r225|%p11, %r223, %r224, %r211, %r213;
	mov.b32 	%f23, %r225;
	add.f32 	%f24, %f22, %f23;
	st.local.f32 	[%rd3], %f24;
	st.shared.f32 	[%r39], %f24;
	bar.sync 	0;
	@%p1 bra 	$L__BB83_11;

	ld.shared.f32 	%f25, [%r4];
	mov.b32 	%r226, %f25;
	shfl.sync.bfly.b32 	%r230|%p13, %r226, %r212, %r211, %r213;
	mov.b32 	%f26, %r230;
	add.f32 	%f27, %f25, %f26;
	mov.b32 	%r231, %f27;
	shfl.sync.bfly.b32 	%r233|%p14, %r231, %r216, %r211, %r213;
	mov.b32 	%f28, %r233;
	add.f32 	%f29, %f27, %f28;
	mov.b32 	%r234, %f29;
	shfl.sync.bfly.b32 	%r236|%p15, %r234, %r219, %r211, %r213;
	mov.b32 	%f30, %r236;
	add.f32 	%f31, %f29, %f30;
	mov.b32 	%r237, %f31;
	shfl.sync.bfly.b32 	%r239|%p16, %r237, %r209, %r211, %r213;
	mov.b32 	%f32, %r239;
	add.f32 	%f33, %f31, %f32;
	mov.b32 	%r240, %f33;
	shfl.sync.bfly.b32 	%r242|%p17, %r240, %r224, %r211, %r213;
	mov.b32 	%f34, %r242;
	add.f32 	%f35, %f33, %f34;
	st.local.f32 	[%rd3], %f35;

$L__BB83_11:
	bar.sync 	0;
	mov.b32 	%r243, %f1;
	shfl.sync.bfly.b32 	%r247|%p19, %r243, %r212, %r211, %r213;
	mov.b32 	%f36, %r247;
	add.f32 	%f37, %f1, %f36;
	mov.b32 	%r248, %f37;
	shfl.sync.bfly.b32 	%r250|%p20, %r248, %r216, %r211, %r213;
	mov.b32 	%f38, %r250;
	add.f32 	%f39, %f37, %f38;
	mov.b32 	%r251, %f39;
	shfl.sync.bfly.b32 	%r253|%p21, %r251, %r219, %r211, %r213;
	mov.b32 	%f40, %r253;
	add.f32 	%f41, %f39, %f40;
	mov.b32 	%r254, %f41;
	shfl.sync.bfly.b32 	%r256|%p22, %r254, %r209, %r211, %r213;
	mov.b32 	%f42, %r256;
	add.f32 	%f43, %f41, %f42;
	mov.b32 	%r257, %f43;
	shfl.sync.bfly.b32 	%r259|%p23, %r257, %r224, %r211, %r213;
	mov.b32 	%f44, %r259;
	add.f32 	%f45, %f43, %f44;
	st.local.f32 	[%rd3+4], %f45;
	st.shared.f32 	[%r39], %f45;
	bar.sync 	0;
	@%p1 bra 	$L__BB83_13;

	ld.shared.f32 	%f46, [%r4];
	mov.b32 	%r260, %f46;
	mov.u32 	%r261, 31;
	mov.u32 	%r262, 16;
	mov.u32 	%r263, -1;
	shfl.sync.bfly.b32 	%r264|%p24, %r260, %r262, %r261, %r263;
	mov.b32 	%f47, %r264;
	add.f32 	%f48, %f46, %f47;
	mov.b32 	%r265, %f48;
	mov.u32 	%r266, 8;
	shfl.sync.bfly.b32 	%r267|%p25, %r265, %r266, %r261, %r263;
	mov.b32 	%f49, %r267;
	add.f32 	%f50, %f48, %f49;
	mov.b32 	%r268, %f50;
	mov.u32 	%r269, 4;
	shfl.sync.bfly.b32 	%r270|%p26, %r268, %r269, %r261, %r263;
	mov.b32 	%f51, %r270;
	add.f32 	%f52, %f50, %f51;
	mov.b32 	%r271, %f52;
	mov.u32 	%r272, 2;
	shfl.sync.bfly.b32 	%r273|%p27, %r271, %r272, %r261, %r263;
	mov.b32 	%f53, %r273;
	add.f32 	%f54, %f52, %f53;
	mov.b32 	%r274, %f54;
	mov.u32 	%r275, 1;
	shfl.sync.bfly.b32 	%r276|%p28, %r274, %r275, %r261, %r263;
	mov.b32 	%f55, %r276;
	add.f32 	%f56, %f54, %f55;
	st.local.f32 	[%rd3+4], %f56;

$L__BB83_13:
	bar.sync 	0;
	mov.b32 	%r277, %f2;
	mov.u32 	%r278, 31;
	mov.u32 	%r279, 16;
	mov.u32 	%r280, -1;
	shfl.sync.bfly.b32 	%r281|%p30, %r277, %r279, %r278, %r280;
	mov.b32 	%f57, %r281;
	add.f32 	%f58, %f2, %f57;
	mov.b32 	%r282, %f58;
	mov.u32 	%r283, 8;
	shfl.sync.bfly.b32 	%r284|%p31, %r282, %r283, %r278, %r280;
	mov.b32 	%f59, %r284;
	add.f32 	%f60, %f58, %f59;
	mov.b32 	%r285, %f60;
	mov.u32 	%r286, 4;
	shfl.sync.bfly.b32 	%r287|%p32, %r285, %r286, %r278, %r280;
	mov.b32 	%f61, %r287;
	add.f32 	%f62, %f60, %f61;
	mov.b32 	%r288, %f62;
	mov.u32 	%r289, 2;
	shfl.sync.bfly.b32 	%r290|%p33, %r288, %r289, %r278, %r280;
	mov.b32 	%f63, %r290;
	add.f32 	%f64, %f62, %f63;
	mov.b32 	%r291, %f64;
	mov.u32 	%r292, 1;
	shfl.sync.bfly.b32 	%r293|%p34, %r291, %r292, %r278, %r280;
	mov.b32 	%f65, %r293;
	add.f32 	%f66, %f64, %f65;
	st.local.f32 	[%rd3+8], %f66;
	st.shared.f32 	[%r39], %f66;
	bar.sync 	0;
	@%p1 bra 	$L__BB83_15;

	ld.shared.f32 	%f67, [%r4];
	mov.b32 	%r294, %f67;
	shfl.sync.bfly.b32 	%r298|%p35, %r294, %r279, %r278, %r280;
	mov.b32 	%f68, %r298;
	add.f32 	%f69, %f67, %f68;
	mov.b32 	%r299, %f69;
	shfl.sync.bfly.b32 	%r301|%p36, %r299, %r283, %r278, %r280;
	mov.b32 	%f70, %r301;
	add.f32 	%f71, %f69, %f70;
	mov.b32 	%r302, %f71;
	shfl.sync.bfly.b32 	%r304|%p37, %r302, %r286, %r278, %r280;
	mov.b32 	%f72, %r304;
	add.f32 	%f73, %f71, %f72;
	mov.b32 	%r305, %f73;
	shfl.sync.bfly.b32 	%r307|%p38, %r305, %r289, %r278, %r280;
	mov.b32 	%f74, %r307;
	add.f32 	%f75, %f73, %f74;
	mov.b32 	%r308, %f75;
	shfl.sync.bfly.b32 	%r310|%p39, %r308, %r292, %r278, %r280;
	mov.b32 	%f76, %r310;
	add.f32 	%f77, %f75, %f76;
	st.local.f32 	[%rd3+8], %f77;

$L__BB83_15:
	bar.sync 	0;
	mov.b32 	%r311, %f3;
	shfl.sync.bfly.b32 	%r315|%p41, %r311, %r279, %r278, %r280;
	mov.b32 	%f78, %r315;
	add.f32 	%f79, %f3, %f78;
	mov.b32 	%r316, %f79;
	shfl.sync.bfly.b32 	%r318|%p42, %r316, %r283, %r278, %r280;
	mov.b32 	%f80, %r318;
	add.f32 	%f81, %f79, %f80;
	mov.b32 	%r319, %f81;
	shfl.sync.bfly.b32 	%r321|%p43, %r319, %r286, %r278, %r280;
	mov.b32 	%f82, %r321;
	add.f32 	%f83, %f81, %f82;
	mov.b32 	%r322, %f83;
	shfl.sync.bfly.b32 	%r324|%p44, %r322, %r289, %r278, %r280;
	mov.b32 	%f84, %r324;
	add.f32 	%f85, %f83, %f84;
	mov.b32 	%r325, %f85;
	shfl.sync.bfly.b32 	%r327|%p45, %r325, %r292, %r278, %r280;
	mov.b32 	%f86, %r327;
	add.f32 	%f87, %f85, %f86;
	st.local.f32 	[%rd3+12], %f87;
	st.shared.f32 	[%r39], %f87;
	bar.sync 	0;
	@%p1 bra 	$L__BB83_17;

	ld.shared.f32 	%f88, [%r4];
	mov.b32 	%r328, %f88;
	mov.u32 	%r329, 31;
	mov.u32 	%r330, 16;
	mov.u32 	%r331, -1;
	shfl.sync.bfly.b32 	%r332|%p46, %r328, %r330, %r329, %r331;
	mov.b32 	%f89, %r332;
	add.f32 	%f90, %f88, %f89;
	mov.b32 	%r333, %f90;
	mov.u32 	%r334, 8;
	shfl.sync.bfly.b32 	%r335|%p47, %r333, %r334, %r329, %r331;
	mov.b32 	%f91, %r335;
	add.f32 	%f92, %f90, %f91;
	mov.b32 	%r336, %f92;
	mov.u32 	%r337, 4;
	shfl.sync.bfly.b32 	%r338|%p48, %r336, %r337, %r329, %r331;
	mov.b32 	%f93, %r338;
	add.f32 	%f94, %f92, %f93;
	mov.b32 	%r339, %f94;
	mov.u32 	%r340, 2;
	shfl.sync.bfly.b32 	%r341|%p49, %r339, %r340, %r329, %r331;
	mov.b32 	%f95, %r341;
	add.f32 	%f96, %f94, %f95;
	mov.b32 	%r342, %f96;
	mov.u32 	%r343, 1;
	shfl.sync.bfly.b32 	%r344|%p50, %r342, %r343, %r329, %r331;
	mov.b32 	%f97, %r344;
	add.f32 	%f98, %f96, %f97;
	st.local.f32 	[%rd3+12], %f98;

$L__BB83_17:
	bar.sync 	0;
	setp.gt.s32 	%p51, %r3, 3;
	@%p51 bra 	$L__BB83_19;

	mad.lo.s32 	%r345, %r3, %r42, %r2;
	cvt.s64.s32 	%rd70, %r345;
	mul.lo.s32 	%r346, %r1, %r43;
	cvt.s64.s32 	%rd71, %r346;
	add.s64 	%rd72, %rd71, %rd70;
	mul.wide.s32 	%rd73, %r3, 4;
	add.s64 	%rd74, %rd3, %rd73;
	ld.local.f32 	%f99, [%rd74];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs3, %f99;}

	// end inline asm
	cvta.to.global.u64 	%rd75, %rd33;
	shl.b64 	%rd76, %rd72, 1;
	add.s64 	%rd77, %rd75, %rd76;
	st.global.u16 	[%rd77], %rs3;

$L__BB83_19:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_5_bs_96
.visible .entry ggml_matvec_f16_ncols_5_bs_96(
	.param .u64 ggml_matvec_f16_ncols_5_bs_96_param_0,
	.param .u64 ggml_matvec_f16_ncols_5_bs_96_param_1,
	.param .u64 ggml_matvec_f16_ncols_5_bs_96_param_2,
	.param .u32 ggml_matvec_f16_ncols_5_bs_96_param_3,
	.param .u32 ggml_matvec_f16_ncols_5_bs_96_param_4,
	.param .u32 ggml_matvec_f16_ncols_5_bs_96_param_5,
	.param .u32 ggml_matvec_f16_ncols_5_bs_96_param_6,
	.param .u32 ggml_matvec_f16_ncols_5_bs_96_param_7,
	.param .u32 ggml_matvec_f16_ncols_5_bs_96_param_8,
	.param .u32 ggml_matvec_f16_ncols_5_bs_96_param_9,
	.param .u32 ggml_matvec_f16_ncols_5_bs_96_param_10,
	.param .u32 ggml_matvec_f16_ncols_5_bs_96_param_11
)
{
	.local .align 4 .b8 	__local_depot84[20];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<63>;
	.reg .b16 	%rs<4>;
	.reg .f32 	%f<124>;
	.reg .b32 	%r<447>;
	.reg .b64 	%rd<76>;


	mov.u64 	%SPL, __local_depot84;
	ld.param.u64 	%rd28, [ggml_matvec_f16_ncols_5_bs_96_param_0];
	ld.param.u64 	%rd29, [ggml_matvec_f16_ncols_5_bs_96_param_1];
	ld.param.u64 	%rd27, [ggml_matvec_f16_ncols_5_bs_96_param_2];
	ld.param.u32 	%r46, [ggml_matvec_f16_ncols_5_bs_96_param_3];
	ld.param.u32 	%r50, [ggml_matvec_f16_ncols_5_bs_96_param_5];
	ld.param.u32 	%r47, [ggml_matvec_f16_ncols_5_bs_96_param_6];
	ld.param.u32 	%r48, [ggml_matvec_f16_ncols_5_bs_96_param_7];
	ld.param.u32 	%r51, [ggml_matvec_f16_ncols_5_bs_96_param_8];
	ld.param.u32 	%r52, [ggml_matvec_f16_ncols_5_bs_96_param_9];
	ld.param.u32 	%r53, [ggml_matvec_f16_ncols_5_bs_96_param_10];
	ld.param.u32 	%r49, [ggml_matvec_f16_ncols_5_bs_96_param_11];
	cvta.to.global.u64 	%rd75, %rd29;
	cvta.to.global.u64 	%rd2, %rd28;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r54, %r1, %r51;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r55, %r2, %r50;
	mad.lo.s32 	%r56, %r54, %r52, %r55;
	cvt.s64.s32 	%rd4, %r56;
	mul.lo.s32 	%r57, %r1, %r53;
	cvt.s64.s32 	%rd5, %r57;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r58, %r3, 2;
	mov.u32 	%r59, data_mmv;
	add.s32 	%r4, %r59, %r58;
	@%p1 bra 	$L__BB84_2;

	mov.u32 	%r60, 0;
	st.shared.u32 	[%r4], %r60;

$L__BB84_2:
	bar.sync 	0;
	mov.f32 	%f6, 0f00000000;
	mov.u32 	%r442, 0;
	st.local.u32 	[%rd3], %r442;
	st.local.u32 	[%rd3+4], %r442;
	st.local.u32 	[%rd3+8], %r442;
	st.local.u32 	[%rd3+12], %r442;
	st.local.u32 	[%rd3+16], %r442;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs2, %f6;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f6;}

	// end inline asm
	mov.b32 	%r446, {%rs1, %rs2};
	setp.ge.s32 	%p2, %r3, %r46;
	mov.u32 	%r443, %r442;
	mov.u32 	%r444, %r442;
	mov.u32 	%r445, %r442;
	@%p2 bra 	$L__BB84_9;

	not.b32 	%r69, %r3;
	add.s32 	%r6, %r69, %r46;
	mul.wide.u32 	%rd31, %r6, -1431655765;
	shr.u64 	%rd32, %rd31, 38;
	cvt.u32.u64 	%r70, %rd32;
	add.s32 	%r71, %r70, 1;
	and.b32  	%r429, %r71, 3;
	setp.eq.s32 	%p3, %r429, 0;
	mov.u32 	%r442, 0;
	mov.u32 	%r435, %r3;
	@%p3 bra 	$L__BB84_6;

	shl.b32 	%r76, %r47, 1;
	mad.lo.s32 	%r77, %r47, 3, %r3;
	mul.wide.s32 	%rd33, %r77, 4;
	shl.b64 	%rd34, %rd5, 1;
	add.s64 	%rd7, %rd33, %rd34;
	mul.wide.s32 	%rd35, %r3, 4;
	mul.wide.s32 	%rd36, %r47, 4;
	add.s64 	%rd37, %rd35, %rd36;
	add.s64 	%rd8, %rd37, %rd34;
	add.s64 	%rd9, %rd35, %rd34;
	mul.wide.s32 	%rd38, %r3, 2;
	add.s64 	%rd39, %rd38, %rd4;
	shl.b64 	%rd40, %rd39, 1;
	add.s64 	%rd72, %rd2, %rd40;
	mul.wide.s32 	%rd11, %r76, 4;
	mov.u32 	%r442, 0;
	mov.u64 	%rd73, %rd75;
	mov.u32 	%r443, %r442;
	mov.u32 	%r444, %r442;
	mov.u32 	%r445, %r442;
	mov.u32 	%r435, %r3;

$L__BB84_5:
	.pragma "nounroll";
	ld.global.nc.u32 	%r79, [%rd72];
	add.s64 	%rd41, %rd73, %rd9;
	ld.global.nc.u32 	%r80, [%rd41];
	// begin inline asm
	{mul.f16x2 %r78,%r79,%r80;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r446,%r446,%r78;
}
	// end inline asm
	add.s64 	%rd42, %rd73, %rd8;
	ld.global.nc.u32 	%r86, [%rd42];
	// begin inline asm
	{mul.f16x2 %r84,%r79,%r86;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r445,%r445,%r84;
}
	// end inline asm
	add.s64 	%rd43, %rd41, %rd11;
	ld.global.nc.u32 	%r92, [%rd43];
	// begin inline asm
	{mul.f16x2 %r90,%r79,%r92;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r444,%r444,%r90;
}
	// end inline asm
	add.s64 	%rd44, %rd73, %rd7;
	ld.global.nc.u32 	%r98, [%rd44];
	// begin inline asm
	{mul.f16x2 %r96,%r79,%r98;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r443,%r443,%r96;
}
	// end inline asm
	add.s64 	%rd45, %rd43, %rd11;
	ld.global.nc.u32 	%r104, [%rd45];
	// begin inline asm
	{mul.f16x2 %r102,%r79,%r104;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r442,%r442,%r102;
}
	// end inline asm
	add.s32 	%r435, %r435, 96;
	add.s64 	%rd73, %rd73, 384;
	add.s64 	%rd72, %rd72, 384;
	add.s32 	%r429, %r429, -1;
	setp.ne.s32 	%p4, %r429, 0;
	@%p4 bra 	$L__BB84_5;

$L__BB84_6:
	setp.lt.u32 	%p5, %r6, 288;
	@%p5 bra 	$L__BB84_9;

	add.s32 	%r108, %r435, %r47;
	shl.b32 	%r109, %r47, 1;
	add.s32 	%r110, %r435, %r109;
	mad.lo.s32 	%r111, %r47, 3, %r435;
	shl.b32 	%r112, %r47, 2;
	add.s32 	%r113, %r435, %r112;
	add.s32 	%r114, %r108, 96;
	mul.wide.s32 	%rd46, %r114, 4;
	shl.b64 	%rd47, %rd5, 1;
	add.s64 	%rd16, %rd46, %rd47;
	mul.wide.s32 	%rd48, %r110, 4;
	add.s64 	%rd17, %rd48, %rd47;
	mul.wide.s32 	%rd49, %r111, 4;
	add.s64 	%rd18, %rd49, %rd47;
	mul.wide.s32 	%rd50, %r113, 4;
	add.s64 	%rd19, %rd50, %rd47;
	mul.wide.s32 	%rd51, %r435, 2;
	add.s64 	%rd52, %rd51, %rd4;
	shl.b64 	%rd53, %rd52, 1;
	add.s64 	%rd54, %rd2, %rd53;
	add.s64 	%rd74, %rd54, 768;
	mul.wide.s32 	%rd55, %r435, 4;
	add.s64 	%rd21, %rd55, %rd47;
	mul.wide.s32 	%rd56, %r47, 4;
	add.s64 	%rd57, %rd55, %rd56;
	add.s64 	%rd22, %rd57, %rd47;

$L__BB84_8:
	ld.global.nc.u32 	%r116, [%rd74+-768];
	add.s64 	%rd58, %rd75, %rd21;
	ld.global.nc.u32 	%r117, [%rd58];
	// begin inline asm
	{mul.f16x2 %r115,%r116,%r117;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r118,%r446,%r115;
}
	// end inline asm
	add.s64 	%rd59, %rd75, %rd22;
	ld.global.nc.u32 	%r123, [%rd59];
	// begin inline asm
	{mul.f16x2 %r121,%r116,%r123;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r124,%r445,%r121;
}
	// end inline asm
	add.s64 	%rd60, %rd75, %rd17;
	ld.global.nc.u32 	%r129, [%rd60];
	// begin inline asm
	{mul.f16x2 %r127,%r116,%r129;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r130,%r444,%r127;
}
	// end inline asm
	add.s64 	%rd61, %rd75, %rd18;
	ld.global.nc.u32 	%r135, [%rd61];
	// begin inline asm
	{mul.f16x2 %r133,%r116,%r135;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r136,%r443,%r133;
}
	// end inline asm
	add.s64 	%rd62, %rd75, %rd19;
	ld.global.nc.u32 	%r141, [%rd62];
	// begin inline asm
	{mul.f16x2 %r139,%r116,%r141;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r142,%r442,%r139;
}
	// end inline asm
	ld.global.nc.u32 	%r146, [%rd74+-384];
	ld.global.nc.u32 	%r147, [%rd58+384];
	// begin inline asm
	{mul.f16x2 %r145,%r146,%r147;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r148,%r118,%r145;
}
	// end inline asm
	add.s64 	%rd63, %rd75, %rd16;
	ld.global.nc.u32 	%r153, [%rd63];
	// begin inline asm
	{mul.f16x2 %r151,%r146,%r153;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r154,%r124,%r151;
}
	// end inline asm
	ld.global.nc.u32 	%r159, [%rd60+384];
	// begin inline asm
	{mul.f16x2 %r157,%r146,%r159;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r160,%r130,%r157;
}
	// end inline asm
	ld.global.nc.u32 	%r165, [%rd61+384];
	// begin inline asm
	{mul.f16x2 %r163,%r146,%r165;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r166,%r136,%r163;
}
	// end inline asm
	ld.global.nc.u32 	%r171, [%rd62+384];
	// begin inline asm
	{mul.f16x2 %r169,%r146,%r171;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r172,%r142,%r169;
}
	// end inline asm
	ld.global.nc.u32 	%r176, [%rd74];
	ld.global.nc.u32 	%r177, [%rd58+768];
	// begin inline asm
	{mul.f16x2 %r175,%r176,%r177;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r178,%r148,%r175;
}
	// end inline asm
	ld.global.nc.u32 	%r183, [%rd63+384];
	// begin inline asm
	{mul.f16x2 %r181,%r176,%r183;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r184,%r154,%r181;
}
	// end inline asm
	ld.global.nc.u32 	%r189, [%rd60+768];
	// begin inline asm
	{mul.f16x2 %r187,%r176,%r189;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r190,%r160,%r187;
}
	// end inline asm
	ld.global.nc.u32 	%r195, [%rd61+768];
	// begin inline asm
	{mul.f16x2 %r193,%r176,%r195;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r196,%r166,%r193;
}
	// end inline asm
	ld.global.nc.u32 	%r201, [%rd62+768];
	// begin inline asm
	{mul.f16x2 %r199,%r176,%r201;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r202,%r172,%r199;
}
	// end inline asm
	ld.global.nc.u32 	%r206, [%rd74+384];
	ld.global.nc.u32 	%r207, [%rd58+1152];
	// begin inline asm
	{mul.f16x2 %r205,%r206,%r207;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r446,%r178,%r205;
}
	// end inline asm
	ld.global.nc.u32 	%r213, [%rd63+768];
	// begin inline asm
	{mul.f16x2 %r211,%r206,%r213;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r445,%r184,%r211;
}
	// end inline asm
	ld.global.nc.u32 	%r219, [%rd60+1152];
	// begin inline asm
	{mul.f16x2 %r217,%r206,%r219;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r444,%r190,%r217;
}
	// end inline asm
	ld.global.nc.u32 	%r225, [%rd61+1152];
	// begin inline asm
	{mul.f16x2 %r223,%r206,%r225;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r443,%r196,%r223;
}
	// end inline asm
	ld.global.nc.u32 	%r231, [%rd62+1152];
	// begin inline asm
	{mul.f16x2 %r229,%r206,%r231;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r442,%r202,%r229;
}
	// end inline asm
	add.s64 	%rd75, %rd75, 1536;
	add.s64 	%rd74, %rd74, 1536;
	add.s32 	%r435, %r435, 384;
	setp.lt.s32 	%p6, %r435, %r46;
	@%p6 bra 	$L__BB84_8;

$L__BB84_9:
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r446;
  cvt.f32.f16 %f7, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r446;
  cvt.f32.f16 %f8, high;}

	// end inline asm
	add.f32 	%f17, %f7, %f8;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r445;
  cvt.f32.f16 %f9, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r445;
  cvt.f32.f16 %f10, high;}

	// end inline asm
	add.f32 	%f1, %f9, %f10;
	st.local.f32 	[%rd3+4], %f1;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r444;
  cvt.f32.f16 %f11, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r444;
  cvt.f32.f16 %f12, high;}

	// end inline asm
	add.f32 	%f2, %f11, %f12;
	st.local.f32 	[%rd3+8], %f2;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r443;
  cvt.f32.f16 %f13, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r443;
  cvt.f32.f16 %f14, high;}

	// end inline asm
	add.f32 	%f3, %f13, %f14;
	st.local.f32 	[%rd3+12], %f3;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r442;
  cvt.f32.f16 %f15, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r442;
  cvt.f32.f16 %f16, high;}

	// end inline asm
	add.f32 	%f4, %f15, %f16;
	st.local.f32 	[%rd3+16], %f4;
	shr.s32 	%r245, %r3, 31;
	shr.u32 	%r246, %r245, 27;
	add.s32 	%r247, %r3, %r246;
	shr.s32 	%r248, %r247, 5;
	shl.b32 	%r249, %r248, 2;
	add.s32 	%r45, %r59, %r249;
	mov.u32 	%r251, 2;
	mov.b32 	%r252, %f17;
	mov.u32 	%r253, 31;
	mov.u32 	%r254, 16;
	mov.u32 	%r255, -1;
	shfl.sync.bfly.b32 	%r256|%p7, %r252, %r254, %r253, %r255;
	mov.b32 	%f18, %r256;
	add.f32 	%f19, %f17, %f18;
	mov.b32 	%r257, %f19;
	mov.u32 	%r258, 8;
	shfl.sync.bfly.b32 	%r259|%p8, %r257, %r258, %r253, %r255;
	mov.b32 	%f20, %r259;
	add.f32 	%f21, %f19, %f20;
	mov.b32 	%r260, %f21;
	mov.u32 	%r261, 4;
	shfl.sync.bfly.b32 	%r262|%p9, %r260, %r261, %r253, %r255;
	mov.b32 	%f22, %r262;
	add.f32 	%f23, %f21, %f22;
	mov.b32 	%r263, %f23;
	shfl.sync.bfly.b32 	%r264|%p10, %r263, %r251, %r253, %r255;
	mov.b32 	%f24, %r264;
	add.f32 	%f25, %f23, %f24;
	mov.b32 	%r265, %f25;
	mov.u32 	%r266, 1;
	shfl.sync.bfly.b32 	%r267|%p11, %r265, %r266, %r253, %r255;
	mov.b32 	%f26, %r267;
	add.f32 	%f27, %f25, %f26;
	st.local.f32 	[%rd3], %f27;
	st.shared.f32 	[%r45], %f27;
	bar.sync 	0;
	@%p1 bra 	$L__BB84_11;

	ld.shared.f32 	%f28, [%r4];
	mov.b32 	%r268, %f28;
	shfl.sync.bfly.b32 	%r272|%p13, %r268, %r254, %r253, %r255;
	mov.b32 	%f29, %r272;
	add.f32 	%f30, %f28, %f29;
	mov.b32 	%r273, %f30;
	shfl.sync.bfly.b32 	%r275|%p14, %r273, %r258, %r253, %r255;
	mov.b32 	%f31, %r275;
	add.f32 	%f32, %f30, %f31;
	mov.b32 	%r276, %f32;
	shfl.sync.bfly.b32 	%r278|%p15, %r276, %r261, %r253, %r255;
	mov.b32 	%f33, %r278;
	add.f32 	%f34, %f32, %f33;
	mov.b32 	%r279, %f34;
	shfl.sync.bfly.b32 	%r281|%p16, %r279, %r251, %r253, %r255;
	mov.b32 	%f35, %r281;
	add.f32 	%f36, %f34, %f35;
	mov.b32 	%r282, %f36;
	shfl.sync.bfly.b32 	%r284|%p17, %r282, %r266, %r253, %r255;
	mov.b32 	%f37, %r284;
	add.f32 	%f38, %f36, %f37;
	st.local.f32 	[%rd3], %f38;

$L__BB84_11:
	bar.sync 	0;
	mov.b32 	%r285, %f1;
	shfl.sync.bfly.b32 	%r289|%p19, %r285, %r254, %r253, %r255;
	mov.b32 	%f39, %r289;
	add.f32 	%f40, %f1, %f39;
	mov.b32 	%r290, %f40;
	shfl.sync.bfly.b32 	%r292|%p20, %r290, %r258, %r253, %r255;
	mov.b32 	%f41, %r292;
	add.f32 	%f42, %f40, %f41;
	mov.b32 	%r293, %f42;
	shfl.sync.bfly.b32 	%r295|%p21, %r293, %r261, %r253, %r255;
	mov.b32 	%f43, %r295;
	add.f32 	%f44, %f42, %f43;
	mov.b32 	%r296, %f44;
	shfl.sync.bfly.b32 	%r298|%p22, %r296, %r251, %r253, %r255;
	mov.b32 	%f45, %r298;
	add.f32 	%f46, %f44, %f45;
	mov.b32 	%r299, %f46;
	shfl.sync.bfly.b32 	%r301|%p23, %r299, %r266, %r253, %r255;
	mov.b32 	%f47, %r301;
	add.f32 	%f48, %f46, %f47;
	st.local.f32 	[%rd3+4], %f48;
	st.shared.f32 	[%r45], %f48;
	bar.sync 	0;
	@%p1 bra 	$L__BB84_13;

	ld.shared.f32 	%f49, [%r4];
	mov.b32 	%r302, %f49;
	mov.u32 	%r303, 31;
	mov.u32 	%r304, 16;
	mov.u32 	%r305, -1;
	shfl.sync.bfly.b32 	%r306|%p24, %r302, %r304, %r303, %r305;
	mov.b32 	%f50, %r306;
	add.f32 	%f51, %f49, %f50;
	mov.b32 	%r307, %f51;
	mov.u32 	%r308, 8;
	shfl.sync.bfly.b32 	%r309|%p25, %r307, %r308, %r303, %r305;
	mov.b32 	%f52, %r309;
	add.f32 	%f53, %f51, %f52;
	mov.b32 	%r310, %f53;
	mov.u32 	%r311, 4;
	shfl.sync.bfly.b32 	%r312|%p26, %r310, %r311, %r303, %r305;
	mov.b32 	%f54, %r312;
	add.f32 	%f55, %f53, %f54;
	mov.b32 	%r313, %f55;
	mov.u32 	%r314, 2;
	shfl.sync.bfly.b32 	%r315|%p27, %r313, %r314, %r303, %r305;
	mov.b32 	%f56, %r315;
	add.f32 	%f57, %f55, %f56;
	mov.b32 	%r316, %f57;
	mov.u32 	%r317, 1;
	shfl.sync.bfly.b32 	%r318|%p28, %r316, %r317, %r303, %r305;
	mov.b32 	%f58, %r318;
	add.f32 	%f59, %f57, %f58;
	st.local.f32 	[%rd3+4], %f59;

$L__BB84_13:
	bar.sync 	0;
	mov.b32 	%r319, %f2;
	mov.u32 	%r320, 31;
	mov.u32 	%r321, 16;
	mov.u32 	%r322, -1;
	shfl.sync.bfly.b32 	%r323|%p30, %r319, %r321, %r320, %r322;
	mov.b32 	%f60, %r323;
	add.f32 	%f61, %f2, %f60;
	mov.b32 	%r324, %f61;
	mov.u32 	%r325, 8;
	shfl.sync.bfly.b32 	%r326|%p31, %r324, %r325, %r320, %r322;
	mov.b32 	%f62, %r326;
	add.f32 	%f63, %f61, %f62;
	mov.b32 	%r327, %f63;
	mov.u32 	%r328, 4;
	shfl.sync.bfly.b32 	%r329|%p32, %r327, %r328, %r320, %r322;
	mov.b32 	%f64, %r329;
	add.f32 	%f65, %f63, %f64;
	mov.b32 	%r330, %f65;
	mov.u32 	%r331, 2;
	shfl.sync.bfly.b32 	%r332|%p33, %r330, %r331, %r320, %r322;
	mov.b32 	%f66, %r332;
	add.f32 	%f67, %f65, %f66;
	mov.b32 	%r333, %f67;
	mov.u32 	%r334, 1;
	shfl.sync.bfly.b32 	%r335|%p34, %r333, %r334, %r320, %r322;
	mov.b32 	%f68, %r335;
	add.f32 	%f69, %f67, %f68;
	st.local.f32 	[%rd3+8], %f69;
	st.shared.f32 	[%r45], %f69;
	bar.sync 	0;
	@%p1 bra 	$L__BB84_15;

	ld.shared.f32 	%f70, [%r4];
	mov.b32 	%r336, %f70;
	shfl.sync.bfly.b32 	%r340|%p35, %r336, %r321, %r320, %r322;
	mov.b32 	%f71, %r340;
	add.f32 	%f72, %f70, %f71;
	mov.b32 	%r341, %f72;
	shfl.sync.bfly.b32 	%r343|%p36, %r341, %r325, %r320, %r322;
	mov.b32 	%f73, %r343;
	add.f32 	%f74, %f72, %f73;
	mov.b32 	%r344, %f74;
	shfl.sync.bfly.b32 	%r346|%p37, %r344, %r328, %r320, %r322;
	mov.b32 	%f75, %r346;
	add.f32 	%f76, %f74, %f75;
	mov.b32 	%r347, %f76;
	shfl.sync.bfly.b32 	%r349|%p38, %r347, %r331, %r320, %r322;
	mov.b32 	%f77, %r349;
	add.f32 	%f78, %f76, %f77;
	mov.b32 	%r350, %f78;
	shfl.sync.bfly.b32 	%r352|%p39, %r350, %r334, %r320, %r322;
	mov.b32 	%f79, %r352;
	add.f32 	%f80, %f78, %f79;
	st.local.f32 	[%rd3+8], %f80;

$L__BB84_15:
	bar.sync 	0;
	mov.b32 	%r353, %f3;
	shfl.sync.bfly.b32 	%r357|%p41, %r353, %r321, %r320, %r322;
	mov.b32 	%f81, %r357;
	add.f32 	%f82, %f3, %f81;
	mov.b32 	%r358, %f82;
	shfl.sync.bfly.b32 	%r360|%p42, %r358, %r325, %r320, %r322;
	mov.b32 	%f83, %r360;
	add.f32 	%f84, %f82, %f83;
	mov.b32 	%r361, %f84;
	shfl.sync.bfly.b32 	%r363|%p43, %r361, %r328, %r320, %r322;
	mov.b32 	%f85, %r363;
	add.f32 	%f86, %f84, %f85;
	mov.b32 	%r364, %f86;
	shfl.sync.bfly.b32 	%r366|%p44, %r364, %r331, %r320, %r322;
	mov.b32 	%f87, %r366;
	add.f32 	%f88, %f86, %f87;
	mov.b32 	%r367, %f88;
	shfl.sync.bfly.b32 	%r369|%p45, %r367, %r334, %r320, %r322;
	mov.b32 	%f89, %r369;
	add.f32 	%f90, %f88, %f89;
	st.local.f32 	[%rd3+12], %f90;
	st.shared.f32 	[%r45], %f90;
	bar.sync 	0;
	@%p1 bra 	$L__BB84_17;

	ld.shared.f32 	%f91, [%r4];
	mov.b32 	%r370, %f91;
	mov.u32 	%r371, 31;
	mov.u32 	%r372, 16;
	mov.u32 	%r373, -1;
	shfl.sync.bfly.b32 	%r374|%p46, %r370, %r372, %r371, %r373;
	mov.b32 	%f92, %r374;
	add.f32 	%f93, %f91, %f92;
	mov.b32 	%r375, %f93;
	mov.u32 	%r376, 8;
	shfl.sync.bfly.b32 	%r377|%p47, %r375, %r376, %r371, %r373;
	mov.b32 	%f94, %r377;
	add.f32 	%f95, %f93, %f94;
	mov.b32 	%r378, %f95;
	mov.u32 	%r379, 4;
	shfl.sync.bfly.b32 	%r380|%p48, %r378, %r379, %r371, %r373;
	mov.b32 	%f96, %r380;
	add.f32 	%f97, %f95, %f96;
	mov.b32 	%r381, %f97;
	mov.u32 	%r382, 2;
	shfl.sync.bfly.b32 	%r383|%p49, %r381, %r382, %r371, %r373;
	mov.b32 	%f98, %r383;
	add.f32 	%f99, %f97, %f98;
	mov.b32 	%r384, %f99;
	mov.u32 	%r385, 1;
	shfl.sync.bfly.b32 	%r386|%p50, %r384, %r385, %r371, %r373;
	mov.b32 	%f100, %r386;
	add.f32 	%f101, %f99, %f100;
	st.local.f32 	[%rd3+12], %f101;

$L__BB84_17:
	bar.sync 	0;
	mov.b32 	%r387, %f4;
	mov.u32 	%r388, 31;
	mov.u32 	%r389, 16;
	mov.u32 	%r390, -1;
	shfl.sync.bfly.b32 	%r391|%p52, %r387, %r389, %r388, %r390;
	mov.b32 	%f102, %r391;
	add.f32 	%f103, %f4, %f102;
	mov.b32 	%r392, %f103;
	mov.u32 	%r393, 8;
	shfl.sync.bfly.b32 	%r394|%p53, %r392, %r393, %r388, %r390;
	mov.b32 	%f104, %r394;
	add.f32 	%f105, %f103, %f104;
	mov.b32 	%r395, %f105;
	mov.u32 	%r396, 4;
	shfl.sync.bfly.b32 	%r397|%p54, %r395, %r396, %r388, %r390;
	mov.b32 	%f106, %r397;
	add.f32 	%f107, %f105, %f106;
	mov.b32 	%r398, %f107;
	mov.u32 	%r399, 2;
	shfl.sync.bfly.b32 	%r400|%p55, %r398, %r399, %r388, %r390;
	mov.b32 	%f108, %r400;
	add.f32 	%f109, %f107, %f108;
	mov.b32 	%r401, %f109;
	mov.u32 	%r402, 1;
	shfl.sync.bfly.b32 	%r403|%p56, %r401, %r402, %r388, %r390;
	mov.b32 	%f110, %r403;
	add.f32 	%f111, %f109, %f110;
	st.local.f32 	[%rd3+16], %f111;
	st.shared.f32 	[%r45], %f111;
	bar.sync 	0;
	@%p1 bra 	$L__BB84_19;

	ld.shared.f32 	%f112, [%r4];
	mov.b32 	%r404, %f112;
	shfl.sync.bfly.b32 	%r408|%p57, %r404, %r389, %r388, %r390;
	mov.b32 	%f113, %r408;
	add.f32 	%f114, %f112, %f113;
	mov.b32 	%r409, %f114;
	shfl.sync.bfly.b32 	%r411|%p58, %r409, %r393, %r388, %r390;
	mov.b32 	%f115, %r411;
	add.f32 	%f116, %f114, %f115;
	mov.b32 	%r412, %f116;
	shfl.sync.bfly.b32 	%r414|%p59, %r412, %r396, %r388, %r390;
	mov.b32 	%f117, %r414;
	add.f32 	%f118, %f116, %f117;
	mov.b32 	%r415, %f118;
	shfl.sync.bfly.b32 	%r417|%p60, %r415, %r399, %r388, %r390;
	mov.b32 	%f119, %r417;
	add.f32 	%f120, %f118, %f119;
	mov.b32 	%r418, %f120;
	shfl.sync.bfly.b32 	%r420|%p61, %r418, %r402, %r388, %r390;
	mov.b32 	%f121, %r420;
	add.f32 	%f122, %f120, %f121;
	st.local.f32 	[%rd3+16], %f122;

$L__BB84_19:
	bar.sync 	0;
	setp.gt.s32 	%p62, %r3, 4;
	@%p62 bra 	$L__BB84_21;

	mad.lo.s32 	%r421, %r3, %r48, %r2;
	cvt.s64.s32 	%rd64, %r421;
	mul.lo.s32 	%r422, %r1, %r49;
	cvt.s64.s32 	%rd65, %r422;
	add.s64 	%rd66, %rd65, %rd64;
	mul.wide.s32 	%rd67, %r3, 4;
	add.s64 	%rd68, %rd3, %rd67;
	ld.local.f32 	%f123, [%rd68];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs3, %f123;}

	// end inline asm
	cvta.to.global.u64 	%rd69, %rd27;
	shl.b64 	%rd70, %rd66, 1;
	add.s64 	%rd71, %rd69, %rd70;
	st.global.u16 	[%rd71], %rs3;

$L__BB84_21:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_6_bs_96
.visible .entry ggml_matvec_f16_ncols_6_bs_96(
	.param .u64 ggml_matvec_f16_ncols_6_bs_96_param_0,
	.param .u64 ggml_matvec_f16_ncols_6_bs_96_param_1,
	.param .u64 ggml_matvec_f16_ncols_6_bs_96_param_2,
	.param .u32 ggml_matvec_f16_ncols_6_bs_96_param_3,
	.param .u32 ggml_matvec_f16_ncols_6_bs_96_param_4,
	.param .u32 ggml_matvec_f16_ncols_6_bs_96_param_5,
	.param .u32 ggml_matvec_f16_ncols_6_bs_96_param_6,
	.param .u32 ggml_matvec_f16_ncols_6_bs_96_param_7,
	.param .u32 ggml_matvec_f16_ncols_6_bs_96_param_8,
	.param .u32 ggml_matvec_f16_ncols_6_bs_96_param_9,
	.param .u32 ggml_matvec_f16_ncols_6_bs_96_param_10,
	.param .u32 ggml_matvec_f16_ncols_6_bs_96_param_11
)
{
	.local .align 8 .b8 	__local_depot85[24];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<74>;
	.reg .b16 	%rs<4>;
	.reg .f32 	%f<148>;
	.reg .b32 	%r<527>;
	.reg .b64 	%rd<79>;


	mov.u64 	%SPL, __local_depot85;
	ld.param.u64 	%rd29, [ggml_matvec_f16_ncols_6_bs_96_param_0];
	ld.param.u64 	%rd30, [ggml_matvec_f16_ncols_6_bs_96_param_1];
	ld.param.u64 	%rd28, [ggml_matvec_f16_ncols_6_bs_96_param_2];
	ld.param.u32 	%r52, [ggml_matvec_f16_ncols_6_bs_96_param_3];
	ld.param.u32 	%r56, [ggml_matvec_f16_ncols_6_bs_96_param_5];
	ld.param.u32 	%r53, [ggml_matvec_f16_ncols_6_bs_96_param_6];
	ld.param.u32 	%r54, [ggml_matvec_f16_ncols_6_bs_96_param_7];
	ld.param.u32 	%r57, [ggml_matvec_f16_ncols_6_bs_96_param_8];
	ld.param.u32 	%r58, [ggml_matvec_f16_ncols_6_bs_96_param_9];
	ld.param.u32 	%r59, [ggml_matvec_f16_ncols_6_bs_96_param_10];
	ld.param.u32 	%r55, [ggml_matvec_f16_ncols_6_bs_96_param_11];
	cvta.to.global.u64 	%rd78, %rd30;
	cvta.to.global.u64 	%rd2, %rd29;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r60, %r1, %r57;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r61, %r2, %r56;
	mad.lo.s32 	%r62, %r60, %r58, %r61;
	cvt.s64.s32 	%rd4, %r62;
	mul.lo.s32 	%r63, %r1, %r59;
	cvt.s64.s32 	%rd5, %r63;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r64, %r3, 2;
	mov.u32 	%r65, data_mmv;
	add.s32 	%r4, %r65, %r64;
	@%p1 bra 	$L__BB85_2;

	mov.u32 	%r66, 0;
	st.shared.u32 	[%r4], %r66;

$L__BB85_2:
	bar.sync 	0;
	mov.f32 	%f7, 0f00000000;
	st.local.v2.f32 	[%rd3], {%f7, %f7};
	st.local.v2.f32 	[%rd3+8], {%f7, %f7};
	st.local.v2.f32 	[%rd3+16], {%f7, %f7};
	mov.u32 	%r521, 0;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs2, %f7;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f7;}

	// end inline asm
	mov.b32 	%r526, {%rs1, %rs2};
	setp.ge.s32 	%p2, %r3, %r52;
	mov.u32 	%r522, %r521;
	mov.u32 	%r523, %r521;
	mov.u32 	%r524, %r521;
	mov.u32 	%r525, %r521;
	@%p2 bra 	$L__BB85_9;

	not.b32 	%r77, %r3;
	add.s32 	%r6, %r77, %r52;
	mul.wide.u32 	%rd32, %r6, -1431655765;
	shr.u64 	%rd33, %rd32, 38;
	cvt.u32.u64 	%r78, %rd33;
	add.s32 	%r79, %r78, 1;
	and.b32  	%r506, %r79, 3;
	setp.eq.s32 	%p3, %r506, 0;
	mov.u32 	%r521, 0;
	mov.u32 	%r513, %r3;
	@%p3 bra 	$L__BB85_6;

	shl.b32 	%r85, %r53, 1;
	add.s32 	%r86, %r3, %r85;
	mul.wide.s32 	%rd34, %r86, 4;
	shl.b64 	%rd35, %rd5, 1;
	add.s64 	%rd7, %rd34, %rd35;
	mul.wide.s32 	%rd36, %r3, 4;
	mul.wide.s32 	%rd8, %r53, 4;
	add.s64 	%rd37, %rd36, %rd8;
	add.s64 	%rd9, %rd37, %rd35;
	add.s64 	%rd10, %rd36, %rd35;
	mul.wide.s32 	%rd38, %r3, 2;
	add.s64 	%rd39, %rd38, %rd4;
	shl.b64 	%rd40, %rd39, 1;
	add.s64 	%rd75, %rd2, %rd40;
	mov.u32 	%r521, 0;
	mov.u64 	%rd76, %rd78;
	mov.u32 	%r522, %r521;
	mov.u32 	%r523, %r521;
	mov.u32 	%r524, %r521;
	mov.u32 	%r525, %r521;
	mov.u32 	%r513, %r3;

$L__BB85_5:
	.pragma "nounroll";
	ld.global.nc.u32 	%r88, [%rd75];
	add.s64 	%rd41, %rd76, %rd10;
	ld.global.nc.u32 	%r89, [%rd41];
	// begin inline asm
	{mul.f16x2 %r87,%r88,%r89;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r526,%r526,%r87;
}
	// end inline asm
	add.s64 	%rd42, %rd76, %rd9;
	ld.global.nc.u32 	%r95, [%rd42];
	// begin inline asm
	{mul.f16x2 %r93,%r88,%r95;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r525,%r525,%r93;
}
	// end inline asm
	add.s64 	%rd43, %rd76, %rd7;
	ld.global.nc.u32 	%r101, [%rd43];
	// begin inline asm
	{mul.f16x2 %r99,%r88,%r101;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r524,%r524,%r99;
}
	// end inline asm
	add.s64 	%rd44, %rd43, %rd8;
	ld.global.nc.u32 	%r107, [%rd44];
	// begin inline asm
	{mul.f16x2 %r105,%r88,%r107;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r523,%r523,%r105;
}
	// end inline asm
	add.s64 	%rd45, %rd44, %rd8;
	ld.global.nc.u32 	%r113, [%rd45];
	// begin inline asm
	{mul.f16x2 %r111,%r88,%r113;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r522,%r522,%r111;
}
	// end inline asm
	add.s64 	%rd46, %rd45, %rd8;
	ld.global.nc.u32 	%r119, [%rd46];
	// begin inline asm
	{mul.f16x2 %r117,%r88,%r119;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r521,%r521,%r117;
}
	// end inline asm
	add.s32 	%r513, %r513, 96;
	add.s64 	%rd76, %rd76, 384;
	add.s64 	%rd75, %rd75, 384;
	add.s32 	%r506, %r506, -1;
	setp.ne.s32 	%p4, %r506, 0;
	@%p4 bra 	$L__BB85_5;

$L__BB85_6:
	setp.lt.u32 	%p5, %r6, 288;
	@%p5 bra 	$L__BB85_9;

	add.s32 	%r123, %r513, %r53;
	shl.b32 	%r124, %r53, 1;
	add.s32 	%r125, %r513, %r124;
	mad.lo.s32 	%r126, %r53, 3, %r513;
	shl.b32 	%r127, %r53, 2;
	add.s32 	%r128, %r513, %r127;
	mad.lo.s32 	%r129, %r53, 5, %r513;
	add.s32 	%r130, %r123, 96;
	mul.wide.s32 	%rd47, %r130, 4;
	shl.b64 	%rd48, %rd5, 1;
	add.s64 	%rd16, %rd47, %rd48;
	mul.wide.s32 	%rd49, %r125, 4;
	add.s64 	%rd17, %rd49, %rd48;
	mul.wide.s32 	%rd50, %r126, 4;
	add.s64 	%rd18, %rd50, %rd48;
	mul.wide.s32 	%rd51, %r128, 4;
	add.s64 	%rd19, %rd51, %rd48;
	mul.wide.s32 	%rd52, %r129, 4;
	add.s64 	%rd20, %rd52, %rd48;
	mul.wide.s32 	%rd53, %r513, 2;
	add.s64 	%rd54, %rd53, %rd4;
	shl.b64 	%rd55, %rd54, 1;
	add.s64 	%rd56, %rd2, %rd55;
	add.s64 	%rd77, %rd56, 768;
	mul.wide.s32 	%rd57, %r513, 4;
	add.s64 	%rd22, %rd57, %rd48;
	mul.wide.s32 	%rd58, %r53, 4;
	add.s64 	%rd59, %rd57, %rd58;
	add.s64 	%rd23, %rd59, %rd48;

$L__BB85_8:
	ld.global.nc.u32 	%r132, [%rd77+-768];
	add.s64 	%rd60, %rd78, %rd22;
	ld.global.nc.u32 	%r133, [%rd60];
	// begin inline asm
	{mul.f16x2 %r131,%r132,%r133;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r134,%r526,%r131;
}
	// end inline asm
	add.s64 	%rd61, %rd78, %rd23;
	ld.global.nc.u32 	%r139, [%rd61];
	// begin inline asm
	{mul.f16x2 %r137,%r132,%r139;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r140,%r525,%r137;
}
	// end inline asm
	add.s64 	%rd62, %rd78, %rd17;
	ld.global.nc.u32 	%r145, [%rd62];
	// begin inline asm
	{mul.f16x2 %r143,%r132,%r145;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r146,%r524,%r143;
}
	// end inline asm
	add.s64 	%rd63, %rd78, %rd18;
	ld.global.nc.u32 	%r151, [%rd63];
	// begin inline asm
	{mul.f16x2 %r149,%r132,%r151;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r152,%r523,%r149;
}
	// end inline asm
	add.s64 	%rd64, %rd78, %rd19;
	ld.global.nc.u32 	%r157, [%rd64];
	// begin inline asm
	{mul.f16x2 %r155,%r132,%r157;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r158,%r522,%r155;
}
	// end inline asm
	add.s64 	%rd65, %rd78, %rd20;
	ld.global.nc.u32 	%r163, [%rd65];
	// begin inline asm
	{mul.f16x2 %r161,%r132,%r163;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r164,%r521,%r161;
}
	// end inline asm
	ld.global.nc.u32 	%r168, [%rd77+-384];
	ld.global.nc.u32 	%r169, [%rd60+384];
	// begin inline asm
	{mul.f16x2 %r167,%r168,%r169;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r170,%r134,%r167;
}
	// end inline asm
	add.s64 	%rd66, %rd78, %rd16;
	ld.global.nc.u32 	%r175, [%rd66];
	// begin inline asm
	{mul.f16x2 %r173,%r168,%r175;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r176,%r140,%r173;
}
	// end inline asm
	ld.global.nc.u32 	%r181, [%rd62+384];
	// begin inline asm
	{mul.f16x2 %r179,%r168,%r181;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r182,%r146,%r179;
}
	// end inline asm
	ld.global.nc.u32 	%r187, [%rd63+384];
	// begin inline asm
	{mul.f16x2 %r185,%r168,%r187;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r188,%r152,%r185;
}
	// end inline asm
	ld.global.nc.u32 	%r193, [%rd64+384];
	// begin inline asm
	{mul.f16x2 %r191,%r168,%r193;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r194,%r158,%r191;
}
	// end inline asm
	ld.global.nc.u32 	%r199, [%rd65+384];
	// begin inline asm
	{mul.f16x2 %r197,%r168,%r199;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r200,%r164,%r197;
}
	// end inline asm
	ld.global.nc.u32 	%r204, [%rd77];
	ld.global.nc.u32 	%r205, [%rd60+768];
	// begin inline asm
	{mul.f16x2 %r203,%r204,%r205;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r206,%r170,%r203;
}
	// end inline asm
	ld.global.nc.u32 	%r211, [%rd66+384];
	// begin inline asm
	{mul.f16x2 %r209,%r204,%r211;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r212,%r176,%r209;
}
	// end inline asm
	ld.global.nc.u32 	%r217, [%rd62+768];
	// begin inline asm
	{mul.f16x2 %r215,%r204,%r217;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r218,%r182,%r215;
}
	// end inline asm
	ld.global.nc.u32 	%r223, [%rd63+768];
	// begin inline asm
	{mul.f16x2 %r221,%r204,%r223;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r224,%r188,%r221;
}
	// end inline asm
	ld.global.nc.u32 	%r229, [%rd64+768];
	// begin inline asm
	{mul.f16x2 %r227,%r204,%r229;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r230,%r194,%r227;
}
	// end inline asm
	ld.global.nc.u32 	%r235, [%rd65+768];
	// begin inline asm
	{mul.f16x2 %r233,%r204,%r235;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r236,%r200,%r233;
}
	// end inline asm
	ld.global.nc.u32 	%r240, [%rd77+384];
	ld.global.nc.u32 	%r241, [%rd60+1152];
	// begin inline asm
	{mul.f16x2 %r239,%r240,%r241;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r526,%r206,%r239;
}
	// end inline asm
	ld.global.nc.u32 	%r247, [%rd66+768];
	// begin inline asm
	{mul.f16x2 %r245,%r240,%r247;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r525,%r212,%r245;
}
	// end inline asm
	ld.global.nc.u32 	%r253, [%rd62+1152];
	// begin inline asm
	{mul.f16x2 %r251,%r240,%r253;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r524,%r218,%r251;
}
	// end inline asm
	ld.global.nc.u32 	%r259, [%rd63+1152];
	// begin inline asm
	{mul.f16x2 %r257,%r240,%r259;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r523,%r224,%r257;
}
	// end inline asm
	ld.global.nc.u32 	%r265, [%rd64+1152];
	// begin inline asm
	{mul.f16x2 %r263,%r240,%r265;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r522,%r230,%r263;
}
	// end inline asm
	ld.global.nc.u32 	%r271, [%rd65+1152];
	// begin inline asm
	{mul.f16x2 %r269,%r240,%r271;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r521,%r236,%r269;
}
	// end inline asm
	add.s64 	%rd78, %rd78, 1536;
	add.s64 	%rd77, %rd77, 1536;
	add.s32 	%r513, %r513, 384;
	setp.lt.s32 	%p6, %r513, %r52;
	@%p6 bra 	$L__BB85_8;

$L__BB85_9:
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r526;
  cvt.f32.f16 %f8, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r526;
  cvt.f32.f16 %f9, high;}

	// end inline asm
	add.f32 	%f20, %f8, %f9;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r525;
  cvt.f32.f16 %f10, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r525;
  cvt.f32.f16 %f11, high;}

	// end inline asm
	add.f32 	%f1, %f10, %f11;
	st.local.f32 	[%rd3+4], %f1;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r524;
  cvt.f32.f16 %f12, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r524;
  cvt.f32.f16 %f13, high;}

	// end inline asm
	add.f32 	%f2, %f12, %f13;
	st.local.f32 	[%rd3+8], %f2;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r523;
  cvt.f32.f16 %f14, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r523;
  cvt.f32.f16 %f15, high;}

	// end inline asm
	add.f32 	%f3, %f14, %f15;
	st.local.f32 	[%rd3+12], %f3;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r522;
  cvt.f32.f16 %f16, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r522;
  cvt.f32.f16 %f17, high;}

	// end inline asm
	add.f32 	%f4, %f16, %f17;
	st.local.f32 	[%rd3+16], %f4;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r521;
  cvt.f32.f16 %f18, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r521;
  cvt.f32.f16 %f19, high;}

	// end inline asm
	add.f32 	%f5, %f18, %f19;
	st.local.f32 	[%rd3+20], %f5;
	shr.s32 	%r287, %r3, 31;
	shr.u32 	%r288, %r287, 27;
	add.s32 	%r289, %r3, %r288;
	shr.s32 	%r290, %r289, 5;
	shl.b32 	%r291, %r290, 2;
	add.s32 	%r51, %r65, %r291;
	mov.u32 	%r293, 2;
	mov.b32 	%r294, %f20;
	mov.u32 	%r295, 31;
	mov.u32 	%r296, 16;
	mov.u32 	%r297, -1;
	shfl.sync.bfly.b32 	%r298|%p7, %r294, %r296, %r295, %r297;
	mov.b32 	%f21, %r298;
	add.f32 	%f22, %f20, %f21;
	mov.b32 	%r299, %f22;
	mov.u32 	%r300, 8;
	shfl.sync.bfly.b32 	%r301|%p8, %r299, %r300, %r295, %r297;
	mov.b32 	%f23, %r301;
	add.f32 	%f24, %f22, %f23;
	mov.b32 	%r302, %f24;
	mov.u32 	%r303, 4;
	shfl.sync.bfly.b32 	%r304|%p9, %r302, %r303, %r295, %r297;
	mov.b32 	%f25, %r304;
	add.f32 	%f26, %f24, %f25;
	mov.b32 	%r305, %f26;
	shfl.sync.bfly.b32 	%r306|%p10, %r305, %r293, %r295, %r297;
	mov.b32 	%f27, %r306;
	add.f32 	%f28, %f26, %f27;
	mov.b32 	%r307, %f28;
	mov.u32 	%r308, 1;
	shfl.sync.bfly.b32 	%r309|%p11, %r307, %r308, %r295, %r297;
	mov.b32 	%f29, %r309;
	add.f32 	%f30, %f28, %f29;
	st.local.f32 	[%rd3], %f30;
	st.shared.f32 	[%r51], %f30;
	bar.sync 	0;
	@%p1 bra 	$L__BB85_11;

	ld.shared.f32 	%f31, [%r4];
	mov.b32 	%r310, %f31;
	shfl.sync.bfly.b32 	%r314|%p13, %r310, %r296, %r295, %r297;
	mov.b32 	%f32, %r314;
	add.f32 	%f33, %f31, %f32;
	mov.b32 	%r315, %f33;
	shfl.sync.bfly.b32 	%r317|%p14, %r315, %r300, %r295, %r297;
	mov.b32 	%f34, %r317;
	add.f32 	%f35, %f33, %f34;
	mov.b32 	%r318, %f35;
	shfl.sync.bfly.b32 	%r320|%p15, %r318, %r303, %r295, %r297;
	mov.b32 	%f36, %r320;
	add.f32 	%f37, %f35, %f36;
	mov.b32 	%r321, %f37;
	shfl.sync.bfly.b32 	%r323|%p16, %r321, %r293, %r295, %r297;
	mov.b32 	%f38, %r323;
	add.f32 	%f39, %f37, %f38;
	mov.b32 	%r324, %f39;
	shfl.sync.bfly.b32 	%r326|%p17, %r324, %r308, %r295, %r297;
	mov.b32 	%f40, %r326;
	add.f32 	%f41, %f39, %f40;
	st.local.f32 	[%rd3], %f41;

$L__BB85_11:
	bar.sync 	0;
	mov.b32 	%r327, %f1;
	shfl.sync.bfly.b32 	%r331|%p19, %r327, %r296, %r295, %r297;
	mov.b32 	%f42, %r331;
	add.f32 	%f43, %f1, %f42;
	mov.b32 	%r332, %f43;
	shfl.sync.bfly.b32 	%r334|%p20, %r332, %r300, %r295, %r297;
	mov.b32 	%f44, %r334;
	add.f32 	%f45, %f43, %f44;
	mov.b32 	%r335, %f45;
	shfl.sync.bfly.b32 	%r337|%p21, %r335, %r303, %r295, %r297;
	mov.b32 	%f46, %r337;
	add.f32 	%f47, %f45, %f46;
	mov.b32 	%r338, %f47;
	shfl.sync.bfly.b32 	%r340|%p22, %r338, %r293, %r295, %r297;
	mov.b32 	%f48, %r340;
	add.f32 	%f49, %f47, %f48;
	mov.b32 	%r341, %f49;
	shfl.sync.bfly.b32 	%r343|%p23, %r341, %r308, %r295, %r297;
	mov.b32 	%f50, %r343;
	add.f32 	%f51, %f49, %f50;
	st.local.f32 	[%rd3+4], %f51;
	st.shared.f32 	[%r51], %f51;
	bar.sync 	0;
	@%p1 bra 	$L__BB85_13;

	ld.shared.f32 	%f52, [%r4];
	mov.b32 	%r344, %f52;
	mov.u32 	%r345, 31;
	mov.u32 	%r346, 16;
	mov.u32 	%r347, -1;
	shfl.sync.bfly.b32 	%r348|%p24, %r344, %r346, %r345, %r347;
	mov.b32 	%f53, %r348;
	add.f32 	%f54, %f52, %f53;
	mov.b32 	%r349, %f54;
	mov.u32 	%r350, 8;
	shfl.sync.bfly.b32 	%r351|%p25, %r349, %r350, %r345, %r347;
	mov.b32 	%f55, %r351;
	add.f32 	%f56, %f54, %f55;
	mov.b32 	%r352, %f56;
	mov.u32 	%r353, 4;
	shfl.sync.bfly.b32 	%r354|%p26, %r352, %r353, %r345, %r347;
	mov.b32 	%f57, %r354;
	add.f32 	%f58, %f56, %f57;
	mov.b32 	%r355, %f58;
	mov.u32 	%r356, 2;
	shfl.sync.bfly.b32 	%r357|%p27, %r355, %r356, %r345, %r347;
	mov.b32 	%f59, %r357;
	add.f32 	%f60, %f58, %f59;
	mov.b32 	%r358, %f60;
	mov.u32 	%r359, 1;
	shfl.sync.bfly.b32 	%r360|%p28, %r358, %r359, %r345, %r347;
	mov.b32 	%f61, %r360;
	add.f32 	%f62, %f60, %f61;
	st.local.f32 	[%rd3+4], %f62;

$L__BB85_13:
	bar.sync 	0;
	mov.b32 	%r361, %f2;
	mov.u32 	%r362, 31;
	mov.u32 	%r363, 16;
	mov.u32 	%r364, -1;
	shfl.sync.bfly.b32 	%r365|%p30, %r361, %r363, %r362, %r364;
	mov.b32 	%f63, %r365;
	add.f32 	%f64, %f2, %f63;
	mov.b32 	%r366, %f64;
	mov.u32 	%r367, 8;
	shfl.sync.bfly.b32 	%r368|%p31, %r366, %r367, %r362, %r364;
	mov.b32 	%f65, %r368;
	add.f32 	%f66, %f64, %f65;
	mov.b32 	%r369, %f66;
	mov.u32 	%r370, 4;
	shfl.sync.bfly.b32 	%r371|%p32, %r369, %r370, %r362, %r364;
	mov.b32 	%f67, %r371;
	add.f32 	%f68, %f66, %f67;
	mov.b32 	%r372, %f68;
	mov.u32 	%r373, 2;
	shfl.sync.bfly.b32 	%r374|%p33, %r372, %r373, %r362, %r364;
	mov.b32 	%f69, %r374;
	add.f32 	%f70, %f68, %f69;
	mov.b32 	%r375, %f70;
	mov.u32 	%r376, 1;
	shfl.sync.bfly.b32 	%r377|%p34, %r375, %r376, %r362, %r364;
	mov.b32 	%f71, %r377;
	add.f32 	%f72, %f70, %f71;
	st.local.f32 	[%rd3+8], %f72;
	st.shared.f32 	[%r51], %f72;
	bar.sync 	0;
	@%p1 bra 	$L__BB85_15;

	ld.shared.f32 	%f73, [%r4];
	mov.b32 	%r378, %f73;
	shfl.sync.bfly.b32 	%r382|%p35, %r378, %r363, %r362, %r364;
	mov.b32 	%f74, %r382;
	add.f32 	%f75, %f73, %f74;
	mov.b32 	%r383, %f75;
	shfl.sync.bfly.b32 	%r385|%p36, %r383, %r367, %r362, %r364;
	mov.b32 	%f76, %r385;
	add.f32 	%f77, %f75, %f76;
	mov.b32 	%r386, %f77;
	shfl.sync.bfly.b32 	%r388|%p37, %r386, %r370, %r362, %r364;
	mov.b32 	%f78, %r388;
	add.f32 	%f79, %f77, %f78;
	mov.b32 	%r389, %f79;
	shfl.sync.bfly.b32 	%r391|%p38, %r389, %r373, %r362, %r364;
	mov.b32 	%f80, %r391;
	add.f32 	%f81, %f79, %f80;
	mov.b32 	%r392, %f81;
	shfl.sync.bfly.b32 	%r394|%p39, %r392, %r376, %r362, %r364;
	mov.b32 	%f82, %r394;
	add.f32 	%f83, %f81, %f82;
	st.local.f32 	[%rd3+8], %f83;

$L__BB85_15:
	bar.sync 	0;
	mov.b32 	%r395, %f3;
	shfl.sync.bfly.b32 	%r399|%p41, %r395, %r363, %r362, %r364;
	mov.b32 	%f84, %r399;
	add.f32 	%f85, %f3, %f84;
	mov.b32 	%r400, %f85;
	shfl.sync.bfly.b32 	%r402|%p42, %r400, %r367, %r362, %r364;
	mov.b32 	%f86, %r402;
	add.f32 	%f87, %f85, %f86;
	mov.b32 	%r403, %f87;
	shfl.sync.bfly.b32 	%r405|%p43, %r403, %r370, %r362, %r364;
	mov.b32 	%f88, %r405;
	add.f32 	%f89, %f87, %f88;
	mov.b32 	%r406, %f89;
	shfl.sync.bfly.b32 	%r408|%p44, %r406, %r373, %r362, %r364;
	mov.b32 	%f90, %r408;
	add.f32 	%f91, %f89, %f90;
	mov.b32 	%r409, %f91;
	shfl.sync.bfly.b32 	%r411|%p45, %r409, %r376, %r362, %r364;
	mov.b32 	%f92, %r411;
	add.f32 	%f93, %f91, %f92;
	st.local.f32 	[%rd3+12], %f93;
	st.shared.f32 	[%r51], %f93;
	bar.sync 	0;
	@%p1 bra 	$L__BB85_17;

	ld.shared.f32 	%f94, [%r4];
	mov.b32 	%r412, %f94;
	mov.u32 	%r413, 31;
	mov.u32 	%r414, 16;
	mov.u32 	%r415, -1;
	shfl.sync.bfly.b32 	%r416|%p46, %r412, %r414, %r413, %r415;
	mov.b32 	%f95, %r416;
	add.f32 	%f96, %f94, %f95;
	mov.b32 	%r417, %f96;
	mov.u32 	%r418, 8;
	shfl.sync.bfly.b32 	%r419|%p47, %r417, %r418, %r413, %r415;
	mov.b32 	%f97, %r419;
	add.f32 	%f98, %f96, %f97;
	mov.b32 	%r420, %f98;
	mov.u32 	%r421, 4;
	shfl.sync.bfly.b32 	%r422|%p48, %r420, %r421, %r413, %r415;
	mov.b32 	%f99, %r422;
	add.f32 	%f100, %f98, %f99;
	mov.b32 	%r423, %f100;
	mov.u32 	%r424, 2;
	shfl.sync.bfly.b32 	%r425|%p49, %r423, %r424, %r413, %r415;
	mov.b32 	%f101, %r425;
	add.f32 	%f102, %f100, %f101;
	mov.b32 	%r426, %f102;
	mov.u32 	%r427, 1;
	shfl.sync.bfly.b32 	%r428|%p50, %r426, %r427, %r413, %r415;
	mov.b32 	%f103, %r428;
	add.f32 	%f104, %f102, %f103;
	st.local.f32 	[%rd3+12], %f104;

$L__BB85_17:
	bar.sync 	0;
	mov.b32 	%r429, %f4;
	mov.u32 	%r430, 31;
	mov.u32 	%r431, 16;
	mov.u32 	%r432, -1;
	shfl.sync.bfly.b32 	%r433|%p52, %r429, %r431, %r430, %r432;
	mov.b32 	%f105, %r433;
	add.f32 	%f106, %f4, %f105;
	mov.b32 	%r434, %f106;
	mov.u32 	%r435, 8;
	shfl.sync.bfly.b32 	%r436|%p53, %r434, %r435, %r430, %r432;
	mov.b32 	%f107, %r436;
	add.f32 	%f108, %f106, %f107;
	mov.b32 	%r437, %f108;
	mov.u32 	%r438, 4;
	shfl.sync.bfly.b32 	%r439|%p54, %r437, %r438, %r430, %r432;
	mov.b32 	%f109, %r439;
	add.f32 	%f110, %f108, %f109;
	mov.b32 	%r440, %f110;
	mov.u32 	%r441, 2;
	shfl.sync.bfly.b32 	%r442|%p55, %r440, %r441, %r430, %r432;
	mov.b32 	%f111, %r442;
	add.f32 	%f112, %f110, %f111;
	mov.b32 	%r443, %f112;
	mov.u32 	%r444, 1;
	shfl.sync.bfly.b32 	%r445|%p56, %r443, %r444, %r430, %r432;
	mov.b32 	%f113, %r445;
	add.f32 	%f114, %f112, %f113;
	st.local.f32 	[%rd3+16], %f114;
	st.shared.f32 	[%r51], %f114;
	bar.sync 	0;
	@%p1 bra 	$L__BB85_19;

	ld.shared.f32 	%f115, [%r4];
	mov.b32 	%r446, %f115;
	shfl.sync.bfly.b32 	%r450|%p57, %r446, %r431, %r430, %r432;
	mov.b32 	%f116, %r450;
	add.f32 	%f117, %f115, %f116;
	mov.b32 	%r451, %f117;
	shfl.sync.bfly.b32 	%r453|%p58, %r451, %r435, %r430, %r432;
	mov.b32 	%f118, %r453;
	add.f32 	%f119, %f117, %f118;
	mov.b32 	%r454, %f119;
	shfl.sync.bfly.b32 	%r456|%p59, %r454, %r438, %r430, %r432;
	mov.b32 	%f120, %r456;
	add.f32 	%f121, %f119, %f120;
	mov.b32 	%r457, %f121;
	shfl.sync.bfly.b32 	%r459|%p60, %r457, %r441, %r430, %r432;
	mov.b32 	%f122, %r459;
	add.f32 	%f123, %f121, %f122;
	mov.b32 	%r460, %f123;
	shfl.sync.bfly.b32 	%r462|%p61, %r460, %r444, %r430, %r432;
	mov.b32 	%f124, %r462;
	add.f32 	%f125, %f123, %f124;
	st.local.f32 	[%rd3+16], %f125;

$L__BB85_19:
	bar.sync 	0;
	mov.b32 	%r463, %f5;
	shfl.sync.bfly.b32 	%r467|%p63, %r463, %r431, %r430, %r432;
	mov.b32 	%f126, %r467;
	add.f32 	%f127, %f5, %f126;
	mov.b32 	%r468, %f127;
	shfl.sync.bfly.b32 	%r470|%p64, %r468, %r435, %r430, %r432;
	mov.b32 	%f128, %r470;
	add.f32 	%f129, %f127, %f128;
	mov.b32 	%r471, %f129;
	shfl.sync.bfly.b32 	%r473|%p65, %r471, %r438, %r430, %r432;
	mov.b32 	%f130, %r473;
	add.f32 	%f131, %f129, %f130;
	mov.b32 	%r474, %f131;
	shfl.sync.bfly.b32 	%r476|%p66, %r474, %r441, %r430, %r432;
	mov.b32 	%f132, %r476;
	add.f32 	%f133, %f131, %f132;
	mov.b32 	%r477, %f133;
	shfl.sync.bfly.b32 	%r479|%p67, %r477, %r444, %r430, %r432;
	mov.b32 	%f134, %r479;
	add.f32 	%f135, %f133, %f134;
	st.local.f32 	[%rd3+20], %f135;
	st.shared.f32 	[%r51], %f135;
	bar.sync 	0;
	@%p1 bra 	$L__BB85_21;

	ld.shared.f32 	%f136, [%r4];
	mov.b32 	%r480, %f136;
	mov.u32 	%r481, 31;
	mov.u32 	%r482, 16;
	mov.u32 	%r483, -1;
	shfl.sync.bfly.b32 	%r484|%p68, %r480, %r482, %r481, %r483;
	mov.b32 	%f137, %r484;
	add.f32 	%f138, %f136, %f137;
	mov.b32 	%r485, %f138;
	mov.u32 	%r486, 8;
	shfl.sync.bfly.b32 	%r487|%p69, %r485, %r486, %r481, %r483;
	mov.b32 	%f139, %r487;
	add.f32 	%f140, %f138, %f139;
	mov.b32 	%r488, %f140;
	mov.u32 	%r489, 4;
	shfl.sync.bfly.b32 	%r490|%p70, %r488, %r489, %r481, %r483;
	mov.b32 	%f141, %r490;
	add.f32 	%f142, %f140, %f141;
	mov.b32 	%r491, %f142;
	mov.u32 	%r492, 2;
	shfl.sync.bfly.b32 	%r493|%p71, %r491, %r492, %r481, %r483;
	mov.b32 	%f143, %r493;
	add.f32 	%f144, %f142, %f143;
	mov.b32 	%r494, %f144;
	mov.u32 	%r495, 1;
	shfl.sync.bfly.b32 	%r496|%p72, %r494, %r495, %r481, %r483;
	mov.b32 	%f145, %r496;
	add.f32 	%f146, %f144, %f145;
	st.local.f32 	[%rd3+20], %f146;

$L__BB85_21:
	bar.sync 	0;
	setp.gt.s32 	%p73, %r3, 5;
	@%p73 bra 	$L__BB85_23;

	mad.lo.s32 	%r497, %r3, %r54, %r2;
	cvt.s64.s32 	%rd67, %r497;
	mul.lo.s32 	%r498, %r1, %r55;
	cvt.s64.s32 	%rd68, %r498;
	add.s64 	%rd69, %rd68, %rd67;
	mul.wide.s32 	%rd70, %r3, 4;
	add.s64 	%rd71, %rd3, %rd70;
	ld.local.f32 	%f147, [%rd71];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs3, %f147;}

	// end inline asm
	cvta.to.global.u64 	%rd72, %rd28;
	shl.b64 	%rd73, %rd69, 1;
	add.s64 	%rd74, %rd72, %rd73;
	st.global.u16 	[%rd74], %rs3;

$L__BB85_23:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_7_bs_96
.visible .entry ggml_matvec_f16_ncols_7_bs_96(
	.param .u64 ggml_matvec_f16_ncols_7_bs_96_param_0,
	.param .u64 ggml_matvec_f16_ncols_7_bs_96_param_1,
	.param .u64 ggml_matvec_f16_ncols_7_bs_96_param_2,
	.param .u32 ggml_matvec_f16_ncols_7_bs_96_param_3,
	.param .u32 ggml_matvec_f16_ncols_7_bs_96_param_4,
	.param .u32 ggml_matvec_f16_ncols_7_bs_96_param_5,
	.param .u32 ggml_matvec_f16_ncols_7_bs_96_param_6,
	.param .u32 ggml_matvec_f16_ncols_7_bs_96_param_7,
	.param .u32 ggml_matvec_f16_ncols_7_bs_96_param_8,
	.param .u32 ggml_matvec_f16_ncols_7_bs_96_param_9,
	.param .u32 ggml_matvec_f16_ncols_7_bs_96_param_10,
	.param .u32 ggml_matvec_f16_ncols_7_bs_96_param_11
)
{
	.local .align 4 .b8 	__local_depot86[28];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<85>;
	.reg .b16 	%rs<4>;
	.reg .f32 	%f<172>;
	.reg .b32 	%r<607>;
	.reg .b64 	%rd<83>;


	mov.u64 	%SPL, __local_depot86;
	ld.param.u64 	%rd30, [ggml_matvec_f16_ncols_7_bs_96_param_0];
	ld.param.u64 	%rd31, [ggml_matvec_f16_ncols_7_bs_96_param_1];
	ld.param.u64 	%rd29, [ggml_matvec_f16_ncols_7_bs_96_param_2];
	ld.param.u32 	%r58, [ggml_matvec_f16_ncols_7_bs_96_param_3];
	ld.param.u32 	%r62, [ggml_matvec_f16_ncols_7_bs_96_param_5];
	ld.param.u32 	%r59, [ggml_matvec_f16_ncols_7_bs_96_param_6];
	ld.param.u32 	%r60, [ggml_matvec_f16_ncols_7_bs_96_param_7];
	ld.param.u32 	%r63, [ggml_matvec_f16_ncols_7_bs_96_param_8];
	ld.param.u32 	%r64, [ggml_matvec_f16_ncols_7_bs_96_param_9];
	ld.param.u32 	%r65, [ggml_matvec_f16_ncols_7_bs_96_param_10];
	ld.param.u32 	%r61, [ggml_matvec_f16_ncols_7_bs_96_param_11];
	cvta.to.global.u64 	%rd82, %rd31;
	cvta.to.global.u64 	%rd2, %rd30;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r66, %r1, %r63;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r67, %r2, %r62;
	mad.lo.s32 	%r68, %r66, %r64, %r67;
	cvt.s64.s32 	%rd4, %r68;
	mul.lo.s32 	%r69, %r1, %r65;
	cvt.s64.s32 	%rd5, %r69;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r70, %r3, 2;
	mov.u32 	%r71, data_mmv;
	add.s32 	%r4, %r71, %r70;
	@%p1 bra 	$L__BB86_2;

	mov.u32 	%r72, 0;
	st.shared.u32 	[%r4], %r72;

$L__BB86_2:
	bar.sync 	0;
	mov.f32 	%f8, 0f00000000;
	mov.u32 	%r600, 0;
	st.local.u32 	[%rd3], %r600;
	st.local.u32 	[%rd3+4], %r600;
	st.local.u32 	[%rd3+8], %r600;
	st.local.u32 	[%rd3+12], %r600;
	st.local.u32 	[%rd3+16], %r600;
	st.local.u32 	[%rd3+20], %r600;
	st.local.u32 	[%rd3+24], %r600;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs2, %f8;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f8;}

	// end inline asm
	mov.b32 	%r606, {%rs1, %rs2};
	setp.ge.s32 	%p2, %r3, %r58;
	mov.u32 	%r601, %r600;
	mov.u32 	%r602, %r600;
	mov.u32 	%r603, %r600;
	mov.u32 	%r604, %r600;
	mov.u32 	%r605, %r600;
	@%p2 bra 	$L__BB86_9;

	not.b32 	%r85, %r3;
	add.s32 	%r6, %r85, %r58;
	mul.wide.u32 	%rd33, %r6, -1431655765;
	shr.u64 	%rd34, %rd33, 38;
	cvt.u32.u64 	%r86, %rd34;
	add.s32 	%r87, %r86, 1;
	and.b32  	%r583, %r87, 3;
	setp.eq.s32 	%p3, %r583, 0;
	mov.u32 	%r600, 0;
	mov.u32 	%r591, %r3;
	@%p3 bra 	$L__BB86_6;

	shl.b32 	%r94, %r59, 1;
	add.s32 	%r95, %r3, %r94;
	mul.wide.s32 	%rd35, %r95, 4;
	shl.b64 	%rd36, %rd5, 1;
	add.s64 	%rd7, %rd35, %rd36;
	mul.wide.s32 	%rd37, %r3, 4;
	mul.wide.s32 	%rd8, %r59, 4;
	add.s64 	%rd38, %rd37, %rd8;
	add.s64 	%rd9, %rd38, %rd36;
	add.s64 	%rd10, %rd37, %rd36;
	mul.wide.s32 	%rd39, %r3, 2;
	add.s64 	%rd40, %rd39, %rd4;
	shl.b64 	%rd41, %rd40, 1;
	add.s64 	%rd79, %rd2, %rd41;
	mov.u32 	%r600, 0;
	mov.u64 	%rd80, %rd82;
	mov.u32 	%r601, %r600;
	mov.u32 	%r602, %r600;
	mov.u32 	%r603, %r600;
	mov.u32 	%r604, %r600;
	mov.u32 	%r605, %r600;
	mov.u32 	%r591, %r3;

$L__BB86_5:
	.pragma "nounroll";
	ld.global.nc.u32 	%r97, [%rd79];
	add.s64 	%rd42, %rd80, %rd10;
	ld.global.nc.u32 	%r98, [%rd42];
	// begin inline asm
	{mul.f16x2 %r96,%r97,%r98;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r606,%r606,%r96;
}
	// end inline asm
	add.s64 	%rd43, %rd80, %rd9;
	ld.global.nc.u32 	%r104, [%rd43];
	// begin inline asm
	{mul.f16x2 %r102,%r97,%r104;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r605,%r605,%r102;
}
	// end inline asm
	add.s64 	%rd44, %rd80, %rd7;
	ld.global.nc.u32 	%r110, [%rd44];
	// begin inline asm
	{mul.f16x2 %r108,%r97,%r110;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r604,%r604,%r108;
}
	// end inline asm
	add.s64 	%rd45, %rd44, %rd8;
	ld.global.nc.u32 	%r116, [%rd45];
	// begin inline asm
	{mul.f16x2 %r114,%r97,%r116;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r603,%r603,%r114;
}
	// end inline asm
	add.s64 	%rd46, %rd45, %rd8;
	ld.global.nc.u32 	%r122, [%rd46];
	// begin inline asm
	{mul.f16x2 %r120,%r97,%r122;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r602,%r602,%r120;
}
	// end inline asm
	add.s64 	%rd47, %rd46, %rd8;
	ld.global.nc.u32 	%r128, [%rd47];
	// begin inline asm
	{mul.f16x2 %r126,%r97,%r128;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r601,%r601,%r126;
}
	// end inline asm
	add.s64 	%rd48, %rd47, %rd8;
	ld.global.nc.u32 	%r134, [%rd48];
	// begin inline asm
	{mul.f16x2 %r132,%r97,%r134;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r600,%r600,%r132;
}
	// end inline asm
	add.s32 	%r591, %r591, 96;
	add.s64 	%rd80, %rd80, 384;
	add.s64 	%rd79, %rd79, 384;
	add.s32 	%r583, %r583, -1;
	setp.ne.s32 	%p4, %r583, 0;
	@%p4 bra 	$L__BB86_5;

$L__BB86_6:
	setp.lt.u32 	%p5, %r6, 288;
	@%p5 bra 	$L__BB86_9;

	add.s32 	%r138, %r591, %r59;
	shl.b32 	%r139, %r59, 1;
	add.s32 	%r140, %r591, %r139;
	mad.lo.s32 	%r141, %r59, 3, %r591;
	shl.b32 	%r142, %r59, 2;
	add.s32 	%r143, %r591, %r142;
	mad.lo.s32 	%r144, %r59, 5, %r591;
	mad.lo.s32 	%r145, %r59, 6, %r591;
	add.s32 	%r146, %r138, 96;
	mul.wide.s32 	%rd49, %r146, 4;
	shl.b64 	%rd50, %rd5, 1;
	add.s64 	%rd16, %rd49, %rd50;
	mul.wide.s32 	%rd51, %r140, 4;
	add.s64 	%rd17, %rd51, %rd50;
	mul.wide.s32 	%rd52, %r141, 4;
	add.s64 	%rd18, %rd52, %rd50;
	mul.wide.s32 	%rd53, %r143, 4;
	add.s64 	%rd19, %rd53, %rd50;
	mul.wide.s32 	%rd54, %r144, 4;
	add.s64 	%rd20, %rd54, %rd50;
	mul.wide.s32 	%rd55, %r145, 4;
	add.s64 	%rd21, %rd55, %rd50;
	mul.wide.s32 	%rd56, %r591, 2;
	add.s64 	%rd57, %rd56, %rd4;
	shl.b64 	%rd58, %rd57, 1;
	add.s64 	%rd59, %rd2, %rd58;
	add.s64 	%rd81, %rd59, 768;
	mul.wide.s32 	%rd60, %r591, 4;
	add.s64 	%rd23, %rd60, %rd50;
	mul.wide.s32 	%rd61, %r59, 4;
	add.s64 	%rd62, %rd60, %rd61;
	add.s64 	%rd24, %rd62, %rd50;

$L__BB86_8:
	ld.global.nc.u32 	%r148, [%rd81+-768];
	add.s64 	%rd63, %rd82, %rd23;
	ld.global.nc.u32 	%r149, [%rd63];
	// begin inline asm
	{mul.f16x2 %r147,%r148,%r149;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r150,%r606,%r147;
}
	// end inline asm
	add.s64 	%rd64, %rd82, %rd24;
	ld.global.nc.u32 	%r155, [%rd64];
	// begin inline asm
	{mul.f16x2 %r153,%r148,%r155;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r156,%r605,%r153;
}
	// end inline asm
	add.s64 	%rd65, %rd82, %rd17;
	ld.global.nc.u32 	%r161, [%rd65];
	// begin inline asm
	{mul.f16x2 %r159,%r148,%r161;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r162,%r604,%r159;
}
	// end inline asm
	add.s64 	%rd66, %rd82, %rd18;
	ld.global.nc.u32 	%r167, [%rd66];
	// begin inline asm
	{mul.f16x2 %r165,%r148,%r167;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r168,%r603,%r165;
}
	// end inline asm
	add.s64 	%rd67, %rd82, %rd19;
	ld.global.nc.u32 	%r173, [%rd67];
	// begin inline asm
	{mul.f16x2 %r171,%r148,%r173;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r174,%r602,%r171;
}
	// end inline asm
	add.s64 	%rd68, %rd82, %rd20;
	ld.global.nc.u32 	%r179, [%rd68];
	// begin inline asm
	{mul.f16x2 %r177,%r148,%r179;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r180,%r601,%r177;
}
	// end inline asm
	add.s64 	%rd69, %rd82, %rd21;
	ld.global.nc.u32 	%r185, [%rd69];
	// begin inline asm
	{mul.f16x2 %r183,%r148,%r185;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r186,%r600,%r183;
}
	// end inline asm
	ld.global.nc.u32 	%r190, [%rd81+-384];
	ld.global.nc.u32 	%r191, [%rd63+384];
	// begin inline asm
	{mul.f16x2 %r189,%r190,%r191;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r192,%r150,%r189;
}
	// end inline asm
	add.s64 	%rd70, %rd82, %rd16;
	ld.global.nc.u32 	%r197, [%rd70];
	// begin inline asm
	{mul.f16x2 %r195,%r190,%r197;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r198,%r156,%r195;
}
	// end inline asm
	ld.global.nc.u32 	%r203, [%rd65+384];
	// begin inline asm
	{mul.f16x2 %r201,%r190,%r203;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r204,%r162,%r201;
}
	// end inline asm
	ld.global.nc.u32 	%r209, [%rd66+384];
	// begin inline asm
	{mul.f16x2 %r207,%r190,%r209;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r210,%r168,%r207;
}
	// end inline asm
	ld.global.nc.u32 	%r215, [%rd67+384];
	// begin inline asm
	{mul.f16x2 %r213,%r190,%r215;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r216,%r174,%r213;
}
	// end inline asm
	ld.global.nc.u32 	%r221, [%rd68+384];
	// begin inline asm
	{mul.f16x2 %r219,%r190,%r221;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r222,%r180,%r219;
}
	// end inline asm
	ld.global.nc.u32 	%r227, [%rd69+384];
	// begin inline asm
	{mul.f16x2 %r225,%r190,%r227;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r228,%r186,%r225;
}
	// end inline asm
	ld.global.nc.u32 	%r232, [%rd81];
	ld.global.nc.u32 	%r233, [%rd63+768];
	// begin inline asm
	{mul.f16x2 %r231,%r232,%r233;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r234,%r192,%r231;
}
	// end inline asm
	ld.global.nc.u32 	%r239, [%rd70+384];
	// begin inline asm
	{mul.f16x2 %r237,%r232,%r239;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r240,%r198,%r237;
}
	// end inline asm
	ld.global.nc.u32 	%r245, [%rd65+768];
	// begin inline asm
	{mul.f16x2 %r243,%r232,%r245;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r246,%r204,%r243;
}
	// end inline asm
	ld.global.nc.u32 	%r251, [%rd66+768];
	// begin inline asm
	{mul.f16x2 %r249,%r232,%r251;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r252,%r210,%r249;
}
	// end inline asm
	ld.global.nc.u32 	%r257, [%rd67+768];
	// begin inline asm
	{mul.f16x2 %r255,%r232,%r257;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r258,%r216,%r255;
}
	// end inline asm
	ld.global.nc.u32 	%r263, [%rd68+768];
	// begin inline asm
	{mul.f16x2 %r261,%r232,%r263;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r264,%r222,%r261;
}
	// end inline asm
	ld.global.nc.u32 	%r269, [%rd69+768];
	// begin inline asm
	{mul.f16x2 %r267,%r232,%r269;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r270,%r228,%r267;
}
	// end inline asm
	ld.global.nc.u32 	%r274, [%rd81+384];
	ld.global.nc.u32 	%r275, [%rd63+1152];
	// begin inline asm
	{mul.f16x2 %r273,%r274,%r275;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r606,%r234,%r273;
}
	// end inline asm
	ld.global.nc.u32 	%r281, [%rd70+768];
	// begin inline asm
	{mul.f16x2 %r279,%r274,%r281;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r605,%r240,%r279;
}
	// end inline asm
	ld.global.nc.u32 	%r287, [%rd65+1152];
	// begin inline asm
	{mul.f16x2 %r285,%r274,%r287;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r604,%r246,%r285;
}
	// end inline asm
	ld.global.nc.u32 	%r293, [%rd66+1152];
	// begin inline asm
	{mul.f16x2 %r291,%r274,%r293;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r603,%r252,%r291;
}
	// end inline asm
	ld.global.nc.u32 	%r299, [%rd67+1152];
	// begin inline asm
	{mul.f16x2 %r297,%r274,%r299;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r602,%r258,%r297;
}
	// end inline asm
	ld.global.nc.u32 	%r305, [%rd68+1152];
	// begin inline asm
	{mul.f16x2 %r303,%r274,%r305;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r601,%r264,%r303;
}
	// end inline asm
	ld.global.nc.u32 	%r311, [%rd69+1152];
	// begin inline asm
	{mul.f16x2 %r309,%r274,%r311;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r600,%r270,%r309;
}
	// end inline asm
	add.s64 	%rd82, %rd82, 1536;
	add.s64 	%rd81, %rd81, 1536;
	add.s32 	%r591, %r591, 384;
	setp.lt.s32 	%p6, %r591, %r58;
	@%p6 bra 	$L__BB86_8;

$L__BB86_9:
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r606;
  cvt.f32.f16 %f9, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r606;
  cvt.f32.f16 %f10, high;}

	// end inline asm
	add.f32 	%f23, %f9, %f10;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r605;
  cvt.f32.f16 %f11, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r605;
  cvt.f32.f16 %f12, high;}

	// end inline asm
	add.f32 	%f1, %f11, %f12;
	st.local.f32 	[%rd3+4], %f1;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r604;
  cvt.f32.f16 %f13, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r604;
  cvt.f32.f16 %f14, high;}

	// end inline asm
	add.f32 	%f2, %f13, %f14;
	st.local.f32 	[%rd3+8], %f2;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r603;
  cvt.f32.f16 %f15, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r603;
  cvt.f32.f16 %f16, high;}

	// end inline asm
	add.f32 	%f3, %f15, %f16;
	st.local.f32 	[%rd3+12], %f3;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r602;
  cvt.f32.f16 %f17, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r602;
  cvt.f32.f16 %f18, high;}

	// end inline asm
	add.f32 	%f4, %f17, %f18;
	st.local.f32 	[%rd3+16], %f4;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r601;
  cvt.f32.f16 %f19, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r601;
  cvt.f32.f16 %f20, high;}

	// end inline asm
	add.f32 	%f5, %f19, %f20;
	st.local.f32 	[%rd3+20], %f5;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r600;
  cvt.f32.f16 %f21, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r600;
  cvt.f32.f16 %f22, high;}

	// end inline asm
	add.f32 	%f6, %f21, %f22;
	st.local.f32 	[%rd3+24], %f6;
	shr.s32 	%r329, %r3, 31;
	shr.u32 	%r330, %r329, 27;
	add.s32 	%r331, %r3, %r330;
	shr.s32 	%r332, %r331, 5;
	shl.b32 	%r333, %r332, 2;
	add.s32 	%r57, %r71, %r333;
	mov.u32 	%r335, 2;
	mov.b32 	%r336, %f23;
	mov.u32 	%r337, 31;
	mov.u32 	%r338, 16;
	mov.u32 	%r339, -1;
	shfl.sync.bfly.b32 	%r340|%p7, %r336, %r338, %r337, %r339;
	mov.b32 	%f24, %r340;
	add.f32 	%f25, %f23, %f24;
	mov.b32 	%r341, %f25;
	mov.u32 	%r342, 8;
	shfl.sync.bfly.b32 	%r343|%p8, %r341, %r342, %r337, %r339;
	mov.b32 	%f26, %r343;
	add.f32 	%f27, %f25, %f26;
	mov.b32 	%r344, %f27;
	mov.u32 	%r345, 4;
	shfl.sync.bfly.b32 	%r346|%p9, %r344, %r345, %r337, %r339;
	mov.b32 	%f28, %r346;
	add.f32 	%f29, %f27, %f28;
	mov.b32 	%r347, %f29;
	shfl.sync.bfly.b32 	%r348|%p10, %r347, %r335, %r337, %r339;
	mov.b32 	%f30, %r348;
	add.f32 	%f31, %f29, %f30;
	mov.b32 	%r349, %f31;
	mov.u32 	%r350, 1;
	shfl.sync.bfly.b32 	%r351|%p11, %r349, %r350, %r337, %r339;
	mov.b32 	%f32, %r351;
	add.f32 	%f33, %f31, %f32;
	st.local.f32 	[%rd3], %f33;
	st.shared.f32 	[%r57], %f33;
	bar.sync 	0;
	@%p1 bra 	$L__BB86_11;

	ld.shared.f32 	%f34, [%r4];
	mov.b32 	%r352, %f34;
	shfl.sync.bfly.b32 	%r356|%p13, %r352, %r338, %r337, %r339;
	mov.b32 	%f35, %r356;
	add.f32 	%f36, %f34, %f35;
	mov.b32 	%r357, %f36;
	shfl.sync.bfly.b32 	%r359|%p14, %r357, %r342, %r337, %r339;
	mov.b32 	%f37, %r359;
	add.f32 	%f38, %f36, %f37;
	mov.b32 	%r360, %f38;
	shfl.sync.bfly.b32 	%r362|%p15, %r360, %r345, %r337, %r339;
	mov.b32 	%f39, %r362;
	add.f32 	%f40, %f38, %f39;
	mov.b32 	%r363, %f40;
	shfl.sync.bfly.b32 	%r365|%p16, %r363, %r335, %r337, %r339;
	mov.b32 	%f41, %r365;
	add.f32 	%f42, %f40, %f41;
	mov.b32 	%r366, %f42;
	shfl.sync.bfly.b32 	%r368|%p17, %r366, %r350, %r337, %r339;
	mov.b32 	%f43, %r368;
	add.f32 	%f44, %f42, %f43;
	st.local.f32 	[%rd3], %f44;

$L__BB86_11:
	bar.sync 	0;
	mov.b32 	%r369, %f1;
	shfl.sync.bfly.b32 	%r373|%p19, %r369, %r338, %r337, %r339;
	mov.b32 	%f45, %r373;
	add.f32 	%f46, %f1, %f45;
	mov.b32 	%r374, %f46;
	shfl.sync.bfly.b32 	%r376|%p20, %r374, %r342, %r337, %r339;
	mov.b32 	%f47, %r376;
	add.f32 	%f48, %f46, %f47;
	mov.b32 	%r377, %f48;
	shfl.sync.bfly.b32 	%r379|%p21, %r377, %r345, %r337, %r339;
	mov.b32 	%f49, %r379;
	add.f32 	%f50, %f48, %f49;
	mov.b32 	%r380, %f50;
	shfl.sync.bfly.b32 	%r382|%p22, %r380, %r335, %r337, %r339;
	mov.b32 	%f51, %r382;
	add.f32 	%f52, %f50, %f51;
	mov.b32 	%r383, %f52;
	shfl.sync.bfly.b32 	%r385|%p23, %r383, %r350, %r337, %r339;
	mov.b32 	%f53, %r385;
	add.f32 	%f54, %f52, %f53;
	st.local.f32 	[%rd3+4], %f54;
	st.shared.f32 	[%r57], %f54;
	bar.sync 	0;
	@%p1 bra 	$L__BB86_13;

	ld.shared.f32 	%f55, [%r4];
	mov.b32 	%r386, %f55;
	mov.u32 	%r387, 31;
	mov.u32 	%r388, 16;
	mov.u32 	%r389, -1;
	shfl.sync.bfly.b32 	%r390|%p24, %r386, %r388, %r387, %r389;
	mov.b32 	%f56, %r390;
	add.f32 	%f57, %f55, %f56;
	mov.b32 	%r391, %f57;
	mov.u32 	%r392, 8;
	shfl.sync.bfly.b32 	%r393|%p25, %r391, %r392, %r387, %r389;
	mov.b32 	%f58, %r393;
	add.f32 	%f59, %f57, %f58;
	mov.b32 	%r394, %f59;
	mov.u32 	%r395, 4;
	shfl.sync.bfly.b32 	%r396|%p26, %r394, %r395, %r387, %r389;
	mov.b32 	%f60, %r396;
	add.f32 	%f61, %f59, %f60;
	mov.b32 	%r397, %f61;
	mov.u32 	%r398, 2;
	shfl.sync.bfly.b32 	%r399|%p27, %r397, %r398, %r387, %r389;
	mov.b32 	%f62, %r399;
	add.f32 	%f63, %f61, %f62;
	mov.b32 	%r400, %f63;
	mov.u32 	%r401, 1;
	shfl.sync.bfly.b32 	%r402|%p28, %r400, %r401, %r387, %r389;
	mov.b32 	%f64, %r402;
	add.f32 	%f65, %f63, %f64;
	st.local.f32 	[%rd3+4], %f65;

$L__BB86_13:
	bar.sync 	0;
	mov.b32 	%r403, %f2;
	mov.u32 	%r404, 31;
	mov.u32 	%r405, 16;
	mov.u32 	%r406, -1;
	shfl.sync.bfly.b32 	%r407|%p30, %r403, %r405, %r404, %r406;
	mov.b32 	%f66, %r407;
	add.f32 	%f67, %f2, %f66;
	mov.b32 	%r408, %f67;
	mov.u32 	%r409, 8;
	shfl.sync.bfly.b32 	%r410|%p31, %r408, %r409, %r404, %r406;
	mov.b32 	%f68, %r410;
	add.f32 	%f69, %f67, %f68;
	mov.b32 	%r411, %f69;
	mov.u32 	%r412, 4;
	shfl.sync.bfly.b32 	%r413|%p32, %r411, %r412, %r404, %r406;
	mov.b32 	%f70, %r413;
	add.f32 	%f71, %f69, %f70;
	mov.b32 	%r414, %f71;
	mov.u32 	%r415, 2;
	shfl.sync.bfly.b32 	%r416|%p33, %r414, %r415, %r404, %r406;
	mov.b32 	%f72, %r416;
	add.f32 	%f73, %f71, %f72;
	mov.b32 	%r417, %f73;
	mov.u32 	%r418, 1;
	shfl.sync.bfly.b32 	%r419|%p34, %r417, %r418, %r404, %r406;
	mov.b32 	%f74, %r419;
	add.f32 	%f75, %f73, %f74;
	st.local.f32 	[%rd3+8], %f75;
	st.shared.f32 	[%r57], %f75;
	bar.sync 	0;
	@%p1 bra 	$L__BB86_15;

	ld.shared.f32 	%f76, [%r4];
	mov.b32 	%r420, %f76;
	shfl.sync.bfly.b32 	%r424|%p35, %r420, %r405, %r404, %r406;
	mov.b32 	%f77, %r424;
	add.f32 	%f78, %f76, %f77;
	mov.b32 	%r425, %f78;
	shfl.sync.bfly.b32 	%r427|%p36, %r425, %r409, %r404, %r406;
	mov.b32 	%f79, %r427;
	add.f32 	%f80, %f78, %f79;
	mov.b32 	%r428, %f80;
	shfl.sync.bfly.b32 	%r430|%p37, %r428, %r412, %r404, %r406;
	mov.b32 	%f81, %r430;
	add.f32 	%f82, %f80, %f81;
	mov.b32 	%r431, %f82;
	shfl.sync.bfly.b32 	%r433|%p38, %r431, %r415, %r404, %r406;
	mov.b32 	%f83, %r433;
	add.f32 	%f84, %f82, %f83;
	mov.b32 	%r434, %f84;
	shfl.sync.bfly.b32 	%r436|%p39, %r434, %r418, %r404, %r406;
	mov.b32 	%f85, %r436;
	add.f32 	%f86, %f84, %f85;
	st.local.f32 	[%rd3+8], %f86;

$L__BB86_15:
	bar.sync 	0;
	mov.b32 	%r437, %f3;
	shfl.sync.bfly.b32 	%r441|%p41, %r437, %r405, %r404, %r406;
	mov.b32 	%f87, %r441;
	add.f32 	%f88, %f3, %f87;
	mov.b32 	%r442, %f88;
	shfl.sync.bfly.b32 	%r444|%p42, %r442, %r409, %r404, %r406;
	mov.b32 	%f89, %r444;
	add.f32 	%f90, %f88, %f89;
	mov.b32 	%r445, %f90;
	shfl.sync.bfly.b32 	%r447|%p43, %r445, %r412, %r404, %r406;
	mov.b32 	%f91, %r447;
	add.f32 	%f92, %f90, %f91;
	mov.b32 	%r448, %f92;
	shfl.sync.bfly.b32 	%r450|%p44, %r448, %r415, %r404, %r406;
	mov.b32 	%f93, %r450;
	add.f32 	%f94, %f92, %f93;
	mov.b32 	%r451, %f94;
	shfl.sync.bfly.b32 	%r453|%p45, %r451, %r418, %r404, %r406;
	mov.b32 	%f95, %r453;
	add.f32 	%f96, %f94, %f95;
	st.local.f32 	[%rd3+12], %f96;
	st.shared.f32 	[%r57], %f96;
	bar.sync 	0;
	@%p1 bra 	$L__BB86_17;

	ld.shared.f32 	%f97, [%r4];
	mov.b32 	%r454, %f97;
	mov.u32 	%r455, 31;
	mov.u32 	%r456, 16;
	mov.u32 	%r457, -1;
	shfl.sync.bfly.b32 	%r458|%p46, %r454, %r456, %r455, %r457;
	mov.b32 	%f98, %r458;
	add.f32 	%f99, %f97, %f98;
	mov.b32 	%r459, %f99;
	mov.u32 	%r460, 8;
	shfl.sync.bfly.b32 	%r461|%p47, %r459, %r460, %r455, %r457;
	mov.b32 	%f100, %r461;
	add.f32 	%f101, %f99, %f100;
	mov.b32 	%r462, %f101;
	mov.u32 	%r463, 4;
	shfl.sync.bfly.b32 	%r464|%p48, %r462, %r463, %r455, %r457;
	mov.b32 	%f102, %r464;
	add.f32 	%f103, %f101, %f102;
	mov.b32 	%r465, %f103;
	mov.u32 	%r466, 2;
	shfl.sync.bfly.b32 	%r467|%p49, %r465, %r466, %r455, %r457;
	mov.b32 	%f104, %r467;
	add.f32 	%f105, %f103, %f104;
	mov.b32 	%r468, %f105;
	mov.u32 	%r469, 1;
	shfl.sync.bfly.b32 	%r470|%p50, %r468, %r469, %r455, %r457;
	mov.b32 	%f106, %r470;
	add.f32 	%f107, %f105, %f106;
	st.local.f32 	[%rd3+12], %f107;

$L__BB86_17:
	bar.sync 	0;
	mov.b32 	%r471, %f4;
	mov.u32 	%r472, 31;
	mov.u32 	%r473, 16;
	mov.u32 	%r474, -1;
	shfl.sync.bfly.b32 	%r475|%p52, %r471, %r473, %r472, %r474;
	mov.b32 	%f108, %r475;
	add.f32 	%f109, %f4, %f108;
	mov.b32 	%r476, %f109;
	mov.u32 	%r477, 8;
	shfl.sync.bfly.b32 	%r478|%p53, %r476, %r477, %r472, %r474;
	mov.b32 	%f110, %r478;
	add.f32 	%f111, %f109, %f110;
	mov.b32 	%r479, %f111;
	mov.u32 	%r480, 4;
	shfl.sync.bfly.b32 	%r481|%p54, %r479, %r480, %r472, %r474;
	mov.b32 	%f112, %r481;
	add.f32 	%f113, %f111, %f112;
	mov.b32 	%r482, %f113;
	mov.u32 	%r483, 2;
	shfl.sync.bfly.b32 	%r484|%p55, %r482, %r483, %r472, %r474;
	mov.b32 	%f114, %r484;
	add.f32 	%f115, %f113, %f114;
	mov.b32 	%r485, %f115;
	mov.u32 	%r486, 1;
	shfl.sync.bfly.b32 	%r487|%p56, %r485, %r486, %r472, %r474;
	mov.b32 	%f116, %r487;
	add.f32 	%f117, %f115, %f116;
	st.local.f32 	[%rd3+16], %f117;
	st.shared.f32 	[%r57], %f117;
	bar.sync 	0;
	@%p1 bra 	$L__BB86_19;

	ld.shared.f32 	%f118, [%r4];
	mov.b32 	%r488, %f118;
	shfl.sync.bfly.b32 	%r492|%p57, %r488, %r473, %r472, %r474;
	mov.b32 	%f119, %r492;
	add.f32 	%f120, %f118, %f119;
	mov.b32 	%r493, %f120;
	shfl.sync.bfly.b32 	%r495|%p58, %r493, %r477, %r472, %r474;
	mov.b32 	%f121, %r495;
	add.f32 	%f122, %f120, %f121;
	mov.b32 	%r496, %f122;
	shfl.sync.bfly.b32 	%r498|%p59, %r496, %r480, %r472, %r474;
	mov.b32 	%f123, %r498;
	add.f32 	%f124, %f122, %f123;
	mov.b32 	%r499, %f124;
	shfl.sync.bfly.b32 	%r501|%p60, %r499, %r483, %r472, %r474;
	mov.b32 	%f125, %r501;
	add.f32 	%f126, %f124, %f125;
	mov.b32 	%r502, %f126;
	shfl.sync.bfly.b32 	%r504|%p61, %r502, %r486, %r472, %r474;
	mov.b32 	%f127, %r504;
	add.f32 	%f128, %f126, %f127;
	st.local.f32 	[%rd3+16], %f128;

$L__BB86_19:
	bar.sync 	0;
	mov.b32 	%r505, %f5;
	shfl.sync.bfly.b32 	%r509|%p63, %r505, %r473, %r472, %r474;
	mov.b32 	%f129, %r509;
	add.f32 	%f130, %f5, %f129;
	mov.b32 	%r510, %f130;
	shfl.sync.bfly.b32 	%r512|%p64, %r510, %r477, %r472, %r474;
	mov.b32 	%f131, %r512;
	add.f32 	%f132, %f130, %f131;
	mov.b32 	%r513, %f132;
	shfl.sync.bfly.b32 	%r515|%p65, %r513, %r480, %r472, %r474;
	mov.b32 	%f133, %r515;
	add.f32 	%f134, %f132, %f133;
	mov.b32 	%r516, %f134;
	shfl.sync.bfly.b32 	%r518|%p66, %r516, %r483, %r472, %r474;
	mov.b32 	%f135, %r518;
	add.f32 	%f136, %f134, %f135;
	mov.b32 	%r519, %f136;
	shfl.sync.bfly.b32 	%r521|%p67, %r519, %r486, %r472, %r474;
	mov.b32 	%f137, %r521;
	add.f32 	%f138, %f136, %f137;
	st.local.f32 	[%rd3+20], %f138;
	st.shared.f32 	[%r57], %f138;
	bar.sync 	0;
	@%p1 bra 	$L__BB86_21;

	ld.shared.f32 	%f139, [%r4];
	mov.b32 	%r522, %f139;
	mov.u32 	%r523, 31;
	mov.u32 	%r524, 16;
	mov.u32 	%r525, -1;
	shfl.sync.bfly.b32 	%r526|%p68, %r522, %r524, %r523, %r525;
	mov.b32 	%f140, %r526;
	add.f32 	%f141, %f139, %f140;
	mov.b32 	%r527, %f141;
	mov.u32 	%r528, 8;
	shfl.sync.bfly.b32 	%r529|%p69, %r527, %r528, %r523, %r525;
	mov.b32 	%f142, %r529;
	add.f32 	%f143, %f141, %f142;
	mov.b32 	%r530, %f143;
	mov.u32 	%r531, 4;
	shfl.sync.bfly.b32 	%r532|%p70, %r530, %r531, %r523, %r525;
	mov.b32 	%f144, %r532;
	add.f32 	%f145, %f143, %f144;
	mov.b32 	%r533, %f145;
	mov.u32 	%r534, 2;
	shfl.sync.bfly.b32 	%r535|%p71, %r533, %r534, %r523, %r525;
	mov.b32 	%f146, %r535;
	add.f32 	%f147, %f145, %f146;
	mov.b32 	%r536, %f147;
	mov.u32 	%r537, 1;
	shfl.sync.bfly.b32 	%r538|%p72, %r536, %r537, %r523, %r525;
	mov.b32 	%f148, %r538;
	add.f32 	%f149, %f147, %f148;
	st.local.f32 	[%rd3+20], %f149;

$L__BB86_21:
	bar.sync 	0;
	mov.b32 	%r539, %f6;
	mov.u32 	%r540, 31;
	mov.u32 	%r541, 16;
	mov.u32 	%r542, -1;
	shfl.sync.bfly.b32 	%r543|%p74, %r539, %r541, %r540, %r542;
	mov.b32 	%f150, %r543;
	add.f32 	%f151, %f6, %f150;
	mov.b32 	%r544, %f151;
	mov.u32 	%r545, 8;
	shfl.sync.bfly.b32 	%r546|%p75, %r544, %r545, %r540, %r542;
	mov.b32 	%f152, %r546;
	add.f32 	%f153, %f151, %f152;
	mov.b32 	%r547, %f153;
	mov.u32 	%r548, 4;
	shfl.sync.bfly.b32 	%r549|%p76, %r547, %r548, %r540, %r542;
	mov.b32 	%f154, %r549;
	add.f32 	%f155, %f153, %f154;
	mov.b32 	%r550, %f155;
	mov.u32 	%r551, 2;
	shfl.sync.bfly.b32 	%r552|%p77, %r550, %r551, %r540, %r542;
	mov.b32 	%f156, %r552;
	add.f32 	%f157, %f155, %f156;
	mov.b32 	%r553, %f157;
	mov.u32 	%r554, 1;
	shfl.sync.bfly.b32 	%r555|%p78, %r553, %r554, %r540, %r542;
	mov.b32 	%f158, %r555;
	add.f32 	%f159, %f157, %f158;
	st.local.f32 	[%rd3+24], %f159;
	st.shared.f32 	[%r57], %f159;
	bar.sync 	0;
	@%p1 bra 	$L__BB86_23;

	ld.shared.f32 	%f160, [%r4];
	mov.b32 	%r556, %f160;
	shfl.sync.bfly.b32 	%r560|%p79, %r556, %r541, %r540, %r542;
	mov.b32 	%f161, %r560;
	add.f32 	%f162, %f160, %f161;
	mov.b32 	%r561, %f162;
	shfl.sync.bfly.b32 	%r563|%p80, %r561, %r545, %r540, %r542;
	mov.b32 	%f163, %r563;
	add.f32 	%f164, %f162, %f163;
	mov.b32 	%r564, %f164;
	shfl.sync.bfly.b32 	%r566|%p81, %r564, %r548, %r540, %r542;
	mov.b32 	%f165, %r566;
	add.f32 	%f166, %f164, %f165;
	mov.b32 	%r567, %f166;
	shfl.sync.bfly.b32 	%r569|%p82, %r567, %r551, %r540, %r542;
	mov.b32 	%f167, %r569;
	add.f32 	%f168, %f166, %f167;
	mov.b32 	%r570, %f168;
	shfl.sync.bfly.b32 	%r572|%p83, %r570, %r554, %r540, %r542;
	mov.b32 	%f169, %r572;
	add.f32 	%f170, %f168, %f169;
	st.local.f32 	[%rd3+24], %f170;

$L__BB86_23:
	bar.sync 	0;
	setp.gt.s32 	%p84, %r3, 6;
	@%p84 bra 	$L__BB86_25;

	mad.lo.s32 	%r573, %r3, %r60, %r2;
	cvt.s64.s32 	%rd71, %r573;
	mul.lo.s32 	%r574, %r1, %r61;
	cvt.s64.s32 	%rd72, %r574;
	add.s64 	%rd73, %rd72, %rd71;
	mul.wide.s32 	%rd74, %r3, 4;
	add.s64 	%rd75, %rd3, %rd74;
	ld.local.f32 	%f171, [%rd75];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs3, %f171;}

	// end inline asm
	cvta.to.global.u64 	%rd76, %rd29;
	shl.b64 	%rd77, %rd73, 1;
	add.s64 	%rd78, %rd76, %rd77;
	st.global.u16 	[%rd78], %rs3;

$L__BB86_25:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_8_bs_96
.visible .entry ggml_matvec_f16_ncols_8_bs_96(
	.param .u64 ggml_matvec_f16_ncols_8_bs_96_param_0,
	.param .u64 ggml_matvec_f16_ncols_8_bs_96_param_1,
	.param .u64 ggml_matvec_f16_ncols_8_bs_96_param_2,
	.param .u32 ggml_matvec_f16_ncols_8_bs_96_param_3,
	.param .u32 ggml_matvec_f16_ncols_8_bs_96_param_4,
	.param .u32 ggml_matvec_f16_ncols_8_bs_96_param_5,
	.param .u32 ggml_matvec_f16_ncols_8_bs_96_param_6,
	.param .u32 ggml_matvec_f16_ncols_8_bs_96_param_7,
	.param .u32 ggml_matvec_f16_ncols_8_bs_96_param_8,
	.param .u32 ggml_matvec_f16_ncols_8_bs_96_param_9,
	.param .u32 ggml_matvec_f16_ncols_8_bs_96_param_10,
	.param .u32 ggml_matvec_f16_ncols_8_bs_96_param_11
)
{
	.local .align 16 .b8 	__local_depot87[32];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<96>;
	.reg .b16 	%rs<4>;
	.reg .f32 	%f<196>;
	.reg .b32 	%r<687>;
	.reg .b64 	%rd<87>;


	mov.u64 	%SPL, __local_depot87;
	ld.param.u64 	%rd31, [ggml_matvec_f16_ncols_8_bs_96_param_0];
	ld.param.u64 	%rd32, [ggml_matvec_f16_ncols_8_bs_96_param_1];
	ld.param.u64 	%rd30, [ggml_matvec_f16_ncols_8_bs_96_param_2];
	ld.param.u32 	%r64, [ggml_matvec_f16_ncols_8_bs_96_param_3];
	ld.param.u32 	%r68, [ggml_matvec_f16_ncols_8_bs_96_param_5];
	ld.param.u32 	%r65, [ggml_matvec_f16_ncols_8_bs_96_param_6];
	ld.param.u32 	%r66, [ggml_matvec_f16_ncols_8_bs_96_param_7];
	ld.param.u32 	%r69, [ggml_matvec_f16_ncols_8_bs_96_param_8];
	ld.param.u32 	%r70, [ggml_matvec_f16_ncols_8_bs_96_param_9];
	ld.param.u32 	%r71, [ggml_matvec_f16_ncols_8_bs_96_param_10];
	ld.param.u32 	%r67, [ggml_matvec_f16_ncols_8_bs_96_param_11];
	cvta.to.global.u64 	%rd86, %rd32;
	cvta.to.global.u64 	%rd2, %rd31;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r72, %r1, %r69;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r73, %r2, %r68;
	mad.lo.s32 	%r74, %r72, %r70, %r73;
	cvt.s64.s32 	%rd4, %r74;
	mul.lo.s32 	%r75, %r1, %r71;
	cvt.s64.s32 	%rd5, %r75;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r76, %r3, 2;
	mov.u32 	%r77, data_mmv;
	add.s32 	%r4, %r77, %r76;
	@%p1 bra 	$L__BB87_2;

	mov.u32 	%r78, 0;
	st.shared.u32 	[%r4], %r78;

$L__BB87_2:
	bar.sync 	0;
	mov.f32 	%f9, 0f00000000;
	st.local.v4.f32 	[%rd3], {%f9, %f9, %f9, %f9};
	st.local.v4.f32 	[%rd3+16], {%f9, %f9, %f9, %f9};
	mov.u32 	%r679, 0;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs2, %f9;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f9;}

	// end inline asm
	mov.b32 	%r686, {%rs1, %rs2};
	setp.ge.s32 	%p2, %r3, %r64;
	mov.u32 	%r680, %r679;
	mov.u32 	%r681, %r679;
	mov.u32 	%r682, %r679;
	mov.u32 	%r683, %r679;
	mov.u32 	%r684, %r679;
	mov.u32 	%r685, %r679;
	@%p2 bra 	$L__BB87_9;

	not.b32 	%r93, %r3;
	add.s32 	%r6, %r93, %r64;
	mul.wide.u32 	%rd34, %r6, -1431655765;
	shr.u64 	%rd35, %rd34, 38;
	cvt.u32.u64 	%r94, %rd35;
	add.s32 	%r95, %r94, 1;
	and.b32  	%r660, %r95, 3;
	setp.eq.s32 	%p3, %r660, 0;
	mov.u32 	%r679, 0;
	mov.u32 	%r669, %r3;
	@%p3 bra 	$L__BB87_6;

	shl.b32 	%r103, %r65, 1;
	add.s32 	%r104, %r3, %r103;
	mul.wide.s32 	%rd36, %r104, 4;
	shl.b64 	%rd37, %rd5, 1;
	add.s64 	%rd7, %rd36, %rd37;
	mul.wide.s32 	%rd38, %r3, 4;
	mul.wide.s32 	%rd8, %r65, 4;
	add.s64 	%rd39, %rd38, %rd8;
	add.s64 	%rd9, %rd39, %rd37;
	add.s64 	%rd10, %rd38, %rd37;
	mul.wide.s32 	%rd40, %r3, 2;
	add.s64 	%rd41, %rd40, %rd4;
	shl.b64 	%rd42, %rd41, 1;
	add.s64 	%rd83, %rd2, %rd42;
	mov.u32 	%r679, 0;
	mov.u64 	%rd84, %rd86;
	mov.u32 	%r680, %r679;
	mov.u32 	%r681, %r679;
	mov.u32 	%r682, %r679;
	mov.u32 	%r683, %r679;
	mov.u32 	%r684, %r679;
	mov.u32 	%r685, %r679;
	mov.u32 	%r669, %r3;

$L__BB87_5:
	.pragma "nounroll";
	ld.global.nc.u32 	%r106, [%rd83];
	add.s64 	%rd43, %rd84, %rd10;
	ld.global.nc.u32 	%r107, [%rd43];
	// begin inline asm
	{mul.f16x2 %r105,%r106,%r107;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r686,%r686,%r105;
}
	// end inline asm
	add.s64 	%rd44, %rd84, %rd9;
	ld.global.nc.u32 	%r113, [%rd44];
	// begin inline asm
	{mul.f16x2 %r111,%r106,%r113;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r685,%r685,%r111;
}
	// end inline asm
	add.s64 	%rd45, %rd84, %rd7;
	ld.global.nc.u32 	%r119, [%rd45];
	// begin inline asm
	{mul.f16x2 %r117,%r106,%r119;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r684,%r684,%r117;
}
	// end inline asm
	add.s64 	%rd46, %rd45, %rd8;
	ld.global.nc.u32 	%r125, [%rd46];
	// begin inline asm
	{mul.f16x2 %r123,%r106,%r125;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r683,%r683,%r123;
}
	// end inline asm
	add.s64 	%rd47, %rd46, %rd8;
	ld.global.nc.u32 	%r131, [%rd47];
	// begin inline asm
	{mul.f16x2 %r129,%r106,%r131;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r682,%r682,%r129;
}
	// end inline asm
	add.s64 	%rd48, %rd47, %rd8;
	ld.global.nc.u32 	%r137, [%rd48];
	// begin inline asm
	{mul.f16x2 %r135,%r106,%r137;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r681,%r681,%r135;
}
	// end inline asm
	add.s64 	%rd49, %rd48, %rd8;
	ld.global.nc.u32 	%r143, [%rd49];
	// begin inline asm
	{mul.f16x2 %r141,%r106,%r143;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r680,%r680,%r141;
}
	// end inline asm
	add.s64 	%rd50, %rd49, %rd8;
	ld.global.nc.u32 	%r149, [%rd50];
	// begin inline asm
	{mul.f16x2 %r147,%r106,%r149;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r679,%r679,%r147;
}
	// end inline asm
	add.s32 	%r669, %r669, 96;
	add.s64 	%rd84, %rd84, 384;
	add.s64 	%rd83, %rd83, 384;
	add.s32 	%r660, %r660, -1;
	setp.ne.s32 	%p4, %r660, 0;
	@%p4 bra 	$L__BB87_5;

$L__BB87_6:
	setp.lt.u32 	%p5, %r6, 288;
	@%p5 bra 	$L__BB87_9;

	add.s32 	%r153, %r669, %r65;
	shl.b32 	%r154, %r65, 1;
	add.s32 	%r155, %r669, %r154;
	mad.lo.s32 	%r156, %r65, 3, %r669;
	shl.b32 	%r157, %r65, 2;
	add.s32 	%r158, %r669, %r157;
	mad.lo.s32 	%r159, %r65, 5, %r669;
	mad.lo.s32 	%r160, %r65, 6, %r669;
	mad.lo.s32 	%r161, %r65, 7, %r669;
	add.s32 	%r162, %r153, 96;
	mul.wide.s32 	%rd51, %r162, 4;
	shl.b64 	%rd52, %rd5, 1;
	add.s64 	%rd16, %rd51, %rd52;
	mul.wide.s32 	%rd53, %r155, 4;
	add.s64 	%rd17, %rd53, %rd52;
	mul.wide.s32 	%rd54, %r156, 4;
	add.s64 	%rd18, %rd54, %rd52;
	mul.wide.s32 	%rd55, %r158, 4;
	add.s64 	%rd19, %rd55, %rd52;
	mul.wide.s32 	%rd56, %r159, 4;
	add.s64 	%rd20, %rd56, %rd52;
	mul.wide.s32 	%rd57, %r160, 4;
	add.s64 	%rd21, %rd57, %rd52;
	mul.wide.s32 	%rd58, %r161, 4;
	add.s64 	%rd22, %rd58, %rd52;
	mul.wide.s32 	%rd59, %r669, 2;
	add.s64 	%rd60, %rd59, %rd4;
	shl.b64 	%rd61, %rd60, 1;
	add.s64 	%rd62, %rd2, %rd61;
	add.s64 	%rd85, %rd62, 768;
	mul.wide.s32 	%rd63, %r669, 4;
	add.s64 	%rd24, %rd63, %rd52;
	mul.wide.s32 	%rd64, %r65, 4;
	add.s64 	%rd65, %rd63, %rd64;
	add.s64 	%rd25, %rd65, %rd52;

$L__BB87_8:
	ld.global.nc.u32 	%r164, [%rd85+-768];
	add.s64 	%rd66, %rd86, %rd24;
	ld.global.nc.u32 	%r165, [%rd66];
	// begin inline asm
	{mul.f16x2 %r163,%r164,%r165;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r166,%r686,%r163;
}
	// end inline asm
	add.s64 	%rd67, %rd86, %rd25;
	ld.global.nc.u32 	%r171, [%rd67];
	// begin inline asm
	{mul.f16x2 %r169,%r164,%r171;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r172,%r685,%r169;
}
	// end inline asm
	add.s64 	%rd68, %rd86, %rd17;
	ld.global.nc.u32 	%r177, [%rd68];
	// begin inline asm
	{mul.f16x2 %r175,%r164,%r177;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r178,%r684,%r175;
}
	// end inline asm
	add.s64 	%rd69, %rd86, %rd18;
	ld.global.nc.u32 	%r183, [%rd69];
	// begin inline asm
	{mul.f16x2 %r181,%r164,%r183;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r184,%r683,%r181;
}
	// end inline asm
	add.s64 	%rd70, %rd86, %rd19;
	ld.global.nc.u32 	%r189, [%rd70];
	// begin inline asm
	{mul.f16x2 %r187,%r164,%r189;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r190,%r682,%r187;
}
	// end inline asm
	add.s64 	%rd71, %rd86, %rd20;
	ld.global.nc.u32 	%r195, [%rd71];
	// begin inline asm
	{mul.f16x2 %r193,%r164,%r195;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r196,%r681,%r193;
}
	// end inline asm
	add.s64 	%rd72, %rd86, %rd21;
	ld.global.nc.u32 	%r201, [%rd72];
	// begin inline asm
	{mul.f16x2 %r199,%r164,%r201;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r202,%r680,%r199;
}
	// end inline asm
	add.s64 	%rd73, %rd86, %rd22;
	ld.global.nc.u32 	%r207, [%rd73];
	// begin inline asm
	{mul.f16x2 %r205,%r164,%r207;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r208,%r679,%r205;
}
	// end inline asm
	ld.global.nc.u32 	%r212, [%rd85+-384];
	ld.global.nc.u32 	%r213, [%rd66+384];
	// begin inline asm
	{mul.f16x2 %r211,%r212,%r213;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r214,%r166,%r211;
}
	// end inline asm
	add.s64 	%rd74, %rd86, %rd16;
	ld.global.nc.u32 	%r219, [%rd74];
	// begin inline asm
	{mul.f16x2 %r217,%r212,%r219;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r220,%r172,%r217;
}
	// end inline asm
	ld.global.nc.u32 	%r225, [%rd68+384];
	// begin inline asm
	{mul.f16x2 %r223,%r212,%r225;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r226,%r178,%r223;
}
	// end inline asm
	ld.global.nc.u32 	%r231, [%rd69+384];
	// begin inline asm
	{mul.f16x2 %r229,%r212,%r231;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r232,%r184,%r229;
}
	// end inline asm
	ld.global.nc.u32 	%r237, [%rd70+384];
	// begin inline asm
	{mul.f16x2 %r235,%r212,%r237;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r238,%r190,%r235;
}
	// end inline asm
	ld.global.nc.u32 	%r243, [%rd71+384];
	// begin inline asm
	{mul.f16x2 %r241,%r212,%r243;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r244,%r196,%r241;
}
	// end inline asm
	ld.global.nc.u32 	%r249, [%rd72+384];
	// begin inline asm
	{mul.f16x2 %r247,%r212,%r249;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r250,%r202,%r247;
}
	// end inline asm
	ld.global.nc.u32 	%r255, [%rd73+384];
	// begin inline asm
	{mul.f16x2 %r253,%r212,%r255;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r256,%r208,%r253;
}
	// end inline asm
	ld.global.nc.u32 	%r260, [%rd85];
	ld.global.nc.u32 	%r261, [%rd66+768];
	// begin inline asm
	{mul.f16x2 %r259,%r260,%r261;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r262,%r214,%r259;
}
	// end inline asm
	ld.global.nc.u32 	%r267, [%rd74+384];
	// begin inline asm
	{mul.f16x2 %r265,%r260,%r267;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r268,%r220,%r265;
}
	// end inline asm
	ld.global.nc.u32 	%r273, [%rd68+768];
	// begin inline asm
	{mul.f16x2 %r271,%r260,%r273;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r274,%r226,%r271;
}
	// end inline asm
	ld.global.nc.u32 	%r279, [%rd69+768];
	// begin inline asm
	{mul.f16x2 %r277,%r260,%r279;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r280,%r232,%r277;
}
	// end inline asm
	ld.global.nc.u32 	%r285, [%rd70+768];
	// begin inline asm
	{mul.f16x2 %r283,%r260,%r285;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r286,%r238,%r283;
}
	// end inline asm
	ld.global.nc.u32 	%r291, [%rd71+768];
	// begin inline asm
	{mul.f16x2 %r289,%r260,%r291;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r292,%r244,%r289;
}
	// end inline asm
	ld.global.nc.u32 	%r297, [%rd72+768];
	// begin inline asm
	{mul.f16x2 %r295,%r260,%r297;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r298,%r250,%r295;
}
	// end inline asm
	ld.global.nc.u32 	%r303, [%rd73+768];
	// begin inline asm
	{mul.f16x2 %r301,%r260,%r303;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r304,%r256,%r301;
}
	// end inline asm
	ld.global.nc.u32 	%r308, [%rd85+384];
	ld.global.nc.u32 	%r309, [%rd66+1152];
	// begin inline asm
	{mul.f16x2 %r307,%r308,%r309;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r686,%r262,%r307;
}
	// end inline asm
	ld.global.nc.u32 	%r315, [%rd74+768];
	// begin inline asm
	{mul.f16x2 %r313,%r308,%r315;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r685,%r268,%r313;
}
	// end inline asm
	ld.global.nc.u32 	%r321, [%rd68+1152];
	// begin inline asm
	{mul.f16x2 %r319,%r308,%r321;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r684,%r274,%r319;
}
	// end inline asm
	ld.global.nc.u32 	%r327, [%rd69+1152];
	// begin inline asm
	{mul.f16x2 %r325,%r308,%r327;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r683,%r280,%r325;
}
	// end inline asm
	ld.global.nc.u32 	%r333, [%rd70+1152];
	// begin inline asm
	{mul.f16x2 %r331,%r308,%r333;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r682,%r286,%r331;
}
	// end inline asm
	ld.global.nc.u32 	%r339, [%rd71+1152];
	// begin inline asm
	{mul.f16x2 %r337,%r308,%r339;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r681,%r292,%r337;
}
	// end inline asm
	ld.global.nc.u32 	%r345, [%rd72+1152];
	// begin inline asm
	{mul.f16x2 %r343,%r308,%r345;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r680,%r298,%r343;
}
	// end inline asm
	ld.global.nc.u32 	%r351, [%rd73+1152];
	// begin inline asm
	{mul.f16x2 %r349,%r308,%r351;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r679,%r304,%r349;
}
	// end inline asm
	add.s64 	%rd86, %rd86, 1536;
	add.s64 	%rd85, %rd85, 1536;
	add.s32 	%r669, %r669, 384;
	setp.lt.s32 	%p6, %r669, %r64;
	@%p6 bra 	$L__BB87_8;

$L__BB87_9:
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r686;
  cvt.f32.f16 %f10, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r686;
  cvt.f32.f16 %f11, high;}

	// end inline asm
	add.f32 	%f26, %f10, %f11;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r685;
  cvt.f32.f16 %f12, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r685;
  cvt.f32.f16 %f13, high;}

	// end inline asm
	add.f32 	%f1, %f12, %f13;
	st.local.f32 	[%rd3+4], %f1;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r684;
  cvt.f32.f16 %f14, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r684;
  cvt.f32.f16 %f15, high;}

	// end inline asm
	add.f32 	%f2, %f14, %f15;
	st.local.f32 	[%rd3+8], %f2;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r683;
  cvt.f32.f16 %f16, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r683;
  cvt.f32.f16 %f17, high;}

	// end inline asm
	add.f32 	%f3, %f16, %f17;
	st.local.f32 	[%rd3+12], %f3;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r682;
  cvt.f32.f16 %f18, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r682;
  cvt.f32.f16 %f19, high;}

	// end inline asm
	add.f32 	%f4, %f18, %f19;
	st.local.f32 	[%rd3+16], %f4;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r681;
  cvt.f32.f16 %f20, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r681;
  cvt.f32.f16 %f21, high;}

	// end inline asm
	add.f32 	%f5, %f20, %f21;
	st.local.f32 	[%rd3+20], %f5;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r680;
  cvt.f32.f16 %f22, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r680;
  cvt.f32.f16 %f23, high;}

	// end inline asm
	add.f32 	%f6, %f22, %f23;
	st.local.f32 	[%rd3+24], %f6;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r679;
  cvt.f32.f16 %f24, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r679;
  cvt.f32.f16 %f25, high;}

	// end inline asm
	add.f32 	%f7, %f24, %f25;
	st.local.f32 	[%rd3+28], %f7;
	shr.s32 	%r371, %r3, 31;
	shr.u32 	%r372, %r371, 27;
	add.s32 	%r373, %r3, %r372;
	shr.s32 	%r374, %r373, 5;
	shl.b32 	%r375, %r374, 2;
	add.s32 	%r63, %r77, %r375;
	mov.u32 	%r377, 2;
	mov.b32 	%r378, %f26;
	mov.u32 	%r379, 31;
	mov.u32 	%r380, 16;
	mov.u32 	%r381, -1;
	shfl.sync.bfly.b32 	%r382|%p7, %r378, %r380, %r379, %r381;
	mov.b32 	%f27, %r382;
	add.f32 	%f28, %f26, %f27;
	mov.b32 	%r383, %f28;
	mov.u32 	%r384, 8;
	shfl.sync.bfly.b32 	%r385|%p8, %r383, %r384, %r379, %r381;
	mov.b32 	%f29, %r385;
	add.f32 	%f30, %f28, %f29;
	mov.b32 	%r386, %f30;
	mov.u32 	%r387, 4;
	shfl.sync.bfly.b32 	%r388|%p9, %r386, %r387, %r379, %r381;
	mov.b32 	%f31, %r388;
	add.f32 	%f32, %f30, %f31;
	mov.b32 	%r389, %f32;
	shfl.sync.bfly.b32 	%r390|%p10, %r389, %r377, %r379, %r381;
	mov.b32 	%f33, %r390;
	add.f32 	%f34, %f32, %f33;
	mov.b32 	%r391, %f34;
	mov.u32 	%r392, 1;
	shfl.sync.bfly.b32 	%r393|%p11, %r391, %r392, %r379, %r381;
	mov.b32 	%f35, %r393;
	add.f32 	%f36, %f34, %f35;
	st.local.f32 	[%rd3], %f36;
	st.shared.f32 	[%r63], %f36;
	bar.sync 	0;
	@%p1 bra 	$L__BB87_11;

	ld.shared.f32 	%f37, [%r4];
	mov.b32 	%r394, %f37;
	shfl.sync.bfly.b32 	%r398|%p13, %r394, %r380, %r379, %r381;
	mov.b32 	%f38, %r398;
	add.f32 	%f39, %f37, %f38;
	mov.b32 	%r399, %f39;
	shfl.sync.bfly.b32 	%r401|%p14, %r399, %r384, %r379, %r381;
	mov.b32 	%f40, %r401;
	add.f32 	%f41, %f39, %f40;
	mov.b32 	%r402, %f41;
	shfl.sync.bfly.b32 	%r404|%p15, %r402, %r387, %r379, %r381;
	mov.b32 	%f42, %r404;
	add.f32 	%f43, %f41, %f42;
	mov.b32 	%r405, %f43;
	shfl.sync.bfly.b32 	%r407|%p16, %r405, %r377, %r379, %r381;
	mov.b32 	%f44, %r407;
	add.f32 	%f45, %f43, %f44;
	mov.b32 	%r408, %f45;
	shfl.sync.bfly.b32 	%r410|%p17, %r408, %r392, %r379, %r381;
	mov.b32 	%f46, %r410;
	add.f32 	%f47, %f45, %f46;
	st.local.f32 	[%rd3], %f47;

$L__BB87_11:
	bar.sync 	0;
	mov.b32 	%r411, %f1;
	shfl.sync.bfly.b32 	%r415|%p19, %r411, %r380, %r379, %r381;
	mov.b32 	%f48, %r415;
	add.f32 	%f49, %f1, %f48;
	mov.b32 	%r416, %f49;
	shfl.sync.bfly.b32 	%r418|%p20, %r416, %r384, %r379, %r381;
	mov.b32 	%f50, %r418;
	add.f32 	%f51, %f49, %f50;
	mov.b32 	%r419, %f51;
	shfl.sync.bfly.b32 	%r421|%p21, %r419, %r387, %r379, %r381;
	mov.b32 	%f52, %r421;
	add.f32 	%f53, %f51, %f52;
	mov.b32 	%r422, %f53;
	shfl.sync.bfly.b32 	%r424|%p22, %r422, %r377, %r379, %r381;
	mov.b32 	%f54, %r424;
	add.f32 	%f55, %f53, %f54;
	mov.b32 	%r425, %f55;
	shfl.sync.bfly.b32 	%r427|%p23, %r425, %r392, %r379, %r381;
	mov.b32 	%f56, %r427;
	add.f32 	%f57, %f55, %f56;
	st.local.f32 	[%rd3+4], %f57;
	st.shared.f32 	[%r63], %f57;
	bar.sync 	0;
	@%p1 bra 	$L__BB87_13;

	ld.shared.f32 	%f58, [%r4];
	mov.b32 	%r428, %f58;
	mov.u32 	%r429, 31;
	mov.u32 	%r430, 16;
	mov.u32 	%r431, -1;
	shfl.sync.bfly.b32 	%r432|%p24, %r428, %r430, %r429, %r431;
	mov.b32 	%f59, %r432;
	add.f32 	%f60, %f58, %f59;
	mov.b32 	%r433, %f60;
	mov.u32 	%r434, 8;
	shfl.sync.bfly.b32 	%r435|%p25, %r433, %r434, %r429, %r431;
	mov.b32 	%f61, %r435;
	add.f32 	%f62, %f60, %f61;
	mov.b32 	%r436, %f62;
	mov.u32 	%r437, 4;
	shfl.sync.bfly.b32 	%r438|%p26, %r436, %r437, %r429, %r431;
	mov.b32 	%f63, %r438;
	add.f32 	%f64, %f62, %f63;
	mov.b32 	%r439, %f64;
	mov.u32 	%r440, 2;
	shfl.sync.bfly.b32 	%r441|%p27, %r439, %r440, %r429, %r431;
	mov.b32 	%f65, %r441;
	add.f32 	%f66, %f64, %f65;
	mov.b32 	%r442, %f66;
	mov.u32 	%r443, 1;
	shfl.sync.bfly.b32 	%r444|%p28, %r442, %r443, %r429, %r431;
	mov.b32 	%f67, %r444;
	add.f32 	%f68, %f66, %f67;
	st.local.f32 	[%rd3+4], %f68;

$L__BB87_13:
	bar.sync 	0;
	mov.b32 	%r445, %f2;
	mov.u32 	%r446, 31;
	mov.u32 	%r447, 16;
	mov.u32 	%r448, -1;
	shfl.sync.bfly.b32 	%r449|%p30, %r445, %r447, %r446, %r448;
	mov.b32 	%f69, %r449;
	add.f32 	%f70, %f2, %f69;
	mov.b32 	%r450, %f70;
	mov.u32 	%r451, 8;
	shfl.sync.bfly.b32 	%r452|%p31, %r450, %r451, %r446, %r448;
	mov.b32 	%f71, %r452;
	add.f32 	%f72, %f70, %f71;
	mov.b32 	%r453, %f72;
	mov.u32 	%r454, 4;
	shfl.sync.bfly.b32 	%r455|%p32, %r453, %r454, %r446, %r448;
	mov.b32 	%f73, %r455;
	add.f32 	%f74, %f72, %f73;
	mov.b32 	%r456, %f74;
	mov.u32 	%r457, 2;
	shfl.sync.bfly.b32 	%r458|%p33, %r456, %r457, %r446, %r448;
	mov.b32 	%f75, %r458;
	add.f32 	%f76, %f74, %f75;
	mov.b32 	%r459, %f76;
	mov.u32 	%r460, 1;
	shfl.sync.bfly.b32 	%r461|%p34, %r459, %r460, %r446, %r448;
	mov.b32 	%f77, %r461;
	add.f32 	%f78, %f76, %f77;
	st.local.f32 	[%rd3+8], %f78;
	st.shared.f32 	[%r63], %f78;
	bar.sync 	0;
	@%p1 bra 	$L__BB87_15;

	ld.shared.f32 	%f79, [%r4];
	mov.b32 	%r462, %f79;
	shfl.sync.bfly.b32 	%r466|%p35, %r462, %r447, %r446, %r448;
	mov.b32 	%f80, %r466;
	add.f32 	%f81, %f79, %f80;
	mov.b32 	%r467, %f81;
	shfl.sync.bfly.b32 	%r469|%p36, %r467, %r451, %r446, %r448;
	mov.b32 	%f82, %r469;
	add.f32 	%f83, %f81, %f82;
	mov.b32 	%r470, %f83;
	shfl.sync.bfly.b32 	%r472|%p37, %r470, %r454, %r446, %r448;
	mov.b32 	%f84, %r472;
	add.f32 	%f85, %f83, %f84;
	mov.b32 	%r473, %f85;
	shfl.sync.bfly.b32 	%r475|%p38, %r473, %r457, %r446, %r448;
	mov.b32 	%f86, %r475;
	add.f32 	%f87, %f85, %f86;
	mov.b32 	%r476, %f87;
	shfl.sync.bfly.b32 	%r478|%p39, %r476, %r460, %r446, %r448;
	mov.b32 	%f88, %r478;
	add.f32 	%f89, %f87, %f88;
	st.local.f32 	[%rd3+8], %f89;

$L__BB87_15:
	bar.sync 	0;
	mov.b32 	%r479, %f3;
	shfl.sync.bfly.b32 	%r483|%p41, %r479, %r447, %r446, %r448;
	mov.b32 	%f90, %r483;
	add.f32 	%f91, %f3, %f90;
	mov.b32 	%r484, %f91;
	shfl.sync.bfly.b32 	%r486|%p42, %r484, %r451, %r446, %r448;
	mov.b32 	%f92, %r486;
	add.f32 	%f93, %f91, %f92;
	mov.b32 	%r487, %f93;
	shfl.sync.bfly.b32 	%r489|%p43, %r487, %r454, %r446, %r448;
	mov.b32 	%f94, %r489;
	add.f32 	%f95, %f93, %f94;
	mov.b32 	%r490, %f95;
	shfl.sync.bfly.b32 	%r492|%p44, %r490, %r457, %r446, %r448;
	mov.b32 	%f96, %r492;
	add.f32 	%f97, %f95, %f96;
	mov.b32 	%r493, %f97;
	shfl.sync.bfly.b32 	%r495|%p45, %r493, %r460, %r446, %r448;
	mov.b32 	%f98, %r495;
	add.f32 	%f99, %f97, %f98;
	st.local.f32 	[%rd3+12], %f99;
	st.shared.f32 	[%r63], %f99;
	bar.sync 	0;
	@%p1 bra 	$L__BB87_17;

	ld.shared.f32 	%f100, [%r4];
	mov.b32 	%r496, %f100;
	mov.u32 	%r497, 31;
	mov.u32 	%r498, 16;
	mov.u32 	%r499, -1;
	shfl.sync.bfly.b32 	%r500|%p46, %r496, %r498, %r497, %r499;
	mov.b32 	%f101, %r500;
	add.f32 	%f102, %f100, %f101;
	mov.b32 	%r501, %f102;
	mov.u32 	%r502, 8;
	shfl.sync.bfly.b32 	%r503|%p47, %r501, %r502, %r497, %r499;
	mov.b32 	%f103, %r503;
	add.f32 	%f104, %f102, %f103;
	mov.b32 	%r504, %f104;
	mov.u32 	%r505, 4;
	shfl.sync.bfly.b32 	%r506|%p48, %r504, %r505, %r497, %r499;
	mov.b32 	%f105, %r506;
	add.f32 	%f106, %f104, %f105;
	mov.b32 	%r507, %f106;
	mov.u32 	%r508, 2;
	shfl.sync.bfly.b32 	%r509|%p49, %r507, %r508, %r497, %r499;
	mov.b32 	%f107, %r509;
	add.f32 	%f108, %f106, %f107;
	mov.b32 	%r510, %f108;
	mov.u32 	%r511, 1;
	shfl.sync.bfly.b32 	%r512|%p50, %r510, %r511, %r497, %r499;
	mov.b32 	%f109, %r512;
	add.f32 	%f110, %f108, %f109;
	st.local.f32 	[%rd3+12], %f110;

$L__BB87_17:
	bar.sync 	0;
	mov.b32 	%r513, %f4;
	mov.u32 	%r514, 31;
	mov.u32 	%r515, 16;
	mov.u32 	%r516, -1;
	shfl.sync.bfly.b32 	%r517|%p52, %r513, %r515, %r514, %r516;
	mov.b32 	%f111, %r517;
	add.f32 	%f112, %f4, %f111;
	mov.b32 	%r518, %f112;
	mov.u32 	%r519, 8;
	shfl.sync.bfly.b32 	%r520|%p53, %r518, %r519, %r514, %r516;
	mov.b32 	%f113, %r520;
	add.f32 	%f114, %f112, %f113;
	mov.b32 	%r521, %f114;
	mov.u32 	%r522, 4;
	shfl.sync.bfly.b32 	%r523|%p54, %r521, %r522, %r514, %r516;
	mov.b32 	%f115, %r523;
	add.f32 	%f116, %f114, %f115;
	mov.b32 	%r524, %f116;
	mov.u32 	%r525, 2;
	shfl.sync.bfly.b32 	%r526|%p55, %r524, %r525, %r514, %r516;
	mov.b32 	%f117, %r526;
	add.f32 	%f118, %f116, %f117;
	mov.b32 	%r527, %f118;
	mov.u32 	%r528, 1;
	shfl.sync.bfly.b32 	%r529|%p56, %r527, %r528, %r514, %r516;
	mov.b32 	%f119, %r529;
	add.f32 	%f120, %f118, %f119;
	st.local.f32 	[%rd3+16], %f120;
	st.shared.f32 	[%r63], %f120;
	bar.sync 	0;
	@%p1 bra 	$L__BB87_19;

	ld.shared.f32 	%f121, [%r4];
	mov.b32 	%r530, %f121;
	shfl.sync.bfly.b32 	%r534|%p57, %r530, %r515, %r514, %r516;
	mov.b32 	%f122, %r534;
	add.f32 	%f123, %f121, %f122;
	mov.b32 	%r535, %f123;
	shfl.sync.bfly.b32 	%r537|%p58, %r535, %r519, %r514, %r516;
	mov.b32 	%f124, %r537;
	add.f32 	%f125, %f123, %f124;
	mov.b32 	%r538, %f125;
	shfl.sync.bfly.b32 	%r540|%p59, %r538, %r522, %r514, %r516;
	mov.b32 	%f126, %r540;
	add.f32 	%f127, %f125, %f126;
	mov.b32 	%r541, %f127;
	shfl.sync.bfly.b32 	%r543|%p60, %r541, %r525, %r514, %r516;
	mov.b32 	%f128, %r543;
	add.f32 	%f129, %f127, %f128;
	mov.b32 	%r544, %f129;
	shfl.sync.bfly.b32 	%r546|%p61, %r544, %r528, %r514, %r516;
	mov.b32 	%f130, %r546;
	add.f32 	%f131, %f129, %f130;
	st.local.f32 	[%rd3+16], %f131;

$L__BB87_19:
	bar.sync 	0;
	mov.b32 	%r547, %f5;
	shfl.sync.bfly.b32 	%r551|%p63, %r547, %r515, %r514, %r516;
	mov.b32 	%f132, %r551;
	add.f32 	%f133, %f5, %f132;
	mov.b32 	%r552, %f133;
	shfl.sync.bfly.b32 	%r554|%p64, %r552, %r519, %r514, %r516;
	mov.b32 	%f134, %r554;
	add.f32 	%f135, %f133, %f134;
	mov.b32 	%r555, %f135;
	shfl.sync.bfly.b32 	%r557|%p65, %r555, %r522, %r514, %r516;
	mov.b32 	%f136, %r557;
	add.f32 	%f137, %f135, %f136;
	mov.b32 	%r558, %f137;
	shfl.sync.bfly.b32 	%r560|%p66, %r558, %r525, %r514, %r516;
	mov.b32 	%f138, %r560;
	add.f32 	%f139, %f137, %f138;
	mov.b32 	%r561, %f139;
	shfl.sync.bfly.b32 	%r563|%p67, %r561, %r528, %r514, %r516;
	mov.b32 	%f140, %r563;
	add.f32 	%f141, %f139, %f140;
	st.local.f32 	[%rd3+20], %f141;
	st.shared.f32 	[%r63], %f141;
	bar.sync 	0;
	@%p1 bra 	$L__BB87_21;

	ld.shared.f32 	%f142, [%r4];
	mov.b32 	%r564, %f142;
	mov.u32 	%r565, 31;
	mov.u32 	%r566, 16;
	mov.u32 	%r567, -1;
	shfl.sync.bfly.b32 	%r568|%p68, %r564, %r566, %r565, %r567;
	mov.b32 	%f143, %r568;
	add.f32 	%f144, %f142, %f143;
	mov.b32 	%r569, %f144;
	mov.u32 	%r570, 8;
	shfl.sync.bfly.b32 	%r571|%p69, %r569, %r570, %r565, %r567;
	mov.b32 	%f145, %r571;
	add.f32 	%f146, %f144, %f145;
	mov.b32 	%r572, %f146;
	mov.u32 	%r573, 4;
	shfl.sync.bfly.b32 	%r574|%p70, %r572, %r573, %r565, %r567;
	mov.b32 	%f147, %r574;
	add.f32 	%f148, %f146, %f147;
	mov.b32 	%r575, %f148;
	mov.u32 	%r576, 2;
	shfl.sync.bfly.b32 	%r577|%p71, %r575, %r576, %r565, %r567;
	mov.b32 	%f149, %r577;
	add.f32 	%f150, %f148, %f149;
	mov.b32 	%r578, %f150;
	mov.u32 	%r579, 1;
	shfl.sync.bfly.b32 	%r580|%p72, %r578, %r579, %r565, %r567;
	mov.b32 	%f151, %r580;
	add.f32 	%f152, %f150, %f151;
	st.local.f32 	[%rd3+20], %f152;

$L__BB87_21:
	bar.sync 	0;
	mov.b32 	%r581, %f6;
	mov.u32 	%r582, 31;
	mov.u32 	%r583, 16;
	mov.u32 	%r584, -1;
	shfl.sync.bfly.b32 	%r585|%p74, %r581, %r583, %r582, %r584;
	mov.b32 	%f153, %r585;
	add.f32 	%f154, %f6, %f153;
	mov.b32 	%r586, %f154;
	mov.u32 	%r587, 8;
	shfl.sync.bfly.b32 	%r588|%p75, %r586, %r587, %r582, %r584;
	mov.b32 	%f155, %r588;
	add.f32 	%f156, %f154, %f155;
	mov.b32 	%r589, %f156;
	mov.u32 	%r590, 4;
	shfl.sync.bfly.b32 	%r591|%p76, %r589, %r590, %r582, %r584;
	mov.b32 	%f157, %r591;
	add.f32 	%f158, %f156, %f157;
	mov.b32 	%r592, %f158;
	mov.u32 	%r593, 2;
	shfl.sync.bfly.b32 	%r594|%p77, %r592, %r593, %r582, %r584;
	mov.b32 	%f159, %r594;
	add.f32 	%f160, %f158, %f159;
	mov.b32 	%r595, %f160;
	mov.u32 	%r596, 1;
	shfl.sync.bfly.b32 	%r597|%p78, %r595, %r596, %r582, %r584;
	mov.b32 	%f161, %r597;
	add.f32 	%f162, %f160, %f161;
	st.local.f32 	[%rd3+24], %f162;
	st.shared.f32 	[%r63], %f162;
	bar.sync 	0;
	@%p1 bra 	$L__BB87_23;

	ld.shared.f32 	%f163, [%r4];
	mov.b32 	%r598, %f163;
	shfl.sync.bfly.b32 	%r602|%p79, %r598, %r583, %r582, %r584;
	mov.b32 	%f164, %r602;
	add.f32 	%f165, %f163, %f164;
	mov.b32 	%r603, %f165;
	shfl.sync.bfly.b32 	%r605|%p80, %r603, %r587, %r582, %r584;
	mov.b32 	%f166, %r605;
	add.f32 	%f167, %f165, %f166;
	mov.b32 	%r606, %f167;
	shfl.sync.bfly.b32 	%r608|%p81, %r606, %r590, %r582, %r584;
	mov.b32 	%f168, %r608;
	add.f32 	%f169, %f167, %f168;
	mov.b32 	%r609, %f169;
	shfl.sync.bfly.b32 	%r611|%p82, %r609, %r593, %r582, %r584;
	mov.b32 	%f170, %r611;
	add.f32 	%f171, %f169, %f170;
	mov.b32 	%r612, %f171;
	shfl.sync.bfly.b32 	%r614|%p83, %r612, %r596, %r582, %r584;
	mov.b32 	%f172, %r614;
	add.f32 	%f173, %f171, %f172;
	st.local.f32 	[%rd3+24], %f173;

$L__BB87_23:
	bar.sync 	0;
	mov.b32 	%r615, %f7;
	shfl.sync.bfly.b32 	%r619|%p85, %r615, %r583, %r582, %r584;
	mov.b32 	%f174, %r619;
	add.f32 	%f175, %f7, %f174;
	mov.b32 	%r620, %f175;
	shfl.sync.bfly.b32 	%r622|%p86, %r620, %r587, %r582, %r584;
	mov.b32 	%f176, %r622;
	add.f32 	%f177, %f175, %f176;
	mov.b32 	%r623, %f177;
	shfl.sync.bfly.b32 	%r625|%p87, %r623, %r590, %r582, %r584;
	mov.b32 	%f178, %r625;
	add.f32 	%f179, %f177, %f178;
	mov.b32 	%r626, %f179;
	shfl.sync.bfly.b32 	%r628|%p88, %r626, %r593, %r582, %r584;
	mov.b32 	%f180, %r628;
	add.f32 	%f181, %f179, %f180;
	mov.b32 	%r629, %f181;
	shfl.sync.bfly.b32 	%r631|%p89, %r629, %r596, %r582, %r584;
	mov.b32 	%f182, %r631;
	add.f32 	%f183, %f181, %f182;
	st.local.f32 	[%rd3+28], %f183;
	st.shared.f32 	[%r63], %f183;
	bar.sync 	0;
	@%p1 bra 	$L__BB87_25;

	ld.shared.f32 	%f184, [%r4];
	mov.b32 	%r632, %f184;
	mov.u32 	%r633, 31;
	mov.u32 	%r634, 16;
	mov.u32 	%r635, -1;
	shfl.sync.bfly.b32 	%r636|%p90, %r632, %r634, %r633, %r635;
	mov.b32 	%f185, %r636;
	add.f32 	%f186, %f184, %f185;
	mov.b32 	%r637, %f186;
	mov.u32 	%r638, 8;
	shfl.sync.bfly.b32 	%r639|%p91, %r637, %r638, %r633, %r635;
	mov.b32 	%f187, %r639;
	add.f32 	%f188, %f186, %f187;
	mov.b32 	%r640, %f188;
	mov.u32 	%r641, 4;
	shfl.sync.bfly.b32 	%r642|%p92, %r640, %r641, %r633, %r635;
	mov.b32 	%f189, %r642;
	add.f32 	%f190, %f188, %f189;
	mov.b32 	%r643, %f190;
	mov.u32 	%r644, 2;
	shfl.sync.bfly.b32 	%r645|%p93, %r643, %r644, %r633, %r635;
	mov.b32 	%f191, %r645;
	add.f32 	%f192, %f190, %f191;
	mov.b32 	%r646, %f192;
	mov.u32 	%r647, 1;
	shfl.sync.bfly.b32 	%r648|%p94, %r646, %r647, %r633, %r635;
	mov.b32 	%f193, %r648;
	add.f32 	%f194, %f192, %f193;
	st.local.f32 	[%rd3+28], %f194;

$L__BB87_25:
	bar.sync 	0;
	setp.gt.s32 	%p95, %r3, 7;
	@%p95 bra 	$L__BB87_27;

	mad.lo.s32 	%r649, %r3, %r66, %r2;
	cvt.s64.s32 	%rd75, %r649;
	mul.lo.s32 	%r650, %r1, %r67;
	cvt.s64.s32 	%rd76, %r650;
	add.s64 	%rd77, %rd76, %rd75;
	mul.wide.s32 	%rd78, %r3, 4;
	add.s64 	%rd79, %rd3, %rd78;
	ld.local.f32 	%f195, [%rd79];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs3, %f195;}

	// end inline asm
	cvta.to.global.u64 	%rd80, %rd30;
	shl.b64 	%rd81, %rd77, 1;
	add.s64 	%rd82, %rd80, %rd81;
	st.global.u16 	[%rd82], %rs3;

$L__BB87_27:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_1_bs_128
.visible .entry ggml_matvec_f16_ncols_1_bs_128(
	.param .u64 ggml_matvec_f16_ncols_1_bs_128_param_0,
	.param .u64 ggml_matvec_f16_ncols_1_bs_128_param_1,
	.param .u64 ggml_matvec_f16_ncols_1_bs_128_param_2,
	.param .u32 ggml_matvec_f16_ncols_1_bs_128_param_3,
	.param .u32 ggml_matvec_f16_ncols_1_bs_128_param_4,
	.param .u32 ggml_matvec_f16_ncols_1_bs_128_param_5,
	.param .u32 ggml_matvec_f16_ncols_1_bs_128_param_6,
	.param .u32 ggml_matvec_f16_ncols_1_bs_128_param_7,
	.param .u32 ggml_matvec_f16_ncols_1_bs_128_param_8,
	.param .u32 ggml_matvec_f16_ncols_1_bs_128_param_9,
	.param .u32 ggml_matvec_f16_ncols_1_bs_128_param_10,
	.param .u32 ggml_matvec_f16_ncols_1_bs_128_param_11
)
{
	.reg .pred 	%p<19>;
	.reg .b16 	%rs<31>;
	.reg .f32 	%f<30>;
	.reg .b32 	%r<127>;
	.reg .b64 	%rd<45>;


	ld.param.u64 	%rd14, [ggml_matvec_f16_ncols_1_bs_128_param_0];
	ld.param.u64 	%rd15, [ggml_matvec_f16_ncols_1_bs_128_param_1];
	ld.param.u64 	%rd16, [ggml_matvec_f16_ncols_1_bs_128_param_2];
	ld.param.u32 	%r13, [ggml_matvec_f16_ncols_1_bs_128_param_3];
	ld.param.u32 	%r17, [ggml_matvec_f16_ncols_1_bs_128_param_5];
	ld.param.u32 	%r14, [ggml_matvec_f16_ncols_1_bs_128_param_7];
	ld.param.u32 	%r18, [ggml_matvec_f16_ncols_1_bs_128_param_8];
	ld.param.u32 	%r19, [ggml_matvec_f16_ncols_1_bs_128_param_9];
	ld.param.u32 	%r15, [ggml_matvec_f16_ncols_1_bs_128_param_10];
	ld.param.u32 	%r16, [ggml_matvec_f16_ncols_1_bs_128_param_11];
	mov.u32 	%r20, %ctaid.x;
	mov.u32 	%r21, %ctaid.y;
	div.s32 	%r22, %r21, %r18;
	mul.lo.s32 	%r23, %r20, %r17;
	mad.lo.s32 	%r24, %r22, %r19, %r23;
	cvt.s64.s32 	%rd1, %r24;
	mov.u32 	%r1, %tid.x;
	setp.gt.s32 	%p1, %r1, 31;
	shl.b32 	%r25, %r1, 2;
	mov.u32 	%r26, data_mmv;
	add.s32 	%r2, %r26, %r25;
	@%p1 bra 	$L__BB88_2;

	mov.u32 	%r27, 0;
	st.shared.u32 	[%r2], %r27;

$L__BB88_2:
	bar.sync 	0;
	mov.f32 	%f5, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs30, %f5;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs29, %f5;}

	// end inline asm
	setp.ge.s32 	%p2, %r1, %r13;
	@%p2 bra 	$L__BB88_9;

	not.b32 	%r28, %r1;
	add.s32 	%r29, %r28, %r13;
	shr.u32 	%r30, %r29, 7;
	add.s32 	%r31, %r30, 1;
	and.b32  	%r124, %r31, 3;
	setp.eq.s32 	%p3, %r124, 0;
	mov.u32 	%r125, %r1;
	@%p3 bra 	$L__BB88_6;

	mov.u32 	%r125, %tid.x;
	mul.wide.s32 	%rd17, %r125, 2;
	mul.lo.s32 	%r33, %r21, %r15;
	cvt.s64.s32 	%rd18, %r33;
	add.s64 	%rd19, %rd17, %rd18;
	cvta.to.global.u64 	%rd20, %rd15;
	shl.b64 	%rd21, %rd19, 1;
	add.s64 	%rd42, %rd20, %rd21;
	add.s64 	%rd22, %rd17, %rd1;
	cvta.to.global.u64 	%rd23, %rd14;
	shl.b64 	%rd24, %rd22, 1;
	add.s64 	%rd41, %rd23, %rd24;

$L__BB88_5:
	.pragma "nounroll";
	ld.global.nc.u32 	%r35, [%rd41];
	ld.global.nc.u32 	%r36, [%rd42];
	// begin inline asm
	{mul.f16x2 %r34,%r35,%r36;
}
	// end inline asm
	mov.b32 	%r38, {%rs30, %rs29};
	// begin inline asm
	{add.f16x2 %r37,%r38,%r34;
}
	// end inline asm
	mov.b32 	{%rs30, %rs29}, %r37;
	add.s32 	%r125, %r125, 128;
	add.s64 	%rd42, %rd42, 512;
	add.s64 	%rd41, %rd41, 512;
	add.s32 	%r124, %r124, -1;
	setp.ne.s32 	%p4, %r124, 0;
	@%p4 bra 	$L__BB88_5;

$L__BB88_6:
	setp.lt.u32 	%p5, %r29, 384;
	@%p5 bra 	$L__BB88_9;

	mul.wide.s32 	%rd25, %r125, 2;
	cvta.to.global.u64 	%rd26, %rd14;
	add.s64 	%rd27, %rd25, %rd1;
	shl.b64 	%rd28, %rd27, 1;
	add.s64 	%rd29, %rd26, %rd28;
	add.s64 	%rd44, %rd29, 1024;
	mul.lo.s32 	%r44, %r21, %r15;
	cvt.s64.s32 	%rd30, %r44;
	cvta.to.global.u64 	%rd31, %rd15;
	add.s64 	%rd32, %rd25, %rd30;
	shl.b64 	%rd33, %rd32, 1;
	add.s64 	%rd34, %rd31, %rd33;
	add.s64 	%rd43, %rd34, 1024;

$L__BB88_8:
	ld.global.nc.u32 	%r46, [%rd44+-1024];
	ld.global.nc.u32 	%r47, [%rd43+-1024];
	// begin inline asm
	{mul.f16x2 %r45,%r46,%r47;
}
	// end inline asm
	mov.b32 	%r49, {%rs30, %rs29};
	// begin inline asm
	{add.f16x2 %r48,%r49,%r45;
}
	// end inline asm
	ld.global.nc.u32 	%r52, [%rd44+-512];
	ld.global.nc.u32 	%r53, [%rd43+-512];
	// begin inline asm
	{mul.f16x2 %r51,%r52,%r53;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r54,%r48,%r51;
}
	// end inline asm
	ld.global.nc.u32 	%r58, [%rd44];
	ld.global.nc.u32 	%r59, [%rd43];
	// begin inline asm
	{mul.f16x2 %r57,%r58,%r59;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r60,%r54,%r57;
}
	// end inline asm
	ld.global.nc.u32 	%r64, [%rd44+512];
	ld.global.nc.u32 	%r65, [%rd43+512];
	// begin inline asm
	{mul.f16x2 %r63,%r64,%r65;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r66,%r60,%r63;
}
	// end inline asm
	mov.b32 	{%rs30, %rs29}, %r66;
	add.s64 	%rd44, %rd44, 2048;
	add.s64 	%rd43, %rd43, 2048;
	add.s32 	%r125, %r125, 512;
	setp.lt.s32 	%p6, %r125, %r13;
	@%p6 bra 	$L__BB88_8;

$L__BB88_9:
	mov.u32 	%r72, 1;
	mov.b32 	%r70, {%rs30, %rs29};
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r70;
  cvt.f32.f16 %f6, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r70;
  cvt.f32.f16 %f7, high;}

	// end inline asm
	add.f32 	%f8, %f6, %f7;
	mov.b32 	%r73, %f8;
	mov.u32 	%r74, 31;
	mov.u32 	%r75, 16;
	mov.u32 	%r76, -1;
	shfl.sync.bfly.b32 	%r77|%p8, %r73, %r75, %r74, %r76;
	mov.b32 	%f9, %r77;
	add.f32 	%f10, %f8, %f9;
	mov.b32 	%r78, %f10;
	mov.u32 	%r79, 8;
	shfl.sync.bfly.b32 	%r80|%p9, %r78, %r79, %r74, %r76;
	mov.b32 	%f11, %r80;
	add.f32 	%f12, %f10, %f11;
	mov.b32 	%r81, %f12;
	mov.u32 	%r82, 4;
	shfl.sync.bfly.b32 	%r83|%p10, %r81, %r82, %r74, %r76;
	mov.b32 	%f13, %r83;
	add.f32 	%f14, %f12, %f13;
	mov.b32 	%r84, %f14;
	mov.u32 	%r85, 2;
	shfl.sync.bfly.b32 	%r86|%p11, %r84, %r85, %r74, %r76;
	mov.b32 	%f15, %r86;
	add.f32 	%f16, %f14, %f15;
	mov.b32 	%r87, %f16;
	shfl.sync.bfly.b32 	%r88|%p12, %r87, %r72, %r74, %r76;
	mov.b32 	%f17, %r88;
	add.f32 	%f29, %f16, %f17;
	shr.s32 	%r89, %r1, 31;
	shr.u32 	%r90, %r89, 27;
	add.s32 	%r91, %r1, %r90;
	shr.s32 	%r92, %r91, 5;
	shl.b32 	%r93, %r92, 2;
	add.s32 	%r95, %r26, %r93;
	st.shared.f32 	[%r95], %f29;
	bar.sync 	0;
	@%p1 bra 	$L__BB88_11;

	ld.shared.f32 	%f18, [%r2];
	mov.b32 	%r101, %f18;
	shfl.sync.bfly.b32 	%r105|%p13, %r101, %r75, %r74, %r76;
	mov.b32 	%f19, %r105;
	add.f32 	%f20, %f18, %f19;
	mov.b32 	%r106, %f20;
	shfl.sync.bfly.b32 	%r108|%p14, %r106, %r79, %r74, %r76;
	mov.b32 	%f21, %r108;
	add.f32 	%f22, %f20, %f21;
	mov.b32 	%r109, %f22;
	shfl.sync.bfly.b32 	%r111|%p15, %r109, %r82, %r74, %r76;
	mov.b32 	%f23, %r111;
	add.f32 	%f24, %f22, %f23;
	mov.b32 	%r112, %f24;
	shfl.sync.bfly.b32 	%r113|%p16, %r112, %r85, %r74, %r76;
	mov.b32 	%f25, %r113;
	add.f32 	%f26, %f24, %f25;
	mov.b32 	%r114, %f26;
	shfl.sync.bfly.b32 	%r116|%p17, %r114, %r72, %r74, %r76;
	mov.b32 	%f27, %r116;
	add.f32 	%f29, %f26, %f27;

$L__BB88_11:
	bar.sync 	0;
	setp.gt.s32 	%p18, %r1, 0;
	@%p18 bra 	$L__BB88_13;

	mad.lo.s32 	%r120, %r1, %r14, %r20;
	cvt.s64.s32 	%rd35, %r120;
	mul.lo.s32 	%r122, %r21, %r16;
	cvt.s64.s32 	%rd36, %r122;
	add.s64 	%rd37, %rd36, %rd35;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs20, %f29;}

	// end inline asm
	cvta.to.global.u64 	%rd38, %rd16;
	shl.b64 	%rd39, %rd37, 1;
	add.s64 	%rd40, %rd38, %rd39;
	st.global.u16 	[%rd40], %rs20;

$L__BB88_13:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_2_bs_128
.visible .entry ggml_matvec_f16_ncols_2_bs_128(
	.param .u64 ggml_matvec_f16_ncols_2_bs_128_param_0,
	.param .u64 ggml_matvec_f16_ncols_2_bs_128_param_1,
	.param .u64 ggml_matvec_f16_ncols_2_bs_128_param_2,
	.param .u32 ggml_matvec_f16_ncols_2_bs_128_param_3,
	.param .u32 ggml_matvec_f16_ncols_2_bs_128_param_4,
	.param .u32 ggml_matvec_f16_ncols_2_bs_128_param_5,
	.param .u32 ggml_matvec_f16_ncols_2_bs_128_param_6,
	.param .u32 ggml_matvec_f16_ncols_2_bs_128_param_7,
	.param .u32 ggml_matvec_f16_ncols_2_bs_128_param_8,
	.param .u32 ggml_matvec_f16_ncols_2_bs_128_param_9,
	.param .u32 ggml_matvec_f16_ncols_2_bs_128_param_10,
	.param .u32 ggml_matvec_f16_ncols_2_bs_128_param_11
)
{
	.local .align 8 .b8 	__local_depot89[8];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<30>;
	.reg .b16 	%rs<4>;
	.reg .f32 	%f<52>;
	.reg .b32 	%r<201>;
	.reg .b64 	%rd<64>;


	mov.u64 	%SPL, __local_depot89;
	ld.param.u64 	%rd27, [ggml_matvec_f16_ncols_2_bs_128_param_0];
	ld.param.u64 	%rd28, [ggml_matvec_f16_ncols_2_bs_128_param_1];
	ld.param.u64 	%rd26, [ggml_matvec_f16_ncols_2_bs_128_param_2];
	ld.param.u32 	%r28, [ggml_matvec_f16_ncols_2_bs_128_param_3];
	ld.param.u32 	%r32, [ggml_matvec_f16_ncols_2_bs_128_param_5];
	ld.param.u32 	%r29, [ggml_matvec_f16_ncols_2_bs_128_param_6];
	ld.param.u32 	%r30, [ggml_matvec_f16_ncols_2_bs_128_param_7];
	ld.param.u32 	%r33, [ggml_matvec_f16_ncols_2_bs_128_param_8];
	ld.param.u32 	%r34, [ggml_matvec_f16_ncols_2_bs_128_param_9];
	ld.param.u32 	%r35, [ggml_matvec_f16_ncols_2_bs_128_param_10];
	ld.param.u32 	%r31, [ggml_matvec_f16_ncols_2_bs_128_param_11];
	cvta.to.global.u64 	%rd1, %rd28;
	cvta.to.global.u64 	%rd2, %rd27;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r36, %r1, %r33;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r37, %r2, %r32;
	mad.lo.s32 	%r38, %r36, %r34, %r37;
	cvt.s64.s32 	%rd4, %r38;
	mul.lo.s32 	%r39, %r1, %r35;
	cvt.s64.s32 	%rd5, %r39;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r40, %r3, 2;
	mov.u32 	%r41, data_mmv;
	add.s32 	%r4, %r41, %r40;
	@%p1 bra 	$L__BB89_2;

	mov.u32 	%r42, 0;
	st.shared.u32 	[%r4], %r42;

$L__BB89_2:
	bar.sync 	0;
	mov.f32 	%f3, 0f00000000;
	st.local.v2.f32 	[%rd3], {%f3, %f3};
	mov.u32 	%r199, 0;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs2, %f3;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f3;}

	// end inline asm
	mov.b32 	%r200, {%rs1, %rs2};
	setp.ge.s32 	%p2, %r3, %r28;
	@%p2 bra 	$L__BB89_9;

	not.b32 	%r45, %r3;
	add.s32 	%r6, %r45, %r28;
	shr.u32 	%r46, %r6, 7;
	add.s32 	%r47, %r46, 1;
	and.b32  	%r192, %r47, 3;
	setp.eq.s32 	%p3, %r192, 0;
	mov.u32 	%r199, 0;
	mov.u32 	%r195, %r3;
	@%p3 bra 	$L__BB89_6;

	mul.wide.s32 	%rd30, %r29, 2;
	mul.wide.s32 	%rd31, %r3, 2;
	add.s64 	%rd32, %rd30, %rd31;
	add.s64 	%rd33, %rd32, %rd5;
	shl.b64 	%rd34, %rd33, 1;
	add.s64 	%rd60, %rd1, %rd34;
	add.s64 	%rd35, %rd31, %rd5;
	shl.b64 	%rd36, %rd35, 1;
	add.s64 	%rd59, %rd1, %rd36;
	add.s64 	%rd37, %rd31, %rd4;
	shl.b64 	%rd38, %rd37, 1;
	add.s64 	%rd58, %rd2, %rd38;
	mov.u32 	%r199, 0;
	mov.u32 	%r195, %r3;

$L__BB89_5:
	.pragma "nounroll";
	ld.global.nc.u32 	%r50, [%rd58];
	ld.global.nc.u32 	%r51, [%rd59];
	// begin inline asm
	{mul.f16x2 %r49,%r50,%r51;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r200,%r200,%r49;
}
	// end inline asm
	ld.global.nc.u32 	%r57, [%rd60];
	// begin inline asm
	{mul.f16x2 %r55,%r50,%r57;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r199,%r199,%r55;
}
	// end inline asm
	add.s32 	%r195, %r195, 128;
	add.s64 	%rd60, %rd60, 512;
	add.s64 	%rd59, %rd59, 512;
	add.s64 	%rd58, %rd58, 512;
	add.s32 	%r192, %r192, -1;
	setp.ne.s32 	%p4, %r192, 0;
	@%p4 bra 	$L__BB89_5;

$L__BB89_6:
	setp.lt.u32 	%p5, %r6, 384;
	@%p5 bra 	$L__BB89_9;

	mul.wide.s32 	%rd39, %r195, 2;
	add.s64 	%rd40, %rd39, %rd4;
	shl.b64 	%rd41, %rd40, 1;
	add.s64 	%rd42, %rd2, %rd41;
	add.s64 	%rd63, %rd42, 1024;
	add.s64 	%rd43, %rd39, %rd5;
	shl.b64 	%rd44, %rd43, 1;
	add.s64 	%rd45, %rd1, %rd44;
	add.s64 	%rd62, %rd45, 1536;
	mul.wide.s32 	%rd46, %r29, 2;
	add.s64 	%rd47, %rd43, %rd46;
	shl.b64 	%rd48, %rd47, 1;
	add.s64 	%rd49, %rd1, %rd48;
	add.s64 	%rd61, %rd49, 1024;

$L__BB89_8:
	ld.global.nc.u32 	%r62, [%rd63+-1024];
	ld.global.nc.u32 	%r63, [%rd62+-1536];
	// begin inline asm
	{mul.f16x2 %r61,%r62,%r63;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r64,%r200,%r61;
}
	// end inline asm
	ld.global.nc.u32 	%r69, [%rd61+-1024];
	// begin inline asm
	{mul.f16x2 %r67,%r62,%r69;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r70,%r199,%r67;
}
	// end inline asm
	ld.global.nc.u32 	%r74, [%rd63+-512];
	ld.global.nc.u32 	%r75, [%rd62+-1024];
	// begin inline asm
	{mul.f16x2 %r73,%r74,%r75;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r76,%r64,%r73;
}
	// end inline asm
	ld.global.nc.u32 	%r81, [%rd61+-512];
	// begin inline asm
	{mul.f16x2 %r79,%r74,%r81;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r82,%r70,%r79;
}
	// end inline asm
	ld.global.nc.u32 	%r86, [%rd63];
	ld.global.nc.u32 	%r87, [%rd62+-512];
	// begin inline asm
	{mul.f16x2 %r85,%r86,%r87;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r88,%r76,%r85;
}
	// end inline asm
	ld.global.nc.u32 	%r93, [%rd61];
	// begin inline asm
	{mul.f16x2 %r91,%r86,%r93;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r94,%r82,%r91;
}
	// end inline asm
	ld.global.nc.u32 	%r98, [%rd63+512];
	ld.global.nc.u32 	%r99, [%rd62];
	// begin inline asm
	{mul.f16x2 %r97,%r98,%r99;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r200,%r88,%r97;
}
	// end inline asm
	ld.global.nc.u32 	%r105, [%rd61+512];
	// begin inline asm
	{mul.f16x2 %r103,%r98,%r105;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r199,%r94,%r103;
}
	// end inline asm
	add.s64 	%rd63, %rd63, 2048;
	add.s64 	%rd62, %rd62, 2048;
	add.s64 	%rd61, %rd61, 2048;
	add.s32 	%r195, %r195, 512;
	setp.lt.s32 	%p6, %r195, %r28;
	@%p6 bra 	$L__BB89_8;

$L__BB89_9:
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r200;
  cvt.f32.f16 %f4, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r200;
  cvt.f32.f16 %f5, high;}

	// end inline asm
	add.f32 	%f8, %f4, %f5;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r199;
  cvt.f32.f16 %f6, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r199;
  cvt.f32.f16 %f7, high;}

	// end inline asm
	add.f32 	%f1, %f6, %f7;
	st.local.f32 	[%rd3+4], %f1;
	shr.s32 	%r113, %r3, 31;
	shr.u32 	%r114, %r113, 27;
	add.s32 	%r115, %r3, %r114;
	shr.s32 	%r116, %r115, 5;
	shl.b32 	%r117, %r116, 2;
	add.s32 	%r27, %r41, %r117;
	mov.u32 	%r119, 2;
	mov.b32 	%r120, %f8;
	mov.u32 	%r121, 31;
	mov.u32 	%r122, 16;
	mov.u32 	%r123, -1;
	shfl.sync.bfly.b32 	%r124|%p7, %r120, %r122, %r121, %r123;
	mov.b32 	%f9, %r124;
	add.f32 	%f10, %f8, %f9;
	mov.b32 	%r125, %f10;
	mov.u32 	%r126, 8;
	shfl.sync.bfly.b32 	%r127|%p8, %r125, %r126, %r121, %r123;
	mov.b32 	%f11, %r127;
	add.f32 	%f12, %f10, %f11;
	mov.b32 	%r128, %f12;
	mov.u32 	%r129, 4;
	shfl.sync.bfly.b32 	%r130|%p9, %r128, %r129, %r121, %r123;
	mov.b32 	%f13, %r130;
	add.f32 	%f14, %f12, %f13;
	mov.b32 	%r131, %f14;
	shfl.sync.bfly.b32 	%r132|%p10, %r131, %r119, %r121, %r123;
	mov.b32 	%f15, %r132;
	add.f32 	%f16, %f14, %f15;
	mov.b32 	%r133, %f16;
	mov.u32 	%r134, 1;
	shfl.sync.bfly.b32 	%r135|%p11, %r133, %r134, %r121, %r123;
	mov.b32 	%f17, %r135;
	add.f32 	%f18, %f16, %f17;
	st.local.f32 	[%rd3], %f18;
	st.shared.f32 	[%r27], %f18;
	bar.sync 	0;
	@%p1 bra 	$L__BB89_11;

	ld.shared.f32 	%f19, [%r4];
	mov.b32 	%r136, %f19;
	shfl.sync.bfly.b32 	%r140|%p13, %r136, %r122, %r121, %r123;
	mov.b32 	%f20, %r140;
	add.f32 	%f21, %f19, %f20;
	mov.b32 	%r141, %f21;
	shfl.sync.bfly.b32 	%r143|%p14, %r141, %r126, %r121, %r123;
	mov.b32 	%f22, %r143;
	add.f32 	%f23, %f21, %f22;
	mov.b32 	%r144, %f23;
	shfl.sync.bfly.b32 	%r146|%p15, %r144, %r129, %r121, %r123;
	mov.b32 	%f24, %r146;
	add.f32 	%f25, %f23, %f24;
	mov.b32 	%r147, %f25;
	shfl.sync.bfly.b32 	%r149|%p16, %r147, %r119, %r121, %r123;
	mov.b32 	%f26, %r149;
	add.f32 	%f27, %f25, %f26;
	mov.b32 	%r150, %f27;
	shfl.sync.bfly.b32 	%r152|%p17, %r150, %r134, %r121, %r123;
	mov.b32 	%f28, %r152;
	add.f32 	%f29, %f27, %f28;
	st.local.f32 	[%rd3], %f29;

$L__BB89_11:
	bar.sync 	0;
	mov.b32 	%r153, %f1;
	shfl.sync.bfly.b32 	%r157|%p19, %r153, %r122, %r121, %r123;
	mov.b32 	%f30, %r157;
	add.f32 	%f31, %f1, %f30;
	mov.b32 	%r158, %f31;
	shfl.sync.bfly.b32 	%r160|%p20, %r158, %r126, %r121, %r123;
	mov.b32 	%f32, %r160;
	add.f32 	%f33, %f31, %f32;
	mov.b32 	%r161, %f33;
	shfl.sync.bfly.b32 	%r163|%p21, %r161, %r129, %r121, %r123;
	mov.b32 	%f34, %r163;
	add.f32 	%f35, %f33, %f34;
	mov.b32 	%r164, %f35;
	shfl.sync.bfly.b32 	%r166|%p22, %r164, %r119, %r121, %r123;
	mov.b32 	%f36, %r166;
	add.f32 	%f37, %f35, %f36;
	mov.b32 	%r167, %f37;
	shfl.sync.bfly.b32 	%r169|%p23, %r167, %r134, %r121, %r123;
	mov.b32 	%f38, %r169;
	add.f32 	%f39, %f37, %f38;
	st.local.f32 	[%rd3+4], %f39;
	st.shared.f32 	[%r27], %f39;
	bar.sync 	0;
	@%p1 bra 	$L__BB89_13;

	ld.shared.f32 	%f40, [%r4];
	mov.b32 	%r170, %f40;
	mov.u32 	%r171, 31;
	mov.u32 	%r172, 16;
	mov.u32 	%r173, -1;
	shfl.sync.bfly.b32 	%r174|%p24, %r170, %r172, %r171, %r173;
	mov.b32 	%f41, %r174;
	add.f32 	%f42, %f40, %f41;
	mov.b32 	%r175, %f42;
	mov.u32 	%r176, 8;
	shfl.sync.bfly.b32 	%r177|%p25, %r175, %r176, %r171, %r173;
	mov.b32 	%f43, %r177;
	add.f32 	%f44, %f42, %f43;
	mov.b32 	%r178, %f44;
	mov.u32 	%r179, 4;
	shfl.sync.bfly.b32 	%r180|%p26, %r178, %r179, %r171, %r173;
	mov.b32 	%f45, %r180;
	add.f32 	%f46, %f44, %f45;
	mov.b32 	%r181, %f46;
	mov.u32 	%r182, 2;
	shfl.sync.bfly.b32 	%r183|%p27, %r181, %r182, %r171, %r173;
	mov.b32 	%f47, %r183;
	add.f32 	%f48, %f46, %f47;
	mov.b32 	%r184, %f48;
	mov.u32 	%r185, 1;
	shfl.sync.bfly.b32 	%r186|%p28, %r184, %r185, %r171, %r173;
	mov.b32 	%f49, %r186;
	add.f32 	%f50, %f48, %f49;
	st.local.f32 	[%rd3+4], %f50;

$L__BB89_13:
	bar.sync 	0;
	setp.gt.s32 	%p29, %r3, 1;
	@%p29 bra 	$L__BB89_15;

	mad.lo.s32 	%r187, %r3, %r30, %r2;
	cvt.s64.s32 	%rd50, %r187;
	mul.lo.s32 	%r188, %r1, %r31;
	cvt.s64.s32 	%rd51, %r188;
	add.s64 	%rd52, %rd51, %rd50;
	mul.wide.s32 	%rd53, %r3, 4;
	add.s64 	%rd54, %rd3, %rd53;
	ld.local.f32 	%f51, [%rd54];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs3, %f51;}

	// end inline asm
	cvta.to.global.u64 	%rd55, %rd26;
	shl.b64 	%rd56, %rd52, 1;
	add.s64 	%rd57, %rd55, %rd56;
	st.global.u16 	[%rd57], %rs3;

$L__BB89_15:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_3_bs_128
.visible .entry ggml_matvec_f16_ncols_3_bs_128(
	.param .u64 ggml_matvec_f16_ncols_3_bs_128_param_0,
	.param .u64 ggml_matvec_f16_ncols_3_bs_128_param_1,
	.param .u64 ggml_matvec_f16_ncols_3_bs_128_param_2,
	.param .u32 ggml_matvec_f16_ncols_3_bs_128_param_3,
	.param .u32 ggml_matvec_f16_ncols_3_bs_128_param_4,
	.param .u32 ggml_matvec_f16_ncols_3_bs_128_param_5,
	.param .u32 ggml_matvec_f16_ncols_3_bs_128_param_6,
	.param .u32 ggml_matvec_f16_ncols_3_bs_128_param_7,
	.param .u32 ggml_matvec_f16_ncols_3_bs_128_param_8,
	.param .u32 ggml_matvec_f16_ncols_3_bs_128_param_9,
	.param .u32 ggml_matvec_f16_ncols_3_bs_128_param_10,
	.param .u32 ggml_matvec_f16_ncols_3_bs_128_param_11
)
{
	.local .align 4 .b8 	__local_depot90[12];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<41>;
	.reg .b16 	%rs<4>;
	.reg .f32 	%f<76>;
	.reg .b32 	%r<286>;
	.reg .b64 	%rd<72>;


	mov.u64 	%SPL, __local_depot90;
	ld.param.u64 	%rd29, [ggml_matvec_f16_ncols_3_bs_128_param_0];
	ld.param.u64 	%rd30, [ggml_matvec_f16_ncols_3_bs_128_param_1];
	ld.param.u64 	%rd28, [ggml_matvec_f16_ncols_3_bs_128_param_2];
	ld.param.u32 	%r34, [ggml_matvec_f16_ncols_3_bs_128_param_3];
	ld.param.u32 	%r38, [ggml_matvec_f16_ncols_3_bs_128_param_5];
	ld.param.u32 	%r35, [ggml_matvec_f16_ncols_3_bs_128_param_6];
	ld.param.u32 	%r36, [ggml_matvec_f16_ncols_3_bs_128_param_7];
	ld.param.u32 	%r39, [ggml_matvec_f16_ncols_3_bs_128_param_8];
	ld.param.u32 	%r40, [ggml_matvec_f16_ncols_3_bs_128_param_9];
	ld.param.u32 	%r41, [ggml_matvec_f16_ncols_3_bs_128_param_10];
	ld.param.u32 	%r37, [ggml_matvec_f16_ncols_3_bs_128_param_11];
	cvta.to.global.u64 	%rd71, %rd30;
	cvta.to.global.u64 	%rd2, %rd29;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r42, %r1, %r39;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r43, %r2, %r38;
	mad.lo.s32 	%r44, %r42, %r40, %r43;
	cvt.s64.s32 	%rd4, %r44;
	mul.lo.s32 	%r45, %r1, %r41;
	cvt.s64.s32 	%rd5, %r45;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r46, %r3, 2;
	mov.u32 	%r47, data_mmv;
	add.s32 	%r4, %r47, %r46;
	@%p1 bra 	$L__BB90_2;

	mov.u32 	%r48, 0;
	st.shared.u32 	[%r4], %r48;

$L__BB90_2:
	bar.sync 	0;
	mov.f32 	%f4, 0f00000000;
	mov.u32 	%r283, 0;
	st.local.u32 	[%rd3], %r283;
	st.local.u32 	[%rd3+4], %r283;
	st.local.u32 	[%rd3+8], %r283;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs2, %f4;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f4;}

	// end inline asm
	mov.b32 	%r285, {%rs1, %rs2};
	setp.ge.s32 	%p2, %r3, %r34;
	mov.u32 	%r284, %r283;
	@%p2 bra 	$L__BB90_9;

	not.b32 	%r53, %r3;
	add.s32 	%r6, %r53, %r34;
	shr.u32 	%r54, %r6, 7;
	add.s32 	%r55, %r54, 1;
	and.b32  	%r274, %r55, 3;
	setp.eq.s32 	%p3, %r274, 0;
	mov.u32 	%r283, 0;
	mov.u32 	%r278, %r3;
	@%p3 bra 	$L__BB90_6;

	shl.b32 	%r58, %r35, 1;
	add.s32 	%r59, %r3, %r58;
	mul.wide.s32 	%rd32, %r59, 2;
	add.s64 	%rd33, %rd32, %rd5;
	shl.b64 	%rd34, %rd33, 1;
	add.s64 	%rd69, %rd71, %rd34;
	mul.wide.s32 	%rd35, %r35, 2;
	mul.wide.s32 	%rd36, %r3, 2;
	add.s64 	%rd37, %rd35, %rd36;
	add.s64 	%rd38, %rd37, %rd5;
	shl.b64 	%rd39, %rd38, 1;
	add.s64 	%rd68, %rd71, %rd39;
	add.s64 	%rd40, %rd36, %rd5;
	shl.b64 	%rd41, %rd40, 1;
	add.s64 	%rd67, %rd71, %rd41;
	add.s64 	%rd42, %rd36, %rd4;
	shl.b64 	%rd43, %rd42, 1;
	add.s64 	%rd66, %rd2, %rd43;
	mov.u32 	%r283, 0;
	mov.u32 	%r284, %r283;
	mov.u32 	%r278, %r3;

$L__BB90_5:
	.pragma "nounroll";
	ld.global.nc.u32 	%r61, [%rd66];
	ld.global.nc.u32 	%r62, [%rd67];
	// begin inline asm
	{mul.f16x2 %r60,%r61,%r62;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r285,%r285,%r60;
}
	// end inline asm
	ld.global.nc.u32 	%r68, [%rd68];
	// begin inline asm
	{mul.f16x2 %r66,%r61,%r68;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r284,%r284,%r66;
}
	// end inline asm
	ld.global.nc.u32 	%r74, [%rd69];
	// begin inline asm
	{mul.f16x2 %r72,%r61,%r74;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r283,%r283,%r72;
}
	// end inline asm
	add.s32 	%r278, %r278, 128;
	add.s64 	%rd69, %rd69, 512;
	add.s64 	%rd68, %rd68, 512;
	add.s64 	%rd67, %rd67, 512;
	add.s64 	%rd66, %rd66, 512;
	add.s32 	%r274, %r274, -1;
	setp.ne.s32 	%p4, %r274, 0;
	@%p4 bra 	$L__BB90_5;

$L__BB90_6:
	setp.lt.u32 	%p5, %r6, 384;
	@%p5 bra 	$L__BB90_9;

	add.s32 	%r78, %r278, %r35;
	shl.b32 	%r79, %r35, 1;
	add.s32 	%r80, %r278, %r79;
	add.s32 	%r81, %r78, 128;
	mul.wide.s32 	%rd44, %r81, 4;
	shl.b64 	%rd45, %rd5, 1;
	add.s64 	%rd19, %rd44, %rd45;
	mul.wide.s32 	%rd46, %r80, 4;
	add.s64 	%rd20, %rd46, %rd45;
	mul.wide.s32 	%rd47, %r278, 2;
	add.s64 	%rd48, %rd47, %rd4;
	shl.b64 	%rd49, %rd48, 1;
	add.s64 	%rd50, %rd2, %rd49;
	add.s64 	%rd70, %rd50, 1024;
	mul.wide.s32 	%rd51, %r278, 4;
	add.s64 	%rd22, %rd51, %rd45;
	mul.wide.s32 	%rd52, %r35, 4;
	add.s64 	%rd53, %rd51, %rd52;
	add.s64 	%rd23, %rd53, %rd45;

$L__BB90_8:
	ld.global.nc.u32 	%r83, [%rd70+-1024];
	add.s64 	%rd54, %rd71, %rd22;
	ld.global.nc.u32 	%r84, [%rd54];
	// begin inline asm
	{mul.f16x2 %r82,%r83,%r84;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r85,%r285,%r82;
}
	// end inline asm
	add.s64 	%rd55, %rd71, %rd23;
	ld.global.nc.u32 	%r90, [%rd55];
	// begin inline asm
	{mul.f16x2 %r88,%r83,%r90;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r91,%r284,%r88;
}
	// end inline asm
	add.s64 	%rd56, %rd71, %rd20;
	ld.global.nc.u32 	%r96, [%rd56];
	// begin inline asm
	{mul.f16x2 %r94,%r83,%r96;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r97,%r283,%r94;
}
	// end inline asm
	ld.global.nc.u32 	%r101, [%rd70+-512];
	ld.global.nc.u32 	%r102, [%rd54+512];
	// begin inline asm
	{mul.f16x2 %r100,%r101,%r102;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r103,%r85,%r100;
}
	// end inline asm
	add.s64 	%rd57, %rd71, %rd19;
	ld.global.nc.u32 	%r108, [%rd57];
	// begin inline asm
	{mul.f16x2 %r106,%r101,%r108;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r109,%r91,%r106;
}
	// end inline asm
	ld.global.nc.u32 	%r114, [%rd56+512];
	// begin inline asm
	{mul.f16x2 %r112,%r101,%r114;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r115,%r97,%r112;
}
	// end inline asm
	ld.global.nc.u32 	%r119, [%rd70];
	ld.global.nc.u32 	%r120, [%rd54+1024];
	// begin inline asm
	{mul.f16x2 %r118,%r119,%r120;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r121,%r103,%r118;
}
	// end inline asm
	ld.global.nc.u32 	%r126, [%rd57+512];
	// begin inline asm
	{mul.f16x2 %r124,%r119,%r126;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r127,%r109,%r124;
}
	// end inline asm
	ld.global.nc.u32 	%r132, [%rd56+1024];
	// begin inline asm
	{mul.f16x2 %r130,%r119,%r132;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r133,%r115,%r130;
}
	// end inline asm
	ld.global.nc.u32 	%r137, [%rd70+512];
	ld.global.nc.u32 	%r138, [%rd54+1536];
	// begin inline asm
	{mul.f16x2 %r136,%r137,%r138;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r285,%r121,%r136;
}
	// end inline asm
	ld.global.nc.u32 	%r144, [%rd57+1024];
	// begin inline asm
	{mul.f16x2 %r142,%r137,%r144;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r284,%r127,%r142;
}
	// end inline asm
	ld.global.nc.u32 	%r150, [%rd56+1536];
	// begin inline asm
	{mul.f16x2 %r148,%r137,%r150;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r283,%r133,%r148;
}
	// end inline asm
	add.s64 	%rd71, %rd71, 2048;
	add.s64 	%rd70, %rd70, 2048;
	add.s32 	%r278, %r278, 512;
	setp.lt.s32 	%p6, %r278, %r34;
	@%p6 bra 	$L__BB90_8;

$L__BB90_9:
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r285;
  cvt.f32.f16 %f5, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r285;
  cvt.f32.f16 %f6, high;}

	// end inline asm
	add.f32 	%f11, %f5, %f6;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r284;
  cvt.f32.f16 %f7, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r284;
  cvt.f32.f16 %f8, high;}

	// end inline asm
	add.f32 	%f1, %f7, %f8;
	st.local.f32 	[%rd3+4], %f1;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r283;
  cvt.f32.f16 %f9, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r283;
  cvt.f32.f16 %f10, high;}

	// end inline asm
	add.f32 	%f2, %f9, %f10;
	st.local.f32 	[%rd3+8], %f2;
	shr.s32 	%r160, %r3, 31;
	shr.u32 	%r161, %r160, 27;
	add.s32 	%r162, %r3, %r161;
	shr.s32 	%r163, %r162, 5;
	shl.b32 	%r164, %r163, 2;
	add.s32 	%r33, %r47, %r164;
	mov.u32 	%r166, 2;
	mov.b32 	%r167, %f11;
	mov.u32 	%r168, 31;
	mov.u32 	%r169, 16;
	mov.u32 	%r170, -1;
	shfl.sync.bfly.b32 	%r171|%p7, %r167, %r169, %r168, %r170;
	mov.b32 	%f12, %r171;
	add.f32 	%f13, %f11, %f12;
	mov.b32 	%r172, %f13;
	mov.u32 	%r173, 8;
	shfl.sync.bfly.b32 	%r174|%p8, %r172, %r173, %r168, %r170;
	mov.b32 	%f14, %r174;
	add.f32 	%f15, %f13, %f14;
	mov.b32 	%r175, %f15;
	mov.u32 	%r176, 4;
	shfl.sync.bfly.b32 	%r177|%p9, %r175, %r176, %r168, %r170;
	mov.b32 	%f16, %r177;
	add.f32 	%f17, %f15, %f16;
	mov.b32 	%r178, %f17;
	shfl.sync.bfly.b32 	%r179|%p10, %r178, %r166, %r168, %r170;
	mov.b32 	%f18, %r179;
	add.f32 	%f19, %f17, %f18;
	mov.b32 	%r180, %f19;
	mov.u32 	%r181, 1;
	shfl.sync.bfly.b32 	%r182|%p11, %r180, %r181, %r168, %r170;
	mov.b32 	%f20, %r182;
	add.f32 	%f21, %f19, %f20;
	st.local.f32 	[%rd3], %f21;
	st.shared.f32 	[%r33], %f21;
	bar.sync 	0;
	@%p1 bra 	$L__BB90_11;

	ld.shared.f32 	%f22, [%r4];
	mov.b32 	%r183, %f22;
	shfl.sync.bfly.b32 	%r187|%p13, %r183, %r169, %r168, %r170;
	mov.b32 	%f23, %r187;
	add.f32 	%f24, %f22, %f23;
	mov.b32 	%r188, %f24;
	shfl.sync.bfly.b32 	%r190|%p14, %r188, %r173, %r168, %r170;
	mov.b32 	%f25, %r190;
	add.f32 	%f26, %f24, %f25;
	mov.b32 	%r191, %f26;
	shfl.sync.bfly.b32 	%r193|%p15, %r191, %r176, %r168, %r170;
	mov.b32 	%f27, %r193;
	add.f32 	%f28, %f26, %f27;
	mov.b32 	%r194, %f28;
	shfl.sync.bfly.b32 	%r196|%p16, %r194, %r166, %r168, %r170;
	mov.b32 	%f29, %r196;
	add.f32 	%f30, %f28, %f29;
	mov.b32 	%r197, %f30;
	shfl.sync.bfly.b32 	%r199|%p17, %r197, %r181, %r168, %r170;
	mov.b32 	%f31, %r199;
	add.f32 	%f32, %f30, %f31;
	st.local.f32 	[%rd3], %f32;

$L__BB90_11:
	bar.sync 	0;
	mov.b32 	%r200, %f1;
	shfl.sync.bfly.b32 	%r204|%p19, %r200, %r169, %r168, %r170;
	mov.b32 	%f33, %r204;
	add.f32 	%f34, %f1, %f33;
	mov.b32 	%r205, %f34;
	shfl.sync.bfly.b32 	%r207|%p20, %r205, %r173, %r168, %r170;
	mov.b32 	%f35, %r207;
	add.f32 	%f36, %f34, %f35;
	mov.b32 	%r208, %f36;
	shfl.sync.bfly.b32 	%r210|%p21, %r208, %r176, %r168, %r170;
	mov.b32 	%f37, %r210;
	add.f32 	%f38, %f36, %f37;
	mov.b32 	%r211, %f38;
	shfl.sync.bfly.b32 	%r213|%p22, %r211, %r166, %r168, %r170;
	mov.b32 	%f39, %r213;
	add.f32 	%f40, %f38, %f39;
	mov.b32 	%r214, %f40;
	shfl.sync.bfly.b32 	%r216|%p23, %r214, %r181, %r168, %r170;
	mov.b32 	%f41, %r216;
	add.f32 	%f42, %f40, %f41;
	st.local.f32 	[%rd3+4], %f42;
	st.shared.f32 	[%r33], %f42;
	bar.sync 	0;
	@%p1 bra 	$L__BB90_13;

	ld.shared.f32 	%f43, [%r4];
	mov.b32 	%r217, %f43;
	mov.u32 	%r218, 31;
	mov.u32 	%r219, 16;
	mov.u32 	%r220, -1;
	shfl.sync.bfly.b32 	%r221|%p24, %r217, %r219, %r218, %r220;
	mov.b32 	%f44, %r221;
	add.f32 	%f45, %f43, %f44;
	mov.b32 	%r222, %f45;
	mov.u32 	%r223, 8;
	shfl.sync.bfly.b32 	%r224|%p25, %r222, %r223, %r218, %r220;
	mov.b32 	%f46, %r224;
	add.f32 	%f47, %f45, %f46;
	mov.b32 	%r225, %f47;
	mov.u32 	%r226, 4;
	shfl.sync.bfly.b32 	%r227|%p26, %r225, %r226, %r218, %r220;
	mov.b32 	%f48, %r227;
	add.f32 	%f49, %f47, %f48;
	mov.b32 	%r228, %f49;
	mov.u32 	%r229, 2;
	shfl.sync.bfly.b32 	%r230|%p27, %r228, %r229, %r218, %r220;
	mov.b32 	%f50, %r230;
	add.f32 	%f51, %f49, %f50;
	mov.b32 	%r231, %f51;
	mov.u32 	%r232, 1;
	shfl.sync.bfly.b32 	%r233|%p28, %r231, %r232, %r218, %r220;
	mov.b32 	%f52, %r233;
	add.f32 	%f53, %f51, %f52;
	st.local.f32 	[%rd3+4], %f53;

$L__BB90_13:
	bar.sync 	0;
	mov.b32 	%r234, %f2;
	mov.u32 	%r235, 31;
	mov.u32 	%r236, 16;
	mov.u32 	%r237, -1;
	shfl.sync.bfly.b32 	%r238|%p30, %r234, %r236, %r235, %r237;
	mov.b32 	%f54, %r238;
	add.f32 	%f55, %f2, %f54;
	mov.b32 	%r239, %f55;
	mov.u32 	%r240, 8;
	shfl.sync.bfly.b32 	%r241|%p31, %r239, %r240, %r235, %r237;
	mov.b32 	%f56, %r241;
	add.f32 	%f57, %f55, %f56;
	mov.b32 	%r242, %f57;
	mov.u32 	%r243, 4;
	shfl.sync.bfly.b32 	%r244|%p32, %r242, %r243, %r235, %r237;
	mov.b32 	%f58, %r244;
	add.f32 	%f59, %f57, %f58;
	mov.b32 	%r245, %f59;
	mov.u32 	%r246, 2;
	shfl.sync.bfly.b32 	%r247|%p33, %r245, %r246, %r235, %r237;
	mov.b32 	%f60, %r247;
	add.f32 	%f61, %f59, %f60;
	mov.b32 	%r248, %f61;
	mov.u32 	%r249, 1;
	shfl.sync.bfly.b32 	%r250|%p34, %r248, %r249, %r235, %r237;
	mov.b32 	%f62, %r250;
	add.f32 	%f63, %f61, %f62;
	st.local.f32 	[%rd3+8], %f63;
	st.shared.f32 	[%r33], %f63;
	bar.sync 	0;
	@%p1 bra 	$L__BB90_15;

	ld.shared.f32 	%f64, [%r4];
	mov.b32 	%r251, %f64;
	shfl.sync.bfly.b32 	%r255|%p35, %r251, %r236, %r235, %r237;
	mov.b32 	%f65, %r255;
	add.f32 	%f66, %f64, %f65;
	mov.b32 	%r256, %f66;
	shfl.sync.bfly.b32 	%r258|%p36, %r256, %r240, %r235, %r237;
	mov.b32 	%f67, %r258;
	add.f32 	%f68, %f66, %f67;
	mov.b32 	%r259, %f68;
	shfl.sync.bfly.b32 	%r261|%p37, %r259, %r243, %r235, %r237;
	mov.b32 	%f69, %r261;
	add.f32 	%f70, %f68, %f69;
	mov.b32 	%r262, %f70;
	shfl.sync.bfly.b32 	%r264|%p38, %r262, %r246, %r235, %r237;
	mov.b32 	%f71, %r264;
	add.f32 	%f72, %f70, %f71;
	mov.b32 	%r265, %f72;
	shfl.sync.bfly.b32 	%r267|%p39, %r265, %r249, %r235, %r237;
	mov.b32 	%f73, %r267;
	add.f32 	%f74, %f72, %f73;
	st.local.f32 	[%rd3+8], %f74;

$L__BB90_15:
	bar.sync 	0;
	setp.gt.s32 	%p40, %r3, 2;
	@%p40 bra 	$L__BB90_17;

	mad.lo.s32 	%r268, %r3, %r36, %r2;
	cvt.s64.s32 	%rd58, %r268;
	mul.lo.s32 	%r269, %r1, %r37;
	cvt.s64.s32 	%rd59, %r269;
	add.s64 	%rd60, %rd59, %rd58;
	mul.wide.s32 	%rd61, %r3, 4;
	add.s64 	%rd62, %rd3, %rd61;
	ld.local.f32 	%f75, [%rd62];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs3, %f75;}

	// end inline asm
	cvta.to.global.u64 	%rd63, %rd28;
	shl.b64 	%rd64, %rd60, 1;
	add.s64 	%rd65, %rd63, %rd64;
	st.global.u16 	[%rd65], %rs3;

$L__BB90_17:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_4_bs_128
.visible .entry ggml_matvec_f16_ncols_4_bs_128(
	.param .u64 ggml_matvec_f16_ncols_4_bs_128_param_0,
	.param .u64 ggml_matvec_f16_ncols_4_bs_128_param_1,
	.param .u64 ggml_matvec_f16_ncols_4_bs_128_param_2,
	.param .u32 ggml_matvec_f16_ncols_4_bs_128_param_3,
	.param .u32 ggml_matvec_f16_ncols_4_bs_128_param_4,
	.param .u32 ggml_matvec_f16_ncols_4_bs_128_param_5,
	.param .u32 ggml_matvec_f16_ncols_4_bs_128_param_6,
	.param .u32 ggml_matvec_f16_ncols_4_bs_128_param_7,
	.param .u32 ggml_matvec_f16_ncols_4_bs_128_param_8,
	.param .u32 ggml_matvec_f16_ncols_4_bs_128_param_9,
	.param .u32 ggml_matvec_f16_ncols_4_bs_128_param_10,
	.param .u32 ggml_matvec_f16_ncols_4_bs_128_param_11
)
{
	.local .align 16 .b8 	__local_depot91[16];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<52>;
	.reg .b16 	%rs<4>;
	.reg .f32 	%f<100>;
	.reg .b32 	%r<367>;
	.reg .b64 	%rd<83>;


	mov.u64 	%SPL, __local_depot91;
	ld.param.u64 	%rd34, [ggml_matvec_f16_ncols_4_bs_128_param_0];
	ld.param.u64 	%rd35, [ggml_matvec_f16_ncols_4_bs_128_param_1];
	ld.param.u64 	%rd33, [ggml_matvec_f16_ncols_4_bs_128_param_2];
	ld.param.u32 	%r40, [ggml_matvec_f16_ncols_4_bs_128_param_3];
	ld.param.u32 	%r44, [ggml_matvec_f16_ncols_4_bs_128_param_5];
	ld.param.u32 	%r41, [ggml_matvec_f16_ncols_4_bs_128_param_6];
	ld.param.u32 	%r42, [ggml_matvec_f16_ncols_4_bs_128_param_7];
	ld.param.u32 	%r45, [ggml_matvec_f16_ncols_4_bs_128_param_8];
	ld.param.u32 	%r46, [ggml_matvec_f16_ncols_4_bs_128_param_9];
	ld.param.u32 	%r47, [ggml_matvec_f16_ncols_4_bs_128_param_10];
	ld.param.u32 	%r43, [ggml_matvec_f16_ncols_4_bs_128_param_11];
	cvta.to.global.u64 	%rd82, %rd35;
	cvta.to.global.u64 	%rd2, %rd34;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r48, %r1, %r45;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r49, %r2, %r44;
	mad.lo.s32 	%r50, %r48, %r46, %r49;
	cvt.s64.s32 	%rd4, %r50;
	mul.lo.s32 	%r51, %r1, %r47;
	cvt.s64.s32 	%rd5, %r51;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r52, %r3, 2;
	mov.u32 	%r53, data_mmv;
	add.s32 	%r4, %r53, %r52;
	@%p1 bra 	$L__BB91_2;

	mov.u32 	%r54, 0;
	st.shared.u32 	[%r4], %r54;

$L__BB91_2:
	bar.sync 	0;
	mov.f32 	%f5, 0f00000000;
	st.local.v4.f32 	[%rd3], {%f5, %f5, %f5, %f5};
	mov.u32 	%r363, 0;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs2, %f5;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f5;}

	// end inline asm
	mov.b32 	%r366, {%rs1, %rs2};
	setp.ge.s32 	%p2, %r3, %r40;
	mov.u32 	%r364, %r363;
	mov.u32 	%r365, %r363;
	@%p2 bra 	$L__BB91_9;

	not.b32 	%r61, %r3;
	add.s32 	%r6, %r61, %r40;
	shr.u32 	%r62, %r6, 7;
	add.s32 	%r63, %r62, 1;
	and.b32  	%r352, %r63, 3;
	setp.eq.s32 	%p3, %r352, 0;
	mov.u32 	%r363, 0;
	mov.u32 	%r357, %r3;
	@%p3 bra 	$L__BB91_6;

	shl.b32 	%r67, %r41, 1;
	add.s32 	%r68, %r3, %r67;
	mul.wide.s32 	%rd37, %r68, 2;
	add.s64 	%rd38, %rd37, %rd5;
	shl.b64 	%rd39, %rd38, 1;
	add.s64 	%rd80, %rd82, %rd39;
	mad.lo.s32 	%r69, %r41, 3, %r3;
	mul.wide.s32 	%rd40, %r69, 2;
	add.s64 	%rd41, %rd40, %rd5;
	shl.b64 	%rd42, %rd41, 1;
	add.s64 	%rd79, %rd82, %rd42;
	mul.wide.s32 	%rd43, %r41, 2;
	mul.wide.s32 	%rd44, %r3, 2;
	add.s64 	%rd45, %rd43, %rd44;
	add.s64 	%rd46, %rd45, %rd5;
	shl.b64 	%rd47, %rd46, 1;
	add.s64 	%rd78, %rd82, %rd47;
	add.s64 	%rd48, %rd44, %rd5;
	shl.b64 	%rd49, %rd48, 1;
	add.s64 	%rd77, %rd82, %rd49;
	add.s64 	%rd50, %rd44, %rd4;
	shl.b64 	%rd51, %rd50, 1;
	add.s64 	%rd76, %rd2, %rd51;
	mov.u32 	%r363, 0;
	mov.u32 	%r364, %r363;
	mov.u32 	%r365, %r363;
	mov.u32 	%r357, %r3;

$L__BB91_5:
	.pragma "nounroll";
	ld.global.nc.u32 	%r71, [%rd76];
	ld.global.nc.u32 	%r72, [%rd77];
	// begin inline asm
	{mul.f16x2 %r70,%r71,%r72;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r366,%r366,%r70;
}
	// end inline asm
	ld.global.nc.u32 	%r78, [%rd78];
	// begin inline asm
	{mul.f16x2 %r76,%r71,%r78;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r365,%r365,%r76;
}
	// end inline asm
	ld.global.nc.u32 	%r84, [%rd80];
	// begin inline asm
	{mul.f16x2 %r82,%r71,%r84;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r364,%r364,%r82;
}
	// end inline asm
	ld.global.nc.u32 	%r90, [%rd79];
	// begin inline asm
	{mul.f16x2 %r88,%r71,%r90;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r363,%r363,%r88;
}
	// end inline asm
	add.s32 	%r357, %r357, 128;
	add.s64 	%rd80, %rd80, 512;
	add.s64 	%rd79, %rd79, 512;
	add.s64 	%rd78, %rd78, 512;
	add.s64 	%rd77, %rd77, 512;
	add.s64 	%rd76, %rd76, 512;
	add.s32 	%r352, %r352, -1;
	setp.ne.s32 	%p4, %r352, 0;
	@%p4 bra 	$L__BB91_5;

$L__BB91_6:
	setp.lt.u32 	%p5, %r6, 384;
	@%p5 bra 	$L__BB91_9;

	add.s32 	%r94, %r357, %r41;
	shl.b32 	%r95, %r41, 1;
	add.s32 	%r96, %r357, %r95;
	mad.lo.s32 	%r97, %r41, 3, %r357;
	add.s32 	%r98, %r94, 128;
	mul.wide.s32 	%rd52, %r98, 4;
	shl.b64 	%rd53, %rd5, 1;
	add.s64 	%rd23, %rd52, %rd53;
	mul.wide.s32 	%rd54, %r96, 4;
	add.s64 	%rd24, %rd54, %rd53;
	mul.wide.s32 	%rd55, %r97, 4;
	add.s64 	%rd25, %rd55, %rd53;
	mul.wide.s32 	%rd56, %r357, 2;
	add.s64 	%rd57, %rd56, %rd4;
	shl.b64 	%rd58, %rd57, 1;
	add.s64 	%rd59, %rd2, %rd58;
	add.s64 	%rd81, %rd59, 1024;
	mul.wide.s32 	%rd60, %r357, 4;
	add.s64 	%rd27, %rd60, %rd53;
	mul.wide.s32 	%rd61, %r41, 4;
	add.s64 	%rd62, %rd60, %rd61;
	add.s64 	%rd28, %rd62, %rd53;

$L__BB91_8:
	ld.global.nc.u32 	%r100, [%rd81+-1024];
	add.s64 	%rd63, %rd82, %rd27;
	ld.global.nc.u32 	%r101, [%rd63];
	// begin inline asm
	{mul.f16x2 %r99,%r100,%r101;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r102,%r366,%r99;
}
	// end inline asm
	add.s64 	%rd64, %rd82, %rd28;
	ld.global.nc.u32 	%r107, [%rd64];
	// begin inline asm
	{mul.f16x2 %r105,%r100,%r107;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r108,%r365,%r105;
}
	// end inline asm
	add.s64 	%rd65, %rd82, %rd24;
	ld.global.nc.u32 	%r113, [%rd65];
	// begin inline asm
	{mul.f16x2 %r111,%r100,%r113;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r114,%r364,%r111;
}
	// end inline asm
	add.s64 	%rd66, %rd82, %rd25;
	ld.global.nc.u32 	%r119, [%rd66];
	// begin inline asm
	{mul.f16x2 %r117,%r100,%r119;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r120,%r363,%r117;
}
	// end inline asm
	ld.global.nc.u32 	%r124, [%rd81+-512];
	ld.global.nc.u32 	%r125, [%rd63+512];
	// begin inline asm
	{mul.f16x2 %r123,%r124,%r125;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r126,%r102,%r123;
}
	// end inline asm
	add.s64 	%rd67, %rd82, %rd23;
	ld.global.nc.u32 	%r131, [%rd67];
	// begin inline asm
	{mul.f16x2 %r129,%r124,%r131;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r132,%r108,%r129;
}
	// end inline asm
	ld.global.nc.u32 	%r137, [%rd65+512];
	// begin inline asm
	{mul.f16x2 %r135,%r124,%r137;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r138,%r114,%r135;
}
	// end inline asm
	ld.global.nc.u32 	%r143, [%rd66+512];
	// begin inline asm
	{mul.f16x2 %r141,%r124,%r143;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r144,%r120,%r141;
}
	// end inline asm
	ld.global.nc.u32 	%r148, [%rd81];
	ld.global.nc.u32 	%r149, [%rd63+1024];
	// begin inline asm
	{mul.f16x2 %r147,%r148,%r149;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r150,%r126,%r147;
}
	// end inline asm
	ld.global.nc.u32 	%r155, [%rd67+512];
	// begin inline asm
	{mul.f16x2 %r153,%r148,%r155;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r156,%r132,%r153;
}
	// end inline asm
	ld.global.nc.u32 	%r161, [%rd65+1024];
	// begin inline asm
	{mul.f16x2 %r159,%r148,%r161;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r162,%r138,%r159;
}
	// end inline asm
	ld.global.nc.u32 	%r167, [%rd66+1024];
	// begin inline asm
	{mul.f16x2 %r165,%r148,%r167;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r168,%r144,%r165;
}
	// end inline asm
	ld.global.nc.u32 	%r172, [%rd81+512];
	ld.global.nc.u32 	%r173, [%rd63+1536];
	// begin inline asm
	{mul.f16x2 %r171,%r172,%r173;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r366,%r150,%r171;
}
	// end inline asm
	ld.global.nc.u32 	%r179, [%rd67+1024];
	// begin inline asm
	{mul.f16x2 %r177,%r172,%r179;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r365,%r156,%r177;
}
	// end inline asm
	ld.global.nc.u32 	%r185, [%rd65+1536];
	// begin inline asm
	{mul.f16x2 %r183,%r172,%r185;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r364,%r162,%r183;
}
	// end inline asm
	ld.global.nc.u32 	%r191, [%rd66+1536];
	// begin inline asm
	{mul.f16x2 %r189,%r172,%r191;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r363,%r168,%r189;
}
	// end inline asm
	add.s64 	%rd82, %rd82, 2048;
	add.s64 	%rd81, %rd81, 2048;
	add.s32 	%r357, %r357, 512;
	setp.lt.s32 	%p6, %r357, %r40;
	@%p6 bra 	$L__BB91_8;

$L__BB91_9:
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r366;
  cvt.f32.f16 %f6, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r366;
  cvt.f32.f16 %f7, high;}

	// end inline asm
	add.f32 	%f14, %f6, %f7;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r365;
  cvt.f32.f16 %f8, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r365;
  cvt.f32.f16 %f9, high;}

	// end inline asm
	add.f32 	%f1, %f8, %f9;
	st.local.f32 	[%rd3+4], %f1;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r364;
  cvt.f32.f16 %f10, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r364;
  cvt.f32.f16 %f11, high;}

	// end inline asm
	add.f32 	%f2, %f10, %f11;
	st.local.f32 	[%rd3+8], %f2;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r363;
  cvt.f32.f16 %f12, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r363;
  cvt.f32.f16 %f13, high;}

	// end inline asm
	add.f32 	%f3, %f12, %f13;
	st.local.f32 	[%rd3+12], %f3;
	shr.s32 	%r203, %r3, 31;
	shr.u32 	%r204, %r203, 27;
	add.s32 	%r205, %r3, %r204;
	shr.s32 	%r206, %r205, 5;
	shl.b32 	%r207, %r206, 2;
	add.s32 	%r39, %r53, %r207;
	mov.u32 	%r209, 2;
	mov.b32 	%r210, %f14;
	mov.u32 	%r211, 31;
	mov.u32 	%r212, 16;
	mov.u32 	%r213, -1;
	shfl.sync.bfly.b32 	%r214|%p7, %r210, %r212, %r211, %r213;
	mov.b32 	%f15, %r214;
	add.f32 	%f16, %f14, %f15;
	mov.b32 	%r215, %f16;
	mov.u32 	%r216, 8;
	shfl.sync.bfly.b32 	%r217|%p8, %r215, %r216, %r211, %r213;
	mov.b32 	%f17, %r217;
	add.f32 	%f18, %f16, %f17;
	mov.b32 	%r218, %f18;
	mov.u32 	%r219, 4;
	shfl.sync.bfly.b32 	%r220|%p9, %r218, %r219, %r211, %r213;
	mov.b32 	%f19, %r220;
	add.f32 	%f20, %f18, %f19;
	mov.b32 	%r221, %f20;
	shfl.sync.bfly.b32 	%r222|%p10, %r221, %r209, %r211, %r213;
	mov.b32 	%f21, %r222;
	add.f32 	%f22, %f20, %f21;
	mov.b32 	%r223, %f22;
	mov.u32 	%r224, 1;
	shfl.sync.bfly.b32 	%r225|%p11, %r223, %r224, %r211, %r213;
	mov.b32 	%f23, %r225;
	add.f32 	%f24, %f22, %f23;
	st.local.f32 	[%rd3], %f24;
	st.shared.f32 	[%r39], %f24;
	bar.sync 	0;
	@%p1 bra 	$L__BB91_11;

	ld.shared.f32 	%f25, [%r4];
	mov.b32 	%r226, %f25;
	shfl.sync.bfly.b32 	%r230|%p13, %r226, %r212, %r211, %r213;
	mov.b32 	%f26, %r230;
	add.f32 	%f27, %f25, %f26;
	mov.b32 	%r231, %f27;
	shfl.sync.bfly.b32 	%r233|%p14, %r231, %r216, %r211, %r213;
	mov.b32 	%f28, %r233;
	add.f32 	%f29, %f27, %f28;
	mov.b32 	%r234, %f29;
	shfl.sync.bfly.b32 	%r236|%p15, %r234, %r219, %r211, %r213;
	mov.b32 	%f30, %r236;
	add.f32 	%f31, %f29, %f30;
	mov.b32 	%r237, %f31;
	shfl.sync.bfly.b32 	%r239|%p16, %r237, %r209, %r211, %r213;
	mov.b32 	%f32, %r239;
	add.f32 	%f33, %f31, %f32;
	mov.b32 	%r240, %f33;
	shfl.sync.bfly.b32 	%r242|%p17, %r240, %r224, %r211, %r213;
	mov.b32 	%f34, %r242;
	add.f32 	%f35, %f33, %f34;
	st.local.f32 	[%rd3], %f35;

$L__BB91_11:
	bar.sync 	0;
	mov.b32 	%r243, %f1;
	shfl.sync.bfly.b32 	%r247|%p19, %r243, %r212, %r211, %r213;
	mov.b32 	%f36, %r247;
	add.f32 	%f37, %f1, %f36;
	mov.b32 	%r248, %f37;
	shfl.sync.bfly.b32 	%r250|%p20, %r248, %r216, %r211, %r213;
	mov.b32 	%f38, %r250;
	add.f32 	%f39, %f37, %f38;
	mov.b32 	%r251, %f39;
	shfl.sync.bfly.b32 	%r253|%p21, %r251, %r219, %r211, %r213;
	mov.b32 	%f40, %r253;
	add.f32 	%f41, %f39, %f40;
	mov.b32 	%r254, %f41;
	shfl.sync.bfly.b32 	%r256|%p22, %r254, %r209, %r211, %r213;
	mov.b32 	%f42, %r256;
	add.f32 	%f43, %f41, %f42;
	mov.b32 	%r257, %f43;
	shfl.sync.bfly.b32 	%r259|%p23, %r257, %r224, %r211, %r213;
	mov.b32 	%f44, %r259;
	add.f32 	%f45, %f43, %f44;
	st.local.f32 	[%rd3+4], %f45;
	st.shared.f32 	[%r39], %f45;
	bar.sync 	0;
	@%p1 bra 	$L__BB91_13;

	ld.shared.f32 	%f46, [%r4];
	mov.b32 	%r260, %f46;
	mov.u32 	%r261, 31;
	mov.u32 	%r262, 16;
	mov.u32 	%r263, -1;
	shfl.sync.bfly.b32 	%r264|%p24, %r260, %r262, %r261, %r263;
	mov.b32 	%f47, %r264;
	add.f32 	%f48, %f46, %f47;
	mov.b32 	%r265, %f48;
	mov.u32 	%r266, 8;
	shfl.sync.bfly.b32 	%r267|%p25, %r265, %r266, %r261, %r263;
	mov.b32 	%f49, %r267;
	add.f32 	%f50, %f48, %f49;
	mov.b32 	%r268, %f50;
	mov.u32 	%r269, 4;
	shfl.sync.bfly.b32 	%r270|%p26, %r268, %r269, %r261, %r263;
	mov.b32 	%f51, %r270;
	add.f32 	%f52, %f50, %f51;
	mov.b32 	%r271, %f52;
	mov.u32 	%r272, 2;
	shfl.sync.bfly.b32 	%r273|%p27, %r271, %r272, %r261, %r263;
	mov.b32 	%f53, %r273;
	add.f32 	%f54, %f52, %f53;
	mov.b32 	%r274, %f54;
	mov.u32 	%r275, 1;
	shfl.sync.bfly.b32 	%r276|%p28, %r274, %r275, %r261, %r263;
	mov.b32 	%f55, %r276;
	add.f32 	%f56, %f54, %f55;
	st.local.f32 	[%rd3+4], %f56;

$L__BB91_13:
	bar.sync 	0;
	mov.b32 	%r277, %f2;
	mov.u32 	%r278, 31;
	mov.u32 	%r279, 16;
	mov.u32 	%r280, -1;
	shfl.sync.bfly.b32 	%r281|%p30, %r277, %r279, %r278, %r280;
	mov.b32 	%f57, %r281;
	add.f32 	%f58, %f2, %f57;
	mov.b32 	%r282, %f58;
	mov.u32 	%r283, 8;
	shfl.sync.bfly.b32 	%r284|%p31, %r282, %r283, %r278, %r280;
	mov.b32 	%f59, %r284;
	add.f32 	%f60, %f58, %f59;
	mov.b32 	%r285, %f60;
	mov.u32 	%r286, 4;
	shfl.sync.bfly.b32 	%r287|%p32, %r285, %r286, %r278, %r280;
	mov.b32 	%f61, %r287;
	add.f32 	%f62, %f60, %f61;
	mov.b32 	%r288, %f62;
	mov.u32 	%r289, 2;
	shfl.sync.bfly.b32 	%r290|%p33, %r288, %r289, %r278, %r280;
	mov.b32 	%f63, %r290;
	add.f32 	%f64, %f62, %f63;
	mov.b32 	%r291, %f64;
	mov.u32 	%r292, 1;
	shfl.sync.bfly.b32 	%r293|%p34, %r291, %r292, %r278, %r280;
	mov.b32 	%f65, %r293;
	add.f32 	%f66, %f64, %f65;
	st.local.f32 	[%rd3+8], %f66;
	st.shared.f32 	[%r39], %f66;
	bar.sync 	0;
	@%p1 bra 	$L__BB91_15;

	ld.shared.f32 	%f67, [%r4];
	mov.b32 	%r294, %f67;
	shfl.sync.bfly.b32 	%r298|%p35, %r294, %r279, %r278, %r280;
	mov.b32 	%f68, %r298;
	add.f32 	%f69, %f67, %f68;
	mov.b32 	%r299, %f69;
	shfl.sync.bfly.b32 	%r301|%p36, %r299, %r283, %r278, %r280;
	mov.b32 	%f70, %r301;
	add.f32 	%f71, %f69, %f70;
	mov.b32 	%r302, %f71;
	shfl.sync.bfly.b32 	%r304|%p37, %r302, %r286, %r278, %r280;
	mov.b32 	%f72, %r304;
	add.f32 	%f73, %f71, %f72;
	mov.b32 	%r305, %f73;
	shfl.sync.bfly.b32 	%r307|%p38, %r305, %r289, %r278, %r280;
	mov.b32 	%f74, %r307;
	add.f32 	%f75, %f73, %f74;
	mov.b32 	%r308, %f75;
	shfl.sync.bfly.b32 	%r310|%p39, %r308, %r292, %r278, %r280;
	mov.b32 	%f76, %r310;
	add.f32 	%f77, %f75, %f76;
	st.local.f32 	[%rd3+8], %f77;

$L__BB91_15:
	bar.sync 	0;
	mov.b32 	%r311, %f3;
	shfl.sync.bfly.b32 	%r315|%p41, %r311, %r279, %r278, %r280;
	mov.b32 	%f78, %r315;
	add.f32 	%f79, %f3, %f78;
	mov.b32 	%r316, %f79;
	shfl.sync.bfly.b32 	%r318|%p42, %r316, %r283, %r278, %r280;
	mov.b32 	%f80, %r318;
	add.f32 	%f81, %f79, %f80;
	mov.b32 	%r319, %f81;
	shfl.sync.bfly.b32 	%r321|%p43, %r319, %r286, %r278, %r280;
	mov.b32 	%f82, %r321;
	add.f32 	%f83, %f81, %f82;
	mov.b32 	%r322, %f83;
	shfl.sync.bfly.b32 	%r324|%p44, %r322, %r289, %r278, %r280;
	mov.b32 	%f84, %r324;
	add.f32 	%f85, %f83, %f84;
	mov.b32 	%r325, %f85;
	shfl.sync.bfly.b32 	%r327|%p45, %r325, %r292, %r278, %r280;
	mov.b32 	%f86, %r327;
	add.f32 	%f87, %f85, %f86;
	st.local.f32 	[%rd3+12], %f87;
	st.shared.f32 	[%r39], %f87;
	bar.sync 	0;
	@%p1 bra 	$L__BB91_17;

	ld.shared.f32 	%f88, [%r4];
	mov.b32 	%r328, %f88;
	mov.u32 	%r329, 31;
	mov.u32 	%r330, 16;
	mov.u32 	%r331, -1;
	shfl.sync.bfly.b32 	%r332|%p46, %r328, %r330, %r329, %r331;
	mov.b32 	%f89, %r332;
	add.f32 	%f90, %f88, %f89;
	mov.b32 	%r333, %f90;
	mov.u32 	%r334, 8;
	shfl.sync.bfly.b32 	%r335|%p47, %r333, %r334, %r329, %r331;
	mov.b32 	%f91, %r335;
	add.f32 	%f92, %f90, %f91;
	mov.b32 	%r336, %f92;
	mov.u32 	%r337, 4;
	shfl.sync.bfly.b32 	%r338|%p48, %r336, %r337, %r329, %r331;
	mov.b32 	%f93, %r338;
	add.f32 	%f94, %f92, %f93;
	mov.b32 	%r339, %f94;
	mov.u32 	%r340, 2;
	shfl.sync.bfly.b32 	%r341|%p49, %r339, %r340, %r329, %r331;
	mov.b32 	%f95, %r341;
	add.f32 	%f96, %f94, %f95;
	mov.b32 	%r342, %f96;
	mov.u32 	%r343, 1;
	shfl.sync.bfly.b32 	%r344|%p50, %r342, %r343, %r329, %r331;
	mov.b32 	%f97, %r344;
	add.f32 	%f98, %f96, %f97;
	st.local.f32 	[%rd3+12], %f98;

$L__BB91_17:
	bar.sync 	0;
	setp.gt.s32 	%p51, %r3, 3;
	@%p51 bra 	$L__BB91_19;

	mad.lo.s32 	%r345, %r3, %r42, %r2;
	cvt.s64.s32 	%rd68, %r345;
	mul.lo.s32 	%r346, %r1, %r43;
	cvt.s64.s32 	%rd69, %r346;
	add.s64 	%rd70, %rd69, %rd68;
	mul.wide.s32 	%rd71, %r3, 4;
	add.s64 	%rd72, %rd3, %rd71;
	ld.local.f32 	%f99, [%rd72];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs3, %f99;}

	// end inline asm
	cvta.to.global.u64 	%rd73, %rd33;
	shl.b64 	%rd74, %rd70, 1;
	add.s64 	%rd75, %rd73, %rd74;
	st.global.u16 	[%rd75], %rs3;

$L__BB91_19:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_5_bs_128
.visible .entry ggml_matvec_f16_ncols_5_bs_128(
	.param .u64 ggml_matvec_f16_ncols_5_bs_128_param_0,
	.param .u64 ggml_matvec_f16_ncols_5_bs_128_param_1,
	.param .u64 ggml_matvec_f16_ncols_5_bs_128_param_2,
	.param .u32 ggml_matvec_f16_ncols_5_bs_128_param_3,
	.param .u32 ggml_matvec_f16_ncols_5_bs_128_param_4,
	.param .u32 ggml_matvec_f16_ncols_5_bs_128_param_5,
	.param .u32 ggml_matvec_f16_ncols_5_bs_128_param_6,
	.param .u32 ggml_matvec_f16_ncols_5_bs_128_param_7,
	.param .u32 ggml_matvec_f16_ncols_5_bs_128_param_8,
	.param .u32 ggml_matvec_f16_ncols_5_bs_128_param_9,
	.param .u32 ggml_matvec_f16_ncols_5_bs_128_param_10,
	.param .u32 ggml_matvec_f16_ncols_5_bs_128_param_11
)
{
	.local .align 4 .b8 	__local_depot92[20];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<63>;
	.reg .b16 	%rs<4>;
	.reg .f32 	%f<124>;
	.reg .b32 	%r<447>;
	.reg .b64 	%rd<74>;


	mov.u64 	%SPL, __local_depot92;
	ld.param.u64 	%rd28, [ggml_matvec_f16_ncols_5_bs_128_param_0];
	ld.param.u64 	%rd29, [ggml_matvec_f16_ncols_5_bs_128_param_1];
	ld.param.u64 	%rd27, [ggml_matvec_f16_ncols_5_bs_128_param_2];
	ld.param.u32 	%r46, [ggml_matvec_f16_ncols_5_bs_128_param_3];
	ld.param.u32 	%r50, [ggml_matvec_f16_ncols_5_bs_128_param_5];
	ld.param.u32 	%r47, [ggml_matvec_f16_ncols_5_bs_128_param_6];
	ld.param.u32 	%r48, [ggml_matvec_f16_ncols_5_bs_128_param_7];
	ld.param.u32 	%r51, [ggml_matvec_f16_ncols_5_bs_128_param_8];
	ld.param.u32 	%r52, [ggml_matvec_f16_ncols_5_bs_128_param_9];
	ld.param.u32 	%r53, [ggml_matvec_f16_ncols_5_bs_128_param_10];
	ld.param.u32 	%r49, [ggml_matvec_f16_ncols_5_bs_128_param_11];
	cvta.to.global.u64 	%rd73, %rd29;
	cvta.to.global.u64 	%rd2, %rd28;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r54, %r1, %r51;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r55, %r2, %r50;
	mad.lo.s32 	%r56, %r54, %r52, %r55;
	cvt.s64.s32 	%rd4, %r56;
	mul.lo.s32 	%r57, %r1, %r53;
	cvt.s64.s32 	%rd5, %r57;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r58, %r3, 2;
	mov.u32 	%r59, data_mmv;
	add.s32 	%r4, %r59, %r58;
	@%p1 bra 	$L__BB92_2;

	mov.u32 	%r60, 0;
	st.shared.u32 	[%r4], %r60;

$L__BB92_2:
	bar.sync 	0;
	mov.f32 	%f6, 0f00000000;
	mov.u32 	%r442, 0;
	st.local.u32 	[%rd3], %r442;
	st.local.u32 	[%rd3+4], %r442;
	st.local.u32 	[%rd3+8], %r442;
	st.local.u32 	[%rd3+12], %r442;
	st.local.u32 	[%rd3+16], %r442;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs2, %f6;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f6;}

	// end inline asm
	mov.b32 	%r446, {%rs1, %rs2};
	setp.ge.s32 	%p2, %r3, %r46;
	mov.u32 	%r443, %r442;
	mov.u32 	%r444, %r442;
	mov.u32 	%r445, %r442;
	@%p2 bra 	$L__BB92_9;

	not.b32 	%r69, %r3;
	add.s32 	%r6, %r69, %r46;
	shr.u32 	%r70, %r6, 7;
	add.s32 	%r71, %r70, 1;
	and.b32  	%r429, %r71, 3;
	setp.eq.s32 	%p3, %r429, 0;
	mov.u32 	%r442, 0;
	mov.u32 	%r435, %r3;
	@%p3 bra 	$L__BB92_6;

	shl.b32 	%r76, %r47, 1;
	mad.lo.s32 	%r77, %r47, 3, %r3;
	mul.wide.s32 	%rd31, %r77, 4;
	shl.b64 	%rd32, %rd5, 1;
	add.s64 	%rd7, %rd31, %rd32;
	mul.wide.s32 	%rd33, %r3, 4;
	mul.wide.s32 	%rd34, %r47, 4;
	add.s64 	%rd35, %rd33, %rd34;
	add.s64 	%rd8, %rd35, %rd32;
	add.s64 	%rd9, %rd33, %rd32;
	mul.wide.s32 	%rd36, %r3, 2;
	add.s64 	%rd37, %rd36, %rd4;
	shl.b64 	%rd38, %rd37, 1;
	add.s64 	%rd70, %rd2, %rd38;
	mul.wide.s32 	%rd11, %r76, 4;
	mov.u32 	%r442, 0;
	mov.u64 	%rd71, %rd73;
	mov.u32 	%r443, %r442;
	mov.u32 	%r444, %r442;
	mov.u32 	%r445, %r442;
	mov.u32 	%r435, %r3;

$L__BB92_5:
	.pragma "nounroll";
	ld.global.nc.u32 	%r79, [%rd70];
	add.s64 	%rd39, %rd71, %rd9;
	ld.global.nc.u32 	%r80, [%rd39];
	// begin inline asm
	{mul.f16x2 %r78,%r79,%r80;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r446,%r446,%r78;
}
	// end inline asm
	add.s64 	%rd40, %rd71, %rd8;
	ld.global.nc.u32 	%r86, [%rd40];
	// begin inline asm
	{mul.f16x2 %r84,%r79,%r86;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r445,%r445,%r84;
}
	// end inline asm
	add.s64 	%rd41, %rd39, %rd11;
	ld.global.nc.u32 	%r92, [%rd41];
	// begin inline asm
	{mul.f16x2 %r90,%r79,%r92;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r444,%r444,%r90;
}
	// end inline asm
	add.s64 	%rd42, %rd71, %rd7;
	ld.global.nc.u32 	%r98, [%rd42];
	// begin inline asm
	{mul.f16x2 %r96,%r79,%r98;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r443,%r443,%r96;
}
	// end inline asm
	add.s64 	%rd43, %rd41, %rd11;
	ld.global.nc.u32 	%r104, [%rd43];
	// begin inline asm
	{mul.f16x2 %r102,%r79,%r104;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r442,%r442,%r102;
}
	// end inline asm
	add.s32 	%r435, %r435, 128;
	add.s64 	%rd71, %rd71, 512;
	add.s64 	%rd70, %rd70, 512;
	add.s32 	%r429, %r429, -1;
	setp.ne.s32 	%p4, %r429, 0;
	@%p4 bra 	$L__BB92_5;

$L__BB92_6:
	setp.lt.u32 	%p5, %r6, 384;
	@%p5 bra 	$L__BB92_9;

	add.s32 	%r108, %r435, %r47;
	shl.b32 	%r109, %r47, 1;
	add.s32 	%r110, %r435, %r109;
	mad.lo.s32 	%r111, %r47, 3, %r435;
	shl.b32 	%r112, %r47, 2;
	add.s32 	%r113, %r435, %r112;
	add.s32 	%r114, %r108, 128;
	mul.wide.s32 	%rd44, %r114, 4;
	shl.b64 	%rd45, %rd5, 1;
	add.s64 	%rd16, %rd44, %rd45;
	mul.wide.s32 	%rd46, %r110, 4;
	add.s64 	%rd17, %rd46, %rd45;
	mul.wide.s32 	%rd47, %r111, 4;
	add.s64 	%rd18, %rd47, %rd45;
	mul.wide.s32 	%rd48, %r113, 4;
	add.s64 	%rd19, %rd48, %rd45;
	mul.wide.s32 	%rd49, %r435, 2;
	add.s64 	%rd50, %rd49, %rd4;
	shl.b64 	%rd51, %rd50, 1;
	add.s64 	%rd52, %rd2, %rd51;
	add.s64 	%rd72, %rd52, 1024;
	mul.wide.s32 	%rd53, %r435, 4;
	add.s64 	%rd21, %rd53, %rd45;
	mul.wide.s32 	%rd54, %r47, 4;
	add.s64 	%rd55, %rd53, %rd54;
	add.s64 	%rd22, %rd55, %rd45;

$L__BB92_8:
	ld.global.nc.u32 	%r116, [%rd72+-1024];
	add.s64 	%rd56, %rd73, %rd21;
	ld.global.nc.u32 	%r117, [%rd56];
	// begin inline asm
	{mul.f16x2 %r115,%r116,%r117;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r118,%r446,%r115;
}
	// end inline asm
	add.s64 	%rd57, %rd73, %rd22;
	ld.global.nc.u32 	%r123, [%rd57];
	// begin inline asm
	{mul.f16x2 %r121,%r116,%r123;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r124,%r445,%r121;
}
	// end inline asm
	add.s64 	%rd58, %rd73, %rd17;
	ld.global.nc.u32 	%r129, [%rd58];
	// begin inline asm
	{mul.f16x2 %r127,%r116,%r129;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r130,%r444,%r127;
}
	// end inline asm
	add.s64 	%rd59, %rd73, %rd18;
	ld.global.nc.u32 	%r135, [%rd59];
	// begin inline asm
	{mul.f16x2 %r133,%r116,%r135;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r136,%r443,%r133;
}
	// end inline asm
	add.s64 	%rd60, %rd73, %rd19;
	ld.global.nc.u32 	%r141, [%rd60];
	// begin inline asm
	{mul.f16x2 %r139,%r116,%r141;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r142,%r442,%r139;
}
	// end inline asm
	ld.global.nc.u32 	%r146, [%rd72+-512];
	ld.global.nc.u32 	%r147, [%rd56+512];
	// begin inline asm
	{mul.f16x2 %r145,%r146,%r147;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r148,%r118,%r145;
}
	// end inline asm
	add.s64 	%rd61, %rd73, %rd16;
	ld.global.nc.u32 	%r153, [%rd61];
	// begin inline asm
	{mul.f16x2 %r151,%r146,%r153;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r154,%r124,%r151;
}
	// end inline asm
	ld.global.nc.u32 	%r159, [%rd58+512];
	// begin inline asm
	{mul.f16x2 %r157,%r146,%r159;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r160,%r130,%r157;
}
	// end inline asm
	ld.global.nc.u32 	%r165, [%rd59+512];
	// begin inline asm
	{mul.f16x2 %r163,%r146,%r165;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r166,%r136,%r163;
}
	// end inline asm
	ld.global.nc.u32 	%r171, [%rd60+512];
	// begin inline asm
	{mul.f16x2 %r169,%r146,%r171;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r172,%r142,%r169;
}
	// end inline asm
	ld.global.nc.u32 	%r176, [%rd72];
	ld.global.nc.u32 	%r177, [%rd56+1024];
	// begin inline asm
	{mul.f16x2 %r175,%r176,%r177;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r178,%r148,%r175;
}
	// end inline asm
	ld.global.nc.u32 	%r183, [%rd61+512];
	// begin inline asm
	{mul.f16x2 %r181,%r176,%r183;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r184,%r154,%r181;
}
	// end inline asm
	ld.global.nc.u32 	%r189, [%rd58+1024];
	// begin inline asm
	{mul.f16x2 %r187,%r176,%r189;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r190,%r160,%r187;
}
	// end inline asm
	ld.global.nc.u32 	%r195, [%rd59+1024];
	// begin inline asm
	{mul.f16x2 %r193,%r176,%r195;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r196,%r166,%r193;
}
	// end inline asm
	ld.global.nc.u32 	%r201, [%rd60+1024];
	// begin inline asm
	{mul.f16x2 %r199,%r176,%r201;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r202,%r172,%r199;
}
	// end inline asm
	ld.global.nc.u32 	%r206, [%rd72+512];
	ld.global.nc.u32 	%r207, [%rd56+1536];
	// begin inline asm
	{mul.f16x2 %r205,%r206,%r207;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r446,%r178,%r205;
}
	// end inline asm
	ld.global.nc.u32 	%r213, [%rd61+1024];
	// begin inline asm
	{mul.f16x2 %r211,%r206,%r213;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r445,%r184,%r211;
}
	// end inline asm
	ld.global.nc.u32 	%r219, [%rd58+1536];
	// begin inline asm
	{mul.f16x2 %r217,%r206,%r219;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r444,%r190,%r217;
}
	// end inline asm
	ld.global.nc.u32 	%r225, [%rd59+1536];
	// begin inline asm
	{mul.f16x2 %r223,%r206,%r225;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r443,%r196,%r223;
}
	// end inline asm
	ld.global.nc.u32 	%r231, [%rd60+1536];
	// begin inline asm
	{mul.f16x2 %r229,%r206,%r231;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r442,%r202,%r229;
}
	// end inline asm
	add.s64 	%rd73, %rd73, 2048;
	add.s64 	%rd72, %rd72, 2048;
	add.s32 	%r435, %r435, 512;
	setp.lt.s32 	%p6, %r435, %r46;
	@%p6 bra 	$L__BB92_8;

$L__BB92_9:
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r446;
  cvt.f32.f16 %f7, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r446;
  cvt.f32.f16 %f8, high;}

	// end inline asm
	add.f32 	%f17, %f7, %f8;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r445;
  cvt.f32.f16 %f9, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r445;
  cvt.f32.f16 %f10, high;}

	// end inline asm
	add.f32 	%f1, %f9, %f10;
	st.local.f32 	[%rd3+4], %f1;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r444;
  cvt.f32.f16 %f11, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r444;
  cvt.f32.f16 %f12, high;}

	// end inline asm
	add.f32 	%f2, %f11, %f12;
	st.local.f32 	[%rd3+8], %f2;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r443;
  cvt.f32.f16 %f13, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r443;
  cvt.f32.f16 %f14, high;}

	// end inline asm
	add.f32 	%f3, %f13, %f14;
	st.local.f32 	[%rd3+12], %f3;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r442;
  cvt.f32.f16 %f15, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r442;
  cvt.f32.f16 %f16, high;}

	// end inline asm
	add.f32 	%f4, %f15, %f16;
	st.local.f32 	[%rd3+16], %f4;
	shr.s32 	%r245, %r3, 31;
	shr.u32 	%r246, %r245, 27;
	add.s32 	%r247, %r3, %r246;
	shr.s32 	%r248, %r247, 5;
	shl.b32 	%r249, %r248, 2;
	add.s32 	%r45, %r59, %r249;
	mov.u32 	%r251, 2;
	mov.b32 	%r252, %f17;
	mov.u32 	%r253, 31;
	mov.u32 	%r254, 16;
	mov.u32 	%r255, -1;
	shfl.sync.bfly.b32 	%r256|%p7, %r252, %r254, %r253, %r255;
	mov.b32 	%f18, %r256;
	add.f32 	%f19, %f17, %f18;
	mov.b32 	%r257, %f19;
	mov.u32 	%r258, 8;
	shfl.sync.bfly.b32 	%r259|%p8, %r257, %r258, %r253, %r255;
	mov.b32 	%f20, %r259;
	add.f32 	%f21, %f19, %f20;
	mov.b32 	%r260, %f21;
	mov.u32 	%r261, 4;
	shfl.sync.bfly.b32 	%r262|%p9, %r260, %r261, %r253, %r255;
	mov.b32 	%f22, %r262;
	add.f32 	%f23, %f21, %f22;
	mov.b32 	%r263, %f23;
	shfl.sync.bfly.b32 	%r264|%p10, %r263, %r251, %r253, %r255;
	mov.b32 	%f24, %r264;
	add.f32 	%f25, %f23, %f24;
	mov.b32 	%r265, %f25;
	mov.u32 	%r266, 1;
	shfl.sync.bfly.b32 	%r267|%p11, %r265, %r266, %r253, %r255;
	mov.b32 	%f26, %r267;
	add.f32 	%f27, %f25, %f26;
	st.local.f32 	[%rd3], %f27;
	st.shared.f32 	[%r45], %f27;
	bar.sync 	0;
	@%p1 bra 	$L__BB92_11;

	ld.shared.f32 	%f28, [%r4];
	mov.b32 	%r268, %f28;
	shfl.sync.bfly.b32 	%r272|%p13, %r268, %r254, %r253, %r255;
	mov.b32 	%f29, %r272;
	add.f32 	%f30, %f28, %f29;
	mov.b32 	%r273, %f30;
	shfl.sync.bfly.b32 	%r275|%p14, %r273, %r258, %r253, %r255;
	mov.b32 	%f31, %r275;
	add.f32 	%f32, %f30, %f31;
	mov.b32 	%r276, %f32;
	shfl.sync.bfly.b32 	%r278|%p15, %r276, %r261, %r253, %r255;
	mov.b32 	%f33, %r278;
	add.f32 	%f34, %f32, %f33;
	mov.b32 	%r279, %f34;
	shfl.sync.bfly.b32 	%r281|%p16, %r279, %r251, %r253, %r255;
	mov.b32 	%f35, %r281;
	add.f32 	%f36, %f34, %f35;
	mov.b32 	%r282, %f36;
	shfl.sync.bfly.b32 	%r284|%p17, %r282, %r266, %r253, %r255;
	mov.b32 	%f37, %r284;
	add.f32 	%f38, %f36, %f37;
	st.local.f32 	[%rd3], %f38;

$L__BB92_11:
	bar.sync 	0;
	mov.b32 	%r285, %f1;
	shfl.sync.bfly.b32 	%r289|%p19, %r285, %r254, %r253, %r255;
	mov.b32 	%f39, %r289;
	add.f32 	%f40, %f1, %f39;
	mov.b32 	%r290, %f40;
	shfl.sync.bfly.b32 	%r292|%p20, %r290, %r258, %r253, %r255;
	mov.b32 	%f41, %r292;
	add.f32 	%f42, %f40, %f41;
	mov.b32 	%r293, %f42;
	shfl.sync.bfly.b32 	%r295|%p21, %r293, %r261, %r253, %r255;
	mov.b32 	%f43, %r295;
	add.f32 	%f44, %f42, %f43;
	mov.b32 	%r296, %f44;
	shfl.sync.bfly.b32 	%r298|%p22, %r296, %r251, %r253, %r255;
	mov.b32 	%f45, %r298;
	add.f32 	%f46, %f44, %f45;
	mov.b32 	%r299, %f46;
	shfl.sync.bfly.b32 	%r301|%p23, %r299, %r266, %r253, %r255;
	mov.b32 	%f47, %r301;
	add.f32 	%f48, %f46, %f47;
	st.local.f32 	[%rd3+4], %f48;
	st.shared.f32 	[%r45], %f48;
	bar.sync 	0;
	@%p1 bra 	$L__BB92_13;

	ld.shared.f32 	%f49, [%r4];
	mov.b32 	%r302, %f49;
	mov.u32 	%r303, 31;
	mov.u32 	%r304, 16;
	mov.u32 	%r305, -1;
	shfl.sync.bfly.b32 	%r306|%p24, %r302, %r304, %r303, %r305;
	mov.b32 	%f50, %r306;
	add.f32 	%f51, %f49, %f50;
	mov.b32 	%r307, %f51;
	mov.u32 	%r308, 8;
	shfl.sync.bfly.b32 	%r309|%p25, %r307, %r308, %r303, %r305;
	mov.b32 	%f52, %r309;
	add.f32 	%f53, %f51, %f52;
	mov.b32 	%r310, %f53;
	mov.u32 	%r311, 4;
	shfl.sync.bfly.b32 	%r312|%p26, %r310, %r311, %r303, %r305;
	mov.b32 	%f54, %r312;
	add.f32 	%f55, %f53, %f54;
	mov.b32 	%r313, %f55;
	mov.u32 	%r314, 2;
	shfl.sync.bfly.b32 	%r315|%p27, %r313, %r314, %r303, %r305;
	mov.b32 	%f56, %r315;
	add.f32 	%f57, %f55, %f56;
	mov.b32 	%r316, %f57;
	mov.u32 	%r317, 1;
	shfl.sync.bfly.b32 	%r318|%p28, %r316, %r317, %r303, %r305;
	mov.b32 	%f58, %r318;
	add.f32 	%f59, %f57, %f58;
	st.local.f32 	[%rd3+4], %f59;

$L__BB92_13:
	bar.sync 	0;
	mov.b32 	%r319, %f2;
	mov.u32 	%r320, 31;
	mov.u32 	%r321, 16;
	mov.u32 	%r322, -1;
	shfl.sync.bfly.b32 	%r323|%p30, %r319, %r321, %r320, %r322;
	mov.b32 	%f60, %r323;
	add.f32 	%f61, %f2, %f60;
	mov.b32 	%r324, %f61;
	mov.u32 	%r325, 8;
	shfl.sync.bfly.b32 	%r326|%p31, %r324, %r325, %r320, %r322;
	mov.b32 	%f62, %r326;
	add.f32 	%f63, %f61, %f62;
	mov.b32 	%r327, %f63;
	mov.u32 	%r328, 4;
	shfl.sync.bfly.b32 	%r329|%p32, %r327, %r328, %r320, %r322;
	mov.b32 	%f64, %r329;
	add.f32 	%f65, %f63, %f64;
	mov.b32 	%r330, %f65;
	mov.u32 	%r331, 2;
	shfl.sync.bfly.b32 	%r332|%p33, %r330, %r331, %r320, %r322;
	mov.b32 	%f66, %r332;
	add.f32 	%f67, %f65, %f66;
	mov.b32 	%r333, %f67;
	mov.u32 	%r334, 1;
	shfl.sync.bfly.b32 	%r335|%p34, %r333, %r334, %r320, %r322;
	mov.b32 	%f68, %r335;
	add.f32 	%f69, %f67, %f68;
	st.local.f32 	[%rd3+8], %f69;
	st.shared.f32 	[%r45], %f69;
	bar.sync 	0;
	@%p1 bra 	$L__BB92_15;

	ld.shared.f32 	%f70, [%r4];
	mov.b32 	%r336, %f70;
	shfl.sync.bfly.b32 	%r340|%p35, %r336, %r321, %r320, %r322;
	mov.b32 	%f71, %r340;
	add.f32 	%f72, %f70, %f71;
	mov.b32 	%r341, %f72;
	shfl.sync.bfly.b32 	%r343|%p36, %r341, %r325, %r320, %r322;
	mov.b32 	%f73, %r343;
	add.f32 	%f74, %f72, %f73;
	mov.b32 	%r344, %f74;
	shfl.sync.bfly.b32 	%r346|%p37, %r344, %r328, %r320, %r322;
	mov.b32 	%f75, %r346;
	add.f32 	%f76, %f74, %f75;
	mov.b32 	%r347, %f76;
	shfl.sync.bfly.b32 	%r349|%p38, %r347, %r331, %r320, %r322;
	mov.b32 	%f77, %r349;
	add.f32 	%f78, %f76, %f77;
	mov.b32 	%r350, %f78;
	shfl.sync.bfly.b32 	%r352|%p39, %r350, %r334, %r320, %r322;
	mov.b32 	%f79, %r352;
	add.f32 	%f80, %f78, %f79;
	st.local.f32 	[%rd3+8], %f80;

$L__BB92_15:
	bar.sync 	0;
	mov.b32 	%r353, %f3;
	shfl.sync.bfly.b32 	%r357|%p41, %r353, %r321, %r320, %r322;
	mov.b32 	%f81, %r357;
	add.f32 	%f82, %f3, %f81;
	mov.b32 	%r358, %f82;
	shfl.sync.bfly.b32 	%r360|%p42, %r358, %r325, %r320, %r322;
	mov.b32 	%f83, %r360;
	add.f32 	%f84, %f82, %f83;
	mov.b32 	%r361, %f84;
	shfl.sync.bfly.b32 	%r363|%p43, %r361, %r328, %r320, %r322;
	mov.b32 	%f85, %r363;
	add.f32 	%f86, %f84, %f85;
	mov.b32 	%r364, %f86;
	shfl.sync.bfly.b32 	%r366|%p44, %r364, %r331, %r320, %r322;
	mov.b32 	%f87, %r366;
	add.f32 	%f88, %f86, %f87;
	mov.b32 	%r367, %f88;
	shfl.sync.bfly.b32 	%r369|%p45, %r367, %r334, %r320, %r322;
	mov.b32 	%f89, %r369;
	add.f32 	%f90, %f88, %f89;
	st.local.f32 	[%rd3+12], %f90;
	st.shared.f32 	[%r45], %f90;
	bar.sync 	0;
	@%p1 bra 	$L__BB92_17;

	ld.shared.f32 	%f91, [%r4];
	mov.b32 	%r370, %f91;
	mov.u32 	%r371, 31;
	mov.u32 	%r372, 16;
	mov.u32 	%r373, -1;
	shfl.sync.bfly.b32 	%r374|%p46, %r370, %r372, %r371, %r373;
	mov.b32 	%f92, %r374;
	add.f32 	%f93, %f91, %f92;
	mov.b32 	%r375, %f93;
	mov.u32 	%r376, 8;
	shfl.sync.bfly.b32 	%r377|%p47, %r375, %r376, %r371, %r373;
	mov.b32 	%f94, %r377;
	add.f32 	%f95, %f93, %f94;
	mov.b32 	%r378, %f95;
	mov.u32 	%r379, 4;
	shfl.sync.bfly.b32 	%r380|%p48, %r378, %r379, %r371, %r373;
	mov.b32 	%f96, %r380;
	add.f32 	%f97, %f95, %f96;
	mov.b32 	%r381, %f97;
	mov.u32 	%r382, 2;
	shfl.sync.bfly.b32 	%r383|%p49, %r381, %r382, %r371, %r373;
	mov.b32 	%f98, %r383;
	add.f32 	%f99, %f97, %f98;
	mov.b32 	%r384, %f99;
	mov.u32 	%r385, 1;
	shfl.sync.bfly.b32 	%r386|%p50, %r384, %r385, %r371, %r373;
	mov.b32 	%f100, %r386;
	add.f32 	%f101, %f99, %f100;
	st.local.f32 	[%rd3+12], %f101;

$L__BB92_17:
	bar.sync 	0;
	mov.b32 	%r387, %f4;
	mov.u32 	%r388, 31;
	mov.u32 	%r389, 16;
	mov.u32 	%r390, -1;
	shfl.sync.bfly.b32 	%r391|%p52, %r387, %r389, %r388, %r390;
	mov.b32 	%f102, %r391;
	add.f32 	%f103, %f4, %f102;
	mov.b32 	%r392, %f103;
	mov.u32 	%r393, 8;
	shfl.sync.bfly.b32 	%r394|%p53, %r392, %r393, %r388, %r390;
	mov.b32 	%f104, %r394;
	add.f32 	%f105, %f103, %f104;
	mov.b32 	%r395, %f105;
	mov.u32 	%r396, 4;
	shfl.sync.bfly.b32 	%r397|%p54, %r395, %r396, %r388, %r390;
	mov.b32 	%f106, %r397;
	add.f32 	%f107, %f105, %f106;
	mov.b32 	%r398, %f107;
	mov.u32 	%r399, 2;
	shfl.sync.bfly.b32 	%r400|%p55, %r398, %r399, %r388, %r390;
	mov.b32 	%f108, %r400;
	add.f32 	%f109, %f107, %f108;
	mov.b32 	%r401, %f109;
	mov.u32 	%r402, 1;
	shfl.sync.bfly.b32 	%r403|%p56, %r401, %r402, %r388, %r390;
	mov.b32 	%f110, %r403;
	add.f32 	%f111, %f109, %f110;
	st.local.f32 	[%rd3+16], %f111;
	st.shared.f32 	[%r45], %f111;
	bar.sync 	0;
	@%p1 bra 	$L__BB92_19;

	ld.shared.f32 	%f112, [%r4];
	mov.b32 	%r404, %f112;
	shfl.sync.bfly.b32 	%r408|%p57, %r404, %r389, %r388, %r390;
	mov.b32 	%f113, %r408;
	add.f32 	%f114, %f112, %f113;
	mov.b32 	%r409, %f114;
	shfl.sync.bfly.b32 	%r411|%p58, %r409, %r393, %r388, %r390;
	mov.b32 	%f115, %r411;
	add.f32 	%f116, %f114, %f115;
	mov.b32 	%r412, %f116;
	shfl.sync.bfly.b32 	%r414|%p59, %r412, %r396, %r388, %r390;
	mov.b32 	%f117, %r414;
	add.f32 	%f118, %f116, %f117;
	mov.b32 	%r415, %f118;
	shfl.sync.bfly.b32 	%r417|%p60, %r415, %r399, %r388, %r390;
	mov.b32 	%f119, %r417;
	add.f32 	%f120, %f118, %f119;
	mov.b32 	%r418, %f120;
	shfl.sync.bfly.b32 	%r420|%p61, %r418, %r402, %r388, %r390;
	mov.b32 	%f121, %r420;
	add.f32 	%f122, %f120, %f121;
	st.local.f32 	[%rd3+16], %f122;

$L__BB92_19:
	bar.sync 	0;
	setp.gt.s32 	%p62, %r3, 4;
	@%p62 bra 	$L__BB92_21;

	mad.lo.s32 	%r421, %r3, %r48, %r2;
	cvt.s64.s32 	%rd62, %r421;
	mul.lo.s32 	%r422, %r1, %r49;
	cvt.s64.s32 	%rd63, %r422;
	add.s64 	%rd64, %rd63, %rd62;
	mul.wide.s32 	%rd65, %r3, 4;
	add.s64 	%rd66, %rd3, %rd65;
	ld.local.f32 	%f123, [%rd66];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs3, %f123;}

	// end inline asm
	cvta.to.global.u64 	%rd67, %rd27;
	shl.b64 	%rd68, %rd64, 1;
	add.s64 	%rd69, %rd67, %rd68;
	st.global.u16 	[%rd69], %rs3;

$L__BB92_21:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_6_bs_128
.visible .entry ggml_matvec_f16_ncols_6_bs_128(
	.param .u64 ggml_matvec_f16_ncols_6_bs_128_param_0,
	.param .u64 ggml_matvec_f16_ncols_6_bs_128_param_1,
	.param .u64 ggml_matvec_f16_ncols_6_bs_128_param_2,
	.param .u32 ggml_matvec_f16_ncols_6_bs_128_param_3,
	.param .u32 ggml_matvec_f16_ncols_6_bs_128_param_4,
	.param .u32 ggml_matvec_f16_ncols_6_bs_128_param_5,
	.param .u32 ggml_matvec_f16_ncols_6_bs_128_param_6,
	.param .u32 ggml_matvec_f16_ncols_6_bs_128_param_7,
	.param .u32 ggml_matvec_f16_ncols_6_bs_128_param_8,
	.param .u32 ggml_matvec_f16_ncols_6_bs_128_param_9,
	.param .u32 ggml_matvec_f16_ncols_6_bs_128_param_10,
	.param .u32 ggml_matvec_f16_ncols_6_bs_128_param_11
)
{
	.local .align 8 .b8 	__local_depot93[24];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<74>;
	.reg .b16 	%rs<4>;
	.reg .f32 	%f<148>;
	.reg .b32 	%r<527>;
	.reg .b64 	%rd<77>;


	mov.u64 	%SPL, __local_depot93;
	ld.param.u64 	%rd29, [ggml_matvec_f16_ncols_6_bs_128_param_0];
	ld.param.u64 	%rd30, [ggml_matvec_f16_ncols_6_bs_128_param_1];
	ld.param.u64 	%rd28, [ggml_matvec_f16_ncols_6_bs_128_param_2];
	ld.param.u32 	%r52, [ggml_matvec_f16_ncols_6_bs_128_param_3];
	ld.param.u32 	%r56, [ggml_matvec_f16_ncols_6_bs_128_param_5];
	ld.param.u32 	%r53, [ggml_matvec_f16_ncols_6_bs_128_param_6];
	ld.param.u32 	%r54, [ggml_matvec_f16_ncols_6_bs_128_param_7];
	ld.param.u32 	%r57, [ggml_matvec_f16_ncols_6_bs_128_param_8];
	ld.param.u32 	%r58, [ggml_matvec_f16_ncols_6_bs_128_param_9];
	ld.param.u32 	%r59, [ggml_matvec_f16_ncols_6_bs_128_param_10];
	ld.param.u32 	%r55, [ggml_matvec_f16_ncols_6_bs_128_param_11];
	cvta.to.global.u64 	%rd76, %rd30;
	cvta.to.global.u64 	%rd2, %rd29;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r60, %r1, %r57;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r61, %r2, %r56;
	mad.lo.s32 	%r62, %r60, %r58, %r61;
	cvt.s64.s32 	%rd4, %r62;
	mul.lo.s32 	%r63, %r1, %r59;
	cvt.s64.s32 	%rd5, %r63;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r64, %r3, 2;
	mov.u32 	%r65, data_mmv;
	add.s32 	%r4, %r65, %r64;
	@%p1 bra 	$L__BB93_2;

	mov.u32 	%r66, 0;
	st.shared.u32 	[%r4], %r66;

$L__BB93_2:
	bar.sync 	0;
	mov.f32 	%f7, 0f00000000;
	st.local.v2.f32 	[%rd3], {%f7, %f7};
	st.local.v2.f32 	[%rd3+8], {%f7, %f7};
	st.local.v2.f32 	[%rd3+16], {%f7, %f7};
	mov.u32 	%r521, 0;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs2, %f7;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f7;}

	// end inline asm
	mov.b32 	%r526, {%rs1, %rs2};
	setp.ge.s32 	%p2, %r3, %r52;
	mov.u32 	%r522, %r521;
	mov.u32 	%r523, %r521;
	mov.u32 	%r524, %r521;
	mov.u32 	%r525, %r521;
	@%p2 bra 	$L__BB93_9;

	not.b32 	%r77, %r3;
	add.s32 	%r6, %r77, %r52;
	shr.u32 	%r78, %r6, 7;
	add.s32 	%r79, %r78, 1;
	and.b32  	%r506, %r79, 3;
	setp.eq.s32 	%p3, %r506, 0;
	mov.u32 	%r521, 0;
	mov.u32 	%r513, %r3;
	@%p3 bra 	$L__BB93_6;

	shl.b32 	%r85, %r53, 1;
	add.s32 	%r86, %r3, %r85;
	mul.wide.s32 	%rd32, %r86, 4;
	shl.b64 	%rd33, %rd5, 1;
	add.s64 	%rd7, %rd32, %rd33;
	mul.wide.s32 	%rd34, %r3, 4;
	mul.wide.s32 	%rd8, %r53, 4;
	add.s64 	%rd35, %rd34, %rd8;
	add.s64 	%rd9, %rd35, %rd33;
	add.s64 	%rd10, %rd34, %rd33;
	mul.wide.s32 	%rd36, %r3, 2;
	add.s64 	%rd37, %rd36, %rd4;
	shl.b64 	%rd38, %rd37, 1;
	add.s64 	%rd73, %rd2, %rd38;
	mov.u32 	%r521, 0;
	mov.u64 	%rd74, %rd76;
	mov.u32 	%r522, %r521;
	mov.u32 	%r523, %r521;
	mov.u32 	%r524, %r521;
	mov.u32 	%r525, %r521;
	mov.u32 	%r513, %r3;

$L__BB93_5:
	.pragma "nounroll";
	ld.global.nc.u32 	%r88, [%rd73];
	add.s64 	%rd39, %rd74, %rd10;
	ld.global.nc.u32 	%r89, [%rd39];
	// begin inline asm
	{mul.f16x2 %r87,%r88,%r89;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r526,%r526,%r87;
}
	// end inline asm
	add.s64 	%rd40, %rd74, %rd9;
	ld.global.nc.u32 	%r95, [%rd40];
	// begin inline asm
	{mul.f16x2 %r93,%r88,%r95;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r525,%r525,%r93;
}
	// end inline asm
	add.s64 	%rd41, %rd74, %rd7;
	ld.global.nc.u32 	%r101, [%rd41];
	// begin inline asm
	{mul.f16x2 %r99,%r88,%r101;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r524,%r524,%r99;
}
	// end inline asm
	add.s64 	%rd42, %rd41, %rd8;
	ld.global.nc.u32 	%r107, [%rd42];
	// begin inline asm
	{mul.f16x2 %r105,%r88,%r107;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r523,%r523,%r105;
}
	// end inline asm
	add.s64 	%rd43, %rd42, %rd8;
	ld.global.nc.u32 	%r113, [%rd43];
	// begin inline asm
	{mul.f16x2 %r111,%r88,%r113;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r522,%r522,%r111;
}
	// end inline asm
	add.s64 	%rd44, %rd43, %rd8;
	ld.global.nc.u32 	%r119, [%rd44];
	// begin inline asm
	{mul.f16x2 %r117,%r88,%r119;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r521,%r521,%r117;
}
	// end inline asm
	add.s32 	%r513, %r513, 128;
	add.s64 	%rd74, %rd74, 512;
	add.s64 	%rd73, %rd73, 512;
	add.s32 	%r506, %r506, -1;
	setp.ne.s32 	%p4, %r506, 0;
	@%p4 bra 	$L__BB93_5;

$L__BB93_6:
	setp.lt.u32 	%p5, %r6, 384;
	@%p5 bra 	$L__BB93_9;

	add.s32 	%r123, %r513, %r53;
	shl.b32 	%r124, %r53, 1;
	add.s32 	%r125, %r513, %r124;
	mad.lo.s32 	%r126, %r53, 3, %r513;
	shl.b32 	%r127, %r53, 2;
	add.s32 	%r128, %r513, %r127;
	mad.lo.s32 	%r129, %r53, 5, %r513;
	add.s32 	%r130, %r123, 128;
	mul.wide.s32 	%rd45, %r130, 4;
	shl.b64 	%rd46, %rd5, 1;
	add.s64 	%rd16, %rd45, %rd46;
	mul.wide.s32 	%rd47, %r125, 4;
	add.s64 	%rd17, %rd47, %rd46;
	mul.wide.s32 	%rd48, %r126, 4;
	add.s64 	%rd18, %rd48, %rd46;
	mul.wide.s32 	%rd49, %r128, 4;
	add.s64 	%rd19, %rd49, %rd46;
	mul.wide.s32 	%rd50, %r129, 4;
	add.s64 	%rd20, %rd50, %rd46;
	mul.wide.s32 	%rd51, %r513, 2;
	add.s64 	%rd52, %rd51, %rd4;
	shl.b64 	%rd53, %rd52, 1;
	add.s64 	%rd54, %rd2, %rd53;
	add.s64 	%rd75, %rd54, 1024;
	mul.wide.s32 	%rd55, %r513, 4;
	add.s64 	%rd22, %rd55, %rd46;
	mul.wide.s32 	%rd56, %r53, 4;
	add.s64 	%rd57, %rd55, %rd56;
	add.s64 	%rd23, %rd57, %rd46;

$L__BB93_8:
	ld.global.nc.u32 	%r132, [%rd75+-1024];
	add.s64 	%rd58, %rd76, %rd22;
	ld.global.nc.u32 	%r133, [%rd58];
	// begin inline asm
	{mul.f16x2 %r131,%r132,%r133;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r134,%r526,%r131;
}
	// end inline asm
	add.s64 	%rd59, %rd76, %rd23;
	ld.global.nc.u32 	%r139, [%rd59];
	// begin inline asm
	{mul.f16x2 %r137,%r132,%r139;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r140,%r525,%r137;
}
	// end inline asm
	add.s64 	%rd60, %rd76, %rd17;
	ld.global.nc.u32 	%r145, [%rd60];
	// begin inline asm
	{mul.f16x2 %r143,%r132,%r145;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r146,%r524,%r143;
}
	// end inline asm
	add.s64 	%rd61, %rd76, %rd18;
	ld.global.nc.u32 	%r151, [%rd61];
	// begin inline asm
	{mul.f16x2 %r149,%r132,%r151;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r152,%r523,%r149;
}
	// end inline asm
	add.s64 	%rd62, %rd76, %rd19;
	ld.global.nc.u32 	%r157, [%rd62];
	// begin inline asm
	{mul.f16x2 %r155,%r132,%r157;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r158,%r522,%r155;
}
	// end inline asm
	add.s64 	%rd63, %rd76, %rd20;
	ld.global.nc.u32 	%r163, [%rd63];
	// begin inline asm
	{mul.f16x2 %r161,%r132,%r163;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r164,%r521,%r161;
}
	// end inline asm
	ld.global.nc.u32 	%r168, [%rd75+-512];
	ld.global.nc.u32 	%r169, [%rd58+512];
	// begin inline asm
	{mul.f16x2 %r167,%r168,%r169;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r170,%r134,%r167;
}
	// end inline asm
	add.s64 	%rd64, %rd76, %rd16;
	ld.global.nc.u32 	%r175, [%rd64];
	// begin inline asm
	{mul.f16x2 %r173,%r168,%r175;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r176,%r140,%r173;
}
	// end inline asm
	ld.global.nc.u32 	%r181, [%rd60+512];
	// begin inline asm
	{mul.f16x2 %r179,%r168,%r181;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r182,%r146,%r179;
}
	// end inline asm
	ld.global.nc.u32 	%r187, [%rd61+512];
	// begin inline asm
	{mul.f16x2 %r185,%r168,%r187;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r188,%r152,%r185;
}
	// end inline asm
	ld.global.nc.u32 	%r193, [%rd62+512];
	// begin inline asm
	{mul.f16x2 %r191,%r168,%r193;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r194,%r158,%r191;
}
	// end inline asm
	ld.global.nc.u32 	%r199, [%rd63+512];
	// begin inline asm
	{mul.f16x2 %r197,%r168,%r199;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r200,%r164,%r197;
}
	// end inline asm
	ld.global.nc.u32 	%r204, [%rd75];
	ld.global.nc.u32 	%r205, [%rd58+1024];
	// begin inline asm
	{mul.f16x2 %r203,%r204,%r205;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r206,%r170,%r203;
}
	// end inline asm
	ld.global.nc.u32 	%r211, [%rd64+512];
	// begin inline asm
	{mul.f16x2 %r209,%r204,%r211;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r212,%r176,%r209;
}
	// end inline asm
	ld.global.nc.u32 	%r217, [%rd60+1024];
	// begin inline asm
	{mul.f16x2 %r215,%r204,%r217;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r218,%r182,%r215;
}
	// end inline asm
	ld.global.nc.u32 	%r223, [%rd61+1024];
	// begin inline asm
	{mul.f16x2 %r221,%r204,%r223;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r224,%r188,%r221;
}
	// end inline asm
	ld.global.nc.u32 	%r229, [%rd62+1024];
	// begin inline asm
	{mul.f16x2 %r227,%r204,%r229;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r230,%r194,%r227;
}
	// end inline asm
	ld.global.nc.u32 	%r235, [%rd63+1024];
	// begin inline asm
	{mul.f16x2 %r233,%r204,%r235;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r236,%r200,%r233;
}
	// end inline asm
	ld.global.nc.u32 	%r240, [%rd75+512];
	ld.global.nc.u32 	%r241, [%rd58+1536];
	// begin inline asm
	{mul.f16x2 %r239,%r240,%r241;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r526,%r206,%r239;
}
	// end inline asm
	ld.global.nc.u32 	%r247, [%rd64+1024];
	// begin inline asm
	{mul.f16x2 %r245,%r240,%r247;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r525,%r212,%r245;
}
	// end inline asm
	ld.global.nc.u32 	%r253, [%rd60+1536];
	// begin inline asm
	{mul.f16x2 %r251,%r240,%r253;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r524,%r218,%r251;
}
	// end inline asm
	ld.global.nc.u32 	%r259, [%rd61+1536];
	// begin inline asm
	{mul.f16x2 %r257,%r240,%r259;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r523,%r224,%r257;
}
	// end inline asm
	ld.global.nc.u32 	%r265, [%rd62+1536];
	// begin inline asm
	{mul.f16x2 %r263,%r240,%r265;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r522,%r230,%r263;
}
	// end inline asm
	ld.global.nc.u32 	%r271, [%rd63+1536];
	// begin inline asm
	{mul.f16x2 %r269,%r240,%r271;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r521,%r236,%r269;
}
	// end inline asm
	add.s64 	%rd76, %rd76, 2048;
	add.s64 	%rd75, %rd75, 2048;
	add.s32 	%r513, %r513, 512;
	setp.lt.s32 	%p6, %r513, %r52;
	@%p6 bra 	$L__BB93_8;

$L__BB93_9:
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r526;
  cvt.f32.f16 %f8, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r526;
  cvt.f32.f16 %f9, high;}

	// end inline asm
	add.f32 	%f20, %f8, %f9;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r525;
  cvt.f32.f16 %f10, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r525;
  cvt.f32.f16 %f11, high;}

	// end inline asm
	add.f32 	%f1, %f10, %f11;
	st.local.f32 	[%rd3+4], %f1;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r524;
  cvt.f32.f16 %f12, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r524;
  cvt.f32.f16 %f13, high;}

	// end inline asm
	add.f32 	%f2, %f12, %f13;
	st.local.f32 	[%rd3+8], %f2;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r523;
  cvt.f32.f16 %f14, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r523;
  cvt.f32.f16 %f15, high;}

	// end inline asm
	add.f32 	%f3, %f14, %f15;
	st.local.f32 	[%rd3+12], %f3;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r522;
  cvt.f32.f16 %f16, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r522;
  cvt.f32.f16 %f17, high;}

	// end inline asm
	add.f32 	%f4, %f16, %f17;
	st.local.f32 	[%rd3+16], %f4;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r521;
  cvt.f32.f16 %f18, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r521;
  cvt.f32.f16 %f19, high;}

	// end inline asm
	add.f32 	%f5, %f18, %f19;
	st.local.f32 	[%rd3+20], %f5;
	shr.s32 	%r287, %r3, 31;
	shr.u32 	%r288, %r287, 27;
	add.s32 	%r289, %r3, %r288;
	shr.s32 	%r290, %r289, 5;
	shl.b32 	%r291, %r290, 2;
	add.s32 	%r51, %r65, %r291;
	mov.u32 	%r293, 2;
	mov.b32 	%r294, %f20;
	mov.u32 	%r295, 31;
	mov.u32 	%r296, 16;
	mov.u32 	%r297, -1;
	shfl.sync.bfly.b32 	%r298|%p7, %r294, %r296, %r295, %r297;
	mov.b32 	%f21, %r298;
	add.f32 	%f22, %f20, %f21;
	mov.b32 	%r299, %f22;
	mov.u32 	%r300, 8;
	shfl.sync.bfly.b32 	%r301|%p8, %r299, %r300, %r295, %r297;
	mov.b32 	%f23, %r301;
	add.f32 	%f24, %f22, %f23;
	mov.b32 	%r302, %f24;
	mov.u32 	%r303, 4;
	shfl.sync.bfly.b32 	%r304|%p9, %r302, %r303, %r295, %r297;
	mov.b32 	%f25, %r304;
	add.f32 	%f26, %f24, %f25;
	mov.b32 	%r305, %f26;
	shfl.sync.bfly.b32 	%r306|%p10, %r305, %r293, %r295, %r297;
	mov.b32 	%f27, %r306;
	add.f32 	%f28, %f26, %f27;
	mov.b32 	%r307, %f28;
	mov.u32 	%r308, 1;
	shfl.sync.bfly.b32 	%r309|%p11, %r307, %r308, %r295, %r297;
	mov.b32 	%f29, %r309;
	add.f32 	%f30, %f28, %f29;
	st.local.f32 	[%rd3], %f30;
	st.shared.f32 	[%r51], %f30;
	bar.sync 	0;
	@%p1 bra 	$L__BB93_11;

	ld.shared.f32 	%f31, [%r4];
	mov.b32 	%r310, %f31;
	shfl.sync.bfly.b32 	%r314|%p13, %r310, %r296, %r295, %r297;
	mov.b32 	%f32, %r314;
	add.f32 	%f33, %f31, %f32;
	mov.b32 	%r315, %f33;
	shfl.sync.bfly.b32 	%r317|%p14, %r315, %r300, %r295, %r297;
	mov.b32 	%f34, %r317;
	add.f32 	%f35, %f33, %f34;
	mov.b32 	%r318, %f35;
	shfl.sync.bfly.b32 	%r320|%p15, %r318, %r303, %r295, %r297;
	mov.b32 	%f36, %r320;
	add.f32 	%f37, %f35, %f36;
	mov.b32 	%r321, %f37;
	shfl.sync.bfly.b32 	%r323|%p16, %r321, %r293, %r295, %r297;
	mov.b32 	%f38, %r323;
	add.f32 	%f39, %f37, %f38;
	mov.b32 	%r324, %f39;
	shfl.sync.bfly.b32 	%r326|%p17, %r324, %r308, %r295, %r297;
	mov.b32 	%f40, %r326;
	add.f32 	%f41, %f39, %f40;
	st.local.f32 	[%rd3], %f41;

$L__BB93_11:
	bar.sync 	0;
	mov.b32 	%r327, %f1;
	shfl.sync.bfly.b32 	%r331|%p19, %r327, %r296, %r295, %r297;
	mov.b32 	%f42, %r331;
	add.f32 	%f43, %f1, %f42;
	mov.b32 	%r332, %f43;
	shfl.sync.bfly.b32 	%r334|%p20, %r332, %r300, %r295, %r297;
	mov.b32 	%f44, %r334;
	add.f32 	%f45, %f43, %f44;
	mov.b32 	%r335, %f45;
	shfl.sync.bfly.b32 	%r337|%p21, %r335, %r303, %r295, %r297;
	mov.b32 	%f46, %r337;
	add.f32 	%f47, %f45, %f46;
	mov.b32 	%r338, %f47;
	shfl.sync.bfly.b32 	%r340|%p22, %r338, %r293, %r295, %r297;
	mov.b32 	%f48, %r340;
	add.f32 	%f49, %f47, %f48;
	mov.b32 	%r341, %f49;
	shfl.sync.bfly.b32 	%r343|%p23, %r341, %r308, %r295, %r297;
	mov.b32 	%f50, %r343;
	add.f32 	%f51, %f49, %f50;
	st.local.f32 	[%rd3+4], %f51;
	st.shared.f32 	[%r51], %f51;
	bar.sync 	0;
	@%p1 bra 	$L__BB93_13;

	ld.shared.f32 	%f52, [%r4];
	mov.b32 	%r344, %f52;
	mov.u32 	%r345, 31;
	mov.u32 	%r346, 16;
	mov.u32 	%r347, -1;
	shfl.sync.bfly.b32 	%r348|%p24, %r344, %r346, %r345, %r347;
	mov.b32 	%f53, %r348;
	add.f32 	%f54, %f52, %f53;
	mov.b32 	%r349, %f54;
	mov.u32 	%r350, 8;
	shfl.sync.bfly.b32 	%r351|%p25, %r349, %r350, %r345, %r347;
	mov.b32 	%f55, %r351;
	add.f32 	%f56, %f54, %f55;
	mov.b32 	%r352, %f56;
	mov.u32 	%r353, 4;
	shfl.sync.bfly.b32 	%r354|%p26, %r352, %r353, %r345, %r347;
	mov.b32 	%f57, %r354;
	add.f32 	%f58, %f56, %f57;
	mov.b32 	%r355, %f58;
	mov.u32 	%r356, 2;
	shfl.sync.bfly.b32 	%r357|%p27, %r355, %r356, %r345, %r347;
	mov.b32 	%f59, %r357;
	add.f32 	%f60, %f58, %f59;
	mov.b32 	%r358, %f60;
	mov.u32 	%r359, 1;
	shfl.sync.bfly.b32 	%r360|%p28, %r358, %r359, %r345, %r347;
	mov.b32 	%f61, %r360;
	add.f32 	%f62, %f60, %f61;
	st.local.f32 	[%rd3+4], %f62;

$L__BB93_13:
	bar.sync 	0;
	mov.b32 	%r361, %f2;
	mov.u32 	%r362, 31;
	mov.u32 	%r363, 16;
	mov.u32 	%r364, -1;
	shfl.sync.bfly.b32 	%r365|%p30, %r361, %r363, %r362, %r364;
	mov.b32 	%f63, %r365;
	add.f32 	%f64, %f2, %f63;
	mov.b32 	%r366, %f64;
	mov.u32 	%r367, 8;
	shfl.sync.bfly.b32 	%r368|%p31, %r366, %r367, %r362, %r364;
	mov.b32 	%f65, %r368;
	add.f32 	%f66, %f64, %f65;
	mov.b32 	%r369, %f66;
	mov.u32 	%r370, 4;
	shfl.sync.bfly.b32 	%r371|%p32, %r369, %r370, %r362, %r364;
	mov.b32 	%f67, %r371;
	add.f32 	%f68, %f66, %f67;
	mov.b32 	%r372, %f68;
	mov.u32 	%r373, 2;
	shfl.sync.bfly.b32 	%r374|%p33, %r372, %r373, %r362, %r364;
	mov.b32 	%f69, %r374;
	add.f32 	%f70, %f68, %f69;
	mov.b32 	%r375, %f70;
	mov.u32 	%r376, 1;
	shfl.sync.bfly.b32 	%r377|%p34, %r375, %r376, %r362, %r364;
	mov.b32 	%f71, %r377;
	add.f32 	%f72, %f70, %f71;
	st.local.f32 	[%rd3+8], %f72;
	st.shared.f32 	[%r51], %f72;
	bar.sync 	0;
	@%p1 bra 	$L__BB93_15;

	ld.shared.f32 	%f73, [%r4];
	mov.b32 	%r378, %f73;
	shfl.sync.bfly.b32 	%r382|%p35, %r378, %r363, %r362, %r364;
	mov.b32 	%f74, %r382;
	add.f32 	%f75, %f73, %f74;
	mov.b32 	%r383, %f75;
	shfl.sync.bfly.b32 	%r385|%p36, %r383, %r367, %r362, %r364;
	mov.b32 	%f76, %r385;
	add.f32 	%f77, %f75, %f76;
	mov.b32 	%r386, %f77;
	shfl.sync.bfly.b32 	%r388|%p37, %r386, %r370, %r362, %r364;
	mov.b32 	%f78, %r388;
	add.f32 	%f79, %f77, %f78;
	mov.b32 	%r389, %f79;
	shfl.sync.bfly.b32 	%r391|%p38, %r389, %r373, %r362, %r364;
	mov.b32 	%f80, %r391;
	add.f32 	%f81, %f79, %f80;
	mov.b32 	%r392, %f81;
	shfl.sync.bfly.b32 	%r394|%p39, %r392, %r376, %r362, %r364;
	mov.b32 	%f82, %r394;
	add.f32 	%f83, %f81, %f82;
	st.local.f32 	[%rd3+8], %f83;

$L__BB93_15:
	bar.sync 	0;
	mov.b32 	%r395, %f3;
	shfl.sync.bfly.b32 	%r399|%p41, %r395, %r363, %r362, %r364;
	mov.b32 	%f84, %r399;
	add.f32 	%f85, %f3, %f84;
	mov.b32 	%r400, %f85;
	shfl.sync.bfly.b32 	%r402|%p42, %r400, %r367, %r362, %r364;
	mov.b32 	%f86, %r402;
	add.f32 	%f87, %f85, %f86;
	mov.b32 	%r403, %f87;
	shfl.sync.bfly.b32 	%r405|%p43, %r403, %r370, %r362, %r364;
	mov.b32 	%f88, %r405;
	add.f32 	%f89, %f87, %f88;
	mov.b32 	%r406, %f89;
	shfl.sync.bfly.b32 	%r408|%p44, %r406, %r373, %r362, %r364;
	mov.b32 	%f90, %r408;
	add.f32 	%f91, %f89, %f90;
	mov.b32 	%r409, %f91;
	shfl.sync.bfly.b32 	%r411|%p45, %r409, %r376, %r362, %r364;
	mov.b32 	%f92, %r411;
	add.f32 	%f93, %f91, %f92;
	st.local.f32 	[%rd3+12], %f93;
	st.shared.f32 	[%r51], %f93;
	bar.sync 	0;
	@%p1 bra 	$L__BB93_17;

	ld.shared.f32 	%f94, [%r4];
	mov.b32 	%r412, %f94;
	mov.u32 	%r413, 31;
	mov.u32 	%r414, 16;
	mov.u32 	%r415, -1;
	shfl.sync.bfly.b32 	%r416|%p46, %r412, %r414, %r413, %r415;
	mov.b32 	%f95, %r416;
	add.f32 	%f96, %f94, %f95;
	mov.b32 	%r417, %f96;
	mov.u32 	%r418, 8;
	shfl.sync.bfly.b32 	%r419|%p47, %r417, %r418, %r413, %r415;
	mov.b32 	%f97, %r419;
	add.f32 	%f98, %f96, %f97;
	mov.b32 	%r420, %f98;
	mov.u32 	%r421, 4;
	shfl.sync.bfly.b32 	%r422|%p48, %r420, %r421, %r413, %r415;
	mov.b32 	%f99, %r422;
	add.f32 	%f100, %f98, %f99;
	mov.b32 	%r423, %f100;
	mov.u32 	%r424, 2;
	shfl.sync.bfly.b32 	%r425|%p49, %r423, %r424, %r413, %r415;
	mov.b32 	%f101, %r425;
	add.f32 	%f102, %f100, %f101;
	mov.b32 	%r426, %f102;
	mov.u32 	%r427, 1;
	shfl.sync.bfly.b32 	%r428|%p50, %r426, %r427, %r413, %r415;
	mov.b32 	%f103, %r428;
	add.f32 	%f104, %f102, %f103;
	st.local.f32 	[%rd3+12], %f104;

$L__BB93_17:
	bar.sync 	0;
	mov.b32 	%r429, %f4;
	mov.u32 	%r430, 31;
	mov.u32 	%r431, 16;
	mov.u32 	%r432, -1;
	shfl.sync.bfly.b32 	%r433|%p52, %r429, %r431, %r430, %r432;
	mov.b32 	%f105, %r433;
	add.f32 	%f106, %f4, %f105;
	mov.b32 	%r434, %f106;
	mov.u32 	%r435, 8;
	shfl.sync.bfly.b32 	%r436|%p53, %r434, %r435, %r430, %r432;
	mov.b32 	%f107, %r436;
	add.f32 	%f108, %f106, %f107;
	mov.b32 	%r437, %f108;
	mov.u32 	%r438, 4;
	shfl.sync.bfly.b32 	%r439|%p54, %r437, %r438, %r430, %r432;
	mov.b32 	%f109, %r439;
	add.f32 	%f110, %f108, %f109;
	mov.b32 	%r440, %f110;
	mov.u32 	%r441, 2;
	shfl.sync.bfly.b32 	%r442|%p55, %r440, %r441, %r430, %r432;
	mov.b32 	%f111, %r442;
	add.f32 	%f112, %f110, %f111;
	mov.b32 	%r443, %f112;
	mov.u32 	%r444, 1;
	shfl.sync.bfly.b32 	%r445|%p56, %r443, %r444, %r430, %r432;
	mov.b32 	%f113, %r445;
	add.f32 	%f114, %f112, %f113;
	st.local.f32 	[%rd3+16], %f114;
	st.shared.f32 	[%r51], %f114;
	bar.sync 	0;
	@%p1 bra 	$L__BB93_19;

	ld.shared.f32 	%f115, [%r4];
	mov.b32 	%r446, %f115;
	shfl.sync.bfly.b32 	%r450|%p57, %r446, %r431, %r430, %r432;
	mov.b32 	%f116, %r450;
	add.f32 	%f117, %f115, %f116;
	mov.b32 	%r451, %f117;
	shfl.sync.bfly.b32 	%r453|%p58, %r451, %r435, %r430, %r432;
	mov.b32 	%f118, %r453;
	add.f32 	%f119, %f117, %f118;
	mov.b32 	%r454, %f119;
	shfl.sync.bfly.b32 	%r456|%p59, %r454, %r438, %r430, %r432;
	mov.b32 	%f120, %r456;
	add.f32 	%f121, %f119, %f120;
	mov.b32 	%r457, %f121;
	shfl.sync.bfly.b32 	%r459|%p60, %r457, %r441, %r430, %r432;
	mov.b32 	%f122, %r459;
	add.f32 	%f123, %f121, %f122;
	mov.b32 	%r460, %f123;
	shfl.sync.bfly.b32 	%r462|%p61, %r460, %r444, %r430, %r432;
	mov.b32 	%f124, %r462;
	add.f32 	%f125, %f123, %f124;
	st.local.f32 	[%rd3+16], %f125;

$L__BB93_19:
	bar.sync 	0;
	mov.b32 	%r463, %f5;
	shfl.sync.bfly.b32 	%r467|%p63, %r463, %r431, %r430, %r432;
	mov.b32 	%f126, %r467;
	add.f32 	%f127, %f5, %f126;
	mov.b32 	%r468, %f127;
	shfl.sync.bfly.b32 	%r470|%p64, %r468, %r435, %r430, %r432;
	mov.b32 	%f128, %r470;
	add.f32 	%f129, %f127, %f128;
	mov.b32 	%r471, %f129;
	shfl.sync.bfly.b32 	%r473|%p65, %r471, %r438, %r430, %r432;
	mov.b32 	%f130, %r473;
	add.f32 	%f131, %f129, %f130;
	mov.b32 	%r474, %f131;
	shfl.sync.bfly.b32 	%r476|%p66, %r474, %r441, %r430, %r432;
	mov.b32 	%f132, %r476;
	add.f32 	%f133, %f131, %f132;
	mov.b32 	%r477, %f133;
	shfl.sync.bfly.b32 	%r479|%p67, %r477, %r444, %r430, %r432;
	mov.b32 	%f134, %r479;
	add.f32 	%f135, %f133, %f134;
	st.local.f32 	[%rd3+20], %f135;
	st.shared.f32 	[%r51], %f135;
	bar.sync 	0;
	@%p1 bra 	$L__BB93_21;

	ld.shared.f32 	%f136, [%r4];
	mov.b32 	%r480, %f136;
	mov.u32 	%r481, 31;
	mov.u32 	%r482, 16;
	mov.u32 	%r483, -1;
	shfl.sync.bfly.b32 	%r484|%p68, %r480, %r482, %r481, %r483;
	mov.b32 	%f137, %r484;
	add.f32 	%f138, %f136, %f137;
	mov.b32 	%r485, %f138;
	mov.u32 	%r486, 8;
	shfl.sync.bfly.b32 	%r487|%p69, %r485, %r486, %r481, %r483;
	mov.b32 	%f139, %r487;
	add.f32 	%f140, %f138, %f139;
	mov.b32 	%r488, %f140;
	mov.u32 	%r489, 4;
	shfl.sync.bfly.b32 	%r490|%p70, %r488, %r489, %r481, %r483;
	mov.b32 	%f141, %r490;
	add.f32 	%f142, %f140, %f141;
	mov.b32 	%r491, %f142;
	mov.u32 	%r492, 2;
	shfl.sync.bfly.b32 	%r493|%p71, %r491, %r492, %r481, %r483;
	mov.b32 	%f143, %r493;
	add.f32 	%f144, %f142, %f143;
	mov.b32 	%r494, %f144;
	mov.u32 	%r495, 1;
	shfl.sync.bfly.b32 	%r496|%p72, %r494, %r495, %r481, %r483;
	mov.b32 	%f145, %r496;
	add.f32 	%f146, %f144, %f145;
	st.local.f32 	[%rd3+20], %f146;

$L__BB93_21:
	bar.sync 	0;
	setp.gt.s32 	%p73, %r3, 5;
	@%p73 bra 	$L__BB93_23;

	mad.lo.s32 	%r497, %r3, %r54, %r2;
	cvt.s64.s32 	%rd65, %r497;
	mul.lo.s32 	%r498, %r1, %r55;
	cvt.s64.s32 	%rd66, %r498;
	add.s64 	%rd67, %rd66, %rd65;
	mul.wide.s32 	%rd68, %r3, 4;
	add.s64 	%rd69, %rd3, %rd68;
	ld.local.f32 	%f147, [%rd69];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs3, %f147;}

	// end inline asm
	cvta.to.global.u64 	%rd70, %rd28;
	shl.b64 	%rd71, %rd67, 1;
	add.s64 	%rd72, %rd70, %rd71;
	st.global.u16 	[%rd72], %rs3;

$L__BB93_23:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_7_bs_128
.visible .entry ggml_matvec_f16_ncols_7_bs_128(
	.param .u64 ggml_matvec_f16_ncols_7_bs_128_param_0,
	.param .u64 ggml_matvec_f16_ncols_7_bs_128_param_1,
	.param .u64 ggml_matvec_f16_ncols_7_bs_128_param_2,
	.param .u32 ggml_matvec_f16_ncols_7_bs_128_param_3,
	.param .u32 ggml_matvec_f16_ncols_7_bs_128_param_4,
	.param .u32 ggml_matvec_f16_ncols_7_bs_128_param_5,
	.param .u32 ggml_matvec_f16_ncols_7_bs_128_param_6,
	.param .u32 ggml_matvec_f16_ncols_7_bs_128_param_7,
	.param .u32 ggml_matvec_f16_ncols_7_bs_128_param_8,
	.param .u32 ggml_matvec_f16_ncols_7_bs_128_param_9,
	.param .u32 ggml_matvec_f16_ncols_7_bs_128_param_10,
	.param .u32 ggml_matvec_f16_ncols_7_bs_128_param_11
)
{
	.local .align 4 .b8 	__local_depot94[28];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<85>;
	.reg .b16 	%rs<4>;
	.reg .f32 	%f<172>;
	.reg .b32 	%r<607>;
	.reg .b64 	%rd<81>;


	mov.u64 	%SPL, __local_depot94;
	ld.param.u64 	%rd30, [ggml_matvec_f16_ncols_7_bs_128_param_0];
	ld.param.u64 	%rd31, [ggml_matvec_f16_ncols_7_bs_128_param_1];
	ld.param.u64 	%rd29, [ggml_matvec_f16_ncols_7_bs_128_param_2];
	ld.param.u32 	%r58, [ggml_matvec_f16_ncols_7_bs_128_param_3];
	ld.param.u32 	%r62, [ggml_matvec_f16_ncols_7_bs_128_param_5];
	ld.param.u32 	%r59, [ggml_matvec_f16_ncols_7_bs_128_param_6];
	ld.param.u32 	%r60, [ggml_matvec_f16_ncols_7_bs_128_param_7];
	ld.param.u32 	%r63, [ggml_matvec_f16_ncols_7_bs_128_param_8];
	ld.param.u32 	%r64, [ggml_matvec_f16_ncols_7_bs_128_param_9];
	ld.param.u32 	%r65, [ggml_matvec_f16_ncols_7_bs_128_param_10];
	ld.param.u32 	%r61, [ggml_matvec_f16_ncols_7_bs_128_param_11];
	cvta.to.global.u64 	%rd80, %rd31;
	cvta.to.global.u64 	%rd2, %rd30;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r66, %r1, %r63;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r67, %r2, %r62;
	mad.lo.s32 	%r68, %r66, %r64, %r67;
	cvt.s64.s32 	%rd4, %r68;
	mul.lo.s32 	%r69, %r1, %r65;
	cvt.s64.s32 	%rd5, %r69;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r70, %r3, 2;
	mov.u32 	%r71, data_mmv;
	add.s32 	%r4, %r71, %r70;
	@%p1 bra 	$L__BB94_2;

	mov.u32 	%r72, 0;
	st.shared.u32 	[%r4], %r72;

$L__BB94_2:
	bar.sync 	0;
	mov.f32 	%f8, 0f00000000;
	mov.u32 	%r600, 0;
	st.local.u32 	[%rd3], %r600;
	st.local.u32 	[%rd3+4], %r600;
	st.local.u32 	[%rd3+8], %r600;
	st.local.u32 	[%rd3+12], %r600;
	st.local.u32 	[%rd3+16], %r600;
	st.local.u32 	[%rd3+20], %r600;
	st.local.u32 	[%rd3+24], %r600;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs2, %f8;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f8;}

	// end inline asm
	mov.b32 	%r606, {%rs1, %rs2};
	setp.ge.s32 	%p2, %r3, %r58;
	mov.u32 	%r601, %r600;
	mov.u32 	%r602, %r600;
	mov.u32 	%r603, %r600;
	mov.u32 	%r604, %r600;
	mov.u32 	%r605, %r600;
	@%p2 bra 	$L__BB94_9;

	not.b32 	%r85, %r3;
	add.s32 	%r6, %r85, %r58;
	shr.u32 	%r86, %r6, 7;
	add.s32 	%r87, %r86, 1;
	and.b32  	%r583, %r87, 3;
	setp.eq.s32 	%p3, %r583, 0;
	mov.u32 	%r600, 0;
	mov.u32 	%r591, %r3;
	@%p3 bra 	$L__BB94_6;

	shl.b32 	%r94, %r59, 1;
	add.s32 	%r95, %r3, %r94;
	mul.wide.s32 	%rd33, %r95, 4;
	shl.b64 	%rd34, %rd5, 1;
	add.s64 	%rd7, %rd33, %rd34;
	mul.wide.s32 	%rd35, %r3, 4;
	mul.wide.s32 	%rd8, %r59, 4;
	add.s64 	%rd36, %rd35, %rd8;
	add.s64 	%rd9, %rd36, %rd34;
	add.s64 	%rd10, %rd35, %rd34;
	mul.wide.s32 	%rd37, %r3, 2;
	add.s64 	%rd38, %rd37, %rd4;
	shl.b64 	%rd39, %rd38, 1;
	add.s64 	%rd77, %rd2, %rd39;
	mov.u32 	%r600, 0;
	mov.u64 	%rd78, %rd80;
	mov.u32 	%r601, %r600;
	mov.u32 	%r602, %r600;
	mov.u32 	%r603, %r600;
	mov.u32 	%r604, %r600;
	mov.u32 	%r605, %r600;
	mov.u32 	%r591, %r3;

$L__BB94_5:
	.pragma "nounroll";
	ld.global.nc.u32 	%r97, [%rd77];
	add.s64 	%rd40, %rd78, %rd10;
	ld.global.nc.u32 	%r98, [%rd40];
	// begin inline asm
	{mul.f16x2 %r96,%r97,%r98;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r606,%r606,%r96;
}
	// end inline asm
	add.s64 	%rd41, %rd78, %rd9;
	ld.global.nc.u32 	%r104, [%rd41];
	// begin inline asm
	{mul.f16x2 %r102,%r97,%r104;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r605,%r605,%r102;
}
	// end inline asm
	add.s64 	%rd42, %rd78, %rd7;
	ld.global.nc.u32 	%r110, [%rd42];
	// begin inline asm
	{mul.f16x2 %r108,%r97,%r110;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r604,%r604,%r108;
}
	// end inline asm
	add.s64 	%rd43, %rd42, %rd8;
	ld.global.nc.u32 	%r116, [%rd43];
	// begin inline asm
	{mul.f16x2 %r114,%r97,%r116;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r603,%r603,%r114;
}
	// end inline asm
	add.s64 	%rd44, %rd43, %rd8;
	ld.global.nc.u32 	%r122, [%rd44];
	// begin inline asm
	{mul.f16x2 %r120,%r97,%r122;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r602,%r602,%r120;
}
	// end inline asm
	add.s64 	%rd45, %rd44, %rd8;
	ld.global.nc.u32 	%r128, [%rd45];
	// begin inline asm
	{mul.f16x2 %r126,%r97,%r128;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r601,%r601,%r126;
}
	// end inline asm
	add.s64 	%rd46, %rd45, %rd8;
	ld.global.nc.u32 	%r134, [%rd46];
	// begin inline asm
	{mul.f16x2 %r132,%r97,%r134;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r600,%r600,%r132;
}
	// end inline asm
	add.s32 	%r591, %r591, 128;
	add.s64 	%rd78, %rd78, 512;
	add.s64 	%rd77, %rd77, 512;
	add.s32 	%r583, %r583, -1;
	setp.ne.s32 	%p4, %r583, 0;
	@%p4 bra 	$L__BB94_5;

$L__BB94_6:
	setp.lt.u32 	%p5, %r6, 384;
	@%p5 bra 	$L__BB94_9;

	add.s32 	%r138, %r591, %r59;
	shl.b32 	%r139, %r59, 1;
	add.s32 	%r140, %r591, %r139;
	mad.lo.s32 	%r141, %r59, 3, %r591;
	shl.b32 	%r142, %r59, 2;
	add.s32 	%r143, %r591, %r142;
	mad.lo.s32 	%r144, %r59, 5, %r591;
	mad.lo.s32 	%r145, %r59, 6, %r591;
	add.s32 	%r146, %r138, 128;
	mul.wide.s32 	%rd47, %r146, 4;
	shl.b64 	%rd48, %rd5, 1;
	add.s64 	%rd16, %rd47, %rd48;
	mul.wide.s32 	%rd49, %r140, 4;
	add.s64 	%rd17, %rd49, %rd48;
	mul.wide.s32 	%rd50, %r141, 4;
	add.s64 	%rd18, %rd50, %rd48;
	mul.wide.s32 	%rd51, %r143, 4;
	add.s64 	%rd19, %rd51, %rd48;
	mul.wide.s32 	%rd52, %r144, 4;
	add.s64 	%rd20, %rd52, %rd48;
	mul.wide.s32 	%rd53, %r145, 4;
	add.s64 	%rd21, %rd53, %rd48;
	mul.wide.s32 	%rd54, %r591, 2;
	add.s64 	%rd55, %rd54, %rd4;
	shl.b64 	%rd56, %rd55, 1;
	add.s64 	%rd57, %rd2, %rd56;
	add.s64 	%rd79, %rd57, 1024;
	mul.wide.s32 	%rd58, %r591, 4;
	add.s64 	%rd23, %rd58, %rd48;
	mul.wide.s32 	%rd59, %r59, 4;
	add.s64 	%rd60, %rd58, %rd59;
	add.s64 	%rd24, %rd60, %rd48;

$L__BB94_8:
	ld.global.nc.u32 	%r148, [%rd79+-1024];
	add.s64 	%rd61, %rd80, %rd23;
	ld.global.nc.u32 	%r149, [%rd61];
	// begin inline asm
	{mul.f16x2 %r147,%r148,%r149;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r150,%r606,%r147;
}
	// end inline asm
	add.s64 	%rd62, %rd80, %rd24;
	ld.global.nc.u32 	%r155, [%rd62];
	// begin inline asm
	{mul.f16x2 %r153,%r148,%r155;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r156,%r605,%r153;
}
	// end inline asm
	add.s64 	%rd63, %rd80, %rd17;
	ld.global.nc.u32 	%r161, [%rd63];
	// begin inline asm
	{mul.f16x2 %r159,%r148,%r161;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r162,%r604,%r159;
}
	// end inline asm
	add.s64 	%rd64, %rd80, %rd18;
	ld.global.nc.u32 	%r167, [%rd64];
	// begin inline asm
	{mul.f16x2 %r165,%r148,%r167;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r168,%r603,%r165;
}
	// end inline asm
	add.s64 	%rd65, %rd80, %rd19;
	ld.global.nc.u32 	%r173, [%rd65];
	// begin inline asm
	{mul.f16x2 %r171,%r148,%r173;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r174,%r602,%r171;
}
	// end inline asm
	add.s64 	%rd66, %rd80, %rd20;
	ld.global.nc.u32 	%r179, [%rd66];
	// begin inline asm
	{mul.f16x2 %r177,%r148,%r179;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r180,%r601,%r177;
}
	// end inline asm
	add.s64 	%rd67, %rd80, %rd21;
	ld.global.nc.u32 	%r185, [%rd67];
	// begin inline asm
	{mul.f16x2 %r183,%r148,%r185;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r186,%r600,%r183;
}
	// end inline asm
	ld.global.nc.u32 	%r190, [%rd79+-512];
	ld.global.nc.u32 	%r191, [%rd61+512];
	// begin inline asm
	{mul.f16x2 %r189,%r190,%r191;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r192,%r150,%r189;
}
	// end inline asm
	add.s64 	%rd68, %rd80, %rd16;
	ld.global.nc.u32 	%r197, [%rd68];
	// begin inline asm
	{mul.f16x2 %r195,%r190,%r197;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r198,%r156,%r195;
}
	// end inline asm
	ld.global.nc.u32 	%r203, [%rd63+512];
	// begin inline asm
	{mul.f16x2 %r201,%r190,%r203;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r204,%r162,%r201;
}
	// end inline asm
	ld.global.nc.u32 	%r209, [%rd64+512];
	// begin inline asm
	{mul.f16x2 %r207,%r190,%r209;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r210,%r168,%r207;
}
	// end inline asm
	ld.global.nc.u32 	%r215, [%rd65+512];
	// begin inline asm
	{mul.f16x2 %r213,%r190,%r215;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r216,%r174,%r213;
}
	// end inline asm
	ld.global.nc.u32 	%r221, [%rd66+512];
	// begin inline asm
	{mul.f16x2 %r219,%r190,%r221;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r222,%r180,%r219;
}
	// end inline asm
	ld.global.nc.u32 	%r227, [%rd67+512];
	// begin inline asm
	{mul.f16x2 %r225,%r190,%r227;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r228,%r186,%r225;
}
	// end inline asm
	ld.global.nc.u32 	%r232, [%rd79];
	ld.global.nc.u32 	%r233, [%rd61+1024];
	// begin inline asm
	{mul.f16x2 %r231,%r232,%r233;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r234,%r192,%r231;
}
	// end inline asm
	ld.global.nc.u32 	%r239, [%rd68+512];
	// begin inline asm
	{mul.f16x2 %r237,%r232,%r239;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r240,%r198,%r237;
}
	// end inline asm
	ld.global.nc.u32 	%r245, [%rd63+1024];
	// begin inline asm
	{mul.f16x2 %r243,%r232,%r245;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r246,%r204,%r243;
}
	// end inline asm
	ld.global.nc.u32 	%r251, [%rd64+1024];
	// begin inline asm
	{mul.f16x2 %r249,%r232,%r251;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r252,%r210,%r249;
}
	// end inline asm
	ld.global.nc.u32 	%r257, [%rd65+1024];
	// begin inline asm
	{mul.f16x2 %r255,%r232,%r257;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r258,%r216,%r255;
}
	// end inline asm
	ld.global.nc.u32 	%r263, [%rd66+1024];
	// begin inline asm
	{mul.f16x2 %r261,%r232,%r263;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r264,%r222,%r261;
}
	// end inline asm
	ld.global.nc.u32 	%r269, [%rd67+1024];
	// begin inline asm
	{mul.f16x2 %r267,%r232,%r269;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r270,%r228,%r267;
}
	// end inline asm
	ld.global.nc.u32 	%r274, [%rd79+512];
	ld.global.nc.u32 	%r275, [%rd61+1536];
	// begin inline asm
	{mul.f16x2 %r273,%r274,%r275;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r606,%r234,%r273;
}
	// end inline asm
	ld.global.nc.u32 	%r281, [%rd68+1024];
	// begin inline asm
	{mul.f16x2 %r279,%r274,%r281;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r605,%r240,%r279;
}
	// end inline asm
	ld.global.nc.u32 	%r287, [%rd63+1536];
	// begin inline asm
	{mul.f16x2 %r285,%r274,%r287;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r604,%r246,%r285;
}
	// end inline asm
	ld.global.nc.u32 	%r293, [%rd64+1536];
	// begin inline asm
	{mul.f16x2 %r291,%r274,%r293;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r603,%r252,%r291;
}
	// end inline asm
	ld.global.nc.u32 	%r299, [%rd65+1536];
	// begin inline asm
	{mul.f16x2 %r297,%r274,%r299;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r602,%r258,%r297;
}
	// end inline asm
	ld.global.nc.u32 	%r305, [%rd66+1536];
	// begin inline asm
	{mul.f16x2 %r303,%r274,%r305;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r601,%r264,%r303;
}
	// end inline asm
	ld.global.nc.u32 	%r311, [%rd67+1536];
	// begin inline asm
	{mul.f16x2 %r309,%r274,%r311;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r600,%r270,%r309;
}
	// end inline asm
	add.s64 	%rd80, %rd80, 2048;
	add.s64 	%rd79, %rd79, 2048;
	add.s32 	%r591, %r591, 512;
	setp.lt.s32 	%p6, %r591, %r58;
	@%p6 bra 	$L__BB94_8;

$L__BB94_9:
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r606;
  cvt.f32.f16 %f9, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r606;
  cvt.f32.f16 %f10, high;}

	// end inline asm
	add.f32 	%f23, %f9, %f10;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r605;
  cvt.f32.f16 %f11, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r605;
  cvt.f32.f16 %f12, high;}

	// end inline asm
	add.f32 	%f1, %f11, %f12;
	st.local.f32 	[%rd3+4], %f1;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r604;
  cvt.f32.f16 %f13, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r604;
  cvt.f32.f16 %f14, high;}

	// end inline asm
	add.f32 	%f2, %f13, %f14;
	st.local.f32 	[%rd3+8], %f2;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r603;
  cvt.f32.f16 %f15, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r603;
  cvt.f32.f16 %f16, high;}

	// end inline asm
	add.f32 	%f3, %f15, %f16;
	st.local.f32 	[%rd3+12], %f3;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r602;
  cvt.f32.f16 %f17, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r602;
  cvt.f32.f16 %f18, high;}

	// end inline asm
	add.f32 	%f4, %f17, %f18;
	st.local.f32 	[%rd3+16], %f4;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r601;
  cvt.f32.f16 %f19, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r601;
  cvt.f32.f16 %f20, high;}

	// end inline asm
	add.f32 	%f5, %f19, %f20;
	st.local.f32 	[%rd3+20], %f5;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r600;
  cvt.f32.f16 %f21, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r600;
  cvt.f32.f16 %f22, high;}

	// end inline asm
	add.f32 	%f6, %f21, %f22;
	st.local.f32 	[%rd3+24], %f6;
	shr.s32 	%r329, %r3, 31;
	shr.u32 	%r330, %r329, 27;
	add.s32 	%r331, %r3, %r330;
	shr.s32 	%r332, %r331, 5;
	shl.b32 	%r333, %r332, 2;
	add.s32 	%r57, %r71, %r333;
	mov.u32 	%r335, 2;
	mov.b32 	%r336, %f23;
	mov.u32 	%r337, 31;
	mov.u32 	%r338, 16;
	mov.u32 	%r339, -1;
	shfl.sync.bfly.b32 	%r340|%p7, %r336, %r338, %r337, %r339;
	mov.b32 	%f24, %r340;
	add.f32 	%f25, %f23, %f24;
	mov.b32 	%r341, %f25;
	mov.u32 	%r342, 8;
	shfl.sync.bfly.b32 	%r343|%p8, %r341, %r342, %r337, %r339;
	mov.b32 	%f26, %r343;
	add.f32 	%f27, %f25, %f26;
	mov.b32 	%r344, %f27;
	mov.u32 	%r345, 4;
	shfl.sync.bfly.b32 	%r346|%p9, %r344, %r345, %r337, %r339;
	mov.b32 	%f28, %r346;
	add.f32 	%f29, %f27, %f28;
	mov.b32 	%r347, %f29;
	shfl.sync.bfly.b32 	%r348|%p10, %r347, %r335, %r337, %r339;
	mov.b32 	%f30, %r348;
	add.f32 	%f31, %f29, %f30;
	mov.b32 	%r349, %f31;
	mov.u32 	%r350, 1;
	shfl.sync.bfly.b32 	%r351|%p11, %r349, %r350, %r337, %r339;
	mov.b32 	%f32, %r351;
	add.f32 	%f33, %f31, %f32;
	st.local.f32 	[%rd3], %f33;
	st.shared.f32 	[%r57], %f33;
	bar.sync 	0;
	@%p1 bra 	$L__BB94_11;

	ld.shared.f32 	%f34, [%r4];
	mov.b32 	%r352, %f34;
	shfl.sync.bfly.b32 	%r356|%p13, %r352, %r338, %r337, %r339;
	mov.b32 	%f35, %r356;
	add.f32 	%f36, %f34, %f35;
	mov.b32 	%r357, %f36;
	shfl.sync.bfly.b32 	%r359|%p14, %r357, %r342, %r337, %r339;
	mov.b32 	%f37, %r359;
	add.f32 	%f38, %f36, %f37;
	mov.b32 	%r360, %f38;
	shfl.sync.bfly.b32 	%r362|%p15, %r360, %r345, %r337, %r339;
	mov.b32 	%f39, %r362;
	add.f32 	%f40, %f38, %f39;
	mov.b32 	%r363, %f40;
	shfl.sync.bfly.b32 	%r365|%p16, %r363, %r335, %r337, %r339;
	mov.b32 	%f41, %r365;
	add.f32 	%f42, %f40, %f41;
	mov.b32 	%r366, %f42;
	shfl.sync.bfly.b32 	%r368|%p17, %r366, %r350, %r337, %r339;
	mov.b32 	%f43, %r368;
	add.f32 	%f44, %f42, %f43;
	st.local.f32 	[%rd3], %f44;

$L__BB94_11:
	bar.sync 	0;
	mov.b32 	%r369, %f1;
	shfl.sync.bfly.b32 	%r373|%p19, %r369, %r338, %r337, %r339;
	mov.b32 	%f45, %r373;
	add.f32 	%f46, %f1, %f45;
	mov.b32 	%r374, %f46;
	shfl.sync.bfly.b32 	%r376|%p20, %r374, %r342, %r337, %r339;
	mov.b32 	%f47, %r376;
	add.f32 	%f48, %f46, %f47;
	mov.b32 	%r377, %f48;
	shfl.sync.bfly.b32 	%r379|%p21, %r377, %r345, %r337, %r339;
	mov.b32 	%f49, %r379;
	add.f32 	%f50, %f48, %f49;
	mov.b32 	%r380, %f50;
	shfl.sync.bfly.b32 	%r382|%p22, %r380, %r335, %r337, %r339;
	mov.b32 	%f51, %r382;
	add.f32 	%f52, %f50, %f51;
	mov.b32 	%r383, %f52;
	shfl.sync.bfly.b32 	%r385|%p23, %r383, %r350, %r337, %r339;
	mov.b32 	%f53, %r385;
	add.f32 	%f54, %f52, %f53;
	st.local.f32 	[%rd3+4], %f54;
	st.shared.f32 	[%r57], %f54;
	bar.sync 	0;
	@%p1 bra 	$L__BB94_13;

	ld.shared.f32 	%f55, [%r4];
	mov.b32 	%r386, %f55;
	mov.u32 	%r387, 31;
	mov.u32 	%r388, 16;
	mov.u32 	%r389, -1;
	shfl.sync.bfly.b32 	%r390|%p24, %r386, %r388, %r387, %r389;
	mov.b32 	%f56, %r390;
	add.f32 	%f57, %f55, %f56;
	mov.b32 	%r391, %f57;
	mov.u32 	%r392, 8;
	shfl.sync.bfly.b32 	%r393|%p25, %r391, %r392, %r387, %r389;
	mov.b32 	%f58, %r393;
	add.f32 	%f59, %f57, %f58;
	mov.b32 	%r394, %f59;
	mov.u32 	%r395, 4;
	shfl.sync.bfly.b32 	%r396|%p26, %r394, %r395, %r387, %r389;
	mov.b32 	%f60, %r396;
	add.f32 	%f61, %f59, %f60;
	mov.b32 	%r397, %f61;
	mov.u32 	%r398, 2;
	shfl.sync.bfly.b32 	%r399|%p27, %r397, %r398, %r387, %r389;
	mov.b32 	%f62, %r399;
	add.f32 	%f63, %f61, %f62;
	mov.b32 	%r400, %f63;
	mov.u32 	%r401, 1;
	shfl.sync.bfly.b32 	%r402|%p28, %r400, %r401, %r387, %r389;
	mov.b32 	%f64, %r402;
	add.f32 	%f65, %f63, %f64;
	st.local.f32 	[%rd3+4], %f65;

$L__BB94_13:
	bar.sync 	0;
	mov.b32 	%r403, %f2;
	mov.u32 	%r404, 31;
	mov.u32 	%r405, 16;
	mov.u32 	%r406, -1;
	shfl.sync.bfly.b32 	%r407|%p30, %r403, %r405, %r404, %r406;
	mov.b32 	%f66, %r407;
	add.f32 	%f67, %f2, %f66;
	mov.b32 	%r408, %f67;
	mov.u32 	%r409, 8;
	shfl.sync.bfly.b32 	%r410|%p31, %r408, %r409, %r404, %r406;
	mov.b32 	%f68, %r410;
	add.f32 	%f69, %f67, %f68;
	mov.b32 	%r411, %f69;
	mov.u32 	%r412, 4;
	shfl.sync.bfly.b32 	%r413|%p32, %r411, %r412, %r404, %r406;
	mov.b32 	%f70, %r413;
	add.f32 	%f71, %f69, %f70;
	mov.b32 	%r414, %f71;
	mov.u32 	%r415, 2;
	shfl.sync.bfly.b32 	%r416|%p33, %r414, %r415, %r404, %r406;
	mov.b32 	%f72, %r416;
	add.f32 	%f73, %f71, %f72;
	mov.b32 	%r417, %f73;
	mov.u32 	%r418, 1;
	shfl.sync.bfly.b32 	%r419|%p34, %r417, %r418, %r404, %r406;
	mov.b32 	%f74, %r419;
	add.f32 	%f75, %f73, %f74;
	st.local.f32 	[%rd3+8], %f75;
	st.shared.f32 	[%r57], %f75;
	bar.sync 	0;
	@%p1 bra 	$L__BB94_15;

	ld.shared.f32 	%f76, [%r4];
	mov.b32 	%r420, %f76;
	shfl.sync.bfly.b32 	%r424|%p35, %r420, %r405, %r404, %r406;
	mov.b32 	%f77, %r424;
	add.f32 	%f78, %f76, %f77;
	mov.b32 	%r425, %f78;
	shfl.sync.bfly.b32 	%r427|%p36, %r425, %r409, %r404, %r406;
	mov.b32 	%f79, %r427;
	add.f32 	%f80, %f78, %f79;
	mov.b32 	%r428, %f80;
	shfl.sync.bfly.b32 	%r430|%p37, %r428, %r412, %r404, %r406;
	mov.b32 	%f81, %r430;
	add.f32 	%f82, %f80, %f81;
	mov.b32 	%r431, %f82;
	shfl.sync.bfly.b32 	%r433|%p38, %r431, %r415, %r404, %r406;
	mov.b32 	%f83, %r433;
	add.f32 	%f84, %f82, %f83;
	mov.b32 	%r434, %f84;
	shfl.sync.bfly.b32 	%r436|%p39, %r434, %r418, %r404, %r406;
	mov.b32 	%f85, %r436;
	add.f32 	%f86, %f84, %f85;
	st.local.f32 	[%rd3+8], %f86;

$L__BB94_15:
	bar.sync 	0;
	mov.b32 	%r437, %f3;
	shfl.sync.bfly.b32 	%r441|%p41, %r437, %r405, %r404, %r406;
	mov.b32 	%f87, %r441;
	add.f32 	%f88, %f3, %f87;
	mov.b32 	%r442, %f88;
	shfl.sync.bfly.b32 	%r444|%p42, %r442, %r409, %r404, %r406;
	mov.b32 	%f89, %r444;
	add.f32 	%f90, %f88, %f89;
	mov.b32 	%r445, %f90;
	shfl.sync.bfly.b32 	%r447|%p43, %r445, %r412, %r404, %r406;
	mov.b32 	%f91, %r447;
	add.f32 	%f92, %f90, %f91;
	mov.b32 	%r448, %f92;
	shfl.sync.bfly.b32 	%r450|%p44, %r448, %r415, %r404, %r406;
	mov.b32 	%f93, %r450;
	add.f32 	%f94, %f92, %f93;
	mov.b32 	%r451, %f94;
	shfl.sync.bfly.b32 	%r453|%p45, %r451, %r418, %r404, %r406;
	mov.b32 	%f95, %r453;
	add.f32 	%f96, %f94, %f95;
	st.local.f32 	[%rd3+12], %f96;
	st.shared.f32 	[%r57], %f96;
	bar.sync 	0;
	@%p1 bra 	$L__BB94_17;

	ld.shared.f32 	%f97, [%r4];
	mov.b32 	%r454, %f97;
	mov.u32 	%r455, 31;
	mov.u32 	%r456, 16;
	mov.u32 	%r457, -1;
	shfl.sync.bfly.b32 	%r458|%p46, %r454, %r456, %r455, %r457;
	mov.b32 	%f98, %r458;
	add.f32 	%f99, %f97, %f98;
	mov.b32 	%r459, %f99;
	mov.u32 	%r460, 8;
	shfl.sync.bfly.b32 	%r461|%p47, %r459, %r460, %r455, %r457;
	mov.b32 	%f100, %r461;
	add.f32 	%f101, %f99, %f100;
	mov.b32 	%r462, %f101;
	mov.u32 	%r463, 4;
	shfl.sync.bfly.b32 	%r464|%p48, %r462, %r463, %r455, %r457;
	mov.b32 	%f102, %r464;
	add.f32 	%f103, %f101, %f102;
	mov.b32 	%r465, %f103;
	mov.u32 	%r466, 2;
	shfl.sync.bfly.b32 	%r467|%p49, %r465, %r466, %r455, %r457;
	mov.b32 	%f104, %r467;
	add.f32 	%f105, %f103, %f104;
	mov.b32 	%r468, %f105;
	mov.u32 	%r469, 1;
	shfl.sync.bfly.b32 	%r470|%p50, %r468, %r469, %r455, %r457;
	mov.b32 	%f106, %r470;
	add.f32 	%f107, %f105, %f106;
	st.local.f32 	[%rd3+12], %f107;

$L__BB94_17:
	bar.sync 	0;
	mov.b32 	%r471, %f4;
	mov.u32 	%r472, 31;
	mov.u32 	%r473, 16;
	mov.u32 	%r474, -1;
	shfl.sync.bfly.b32 	%r475|%p52, %r471, %r473, %r472, %r474;
	mov.b32 	%f108, %r475;
	add.f32 	%f109, %f4, %f108;
	mov.b32 	%r476, %f109;
	mov.u32 	%r477, 8;
	shfl.sync.bfly.b32 	%r478|%p53, %r476, %r477, %r472, %r474;
	mov.b32 	%f110, %r478;
	add.f32 	%f111, %f109, %f110;
	mov.b32 	%r479, %f111;
	mov.u32 	%r480, 4;
	shfl.sync.bfly.b32 	%r481|%p54, %r479, %r480, %r472, %r474;
	mov.b32 	%f112, %r481;
	add.f32 	%f113, %f111, %f112;
	mov.b32 	%r482, %f113;
	mov.u32 	%r483, 2;
	shfl.sync.bfly.b32 	%r484|%p55, %r482, %r483, %r472, %r474;
	mov.b32 	%f114, %r484;
	add.f32 	%f115, %f113, %f114;
	mov.b32 	%r485, %f115;
	mov.u32 	%r486, 1;
	shfl.sync.bfly.b32 	%r487|%p56, %r485, %r486, %r472, %r474;
	mov.b32 	%f116, %r487;
	add.f32 	%f117, %f115, %f116;
	st.local.f32 	[%rd3+16], %f117;
	st.shared.f32 	[%r57], %f117;
	bar.sync 	0;
	@%p1 bra 	$L__BB94_19;

	ld.shared.f32 	%f118, [%r4];
	mov.b32 	%r488, %f118;
	shfl.sync.bfly.b32 	%r492|%p57, %r488, %r473, %r472, %r474;
	mov.b32 	%f119, %r492;
	add.f32 	%f120, %f118, %f119;
	mov.b32 	%r493, %f120;
	shfl.sync.bfly.b32 	%r495|%p58, %r493, %r477, %r472, %r474;
	mov.b32 	%f121, %r495;
	add.f32 	%f122, %f120, %f121;
	mov.b32 	%r496, %f122;
	shfl.sync.bfly.b32 	%r498|%p59, %r496, %r480, %r472, %r474;
	mov.b32 	%f123, %r498;
	add.f32 	%f124, %f122, %f123;
	mov.b32 	%r499, %f124;
	shfl.sync.bfly.b32 	%r501|%p60, %r499, %r483, %r472, %r474;
	mov.b32 	%f125, %r501;
	add.f32 	%f126, %f124, %f125;
	mov.b32 	%r502, %f126;
	shfl.sync.bfly.b32 	%r504|%p61, %r502, %r486, %r472, %r474;
	mov.b32 	%f127, %r504;
	add.f32 	%f128, %f126, %f127;
	st.local.f32 	[%rd3+16], %f128;

$L__BB94_19:
	bar.sync 	0;
	mov.b32 	%r505, %f5;
	shfl.sync.bfly.b32 	%r509|%p63, %r505, %r473, %r472, %r474;
	mov.b32 	%f129, %r509;
	add.f32 	%f130, %f5, %f129;
	mov.b32 	%r510, %f130;
	shfl.sync.bfly.b32 	%r512|%p64, %r510, %r477, %r472, %r474;
	mov.b32 	%f131, %r512;
	add.f32 	%f132, %f130, %f131;
	mov.b32 	%r513, %f132;
	shfl.sync.bfly.b32 	%r515|%p65, %r513, %r480, %r472, %r474;
	mov.b32 	%f133, %r515;
	add.f32 	%f134, %f132, %f133;
	mov.b32 	%r516, %f134;
	shfl.sync.bfly.b32 	%r518|%p66, %r516, %r483, %r472, %r474;
	mov.b32 	%f135, %r518;
	add.f32 	%f136, %f134, %f135;
	mov.b32 	%r519, %f136;
	shfl.sync.bfly.b32 	%r521|%p67, %r519, %r486, %r472, %r474;
	mov.b32 	%f137, %r521;
	add.f32 	%f138, %f136, %f137;
	st.local.f32 	[%rd3+20], %f138;
	st.shared.f32 	[%r57], %f138;
	bar.sync 	0;
	@%p1 bra 	$L__BB94_21;

	ld.shared.f32 	%f139, [%r4];
	mov.b32 	%r522, %f139;
	mov.u32 	%r523, 31;
	mov.u32 	%r524, 16;
	mov.u32 	%r525, -1;
	shfl.sync.bfly.b32 	%r526|%p68, %r522, %r524, %r523, %r525;
	mov.b32 	%f140, %r526;
	add.f32 	%f141, %f139, %f140;
	mov.b32 	%r527, %f141;
	mov.u32 	%r528, 8;
	shfl.sync.bfly.b32 	%r529|%p69, %r527, %r528, %r523, %r525;
	mov.b32 	%f142, %r529;
	add.f32 	%f143, %f141, %f142;
	mov.b32 	%r530, %f143;
	mov.u32 	%r531, 4;
	shfl.sync.bfly.b32 	%r532|%p70, %r530, %r531, %r523, %r525;
	mov.b32 	%f144, %r532;
	add.f32 	%f145, %f143, %f144;
	mov.b32 	%r533, %f145;
	mov.u32 	%r534, 2;
	shfl.sync.bfly.b32 	%r535|%p71, %r533, %r534, %r523, %r525;
	mov.b32 	%f146, %r535;
	add.f32 	%f147, %f145, %f146;
	mov.b32 	%r536, %f147;
	mov.u32 	%r537, 1;
	shfl.sync.bfly.b32 	%r538|%p72, %r536, %r537, %r523, %r525;
	mov.b32 	%f148, %r538;
	add.f32 	%f149, %f147, %f148;
	st.local.f32 	[%rd3+20], %f149;

$L__BB94_21:
	bar.sync 	0;
	mov.b32 	%r539, %f6;
	mov.u32 	%r540, 31;
	mov.u32 	%r541, 16;
	mov.u32 	%r542, -1;
	shfl.sync.bfly.b32 	%r543|%p74, %r539, %r541, %r540, %r542;
	mov.b32 	%f150, %r543;
	add.f32 	%f151, %f6, %f150;
	mov.b32 	%r544, %f151;
	mov.u32 	%r545, 8;
	shfl.sync.bfly.b32 	%r546|%p75, %r544, %r545, %r540, %r542;
	mov.b32 	%f152, %r546;
	add.f32 	%f153, %f151, %f152;
	mov.b32 	%r547, %f153;
	mov.u32 	%r548, 4;
	shfl.sync.bfly.b32 	%r549|%p76, %r547, %r548, %r540, %r542;
	mov.b32 	%f154, %r549;
	add.f32 	%f155, %f153, %f154;
	mov.b32 	%r550, %f155;
	mov.u32 	%r551, 2;
	shfl.sync.bfly.b32 	%r552|%p77, %r550, %r551, %r540, %r542;
	mov.b32 	%f156, %r552;
	add.f32 	%f157, %f155, %f156;
	mov.b32 	%r553, %f157;
	mov.u32 	%r554, 1;
	shfl.sync.bfly.b32 	%r555|%p78, %r553, %r554, %r540, %r542;
	mov.b32 	%f158, %r555;
	add.f32 	%f159, %f157, %f158;
	st.local.f32 	[%rd3+24], %f159;
	st.shared.f32 	[%r57], %f159;
	bar.sync 	0;
	@%p1 bra 	$L__BB94_23;

	ld.shared.f32 	%f160, [%r4];
	mov.b32 	%r556, %f160;
	shfl.sync.bfly.b32 	%r560|%p79, %r556, %r541, %r540, %r542;
	mov.b32 	%f161, %r560;
	add.f32 	%f162, %f160, %f161;
	mov.b32 	%r561, %f162;
	shfl.sync.bfly.b32 	%r563|%p80, %r561, %r545, %r540, %r542;
	mov.b32 	%f163, %r563;
	add.f32 	%f164, %f162, %f163;
	mov.b32 	%r564, %f164;
	shfl.sync.bfly.b32 	%r566|%p81, %r564, %r548, %r540, %r542;
	mov.b32 	%f165, %r566;
	add.f32 	%f166, %f164, %f165;
	mov.b32 	%r567, %f166;
	shfl.sync.bfly.b32 	%r569|%p82, %r567, %r551, %r540, %r542;
	mov.b32 	%f167, %r569;
	add.f32 	%f168, %f166, %f167;
	mov.b32 	%r570, %f168;
	shfl.sync.bfly.b32 	%r572|%p83, %r570, %r554, %r540, %r542;
	mov.b32 	%f169, %r572;
	add.f32 	%f170, %f168, %f169;
	st.local.f32 	[%rd3+24], %f170;

$L__BB94_23:
	bar.sync 	0;
	setp.gt.s32 	%p84, %r3, 6;
	@%p84 bra 	$L__BB94_25;

	mad.lo.s32 	%r573, %r3, %r60, %r2;
	cvt.s64.s32 	%rd69, %r573;
	mul.lo.s32 	%r574, %r1, %r61;
	cvt.s64.s32 	%rd70, %r574;
	add.s64 	%rd71, %rd70, %rd69;
	mul.wide.s32 	%rd72, %r3, 4;
	add.s64 	%rd73, %rd3, %rd72;
	ld.local.f32 	%f171, [%rd73];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs3, %f171;}

	// end inline asm
	cvta.to.global.u64 	%rd74, %rd29;
	shl.b64 	%rd75, %rd71, 1;
	add.s64 	%rd76, %rd74, %rd75;
	st.global.u16 	[%rd76], %rs3;

$L__BB94_25:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_8_bs_128
.visible .entry ggml_matvec_f16_ncols_8_bs_128(
	.param .u64 ggml_matvec_f16_ncols_8_bs_128_param_0,
	.param .u64 ggml_matvec_f16_ncols_8_bs_128_param_1,
	.param .u64 ggml_matvec_f16_ncols_8_bs_128_param_2,
	.param .u32 ggml_matvec_f16_ncols_8_bs_128_param_3,
	.param .u32 ggml_matvec_f16_ncols_8_bs_128_param_4,
	.param .u32 ggml_matvec_f16_ncols_8_bs_128_param_5,
	.param .u32 ggml_matvec_f16_ncols_8_bs_128_param_6,
	.param .u32 ggml_matvec_f16_ncols_8_bs_128_param_7,
	.param .u32 ggml_matvec_f16_ncols_8_bs_128_param_8,
	.param .u32 ggml_matvec_f16_ncols_8_bs_128_param_9,
	.param .u32 ggml_matvec_f16_ncols_8_bs_128_param_10,
	.param .u32 ggml_matvec_f16_ncols_8_bs_128_param_11
)
{
	.local .align 16 .b8 	__local_depot95[32];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<96>;
	.reg .b16 	%rs<4>;
	.reg .f32 	%f<196>;
	.reg .b32 	%r<687>;
	.reg .b64 	%rd<85>;


	mov.u64 	%SPL, __local_depot95;
	ld.param.u64 	%rd31, [ggml_matvec_f16_ncols_8_bs_128_param_0];
	ld.param.u64 	%rd32, [ggml_matvec_f16_ncols_8_bs_128_param_1];
	ld.param.u64 	%rd30, [ggml_matvec_f16_ncols_8_bs_128_param_2];
	ld.param.u32 	%r64, [ggml_matvec_f16_ncols_8_bs_128_param_3];
	ld.param.u32 	%r68, [ggml_matvec_f16_ncols_8_bs_128_param_5];
	ld.param.u32 	%r65, [ggml_matvec_f16_ncols_8_bs_128_param_6];
	ld.param.u32 	%r66, [ggml_matvec_f16_ncols_8_bs_128_param_7];
	ld.param.u32 	%r69, [ggml_matvec_f16_ncols_8_bs_128_param_8];
	ld.param.u32 	%r70, [ggml_matvec_f16_ncols_8_bs_128_param_9];
	ld.param.u32 	%r71, [ggml_matvec_f16_ncols_8_bs_128_param_10];
	ld.param.u32 	%r67, [ggml_matvec_f16_ncols_8_bs_128_param_11];
	cvta.to.global.u64 	%rd84, %rd32;
	cvta.to.global.u64 	%rd2, %rd31;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r72, %r1, %r69;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r73, %r2, %r68;
	mad.lo.s32 	%r74, %r72, %r70, %r73;
	cvt.s64.s32 	%rd4, %r74;
	mul.lo.s32 	%r75, %r1, %r71;
	cvt.s64.s32 	%rd5, %r75;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r76, %r3, 2;
	mov.u32 	%r77, data_mmv;
	add.s32 	%r4, %r77, %r76;
	@%p1 bra 	$L__BB95_2;

	mov.u32 	%r78, 0;
	st.shared.u32 	[%r4], %r78;

$L__BB95_2:
	bar.sync 	0;
	mov.f32 	%f9, 0f00000000;
	st.local.v4.f32 	[%rd3], {%f9, %f9, %f9, %f9};
	st.local.v4.f32 	[%rd3+16], {%f9, %f9, %f9, %f9};
	mov.u32 	%r679, 0;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs2, %f9;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f9;}

	// end inline asm
	mov.b32 	%r686, {%rs1, %rs2};
	setp.ge.s32 	%p2, %r3, %r64;
	mov.u32 	%r680, %r679;
	mov.u32 	%r681, %r679;
	mov.u32 	%r682, %r679;
	mov.u32 	%r683, %r679;
	mov.u32 	%r684, %r679;
	mov.u32 	%r685, %r679;
	@%p2 bra 	$L__BB95_9;

	not.b32 	%r93, %r3;
	add.s32 	%r6, %r93, %r64;
	shr.u32 	%r94, %r6, 7;
	add.s32 	%r95, %r94, 1;
	and.b32  	%r660, %r95, 3;
	setp.eq.s32 	%p3, %r660, 0;
	mov.u32 	%r679, 0;
	mov.u32 	%r669, %r3;
	@%p3 bra 	$L__BB95_6;

	shl.b32 	%r103, %r65, 1;
	add.s32 	%r104, %r3, %r103;
	mul.wide.s32 	%rd34, %r104, 4;
	shl.b64 	%rd35, %rd5, 1;
	add.s64 	%rd7, %rd34, %rd35;
	mul.wide.s32 	%rd36, %r3, 4;
	mul.wide.s32 	%rd8, %r65, 4;
	add.s64 	%rd37, %rd36, %rd8;
	add.s64 	%rd9, %rd37, %rd35;
	add.s64 	%rd10, %rd36, %rd35;
	mul.wide.s32 	%rd38, %r3, 2;
	add.s64 	%rd39, %rd38, %rd4;
	shl.b64 	%rd40, %rd39, 1;
	add.s64 	%rd81, %rd2, %rd40;
	mov.u32 	%r679, 0;
	mov.u64 	%rd82, %rd84;
	mov.u32 	%r680, %r679;
	mov.u32 	%r681, %r679;
	mov.u32 	%r682, %r679;
	mov.u32 	%r683, %r679;
	mov.u32 	%r684, %r679;
	mov.u32 	%r685, %r679;
	mov.u32 	%r669, %r3;

$L__BB95_5:
	.pragma "nounroll";
	ld.global.nc.u32 	%r106, [%rd81];
	add.s64 	%rd41, %rd82, %rd10;
	ld.global.nc.u32 	%r107, [%rd41];
	// begin inline asm
	{mul.f16x2 %r105,%r106,%r107;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r686,%r686,%r105;
}
	// end inline asm
	add.s64 	%rd42, %rd82, %rd9;
	ld.global.nc.u32 	%r113, [%rd42];
	// begin inline asm
	{mul.f16x2 %r111,%r106,%r113;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r685,%r685,%r111;
}
	// end inline asm
	add.s64 	%rd43, %rd82, %rd7;
	ld.global.nc.u32 	%r119, [%rd43];
	// begin inline asm
	{mul.f16x2 %r117,%r106,%r119;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r684,%r684,%r117;
}
	// end inline asm
	add.s64 	%rd44, %rd43, %rd8;
	ld.global.nc.u32 	%r125, [%rd44];
	// begin inline asm
	{mul.f16x2 %r123,%r106,%r125;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r683,%r683,%r123;
}
	// end inline asm
	add.s64 	%rd45, %rd44, %rd8;
	ld.global.nc.u32 	%r131, [%rd45];
	// begin inline asm
	{mul.f16x2 %r129,%r106,%r131;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r682,%r682,%r129;
}
	// end inline asm
	add.s64 	%rd46, %rd45, %rd8;
	ld.global.nc.u32 	%r137, [%rd46];
	// begin inline asm
	{mul.f16x2 %r135,%r106,%r137;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r681,%r681,%r135;
}
	// end inline asm
	add.s64 	%rd47, %rd46, %rd8;
	ld.global.nc.u32 	%r143, [%rd47];
	// begin inline asm
	{mul.f16x2 %r141,%r106,%r143;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r680,%r680,%r141;
}
	// end inline asm
	add.s64 	%rd48, %rd47, %rd8;
	ld.global.nc.u32 	%r149, [%rd48];
	// begin inline asm
	{mul.f16x2 %r147,%r106,%r149;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r679,%r679,%r147;
}
	// end inline asm
	add.s32 	%r669, %r669, 128;
	add.s64 	%rd82, %rd82, 512;
	add.s64 	%rd81, %rd81, 512;
	add.s32 	%r660, %r660, -1;
	setp.ne.s32 	%p4, %r660, 0;
	@%p4 bra 	$L__BB95_5;

$L__BB95_6:
	setp.lt.u32 	%p5, %r6, 384;
	@%p5 bra 	$L__BB95_9;

	add.s32 	%r153, %r669, %r65;
	shl.b32 	%r154, %r65, 1;
	add.s32 	%r155, %r669, %r154;
	mad.lo.s32 	%r156, %r65, 3, %r669;
	shl.b32 	%r157, %r65, 2;
	add.s32 	%r158, %r669, %r157;
	mad.lo.s32 	%r159, %r65, 5, %r669;
	mad.lo.s32 	%r160, %r65, 6, %r669;
	mad.lo.s32 	%r161, %r65, 7, %r669;
	add.s32 	%r162, %r153, 128;
	mul.wide.s32 	%rd49, %r162, 4;
	shl.b64 	%rd50, %rd5, 1;
	add.s64 	%rd16, %rd49, %rd50;
	mul.wide.s32 	%rd51, %r155, 4;
	add.s64 	%rd17, %rd51, %rd50;
	mul.wide.s32 	%rd52, %r156, 4;
	add.s64 	%rd18, %rd52, %rd50;
	mul.wide.s32 	%rd53, %r158, 4;
	add.s64 	%rd19, %rd53, %rd50;
	mul.wide.s32 	%rd54, %r159, 4;
	add.s64 	%rd20, %rd54, %rd50;
	mul.wide.s32 	%rd55, %r160, 4;
	add.s64 	%rd21, %rd55, %rd50;
	mul.wide.s32 	%rd56, %r161, 4;
	add.s64 	%rd22, %rd56, %rd50;
	mul.wide.s32 	%rd57, %r669, 2;
	add.s64 	%rd58, %rd57, %rd4;
	shl.b64 	%rd59, %rd58, 1;
	add.s64 	%rd60, %rd2, %rd59;
	add.s64 	%rd83, %rd60, 1024;
	mul.wide.s32 	%rd61, %r669, 4;
	add.s64 	%rd24, %rd61, %rd50;
	mul.wide.s32 	%rd62, %r65, 4;
	add.s64 	%rd63, %rd61, %rd62;
	add.s64 	%rd25, %rd63, %rd50;

$L__BB95_8:
	ld.global.nc.u32 	%r164, [%rd83+-1024];
	add.s64 	%rd64, %rd84, %rd24;
	ld.global.nc.u32 	%r165, [%rd64];
	// begin inline asm
	{mul.f16x2 %r163,%r164,%r165;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r166,%r686,%r163;
}
	// end inline asm
	add.s64 	%rd65, %rd84, %rd25;
	ld.global.nc.u32 	%r171, [%rd65];
	// begin inline asm
	{mul.f16x2 %r169,%r164,%r171;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r172,%r685,%r169;
}
	// end inline asm
	add.s64 	%rd66, %rd84, %rd17;
	ld.global.nc.u32 	%r177, [%rd66];
	// begin inline asm
	{mul.f16x2 %r175,%r164,%r177;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r178,%r684,%r175;
}
	// end inline asm
	add.s64 	%rd67, %rd84, %rd18;
	ld.global.nc.u32 	%r183, [%rd67];
	// begin inline asm
	{mul.f16x2 %r181,%r164,%r183;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r184,%r683,%r181;
}
	// end inline asm
	add.s64 	%rd68, %rd84, %rd19;
	ld.global.nc.u32 	%r189, [%rd68];
	// begin inline asm
	{mul.f16x2 %r187,%r164,%r189;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r190,%r682,%r187;
}
	// end inline asm
	add.s64 	%rd69, %rd84, %rd20;
	ld.global.nc.u32 	%r195, [%rd69];
	// begin inline asm
	{mul.f16x2 %r193,%r164,%r195;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r196,%r681,%r193;
}
	// end inline asm
	add.s64 	%rd70, %rd84, %rd21;
	ld.global.nc.u32 	%r201, [%rd70];
	// begin inline asm
	{mul.f16x2 %r199,%r164,%r201;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r202,%r680,%r199;
}
	// end inline asm
	add.s64 	%rd71, %rd84, %rd22;
	ld.global.nc.u32 	%r207, [%rd71];
	// begin inline asm
	{mul.f16x2 %r205,%r164,%r207;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r208,%r679,%r205;
}
	// end inline asm
	ld.global.nc.u32 	%r212, [%rd83+-512];
	ld.global.nc.u32 	%r213, [%rd64+512];
	// begin inline asm
	{mul.f16x2 %r211,%r212,%r213;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r214,%r166,%r211;
}
	// end inline asm
	add.s64 	%rd72, %rd84, %rd16;
	ld.global.nc.u32 	%r219, [%rd72];
	// begin inline asm
	{mul.f16x2 %r217,%r212,%r219;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r220,%r172,%r217;
}
	// end inline asm
	ld.global.nc.u32 	%r225, [%rd66+512];
	// begin inline asm
	{mul.f16x2 %r223,%r212,%r225;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r226,%r178,%r223;
}
	// end inline asm
	ld.global.nc.u32 	%r231, [%rd67+512];
	// begin inline asm
	{mul.f16x2 %r229,%r212,%r231;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r232,%r184,%r229;
}
	// end inline asm
	ld.global.nc.u32 	%r237, [%rd68+512];
	// begin inline asm
	{mul.f16x2 %r235,%r212,%r237;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r238,%r190,%r235;
}
	// end inline asm
	ld.global.nc.u32 	%r243, [%rd69+512];
	// begin inline asm
	{mul.f16x2 %r241,%r212,%r243;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r244,%r196,%r241;
}
	// end inline asm
	ld.global.nc.u32 	%r249, [%rd70+512];
	// begin inline asm
	{mul.f16x2 %r247,%r212,%r249;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r250,%r202,%r247;
}
	// end inline asm
	ld.global.nc.u32 	%r255, [%rd71+512];
	// begin inline asm
	{mul.f16x2 %r253,%r212,%r255;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r256,%r208,%r253;
}
	// end inline asm
	ld.global.nc.u32 	%r260, [%rd83];
	ld.global.nc.u32 	%r261, [%rd64+1024];
	// begin inline asm
	{mul.f16x2 %r259,%r260,%r261;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r262,%r214,%r259;
}
	// end inline asm
	ld.global.nc.u32 	%r267, [%rd72+512];
	// begin inline asm
	{mul.f16x2 %r265,%r260,%r267;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r268,%r220,%r265;
}
	// end inline asm
	ld.global.nc.u32 	%r273, [%rd66+1024];
	// begin inline asm
	{mul.f16x2 %r271,%r260,%r273;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r274,%r226,%r271;
}
	// end inline asm
	ld.global.nc.u32 	%r279, [%rd67+1024];
	// begin inline asm
	{mul.f16x2 %r277,%r260,%r279;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r280,%r232,%r277;
}
	// end inline asm
	ld.global.nc.u32 	%r285, [%rd68+1024];
	// begin inline asm
	{mul.f16x2 %r283,%r260,%r285;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r286,%r238,%r283;
}
	// end inline asm
	ld.global.nc.u32 	%r291, [%rd69+1024];
	// begin inline asm
	{mul.f16x2 %r289,%r260,%r291;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r292,%r244,%r289;
}
	// end inline asm
	ld.global.nc.u32 	%r297, [%rd70+1024];
	// begin inline asm
	{mul.f16x2 %r295,%r260,%r297;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r298,%r250,%r295;
}
	// end inline asm
	ld.global.nc.u32 	%r303, [%rd71+1024];
	// begin inline asm
	{mul.f16x2 %r301,%r260,%r303;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r304,%r256,%r301;
}
	// end inline asm
	ld.global.nc.u32 	%r308, [%rd83+512];
	ld.global.nc.u32 	%r309, [%rd64+1536];
	// begin inline asm
	{mul.f16x2 %r307,%r308,%r309;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r686,%r262,%r307;
}
	// end inline asm
	ld.global.nc.u32 	%r315, [%rd72+1024];
	// begin inline asm
	{mul.f16x2 %r313,%r308,%r315;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r685,%r268,%r313;
}
	// end inline asm
	ld.global.nc.u32 	%r321, [%rd66+1536];
	// begin inline asm
	{mul.f16x2 %r319,%r308,%r321;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r684,%r274,%r319;
}
	// end inline asm
	ld.global.nc.u32 	%r327, [%rd67+1536];
	// begin inline asm
	{mul.f16x2 %r325,%r308,%r327;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r683,%r280,%r325;
}
	// end inline asm
	ld.global.nc.u32 	%r333, [%rd68+1536];
	// begin inline asm
	{mul.f16x2 %r331,%r308,%r333;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r682,%r286,%r331;
}
	// end inline asm
	ld.global.nc.u32 	%r339, [%rd69+1536];
	// begin inline asm
	{mul.f16x2 %r337,%r308,%r339;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r681,%r292,%r337;
}
	// end inline asm
	ld.global.nc.u32 	%r345, [%rd70+1536];
	// begin inline asm
	{mul.f16x2 %r343,%r308,%r345;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r680,%r298,%r343;
}
	// end inline asm
	ld.global.nc.u32 	%r351, [%rd71+1536];
	// begin inline asm
	{mul.f16x2 %r349,%r308,%r351;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r679,%r304,%r349;
}
	// end inline asm
	add.s64 	%rd84, %rd84, 2048;
	add.s64 	%rd83, %rd83, 2048;
	add.s32 	%r669, %r669, 512;
	setp.lt.s32 	%p6, %r669, %r64;
	@%p6 bra 	$L__BB95_8;

$L__BB95_9:
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r686;
  cvt.f32.f16 %f10, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r686;
  cvt.f32.f16 %f11, high;}

	// end inline asm
	add.f32 	%f26, %f10, %f11;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r685;
  cvt.f32.f16 %f12, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r685;
  cvt.f32.f16 %f13, high;}

	// end inline asm
	add.f32 	%f1, %f12, %f13;
	st.local.f32 	[%rd3+4], %f1;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r684;
  cvt.f32.f16 %f14, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r684;
  cvt.f32.f16 %f15, high;}

	// end inline asm
	add.f32 	%f2, %f14, %f15;
	st.local.f32 	[%rd3+8], %f2;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r683;
  cvt.f32.f16 %f16, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r683;
  cvt.f32.f16 %f17, high;}

	// end inline asm
	add.f32 	%f3, %f16, %f17;
	st.local.f32 	[%rd3+12], %f3;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r682;
  cvt.f32.f16 %f18, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r682;
  cvt.f32.f16 %f19, high;}

	// end inline asm
	add.f32 	%f4, %f18, %f19;
	st.local.f32 	[%rd3+16], %f4;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r681;
  cvt.f32.f16 %f20, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r681;
  cvt.f32.f16 %f21, high;}

	// end inline asm
	add.f32 	%f5, %f20, %f21;
	st.local.f32 	[%rd3+20], %f5;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r680;
  cvt.f32.f16 %f22, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r680;
  cvt.f32.f16 %f23, high;}

	// end inline asm
	add.f32 	%f6, %f22, %f23;
	st.local.f32 	[%rd3+24], %f6;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r679;
  cvt.f32.f16 %f24, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r679;
  cvt.f32.f16 %f25, high;}

	// end inline asm
	add.f32 	%f7, %f24, %f25;
	st.local.f32 	[%rd3+28], %f7;
	shr.s32 	%r371, %r3, 31;
	shr.u32 	%r372, %r371, 27;
	add.s32 	%r373, %r3, %r372;
	shr.s32 	%r374, %r373, 5;
	shl.b32 	%r375, %r374, 2;
	add.s32 	%r63, %r77, %r375;
	mov.u32 	%r377, 2;
	mov.b32 	%r378, %f26;
	mov.u32 	%r379, 31;
	mov.u32 	%r380, 16;
	mov.u32 	%r381, -1;
	shfl.sync.bfly.b32 	%r382|%p7, %r378, %r380, %r379, %r381;
	mov.b32 	%f27, %r382;
	add.f32 	%f28, %f26, %f27;
	mov.b32 	%r383, %f28;
	mov.u32 	%r384, 8;
	shfl.sync.bfly.b32 	%r385|%p8, %r383, %r384, %r379, %r381;
	mov.b32 	%f29, %r385;
	add.f32 	%f30, %f28, %f29;
	mov.b32 	%r386, %f30;
	mov.u32 	%r387, 4;
	shfl.sync.bfly.b32 	%r388|%p9, %r386, %r387, %r379, %r381;
	mov.b32 	%f31, %r388;
	add.f32 	%f32, %f30, %f31;
	mov.b32 	%r389, %f32;
	shfl.sync.bfly.b32 	%r390|%p10, %r389, %r377, %r379, %r381;
	mov.b32 	%f33, %r390;
	add.f32 	%f34, %f32, %f33;
	mov.b32 	%r391, %f34;
	mov.u32 	%r392, 1;
	shfl.sync.bfly.b32 	%r393|%p11, %r391, %r392, %r379, %r381;
	mov.b32 	%f35, %r393;
	add.f32 	%f36, %f34, %f35;
	st.local.f32 	[%rd3], %f36;
	st.shared.f32 	[%r63], %f36;
	bar.sync 	0;
	@%p1 bra 	$L__BB95_11;

	ld.shared.f32 	%f37, [%r4];
	mov.b32 	%r394, %f37;
	shfl.sync.bfly.b32 	%r398|%p13, %r394, %r380, %r379, %r381;
	mov.b32 	%f38, %r398;
	add.f32 	%f39, %f37, %f38;
	mov.b32 	%r399, %f39;
	shfl.sync.bfly.b32 	%r401|%p14, %r399, %r384, %r379, %r381;
	mov.b32 	%f40, %r401;
	add.f32 	%f41, %f39, %f40;
	mov.b32 	%r402, %f41;
	shfl.sync.bfly.b32 	%r404|%p15, %r402, %r387, %r379, %r381;
	mov.b32 	%f42, %r404;
	add.f32 	%f43, %f41, %f42;
	mov.b32 	%r405, %f43;
	shfl.sync.bfly.b32 	%r407|%p16, %r405, %r377, %r379, %r381;
	mov.b32 	%f44, %r407;
	add.f32 	%f45, %f43, %f44;
	mov.b32 	%r408, %f45;
	shfl.sync.bfly.b32 	%r410|%p17, %r408, %r392, %r379, %r381;
	mov.b32 	%f46, %r410;
	add.f32 	%f47, %f45, %f46;
	st.local.f32 	[%rd3], %f47;

$L__BB95_11:
	bar.sync 	0;
	mov.b32 	%r411, %f1;
	shfl.sync.bfly.b32 	%r415|%p19, %r411, %r380, %r379, %r381;
	mov.b32 	%f48, %r415;
	add.f32 	%f49, %f1, %f48;
	mov.b32 	%r416, %f49;
	shfl.sync.bfly.b32 	%r418|%p20, %r416, %r384, %r379, %r381;
	mov.b32 	%f50, %r418;
	add.f32 	%f51, %f49, %f50;
	mov.b32 	%r419, %f51;
	shfl.sync.bfly.b32 	%r421|%p21, %r419, %r387, %r379, %r381;
	mov.b32 	%f52, %r421;
	add.f32 	%f53, %f51, %f52;
	mov.b32 	%r422, %f53;
	shfl.sync.bfly.b32 	%r424|%p22, %r422, %r377, %r379, %r381;
	mov.b32 	%f54, %r424;
	add.f32 	%f55, %f53, %f54;
	mov.b32 	%r425, %f55;
	shfl.sync.bfly.b32 	%r427|%p23, %r425, %r392, %r379, %r381;
	mov.b32 	%f56, %r427;
	add.f32 	%f57, %f55, %f56;
	st.local.f32 	[%rd3+4], %f57;
	st.shared.f32 	[%r63], %f57;
	bar.sync 	0;
	@%p1 bra 	$L__BB95_13;

	ld.shared.f32 	%f58, [%r4];
	mov.b32 	%r428, %f58;
	mov.u32 	%r429, 31;
	mov.u32 	%r430, 16;
	mov.u32 	%r431, -1;
	shfl.sync.bfly.b32 	%r432|%p24, %r428, %r430, %r429, %r431;
	mov.b32 	%f59, %r432;
	add.f32 	%f60, %f58, %f59;
	mov.b32 	%r433, %f60;
	mov.u32 	%r434, 8;
	shfl.sync.bfly.b32 	%r435|%p25, %r433, %r434, %r429, %r431;
	mov.b32 	%f61, %r435;
	add.f32 	%f62, %f60, %f61;
	mov.b32 	%r436, %f62;
	mov.u32 	%r437, 4;
	shfl.sync.bfly.b32 	%r438|%p26, %r436, %r437, %r429, %r431;
	mov.b32 	%f63, %r438;
	add.f32 	%f64, %f62, %f63;
	mov.b32 	%r439, %f64;
	mov.u32 	%r440, 2;
	shfl.sync.bfly.b32 	%r441|%p27, %r439, %r440, %r429, %r431;
	mov.b32 	%f65, %r441;
	add.f32 	%f66, %f64, %f65;
	mov.b32 	%r442, %f66;
	mov.u32 	%r443, 1;
	shfl.sync.bfly.b32 	%r444|%p28, %r442, %r443, %r429, %r431;
	mov.b32 	%f67, %r444;
	add.f32 	%f68, %f66, %f67;
	st.local.f32 	[%rd3+4], %f68;

$L__BB95_13:
	bar.sync 	0;
	mov.b32 	%r445, %f2;
	mov.u32 	%r446, 31;
	mov.u32 	%r447, 16;
	mov.u32 	%r448, -1;
	shfl.sync.bfly.b32 	%r449|%p30, %r445, %r447, %r446, %r448;
	mov.b32 	%f69, %r449;
	add.f32 	%f70, %f2, %f69;
	mov.b32 	%r450, %f70;
	mov.u32 	%r451, 8;
	shfl.sync.bfly.b32 	%r452|%p31, %r450, %r451, %r446, %r448;
	mov.b32 	%f71, %r452;
	add.f32 	%f72, %f70, %f71;
	mov.b32 	%r453, %f72;
	mov.u32 	%r454, 4;
	shfl.sync.bfly.b32 	%r455|%p32, %r453, %r454, %r446, %r448;
	mov.b32 	%f73, %r455;
	add.f32 	%f74, %f72, %f73;
	mov.b32 	%r456, %f74;
	mov.u32 	%r457, 2;
	shfl.sync.bfly.b32 	%r458|%p33, %r456, %r457, %r446, %r448;
	mov.b32 	%f75, %r458;
	add.f32 	%f76, %f74, %f75;
	mov.b32 	%r459, %f76;
	mov.u32 	%r460, 1;
	shfl.sync.bfly.b32 	%r461|%p34, %r459, %r460, %r446, %r448;
	mov.b32 	%f77, %r461;
	add.f32 	%f78, %f76, %f77;
	st.local.f32 	[%rd3+8], %f78;
	st.shared.f32 	[%r63], %f78;
	bar.sync 	0;
	@%p1 bra 	$L__BB95_15;

	ld.shared.f32 	%f79, [%r4];
	mov.b32 	%r462, %f79;
	shfl.sync.bfly.b32 	%r466|%p35, %r462, %r447, %r446, %r448;
	mov.b32 	%f80, %r466;
	add.f32 	%f81, %f79, %f80;
	mov.b32 	%r467, %f81;
	shfl.sync.bfly.b32 	%r469|%p36, %r467, %r451, %r446, %r448;
	mov.b32 	%f82, %r469;
	add.f32 	%f83, %f81, %f82;
	mov.b32 	%r470, %f83;
	shfl.sync.bfly.b32 	%r472|%p37, %r470, %r454, %r446, %r448;
	mov.b32 	%f84, %r472;
	add.f32 	%f85, %f83, %f84;
	mov.b32 	%r473, %f85;
	shfl.sync.bfly.b32 	%r475|%p38, %r473, %r457, %r446, %r448;
	mov.b32 	%f86, %r475;
	add.f32 	%f87, %f85, %f86;
	mov.b32 	%r476, %f87;
	shfl.sync.bfly.b32 	%r478|%p39, %r476, %r460, %r446, %r448;
	mov.b32 	%f88, %r478;
	add.f32 	%f89, %f87, %f88;
	st.local.f32 	[%rd3+8], %f89;

$L__BB95_15:
	bar.sync 	0;
	mov.b32 	%r479, %f3;
	shfl.sync.bfly.b32 	%r483|%p41, %r479, %r447, %r446, %r448;
	mov.b32 	%f90, %r483;
	add.f32 	%f91, %f3, %f90;
	mov.b32 	%r484, %f91;
	shfl.sync.bfly.b32 	%r486|%p42, %r484, %r451, %r446, %r448;
	mov.b32 	%f92, %r486;
	add.f32 	%f93, %f91, %f92;
	mov.b32 	%r487, %f93;
	shfl.sync.bfly.b32 	%r489|%p43, %r487, %r454, %r446, %r448;
	mov.b32 	%f94, %r489;
	add.f32 	%f95, %f93, %f94;
	mov.b32 	%r490, %f95;
	shfl.sync.bfly.b32 	%r492|%p44, %r490, %r457, %r446, %r448;
	mov.b32 	%f96, %r492;
	add.f32 	%f97, %f95, %f96;
	mov.b32 	%r493, %f97;
	shfl.sync.bfly.b32 	%r495|%p45, %r493, %r460, %r446, %r448;
	mov.b32 	%f98, %r495;
	add.f32 	%f99, %f97, %f98;
	st.local.f32 	[%rd3+12], %f99;
	st.shared.f32 	[%r63], %f99;
	bar.sync 	0;
	@%p1 bra 	$L__BB95_17;

	ld.shared.f32 	%f100, [%r4];
	mov.b32 	%r496, %f100;
	mov.u32 	%r497, 31;
	mov.u32 	%r498, 16;
	mov.u32 	%r499, -1;
	shfl.sync.bfly.b32 	%r500|%p46, %r496, %r498, %r497, %r499;
	mov.b32 	%f101, %r500;
	add.f32 	%f102, %f100, %f101;
	mov.b32 	%r501, %f102;
	mov.u32 	%r502, 8;
	shfl.sync.bfly.b32 	%r503|%p47, %r501, %r502, %r497, %r499;
	mov.b32 	%f103, %r503;
	add.f32 	%f104, %f102, %f103;
	mov.b32 	%r504, %f104;
	mov.u32 	%r505, 4;
	shfl.sync.bfly.b32 	%r506|%p48, %r504, %r505, %r497, %r499;
	mov.b32 	%f105, %r506;
	add.f32 	%f106, %f104, %f105;
	mov.b32 	%r507, %f106;
	mov.u32 	%r508, 2;
	shfl.sync.bfly.b32 	%r509|%p49, %r507, %r508, %r497, %r499;
	mov.b32 	%f107, %r509;
	add.f32 	%f108, %f106, %f107;
	mov.b32 	%r510, %f108;
	mov.u32 	%r511, 1;
	shfl.sync.bfly.b32 	%r512|%p50, %r510, %r511, %r497, %r499;
	mov.b32 	%f109, %r512;
	add.f32 	%f110, %f108, %f109;
	st.local.f32 	[%rd3+12], %f110;

$L__BB95_17:
	bar.sync 	0;
	mov.b32 	%r513, %f4;
	mov.u32 	%r514, 31;
	mov.u32 	%r515, 16;
	mov.u32 	%r516, -1;
	shfl.sync.bfly.b32 	%r517|%p52, %r513, %r515, %r514, %r516;
	mov.b32 	%f111, %r517;
	add.f32 	%f112, %f4, %f111;
	mov.b32 	%r518, %f112;
	mov.u32 	%r519, 8;
	shfl.sync.bfly.b32 	%r520|%p53, %r518, %r519, %r514, %r516;
	mov.b32 	%f113, %r520;
	add.f32 	%f114, %f112, %f113;
	mov.b32 	%r521, %f114;
	mov.u32 	%r522, 4;
	shfl.sync.bfly.b32 	%r523|%p54, %r521, %r522, %r514, %r516;
	mov.b32 	%f115, %r523;
	add.f32 	%f116, %f114, %f115;
	mov.b32 	%r524, %f116;
	mov.u32 	%r525, 2;
	shfl.sync.bfly.b32 	%r526|%p55, %r524, %r525, %r514, %r516;
	mov.b32 	%f117, %r526;
	add.f32 	%f118, %f116, %f117;
	mov.b32 	%r527, %f118;
	mov.u32 	%r528, 1;
	shfl.sync.bfly.b32 	%r529|%p56, %r527, %r528, %r514, %r516;
	mov.b32 	%f119, %r529;
	add.f32 	%f120, %f118, %f119;
	st.local.f32 	[%rd3+16], %f120;
	st.shared.f32 	[%r63], %f120;
	bar.sync 	0;
	@%p1 bra 	$L__BB95_19;

	ld.shared.f32 	%f121, [%r4];
	mov.b32 	%r530, %f121;
	shfl.sync.bfly.b32 	%r534|%p57, %r530, %r515, %r514, %r516;
	mov.b32 	%f122, %r534;
	add.f32 	%f123, %f121, %f122;
	mov.b32 	%r535, %f123;
	shfl.sync.bfly.b32 	%r537|%p58, %r535, %r519, %r514, %r516;
	mov.b32 	%f124, %r537;
	add.f32 	%f125, %f123, %f124;
	mov.b32 	%r538, %f125;
	shfl.sync.bfly.b32 	%r540|%p59, %r538, %r522, %r514, %r516;
	mov.b32 	%f126, %r540;
	add.f32 	%f127, %f125, %f126;
	mov.b32 	%r541, %f127;
	shfl.sync.bfly.b32 	%r543|%p60, %r541, %r525, %r514, %r516;
	mov.b32 	%f128, %r543;
	add.f32 	%f129, %f127, %f128;
	mov.b32 	%r544, %f129;
	shfl.sync.bfly.b32 	%r546|%p61, %r544, %r528, %r514, %r516;
	mov.b32 	%f130, %r546;
	add.f32 	%f131, %f129, %f130;
	st.local.f32 	[%rd3+16], %f131;

$L__BB95_19:
	bar.sync 	0;
	mov.b32 	%r547, %f5;
	shfl.sync.bfly.b32 	%r551|%p63, %r547, %r515, %r514, %r516;
	mov.b32 	%f132, %r551;
	add.f32 	%f133, %f5, %f132;
	mov.b32 	%r552, %f133;
	shfl.sync.bfly.b32 	%r554|%p64, %r552, %r519, %r514, %r516;
	mov.b32 	%f134, %r554;
	add.f32 	%f135, %f133, %f134;
	mov.b32 	%r555, %f135;
	shfl.sync.bfly.b32 	%r557|%p65, %r555, %r522, %r514, %r516;
	mov.b32 	%f136, %r557;
	add.f32 	%f137, %f135, %f136;
	mov.b32 	%r558, %f137;
	shfl.sync.bfly.b32 	%r560|%p66, %r558, %r525, %r514, %r516;
	mov.b32 	%f138, %r560;
	add.f32 	%f139, %f137, %f138;
	mov.b32 	%r561, %f139;
	shfl.sync.bfly.b32 	%r563|%p67, %r561, %r528, %r514, %r516;
	mov.b32 	%f140, %r563;
	add.f32 	%f141, %f139, %f140;
	st.local.f32 	[%rd3+20], %f141;
	st.shared.f32 	[%r63], %f141;
	bar.sync 	0;
	@%p1 bra 	$L__BB95_21;

	ld.shared.f32 	%f142, [%r4];
	mov.b32 	%r564, %f142;
	mov.u32 	%r565, 31;
	mov.u32 	%r566, 16;
	mov.u32 	%r567, -1;
	shfl.sync.bfly.b32 	%r568|%p68, %r564, %r566, %r565, %r567;
	mov.b32 	%f143, %r568;
	add.f32 	%f144, %f142, %f143;
	mov.b32 	%r569, %f144;
	mov.u32 	%r570, 8;
	shfl.sync.bfly.b32 	%r571|%p69, %r569, %r570, %r565, %r567;
	mov.b32 	%f145, %r571;
	add.f32 	%f146, %f144, %f145;
	mov.b32 	%r572, %f146;
	mov.u32 	%r573, 4;
	shfl.sync.bfly.b32 	%r574|%p70, %r572, %r573, %r565, %r567;
	mov.b32 	%f147, %r574;
	add.f32 	%f148, %f146, %f147;
	mov.b32 	%r575, %f148;
	mov.u32 	%r576, 2;
	shfl.sync.bfly.b32 	%r577|%p71, %r575, %r576, %r565, %r567;
	mov.b32 	%f149, %r577;
	add.f32 	%f150, %f148, %f149;
	mov.b32 	%r578, %f150;
	mov.u32 	%r579, 1;
	shfl.sync.bfly.b32 	%r580|%p72, %r578, %r579, %r565, %r567;
	mov.b32 	%f151, %r580;
	add.f32 	%f152, %f150, %f151;
	st.local.f32 	[%rd3+20], %f152;

$L__BB95_21:
	bar.sync 	0;
	mov.b32 	%r581, %f6;
	mov.u32 	%r582, 31;
	mov.u32 	%r583, 16;
	mov.u32 	%r584, -1;
	shfl.sync.bfly.b32 	%r585|%p74, %r581, %r583, %r582, %r584;
	mov.b32 	%f153, %r585;
	add.f32 	%f154, %f6, %f153;
	mov.b32 	%r586, %f154;
	mov.u32 	%r587, 8;
	shfl.sync.bfly.b32 	%r588|%p75, %r586, %r587, %r582, %r584;
	mov.b32 	%f155, %r588;
	add.f32 	%f156, %f154, %f155;
	mov.b32 	%r589, %f156;
	mov.u32 	%r590, 4;
	shfl.sync.bfly.b32 	%r591|%p76, %r589, %r590, %r582, %r584;
	mov.b32 	%f157, %r591;
	add.f32 	%f158, %f156, %f157;
	mov.b32 	%r592, %f158;
	mov.u32 	%r593, 2;
	shfl.sync.bfly.b32 	%r594|%p77, %r592, %r593, %r582, %r584;
	mov.b32 	%f159, %r594;
	add.f32 	%f160, %f158, %f159;
	mov.b32 	%r595, %f160;
	mov.u32 	%r596, 1;
	shfl.sync.bfly.b32 	%r597|%p78, %r595, %r596, %r582, %r584;
	mov.b32 	%f161, %r597;
	add.f32 	%f162, %f160, %f161;
	st.local.f32 	[%rd3+24], %f162;
	st.shared.f32 	[%r63], %f162;
	bar.sync 	0;
	@%p1 bra 	$L__BB95_23;

	ld.shared.f32 	%f163, [%r4];
	mov.b32 	%r598, %f163;
	shfl.sync.bfly.b32 	%r602|%p79, %r598, %r583, %r582, %r584;
	mov.b32 	%f164, %r602;
	add.f32 	%f165, %f163, %f164;
	mov.b32 	%r603, %f165;
	shfl.sync.bfly.b32 	%r605|%p80, %r603, %r587, %r582, %r584;
	mov.b32 	%f166, %r605;
	add.f32 	%f167, %f165, %f166;
	mov.b32 	%r606, %f167;
	shfl.sync.bfly.b32 	%r608|%p81, %r606, %r590, %r582, %r584;
	mov.b32 	%f168, %r608;
	add.f32 	%f169, %f167, %f168;
	mov.b32 	%r609, %f169;
	shfl.sync.bfly.b32 	%r611|%p82, %r609, %r593, %r582, %r584;
	mov.b32 	%f170, %r611;
	add.f32 	%f171, %f169, %f170;
	mov.b32 	%r612, %f171;
	shfl.sync.bfly.b32 	%r614|%p83, %r612, %r596, %r582, %r584;
	mov.b32 	%f172, %r614;
	add.f32 	%f173, %f171, %f172;
	st.local.f32 	[%rd3+24], %f173;

$L__BB95_23:
	bar.sync 	0;
	mov.b32 	%r615, %f7;
	shfl.sync.bfly.b32 	%r619|%p85, %r615, %r583, %r582, %r584;
	mov.b32 	%f174, %r619;
	add.f32 	%f175, %f7, %f174;
	mov.b32 	%r620, %f175;
	shfl.sync.bfly.b32 	%r622|%p86, %r620, %r587, %r582, %r584;
	mov.b32 	%f176, %r622;
	add.f32 	%f177, %f175, %f176;
	mov.b32 	%r623, %f177;
	shfl.sync.bfly.b32 	%r625|%p87, %r623, %r590, %r582, %r584;
	mov.b32 	%f178, %r625;
	add.f32 	%f179, %f177, %f178;
	mov.b32 	%r626, %f179;
	shfl.sync.bfly.b32 	%r628|%p88, %r626, %r593, %r582, %r584;
	mov.b32 	%f180, %r628;
	add.f32 	%f181, %f179, %f180;
	mov.b32 	%r629, %f181;
	shfl.sync.bfly.b32 	%r631|%p89, %r629, %r596, %r582, %r584;
	mov.b32 	%f182, %r631;
	add.f32 	%f183, %f181, %f182;
	st.local.f32 	[%rd3+28], %f183;
	st.shared.f32 	[%r63], %f183;
	bar.sync 	0;
	@%p1 bra 	$L__BB95_25;

	ld.shared.f32 	%f184, [%r4];
	mov.b32 	%r632, %f184;
	mov.u32 	%r633, 31;
	mov.u32 	%r634, 16;
	mov.u32 	%r635, -1;
	shfl.sync.bfly.b32 	%r636|%p90, %r632, %r634, %r633, %r635;
	mov.b32 	%f185, %r636;
	add.f32 	%f186, %f184, %f185;
	mov.b32 	%r637, %f186;
	mov.u32 	%r638, 8;
	shfl.sync.bfly.b32 	%r639|%p91, %r637, %r638, %r633, %r635;
	mov.b32 	%f187, %r639;
	add.f32 	%f188, %f186, %f187;
	mov.b32 	%r640, %f188;
	mov.u32 	%r641, 4;
	shfl.sync.bfly.b32 	%r642|%p92, %r640, %r641, %r633, %r635;
	mov.b32 	%f189, %r642;
	add.f32 	%f190, %f188, %f189;
	mov.b32 	%r643, %f190;
	mov.u32 	%r644, 2;
	shfl.sync.bfly.b32 	%r645|%p93, %r643, %r644, %r633, %r635;
	mov.b32 	%f191, %r645;
	add.f32 	%f192, %f190, %f191;
	mov.b32 	%r646, %f192;
	mov.u32 	%r647, 1;
	shfl.sync.bfly.b32 	%r648|%p94, %r646, %r647, %r633, %r635;
	mov.b32 	%f193, %r648;
	add.f32 	%f194, %f192, %f193;
	st.local.f32 	[%rd3+28], %f194;

$L__BB95_25:
	bar.sync 	0;
	setp.gt.s32 	%p95, %r3, 7;
	@%p95 bra 	$L__BB95_27;

	mad.lo.s32 	%r649, %r3, %r66, %r2;
	cvt.s64.s32 	%rd73, %r649;
	mul.lo.s32 	%r650, %r1, %r67;
	cvt.s64.s32 	%rd74, %r650;
	add.s64 	%rd75, %rd74, %rd73;
	mul.wide.s32 	%rd76, %r3, 4;
	add.s64 	%rd77, %rd3, %rd76;
	ld.local.f32 	%f195, [%rd77];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs3, %f195;}

	// end inline asm
	cvta.to.global.u64 	%rd78, %rd30;
	shl.b64 	%rd79, %rd75, 1;
	add.s64 	%rd80, %rd78, %rd79;
	st.global.u16 	[%rd80], %rs3;

$L__BB95_27:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_1_bs_160
.visible .entry ggml_matvec_f16_ncols_1_bs_160(
	.param .u64 ggml_matvec_f16_ncols_1_bs_160_param_0,
	.param .u64 ggml_matvec_f16_ncols_1_bs_160_param_1,
	.param .u64 ggml_matvec_f16_ncols_1_bs_160_param_2,
	.param .u32 ggml_matvec_f16_ncols_1_bs_160_param_3,
	.param .u32 ggml_matvec_f16_ncols_1_bs_160_param_4,
	.param .u32 ggml_matvec_f16_ncols_1_bs_160_param_5,
	.param .u32 ggml_matvec_f16_ncols_1_bs_160_param_6,
	.param .u32 ggml_matvec_f16_ncols_1_bs_160_param_7,
	.param .u32 ggml_matvec_f16_ncols_1_bs_160_param_8,
	.param .u32 ggml_matvec_f16_ncols_1_bs_160_param_9,
	.param .u32 ggml_matvec_f16_ncols_1_bs_160_param_10,
	.param .u32 ggml_matvec_f16_ncols_1_bs_160_param_11
)
{
	.reg .pred 	%p<19>;
	.reg .b16 	%rs<31>;
	.reg .f32 	%f<30>;
	.reg .b32 	%r<127>;
	.reg .b64 	%rd<47>;


	ld.param.u64 	%rd14, [ggml_matvec_f16_ncols_1_bs_160_param_0];
	ld.param.u64 	%rd15, [ggml_matvec_f16_ncols_1_bs_160_param_1];
	ld.param.u64 	%rd16, [ggml_matvec_f16_ncols_1_bs_160_param_2];
	ld.param.u32 	%r13, [ggml_matvec_f16_ncols_1_bs_160_param_3];
	ld.param.u32 	%r17, [ggml_matvec_f16_ncols_1_bs_160_param_5];
	ld.param.u32 	%r14, [ggml_matvec_f16_ncols_1_bs_160_param_7];
	ld.param.u32 	%r18, [ggml_matvec_f16_ncols_1_bs_160_param_8];
	ld.param.u32 	%r19, [ggml_matvec_f16_ncols_1_bs_160_param_9];
	ld.param.u32 	%r15, [ggml_matvec_f16_ncols_1_bs_160_param_10];
	ld.param.u32 	%r16, [ggml_matvec_f16_ncols_1_bs_160_param_11];
	mov.u32 	%r20, %ctaid.x;
	mov.u32 	%r21, %ctaid.y;
	div.s32 	%r22, %r21, %r18;
	mul.lo.s32 	%r23, %r20, %r17;
	mad.lo.s32 	%r24, %r22, %r19, %r23;
	cvt.s64.s32 	%rd1, %r24;
	mov.u32 	%r1, %tid.x;
	setp.gt.s32 	%p1, %r1, 31;
	shl.b32 	%r25, %r1, 2;
	mov.u32 	%r26, data_mmv;
	add.s32 	%r2, %r26, %r25;
	@%p1 bra 	$L__BB96_2;

	mov.u32 	%r27, 0;
	st.shared.u32 	[%r2], %r27;

$L__BB96_2:
	bar.sync 	0;
	mov.f32 	%f5, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs30, %f5;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs29, %f5;}

	// end inline asm
	setp.ge.s32 	%p2, %r1, %r13;
	@%p2 bra 	$L__BB96_9;

	not.b32 	%r28, %r1;
	add.s32 	%r29, %r28, %r13;
	mul.wide.u32 	%rd17, %r29, -858993459;
	shr.u64 	%rd18, %rd17, 39;
	cvt.u32.u64 	%r30, %rd18;
	add.s32 	%r31, %r30, 1;
	and.b32  	%r124, %r31, 3;
	setp.eq.s32 	%p3, %r124, 0;
	mov.u32 	%r125, %r1;
	@%p3 bra 	$L__BB96_6;

	mov.u32 	%r125, %tid.x;
	mul.wide.s32 	%rd19, %r125, 2;
	mul.lo.s32 	%r33, %r21, %r15;
	cvt.s64.s32 	%rd20, %r33;
	add.s64 	%rd21, %rd19, %rd20;
	cvta.to.global.u64 	%rd22, %rd15;
	shl.b64 	%rd23, %rd21, 1;
	add.s64 	%rd44, %rd22, %rd23;
	add.s64 	%rd24, %rd19, %rd1;
	cvta.to.global.u64 	%rd25, %rd14;
	shl.b64 	%rd26, %rd24, 1;
	add.s64 	%rd43, %rd25, %rd26;

$L__BB96_5:
	.pragma "nounroll";
	ld.global.nc.u32 	%r35, [%rd43];
	ld.global.nc.u32 	%r36, [%rd44];
	// begin inline asm
	{mul.f16x2 %r34,%r35,%r36;
}
	// end inline asm
	mov.b32 	%r38, {%rs30, %rs29};
	// begin inline asm
	{add.f16x2 %r37,%r38,%r34;
}
	// end inline asm
	mov.b32 	{%rs30, %rs29}, %r37;
	add.s32 	%r125, %r125, 160;
	add.s64 	%rd44, %rd44, 640;
	add.s64 	%rd43, %rd43, 640;
	add.s32 	%r124, %r124, -1;
	setp.ne.s32 	%p4, %r124, 0;
	@%p4 bra 	$L__BB96_5;

$L__BB96_6:
	setp.lt.u32 	%p5, %r29, 480;
	@%p5 bra 	$L__BB96_9;

	mul.wide.s32 	%rd27, %r125, 2;
	cvta.to.global.u64 	%rd28, %rd14;
	add.s64 	%rd29, %rd27, %rd1;
	shl.b64 	%rd30, %rd29, 1;
	add.s64 	%rd31, %rd28, %rd30;
	add.s64 	%rd46, %rd31, 1280;
	mul.lo.s32 	%r44, %r21, %r15;
	cvt.s64.s32 	%rd32, %r44;
	cvta.to.global.u64 	%rd33, %rd15;
	add.s64 	%rd34, %rd27, %rd32;
	shl.b64 	%rd35, %rd34, 1;
	add.s64 	%rd36, %rd33, %rd35;
	add.s64 	%rd45, %rd36, 1280;

$L__BB96_8:
	ld.global.nc.u32 	%r46, [%rd46+-1280];
	ld.global.nc.u32 	%r47, [%rd45+-1280];
	// begin inline asm
	{mul.f16x2 %r45,%r46,%r47;
}
	// end inline asm
	mov.b32 	%r49, {%rs30, %rs29};
	// begin inline asm
	{add.f16x2 %r48,%r49,%r45;
}
	// end inline asm
	ld.global.nc.u32 	%r52, [%rd46+-640];
	ld.global.nc.u32 	%r53, [%rd45+-640];
	// begin inline asm
	{mul.f16x2 %r51,%r52,%r53;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r54,%r48,%r51;
}
	// end inline asm
	ld.global.nc.u32 	%r58, [%rd46];
	ld.global.nc.u32 	%r59, [%rd45];
	// begin inline asm
	{mul.f16x2 %r57,%r58,%r59;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r60,%r54,%r57;
}
	// end inline asm
	ld.global.nc.u32 	%r64, [%rd46+640];
	ld.global.nc.u32 	%r65, [%rd45+640];
	// begin inline asm
	{mul.f16x2 %r63,%r64,%r65;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r66,%r60,%r63;
}
	// end inline asm
	mov.b32 	{%rs30, %rs29}, %r66;
	add.s64 	%rd46, %rd46, 2560;
	add.s64 	%rd45, %rd45, 2560;
	add.s32 	%r125, %r125, 640;
	setp.lt.s32 	%p6, %r125, %r13;
	@%p6 bra 	$L__BB96_8;

$L__BB96_9:
	mov.u32 	%r72, 1;
	mov.b32 	%r70, {%rs30, %rs29};
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r70;
  cvt.f32.f16 %f6, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r70;
  cvt.f32.f16 %f7, high;}

	// end inline asm
	add.f32 	%f8, %f6, %f7;
	mov.b32 	%r73, %f8;
	mov.u32 	%r74, 31;
	mov.u32 	%r75, 16;
	mov.u32 	%r76, -1;
	shfl.sync.bfly.b32 	%r77|%p8, %r73, %r75, %r74, %r76;
	mov.b32 	%f9, %r77;
	add.f32 	%f10, %f8, %f9;
	mov.b32 	%r78, %f10;
	mov.u32 	%r79, 8;
	shfl.sync.bfly.b32 	%r80|%p9, %r78, %r79, %r74, %r76;
	mov.b32 	%f11, %r80;
	add.f32 	%f12, %f10, %f11;
	mov.b32 	%r81, %f12;
	mov.u32 	%r82, 4;
	shfl.sync.bfly.b32 	%r83|%p10, %r81, %r82, %r74, %r76;
	mov.b32 	%f13, %r83;
	add.f32 	%f14, %f12, %f13;
	mov.b32 	%r84, %f14;
	mov.u32 	%r85, 2;
	shfl.sync.bfly.b32 	%r86|%p11, %r84, %r85, %r74, %r76;
	mov.b32 	%f15, %r86;
	add.f32 	%f16, %f14, %f15;
	mov.b32 	%r87, %f16;
	shfl.sync.bfly.b32 	%r88|%p12, %r87, %r72, %r74, %r76;
	mov.b32 	%f17, %r88;
	add.f32 	%f29, %f16, %f17;
	shr.s32 	%r89, %r1, 31;
	shr.u32 	%r90, %r89, 27;
	add.s32 	%r91, %r1, %r90;
	shr.s32 	%r92, %r91, 5;
	shl.b32 	%r93, %r92, 2;
	add.s32 	%r95, %r26, %r93;
	st.shared.f32 	[%r95], %f29;
	bar.sync 	0;
	@%p1 bra 	$L__BB96_11;

	ld.shared.f32 	%f18, [%r2];
	mov.b32 	%r101, %f18;
	shfl.sync.bfly.b32 	%r105|%p13, %r101, %r75, %r74, %r76;
	mov.b32 	%f19, %r105;
	add.f32 	%f20, %f18, %f19;
	mov.b32 	%r106, %f20;
	shfl.sync.bfly.b32 	%r108|%p14, %r106, %r79, %r74, %r76;
	mov.b32 	%f21, %r108;
	add.f32 	%f22, %f20, %f21;
	mov.b32 	%r109, %f22;
	shfl.sync.bfly.b32 	%r111|%p15, %r109, %r82, %r74, %r76;
	mov.b32 	%f23, %r111;
	add.f32 	%f24, %f22, %f23;
	mov.b32 	%r112, %f24;
	shfl.sync.bfly.b32 	%r113|%p16, %r112, %r85, %r74, %r76;
	mov.b32 	%f25, %r113;
	add.f32 	%f26, %f24, %f25;
	mov.b32 	%r114, %f26;
	shfl.sync.bfly.b32 	%r116|%p17, %r114, %r72, %r74, %r76;
	mov.b32 	%f27, %r116;
	add.f32 	%f29, %f26, %f27;

$L__BB96_11:
	bar.sync 	0;
	setp.gt.s32 	%p18, %r1, 0;
	@%p18 bra 	$L__BB96_13;

	mad.lo.s32 	%r120, %r1, %r14, %r20;
	cvt.s64.s32 	%rd37, %r120;
	mul.lo.s32 	%r122, %r21, %r16;
	cvt.s64.s32 	%rd38, %r122;
	add.s64 	%rd39, %rd38, %rd37;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs20, %f29;}

	// end inline asm
	cvta.to.global.u64 	%rd40, %rd16;
	shl.b64 	%rd41, %rd39, 1;
	add.s64 	%rd42, %rd40, %rd41;
	st.global.u16 	[%rd42], %rs20;

$L__BB96_13:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_2_bs_160
.visible .entry ggml_matvec_f16_ncols_2_bs_160(
	.param .u64 ggml_matvec_f16_ncols_2_bs_160_param_0,
	.param .u64 ggml_matvec_f16_ncols_2_bs_160_param_1,
	.param .u64 ggml_matvec_f16_ncols_2_bs_160_param_2,
	.param .u32 ggml_matvec_f16_ncols_2_bs_160_param_3,
	.param .u32 ggml_matvec_f16_ncols_2_bs_160_param_4,
	.param .u32 ggml_matvec_f16_ncols_2_bs_160_param_5,
	.param .u32 ggml_matvec_f16_ncols_2_bs_160_param_6,
	.param .u32 ggml_matvec_f16_ncols_2_bs_160_param_7,
	.param .u32 ggml_matvec_f16_ncols_2_bs_160_param_8,
	.param .u32 ggml_matvec_f16_ncols_2_bs_160_param_9,
	.param .u32 ggml_matvec_f16_ncols_2_bs_160_param_10,
	.param .u32 ggml_matvec_f16_ncols_2_bs_160_param_11
)
{
	.local .align 8 .b8 	__local_depot97[8];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<30>;
	.reg .b16 	%rs<4>;
	.reg .f32 	%f<52>;
	.reg .b32 	%r<201>;
	.reg .b64 	%rd<66>;


	mov.u64 	%SPL, __local_depot97;
	ld.param.u64 	%rd27, [ggml_matvec_f16_ncols_2_bs_160_param_0];
	ld.param.u64 	%rd28, [ggml_matvec_f16_ncols_2_bs_160_param_1];
	ld.param.u64 	%rd26, [ggml_matvec_f16_ncols_2_bs_160_param_2];
	ld.param.u32 	%r28, [ggml_matvec_f16_ncols_2_bs_160_param_3];
	ld.param.u32 	%r32, [ggml_matvec_f16_ncols_2_bs_160_param_5];
	ld.param.u32 	%r29, [ggml_matvec_f16_ncols_2_bs_160_param_6];
	ld.param.u32 	%r30, [ggml_matvec_f16_ncols_2_bs_160_param_7];
	ld.param.u32 	%r33, [ggml_matvec_f16_ncols_2_bs_160_param_8];
	ld.param.u32 	%r34, [ggml_matvec_f16_ncols_2_bs_160_param_9];
	ld.param.u32 	%r35, [ggml_matvec_f16_ncols_2_bs_160_param_10];
	ld.param.u32 	%r31, [ggml_matvec_f16_ncols_2_bs_160_param_11];
	cvta.to.global.u64 	%rd1, %rd28;
	cvta.to.global.u64 	%rd2, %rd27;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r36, %r1, %r33;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r37, %r2, %r32;
	mad.lo.s32 	%r38, %r36, %r34, %r37;
	cvt.s64.s32 	%rd4, %r38;
	mul.lo.s32 	%r39, %r1, %r35;
	cvt.s64.s32 	%rd5, %r39;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r40, %r3, 2;
	mov.u32 	%r41, data_mmv;
	add.s32 	%r4, %r41, %r40;
	@%p1 bra 	$L__BB97_2;

	mov.u32 	%r42, 0;
	st.shared.u32 	[%r4], %r42;

$L__BB97_2:
	bar.sync 	0;
	mov.f32 	%f3, 0f00000000;
	st.local.v2.f32 	[%rd3], {%f3, %f3};
	mov.u32 	%r199, 0;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs2, %f3;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f3;}

	// end inline asm
	mov.b32 	%r200, {%rs1, %rs2};
	setp.ge.s32 	%p2, %r3, %r28;
	@%p2 bra 	$L__BB97_9;

	not.b32 	%r45, %r3;
	add.s32 	%r6, %r45, %r28;
	mul.wide.u32 	%rd30, %r6, -858993459;
	shr.u64 	%rd31, %rd30, 39;
	cvt.u32.u64 	%r46, %rd31;
	add.s32 	%r47, %r46, 1;
	and.b32  	%r192, %r47, 3;
	setp.eq.s32 	%p3, %r192, 0;
	mov.u32 	%r199, 0;
	mov.u32 	%r195, %r3;
	@%p3 bra 	$L__BB97_6;

	mul.wide.s32 	%rd32, %r29, 2;
	mul.wide.s32 	%rd33, %r3, 2;
	add.s64 	%rd34, %rd32, %rd33;
	add.s64 	%rd35, %rd34, %rd5;
	shl.b64 	%rd36, %rd35, 1;
	add.s64 	%rd62, %rd1, %rd36;
	add.s64 	%rd37, %rd33, %rd5;
	shl.b64 	%rd38, %rd37, 1;
	add.s64 	%rd61, %rd1, %rd38;
	add.s64 	%rd39, %rd33, %rd4;
	shl.b64 	%rd40, %rd39, 1;
	add.s64 	%rd60, %rd2, %rd40;
	mov.u32 	%r199, 0;
	mov.u32 	%r195, %r3;

$L__BB97_5:
	.pragma "nounroll";
	ld.global.nc.u32 	%r50, [%rd60];
	ld.global.nc.u32 	%r51, [%rd61];
	// begin inline asm
	{mul.f16x2 %r49,%r50,%r51;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r200,%r200,%r49;
}
	// end inline asm
	ld.global.nc.u32 	%r57, [%rd62];
	// begin inline asm
	{mul.f16x2 %r55,%r50,%r57;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r199,%r199,%r55;
}
	// end inline asm
	add.s32 	%r195, %r195, 160;
	add.s64 	%rd62, %rd62, 640;
	add.s64 	%rd61, %rd61, 640;
	add.s64 	%rd60, %rd60, 640;
	add.s32 	%r192, %r192, -1;
	setp.ne.s32 	%p4, %r192, 0;
	@%p4 bra 	$L__BB97_5;

$L__BB97_6:
	setp.lt.u32 	%p5, %r6, 480;
	@%p5 bra 	$L__BB97_9;

	mul.wide.s32 	%rd41, %r195, 2;
	add.s64 	%rd42, %rd41, %rd4;
	shl.b64 	%rd43, %rd42, 1;
	add.s64 	%rd44, %rd2, %rd43;
	add.s64 	%rd65, %rd44, 1280;
	add.s64 	%rd45, %rd41, %rd5;
	shl.b64 	%rd46, %rd45, 1;
	add.s64 	%rd47, %rd1, %rd46;
	add.s64 	%rd64, %rd47, 1920;
	mul.wide.s32 	%rd48, %r29, 2;
	add.s64 	%rd49, %rd45, %rd48;
	shl.b64 	%rd50, %rd49, 1;
	add.s64 	%rd51, %rd1, %rd50;
	add.s64 	%rd63, %rd51, 1280;

$L__BB97_8:
	ld.global.nc.u32 	%r62, [%rd65+-1280];
	ld.global.nc.u32 	%r63, [%rd64+-1920];
	// begin inline asm
	{mul.f16x2 %r61,%r62,%r63;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r64,%r200,%r61;
}
	// end inline asm
	ld.global.nc.u32 	%r69, [%rd63+-1280];
	// begin inline asm
	{mul.f16x2 %r67,%r62,%r69;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r70,%r199,%r67;
}
	// end inline asm
	ld.global.nc.u32 	%r74, [%rd65+-640];
	ld.global.nc.u32 	%r75, [%rd64+-1280];
	// begin inline asm
	{mul.f16x2 %r73,%r74,%r75;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r76,%r64,%r73;
}
	// end inline asm
	ld.global.nc.u32 	%r81, [%rd63+-640];
	// begin inline asm
	{mul.f16x2 %r79,%r74,%r81;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r82,%r70,%r79;
}
	// end inline asm
	ld.global.nc.u32 	%r86, [%rd65];
	ld.global.nc.u32 	%r87, [%rd64+-640];
	// begin inline asm
	{mul.f16x2 %r85,%r86,%r87;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r88,%r76,%r85;
}
	// end inline asm
	ld.global.nc.u32 	%r93, [%rd63];
	// begin inline asm
	{mul.f16x2 %r91,%r86,%r93;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r94,%r82,%r91;
}
	// end inline asm
	ld.global.nc.u32 	%r98, [%rd65+640];
	ld.global.nc.u32 	%r99, [%rd64];
	// begin inline asm
	{mul.f16x2 %r97,%r98,%r99;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r200,%r88,%r97;
}
	// end inline asm
	ld.global.nc.u32 	%r105, [%rd63+640];
	// begin inline asm
	{mul.f16x2 %r103,%r98,%r105;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r199,%r94,%r103;
}
	// end inline asm
	add.s64 	%rd65, %rd65, 2560;
	add.s64 	%rd64, %rd64, 2560;
	add.s64 	%rd63, %rd63, 2560;
	add.s32 	%r195, %r195, 640;
	setp.lt.s32 	%p6, %r195, %r28;
	@%p6 bra 	$L__BB97_8;

$L__BB97_9:
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r200;
  cvt.f32.f16 %f4, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r200;
  cvt.f32.f16 %f5, high;}

	// end inline asm
	add.f32 	%f8, %f4, %f5;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r199;
  cvt.f32.f16 %f6, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r199;
  cvt.f32.f16 %f7, high;}

	// end inline asm
	add.f32 	%f1, %f6, %f7;
	st.local.f32 	[%rd3+4], %f1;
	shr.s32 	%r113, %r3, 31;
	shr.u32 	%r114, %r113, 27;
	add.s32 	%r115, %r3, %r114;
	shr.s32 	%r116, %r115, 5;
	shl.b32 	%r117, %r116, 2;
	add.s32 	%r27, %r41, %r117;
	mov.u32 	%r119, 2;
	mov.b32 	%r120, %f8;
	mov.u32 	%r121, 31;
	mov.u32 	%r122, 16;
	mov.u32 	%r123, -1;
	shfl.sync.bfly.b32 	%r124|%p7, %r120, %r122, %r121, %r123;
	mov.b32 	%f9, %r124;
	add.f32 	%f10, %f8, %f9;
	mov.b32 	%r125, %f10;
	mov.u32 	%r126, 8;
	shfl.sync.bfly.b32 	%r127|%p8, %r125, %r126, %r121, %r123;
	mov.b32 	%f11, %r127;
	add.f32 	%f12, %f10, %f11;
	mov.b32 	%r128, %f12;
	mov.u32 	%r129, 4;
	shfl.sync.bfly.b32 	%r130|%p9, %r128, %r129, %r121, %r123;
	mov.b32 	%f13, %r130;
	add.f32 	%f14, %f12, %f13;
	mov.b32 	%r131, %f14;
	shfl.sync.bfly.b32 	%r132|%p10, %r131, %r119, %r121, %r123;
	mov.b32 	%f15, %r132;
	add.f32 	%f16, %f14, %f15;
	mov.b32 	%r133, %f16;
	mov.u32 	%r134, 1;
	shfl.sync.bfly.b32 	%r135|%p11, %r133, %r134, %r121, %r123;
	mov.b32 	%f17, %r135;
	add.f32 	%f18, %f16, %f17;
	st.local.f32 	[%rd3], %f18;
	st.shared.f32 	[%r27], %f18;
	bar.sync 	0;
	@%p1 bra 	$L__BB97_11;

	ld.shared.f32 	%f19, [%r4];
	mov.b32 	%r136, %f19;
	shfl.sync.bfly.b32 	%r140|%p13, %r136, %r122, %r121, %r123;
	mov.b32 	%f20, %r140;
	add.f32 	%f21, %f19, %f20;
	mov.b32 	%r141, %f21;
	shfl.sync.bfly.b32 	%r143|%p14, %r141, %r126, %r121, %r123;
	mov.b32 	%f22, %r143;
	add.f32 	%f23, %f21, %f22;
	mov.b32 	%r144, %f23;
	shfl.sync.bfly.b32 	%r146|%p15, %r144, %r129, %r121, %r123;
	mov.b32 	%f24, %r146;
	add.f32 	%f25, %f23, %f24;
	mov.b32 	%r147, %f25;
	shfl.sync.bfly.b32 	%r149|%p16, %r147, %r119, %r121, %r123;
	mov.b32 	%f26, %r149;
	add.f32 	%f27, %f25, %f26;
	mov.b32 	%r150, %f27;
	shfl.sync.bfly.b32 	%r152|%p17, %r150, %r134, %r121, %r123;
	mov.b32 	%f28, %r152;
	add.f32 	%f29, %f27, %f28;
	st.local.f32 	[%rd3], %f29;

$L__BB97_11:
	bar.sync 	0;
	mov.b32 	%r153, %f1;
	shfl.sync.bfly.b32 	%r157|%p19, %r153, %r122, %r121, %r123;
	mov.b32 	%f30, %r157;
	add.f32 	%f31, %f1, %f30;
	mov.b32 	%r158, %f31;
	shfl.sync.bfly.b32 	%r160|%p20, %r158, %r126, %r121, %r123;
	mov.b32 	%f32, %r160;
	add.f32 	%f33, %f31, %f32;
	mov.b32 	%r161, %f33;
	shfl.sync.bfly.b32 	%r163|%p21, %r161, %r129, %r121, %r123;
	mov.b32 	%f34, %r163;
	add.f32 	%f35, %f33, %f34;
	mov.b32 	%r164, %f35;
	shfl.sync.bfly.b32 	%r166|%p22, %r164, %r119, %r121, %r123;
	mov.b32 	%f36, %r166;
	add.f32 	%f37, %f35, %f36;
	mov.b32 	%r167, %f37;
	shfl.sync.bfly.b32 	%r169|%p23, %r167, %r134, %r121, %r123;
	mov.b32 	%f38, %r169;
	add.f32 	%f39, %f37, %f38;
	st.local.f32 	[%rd3+4], %f39;
	st.shared.f32 	[%r27], %f39;
	bar.sync 	0;
	@%p1 bra 	$L__BB97_13;

	ld.shared.f32 	%f40, [%r4];
	mov.b32 	%r170, %f40;
	mov.u32 	%r171, 31;
	mov.u32 	%r172, 16;
	mov.u32 	%r173, -1;
	shfl.sync.bfly.b32 	%r174|%p24, %r170, %r172, %r171, %r173;
	mov.b32 	%f41, %r174;
	add.f32 	%f42, %f40, %f41;
	mov.b32 	%r175, %f42;
	mov.u32 	%r176, 8;
	shfl.sync.bfly.b32 	%r177|%p25, %r175, %r176, %r171, %r173;
	mov.b32 	%f43, %r177;
	add.f32 	%f44, %f42, %f43;
	mov.b32 	%r178, %f44;
	mov.u32 	%r179, 4;
	shfl.sync.bfly.b32 	%r180|%p26, %r178, %r179, %r171, %r173;
	mov.b32 	%f45, %r180;
	add.f32 	%f46, %f44, %f45;
	mov.b32 	%r181, %f46;
	mov.u32 	%r182, 2;
	shfl.sync.bfly.b32 	%r183|%p27, %r181, %r182, %r171, %r173;
	mov.b32 	%f47, %r183;
	add.f32 	%f48, %f46, %f47;
	mov.b32 	%r184, %f48;
	mov.u32 	%r185, 1;
	shfl.sync.bfly.b32 	%r186|%p28, %r184, %r185, %r171, %r173;
	mov.b32 	%f49, %r186;
	add.f32 	%f50, %f48, %f49;
	st.local.f32 	[%rd3+4], %f50;

$L__BB97_13:
	bar.sync 	0;
	setp.gt.s32 	%p29, %r3, 1;
	@%p29 bra 	$L__BB97_15;

	mad.lo.s32 	%r187, %r3, %r30, %r2;
	cvt.s64.s32 	%rd52, %r187;
	mul.lo.s32 	%r188, %r1, %r31;
	cvt.s64.s32 	%rd53, %r188;
	add.s64 	%rd54, %rd53, %rd52;
	mul.wide.s32 	%rd55, %r3, 4;
	add.s64 	%rd56, %rd3, %rd55;
	ld.local.f32 	%f51, [%rd56];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs3, %f51;}

	// end inline asm
	cvta.to.global.u64 	%rd57, %rd26;
	shl.b64 	%rd58, %rd54, 1;
	add.s64 	%rd59, %rd57, %rd58;
	st.global.u16 	[%rd59], %rs3;

$L__BB97_15:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_3_bs_160
.visible .entry ggml_matvec_f16_ncols_3_bs_160(
	.param .u64 ggml_matvec_f16_ncols_3_bs_160_param_0,
	.param .u64 ggml_matvec_f16_ncols_3_bs_160_param_1,
	.param .u64 ggml_matvec_f16_ncols_3_bs_160_param_2,
	.param .u32 ggml_matvec_f16_ncols_3_bs_160_param_3,
	.param .u32 ggml_matvec_f16_ncols_3_bs_160_param_4,
	.param .u32 ggml_matvec_f16_ncols_3_bs_160_param_5,
	.param .u32 ggml_matvec_f16_ncols_3_bs_160_param_6,
	.param .u32 ggml_matvec_f16_ncols_3_bs_160_param_7,
	.param .u32 ggml_matvec_f16_ncols_3_bs_160_param_8,
	.param .u32 ggml_matvec_f16_ncols_3_bs_160_param_9,
	.param .u32 ggml_matvec_f16_ncols_3_bs_160_param_10,
	.param .u32 ggml_matvec_f16_ncols_3_bs_160_param_11
)
{
	.local .align 4 .b8 	__local_depot98[12];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<41>;
	.reg .b16 	%rs<4>;
	.reg .f32 	%f<76>;
	.reg .b32 	%r<286>;
	.reg .b64 	%rd<74>;


	mov.u64 	%SPL, __local_depot98;
	ld.param.u64 	%rd29, [ggml_matvec_f16_ncols_3_bs_160_param_0];
	ld.param.u64 	%rd30, [ggml_matvec_f16_ncols_3_bs_160_param_1];
	ld.param.u64 	%rd28, [ggml_matvec_f16_ncols_3_bs_160_param_2];
	ld.param.u32 	%r34, [ggml_matvec_f16_ncols_3_bs_160_param_3];
	ld.param.u32 	%r38, [ggml_matvec_f16_ncols_3_bs_160_param_5];
	ld.param.u32 	%r35, [ggml_matvec_f16_ncols_3_bs_160_param_6];
	ld.param.u32 	%r36, [ggml_matvec_f16_ncols_3_bs_160_param_7];
	ld.param.u32 	%r39, [ggml_matvec_f16_ncols_3_bs_160_param_8];
	ld.param.u32 	%r40, [ggml_matvec_f16_ncols_3_bs_160_param_9];
	ld.param.u32 	%r41, [ggml_matvec_f16_ncols_3_bs_160_param_10];
	ld.param.u32 	%r37, [ggml_matvec_f16_ncols_3_bs_160_param_11];
	cvta.to.global.u64 	%rd73, %rd30;
	cvta.to.global.u64 	%rd2, %rd29;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r42, %r1, %r39;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r43, %r2, %r38;
	mad.lo.s32 	%r44, %r42, %r40, %r43;
	cvt.s64.s32 	%rd4, %r44;
	mul.lo.s32 	%r45, %r1, %r41;
	cvt.s64.s32 	%rd5, %r45;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r46, %r3, 2;
	mov.u32 	%r47, data_mmv;
	add.s32 	%r4, %r47, %r46;
	@%p1 bra 	$L__BB98_2;

	mov.u32 	%r48, 0;
	st.shared.u32 	[%r4], %r48;

$L__BB98_2:
	bar.sync 	0;
	mov.f32 	%f4, 0f00000000;
	mov.u32 	%r283, 0;
	st.local.u32 	[%rd3], %r283;
	st.local.u32 	[%rd3+4], %r283;
	st.local.u32 	[%rd3+8], %r283;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs2, %f4;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f4;}

	// end inline asm
	mov.b32 	%r285, {%rs1, %rs2};
	setp.ge.s32 	%p2, %r3, %r34;
	mov.u32 	%r284, %r283;
	@%p2 bra 	$L__BB98_9;

	not.b32 	%r53, %r3;
	add.s32 	%r6, %r53, %r34;
	mul.wide.u32 	%rd32, %r6, -858993459;
	shr.u64 	%rd33, %rd32, 39;
	cvt.u32.u64 	%r54, %rd33;
	add.s32 	%r55, %r54, 1;
	and.b32  	%r274, %r55, 3;
	setp.eq.s32 	%p3, %r274, 0;
	mov.u32 	%r283, 0;
	mov.u32 	%r278, %r3;
	@%p3 bra 	$L__BB98_6;

	shl.b32 	%r58, %r35, 1;
	add.s32 	%r59, %r3, %r58;
	mul.wide.s32 	%rd34, %r59, 2;
	add.s64 	%rd35, %rd34, %rd5;
	shl.b64 	%rd36, %rd35, 1;
	add.s64 	%rd71, %rd73, %rd36;
	mul.wide.s32 	%rd37, %r35, 2;
	mul.wide.s32 	%rd38, %r3, 2;
	add.s64 	%rd39, %rd37, %rd38;
	add.s64 	%rd40, %rd39, %rd5;
	shl.b64 	%rd41, %rd40, 1;
	add.s64 	%rd70, %rd73, %rd41;
	add.s64 	%rd42, %rd38, %rd5;
	shl.b64 	%rd43, %rd42, 1;
	add.s64 	%rd69, %rd73, %rd43;
	add.s64 	%rd44, %rd38, %rd4;
	shl.b64 	%rd45, %rd44, 1;
	add.s64 	%rd68, %rd2, %rd45;
	mov.u32 	%r283, 0;
	mov.u32 	%r284, %r283;
	mov.u32 	%r278, %r3;

$L__BB98_5:
	.pragma "nounroll";
	ld.global.nc.u32 	%r61, [%rd68];
	ld.global.nc.u32 	%r62, [%rd69];
	// begin inline asm
	{mul.f16x2 %r60,%r61,%r62;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r285,%r285,%r60;
}
	// end inline asm
	ld.global.nc.u32 	%r68, [%rd70];
	// begin inline asm
	{mul.f16x2 %r66,%r61,%r68;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r284,%r284,%r66;
}
	// end inline asm
	ld.global.nc.u32 	%r74, [%rd71];
	// begin inline asm
	{mul.f16x2 %r72,%r61,%r74;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r283,%r283,%r72;
}
	// end inline asm
	add.s32 	%r278, %r278, 160;
	add.s64 	%rd71, %rd71, 640;
	add.s64 	%rd70, %rd70, 640;
	add.s64 	%rd69, %rd69, 640;
	add.s64 	%rd68, %rd68, 640;
	add.s32 	%r274, %r274, -1;
	setp.ne.s32 	%p4, %r274, 0;
	@%p4 bra 	$L__BB98_5;

$L__BB98_6:
	setp.lt.u32 	%p5, %r6, 480;
	@%p5 bra 	$L__BB98_9;

	add.s32 	%r78, %r278, %r35;
	shl.b32 	%r79, %r35, 1;
	add.s32 	%r80, %r278, %r79;
	add.s32 	%r81, %r78, 160;
	mul.wide.s32 	%rd46, %r81, 4;
	shl.b64 	%rd47, %rd5, 1;
	add.s64 	%rd19, %rd46, %rd47;
	mul.wide.s32 	%rd48, %r80, 4;
	add.s64 	%rd20, %rd48, %rd47;
	mul.wide.s32 	%rd49, %r278, 2;
	add.s64 	%rd50, %rd49, %rd4;
	shl.b64 	%rd51, %rd50, 1;
	add.s64 	%rd52, %rd2, %rd51;
	add.s64 	%rd72, %rd52, 1280;
	mul.wide.s32 	%rd53, %r278, 4;
	add.s64 	%rd22, %rd53, %rd47;
	mul.wide.s32 	%rd54, %r35, 4;
	add.s64 	%rd55, %rd53, %rd54;
	add.s64 	%rd23, %rd55, %rd47;

$L__BB98_8:
	ld.global.nc.u32 	%r83, [%rd72+-1280];
	add.s64 	%rd56, %rd73, %rd22;
	ld.global.nc.u32 	%r84, [%rd56];
	// begin inline asm
	{mul.f16x2 %r82,%r83,%r84;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r85,%r285,%r82;
}
	// end inline asm
	add.s64 	%rd57, %rd73, %rd23;
	ld.global.nc.u32 	%r90, [%rd57];
	// begin inline asm
	{mul.f16x2 %r88,%r83,%r90;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r91,%r284,%r88;
}
	// end inline asm
	add.s64 	%rd58, %rd73, %rd20;
	ld.global.nc.u32 	%r96, [%rd58];
	// begin inline asm
	{mul.f16x2 %r94,%r83,%r96;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r97,%r283,%r94;
}
	// end inline asm
	ld.global.nc.u32 	%r101, [%rd72+-640];
	ld.global.nc.u32 	%r102, [%rd56+640];
	// begin inline asm
	{mul.f16x2 %r100,%r101,%r102;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r103,%r85,%r100;
}
	// end inline asm
	add.s64 	%rd59, %rd73, %rd19;
	ld.global.nc.u32 	%r108, [%rd59];
	// begin inline asm
	{mul.f16x2 %r106,%r101,%r108;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r109,%r91,%r106;
}
	// end inline asm
	ld.global.nc.u32 	%r114, [%rd58+640];
	// begin inline asm
	{mul.f16x2 %r112,%r101,%r114;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r115,%r97,%r112;
}
	// end inline asm
	ld.global.nc.u32 	%r119, [%rd72];
	ld.global.nc.u32 	%r120, [%rd56+1280];
	// begin inline asm
	{mul.f16x2 %r118,%r119,%r120;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r121,%r103,%r118;
}
	// end inline asm
	ld.global.nc.u32 	%r126, [%rd59+640];
	// begin inline asm
	{mul.f16x2 %r124,%r119,%r126;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r127,%r109,%r124;
}
	// end inline asm
	ld.global.nc.u32 	%r132, [%rd58+1280];
	// begin inline asm
	{mul.f16x2 %r130,%r119,%r132;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r133,%r115,%r130;
}
	// end inline asm
	ld.global.nc.u32 	%r137, [%rd72+640];
	ld.global.nc.u32 	%r138, [%rd56+1920];
	// begin inline asm
	{mul.f16x2 %r136,%r137,%r138;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r285,%r121,%r136;
}
	// end inline asm
	ld.global.nc.u32 	%r144, [%rd59+1280];
	// begin inline asm
	{mul.f16x2 %r142,%r137,%r144;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r284,%r127,%r142;
}
	// end inline asm
	ld.global.nc.u32 	%r150, [%rd58+1920];
	// begin inline asm
	{mul.f16x2 %r148,%r137,%r150;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r283,%r133,%r148;
}
	// end inline asm
	add.s64 	%rd73, %rd73, 2560;
	add.s64 	%rd72, %rd72, 2560;
	add.s32 	%r278, %r278, 640;
	setp.lt.s32 	%p6, %r278, %r34;
	@%p6 bra 	$L__BB98_8;

$L__BB98_9:
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r285;
  cvt.f32.f16 %f5, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r285;
  cvt.f32.f16 %f6, high;}

	// end inline asm
	add.f32 	%f11, %f5, %f6;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r284;
  cvt.f32.f16 %f7, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r284;
  cvt.f32.f16 %f8, high;}

	// end inline asm
	add.f32 	%f1, %f7, %f8;
	st.local.f32 	[%rd3+4], %f1;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r283;
  cvt.f32.f16 %f9, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r283;
  cvt.f32.f16 %f10, high;}

	// end inline asm
	add.f32 	%f2, %f9, %f10;
	st.local.f32 	[%rd3+8], %f2;
	shr.s32 	%r160, %r3, 31;
	shr.u32 	%r161, %r160, 27;
	add.s32 	%r162, %r3, %r161;
	shr.s32 	%r163, %r162, 5;
	shl.b32 	%r164, %r163, 2;
	add.s32 	%r33, %r47, %r164;
	mov.u32 	%r166, 2;
	mov.b32 	%r167, %f11;
	mov.u32 	%r168, 31;
	mov.u32 	%r169, 16;
	mov.u32 	%r170, -1;
	shfl.sync.bfly.b32 	%r171|%p7, %r167, %r169, %r168, %r170;
	mov.b32 	%f12, %r171;
	add.f32 	%f13, %f11, %f12;
	mov.b32 	%r172, %f13;
	mov.u32 	%r173, 8;
	shfl.sync.bfly.b32 	%r174|%p8, %r172, %r173, %r168, %r170;
	mov.b32 	%f14, %r174;
	add.f32 	%f15, %f13, %f14;
	mov.b32 	%r175, %f15;
	mov.u32 	%r176, 4;
	shfl.sync.bfly.b32 	%r177|%p9, %r175, %r176, %r168, %r170;
	mov.b32 	%f16, %r177;
	add.f32 	%f17, %f15, %f16;
	mov.b32 	%r178, %f17;
	shfl.sync.bfly.b32 	%r179|%p10, %r178, %r166, %r168, %r170;
	mov.b32 	%f18, %r179;
	add.f32 	%f19, %f17, %f18;
	mov.b32 	%r180, %f19;
	mov.u32 	%r181, 1;
	shfl.sync.bfly.b32 	%r182|%p11, %r180, %r181, %r168, %r170;
	mov.b32 	%f20, %r182;
	add.f32 	%f21, %f19, %f20;
	st.local.f32 	[%rd3], %f21;
	st.shared.f32 	[%r33], %f21;
	bar.sync 	0;
	@%p1 bra 	$L__BB98_11;

	ld.shared.f32 	%f22, [%r4];
	mov.b32 	%r183, %f22;
	shfl.sync.bfly.b32 	%r187|%p13, %r183, %r169, %r168, %r170;
	mov.b32 	%f23, %r187;
	add.f32 	%f24, %f22, %f23;
	mov.b32 	%r188, %f24;
	shfl.sync.bfly.b32 	%r190|%p14, %r188, %r173, %r168, %r170;
	mov.b32 	%f25, %r190;
	add.f32 	%f26, %f24, %f25;
	mov.b32 	%r191, %f26;
	shfl.sync.bfly.b32 	%r193|%p15, %r191, %r176, %r168, %r170;
	mov.b32 	%f27, %r193;
	add.f32 	%f28, %f26, %f27;
	mov.b32 	%r194, %f28;
	shfl.sync.bfly.b32 	%r196|%p16, %r194, %r166, %r168, %r170;
	mov.b32 	%f29, %r196;
	add.f32 	%f30, %f28, %f29;
	mov.b32 	%r197, %f30;
	shfl.sync.bfly.b32 	%r199|%p17, %r197, %r181, %r168, %r170;
	mov.b32 	%f31, %r199;
	add.f32 	%f32, %f30, %f31;
	st.local.f32 	[%rd3], %f32;

$L__BB98_11:
	bar.sync 	0;
	mov.b32 	%r200, %f1;
	shfl.sync.bfly.b32 	%r204|%p19, %r200, %r169, %r168, %r170;
	mov.b32 	%f33, %r204;
	add.f32 	%f34, %f1, %f33;
	mov.b32 	%r205, %f34;
	shfl.sync.bfly.b32 	%r207|%p20, %r205, %r173, %r168, %r170;
	mov.b32 	%f35, %r207;
	add.f32 	%f36, %f34, %f35;
	mov.b32 	%r208, %f36;
	shfl.sync.bfly.b32 	%r210|%p21, %r208, %r176, %r168, %r170;
	mov.b32 	%f37, %r210;
	add.f32 	%f38, %f36, %f37;
	mov.b32 	%r211, %f38;
	shfl.sync.bfly.b32 	%r213|%p22, %r211, %r166, %r168, %r170;
	mov.b32 	%f39, %r213;
	add.f32 	%f40, %f38, %f39;
	mov.b32 	%r214, %f40;
	shfl.sync.bfly.b32 	%r216|%p23, %r214, %r181, %r168, %r170;
	mov.b32 	%f41, %r216;
	add.f32 	%f42, %f40, %f41;
	st.local.f32 	[%rd3+4], %f42;
	st.shared.f32 	[%r33], %f42;
	bar.sync 	0;
	@%p1 bra 	$L__BB98_13;

	ld.shared.f32 	%f43, [%r4];
	mov.b32 	%r217, %f43;
	mov.u32 	%r218, 31;
	mov.u32 	%r219, 16;
	mov.u32 	%r220, -1;
	shfl.sync.bfly.b32 	%r221|%p24, %r217, %r219, %r218, %r220;
	mov.b32 	%f44, %r221;
	add.f32 	%f45, %f43, %f44;
	mov.b32 	%r222, %f45;
	mov.u32 	%r223, 8;
	shfl.sync.bfly.b32 	%r224|%p25, %r222, %r223, %r218, %r220;
	mov.b32 	%f46, %r224;
	add.f32 	%f47, %f45, %f46;
	mov.b32 	%r225, %f47;
	mov.u32 	%r226, 4;
	shfl.sync.bfly.b32 	%r227|%p26, %r225, %r226, %r218, %r220;
	mov.b32 	%f48, %r227;
	add.f32 	%f49, %f47, %f48;
	mov.b32 	%r228, %f49;
	mov.u32 	%r229, 2;
	shfl.sync.bfly.b32 	%r230|%p27, %r228, %r229, %r218, %r220;
	mov.b32 	%f50, %r230;
	add.f32 	%f51, %f49, %f50;
	mov.b32 	%r231, %f51;
	mov.u32 	%r232, 1;
	shfl.sync.bfly.b32 	%r233|%p28, %r231, %r232, %r218, %r220;
	mov.b32 	%f52, %r233;
	add.f32 	%f53, %f51, %f52;
	st.local.f32 	[%rd3+4], %f53;

$L__BB98_13:
	bar.sync 	0;
	mov.b32 	%r234, %f2;
	mov.u32 	%r235, 31;
	mov.u32 	%r236, 16;
	mov.u32 	%r237, -1;
	shfl.sync.bfly.b32 	%r238|%p30, %r234, %r236, %r235, %r237;
	mov.b32 	%f54, %r238;
	add.f32 	%f55, %f2, %f54;
	mov.b32 	%r239, %f55;
	mov.u32 	%r240, 8;
	shfl.sync.bfly.b32 	%r241|%p31, %r239, %r240, %r235, %r237;
	mov.b32 	%f56, %r241;
	add.f32 	%f57, %f55, %f56;
	mov.b32 	%r242, %f57;
	mov.u32 	%r243, 4;
	shfl.sync.bfly.b32 	%r244|%p32, %r242, %r243, %r235, %r237;
	mov.b32 	%f58, %r244;
	add.f32 	%f59, %f57, %f58;
	mov.b32 	%r245, %f59;
	mov.u32 	%r246, 2;
	shfl.sync.bfly.b32 	%r247|%p33, %r245, %r246, %r235, %r237;
	mov.b32 	%f60, %r247;
	add.f32 	%f61, %f59, %f60;
	mov.b32 	%r248, %f61;
	mov.u32 	%r249, 1;
	shfl.sync.bfly.b32 	%r250|%p34, %r248, %r249, %r235, %r237;
	mov.b32 	%f62, %r250;
	add.f32 	%f63, %f61, %f62;
	st.local.f32 	[%rd3+8], %f63;
	st.shared.f32 	[%r33], %f63;
	bar.sync 	0;
	@%p1 bra 	$L__BB98_15;

	ld.shared.f32 	%f64, [%r4];
	mov.b32 	%r251, %f64;
	shfl.sync.bfly.b32 	%r255|%p35, %r251, %r236, %r235, %r237;
	mov.b32 	%f65, %r255;
	add.f32 	%f66, %f64, %f65;
	mov.b32 	%r256, %f66;
	shfl.sync.bfly.b32 	%r258|%p36, %r256, %r240, %r235, %r237;
	mov.b32 	%f67, %r258;
	add.f32 	%f68, %f66, %f67;
	mov.b32 	%r259, %f68;
	shfl.sync.bfly.b32 	%r261|%p37, %r259, %r243, %r235, %r237;
	mov.b32 	%f69, %r261;
	add.f32 	%f70, %f68, %f69;
	mov.b32 	%r262, %f70;
	shfl.sync.bfly.b32 	%r264|%p38, %r262, %r246, %r235, %r237;
	mov.b32 	%f71, %r264;
	add.f32 	%f72, %f70, %f71;
	mov.b32 	%r265, %f72;
	shfl.sync.bfly.b32 	%r267|%p39, %r265, %r249, %r235, %r237;
	mov.b32 	%f73, %r267;
	add.f32 	%f74, %f72, %f73;
	st.local.f32 	[%rd3+8], %f74;

$L__BB98_15:
	bar.sync 	0;
	setp.gt.s32 	%p40, %r3, 2;
	@%p40 bra 	$L__BB98_17;

	mad.lo.s32 	%r268, %r3, %r36, %r2;
	cvt.s64.s32 	%rd60, %r268;
	mul.lo.s32 	%r269, %r1, %r37;
	cvt.s64.s32 	%rd61, %r269;
	add.s64 	%rd62, %rd61, %rd60;
	mul.wide.s32 	%rd63, %r3, 4;
	add.s64 	%rd64, %rd3, %rd63;
	ld.local.f32 	%f75, [%rd64];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs3, %f75;}

	// end inline asm
	cvta.to.global.u64 	%rd65, %rd28;
	shl.b64 	%rd66, %rd62, 1;
	add.s64 	%rd67, %rd65, %rd66;
	st.global.u16 	[%rd67], %rs3;

$L__BB98_17:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_4_bs_160
.visible .entry ggml_matvec_f16_ncols_4_bs_160(
	.param .u64 ggml_matvec_f16_ncols_4_bs_160_param_0,
	.param .u64 ggml_matvec_f16_ncols_4_bs_160_param_1,
	.param .u64 ggml_matvec_f16_ncols_4_bs_160_param_2,
	.param .u32 ggml_matvec_f16_ncols_4_bs_160_param_3,
	.param .u32 ggml_matvec_f16_ncols_4_bs_160_param_4,
	.param .u32 ggml_matvec_f16_ncols_4_bs_160_param_5,
	.param .u32 ggml_matvec_f16_ncols_4_bs_160_param_6,
	.param .u32 ggml_matvec_f16_ncols_4_bs_160_param_7,
	.param .u32 ggml_matvec_f16_ncols_4_bs_160_param_8,
	.param .u32 ggml_matvec_f16_ncols_4_bs_160_param_9,
	.param .u32 ggml_matvec_f16_ncols_4_bs_160_param_10,
	.param .u32 ggml_matvec_f16_ncols_4_bs_160_param_11
)
{
	.local .align 16 .b8 	__local_depot99[16];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<52>;
	.reg .b16 	%rs<4>;
	.reg .f32 	%f<100>;
	.reg .b32 	%r<367>;
	.reg .b64 	%rd<85>;


	mov.u64 	%SPL, __local_depot99;
	ld.param.u64 	%rd34, [ggml_matvec_f16_ncols_4_bs_160_param_0];
	ld.param.u64 	%rd35, [ggml_matvec_f16_ncols_4_bs_160_param_1];
	ld.param.u64 	%rd33, [ggml_matvec_f16_ncols_4_bs_160_param_2];
	ld.param.u32 	%r40, [ggml_matvec_f16_ncols_4_bs_160_param_3];
	ld.param.u32 	%r44, [ggml_matvec_f16_ncols_4_bs_160_param_5];
	ld.param.u32 	%r41, [ggml_matvec_f16_ncols_4_bs_160_param_6];
	ld.param.u32 	%r42, [ggml_matvec_f16_ncols_4_bs_160_param_7];
	ld.param.u32 	%r45, [ggml_matvec_f16_ncols_4_bs_160_param_8];
	ld.param.u32 	%r46, [ggml_matvec_f16_ncols_4_bs_160_param_9];
	ld.param.u32 	%r47, [ggml_matvec_f16_ncols_4_bs_160_param_10];
	ld.param.u32 	%r43, [ggml_matvec_f16_ncols_4_bs_160_param_11];
	cvta.to.global.u64 	%rd84, %rd35;
	cvta.to.global.u64 	%rd2, %rd34;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r48, %r1, %r45;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r49, %r2, %r44;
	mad.lo.s32 	%r50, %r48, %r46, %r49;
	cvt.s64.s32 	%rd4, %r50;
	mul.lo.s32 	%r51, %r1, %r47;
	cvt.s64.s32 	%rd5, %r51;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r52, %r3, 2;
	mov.u32 	%r53, data_mmv;
	add.s32 	%r4, %r53, %r52;
	@%p1 bra 	$L__BB99_2;

	mov.u32 	%r54, 0;
	st.shared.u32 	[%r4], %r54;

$L__BB99_2:
	bar.sync 	0;
	mov.f32 	%f5, 0f00000000;
	st.local.v4.f32 	[%rd3], {%f5, %f5, %f5, %f5};
	mov.u32 	%r363, 0;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs2, %f5;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f5;}

	// end inline asm
	mov.b32 	%r366, {%rs1, %rs2};
	setp.ge.s32 	%p2, %r3, %r40;
	mov.u32 	%r364, %r363;
	mov.u32 	%r365, %r363;
	@%p2 bra 	$L__BB99_9;

	not.b32 	%r61, %r3;
	add.s32 	%r6, %r61, %r40;
	mul.wide.u32 	%rd37, %r6, -858993459;
	shr.u64 	%rd38, %rd37, 39;
	cvt.u32.u64 	%r62, %rd38;
	add.s32 	%r63, %r62, 1;
	and.b32  	%r352, %r63, 3;
	setp.eq.s32 	%p3, %r352, 0;
	mov.u32 	%r363, 0;
	mov.u32 	%r357, %r3;
	@%p3 bra 	$L__BB99_6;

	shl.b32 	%r67, %r41, 1;
	add.s32 	%r68, %r3, %r67;
	mul.wide.s32 	%rd39, %r68, 2;
	add.s64 	%rd40, %rd39, %rd5;
	shl.b64 	%rd41, %rd40, 1;
	add.s64 	%rd82, %rd84, %rd41;
	mad.lo.s32 	%r69, %r41, 3, %r3;
	mul.wide.s32 	%rd42, %r69, 2;
	add.s64 	%rd43, %rd42, %rd5;
	shl.b64 	%rd44, %rd43, 1;
	add.s64 	%rd81, %rd84, %rd44;
	mul.wide.s32 	%rd45, %r41, 2;
	mul.wide.s32 	%rd46, %r3, 2;
	add.s64 	%rd47, %rd45, %rd46;
	add.s64 	%rd48, %rd47, %rd5;
	shl.b64 	%rd49, %rd48, 1;
	add.s64 	%rd80, %rd84, %rd49;
	add.s64 	%rd50, %rd46, %rd5;
	shl.b64 	%rd51, %rd50, 1;
	add.s64 	%rd79, %rd84, %rd51;
	add.s64 	%rd52, %rd46, %rd4;
	shl.b64 	%rd53, %rd52, 1;
	add.s64 	%rd78, %rd2, %rd53;
	mov.u32 	%r363, 0;
	mov.u32 	%r364, %r363;
	mov.u32 	%r365, %r363;
	mov.u32 	%r357, %r3;

$L__BB99_5:
	.pragma "nounroll";
	ld.global.nc.u32 	%r71, [%rd78];
	ld.global.nc.u32 	%r72, [%rd79];
	// begin inline asm
	{mul.f16x2 %r70,%r71,%r72;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r366,%r366,%r70;
}
	// end inline asm
	ld.global.nc.u32 	%r78, [%rd80];
	// begin inline asm
	{mul.f16x2 %r76,%r71,%r78;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r365,%r365,%r76;
}
	// end inline asm
	ld.global.nc.u32 	%r84, [%rd82];
	// begin inline asm
	{mul.f16x2 %r82,%r71,%r84;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r364,%r364,%r82;
}
	// end inline asm
	ld.global.nc.u32 	%r90, [%rd81];
	// begin inline asm
	{mul.f16x2 %r88,%r71,%r90;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r363,%r363,%r88;
}
	// end inline asm
	add.s32 	%r357, %r357, 160;
	add.s64 	%rd82, %rd82, 640;
	add.s64 	%rd81, %rd81, 640;
	add.s64 	%rd80, %rd80, 640;
	add.s64 	%rd79, %rd79, 640;
	add.s64 	%rd78, %rd78, 640;
	add.s32 	%r352, %r352, -1;
	setp.ne.s32 	%p4, %r352, 0;
	@%p4 bra 	$L__BB99_5;

$L__BB99_6:
	setp.lt.u32 	%p5, %r6, 480;
	@%p5 bra 	$L__BB99_9;

	add.s32 	%r94, %r357, %r41;
	shl.b32 	%r95, %r41, 1;
	add.s32 	%r96, %r357, %r95;
	mad.lo.s32 	%r97, %r41, 3, %r357;
	add.s32 	%r98, %r94, 160;
	mul.wide.s32 	%rd54, %r98, 4;
	shl.b64 	%rd55, %rd5, 1;
	add.s64 	%rd23, %rd54, %rd55;
	mul.wide.s32 	%rd56, %r96, 4;
	add.s64 	%rd24, %rd56, %rd55;
	mul.wide.s32 	%rd57, %r97, 4;
	add.s64 	%rd25, %rd57, %rd55;
	mul.wide.s32 	%rd58, %r357, 2;
	add.s64 	%rd59, %rd58, %rd4;
	shl.b64 	%rd60, %rd59, 1;
	add.s64 	%rd61, %rd2, %rd60;
	add.s64 	%rd83, %rd61, 1280;
	mul.wide.s32 	%rd62, %r357, 4;
	add.s64 	%rd27, %rd62, %rd55;
	mul.wide.s32 	%rd63, %r41, 4;
	add.s64 	%rd64, %rd62, %rd63;
	add.s64 	%rd28, %rd64, %rd55;

$L__BB99_8:
	ld.global.nc.u32 	%r100, [%rd83+-1280];
	add.s64 	%rd65, %rd84, %rd27;
	ld.global.nc.u32 	%r101, [%rd65];
	// begin inline asm
	{mul.f16x2 %r99,%r100,%r101;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r102,%r366,%r99;
}
	// end inline asm
	add.s64 	%rd66, %rd84, %rd28;
	ld.global.nc.u32 	%r107, [%rd66];
	// begin inline asm
	{mul.f16x2 %r105,%r100,%r107;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r108,%r365,%r105;
}
	// end inline asm
	add.s64 	%rd67, %rd84, %rd24;
	ld.global.nc.u32 	%r113, [%rd67];
	// begin inline asm
	{mul.f16x2 %r111,%r100,%r113;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r114,%r364,%r111;
}
	// end inline asm
	add.s64 	%rd68, %rd84, %rd25;
	ld.global.nc.u32 	%r119, [%rd68];
	// begin inline asm
	{mul.f16x2 %r117,%r100,%r119;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r120,%r363,%r117;
}
	// end inline asm
	ld.global.nc.u32 	%r124, [%rd83+-640];
	ld.global.nc.u32 	%r125, [%rd65+640];
	// begin inline asm
	{mul.f16x2 %r123,%r124,%r125;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r126,%r102,%r123;
}
	// end inline asm
	add.s64 	%rd69, %rd84, %rd23;
	ld.global.nc.u32 	%r131, [%rd69];
	// begin inline asm
	{mul.f16x2 %r129,%r124,%r131;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r132,%r108,%r129;
}
	// end inline asm
	ld.global.nc.u32 	%r137, [%rd67+640];
	// begin inline asm
	{mul.f16x2 %r135,%r124,%r137;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r138,%r114,%r135;
}
	// end inline asm
	ld.global.nc.u32 	%r143, [%rd68+640];
	// begin inline asm
	{mul.f16x2 %r141,%r124,%r143;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r144,%r120,%r141;
}
	// end inline asm
	ld.global.nc.u32 	%r148, [%rd83];
	ld.global.nc.u32 	%r149, [%rd65+1280];
	// begin inline asm
	{mul.f16x2 %r147,%r148,%r149;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r150,%r126,%r147;
}
	// end inline asm
	ld.global.nc.u32 	%r155, [%rd69+640];
	// begin inline asm
	{mul.f16x2 %r153,%r148,%r155;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r156,%r132,%r153;
}
	// end inline asm
	ld.global.nc.u32 	%r161, [%rd67+1280];
	// begin inline asm
	{mul.f16x2 %r159,%r148,%r161;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r162,%r138,%r159;
}
	// end inline asm
	ld.global.nc.u32 	%r167, [%rd68+1280];
	// begin inline asm
	{mul.f16x2 %r165,%r148,%r167;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r168,%r144,%r165;
}
	// end inline asm
	ld.global.nc.u32 	%r172, [%rd83+640];
	ld.global.nc.u32 	%r173, [%rd65+1920];
	// begin inline asm
	{mul.f16x2 %r171,%r172,%r173;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r366,%r150,%r171;
}
	// end inline asm
	ld.global.nc.u32 	%r179, [%rd69+1280];
	// begin inline asm
	{mul.f16x2 %r177,%r172,%r179;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r365,%r156,%r177;
}
	// end inline asm
	ld.global.nc.u32 	%r185, [%rd67+1920];
	// begin inline asm
	{mul.f16x2 %r183,%r172,%r185;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r364,%r162,%r183;
}
	// end inline asm
	ld.global.nc.u32 	%r191, [%rd68+1920];
	// begin inline asm
	{mul.f16x2 %r189,%r172,%r191;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r363,%r168,%r189;
}
	// end inline asm
	add.s64 	%rd84, %rd84, 2560;
	add.s64 	%rd83, %rd83, 2560;
	add.s32 	%r357, %r357, 640;
	setp.lt.s32 	%p6, %r357, %r40;
	@%p6 bra 	$L__BB99_8;

$L__BB99_9:
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r366;
  cvt.f32.f16 %f6, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r366;
  cvt.f32.f16 %f7, high;}

	// end inline asm
	add.f32 	%f14, %f6, %f7;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r365;
  cvt.f32.f16 %f8, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r365;
  cvt.f32.f16 %f9, high;}

	// end inline asm
	add.f32 	%f1, %f8, %f9;
	st.local.f32 	[%rd3+4], %f1;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r364;
  cvt.f32.f16 %f10, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r364;
  cvt.f32.f16 %f11, high;}

	// end inline asm
	add.f32 	%f2, %f10, %f11;
	st.local.f32 	[%rd3+8], %f2;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r363;
  cvt.f32.f16 %f12, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r363;
  cvt.f32.f16 %f13, high;}

	// end inline asm
	add.f32 	%f3, %f12, %f13;
	st.local.f32 	[%rd3+12], %f3;
	shr.s32 	%r203, %r3, 31;
	shr.u32 	%r204, %r203, 27;
	add.s32 	%r205, %r3, %r204;
	shr.s32 	%r206, %r205, 5;
	shl.b32 	%r207, %r206, 2;
	add.s32 	%r39, %r53, %r207;
	mov.u32 	%r209, 2;
	mov.b32 	%r210, %f14;
	mov.u32 	%r211, 31;
	mov.u32 	%r212, 16;
	mov.u32 	%r213, -1;
	shfl.sync.bfly.b32 	%r214|%p7, %r210, %r212, %r211, %r213;
	mov.b32 	%f15, %r214;
	add.f32 	%f16, %f14, %f15;
	mov.b32 	%r215, %f16;
	mov.u32 	%r216, 8;
	shfl.sync.bfly.b32 	%r217|%p8, %r215, %r216, %r211, %r213;
	mov.b32 	%f17, %r217;
	add.f32 	%f18, %f16, %f17;
	mov.b32 	%r218, %f18;
	mov.u32 	%r219, 4;
	shfl.sync.bfly.b32 	%r220|%p9, %r218, %r219, %r211, %r213;
	mov.b32 	%f19, %r220;
	add.f32 	%f20, %f18, %f19;
	mov.b32 	%r221, %f20;
	shfl.sync.bfly.b32 	%r222|%p10, %r221, %r209, %r211, %r213;
	mov.b32 	%f21, %r222;
	add.f32 	%f22, %f20, %f21;
	mov.b32 	%r223, %f22;
	mov.u32 	%r224, 1;
	shfl.sync.bfly.b32 	%r225|%p11, %r223, %r224, %r211, %r213;
	mov.b32 	%f23, %r225;
	add.f32 	%f24, %f22, %f23;
	st.local.f32 	[%rd3], %f24;
	st.shared.f32 	[%r39], %f24;
	bar.sync 	0;
	@%p1 bra 	$L__BB99_11;

	ld.shared.f32 	%f25, [%r4];
	mov.b32 	%r226, %f25;
	shfl.sync.bfly.b32 	%r230|%p13, %r226, %r212, %r211, %r213;
	mov.b32 	%f26, %r230;
	add.f32 	%f27, %f25, %f26;
	mov.b32 	%r231, %f27;
	shfl.sync.bfly.b32 	%r233|%p14, %r231, %r216, %r211, %r213;
	mov.b32 	%f28, %r233;
	add.f32 	%f29, %f27, %f28;
	mov.b32 	%r234, %f29;
	shfl.sync.bfly.b32 	%r236|%p15, %r234, %r219, %r211, %r213;
	mov.b32 	%f30, %r236;
	add.f32 	%f31, %f29, %f30;
	mov.b32 	%r237, %f31;
	shfl.sync.bfly.b32 	%r239|%p16, %r237, %r209, %r211, %r213;
	mov.b32 	%f32, %r239;
	add.f32 	%f33, %f31, %f32;
	mov.b32 	%r240, %f33;
	shfl.sync.bfly.b32 	%r242|%p17, %r240, %r224, %r211, %r213;
	mov.b32 	%f34, %r242;
	add.f32 	%f35, %f33, %f34;
	st.local.f32 	[%rd3], %f35;

$L__BB99_11:
	bar.sync 	0;
	mov.b32 	%r243, %f1;
	shfl.sync.bfly.b32 	%r247|%p19, %r243, %r212, %r211, %r213;
	mov.b32 	%f36, %r247;
	add.f32 	%f37, %f1, %f36;
	mov.b32 	%r248, %f37;
	shfl.sync.bfly.b32 	%r250|%p20, %r248, %r216, %r211, %r213;
	mov.b32 	%f38, %r250;
	add.f32 	%f39, %f37, %f38;
	mov.b32 	%r251, %f39;
	shfl.sync.bfly.b32 	%r253|%p21, %r251, %r219, %r211, %r213;
	mov.b32 	%f40, %r253;
	add.f32 	%f41, %f39, %f40;
	mov.b32 	%r254, %f41;
	shfl.sync.bfly.b32 	%r256|%p22, %r254, %r209, %r211, %r213;
	mov.b32 	%f42, %r256;
	add.f32 	%f43, %f41, %f42;
	mov.b32 	%r257, %f43;
	shfl.sync.bfly.b32 	%r259|%p23, %r257, %r224, %r211, %r213;
	mov.b32 	%f44, %r259;
	add.f32 	%f45, %f43, %f44;
	st.local.f32 	[%rd3+4], %f45;
	st.shared.f32 	[%r39], %f45;
	bar.sync 	0;
	@%p1 bra 	$L__BB99_13;

	ld.shared.f32 	%f46, [%r4];
	mov.b32 	%r260, %f46;
	mov.u32 	%r261, 31;
	mov.u32 	%r262, 16;
	mov.u32 	%r263, -1;
	shfl.sync.bfly.b32 	%r264|%p24, %r260, %r262, %r261, %r263;
	mov.b32 	%f47, %r264;
	add.f32 	%f48, %f46, %f47;
	mov.b32 	%r265, %f48;
	mov.u32 	%r266, 8;
	shfl.sync.bfly.b32 	%r267|%p25, %r265, %r266, %r261, %r263;
	mov.b32 	%f49, %r267;
	add.f32 	%f50, %f48, %f49;
	mov.b32 	%r268, %f50;
	mov.u32 	%r269, 4;
	shfl.sync.bfly.b32 	%r270|%p26, %r268, %r269, %r261, %r263;
	mov.b32 	%f51, %r270;
	add.f32 	%f52, %f50, %f51;
	mov.b32 	%r271, %f52;
	mov.u32 	%r272, 2;
	shfl.sync.bfly.b32 	%r273|%p27, %r271, %r272, %r261, %r263;
	mov.b32 	%f53, %r273;
	add.f32 	%f54, %f52, %f53;
	mov.b32 	%r274, %f54;
	mov.u32 	%r275, 1;
	shfl.sync.bfly.b32 	%r276|%p28, %r274, %r275, %r261, %r263;
	mov.b32 	%f55, %r276;
	add.f32 	%f56, %f54, %f55;
	st.local.f32 	[%rd3+4], %f56;

$L__BB99_13:
	bar.sync 	0;
	mov.b32 	%r277, %f2;
	mov.u32 	%r278, 31;
	mov.u32 	%r279, 16;
	mov.u32 	%r280, -1;
	shfl.sync.bfly.b32 	%r281|%p30, %r277, %r279, %r278, %r280;
	mov.b32 	%f57, %r281;
	add.f32 	%f58, %f2, %f57;
	mov.b32 	%r282, %f58;
	mov.u32 	%r283, 8;
	shfl.sync.bfly.b32 	%r284|%p31, %r282, %r283, %r278, %r280;
	mov.b32 	%f59, %r284;
	add.f32 	%f60, %f58, %f59;
	mov.b32 	%r285, %f60;
	mov.u32 	%r286, 4;
	shfl.sync.bfly.b32 	%r287|%p32, %r285, %r286, %r278, %r280;
	mov.b32 	%f61, %r287;
	add.f32 	%f62, %f60, %f61;
	mov.b32 	%r288, %f62;
	mov.u32 	%r289, 2;
	shfl.sync.bfly.b32 	%r290|%p33, %r288, %r289, %r278, %r280;
	mov.b32 	%f63, %r290;
	add.f32 	%f64, %f62, %f63;
	mov.b32 	%r291, %f64;
	mov.u32 	%r292, 1;
	shfl.sync.bfly.b32 	%r293|%p34, %r291, %r292, %r278, %r280;
	mov.b32 	%f65, %r293;
	add.f32 	%f66, %f64, %f65;
	st.local.f32 	[%rd3+8], %f66;
	st.shared.f32 	[%r39], %f66;
	bar.sync 	0;
	@%p1 bra 	$L__BB99_15;

	ld.shared.f32 	%f67, [%r4];
	mov.b32 	%r294, %f67;
	shfl.sync.bfly.b32 	%r298|%p35, %r294, %r279, %r278, %r280;
	mov.b32 	%f68, %r298;
	add.f32 	%f69, %f67, %f68;
	mov.b32 	%r299, %f69;
	shfl.sync.bfly.b32 	%r301|%p36, %r299, %r283, %r278, %r280;
	mov.b32 	%f70, %r301;
	add.f32 	%f71, %f69, %f70;
	mov.b32 	%r302, %f71;
	shfl.sync.bfly.b32 	%r304|%p37, %r302, %r286, %r278, %r280;
	mov.b32 	%f72, %r304;
	add.f32 	%f73, %f71, %f72;
	mov.b32 	%r305, %f73;
	shfl.sync.bfly.b32 	%r307|%p38, %r305, %r289, %r278, %r280;
	mov.b32 	%f74, %r307;
	add.f32 	%f75, %f73, %f74;
	mov.b32 	%r308, %f75;
	shfl.sync.bfly.b32 	%r310|%p39, %r308, %r292, %r278, %r280;
	mov.b32 	%f76, %r310;
	add.f32 	%f77, %f75, %f76;
	st.local.f32 	[%rd3+8], %f77;

$L__BB99_15:
	bar.sync 	0;
	mov.b32 	%r311, %f3;
	shfl.sync.bfly.b32 	%r315|%p41, %r311, %r279, %r278, %r280;
	mov.b32 	%f78, %r315;
	add.f32 	%f79, %f3, %f78;
	mov.b32 	%r316, %f79;
	shfl.sync.bfly.b32 	%r318|%p42, %r316, %r283, %r278, %r280;
	mov.b32 	%f80, %r318;
	add.f32 	%f81, %f79, %f80;
	mov.b32 	%r319, %f81;
	shfl.sync.bfly.b32 	%r321|%p43, %r319, %r286, %r278, %r280;
	mov.b32 	%f82, %r321;
	add.f32 	%f83, %f81, %f82;
	mov.b32 	%r322, %f83;
	shfl.sync.bfly.b32 	%r324|%p44, %r322, %r289, %r278, %r280;
	mov.b32 	%f84, %r324;
	add.f32 	%f85, %f83, %f84;
	mov.b32 	%r325, %f85;
	shfl.sync.bfly.b32 	%r327|%p45, %r325, %r292, %r278, %r280;
	mov.b32 	%f86, %r327;
	add.f32 	%f87, %f85, %f86;
	st.local.f32 	[%rd3+12], %f87;
	st.shared.f32 	[%r39], %f87;
	bar.sync 	0;
	@%p1 bra 	$L__BB99_17;

	ld.shared.f32 	%f88, [%r4];
	mov.b32 	%r328, %f88;
	mov.u32 	%r329, 31;
	mov.u32 	%r330, 16;
	mov.u32 	%r331, -1;
	shfl.sync.bfly.b32 	%r332|%p46, %r328, %r330, %r329, %r331;
	mov.b32 	%f89, %r332;
	add.f32 	%f90, %f88, %f89;
	mov.b32 	%r333, %f90;
	mov.u32 	%r334, 8;
	shfl.sync.bfly.b32 	%r335|%p47, %r333, %r334, %r329, %r331;
	mov.b32 	%f91, %r335;
	add.f32 	%f92, %f90, %f91;
	mov.b32 	%r336, %f92;
	mov.u32 	%r337, 4;
	shfl.sync.bfly.b32 	%r338|%p48, %r336, %r337, %r329, %r331;
	mov.b32 	%f93, %r338;
	add.f32 	%f94, %f92, %f93;
	mov.b32 	%r339, %f94;
	mov.u32 	%r340, 2;
	shfl.sync.bfly.b32 	%r341|%p49, %r339, %r340, %r329, %r331;
	mov.b32 	%f95, %r341;
	add.f32 	%f96, %f94, %f95;
	mov.b32 	%r342, %f96;
	mov.u32 	%r343, 1;
	shfl.sync.bfly.b32 	%r344|%p50, %r342, %r343, %r329, %r331;
	mov.b32 	%f97, %r344;
	add.f32 	%f98, %f96, %f97;
	st.local.f32 	[%rd3+12], %f98;

$L__BB99_17:
	bar.sync 	0;
	setp.gt.s32 	%p51, %r3, 3;
	@%p51 bra 	$L__BB99_19;

	mad.lo.s32 	%r345, %r3, %r42, %r2;
	cvt.s64.s32 	%rd70, %r345;
	mul.lo.s32 	%r346, %r1, %r43;
	cvt.s64.s32 	%rd71, %r346;
	add.s64 	%rd72, %rd71, %rd70;
	mul.wide.s32 	%rd73, %r3, 4;
	add.s64 	%rd74, %rd3, %rd73;
	ld.local.f32 	%f99, [%rd74];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs3, %f99;}

	// end inline asm
	cvta.to.global.u64 	%rd75, %rd33;
	shl.b64 	%rd76, %rd72, 1;
	add.s64 	%rd77, %rd75, %rd76;
	st.global.u16 	[%rd77], %rs3;

$L__BB99_19:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_5_bs_160
.visible .entry ggml_matvec_f16_ncols_5_bs_160(
	.param .u64 ggml_matvec_f16_ncols_5_bs_160_param_0,
	.param .u64 ggml_matvec_f16_ncols_5_bs_160_param_1,
	.param .u64 ggml_matvec_f16_ncols_5_bs_160_param_2,
	.param .u32 ggml_matvec_f16_ncols_5_bs_160_param_3,
	.param .u32 ggml_matvec_f16_ncols_5_bs_160_param_4,
	.param .u32 ggml_matvec_f16_ncols_5_bs_160_param_5,
	.param .u32 ggml_matvec_f16_ncols_5_bs_160_param_6,
	.param .u32 ggml_matvec_f16_ncols_5_bs_160_param_7,
	.param .u32 ggml_matvec_f16_ncols_5_bs_160_param_8,
	.param .u32 ggml_matvec_f16_ncols_5_bs_160_param_9,
	.param .u32 ggml_matvec_f16_ncols_5_bs_160_param_10,
	.param .u32 ggml_matvec_f16_ncols_5_bs_160_param_11
)
{
	.local .align 4 .b8 	__local_depot100[20];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<63>;
	.reg .b16 	%rs<4>;
	.reg .f32 	%f<124>;
	.reg .b32 	%r<447>;
	.reg .b64 	%rd<76>;


	mov.u64 	%SPL, __local_depot100;
	ld.param.u64 	%rd28, [ggml_matvec_f16_ncols_5_bs_160_param_0];
	ld.param.u64 	%rd29, [ggml_matvec_f16_ncols_5_bs_160_param_1];
	ld.param.u64 	%rd27, [ggml_matvec_f16_ncols_5_bs_160_param_2];
	ld.param.u32 	%r46, [ggml_matvec_f16_ncols_5_bs_160_param_3];
	ld.param.u32 	%r50, [ggml_matvec_f16_ncols_5_bs_160_param_5];
	ld.param.u32 	%r47, [ggml_matvec_f16_ncols_5_bs_160_param_6];
	ld.param.u32 	%r48, [ggml_matvec_f16_ncols_5_bs_160_param_7];
	ld.param.u32 	%r51, [ggml_matvec_f16_ncols_5_bs_160_param_8];
	ld.param.u32 	%r52, [ggml_matvec_f16_ncols_5_bs_160_param_9];
	ld.param.u32 	%r53, [ggml_matvec_f16_ncols_5_bs_160_param_10];
	ld.param.u32 	%r49, [ggml_matvec_f16_ncols_5_bs_160_param_11];
	cvta.to.global.u64 	%rd75, %rd29;
	cvta.to.global.u64 	%rd2, %rd28;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r54, %r1, %r51;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r55, %r2, %r50;
	mad.lo.s32 	%r56, %r54, %r52, %r55;
	cvt.s64.s32 	%rd4, %r56;
	mul.lo.s32 	%r57, %r1, %r53;
	cvt.s64.s32 	%rd5, %r57;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r58, %r3, 2;
	mov.u32 	%r59, data_mmv;
	add.s32 	%r4, %r59, %r58;
	@%p1 bra 	$L__BB100_2;

	mov.u32 	%r60, 0;
	st.shared.u32 	[%r4], %r60;

$L__BB100_2:
	bar.sync 	0;
	mov.f32 	%f6, 0f00000000;
	mov.u32 	%r442, 0;
	st.local.u32 	[%rd3], %r442;
	st.local.u32 	[%rd3+4], %r442;
	st.local.u32 	[%rd3+8], %r442;
	st.local.u32 	[%rd3+12], %r442;
	st.local.u32 	[%rd3+16], %r442;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs2, %f6;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f6;}

	// end inline asm
	mov.b32 	%r446, {%rs1, %rs2};
	setp.ge.s32 	%p2, %r3, %r46;
	mov.u32 	%r443, %r442;
	mov.u32 	%r444, %r442;
	mov.u32 	%r445, %r442;
	@%p2 bra 	$L__BB100_9;

	not.b32 	%r69, %r3;
	add.s32 	%r6, %r69, %r46;
	mul.wide.u32 	%rd31, %r6, -858993459;
	shr.u64 	%rd32, %rd31, 39;
	cvt.u32.u64 	%r70, %rd32;
	add.s32 	%r71, %r70, 1;
	and.b32  	%r429, %r71, 3;
	setp.eq.s32 	%p3, %r429, 0;
	mov.u32 	%r442, 0;
	mov.u32 	%r435, %r3;
	@%p3 bra 	$L__BB100_6;

	shl.b32 	%r76, %r47, 1;
	mad.lo.s32 	%r77, %r47, 3, %r3;
	mul.wide.s32 	%rd33, %r77, 4;
	shl.b64 	%rd34, %rd5, 1;
	add.s64 	%rd7, %rd33, %rd34;
	mul.wide.s32 	%rd35, %r3, 4;
	mul.wide.s32 	%rd36, %r47, 4;
	add.s64 	%rd37, %rd35, %rd36;
	add.s64 	%rd8, %rd37, %rd34;
	add.s64 	%rd9, %rd35, %rd34;
	mul.wide.s32 	%rd38, %r3, 2;
	add.s64 	%rd39, %rd38, %rd4;
	shl.b64 	%rd40, %rd39, 1;
	add.s64 	%rd72, %rd2, %rd40;
	mul.wide.s32 	%rd11, %r76, 4;
	mov.u32 	%r442, 0;
	mov.u64 	%rd73, %rd75;
	mov.u32 	%r443, %r442;
	mov.u32 	%r444, %r442;
	mov.u32 	%r445, %r442;
	mov.u32 	%r435, %r3;

$L__BB100_5:
	.pragma "nounroll";
	ld.global.nc.u32 	%r79, [%rd72];
	add.s64 	%rd41, %rd73, %rd9;
	ld.global.nc.u32 	%r80, [%rd41];
	// begin inline asm
	{mul.f16x2 %r78,%r79,%r80;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r446,%r446,%r78;
}
	// end inline asm
	add.s64 	%rd42, %rd73, %rd8;
	ld.global.nc.u32 	%r86, [%rd42];
	// begin inline asm
	{mul.f16x2 %r84,%r79,%r86;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r445,%r445,%r84;
}
	// end inline asm
	add.s64 	%rd43, %rd41, %rd11;
	ld.global.nc.u32 	%r92, [%rd43];
	// begin inline asm
	{mul.f16x2 %r90,%r79,%r92;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r444,%r444,%r90;
}
	// end inline asm
	add.s64 	%rd44, %rd73, %rd7;
	ld.global.nc.u32 	%r98, [%rd44];
	// begin inline asm
	{mul.f16x2 %r96,%r79,%r98;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r443,%r443,%r96;
}
	// end inline asm
	add.s64 	%rd45, %rd43, %rd11;
	ld.global.nc.u32 	%r104, [%rd45];
	// begin inline asm
	{mul.f16x2 %r102,%r79,%r104;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r442,%r442,%r102;
}
	// end inline asm
	add.s32 	%r435, %r435, 160;
	add.s64 	%rd73, %rd73, 640;
	add.s64 	%rd72, %rd72, 640;
	add.s32 	%r429, %r429, -1;
	setp.ne.s32 	%p4, %r429, 0;
	@%p4 bra 	$L__BB100_5;

$L__BB100_6:
	setp.lt.u32 	%p5, %r6, 480;
	@%p5 bra 	$L__BB100_9;

	add.s32 	%r108, %r435, %r47;
	shl.b32 	%r109, %r47, 1;
	add.s32 	%r110, %r435, %r109;
	mad.lo.s32 	%r111, %r47, 3, %r435;
	shl.b32 	%r112, %r47, 2;
	add.s32 	%r113, %r435, %r112;
	add.s32 	%r114, %r108, 160;
	mul.wide.s32 	%rd46, %r114, 4;
	shl.b64 	%rd47, %rd5, 1;
	add.s64 	%rd16, %rd46, %rd47;
	mul.wide.s32 	%rd48, %r110, 4;
	add.s64 	%rd17, %rd48, %rd47;
	mul.wide.s32 	%rd49, %r111, 4;
	add.s64 	%rd18, %rd49, %rd47;
	mul.wide.s32 	%rd50, %r113, 4;
	add.s64 	%rd19, %rd50, %rd47;
	mul.wide.s32 	%rd51, %r435, 2;
	add.s64 	%rd52, %rd51, %rd4;
	shl.b64 	%rd53, %rd52, 1;
	add.s64 	%rd54, %rd2, %rd53;
	add.s64 	%rd74, %rd54, 1280;
	mul.wide.s32 	%rd55, %r435, 4;
	add.s64 	%rd21, %rd55, %rd47;
	mul.wide.s32 	%rd56, %r47, 4;
	add.s64 	%rd57, %rd55, %rd56;
	add.s64 	%rd22, %rd57, %rd47;

$L__BB100_8:
	ld.global.nc.u32 	%r116, [%rd74+-1280];
	add.s64 	%rd58, %rd75, %rd21;
	ld.global.nc.u32 	%r117, [%rd58];
	// begin inline asm
	{mul.f16x2 %r115,%r116,%r117;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r118,%r446,%r115;
}
	// end inline asm
	add.s64 	%rd59, %rd75, %rd22;
	ld.global.nc.u32 	%r123, [%rd59];
	// begin inline asm
	{mul.f16x2 %r121,%r116,%r123;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r124,%r445,%r121;
}
	// end inline asm
	add.s64 	%rd60, %rd75, %rd17;
	ld.global.nc.u32 	%r129, [%rd60];
	// begin inline asm
	{mul.f16x2 %r127,%r116,%r129;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r130,%r444,%r127;
}
	// end inline asm
	add.s64 	%rd61, %rd75, %rd18;
	ld.global.nc.u32 	%r135, [%rd61];
	// begin inline asm
	{mul.f16x2 %r133,%r116,%r135;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r136,%r443,%r133;
}
	// end inline asm
	add.s64 	%rd62, %rd75, %rd19;
	ld.global.nc.u32 	%r141, [%rd62];
	// begin inline asm
	{mul.f16x2 %r139,%r116,%r141;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r142,%r442,%r139;
}
	// end inline asm
	ld.global.nc.u32 	%r146, [%rd74+-640];
	ld.global.nc.u32 	%r147, [%rd58+640];
	// begin inline asm
	{mul.f16x2 %r145,%r146,%r147;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r148,%r118,%r145;
}
	// end inline asm
	add.s64 	%rd63, %rd75, %rd16;
	ld.global.nc.u32 	%r153, [%rd63];
	// begin inline asm
	{mul.f16x2 %r151,%r146,%r153;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r154,%r124,%r151;
}
	// end inline asm
	ld.global.nc.u32 	%r159, [%rd60+640];
	// begin inline asm
	{mul.f16x2 %r157,%r146,%r159;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r160,%r130,%r157;
}
	// end inline asm
	ld.global.nc.u32 	%r165, [%rd61+640];
	// begin inline asm
	{mul.f16x2 %r163,%r146,%r165;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r166,%r136,%r163;
}
	// end inline asm
	ld.global.nc.u32 	%r171, [%rd62+640];
	// begin inline asm
	{mul.f16x2 %r169,%r146,%r171;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r172,%r142,%r169;
}
	// end inline asm
	ld.global.nc.u32 	%r176, [%rd74];
	ld.global.nc.u32 	%r177, [%rd58+1280];
	// begin inline asm
	{mul.f16x2 %r175,%r176,%r177;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r178,%r148,%r175;
}
	// end inline asm
	ld.global.nc.u32 	%r183, [%rd63+640];
	// begin inline asm
	{mul.f16x2 %r181,%r176,%r183;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r184,%r154,%r181;
}
	// end inline asm
	ld.global.nc.u32 	%r189, [%rd60+1280];
	// begin inline asm
	{mul.f16x2 %r187,%r176,%r189;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r190,%r160,%r187;
}
	// end inline asm
	ld.global.nc.u32 	%r195, [%rd61+1280];
	// begin inline asm
	{mul.f16x2 %r193,%r176,%r195;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r196,%r166,%r193;
}
	// end inline asm
	ld.global.nc.u32 	%r201, [%rd62+1280];
	// begin inline asm
	{mul.f16x2 %r199,%r176,%r201;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r202,%r172,%r199;
}
	// end inline asm
	ld.global.nc.u32 	%r206, [%rd74+640];
	ld.global.nc.u32 	%r207, [%rd58+1920];
	// begin inline asm
	{mul.f16x2 %r205,%r206,%r207;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r446,%r178,%r205;
}
	// end inline asm
	ld.global.nc.u32 	%r213, [%rd63+1280];
	// begin inline asm
	{mul.f16x2 %r211,%r206,%r213;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r445,%r184,%r211;
}
	// end inline asm
	ld.global.nc.u32 	%r219, [%rd60+1920];
	// begin inline asm
	{mul.f16x2 %r217,%r206,%r219;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r444,%r190,%r217;
}
	// end inline asm
	ld.global.nc.u32 	%r225, [%rd61+1920];
	// begin inline asm
	{mul.f16x2 %r223,%r206,%r225;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r443,%r196,%r223;
}
	// end inline asm
	ld.global.nc.u32 	%r231, [%rd62+1920];
	// begin inline asm
	{mul.f16x2 %r229,%r206,%r231;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r442,%r202,%r229;
}
	// end inline asm
	add.s64 	%rd75, %rd75, 2560;
	add.s64 	%rd74, %rd74, 2560;
	add.s32 	%r435, %r435, 640;
	setp.lt.s32 	%p6, %r435, %r46;
	@%p6 bra 	$L__BB100_8;

$L__BB100_9:
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r446;
  cvt.f32.f16 %f7, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r446;
  cvt.f32.f16 %f8, high;}

	// end inline asm
	add.f32 	%f17, %f7, %f8;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r445;
  cvt.f32.f16 %f9, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r445;
  cvt.f32.f16 %f10, high;}

	// end inline asm
	add.f32 	%f1, %f9, %f10;
	st.local.f32 	[%rd3+4], %f1;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r444;
  cvt.f32.f16 %f11, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r444;
  cvt.f32.f16 %f12, high;}

	// end inline asm
	add.f32 	%f2, %f11, %f12;
	st.local.f32 	[%rd3+8], %f2;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r443;
  cvt.f32.f16 %f13, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r443;
  cvt.f32.f16 %f14, high;}

	// end inline asm
	add.f32 	%f3, %f13, %f14;
	st.local.f32 	[%rd3+12], %f3;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r442;
  cvt.f32.f16 %f15, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r442;
  cvt.f32.f16 %f16, high;}

	// end inline asm
	add.f32 	%f4, %f15, %f16;
	st.local.f32 	[%rd3+16], %f4;
	shr.s32 	%r245, %r3, 31;
	shr.u32 	%r246, %r245, 27;
	add.s32 	%r247, %r3, %r246;
	shr.s32 	%r248, %r247, 5;
	shl.b32 	%r249, %r248, 2;
	add.s32 	%r45, %r59, %r249;
	mov.u32 	%r251, 2;
	mov.b32 	%r252, %f17;
	mov.u32 	%r253, 31;
	mov.u32 	%r254, 16;
	mov.u32 	%r255, -1;
	shfl.sync.bfly.b32 	%r256|%p7, %r252, %r254, %r253, %r255;
	mov.b32 	%f18, %r256;
	add.f32 	%f19, %f17, %f18;
	mov.b32 	%r257, %f19;
	mov.u32 	%r258, 8;
	shfl.sync.bfly.b32 	%r259|%p8, %r257, %r258, %r253, %r255;
	mov.b32 	%f20, %r259;
	add.f32 	%f21, %f19, %f20;
	mov.b32 	%r260, %f21;
	mov.u32 	%r261, 4;
	shfl.sync.bfly.b32 	%r262|%p9, %r260, %r261, %r253, %r255;
	mov.b32 	%f22, %r262;
	add.f32 	%f23, %f21, %f22;
	mov.b32 	%r263, %f23;
	shfl.sync.bfly.b32 	%r264|%p10, %r263, %r251, %r253, %r255;
	mov.b32 	%f24, %r264;
	add.f32 	%f25, %f23, %f24;
	mov.b32 	%r265, %f25;
	mov.u32 	%r266, 1;
	shfl.sync.bfly.b32 	%r267|%p11, %r265, %r266, %r253, %r255;
	mov.b32 	%f26, %r267;
	add.f32 	%f27, %f25, %f26;
	st.local.f32 	[%rd3], %f27;
	st.shared.f32 	[%r45], %f27;
	bar.sync 	0;
	@%p1 bra 	$L__BB100_11;

	ld.shared.f32 	%f28, [%r4];
	mov.b32 	%r268, %f28;
	shfl.sync.bfly.b32 	%r272|%p13, %r268, %r254, %r253, %r255;
	mov.b32 	%f29, %r272;
	add.f32 	%f30, %f28, %f29;
	mov.b32 	%r273, %f30;
	shfl.sync.bfly.b32 	%r275|%p14, %r273, %r258, %r253, %r255;
	mov.b32 	%f31, %r275;
	add.f32 	%f32, %f30, %f31;
	mov.b32 	%r276, %f32;
	shfl.sync.bfly.b32 	%r278|%p15, %r276, %r261, %r253, %r255;
	mov.b32 	%f33, %r278;
	add.f32 	%f34, %f32, %f33;
	mov.b32 	%r279, %f34;
	shfl.sync.bfly.b32 	%r281|%p16, %r279, %r251, %r253, %r255;
	mov.b32 	%f35, %r281;
	add.f32 	%f36, %f34, %f35;
	mov.b32 	%r282, %f36;
	shfl.sync.bfly.b32 	%r284|%p17, %r282, %r266, %r253, %r255;
	mov.b32 	%f37, %r284;
	add.f32 	%f38, %f36, %f37;
	st.local.f32 	[%rd3], %f38;

$L__BB100_11:
	bar.sync 	0;
	mov.b32 	%r285, %f1;
	shfl.sync.bfly.b32 	%r289|%p19, %r285, %r254, %r253, %r255;
	mov.b32 	%f39, %r289;
	add.f32 	%f40, %f1, %f39;
	mov.b32 	%r290, %f40;
	shfl.sync.bfly.b32 	%r292|%p20, %r290, %r258, %r253, %r255;
	mov.b32 	%f41, %r292;
	add.f32 	%f42, %f40, %f41;
	mov.b32 	%r293, %f42;
	shfl.sync.bfly.b32 	%r295|%p21, %r293, %r261, %r253, %r255;
	mov.b32 	%f43, %r295;
	add.f32 	%f44, %f42, %f43;
	mov.b32 	%r296, %f44;
	shfl.sync.bfly.b32 	%r298|%p22, %r296, %r251, %r253, %r255;
	mov.b32 	%f45, %r298;
	add.f32 	%f46, %f44, %f45;
	mov.b32 	%r299, %f46;
	shfl.sync.bfly.b32 	%r301|%p23, %r299, %r266, %r253, %r255;
	mov.b32 	%f47, %r301;
	add.f32 	%f48, %f46, %f47;
	st.local.f32 	[%rd3+4], %f48;
	st.shared.f32 	[%r45], %f48;
	bar.sync 	0;
	@%p1 bra 	$L__BB100_13;

	ld.shared.f32 	%f49, [%r4];
	mov.b32 	%r302, %f49;
	mov.u32 	%r303, 31;
	mov.u32 	%r304, 16;
	mov.u32 	%r305, -1;
	shfl.sync.bfly.b32 	%r306|%p24, %r302, %r304, %r303, %r305;
	mov.b32 	%f50, %r306;
	add.f32 	%f51, %f49, %f50;
	mov.b32 	%r307, %f51;
	mov.u32 	%r308, 8;
	shfl.sync.bfly.b32 	%r309|%p25, %r307, %r308, %r303, %r305;
	mov.b32 	%f52, %r309;
	add.f32 	%f53, %f51, %f52;
	mov.b32 	%r310, %f53;
	mov.u32 	%r311, 4;
	shfl.sync.bfly.b32 	%r312|%p26, %r310, %r311, %r303, %r305;
	mov.b32 	%f54, %r312;
	add.f32 	%f55, %f53, %f54;
	mov.b32 	%r313, %f55;
	mov.u32 	%r314, 2;
	shfl.sync.bfly.b32 	%r315|%p27, %r313, %r314, %r303, %r305;
	mov.b32 	%f56, %r315;
	add.f32 	%f57, %f55, %f56;
	mov.b32 	%r316, %f57;
	mov.u32 	%r317, 1;
	shfl.sync.bfly.b32 	%r318|%p28, %r316, %r317, %r303, %r305;
	mov.b32 	%f58, %r318;
	add.f32 	%f59, %f57, %f58;
	st.local.f32 	[%rd3+4], %f59;

$L__BB100_13:
	bar.sync 	0;
	mov.b32 	%r319, %f2;
	mov.u32 	%r320, 31;
	mov.u32 	%r321, 16;
	mov.u32 	%r322, -1;
	shfl.sync.bfly.b32 	%r323|%p30, %r319, %r321, %r320, %r322;
	mov.b32 	%f60, %r323;
	add.f32 	%f61, %f2, %f60;
	mov.b32 	%r324, %f61;
	mov.u32 	%r325, 8;
	shfl.sync.bfly.b32 	%r326|%p31, %r324, %r325, %r320, %r322;
	mov.b32 	%f62, %r326;
	add.f32 	%f63, %f61, %f62;
	mov.b32 	%r327, %f63;
	mov.u32 	%r328, 4;
	shfl.sync.bfly.b32 	%r329|%p32, %r327, %r328, %r320, %r322;
	mov.b32 	%f64, %r329;
	add.f32 	%f65, %f63, %f64;
	mov.b32 	%r330, %f65;
	mov.u32 	%r331, 2;
	shfl.sync.bfly.b32 	%r332|%p33, %r330, %r331, %r320, %r322;
	mov.b32 	%f66, %r332;
	add.f32 	%f67, %f65, %f66;
	mov.b32 	%r333, %f67;
	mov.u32 	%r334, 1;
	shfl.sync.bfly.b32 	%r335|%p34, %r333, %r334, %r320, %r322;
	mov.b32 	%f68, %r335;
	add.f32 	%f69, %f67, %f68;
	st.local.f32 	[%rd3+8], %f69;
	st.shared.f32 	[%r45], %f69;
	bar.sync 	0;
	@%p1 bra 	$L__BB100_15;

	ld.shared.f32 	%f70, [%r4];
	mov.b32 	%r336, %f70;
	shfl.sync.bfly.b32 	%r340|%p35, %r336, %r321, %r320, %r322;
	mov.b32 	%f71, %r340;
	add.f32 	%f72, %f70, %f71;
	mov.b32 	%r341, %f72;
	shfl.sync.bfly.b32 	%r343|%p36, %r341, %r325, %r320, %r322;
	mov.b32 	%f73, %r343;
	add.f32 	%f74, %f72, %f73;
	mov.b32 	%r344, %f74;
	shfl.sync.bfly.b32 	%r346|%p37, %r344, %r328, %r320, %r322;
	mov.b32 	%f75, %r346;
	add.f32 	%f76, %f74, %f75;
	mov.b32 	%r347, %f76;
	shfl.sync.bfly.b32 	%r349|%p38, %r347, %r331, %r320, %r322;
	mov.b32 	%f77, %r349;
	add.f32 	%f78, %f76, %f77;
	mov.b32 	%r350, %f78;
	shfl.sync.bfly.b32 	%r352|%p39, %r350, %r334, %r320, %r322;
	mov.b32 	%f79, %r352;
	add.f32 	%f80, %f78, %f79;
	st.local.f32 	[%rd3+8], %f80;

$L__BB100_15:
	bar.sync 	0;
	mov.b32 	%r353, %f3;
	shfl.sync.bfly.b32 	%r357|%p41, %r353, %r321, %r320, %r322;
	mov.b32 	%f81, %r357;
	add.f32 	%f82, %f3, %f81;
	mov.b32 	%r358, %f82;
	shfl.sync.bfly.b32 	%r360|%p42, %r358, %r325, %r320, %r322;
	mov.b32 	%f83, %r360;
	add.f32 	%f84, %f82, %f83;
	mov.b32 	%r361, %f84;
	shfl.sync.bfly.b32 	%r363|%p43, %r361, %r328, %r320, %r322;
	mov.b32 	%f85, %r363;
	add.f32 	%f86, %f84, %f85;
	mov.b32 	%r364, %f86;
	shfl.sync.bfly.b32 	%r366|%p44, %r364, %r331, %r320, %r322;
	mov.b32 	%f87, %r366;
	add.f32 	%f88, %f86, %f87;
	mov.b32 	%r367, %f88;
	shfl.sync.bfly.b32 	%r369|%p45, %r367, %r334, %r320, %r322;
	mov.b32 	%f89, %r369;
	add.f32 	%f90, %f88, %f89;
	st.local.f32 	[%rd3+12], %f90;
	st.shared.f32 	[%r45], %f90;
	bar.sync 	0;
	@%p1 bra 	$L__BB100_17;

	ld.shared.f32 	%f91, [%r4];
	mov.b32 	%r370, %f91;
	mov.u32 	%r371, 31;
	mov.u32 	%r372, 16;
	mov.u32 	%r373, -1;
	shfl.sync.bfly.b32 	%r374|%p46, %r370, %r372, %r371, %r373;
	mov.b32 	%f92, %r374;
	add.f32 	%f93, %f91, %f92;
	mov.b32 	%r375, %f93;
	mov.u32 	%r376, 8;
	shfl.sync.bfly.b32 	%r377|%p47, %r375, %r376, %r371, %r373;
	mov.b32 	%f94, %r377;
	add.f32 	%f95, %f93, %f94;
	mov.b32 	%r378, %f95;
	mov.u32 	%r379, 4;
	shfl.sync.bfly.b32 	%r380|%p48, %r378, %r379, %r371, %r373;
	mov.b32 	%f96, %r380;
	add.f32 	%f97, %f95, %f96;
	mov.b32 	%r381, %f97;
	mov.u32 	%r382, 2;
	shfl.sync.bfly.b32 	%r383|%p49, %r381, %r382, %r371, %r373;
	mov.b32 	%f98, %r383;
	add.f32 	%f99, %f97, %f98;
	mov.b32 	%r384, %f99;
	mov.u32 	%r385, 1;
	shfl.sync.bfly.b32 	%r386|%p50, %r384, %r385, %r371, %r373;
	mov.b32 	%f100, %r386;
	add.f32 	%f101, %f99, %f100;
	st.local.f32 	[%rd3+12], %f101;

$L__BB100_17:
	bar.sync 	0;
	mov.b32 	%r387, %f4;
	mov.u32 	%r388, 31;
	mov.u32 	%r389, 16;
	mov.u32 	%r390, -1;
	shfl.sync.bfly.b32 	%r391|%p52, %r387, %r389, %r388, %r390;
	mov.b32 	%f102, %r391;
	add.f32 	%f103, %f4, %f102;
	mov.b32 	%r392, %f103;
	mov.u32 	%r393, 8;
	shfl.sync.bfly.b32 	%r394|%p53, %r392, %r393, %r388, %r390;
	mov.b32 	%f104, %r394;
	add.f32 	%f105, %f103, %f104;
	mov.b32 	%r395, %f105;
	mov.u32 	%r396, 4;
	shfl.sync.bfly.b32 	%r397|%p54, %r395, %r396, %r388, %r390;
	mov.b32 	%f106, %r397;
	add.f32 	%f107, %f105, %f106;
	mov.b32 	%r398, %f107;
	mov.u32 	%r399, 2;
	shfl.sync.bfly.b32 	%r400|%p55, %r398, %r399, %r388, %r390;
	mov.b32 	%f108, %r400;
	add.f32 	%f109, %f107, %f108;
	mov.b32 	%r401, %f109;
	mov.u32 	%r402, 1;
	shfl.sync.bfly.b32 	%r403|%p56, %r401, %r402, %r388, %r390;
	mov.b32 	%f110, %r403;
	add.f32 	%f111, %f109, %f110;
	st.local.f32 	[%rd3+16], %f111;
	st.shared.f32 	[%r45], %f111;
	bar.sync 	0;
	@%p1 bra 	$L__BB100_19;

	ld.shared.f32 	%f112, [%r4];
	mov.b32 	%r404, %f112;
	shfl.sync.bfly.b32 	%r408|%p57, %r404, %r389, %r388, %r390;
	mov.b32 	%f113, %r408;
	add.f32 	%f114, %f112, %f113;
	mov.b32 	%r409, %f114;
	shfl.sync.bfly.b32 	%r411|%p58, %r409, %r393, %r388, %r390;
	mov.b32 	%f115, %r411;
	add.f32 	%f116, %f114, %f115;
	mov.b32 	%r412, %f116;
	shfl.sync.bfly.b32 	%r414|%p59, %r412, %r396, %r388, %r390;
	mov.b32 	%f117, %r414;
	add.f32 	%f118, %f116, %f117;
	mov.b32 	%r415, %f118;
	shfl.sync.bfly.b32 	%r417|%p60, %r415, %r399, %r388, %r390;
	mov.b32 	%f119, %r417;
	add.f32 	%f120, %f118, %f119;
	mov.b32 	%r418, %f120;
	shfl.sync.bfly.b32 	%r420|%p61, %r418, %r402, %r388, %r390;
	mov.b32 	%f121, %r420;
	add.f32 	%f122, %f120, %f121;
	st.local.f32 	[%rd3+16], %f122;

$L__BB100_19:
	bar.sync 	0;
	setp.gt.s32 	%p62, %r3, 4;
	@%p62 bra 	$L__BB100_21;

	mad.lo.s32 	%r421, %r3, %r48, %r2;
	cvt.s64.s32 	%rd64, %r421;
	mul.lo.s32 	%r422, %r1, %r49;
	cvt.s64.s32 	%rd65, %r422;
	add.s64 	%rd66, %rd65, %rd64;
	mul.wide.s32 	%rd67, %r3, 4;
	add.s64 	%rd68, %rd3, %rd67;
	ld.local.f32 	%f123, [%rd68];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs3, %f123;}

	// end inline asm
	cvta.to.global.u64 	%rd69, %rd27;
	shl.b64 	%rd70, %rd66, 1;
	add.s64 	%rd71, %rd69, %rd70;
	st.global.u16 	[%rd71], %rs3;

$L__BB100_21:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_6_bs_160
.visible .entry ggml_matvec_f16_ncols_6_bs_160(
	.param .u64 ggml_matvec_f16_ncols_6_bs_160_param_0,
	.param .u64 ggml_matvec_f16_ncols_6_bs_160_param_1,
	.param .u64 ggml_matvec_f16_ncols_6_bs_160_param_2,
	.param .u32 ggml_matvec_f16_ncols_6_bs_160_param_3,
	.param .u32 ggml_matvec_f16_ncols_6_bs_160_param_4,
	.param .u32 ggml_matvec_f16_ncols_6_bs_160_param_5,
	.param .u32 ggml_matvec_f16_ncols_6_bs_160_param_6,
	.param .u32 ggml_matvec_f16_ncols_6_bs_160_param_7,
	.param .u32 ggml_matvec_f16_ncols_6_bs_160_param_8,
	.param .u32 ggml_matvec_f16_ncols_6_bs_160_param_9,
	.param .u32 ggml_matvec_f16_ncols_6_bs_160_param_10,
	.param .u32 ggml_matvec_f16_ncols_6_bs_160_param_11
)
{
	.local .align 8 .b8 	__local_depot101[24];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<74>;
	.reg .b16 	%rs<4>;
	.reg .f32 	%f<148>;
	.reg .b32 	%r<527>;
	.reg .b64 	%rd<79>;


	mov.u64 	%SPL, __local_depot101;
	ld.param.u64 	%rd29, [ggml_matvec_f16_ncols_6_bs_160_param_0];
	ld.param.u64 	%rd30, [ggml_matvec_f16_ncols_6_bs_160_param_1];
	ld.param.u64 	%rd28, [ggml_matvec_f16_ncols_6_bs_160_param_2];
	ld.param.u32 	%r52, [ggml_matvec_f16_ncols_6_bs_160_param_3];
	ld.param.u32 	%r56, [ggml_matvec_f16_ncols_6_bs_160_param_5];
	ld.param.u32 	%r53, [ggml_matvec_f16_ncols_6_bs_160_param_6];
	ld.param.u32 	%r54, [ggml_matvec_f16_ncols_6_bs_160_param_7];
	ld.param.u32 	%r57, [ggml_matvec_f16_ncols_6_bs_160_param_8];
	ld.param.u32 	%r58, [ggml_matvec_f16_ncols_6_bs_160_param_9];
	ld.param.u32 	%r59, [ggml_matvec_f16_ncols_6_bs_160_param_10];
	ld.param.u32 	%r55, [ggml_matvec_f16_ncols_6_bs_160_param_11];
	cvta.to.global.u64 	%rd78, %rd30;
	cvta.to.global.u64 	%rd2, %rd29;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r60, %r1, %r57;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r61, %r2, %r56;
	mad.lo.s32 	%r62, %r60, %r58, %r61;
	cvt.s64.s32 	%rd4, %r62;
	mul.lo.s32 	%r63, %r1, %r59;
	cvt.s64.s32 	%rd5, %r63;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r64, %r3, 2;
	mov.u32 	%r65, data_mmv;
	add.s32 	%r4, %r65, %r64;
	@%p1 bra 	$L__BB101_2;

	mov.u32 	%r66, 0;
	st.shared.u32 	[%r4], %r66;

$L__BB101_2:
	bar.sync 	0;
	mov.f32 	%f7, 0f00000000;
	st.local.v2.f32 	[%rd3], {%f7, %f7};
	st.local.v2.f32 	[%rd3+8], {%f7, %f7};
	st.local.v2.f32 	[%rd3+16], {%f7, %f7};
	mov.u32 	%r521, 0;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs2, %f7;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f7;}

	// end inline asm
	mov.b32 	%r526, {%rs1, %rs2};
	setp.ge.s32 	%p2, %r3, %r52;
	mov.u32 	%r522, %r521;
	mov.u32 	%r523, %r521;
	mov.u32 	%r524, %r521;
	mov.u32 	%r525, %r521;
	@%p2 bra 	$L__BB101_9;

	not.b32 	%r77, %r3;
	add.s32 	%r6, %r77, %r52;
	mul.wide.u32 	%rd32, %r6, -858993459;
	shr.u64 	%rd33, %rd32, 39;
	cvt.u32.u64 	%r78, %rd33;
	add.s32 	%r79, %r78, 1;
	and.b32  	%r506, %r79, 3;
	setp.eq.s32 	%p3, %r506, 0;
	mov.u32 	%r521, 0;
	mov.u32 	%r513, %r3;
	@%p3 bra 	$L__BB101_6;

	shl.b32 	%r85, %r53, 1;
	add.s32 	%r86, %r3, %r85;
	mul.wide.s32 	%rd34, %r86, 4;
	shl.b64 	%rd35, %rd5, 1;
	add.s64 	%rd7, %rd34, %rd35;
	mul.wide.s32 	%rd36, %r3, 4;
	mul.wide.s32 	%rd8, %r53, 4;
	add.s64 	%rd37, %rd36, %rd8;
	add.s64 	%rd9, %rd37, %rd35;
	add.s64 	%rd10, %rd36, %rd35;
	mul.wide.s32 	%rd38, %r3, 2;
	add.s64 	%rd39, %rd38, %rd4;
	shl.b64 	%rd40, %rd39, 1;
	add.s64 	%rd75, %rd2, %rd40;
	mov.u32 	%r521, 0;
	mov.u64 	%rd76, %rd78;
	mov.u32 	%r522, %r521;
	mov.u32 	%r523, %r521;
	mov.u32 	%r524, %r521;
	mov.u32 	%r525, %r521;
	mov.u32 	%r513, %r3;

$L__BB101_5:
	.pragma "nounroll";
	ld.global.nc.u32 	%r88, [%rd75];
	add.s64 	%rd41, %rd76, %rd10;
	ld.global.nc.u32 	%r89, [%rd41];
	// begin inline asm
	{mul.f16x2 %r87,%r88,%r89;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r526,%r526,%r87;
}
	// end inline asm
	add.s64 	%rd42, %rd76, %rd9;
	ld.global.nc.u32 	%r95, [%rd42];
	// begin inline asm
	{mul.f16x2 %r93,%r88,%r95;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r525,%r525,%r93;
}
	// end inline asm
	add.s64 	%rd43, %rd76, %rd7;
	ld.global.nc.u32 	%r101, [%rd43];
	// begin inline asm
	{mul.f16x2 %r99,%r88,%r101;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r524,%r524,%r99;
}
	// end inline asm
	add.s64 	%rd44, %rd43, %rd8;
	ld.global.nc.u32 	%r107, [%rd44];
	// begin inline asm
	{mul.f16x2 %r105,%r88,%r107;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r523,%r523,%r105;
}
	// end inline asm
	add.s64 	%rd45, %rd44, %rd8;
	ld.global.nc.u32 	%r113, [%rd45];
	// begin inline asm
	{mul.f16x2 %r111,%r88,%r113;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r522,%r522,%r111;
}
	// end inline asm
	add.s64 	%rd46, %rd45, %rd8;
	ld.global.nc.u32 	%r119, [%rd46];
	// begin inline asm
	{mul.f16x2 %r117,%r88,%r119;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r521,%r521,%r117;
}
	// end inline asm
	add.s32 	%r513, %r513, 160;
	add.s64 	%rd76, %rd76, 640;
	add.s64 	%rd75, %rd75, 640;
	add.s32 	%r506, %r506, -1;
	setp.ne.s32 	%p4, %r506, 0;
	@%p4 bra 	$L__BB101_5;

$L__BB101_6:
	setp.lt.u32 	%p5, %r6, 480;
	@%p5 bra 	$L__BB101_9;

	add.s32 	%r123, %r513, %r53;
	shl.b32 	%r124, %r53, 1;
	add.s32 	%r125, %r513, %r124;
	mad.lo.s32 	%r126, %r53, 3, %r513;
	shl.b32 	%r127, %r53, 2;
	add.s32 	%r128, %r513, %r127;
	mad.lo.s32 	%r129, %r53, 5, %r513;
	add.s32 	%r130, %r123, 160;
	mul.wide.s32 	%rd47, %r130, 4;
	shl.b64 	%rd48, %rd5, 1;
	add.s64 	%rd16, %rd47, %rd48;
	mul.wide.s32 	%rd49, %r125, 4;
	add.s64 	%rd17, %rd49, %rd48;
	mul.wide.s32 	%rd50, %r126, 4;
	add.s64 	%rd18, %rd50, %rd48;
	mul.wide.s32 	%rd51, %r128, 4;
	add.s64 	%rd19, %rd51, %rd48;
	mul.wide.s32 	%rd52, %r129, 4;
	add.s64 	%rd20, %rd52, %rd48;
	mul.wide.s32 	%rd53, %r513, 2;
	add.s64 	%rd54, %rd53, %rd4;
	shl.b64 	%rd55, %rd54, 1;
	add.s64 	%rd56, %rd2, %rd55;
	add.s64 	%rd77, %rd56, 1280;
	mul.wide.s32 	%rd57, %r513, 4;
	add.s64 	%rd22, %rd57, %rd48;
	mul.wide.s32 	%rd58, %r53, 4;
	add.s64 	%rd59, %rd57, %rd58;
	add.s64 	%rd23, %rd59, %rd48;

$L__BB101_8:
	ld.global.nc.u32 	%r132, [%rd77+-1280];
	add.s64 	%rd60, %rd78, %rd22;
	ld.global.nc.u32 	%r133, [%rd60];
	// begin inline asm
	{mul.f16x2 %r131,%r132,%r133;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r134,%r526,%r131;
}
	// end inline asm
	add.s64 	%rd61, %rd78, %rd23;
	ld.global.nc.u32 	%r139, [%rd61];
	// begin inline asm
	{mul.f16x2 %r137,%r132,%r139;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r140,%r525,%r137;
}
	// end inline asm
	add.s64 	%rd62, %rd78, %rd17;
	ld.global.nc.u32 	%r145, [%rd62];
	// begin inline asm
	{mul.f16x2 %r143,%r132,%r145;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r146,%r524,%r143;
}
	// end inline asm
	add.s64 	%rd63, %rd78, %rd18;
	ld.global.nc.u32 	%r151, [%rd63];
	// begin inline asm
	{mul.f16x2 %r149,%r132,%r151;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r152,%r523,%r149;
}
	// end inline asm
	add.s64 	%rd64, %rd78, %rd19;
	ld.global.nc.u32 	%r157, [%rd64];
	// begin inline asm
	{mul.f16x2 %r155,%r132,%r157;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r158,%r522,%r155;
}
	// end inline asm
	add.s64 	%rd65, %rd78, %rd20;
	ld.global.nc.u32 	%r163, [%rd65];
	// begin inline asm
	{mul.f16x2 %r161,%r132,%r163;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r164,%r521,%r161;
}
	// end inline asm
	ld.global.nc.u32 	%r168, [%rd77+-640];
	ld.global.nc.u32 	%r169, [%rd60+640];
	// begin inline asm
	{mul.f16x2 %r167,%r168,%r169;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r170,%r134,%r167;
}
	// end inline asm
	add.s64 	%rd66, %rd78, %rd16;
	ld.global.nc.u32 	%r175, [%rd66];
	// begin inline asm
	{mul.f16x2 %r173,%r168,%r175;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r176,%r140,%r173;
}
	// end inline asm
	ld.global.nc.u32 	%r181, [%rd62+640];
	// begin inline asm
	{mul.f16x2 %r179,%r168,%r181;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r182,%r146,%r179;
}
	// end inline asm
	ld.global.nc.u32 	%r187, [%rd63+640];
	// begin inline asm
	{mul.f16x2 %r185,%r168,%r187;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r188,%r152,%r185;
}
	// end inline asm
	ld.global.nc.u32 	%r193, [%rd64+640];
	// begin inline asm
	{mul.f16x2 %r191,%r168,%r193;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r194,%r158,%r191;
}
	// end inline asm
	ld.global.nc.u32 	%r199, [%rd65+640];
	// begin inline asm
	{mul.f16x2 %r197,%r168,%r199;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r200,%r164,%r197;
}
	// end inline asm
	ld.global.nc.u32 	%r204, [%rd77];
	ld.global.nc.u32 	%r205, [%rd60+1280];
	// begin inline asm
	{mul.f16x2 %r203,%r204,%r205;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r206,%r170,%r203;
}
	// end inline asm
	ld.global.nc.u32 	%r211, [%rd66+640];
	// begin inline asm
	{mul.f16x2 %r209,%r204,%r211;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r212,%r176,%r209;
}
	// end inline asm
	ld.global.nc.u32 	%r217, [%rd62+1280];
	// begin inline asm
	{mul.f16x2 %r215,%r204,%r217;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r218,%r182,%r215;
}
	// end inline asm
	ld.global.nc.u32 	%r223, [%rd63+1280];
	// begin inline asm
	{mul.f16x2 %r221,%r204,%r223;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r224,%r188,%r221;
}
	// end inline asm
	ld.global.nc.u32 	%r229, [%rd64+1280];
	// begin inline asm
	{mul.f16x2 %r227,%r204,%r229;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r230,%r194,%r227;
}
	// end inline asm
	ld.global.nc.u32 	%r235, [%rd65+1280];
	// begin inline asm
	{mul.f16x2 %r233,%r204,%r235;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r236,%r200,%r233;
}
	// end inline asm
	ld.global.nc.u32 	%r240, [%rd77+640];
	ld.global.nc.u32 	%r241, [%rd60+1920];
	// begin inline asm
	{mul.f16x2 %r239,%r240,%r241;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r526,%r206,%r239;
}
	// end inline asm
	ld.global.nc.u32 	%r247, [%rd66+1280];
	// begin inline asm
	{mul.f16x2 %r245,%r240,%r247;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r525,%r212,%r245;
}
	// end inline asm
	ld.global.nc.u32 	%r253, [%rd62+1920];
	// begin inline asm
	{mul.f16x2 %r251,%r240,%r253;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r524,%r218,%r251;
}
	// end inline asm
	ld.global.nc.u32 	%r259, [%rd63+1920];
	// begin inline asm
	{mul.f16x2 %r257,%r240,%r259;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r523,%r224,%r257;
}
	// end inline asm
	ld.global.nc.u32 	%r265, [%rd64+1920];
	// begin inline asm
	{mul.f16x2 %r263,%r240,%r265;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r522,%r230,%r263;
}
	// end inline asm
	ld.global.nc.u32 	%r271, [%rd65+1920];
	// begin inline asm
	{mul.f16x2 %r269,%r240,%r271;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r521,%r236,%r269;
}
	// end inline asm
	add.s64 	%rd78, %rd78, 2560;
	add.s64 	%rd77, %rd77, 2560;
	add.s32 	%r513, %r513, 640;
	setp.lt.s32 	%p6, %r513, %r52;
	@%p6 bra 	$L__BB101_8;

$L__BB101_9:
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r526;
  cvt.f32.f16 %f8, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r526;
  cvt.f32.f16 %f9, high;}

	// end inline asm
	add.f32 	%f20, %f8, %f9;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r525;
  cvt.f32.f16 %f10, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r525;
  cvt.f32.f16 %f11, high;}

	// end inline asm
	add.f32 	%f1, %f10, %f11;
	st.local.f32 	[%rd3+4], %f1;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r524;
  cvt.f32.f16 %f12, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r524;
  cvt.f32.f16 %f13, high;}

	// end inline asm
	add.f32 	%f2, %f12, %f13;
	st.local.f32 	[%rd3+8], %f2;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r523;
  cvt.f32.f16 %f14, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r523;
  cvt.f32.f16 %f15, high;}

	// end inline asm
	add.f32 	%f3, %f14, %f15;
	st.local.f32 	[%rd3+12], %f3;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r522;
  cvt.f32.f16 %f16, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r522;
  cvt.f32.f16 %f17, high;}

	// end inline asm
	add.f32 	%f4, %f16, %f17;
	st.local.f32 	[%rd3+16], %f4;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r521;
  cvt.f32.f16 %f18, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r521;
  cvt.f32.f16 %f19, high;}

	// end inline asm
	add.f32 	%f5, %f18, %f19;
	st.local.f32 	[%rd3+20], %f5;
	shr.s32 	%r287, %r3, 31;
	shr.u32 	%r288, %r287, 27;
	add.s32 	%r289, %r3, %r288;
	shr.s32 	%r290, %r289, 5;
	shl.b32 	%r291, %r290, 2;
	add.s32 	%r51, %r65, %r291;
	mov.u32 	%r293, 2;
	mov.b32 	%r294, %f20;
	mov.u32 	%r295, 31;
	mov.u32 	%r296, 16;
	mov.u32 	%r297, -1;
	shfl.sync.bfly.b32 	%r298|%p7, %r294, %r296, %r295, %r297;
	mov.b32 	%f21, %r298;
	add.f32 	%f22, %f20, %f21;
	mov.b32 	%r299, %f22;
	mov.u32 	%r300, 8;
	shfl.sync.bfly.b32 	%r301|%p8, %r299, %r300, %r295, %r297;
	mov.b32 	%f23, %r301;
	add.f32 	%f24, %f22, %f23;
	mov.b32 	%r302, %f24;
	mov.u32 	%r303, 4;
	shfl.sync.bfly.b32 	%r304|%p9, %r302, %r303, %r295, %r297;
	mov.b32 	%f25, %r304;
	add.f32 	%f26, %f24, %f25;
	mov.b32 	%r305, %f26;
	shfl.sync.bfly.b32 	%r306|%p10, %r305, %r293, %r295, %r297;
	mov.b32 	%f27, %r306;
	add.f32 	%f28, %f26, %f27;
	mov.b32 	%r307, %f28;
	mov.u32 	%r308, 1;
	shfl.sync.bfly.b32 	%r309|%p11, %r307, %r308, %r295, %r297;
	mov.b32 	%f29, %r309;
	add.f32 	%f30, %f28, %f29;
	st.local.f32 	[%rd3], %f30;
	st.shared.f32 	[%r51], %f30;
	bar.sync 	0;
	@%p1 bra 	$L__BB101_11;

	ld.shared.f32 	%f31, [%r4];
	mov.b32 	%r310, %f31;
	shfl.sync.bfly.b32 	%r314|%p13, %r310, %r296, %r295, %r297;
	mov.b32 	%f32, %r314;
	add.f32 	%f33, %f31, %f32;
	mov.b32 	%r315, %f33;
	shfl.sync.bfly.b32 	%r317|%p14, %r315, %r300, %r295, %r297;
	mov.b32 	%f34, %r317;
	add.f32 	%f35, %f33, %f34;
	mov.b32 	%r318, %f35;
	shfl.sync.bfly.b32 	%r320|%p15, %r318, %r303, %r295, %r297;
	mov.b32 	%f36, %r320;
	add.f32 	%f37, %f35, %f36;
	mov.b32 	%r321, %f37;
	shfl.sync.bfly.b32 	%r323|%p16, %r321, %r293, %r295, %r297;
	mov.b32 	%f38, %r323;
	add.f32 	%f39, %f37, %f38;
	mov.b32 	%r324, %f39;
	shfl.sync.bfly.b32 	%r326|%p17, %r324, %r308, %r295, %r297;
	mov.b32 	%f40, %r326;
	add.f32 	%f41, %f39, %f40;
	st.local.f32 	[%rd3], %f41;

$L__BB101_11:
	bar.sync 	0;
	mov.b32 	%r327, %f1;
	shfl.sync.bfly.b32 	%r331|%p19, %r327, %r296, %r295, %r297;
	mov.b32 	%f42, %r331;
	add.f32 	%f43, %f1, %f42;
	mov.b32 	%r332, %f43;
	shfl.sync.bfly.b32 	%r334|%p20, %r332, %r300, %r295, %r297;
	mov.b32 	%f44, %r334;
	add.f32 	%f45, %f43, %f44;
	mov.b32 	%r335, %f45;
	shfl.sync.bfly.b32 	%r337|%p21, %r335, %r303, %r295, %r297;
	mov.b32 	%f46, %r337;
	add.f32 	%f47, %f45, %f46;
	mov.b32 	%r338, %f47;
	shfl.sync.bfly.b32 	%r340|%p22, %r338, %r293, %r295, %r297;
	mov.b32 	%f48, %r340;
	add.f32 	%f49, %f47, %f48;
	mov.b32 	%r341, %f49;
	shfl.sync.bfly.b32 	%r343|%p23, %r341, %r308, %r295, %r297;
	mov.b32 	%f50, %r343;
	add.f32 	%f51, %f49, %f50;
	st.local.f32 	[%rd3+4], %f51;
	st.shared.f32 	[%r51], %f51;
	bar.sync 	0;
	@%p1 bra 	$L__BB101_13;

	ld.shared.f32 	%f52, [%r4];
	mov.b32 	%r344, %f52;
	mov.u32 	%r345, 31;
	mov.u32 	%r346, 16;
	mov.u32 	%r347, -1;
	shfl.sync.bfly.b32 	%r348|%p24, %r344, %r346, %r345, %r347;
	mov.b32 	%f53, %r348;
	add.f32 	%f54, %f52, %f53;
	mov.b32 	%r349, %f54;
	mov.u32 	%r350, 8;
	shfl.sync.bfly.b32 	%r351|%p25, %r349, %r350, %r345, %r347;
	mov.b32 	%f55, %r351;
	add.f32 	%f56, %f54, %f55;
	mov.b32 	%r352, %f56;
	mov.u32 	%r353, 4;
	shfl.sync.bfly.b32 	%r354|%p26, %r352, %r353, %r345, %r347;
	mov.b32 	%f57, %r354;
	add.f32 	%f58, %f56, %f57;
	mov.b32 	%r355, %f58;
	mov.u32 	%r356, 2;
	shfl.sync.bfly.b32 	%r357|%p27, %r355, %r356, %r345, %r347;
	mov.b32 	%f59, %r357;
	add.f32 	%f60, %f58, %f59;
	mov.b32 	%r358, %f60;
	mov.u32 	%r359, 1;
	shfl.sync.bfly.b32 	%r360|%p28, %r358, %r359, %r345, %r347;
	mov.b32 	%f61, %r360;
	add.f32 	%f62, %f60, %f61;
	st.local.f32 	[%rd3+4], %f62;

$L__BB101_13:
	bar.sync 	0;
	mov.b32 	%r361, %f2;
	mov.u32 	%r362, 31;
	mov.u32 	%r363, 16;
	mov.u32 	%r364, -1;
	shfl.sync.bfly.b32 	%r365|%p30, %r361, %r363, %r362, %r364;
	mov.b32 	%f63, %r365;
	add.f32 	%f64, %f2, %f63;
	mov.b32 	%r366, %f64;
	mov.u32 	%r367, 8;
	shfl.sync.bfly.b32 	%r368|%p31, %r366, %r367, %r362, %r364;
	mov.b32 	%f65, %r368;
	add.f32 	%f66, %f64, %f65;
	mov.b32 	%r369, %f66;
	mov.u32 	%r370, 4;
	shfl.sync.bfly.b32 	%r371|%p32, %r369, %r370, %r362, %r364;
	mov.b32 	%f67, %r371;
	add.f32 	%f68, %f66, %f67;
	mov.b32 	%r372, %f68;
	mov.u32 	%r373, 2;
	shfl.sync.bfly.b32 	%r374|%p33, %r372, %r373, %r362, %r364;
	mov.b32 	%f69, %r374;
	add.f32 	%f70, %f68, %f69;
	mov.b32 	%r375, %f70;
	mov.u32 	%r376, 1;
	shfl.sync.bfly.b32 	%r377|%p34, %r375, %r376, %r362, %r364;
	mov.b32 	%f71, %r377;
	add.f32 	%f72, %f70, %f71;
	st.local.f32 	[%rd3+8], %f72;
	st.shared.f32 	[%r51], %f72;
	bar.sync 	0;
	@%p1 bra 	$L__BB101_15;

	ld.shared.f32 	%f73, [%r4];
	mov.b32 	%r378, %f73;
	shfl.sync.bfly.b32 	%r382|%p35, %r378, %r363, %r362, %r364;
	mov.b32 	%f74, %r382;
	add.f32 	%f75, %f73, %f74;
	mov.b32 	%r383, %f75;
	shfl.sync.bfly.b32 	%r385|%p36, %r383, %r367, %r362, %r364;
	mov.b32 	%f76, %r385;
	add.f32 	%f77, %f75, %f76;
	mov.b32 	%r386, %f77;
	shfl.sync.bfly.b32 	%r388|%p37, %r386, %r370, %r362, %r364;
	mov.b32 	%f78, %r388;
	add.f32 	%f79, %f77, %f78;
	mov.b32 	%r389, %f79;
	shfl.sync.bfly.b32 	%r391|%p38, %r389, %r373, %r362, %r364;
	mov.b32 	%f80, %r391;
	add.f32 	%f81, %f79, %f80;
	mov.b32 	%r392, %f81;
	shfl.sync.bfly.b32 	%r394|%p39, %r392, %r376, %r362, %r364;
	mov.b32 	%f82, %r394;
	add.f32 	%f83, %f81, %f82;
	st.local.f32 	[%rd3+8], %f83;

$L__BB101_15:
	bar.sync 	0;
	mov.b32 	%r395, %f3;
	shfl.sync.bfly.b32 	%r399|%p41, %r395, %r363, %r362, %r364;
	mov.b32 	%f84, %r399;
	add.f32 	%f85, %f3, %f84;
	mov.b32 	%r400, %f85;
	shfl.sync.bfly.b32 	%r402|%p42, %r400, %r367, %r362, %r364;
	mov.b32 	%f86, %r402;
	add.f32 	%f87, %f85, %f86;
	mov.b32 	%r403, %f87;
	shfl.sync.bfly.b32 	%r405|%p43, %r403, %r370, %r362, %r364;
	mov.b32 	%f88, %r405;
	add.f32 	%f89, %f87, %f88;
	mov.b32 	%r406, %f89;
	shfl.sync.bfly.b32 	%r408|%p44, %r406, %r373, %r362, %r364;
	mov.b32 	%f90, %r408;
	add.f32 	%f91, %f89, %f90;
	mov.b32 	%r409, %f91;
	shfl.sync.bfly.b32 	%r411|%p45, %r409, %r376, %r362, %r364;
	mov.b32 	%f92, %r411;
	add.f32 	%f93, %f91, %f92;
	st.local.f32 	[%rd3+12], %f93;
	st.shared.f32 	[%r51], %f93;
	bar.sync 	0;
	@%p1 bra 	$L__BB101_17;

	ld.shared.f32 	%f94, [%r4];
	mov.b32 	%r412, %f94;
	mov.u32 	%r413, 31;
	mov.u32 	%r414, 16;
	mov.u32 	%r415, -1;
	shfl.sync.bfly.b32 	%r416|%p46, %r412, %r414, %r413, %r415;
	mov.b32 	%f95, %r416;
	add.f32 	%f96, %f94, %f95;
	mov.b32 	%r417, %f96;
	mov.u32 	%r418, 8;
	shfl.sync.bfly.b32 	%r419|%p47, %r417, %r418, %r413, %r415;
	mov.b32 	%f97, %r419;
	add.f32 	%f98, %f96, %f97;
	mov.b32 	%r420, %f98;
	mov.u32 	%r421, 4;
	shfl.sync.bfly.b32 	%r422|%p48, %r420, %r421, %r413, %r415;
	mov.b32 	%f99, %r422;
	add.f32 	%f100, %f98, %f99;
	mov.b32 	%r423, %f100;
	mov.u32 	%r424, 2;
	shfl.sync.bfly.b32 	%r425|%p49, %r423, %r424, %r413, %r415;
	mov.b32 	%f101, %r425;
	add.f32 	%f102, %f100, %f101;
	mov.b32 	%r426, %f102;
	mov.u32 	%r427, 1;
	shfl.sync.bfly.b32 	%r428|%p50, %r426, %r427, %r413, %r415;
	mov.b32 	%f103, %r428;
	add.f32 	%f104, %f102, %f103;
	st.local.f32 	[%rd3+12], %f104;

$L__BB101_17:
	bar.sync 	0;
	mov.b32 	%r429, %f4;
	mov.u32 	%r430, 31;
	mov.u32 	%r431, 16;
	mov.u32 	%r432, -1;
	shfl.sync.bfly.b32 	%r433|%p52, %r429, %r431, %r430, %r432;
	mov.b32 	%f105, %r433;
	add.f32 	%f106, %f4, %f105;
	mov.b32 	%r434, %f106;
	mov.u32 	%r435, 8;
	shfl.sync.bfly.b32 	%r436|%p53, %r434, %r435, %r430, %r432;
	mov.b32 	%f107, %r436;
	add.f32 	%f108, %f106, %f107;
	mov.b32 	%r437, %f108;
	mov.u32 	%r438, 4;
	shfl.sync.bfly.b32 	%r439|%p54, %r437, %r438, %r430, %r432;
	mov.b32 	%f109, %r439;
	add.f32 	%f110, %f108, %f109;
	mov.b32 	%r440, %f110;
	mov.u32 	%r441, 2;
	shfl.sync.bfly.b32 	%r442|%p55, %r440, %r441, %r430, %r432;
	mov.b32 	%f111, %r442;
	add.f32 	%f112, %f110, %f111;
	mov.b32 	%r443, %f112;
	mov.u32 	%r444, 1;
	shfl.sync.bfly.b32 	%r445|%p56, %r443, %r444, %r430, %r432;
	mov.b32 	%f113, %r445;
	add.f32 	%f114, %f112, %f113;
	st.local.f32 	[%rd3+16], %f114;
	st.shared.f32 	[%r51], %f114;
	bar.sync 	0;
	@%p1 bra 	$L__BB101_19;

	ld.shared.f32 	%f115, [%r4];
	mov.b32 	%r446, %f115;
	shfl.sync.bfly.b32 	%r450|%p57, %r446, %r431, %r430, %r432;
	mov.b32 	%f116, %r450;
	add.f32 	%f117, %f115, %f116;
	mov.b32 	%r451, %f117;
	shfl.sync.bfly.b32 	%r453|%p58, %r451, %r435, %r430, %r432;
	mov.b32 	%f118, %r453;
	add.f32 	%f119, %f117, %f118;
	mov.b32 	%r454, %f119;
	shfl.sync.bfly.b32 	%r456|%p59, %r454, %r438, %r430, %r432;
	mov.b32 	%f120, %r456;
	add.f32 	%f121, %f119, %f120;
	mov.b32 	%r457, %f121;
	shfl.sync.bfly.b32 	%r459|%p60, %r457, %r441, %r430, %r432;
	mov.b32 	%f122, %r459;
	add.f32 	%f123, %f121, %f122;
	mov.b32 	%r460, %f123;
	shfl.sync.bfly.b32 	%r462|%p61, %r460, %r444, %r430, %r432;
	mov.b32 	%f124, %r462;
	add.f32 	%f125, %f123, %f124;
	st.local.f32 	[%rd3+16], %f125;

$L__BB101_19:
	bar.sync 	0;
	mov.b32 	%r463, %f5;
	shfl.sync.bfly.b32 	%r467|%p63, %r463, %r431, %r430, %r432;
	mov.b32 	%f126, %r467;
	add.f32 	%f127, %f5, %f126;
	mov.b32 	%r468, %f127;
	shfl.sync.bfly.b32 	%r470|%p64, %r468, %r435, %r430, %r432;
	mov.b32 	%f128, %r470;
	add.f32 	%f129, %f127, %f128;
	mov.b32 	%r471, %f129;
	shfl.sync.bfly.b32 	%r473|%p65, %r471, %r438, %r430, %r432;
	mov.b32 	%f130, %r473;
	add.f32 	%f131, %f129, %f130;
	mov.b32 	%r474, %f131;
	shfl.sync.bfly.b32 	%r476|%p66, %r474, %r441, %r430, %r432;
	mov.b32 	%f132, %r476;
	add.f32 	%f133, %f131, %f132;
	mov.b32 	%r477, %f133;
	shfl.sync.bfly.b32 	%r479|%p67, %r477, %r444, %r430, %r432;
	mov.b32 	%f134, %r479;
	add.f32 	%f135, %f133, %f134;
	st.local.f32 	[%rd3+20], %f135;
	st.shared.f32 	[%r51], %f135;
	bar.sync 	0;
	@%p1 bra 	$L__BB101_21;

	ld.shared.f32 	%f136, [%r4];
	mov.b32 	%r480, %f136;
	mov.u32 	%r481, 31;
	mov.u32 	%r482, 16;
	mov.u32 	%r483, -1;
	shfl.sync.bfly.b32 	%r484|%p68, %r480, %r482, %r481, %r483;
	mov.b32 	%f137, %r484;
	add.f32 	%f138, %f136, %f137;
	mov.b32 	%r485, %f138;
	mov.u32 	%r486, 8;
	shfl.sync.bfly.b32 	%r487|%p69, %r485, %r486, %r481, %r483;
	mov.b32 	%f139, %r487;
	add.f32 	%f140, %f138, %f139;
	mov.b32 	%r488, %f140;
	mov.u32 	%r489, 4;
	shfl.sync.bfly.b32 	%r490|%p70, %r488, %r489, %r481, %r483;
	mov.b32 	%f141, %r490;
	add.f32 	%f142, %f140, %f141;
	mov.b32 	%r491, %f142;
	mov.u32 	%r492, 2;
	shfl.sync.bfly.b32 	%r493|%p71, %r491, %r492, %r481, %r483;
	mov.b32 	%f143, %r493;
	add.f32 	%f144, %f142, %f143;
	mov.b32 	%r494, %f144;
	mov.u32 	%r495, 1;
	shfl.sync.bfly.b32 	%r496|%p72, %r494, %r495, %r481, %r483;
	mov.b32 	%f145, %r496;
	add.f32 	%f146, %f144, %f145;
	st.local.f32 	[%rd3+20], %f146;

$L__BB101_21:
	bar.sync 	0;
	setp.gt.s32 	%p73, %r3, 5;
	@%p73 bra 	$L__BB101_23;

	mad.lo.s32 	%r497, %r3, %r54, %r2;
	cvt.s64.s32 	%rd67, %r497;
	mul.lo.s32 	%r498, %r1, %r55;
	cvt.s64.s32 	%rd68, %r498;
	add.s64 	%rd69, %rd68, %rd67;
	mul.wide.s32 	%rd70, %r3, 4;
	add.s64 	%rd71, %rd3, %rd70;
	ld.local.f32 	%f147, [%rd71];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs3, %f147;}

	// end inline asm
	cvta.to.global.u64 	%rd72, %rd28;
	shl.b64 	%rd73, %rd69, 1;
	add.s64 	%rd74, %rd72, %rd73;
	st.global.u16 	[%rd74], %rs3;

$L__BB101_23:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_7_bs_160
.visible .entry ggml_matvec_f16_ncols_7_bs_160(
	.param .u64 ggml_matvec_f16_ncols_7_bs_160_param_0,
	.param .u64 ggml_matvec_f16_ncols_7_bs_160_param_1,
	.param .u64 ggml_matvec_f16_ncols_7_bs_160_param_2,
	.param .u32 ggml_matvec_f16_ncols_7_bs_160_param_3,
	.param .u32 ggml_matvec_f16_ncols_7_bs_160_param_4,
	.param .u32 ggml_matvec_f16_ncols_7_bs_160_param_5,
	.param .u32 ggml_matvec_f16_ncols_7_bs_160_param_6,
	.param .u32 ggml_matvec_f16_ncols_7_bs_160_param_7,
	.param .u32 ggml_matvec_f16_ncols_7_bs_160_param_8,
	.param .u32 ggml_matvec_f16_ncols_7_bs_160_param_9,
	.param .u32 ggml_matvec_f16_ncols_7_bs_160_param_10,
	.param .u32 ggml_matvec_f16_ncols_7_bs_160_param_11
)
{
	.local .align 4 .b8 	__local_depot102[28];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<85>;
	.reg .b16 	%rs<4>;
	.reg .f32 	%f<172>;
	.reg .b32 	%r<607>;
	.reg .b64 	%rd<83>;


	mov.u64 	%SPL, __local_depot102;
	ld.param.u64 	%rd30, [ggml_matvec_f16_ncols_7_bs_160_param_0];
	ld.param.u64 	%rd31, [ggml_matvec_f16_ncols_7_bs_160_param_1];
	ld.param.u64 	%rd29, [ggml_matvec_f16_ncols_7_bs_160_param_2];
	ld.param.u32 	%r58, [ggml_matvec_f16_ncols_7_bs_160_param_3];
	ld.param.u32 	%r62, [ggml_matvec_f16_ncols_7_bs_160_param_5];
	ld.param.u32 	%r59, [ggml_matvec_f16_ncols_7_bs_160_param_6];
	ld.param.u32 	%r60, [ggml_matvec_f16_ncols_7_bs_160_param_7];
	ld.param.u32 	%r63, [ggml_matvec_f16_ncols_7_bs_160_param_8];
	ld.param.u32 	%r64, [ggml_matvec_f16_ncols_7_bs_160_param_9];
	ld.param.u32 	%r65, [ggml_matvec_f16_ncols_7_bs_160_param_10];
	ld.param.u32 	%r61, [ggml_matvec_f16_ncols_7_bs_160_param_11];
	cvta.to.global.u64 	%rd82, %rd31;
	cvta.to.global.u64 	%rd2, %rd30;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r66, %r1, %r63;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r67, %r2, %r62;
	mad.lo.s32 	%r68, %r66, %r64, %r67;
	cvt.s64.s32 	%rd4, %r68;
	mul.lo.s32 	%r69, %r1, %r65;
	cvt.s64.s32 	%rd5, %r69;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r70, %r3, 2;
	mov.u32 	%r71, data_mmv;
	add.s32 	%r4, %r71, %r70;
	@%p1 bra 	$L__BB102_2;

	mov.u32 	%r72, 0;
	st.shared.u32 	[%r4], %r72;

$L__BB102_2:
	bar.sync 	0;
	mov.f32 	%f8, 0f00000000;
	mov.u32 	%r600, 0;
	st.local.u32 	[%rd3], %r600;
	st.local.u32 	[%rd3+4], %r600;
	st.local.u32 	[%rd3+8], %r600;
	st.local.u32 	[%rd3+12], %r600;
	st.local.u32 	[%rd3+16], %r600;
	st.local.u32 	[%rd3+20], %r600;
	st.local.u32 	[%rd3+24], %r600;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs2, %f8;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f8;}

	// end inline asm
	mov.b32 	%r606, {%rs1, %rs2};
	setp.ge.s32 	%p2, %r3, %r58;
	mov.u32 	%r601, %r600;
	mov.u32 	%r602, %r600;
	mov.u32 	%r603, %r600;
	mov.u32 	%r604, %r600;
	mov.u32 	%r605, %r600;
	@%p2 bra 	$L__BB102_9;

	not.b32 	%r85, %r3;
	add.s32 	%r6, %r85, %r58;
	mul.wide.u32 	%rd33, %r6, -858993459;
	shr.u64 	%rd34, %rd33, 39;
	cvt.u32.u64 	%r86, %rd34;
	add.s32 	%r87, %r86, 1;
	and.b32  	%r583, %r87, 3;
	setp.eq.s32 	%p3, %r583, 0;
	mov.u32 	%r600, 0;
	mov.u32 	%r591, %r3;
	@%p3 bra 	$L__BB102_6;

	shl.b32 	%r94, %r59, 1;
	add.s32 	%r95, %r3, %r94;
	mul.wide.s32 	%rd35, %r95, 4;
	shl.b64 	%rd36, %rd5, 1;
	add.s64 	%rd7, %rd35, %rd36;
	mul.wide.s32 	%rd37, %r3, 4;
	mul.wide.s32 	%rd8, %r59, 4;
	add.s64 	%rd38, %rd37, %rd8;
	add.s64 	%rd9, %rd38, %rd36;
	add.s64 	%rd10, %rd37, %rd36;
	mul.wide.s32 	%rd39, %r3, 2;
	add.s64 	%rd40, %rd39, %rd4;
	shl.b64 	%rd41, %rd40, 1;
	add.s64 	%rd79, %rd2, %rd41;
	mov.u32 	%r600, 0;
	mov.u64 	%rd80, %rd82;
	mov.u32 	%r601, %r600;
	mov.u32 	%r602, %r600;
	mov.u32 	%r603, %r600;
	mov.u32 	%r604, %r600;
	mov.u32 	%r605, %r600;
	mov.u32 	%r591, %r3;

$L__BB102_5:
	.pragma "nounroll";
	ld.global.nc.u32 	%r97, [%rd79];
	add.s64 	%rd42, %rd80, %rd10;
	ld.global.nc.u32 	%r98, [%rd42];
	// begin inline asm
	{mul.f16x2 %r96,%r97,%r98;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r606,%r606,%r96;
}
	// end inline asm
	add.s64 	%rd43, %rd80, %rd9;
	ld.global.nc.u32 	%r104, [%rd43];
	// begin inline asm
	{mul.f16x2 %r102,%r97,%r104;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r605,%r605,%r102;
}
	// end inline asm
	add.s64 	%rd44, %rd80, %rd7;
	ld.global.nc.u32 	%r110, [%rd44];
	// begin inline asm
	{mul.f16x2 %r108,%r97,%r110;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r604,%r604,%r108;
}
	// end inline asm
	add.s64 	%rd45, %rd44, %rd8;
	ld.global.nc.u32 	%r116, [%rd45];
	// begin inline asm
	{mul.f16x2 %r114,%r97,%r116;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r603,%r603,%r114;
}
	// end inline asm
	add.s64 	%rd46, %rd45, %rd8;
	ld.global.nc.u32 	%r122, [%rd46];
	// begin inline asm
	{mul.f16x2 %r120,%r97,%r122;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r602,%r602,%r120;
}
	// end inline asm
	add.s64 	%rd47, %rd46, %rd8;
	ld.global.nc.u32 	%r128, [%rd47];
	// begin inline asm
	{mul.f16x2 %r126,%r97,%r128;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r601,%r601,%r126;
}
	// end inline asm
	add.s64 	%rd48, %rd47, %rd8;
	ld.global.nc.u32 	%r134, [%rd48];
	// begin inline asm
	{mul.f16x2 %r132,%r97,%r134;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r600,%r600,%r132;
}
	// end inline asm
	add.s32 	%r591, %r591, 160;
	add.s64 	%rd80, %rd80, 640;
	add.s64 	%rd79, %rd79, 640;
	add.s32 	%r583, %r583, -1;
	setp.ne.s32 	%p4, %r583, 0;
	@%p4 bra 	$L__BB102_5;

$L__BB102_6:
	setp.lt.u32 	%p5, %r6, 480;
	@%p5 bra 	$L__BB102_9;

	add.s32 	%r138, %r591, %r59;
	shl.b32 	%r139, %r59, 1;
	add.s32 	%r140, %r591, %r139;
	mad.lo.s32 	%r141, %r59, 3, %r591;
	shl.b32 	%r142, %r59, 2;
	add.s32 	%r143, %r591, %r142;
	mad.lo.s32 	%r144, %r59, 5, %r591;
	mad.lo.s32 	%r145, %r59, 6, %r591;
	add.s32 	%r146, %r138, 160;
	mul.wide.s32 	%rd49, %r146, 4;
	shl.b64 	%rd50, %rd5, 1;
	add.s64 	%rd16, %rd49, %rd50;
	mul.wide.s32 	%rd51, %r140, 4;
	add.s64 	%rd17, %rd51, %rd50;
	mul.wide.s32 	%rd52, %r141, 4;
	add.s64 	%rd18, %rd52, %rd50;
	mul.wide.s32 	%rd53, %r143, 4;
	add.s64 	%rd19, %rd53, %rd50;
	mul.wide.s32 	%rd54, %r144, 4;
	add.s64 	%rd20, %rd54, %rd50;
	mul.wide.s32 	%rd55, %r145, 4;
	add.s64 	%rd21, %rd55, %rd50;
	mul.wide.s32 	%rd56, %r591, 2;
	add.s64 	%rd57, %rd56, %rd4;
	shl.b64 	%rd58, %rd57, 1;
	add.s64 	%rd59, %rd2, %rd58;
	add.s64 	%rd81, %rd59, 1280;
	mul.wide.s32 	%rd60, %r591, 4;
	add.s64 	%rd23, %rd60, %rd50;
	mul.wide.s32 	%rd61, %r59, 4;
	add.s64 	%rd62, %rd60, %rd61;
	add.s64 	%rd24, %rd62, %rd50;

$L__BB102_8:
	ld.global.nc.u32 	%r148, [%rd81+-1280];
	add.s64 	%rd63, %rd82, %rd23;
	ld.global.nc.u32 	%r149, [%rd63];
	// begin inline asm
	{mul.f16x2 %r147,%r148,%r149;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r150,%r606,%r147;
}
	// end inline asm
	add.s64 	%rd64, %rd82, %rd24;
	ld.global.nc.u32 	%r155, [%rd64];
	// begin inline asm
	{mul.f16x2 %r153,%r148,%r155;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r156,%r605,%r153;
}
	// end inline asm
	add.s64 	%rd65, %rd82, %rd17;
	ld.global.nc.u32 	%r161, [%rd65];
	// begin inline asm
	{mul.f16x2 %r159,%r148,%r161;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r162,%r604,%r159;
}
	// end inline asm
	add.s64 	%rd66, %rd82, %rd18;
	ld.global.nc.u32 	%r167, [%rd66];
	// begin inline asm
	{mul.f16x2 %r165,%r148,%r167;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r168,%r603,%r165;
}
	// end inline asm
	add.s64 	%rd67, %rd82, %rd19;
	ld.global.nc.u32 	%r173, [%rd67];
	// begin inline asm
	{mul.f16x2 %r171,%r148,%r173;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r174,%r602,%r171;
}
	// end inline asm
	add.s64 	%rd68, %rd82, %rd20;
	ld.global.nc.u32 	%r179, [%rd68];
	// begin inline asm
	{mul.f16x2 %r177,%r148,%r179;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r180,%r601,%r177;
}
	// end inline asm
	add.s64 	%rd69, %rd82, %rd21;
	ld.global.nc.u32 	%r185, [%rd69];
	// begin inline asm
	{mul.f16x2 %r183,%r148,%r185;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r186,%r600,%r183;
}
	// end inline asm
	ld.global.nc.u32 	%r190, [%rd81+-640];
	ld.global.nc.u32 	%r191, [%rd63+640];
	// begin inline asm
	{mul.f16x2 %r189,%r190,%r191;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r192,%r150,%r189;
}
	// end inline asm
	add.s64 	%rd70, %rd82, %rd16;
	ld.global.nc.u32 	%r197, [%rd70];
	// begin inline asm
	{mul.f16x2 %r195,%r190,%r197;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r198,%r156,%r195;
}
	// end inline asm
	ld.global.nc.u32 	%r203, [%rd65+640];
	// begin inline asm
	{mul.f16x2 %r201,%r190,%r203;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r204,%r162,%r201;
}
	// end inline asm
	ld.global.nc.u32 	%r209, [%rd66+640];
	// begin inline asm
	{mul.f16x2 %r207,%r190,%r209;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r210,%r168,%r207;
}
	// end inline asm
	ld.global.nc.u32 	%r215, [%rd67+640];
	// begin inline asm
	{mul.f16x2 %r213,%r190,%r215;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r216,%r174,%r213;
}
	// end inline asm
	ld.global.nc.u32 	%r221, [%rd68+640];
	// begin inline asm
	{mul.f16x2 %r219,%r190,%r221;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r222,%r180,%r219;
}
	// end inline asm
	ld.global.nc.u32 	%r227, [%rd69+640];
	// begin inline asm
	{mul.f16x2 %r225,%r190,%r227;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r228,%r186,%r225;
}
	// end inline asm
	ld.global.nc.u32 	%r232, [%rd81];
	ld.global.nc.u32 	%r233, [%rd63+1280];
	// begin inline asm
	{mul.f16x2 %r231,%r232,%r233;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r234,%r192,%r231;
}
	// end inline asm
	ld.global.nc.u32 	%r239, [%rd70+640];
	// begin inline asm
	{mul.f16x2 %r237,%r232,%r239;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r240,%r198,%r237;
}
	// end inline asm
	ld.global.nc.u32 	%r245, [%rd65+1280];
	// begin inline asm
	{mul.f16x2 %r243,%r232,%r245;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r246,%r204,%r243;
}
	// end inline asm
	ld.global.nc.u32 	%r251, [%rd66+1280];
	// begin inline asm
	{mul.f16x2 %r249,%r232,%r251;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r252,%r210,%r249;
}
	// end inline asm
	ld.global.nc.u32 	%r257, [%rd67+1280];
	// begin inline asm
	{mul.f16x2 %r255,%r232,%r257;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r258,%r216,%r255;
}
	// end inline asm
	ld.global.nc.u32 	%r263, [%rd68+1280];
	// begin inline asm
	{mul.f16x2 %r261,%r232,%r263;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r264,%r222,%r261;
}
	// end inline asm
	ld.global.nc.u32 	%r269, [%rd69+1280];
	// begin inline asm
	{mul.f16x2 %r267,%r232,%r269;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r270,%r228,%r267;
}
	// end inline asm
	ld.global.nc.u32 	%r274, [%rd81+640];
	ld.global.nc.u32 	%r275, [%rd63+1920];
	// begin inline asm
	{mul.f16x2 %r273,%r274,%r275;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r606,%r234,%r273;
}
	// end inline asm
	ld.global.nc.u32 	%r281, [%rd70+1280];
	// begin inline asm
	{mul.f16x2 %r279,%r274,%r281;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r605,%r240,%r279;
}
	// end inline asm
	ld.global.nc.u32 	%r287, [%rd65+1920];
	// begin inline asm
	{mul.f16x2 %r285,%r274,%r287;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r604,%r246,%r285;
}
	// end inline asm
	ld.global.nc.u32 	%r293, [%rd66+1920];
	// begin inline asm
	{mul.f16x2 %r291,%r274,%r293;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r603,%r252,%r291;
}
	// end inline asm
	ld.global.nc.u32 	%r299, [%rd67+1920];
	// begin inline asm
	{mul.f16x2 %r297,%r274,%r299;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r602,%r258,%r297;
}
	// end inline asm
	ld.global.nc.u32 	%r305, [%rd68+1920];
	// begin inline asm
	{mul.f16x2 %r303,%r274,%r305;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r601,%r264,%r303;
}
	// end inline asm
	ld.global.nc.u32 	%r311, [%rd69+1920];
	// begin inline asm
	{mul.f16x2 %r309,%r274,%r311;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r600,%r270,%r309;
}
	// end inline asm
	add.s64 	%rd82, %rd82, 2560;
	add.s64 	%rd81, %rd81, 2560;
	add.s32 	%r591, %r591, 640;
	setp.lt.s32 	%p6, %r591, %r58;
	@%p6 bra 	$L__BB102_8;

$L__BB102_9:
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r606;
  cvt.f32.f16 %f9, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r606;
  cvt.f32.f16 %f10, high;}

	// end inline asm
	add.f32 	%f23, %f9, %f10;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r605;
  cvt.f32.f16 %f11, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r605;
  cvt.f32.f16 %f12, high;}

	// end inline asm
	add.f32 	%f1, %f11, %f12;
	st.local.f32 	[%rd3+4], %f1;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r604;
  cvt.f32.f16 %f13, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r604;
  cvt.f32.f16 %f14, high;}

	// end inline asm
	add.f32 	%f2, %f13, %f14;
	st.local.f32 	[%rd3+8], %f2;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r603;
  cvt.f32.f16 %f15, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r603;
  cvt.f32.f16 %f16, high;}

	// end inline asm
	add.f32 	%f3, %f15, %f16;
	st.local.f32 	[%rd3+12], %f3;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r602;
  cvt.f32.f16 %f17, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r602;
  cvt.f32.f16 %f18, high;}

	// end inline asm
	add.f32 	%f4, %f17, %f18;
	st.local.f32 	[%rd3+16], %f4;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r601;
  cvt.f32.f16 %f19, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r601;
  cvt.f32.f16 %f20, high;}

	// end inline asm
	add.f32 	%f5, %f19, %f20;
	st.local.f32 	[%rd3+20], %f5;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r600;
  cvt.f32.f16 %f21, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r600;
  cvt.f32.f16 %f22, high;}

	// end inline asm
	add.f32 	%f6, %f21, %f22;
	st.local.f32 	[%rd3+24], %f6;
	shr.s32 	%r329, %r3, 31;
	shr.u32 	%r330, %r329, 27;
	add.s32 	%r331, %r3, %r330;
	shr.s32 	%r332, %r331, 5;
	shl.b32 	%r333, %r332, 2;
	add.s32 	%r57, %r71, %r333;
	mov.u32 	%r335, 2;
	mov.b32 	%r336, %f23;
	mov.u32 	%r337, 31;
	mov.u32 	%r338, 16;
	mov.u32 	%r339, -1;
	shfl.sync.bfly.b32 	%r340|%p7, %r336, %r338, %r337, %r339;
	mov.b32 	%f24, %r340;
	add.f32 	%f25, %f23, %f24;
	mov.b32 	%r341, %f25;
	mov.u32 	%r342, 8;
	shfl.sync.bfly.b32 	%r343|%p8, %r341, %r342, %r337, %r339;
	mov.b32 	%f26, %r343;
	add.f32 	%f27, %f25, %f26;
	mov.b32 	%r344, %f27;
	mov.u32 	%r345, 4;
	shfl.sync.bfly.b32 	%r346|%p9, %r344, %r345, %r337, %r339;
	mov.b32 	%f28, %r346;
	add.f32 	%f29, %f27, %f28;
	mov.b32 	%r347, %f29;
	shfl.sync.bfly.b32 	%r348|%p10, %r347, %r335, %r337, %r339;
	mov.b32 	%f30, %r348;
	add.f32 	%f31, %f29, %f30;
	mov.b32 	%r349, %f31;
	mov.u32 	%r350, 1;
	shfl.sync.bfly.b32 	%r351|%p11, %r349, %r350, %r337, %r339;
	mov.b32 	%f32, %r351;
	add.f32 	%f33, %f31, %f32;
	st.local.f32 	[%rd3], %f33;
	st.shared.f32 	[%r57], %f33;
	bar.sync 	0;
	@%p1 bra 	$L__BB102_11;

	ld.shared.f32 	%f34, [%r4];
	mov.b32 	%r352, %f34;
	shfl.sync.bfly.b32 	%r356|%p13, %r352, %r338, %r337, %r339;
	mov.b32 	%f35, %r356;
	add.f32 	%f36, %f34, %f35;
	mov.b32 	%r357, %f36;
	shfl.sync.bfly.b32 	%r359|%p14, %r357, %r342, %r337, %r339;
	mov.b32 	%f37, %r359;
	add.f32 	%f38, %f36, %f37;
	mov.b32 	%r360, %f38;
	shfl.sync.bfly.b32 	%r362|%p15, %r360, %r345, %r337, %r339;
	mov.b32 	%f39, %r362;
	add.f32 	%f40, %f38, %f39;
	mov.b32 	%r363, %f40;
	shfl.sync.bfly.b32 	%r365|%p16, %r363, %r335, %r337, %r339;
	mov.b32 	%f41, %r365;
	add.f32 	%f42, %f40, %f41;
	mov.b32 	%r366, %f42;
	shfl.sync.bfly.b32 	%r368|%p17, %r366, %r350, %r337, %r339;
	mov.b32 	%f43, %r368;
	add.f32 	%f44, %f42, %f43;
	st.local.f32 	[%rd3], %f44;

$L__BB102_11:
	bar.sync 	0;
	mov.b32 	%r369, %f1;
	shfl.sync.bfly.b32 	%r373|%p19, %r369, %r338, %r337, %r339;
	mov.b32 	%f45, %r373;
	add.f32 	%f46, %f1, %f45;
	mov.b32 	%r374, %f46;
	shfl.sync.bfly.b32 	%r376|%p20, %r374, %r342, %r337, %r339;
	mov.b32 	%f47, %r376;
	add.f32 	%f48, %f46, %f47;
	mov.b32 	%r377, %f48;
	shfl.sync.bfly.b32 	%r379|%p21, %r377, %r345, %r337, %r339;
	mov.b32 	%f49, %r379;
	add.f32 	%f50, %f48, %f49;
	mov.b32 	%r380, %f50;
	shfl.sync.bfly.b32 	%r382|%p22, %r380, %r335, %r337, %r339;
	mov.b32 	%f51, %r382;
	add.f32 	%f52, %f50, %f51;
	mov.b32 	%r383, %f52;
	shfl.sync.bfly.b32 	%r385|%p23, %r383, %r350, %r337, %r339;
	mov.b32 	%f53, %r385;
	add.f32 	%f54, %f52, %f53;
	st.local.f32 	[%rd3+4], %f54;
	st.shared.f32 	[%r57], %f54;
	bar.sync 	0;
	@%p1 bra 	$L__BB102_13;

	ld.shared.f32 	%f55, [%r4];
	mov.b32 	%r386, %f55;
	mov.u32 	%r387, 31;
	mov.u32 	%r388, 16;
	mov.u32 	%r389, -1;
	shfl.sync.bfly.b32 	%r390|%p24, %r386, %r388, %r387, %r389;
	mov.b32 	%f56, %r390;
	add.f32 	%f57, %f55, %f56;
	mov.b32 	%r391, %f57;
	mov.u32 	%r392, 8;
	shfl.sync.bfly.b32 	%r393|%p25, %r391, %r392, %r387, %r389;
	mov.b32 	%f58, %r393;
	add.f32 	%f59, %f57, %f58;
	mov.b32 	%r394, %f59;
	mov.u32 	%r395, 4;
	shfl.sync.bfly.b32 	%r396|%p26, %r394, %r395, %r387, %r389;
	mov.b32 	%f60, %r396;
	add.f32 	%f61, %f59, %f60;
	mov.b32 	%r397, %f61;
	mov.u32 	%r398, 2;
	shfl.sync.bfly.b32 	%r399|%p27, %r397, %r398, %r387, %r389;
	mov.b32 	%f62, %r399;
	add.f32 	%f63, %f61, %f62;
	mov.b32 	%r400, %f63;
	mov.u32 	%r401, 1;
	shfl.sync.bfly.b32 	%r402|%p28, %r400, %r401, %r387, %r389;
	mov.b32 	%f64, %r402;
	add.f32 	%f65, %f63, %f64;
	st.local.f32 	[%rd3+4], %f65;

$L__BB102_13:
	bar.sync 	0;
	mov.b32 	%r403, %f2;
	mov.u32 	%r404, 31;
	mov.u32 	%r405, 16;
	mov.u32 	%r406, -1;
	shfl.sync.bfly.b32 	%r407|%p30, %r403, %r405, %r404, %r406;
	mov.b32 	%f66, %r407;
	add.f32 	%f67, %f2, %f66;
	mov.b32 	%r408, %f67;
	mov.u32 	%r409, 8;
	shfl.sync.bfly.b32 	%r410|%p31, %r408, %r409, %r404, %r406;
	mov.b32 	%f68, %r410;
	add.f32 	%f69, %f67, %f68;
	mov.b32 	%r411, %f69;
	mov.u32 	%r412, 4;
	shfl.sync.bfly.b32 	%r413|%p32, %r411, %r412, %r404, %r406;
	mov.b32 	%f70, %r413;
	add.f32 	%f71, %f69, %f70;
	mov.b32 	%r414, %f71;
	mov.u32 	%r415, 2;
	shfl.sync.bfly.b32 	%r416|%p33, %r414, %r415, %r404, %r406;
	mov.b32 	%f72, %r416;
	add.f32 	%f73, %f71, %f72;
	mov.b32 	%r417, %f73;
	mov.u32 	%r418, 1;
	shfl.sync.bfly.b32 	%r419|%p34, %r417, %r418, %r404, %r406;
	mov.b32 	%f74, %r419;
	add.f32 	%f75, %f73, %f74;
	st.local.f32 	[%rd3+8], %f75;
	st.shared.f32 	[%r57], %f75;
	bar.sync 	0;
	@%p1 bra 	$L__BB102_15;

	ld.shared.f32 	%f76, [%r4];
	mov.b32 	%r420, %f76;
	shfl.sync.bfly.b32 	%r424|%p35, %r420, %r405, %r404, %r406;
	mov.b32 	%f77, %r424;
	add.f32 	%f78, %f76, %f77;
	mov.b32 	%r425, %f78;
	shfl.sync.bfly.b32 	%r427|%p36, %r425, %r409, %r404, %r406;
	mov.b32 	%f79, %r427;
	add.f32 	%f80, %f78, %f79;
	mov.b32 	%r428, %f80;
	shfl.sync.bfly.b32 	%r430|%p37, %r428, %r412, %r404, %r406;
	mov.b32 	%f81, %r430;
	add.f32 	%f82, %f80, %f81;
	mov.b32 	%r431, %f82;
	shfl.sync.bfly.b32 	%r433|%p38, %r431, %r415, %r404, %r406;
	mov.b32 	%f83, %r433;
	add.f32 	%f84, %f82, %f83;
	mov.b32 	%r434, %f84;
	shfl.sync.bfly.b32 	%r436|%p39, %r434, %r418, %r404, %r406;
	mov.b32 	%f85, %r436;
	add.f32 	%f86, %f84, %f85;
	st.local.f32 	[%rd3+8], %f86;

$L__BB102_15:
	bar.sync 	0;
	mov.b32 	%r437, %f3;
	shfl.sync.bfly.b32 	%r441|%p41, %r437, %r405, %r404, %r406;
	mov.b32 	%f87, %r441;
	add.f32 	%f88, %f3, %f87;
	mov.b32 	%r442, %f88;
	shfl.sync.bfly.b32 	%r444|%p42, %r442, %r409, %r404, %r406;
	mov.b32 	%f89, %r444;
	add.f32 	%f90, %f88, %f89;
	mov.b32 	%r445, %f90;
	shfl.sync.bfly.b32 	%r447|%p43, %r445, %r412, %r404, %r406;
	mov.b32 	%f91, %r447;
	add.f32 	%f92, %f90, %f91;
	mov.b32 	%r448, %f92;
	shfl.sync.bfly.b32 	%r450|%p44, %r448, %r415, %r404, %r406;
	mov.b32 	%f93, %r450;
	add.f32 	%f94, %f92, %f93;
	mov.b32 	%r451, %f94;
	shfl.sync.bfly.b32 	%r453|%p45, %r451, %r418, %r404, %r406;
	mov.b32 	%f95, %r453;
	add.f32 	%f96, %f94, %f95;
	st.local.f32 	[%rd3+12], %f96;
	st.shared.f32 	[%r57], %f96;
	bar.sync 	0;
	@%p1 bra 	$L__BB102_17;

	ld.shared.f32 	%f97, [%r4];
	mov.b32 	%r454, %f97;
	mov.u32 	%r455, 31;
	mov.u32 	%r456, 16;
	mov.u32 	%r457, -1;
	shfl.sync.bfly.b32 	%r458|%p46, %r454, %r456, %r455, %r457;
	mov.b32 	%f98, %r458;
	add.f32 	%f99, %f97, %f98;
	mov.b32 	%r459, %f99;
	mov.u32 	%r460, 8;
	shfl.sync.bfly.b32 	%r461|%p47, %r459, %r460, %r455, %r457;
	mov.b32 	%f100, %r461;
	add.f32 	%f101, %f99, %f100;
	mov.b32 	%r462, %f101;
	mov.u32 	%r463, 4;
	shfl.sync.bfly.b32 	%r464|%p48, %r462, %r463, %r455, %r457;
	mov.b32 	%f102, %r464;
	add.f32 	%f103, %f101, %f102;
	mov.b32 	%r465, %f103;
	mov.u32 	%r466, 2;
	shfl.sync.bfly.b32 	%r467|%p49, %r465, %r466, %r455, %r457;
	mov.b32 	%f104, %r467;
	add.f32 	%f105, %f103, %f104;
	mov.b32 	%r468, %f105;
	mov.u32 	%r469, 1;
	shfl.sync.bfly.b32 	%r470|%p50, %r468, %r469, %r455, %r457;
	mov.b32 	%f106, %r470;
	add.f32 	%f107, %f105, %f106;
	st.local.f32 	[%rd3+12], %f107;

$L__BB102_17:
	bar.sync 	0;
	mov.b32 	%r471, %f4;
	mov.u32 	%r472, 31;
	mov.u32 	%r473, 16;
	mov.u32 	%r474, -1;
	shfl.sync.bfly.b32 	%r475|%p52, %r471, %r473, %r472, %r474;
	mov.b32 	%f108, %r475;
	add.f32 	%f109, %f4, %f108;
	mov.b32 	%r476, %f109;
	mov.u32 	%r477, 8;
	shfl.sync.bfly.b32 	%r478|%p53, %r476, %r477, %r472, %r474;
	mov.b32 	%f110, %r478;
	add.f32 	%f111, %f109, %f110;
	mov.b32 	%r479, %f111;
	mov.u32 	%r480, 4;
	shfl.sync.bfly.b32 	%r481|%p54, %r479, %r480, %r472, %r474;
	mov.b32 	%f112, %r481;
	add.f32 	%f113, %f111, %f112;
	mov.b32 	%r482, %f113;
	mov.u32 	%r483, 2;
	shfl.sync.bfly.b32 	%r484|%p55, %r482, %r483, %r472, %r474;
	mov.b32 	%f114, %r484;
	add.f32 	%f115, %f113, %f114;
	mov.b32 	%r485, %f115;
	mov.u32 	%r486, 1;
	shfl.sync.bfly.b32 	%r487|%p56, %r485, %r486, %r472, %r474;
	mov.b32 	%f116, %r487;
	add.f32 	%f117, %f115, %f116;
	st.local.f32 	[%rd3+16], %f117;
	st.shared.f32 	[%r57], %f117;
	bar.sync 	0;
	@%p1 bra 	$L__BB102_19;

	ld.shared.f32 	%f118, [%r4];
	mov.b32 	%r488, %f118;
	shfl.sync.bfly.b32 	%r492|%p57, %r488, %r473, %r472, %r474;
	mov.b32 	%f119, %r492;
	add.f32 	%f120, %f118, %f119;
	mov.b32 	%r493, %f120;
	shfl.sync.bfly.b32 	%r495|%p58, %r493, %r477, %r472, %r474;
	mov.b32 	%f121, %r495;
	add.f32 	%f122, %f120, %f121;
	mov.b32 	%r496, %f122;
	shfl.sync.bfly.b32 	%r498|%p59, %r496, %r480, %r472, %r474;
	mov.b32 	%f123, %r498;
	add.f32 	%f124, %f122, %f123;
	mov.b32 	%r499, %f124;
	shfl.sync.bfly.b32 	%r501|%p60, %r499, %r483, %r472, %r474;
	mov.b32 	%f125, %r501;
	add.f32 	%f126, %f124, %f125;
	mov.b32 	%r502, %f126;
	shfl.sync.bfly.b32 	%r504|%p61, %r502, %r486, %r472, %r474;
	mov.b32 	%f127, %r504;
	add.f32 	%f128, %f126, %f127;
	st.local.f32 	[%rd3+16], %f128;

$L__BB102_19:
	bar.sync 	0;
	mov.b32 	%r505, %f5;
	shfl.sync.bfly.b32 	%r509|%p63, %r505, %r473, %r472, %r474;
	mov.b32 	%f129, %r509;
	add.f32 	%f130, %f5, %f129;
	mov.b32 	%r510, %f130;
	shfl.sync.bfly.b32 	%r512|%p64, %r510, %r477, %r472, %r474;
	mov.b32 	%f131, %r512;
	add.f32 	%f132, %f130, %f131;
	mov.b32 	%r513, %f132;
	shfl.sync.bfly.b32 	%r515|%p65, %r513, %r480, %r472, %r474;
	mov.b32 	%f133, %r515;
	add.f32 	%f134, %f132, %f133;
	mov.b32 	%r516, %f134;
	shfl.sync.bfly.b32 	%r518|%p66, %r516, %r483, %r472, %r474;
	mov.b32 	%f135, %r518;
	add.f32 	%f136, %f134, %f135;
	mov.b32 	%r519, %f136;
	shfl.sync.bfly.b32 	%r521|%p67, %r519, %r486, %r472, %r474;
	mov.b32 	%f137, %r521;
	add.f32 	%f138, %f136, %f137;
	st.local.f32 	[%rd3+20], %f138;
	st.shared.f32 	[%r57], %f138;
	bar.sync 	0;
	@%p1 bra 	$L__BB102_21;

	ld.shared.f32 	%f139, [%r4];
	mov.b32 	%r522, %f139;
	mov.u32 	%r523, 31;
	mov.u32 	%r524, 16;
	mov.u32 	%r525, -1;
	shfl.sync.bfly.b32 	%r526|%p68, %r522, %r524, %r523, %r525;
	mov.b32 	%f140, %r526;
	add.f32 	%f141, %f139, %f140;
	mov.b32 	%r527, %f141;
	mov.u32 	%r528, 8;
	shfl.sync.bfly.b32 	%r529|%p69, %r527, %r528, %r523, %r525;
	mov.b32 	%f142, %r529;
	add.f32 	%f143, %f141, %f142;
	mov.b32 	%r530, %f143;
	mov.u32 	%r531, 4;
	shfl.sync.bfly.b32 	%r532|%p70, %r530, %r531, %r523, %r525;
	mov.b32 	%f144, %r532;
	add.f32 	%f145, %f143, %f144;
	mov.b32 	%r533, %f145;
	mov.u32 	%r534, 2;
	shfl.sync.bfly.b32 	%r535|%p71, %r533, %r534, %r523, %r525;
	mov.b32 	%f146, %r535;
	add.f32 	%f147, %f145, %f146;
	mov.b32 	%r536, %f147;
	mov.u32 	%r537, 1;
	shfl.sync.bfly.b32 	%r538|%p72, %r536, %r537, %r523, %r525;
	mov.b32 	%f148, %r538;
	add.f32 	%f149, %f147, %f148;
	st.local.f32 	[%rd3+20], %f149;

$L__BB102_21:
	bar.sync 	0;
	mov.b32 	%r539, %f6;
	mov.u32 	%r540, 31;
	mov.u32 	%r541, 16;
	mov.u32 	%r542, -1;
	shfl.sync.bfly.b32 	%r543|%p74, %r539, %r541, %r540, %r542;
	mov.b32 	%f150, %r543;
	add.f32 	%f151, %f6, %f150;
	mov.b32 	%r544, %f151;
	mov.u32 	%r545, 8;
	shfl.sync.bfly.b32 	%r546|%p75, %r544, %r545, %r540, %r542;
	mov.b32 	%f152, %r546;
	add.f32 	%f153, %f151, %f152;
	mov.b32 	%r547, %f153;
	mov.u32 	%r548, 4;
	shfl.sync.bfly.b32 	%r549|%p76, %r547, %r548, %r540, %r542;
	mov.b32 	%f154, %r549;
	add.f32 	%f155, %f153, %f154;
	mov.b32 	%r550, %f155;
	mov.u32 	%r551, 2;
	shfl.sync.bfly.b32 	%r552|%p77, %r550, %r551, %r540, %r542;
	mov.b32 	%f156, %r552;
	add.f32 	%f157, %f155, %f156;
	mov.b32 	%r553, %f157;
	mov.u32 	%r554, 1;
	shfl.sync.bfly.b32 	%r555|%p78, %r553, %r554, %r540, %r542;
	mov.b32 	%f158, %r555;
	add.f32 	%f159, %f157, %f158;
	st.local.f32 	[%rd3+24], %f159;
	st.shared.f32 	[%r57], %f159;
	bar.sync 	0;
	@%p1 bra 	$L__BB102_23;

	ld.shared.f32 	%f160, [%r4];
	mov.b32 	%r556, %f160;
	shfl.sync.bfly.b32 	%r560|%p79, %r556, %r541, %r540, %r542;
	mov.b32 	%f161, %r560;
	add.f32 	%f162, %f160, %f161;
	mov.b32 	%r561, %f162;
	shfl.sync.bfly.b32 	%r563|%p80, %r561, %r545, %r540, %r542;
	mov.b32 	%f163, %r563;
	add.f32 	%f164, %f162, %f163;
	mov.b32 	%r564, %f164;
	shfl.sync.bfly.b32 	%r566|%p81, %r564, %r548, %r540, %r542;
	mov.b32 	%f165, %r566;
	add.f32 	%f166, %f164, %f165;
	mov.b32 	%r567, %f166;
	shfl.sync.bfly.b32 	%r569|%p82, %r567, %r551, %r540, %r542;
	mov.b32 	%f167, %r569;
	add.f32 	%f168, %f166, %f167;
	mov.b32 	%r570, %f168;
	shfl.sync.bfly.b32 	%r572|%p83, %r570, %r554, %r540, %r542;
	mov.b32 	%f169, %r572;
	add.f32 	%f170, %f168, %f169;
	st.local.f32 	[%rd3+24], %f170;

$L__BB102_23:
	bar.sync 	0;
	setp.gt.s32 	%p84, %r3, 6;
	@%p84 bra 	$L__BB102_25;

	mad.lo.s32 	%r573, %r3, %r60, %r2;
	cvt.s64.s32 	%rd71, %r573;
	mul.lo.s32 	%r574, %r1, %r61;
	cvt.s64.s32 	%rd72, %r574;
	add.s64 	%rd73, %rd72, %rd71;
	mul.wide.s32 	%rd74, %r3, 4;
	add.s64 	%rd75, %rd3, %rd74;
	ld.local.f32 	%f171, [%rd75];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs3, %f171;}

	// end inline asm
	cvta.to.global.u64 	%rd76, %rd29;
	shl.b64 	%rd77, %rd73, 1;
	add.s64 	%rd78, %rd76, %rd77;
	st.global.u16 	[%rd78], %rs3;

$L__BB102_25:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_8_bs_160
.visible .entry ggml_matvec_f16_ncols_8_bs_160(
	.param .u64 ggml_matvec_f16_ncols_8_bs_160_param_0,
	.param .u64 ggml_matvec_f16_ncols_8_bs_160_param_1,
	.param .u64 ggml_matvec_f16_ncols_8_bs_160_param_2,
	.param .u32 ggml_matvec_f16_ncols_8_bs_160_param_3,
	.param .u32 ggml_matvec_f16_ncols_8_bs_160_param_4,
	.param .u32 ggml_matvec_f16_ncols_8_bs_160_param_5,
	.param .u32 ggml_matvec_f16_ncols_8_bs_160_param_6,
	.param .u32 ggml_matvec_f16_ncols_8_bs_160_param_7,
	.param .u32 ggml_matvec_f16_ncols_8_bs_160_param_8,
	.param .u32 ggml_matvec_f16_ncols_8_bs_160_param_9,
	.param .u32 ggml_matvec_f16_ncols_8_bs_160_param_10,
	.param .u32 ggml_matvec_f16_ncols_8_bs_160_param_11
)
{
	.local .align 16 .b8 	__local_depot103[32];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<96>;
	.reg .b16 	%rs<4>;
	.reg .f32 	%f<196>;
	.reg .b32 	%r<687>;
	.reg .b64 	%rd<87>;


	mov.u64 	%SPL, __local_depot103;
	ld.param.u64 	%rd31, [ggml_matvec_f16_ncols_8_bs_160_param_0];
	ld.param.u64 	%rd32, [ggml_matvec_f16_ncols_8_bs_160_param_1];
	ld.param.u64 	%rd30, [ggml_matvec_f16_ncols_8_bs_160_param_2];
	ld.param.u32 	%r64, [ggml_matvec_f16_ncols_8_bs_160_param_3];
	ld.param.u32 	%r68, [ggml_matvec_f16_ncols_8_bs_160_param_5];
	ld.param.u32 	%r65, [ggml_matvec_f16_ncols_8_bs_160_param_6];
	ld.param.u32 	%r66, [ggml_matvec_f16_ncols_8_bs_160_param_7];
	ld.param.u32 	%r69, [ggml_matvec_f16_ncols_8_bs_160_param_8];
	ld.param.u32 	%r70, [ggml_matvec_f16_ncols_8_bs_160_param_9];
	ld.param.u32 	%r71, [ggml_matvec_f16_ncols_8_bs_160_param_10];
	ld.param.u32 	%r67, [ggml_matvec_f16_ncols_8_bs_160_param_11];
	cvta.to.global.u64 	%rd86, %rd32;
	cvta.to.global.u64 	%rd2, %rd31;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r72, %r1, %r69;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r73, %r2, %r68;
	mad.lo.s32 	%r74, %r72, %r70, %r73;
	cvt.s64.s32 	%rd4, %r74;
	mul.lo.s32 	%r75, %r1, %r71;
	cvt.s64.s32 	%rd5, %r75;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r76, %r3, 2;
	mov.u32 	%r77, data_mmv;
	add.s32 	%r4, %r77, %r76;
	@%p1 bra 	$L__BB103_2;

	mov.u32 	%r78, 0;
	st.shared.u32 	[%r4], %r78;

$L__BB103_2:
	bar.sync 	0;
	mov.f32 	%f9, 0f00000000;
	st.local.v4.f32 	[%rd3], {%f9, %f9, %f9, %f9};
	st.local.v4.f32 	[%rd3+16], {%f9, %f9, %f9, %f9};
	mov.u32 	%r679, 0;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs2, %f9;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f9;}

	// end inline asm
	mov.b32 	%r686, {%rs1, %rs2};
	setp.ge.s32 	%p2, %r3, %r64;
	mov.u32 	%r680, %r679;
	mov.u32 	%r681, %r679;
	mov.u32 	%r682, %r679;
	mov.u32 	%r683, %r679;
	mov.u32 	%r684, %r679;
	mov.u32 	%r685, %r679;
	@%p2 bra 	$L__BB103_9;

	not.b32 	%r93, %r3;
	add.s32 	%r6, %r93, %r64;
	mul.wide.u32 	%rd34, %r6, -858993459;
	shr.u64 	%rd35, %rd34, 39;
	cvt.u32.u64 	%r94, %rd35;
	add.s32 	%r95, %r94, 1;
	and.b32  	%r660, %r95, 3;
	setp.eq.s32 	%p3, %r660, 0;
	mov.u32 	%r679, 0;
	mov.u32 	%r669, %r3;
	@%p3 bra 	$L__BB103_6;

	shl.b32 	%r103, %r65, 1;
	add.s32 	%r104, %r3, %r103;
	mul.wide.s32 	%rd36, %r104, 4;
	shl.b64 	%rd37, %rd5, 1;
	add.s64 	%rd7, %rd36, %rd37;
	mul.wide.s32 	%rd38, %r3, 4;
	mul.wide.s32 	%rd8, %r65, 4;
	add.s64 	%rd39, %rd38, %rd8;
	add.s64 	%rd9, %rd39, %rd37;
	add.s64 	%rd10, %rd38, %rd37;
	mul.wide.s32 	%rd40, %r3, 2;
	add.s64 	%rd41, %rd40, %rd4;
	shl.b64 	%rd42, %rd41, 1;
	add.s64 	%rd83, %rd2, %rd42;
	mov.u32 	%r679, 0;
	mov.u64 	%rd84, %rd86;
	mov.u32 	%r680, %r679;
	mov.u32 	%r681, %r679;
	mov.u32 	%r682, %r679;
	mov.u32 	%r683, %r679;
	mov.u32 	%r684, %r679;
	mov.u32 	%r685, %r679;
	mov.u32 	%r669, %r3;

$L__BB103_5:
	.pragma "nounroll";
	ld.global.nc.u32 	%r106, [%rd83];
	add.s64 	%rd43, %rd84, %rd10;
	ld.global.nc.u32 	%r107, [%rd43];
	// begin inline asm
	{mul.f16x2 %r105,%r106,%r107;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r686,%r686,%r105;
}
	// end inline asm
	add.s64 	%rd44, %rd84, %rd9;
	ld.global.nc.u32 	%r113, [%rd44];
	// begin inline asm
	{mul.f16x2 %r111,%r106,%r113;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r685,%r685,%r111;
}
	// end inline asm
	add.s64 	%rd45, %rd84, %rd7;
	ld.global.nc.u32 	%r119, [%rd45];
	// begin inline asm
	{mul.f16x2 %r117,%r106,%r119;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r684,%r684,%r117;
}
	// end inline asm
	add.s64 	%rd46, %rd45, %rd8;
	ld.global.nc.u32 	%r125, [%rd46];
	// begin inline asm
	{mul.f16x2 %r123,%r106,%r125;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r683,%r683,%r123;
}
	// end inline asm
	add.s64 	%rd47, %rd46, %rd8;
	ld.global.nc.u32 	%r131, [%rd47];
	// begin inline asm
	{mul.f16x2 %r129,%r106,%r131;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r682,%r682,%r129;
}
	// end inline asm
	add.s64 	%rd48, %rd47, %rd8;
	ld.global.nc.u32 	%r137, [%rd48];
	// begin inline asm
	{mul.f16x2 %r135,%r106,%r137;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r681,%r681,%r135;
}
	// end inline asm
	add.s64 	%rd49, %rd48, %rd8;
	ld.global.nc.u32 	%r143, [%rd49];
	// begin inline asm
	{mul.f16x2 %r141,%r106,%r143;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r680,%r680,%r141;
}
	// end inline asm
	add.s64 	%rd50, %rd49, %rd8;
	ld.global.nc.u32 	%r149, [%rd50];
	// begin inline asm
	{mul.f16x2 %r147,%r106,%r149;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r679,%r679,%r147;
}
	// end inline asm
	add.s32 	%r669, %r669, 160;
	add.s64 	%rd84, %rd84, 640;
	add.s64 	%rd83, %rd83, 640;
	add.s32 	%r660, %r660, -1;
	setp.ne.s32 	%p4, %r660, 0;
	@%p4 bra 	$L__BB103_5;

$L__BB103_6:
	setp.lt.u32 	%p5, %r6, 480;
	@%p5 bra 	$L__BB103_9;

	add.s32 	%r153, %r669, %r65;
	shl.b32 	%r154, %r65, 1;
	add.s32 	%r155, %r669, %r154;
	mad.lo.s32 	%r156, %r65, 3, %r669;
	shl.b32 	%r157, %r65, 2;
	add.s32 	%r158, %r669, %r157;
	mad.lo.s32 	%r159, %r65, 5, %r669;
	mad.lo.s32 	%r160, %r65, 6, %r669;
	mad.lo.s32 	%r161, %r65, 7, %r669;
	add.s32 	%r162, %r153, 160;
	mul.wide.s32 	%rd51, %r162, 4;
	shl.b64 	%rd52, %rd5, 1;
	add.s64 	%rd16, %rd51, %rd52;
	mul.wide.s32 	%rd53, %r155, 4;
	add.s64 	%rd17, %rd53, %rd52;
	mul.wide.s32 	%rd54, %r156, 4;
	add.s64 	%rd18, %rd54, %rd52;
	mul.wide.s32 	%rd55, %r158, 4;
	add.s64 	%rd19, %rd55, %rd52;
	mul.wide.s32 	%rd56, %r159, 4;
	add.s64 	%rd20, %rd56, %rd52;
	mul.wide.s32 	%rd57, %r160, 4;
	add.s64 	%rd21, %rd57, %rd52;
	mul.wide.s32 	%rd58, %r161, 4;
	add.s64 	%rd22, %rd58, %rd52;
	mul.wide.s32 	%rd59, %r669, 2;
	add.s64 	%rd60, %rd59, %rd4;
	shl.b64 	%rd61, %rd60, 1;
	add.s64 	%rd62, %rd2, %rd61;
	add.s64 	%rd85, %rd62, 1280;
	mul.wide.s32 	%rd63, %r669, 4;
	add.s64 	%rd24, %rd63, %rd52;
	mul.wide.s32 	%rd64, %r65, 4;
	add.s64 	%rd65, %rd63, %rd64;
	add.s64 	%rd25, %rd65, %rd52;

$L__BB103_8:
	ld.global.nc.u32 	%r164, [%rd85+-1280];
	add.s64 	%rd66, %rd86, %rd24;
	ld.global.nc.u32 	%r165, [%rd66];
	// begin inline asm
	{mul.f16x2 %r163,%r164,%r165;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r166,%r686,%r163;
}
	// end inline asm
	add.s64 	%rd67, %rd86, %rd25;
	ld.global.nc.u32 	%r171, [%rd67];
	// begin inline asm
	{mul.f16x2 %r169,%r164,%r171;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r172,%r685,%r169;
}
	// end inline asm
	add.s64 	%rd68, %rd86, %rd17;
	ld.global.nc.u32 	%r177, [%rd68];
	// begin inline asm
	{mul.f16x2 %r175,%r164,%r177;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r178,%r684,%r175;
}
	// end inline asm
	add.s64 	%rd69, %rd86, %rd18;
	ld.global.nc.u32 	%r183, [%rd69];
	// begin inline asm
	{mul.f16x2 %r181,%r164,%r183;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r184,%r683,%r181;
}
	// end inline asm
	add.s64 	%rd70, %rd86, %rd19;
	ld.global.nc.u32 	%r189, [%rd70];
	// begin inline asm
	{mul.f16x2 %r187,%r164,%r189;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r190,%r682,%r187;
}
	// end inline asm
	add.s64 	%rd71, %rd86, %rd20;
	ld.global.nc.u32 	%r195, [%rd71];
	// begin inline asm
	{mul.f16x2 %r193,%r164,%r195;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r196,%r681,%r193;
}
	// end inline asm
	add.s64 	%rd72, %rd86, %rd21;
	ld.global.nc.u32 	%r201, [%rd72];
	// begin inline asm
	{mul.f16x2 %r199,%r164,%r201;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r202,%r680,%r199;
}
	// end inline asm
	add.s64 	%rd73, %rd86, %rd22;
	ld.global.nc.u32 	%r207, [%rd73];
	// begin inline asm
	{mul.f16x2 %r205,%r164,%r207;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r208,%r679,%r205;
}
	// end inline asm
	ld.global.nc.u32 	%r212, [%rd85+-640];
	ld.global.nc.u32 	%r213, [%rd66+640];
	// begin inline asm
	{mul.f16x2 %r211,%r212,%r213;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r214,%r166,%r211;
}
	// end inline asm
	add.s64 	%rd74, %rd86, %rd16;
	ld.global.nc.u32 	%r219, [%rd74];
	// begin inline asm
	{mul.f16x2 %r217,%r212,%r219;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r220,%r172,%r217;
}
	// end inline asm
	ld.global.nc.u32 	%r225, [%rd68+640];
	// begin inline asm
	{mul.f16x2 %r223,%r212,%r225;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r226,%r178,%r223;
}
	// end inline asm
	ld.global.nc.u32 	%r231, [%rd69+640];
	// begin inline asm
	{mul.f16x2 %r229,%r212,%r231;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r232,%r184,%r229;
}
	// end inline asm
	ld.global.nc.u32 	%r237, [%rd70+640];
	// begin inline asm
	{mul.f16x2 %r235,%r212,%r237;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r238,%r190,%r235;
}
	// end inline asm
	ld.global.nc.u32 	%r243, [%rd71+640];
	// begin inline asm
	{mul.f16x2 %r241,%r212,%r243;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r244,%r196,%r241;
}
	// end inline asm
	ld.global.nc.u32 	%r249, [%rd72+640];
	// begin inline asm
	{mul.f16x2 %r247,%r212,%r249;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r250,%r202,%r247;
}
	// end inline asm
	ld.global.nc.u32 	%r255, [%rd73+640];
	// begin inline asm
	{mul.f16x2 %r253,%r212,%r255;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r256,%r208,%r253;
}
	// end inline asm
	ld.global.nc.u32 	%r260, [%rd85];
	ld.global.nc.u32 	%r261, [%rd66+1280];
	// begin inline asm
	{mul.f16x2 %r259,%r260,%r261;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r262,%r214,%r259;
}
	// end inline asm
	ld.global.nc.u32 	%r267, [%rd74+640];
	// begin inline asm
	{mul.f16x2 %r265,%r260,%r267;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r268,%r220,%r265;
}
	// end inline asm
	ld.global.nc.u32 	%r273, [%rd68+1280];
	// begin inline asm
	{mul.f16x2 %r271,%r260,%r273;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r274,%r226,%r271;
}
	// end inline asm
	ld.global.nc.u32 	%r279, [%rd69+1280];
	// begin inline asm
	{mul.f16x2 %r277,%r260,%r279;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r280,%r232,%r277;
}
	// end inline asm
	ld.global.nc.u32 	%r285, [%rd70+1280];
	// begin inline asm
	{mul.f16x2 %r283,%r260,%r285;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r286,%r238,%r283;
}
	// end inline asm
	ld.global.nc.u32 	%r291, [%rd71+1280];
	// begin inline asm
	{mul.f16x2 %r289,%r260,%r291;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r292,%r244,%r289;
}
	// end inline asm
	ld.global.nc.u32 	%r297, [%rd72+1280];
	// begin inline asm
	{mul.f16x2 %r295,%r260,%r297;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r298,%r250,%r295;
}
	// end inline asm
	ld.global.nc.u32 	%r303, [%rd73+1280];
	// begin inline asm
	{mul.f16x2 %r301,%r260,%r303;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r304,%r256,%r301;
}
	// end inline asm
	ld.global.nc.u32 	%r308, [%rd85+640];
	ld.global.nc.u32 	%r309, [%rd66+1920];
	// begin inline asm
	{mul.f16x2 %r307,%r308,%r309;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r686,%r262,%r307;
}
	// end inline asm
	ld.global.nc.u32 	%r315, [%rd74+1280];
	// begin inline asm
	{mul.f16x2 %r313,%r308,%r315;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r685,%r268,%r313;
}
	// end inline asm
	ld.global.nc.u32 	%r321, [%rd68+1920];
	// begin inline asm
	{mul.f16x2 %r319,%r308,%r321;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r684,%r274,%r319;
}
	// end inline asm
	ld.global.nc.u32 	%r327, [%rd69+1920];
	// begin inline asm
	{mul.f16x2 %r325,%r308,%r327;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r683,%r280,%r325;
}
	// end inline asm
	ld.global.nc.u32 	%r333, [%rd70+1920];
	// begin inline asm
	{mul.f16x2 %r331,%r308,%r333;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r682,%r286,%r331;
}
	// end inline asm
	ld.global.nc.u32 	%r339, [%rd71+1920];
	// begin inline asm
	{mul.f16x2 %r337,%r308,%r339;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r681,%r292,%r337;
}
	// end inline asm
	ld.global.nc.u32 	%r345, [%rd72+1920];
	// begin inline asm
	{mul.f16x2 %r343,%r308,%r345;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r680,%r298,%r343;
}
	// end inline asm
	ld.global.nc.u32 	%r351, [%rd73+1920];
	// begin inline asm
	{mul.f16x2 %r349,%r308,%r351;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r679,%r304,%r349;
}
	// end inline asm
	add.s64 	%rd86, %rd86, 2560;
	add.s64 	%rd85, %rd85, 2560;
	add.s32 	%r669, %r669, 640;
	setp.lt.s32 	%p6, %r669, %r64;
	@%p6 bra 	$L__BB103_8;

$L__BB103_9:
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r686;
  cvt.f32.f16 %f10, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r686;
  cvt.f32.f16 %f11, high;}

	// end inline asm
	add.f32 	%f26, %f10, %f11;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r685;
  cvt.f32.f16 %f12, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r685;
  cvt.f32.f16 %f13, high;}

	// end inline asm
	add.f32 	%f1, %f12, %f13;
	st.local.f32 	[%rd3+4], %f1;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r684;
  cvt.f32.f16 %f14, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r684;
  cvt.f32.f16 %f15, high;}

	// end inline asm
	add.f32 	%f2, %f14, %f15;
	st.local.f32 	[%rd3+8], %f2;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r683;
  cvt.f32.f16 %f16, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r683;
  cvt.f32.f16 %f17, high;}

	// end inline asm
	add.f32 	%f3, %f16, %f17;
	st.local.f32 	[%rd3+12], %f3;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r682;
  cvt.f32.f16 %f18, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r682;
  cvt.f32.f16 %f19, high;}

	// end inline asm
	add.f32 	%f4, %f18, %f19;
	st.local.f32 	[%rd3+16], %f4;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r681;
  cvt.f32.f16 %f20, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r681;
  cvt.f32.f16 %f21, high;}

	// end inline asm
	add.f32 	%f5, %f20, %f21;
	st.local.f32 	[%rd3+20], %f5;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r680;
  cvt.f32.f16 %f22, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r680;
  cvt.f32.f16 %f23, high;}

	// end inline asm
	add.f32 	%f6, %f22, %f23;
	st.local.f32 	[%rd3+24], %f6;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r679;
  cvt.f32.f16 %f24, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r679;
  cvt.f32.f16 %f25, high;}

	// end inline asm
	add.f32 	%f7, %f24, %f25;
	st.local.f32 	[%rd3+28], %f7;
	shr.s32 	%r371, %r3, 31;
	shr.u32 	%r372, %r371, 27;
	add.s32 	%r373, %r3, %r372;
	shr.s32 	%r374, %r373, 5;
	shl.b32 	%r375, %r374, 2;
	add.s32 	%r63, %r77, %r375;
	mov.u32 	%r377, 2;
	mov.b32 	%r378, %f26;
	mov.u32 	%r379, 31;
	mov.u32 	%r380, 16;
	mov.u32 	%r381, -1;
	shfl.sync.bfly.b32 	%r382|%p7, %r378, %r380, %r379, %r381;
	mov.b32 	%f27, %r382;
	add.f32 	%f28, %f26, %f27;
	mov.b32 	%r383, %f28;
	mov.u32 	%r384, 8;
	shfl.sync.bfly.b32 	%r385|%p8, %r383, %r384, %r379, %r381;
	mov.b32 	%f29, %r385;
	add.f32 	%f30, %f28, %f29;
	mov.b32 	%r386, %f30;
	mov.u32 	%r387, 4;
	shfl.sync.bfly.b32 	%r388|%p9, %r386, %r387, %r379, %r381;
	mov.b32 	%f31, %r388;
	add.f32 	%f32, %f30, %f31;
	mov.b32 	%r389, %f32;
	shfl.sync.bfly.b32 	%r390|%p10, %r389, %r377, %r379, %r381;
	mov.b32 	%f33, %r390;
	add.f32 	%f34, %f32, %f33;
	mov.b32 	%r391, %f34;
	mov.u32 	%r392, 1;
	shfl.sync.bfly.b32 	%r393|%p11, %r391, %r392, %r379, %r381;
	mov.b32 	%f35, %r393;
	add.f32 	%f36, %f34, %f35;
	st.local.f32 	[%rd3], %f36;
	st.shared.f32 	[%r63], %f36;
	bar.sync 	0;
	@%p1 bra 	$L__BB103_11;

	ld.shared.f32 	%f37, [%r4];
	mov.b32 	%r394, %f37;
	shfl.sync.bfly.b32 	%r398|%p13, %r394, %r380, %r379, %r381;
	mov.b32 	%f38, %r398;
	add.f32 	%f39, %f37, %f38;
	mov.b32 	%r399, %f39;
	shfl.sync.bfly.b32 	%r401|%p14, %r399, %r384, %r379, %r381;
	mov.b32 	%f40, %r401;
	add.f32 	%f41, %f39, %f40;
	mov.b32 	%r402, %f41;
	shfl.sync.bfly.b32 	%r404|%p15, %r402, %r387, %r379, %r381;
	mov.b32 	%f42, %r404;
	add.f32 	%f43, %f41, %f42;
	mov.b32 	%r405, %f43;
	shfl.sync.bfly.b32 	%r407|%p16, %r405, %r377, %r379, %r381;
	mov.b32 	%f44, %r407;
	add.f32 	%f45, %f43, %f44;
	mov.b32 	%r408, %f45;
	shfl.sync.bfly.b32 	%r410|%p17, %r408, %r392, %r379, %r381;
	mov.b32 	%f46, %r410;
	add.f32 	%f47, %f45, %f46;
	st.local.f32 	[%rd3], %f47;

$L__BB103_11:
	bar.sync 	0;
	mov.b32 	%r411, %f1;
	shfl.sync.bfly.b32 	%r415|%p19, %r411, %r380, %r379, %r381;
	mov.b32 	%f48, %r415;
	add.f32 	%f49, %f1, %f48;
	mov.b32 	%r416, %f49;
	shfl.sync.bfly.b32 	%r418|%p20, %r416, %r384, %r379, %r381;
	mov.b32 	%f50, %r418;
	add.f32 	%f51, %f49, %f50;
	mov.b32 	%r419, %f51;
	shfl.sync.bfly.b32 	%r421|%p21, %r419, %r387, %r379, %r381;
	mov.b32 	%f52, %r421;
	add.f32 	%f53, %f51, %f52;
	mov.b32 	%r422, %f53;
	shfl.sync.bfly.b32 	%r424|%p22, %r422, %r377, %r379, %r381;
	mov.b32 	%f54, %r424;
	add.f32 	%f55, %f53, %f54;
	mov.b32 	%r425, %f55;
	shfl.sync.bfly.b32 	%r427|%p23, %r425, %r392, %r379, %r381;
	mov.b32 	%f56, %r427;
	add.f32 	%f57, %f55, %f56;
	st.local.f32 	[%rd3+4], %f57;
	st.shared.f32 	[%r63], %f57;
	bar.sync 	0;
	@%p1 bra 	$L__BB103_13;

	ld.shared.f32 	%f58, [%r4];
	mov.b32 	%r428, %f58;
	mov.u32 	%r429, 31;
	mov.u32 	%r430, 16;
	mov.u32 	%r431, -1;
	shfl.sync.bfly.b32 	%r432|%p24, %r428, %r430, %r429, %r431;
	mov.b32 	%f59, %r432;
	add.f32 	%f60, %f58, %f59;
	mov.b32 	%r433, %f60;
	mov.u32 	%r434, 8;
	shfl.sync.bfly.b32 	%r435|%p25, %r433, %r434, %r429, %r431;
	mov.b32 	%f61, %r435;
	add.f32 	%f62, %f60, %f61;
	mov.b32 	%r436, %f62;
	mov.u32 	%r437, 4;
	shfl.sync.bfly.b32 	%r438|%p26, %r436, %r437, %r429, %r431;
	mov.b32 	%f63, %r438;
	add.f32 	%f64, %f62, %f63;
	mov.b32 	%r439, %f64;
	mov.u32 	%r440, 2;
	shfl.sync.bfly.b32 	%r441|%p27, %r439, %r440, %r429, %r431;
	mov.b32 	%f65, %r441;
	add.f32 	%f66, %f64, %f65;
	mov.b32 	%r442, %f66;
	mov.u32 	%r443, 1;
	shfl.sync.bfly.b32 	%r444|%p28, %r442, %r443, %r429, %r431;
	mov.b32 	%f67, %r444;
	add.f32 	%f68, %f66, %f67;
	st.local.f32 	[%rd3+4], %f68;

$L__BB103_13:
	bar.sync 	0;
	mov.b32 	%r445, %f2;
	mov.u32 	%r446, 31;
	mov.u32 	%r447, 16;
	mov.u32 	%r448, -1;
	shfl.sync.bfly.b32 	%r449|%p30, %r445, %r447, %r446, %r448;
	mov.b32 	%f69, %r449;
	add.f32 	%f70, %f2, %f69;
	mov.b32 	%r450, %f70;
	mov.u32 	%r451, 8;
	shfl.sync.bfly.b32 	%r452|%p31, %r450, %r451, %r446, %r448;
	mov.b32 	%f71, %r452;
	add.f32 	%f72, %f70, %f71;
	mov.b32 	%r453, %f72;
	mov.u32 	%r454, 4;
	shfl.sync.bfly.b32 	%r455|%p32, %r453, %r454, %r446, %r448;
	mov.b32 	%f73, %r455;
	add.f32 	%f74, %f72, %f73;
	mov.b32 	%r456, %f74;
	mov.u32 	%r457, 2;
	shfl.sync.bfly.b32 	%r458|%p33, %r456, %r457, %r446, %r448;
	mov.b32 	%f75, %r458;
	add.f32 	%f76, %f74, %f75;
	mov.b32 	%r459, %f76;
	mov.u32 	%r460, 1;
	shfl.sync.bfly.b32 	%r461|%p34, %r459, %r460, %r446, %r448;
	mov.b32 	%f77, %r461;
	add.f32 	%f78, %f76, %f77;
	st.local.f32 	[%rd3+8], %f78;
	st.shared.f32 	[%r63], %f78;
	bar.sync 	0;
	@%p1 bra 	$L__BB103_15;

	ld.shared.f32 	%f79, [%r4];
	mov.b32 	%r462, %f79;
	shfl.sync.bfly.b32 	%r466|%p35, %r462, %r447, %r446, %r448;
	mov.b32 	%f80, %r466;
	add.f32 	%f81, %f79, %f80;
	mov.b32 	%r467, %f81;
	shfl.sync.bfly.b32 	%r469|%p36, %r467, %r451, %r446, %r448;
	mov.b32 	%f82, %r469;
	add.f32 	%f83, %f81, %f82;
	mov.b32 	%r470, %f83;
	shfl.sync.bfly.b32 	%r472|%p37, %r470, %r454, %r446, %r448;
	mov.b32 	%f84, %r472;
	add.f32 	%f85, %f83, %f84;
	mov.b32 	%r473, %f85;
	shfl.sync.bfly.b32 	%r475|%p38, %r473, %r457, %r446, %r448;
	mov.b32 	%f86, %r475;
	add.f32 	%f87, %f85, %f86;
	mov.b32 	%r476, %f87;
	shfl.sync.bfly.b32 	%r478|%p39, %r476, %r460, %r446, %r448;
	mov.b32 	%f88, %r478;
	add.f32 	%f89, %f87, %f88;
	st.local.f32 	[%rd3+8], %f89;

$L__BB103_15:
	bar.sync 	0;
	mov.b32 	%r479, %f3;
	shfl.sync.bfly.b32 	%r483|%p41, %r479, %r447, %r446, %r448;
	mov.b32 	%f90, %r483;
	add.f32 	%f91, %f3, %f90;
	mov.b32 	%r484, %f91;
	shfl.sync.bfly.b32 	%r486|%p42, %r484, %r451, %r446, %r448;
	mov.b32 	%f92, %r486;
	add.f32 	%f93, %f91, %f92;
	mov.b32 	%r487, %f93;
	shfl.sync.bfly.b32 	%r489|%p43, %r487, %r454, %r446, %r448;
	mov.b32 	%f94, %r489;
	add.f32 	%f95, %f93, %f94;
	mov.b32 	%r490, %f95;
	shfl.sync.bfly.b32 	%r492|%p44, %r490, %r457, %r446, %r448;
	mov.b32 	%f96, %r492;
	add.f32 	%f97, %f95, %f96;
	mov.b32 	%r493, %f97;
	shfl.sync.bfly.b32 	%r495|%p45, %r493, %r460, %r446, %r448;
	mov.b32 	%f98, %r495;
	add.f32 	%f99, %f97, %f98;
	st.local.f32 	[%rd3+12], %f99;
	st.shared.f32 	[%r63], %f99;
	bar.sync 	0;
	@%p1 bra 	$L__BB103_17;

	ld.shared.f32 	%f100, [%r4];
	mov.b32 	%r496, %f100;
	mov.u32 	%r497, 31;
	mov.u32 	%r498, 16;
	mov.u32 	%r499, -1;
	shfl.sync.bfly.b32 	%r500|%p46, %r496, %r498, %r497, %r499;
	mov.b32 	%f101, %r500;
	add.f32 	%f102, %f100, %f101;
	mov.b32 	%r501, %f102;
	mov.u32 	%r502, 8;
	shfl.sync.bfly.b32 	%r503|%p47, %r501, %r502, %r497, %r499;
	mov.b32 	%f103, %r503;
	add.f32 	%f104, %f102, %f103;
	mov.b32 	%r504, %f104;
	mov.u32 	%r505, 4;
	shfl.sync.bfly.b32 	%r506|%p48, %r504, %r505, %r497, %r499;
	mov.b32 	%f105, %r506;
	add.f32 	%f106, %f104, %f105;
	mov.b32 	%r507, %f106;
	mov.u32 	%r508, 2;
	shfl.sync.bfly.b32 	%r509|%p49, %r507, %r508, %r497, %r499;
	mov.b32 	%f107, %r509;
	add.f32 	%f108, %f106, %f107;
	mov.b32 	%r510, %f108;
	mov.u32 	%r511, 1;
	shfl.sync.bfly.b32 	%r512|%p50, %r510, %r511, %r497, %r499;
	mov.b32 	%f109, %r512;
	add.f32 	%f110, %f108, %f109;
	st.local.f32 	[%rd3+12], %f110;

$L__BB103_17:
	bar.sync 	0;
	mov.b32 	%r513, %f4;
	mov.u32 	%r514, 31;
	mov.u32 	%r515, 16;
	mov.u32 	%r516, -1;
	shfl.sync.bfly.b32 	%r517|%p52, %r513, %r515, %r514, %r516;
	mov.b32 	%f111, %r517;
	add.f32 	%f112, %f4, %f111;
	mov.b32 	%r518, %f112;
	mov.u32 	%r519, 8;
	shfl.sync.bfly.b32 	%r520|%p53, %r518, %r519, %r514, %r516;
	mov.b32 	%f113, %r520;
	add.f32 	%f114, %f112, %f113;
	mov.b32 	%r521, %f114;
	mov.u32 	%r522, 4;
	shfl.sync.bfly.b32 	%r523|%p54, %r521, %r522, %r514, %r516;
	mov.b32 	%f115, %r523;
	add.f32 	%f116, %f114, %f115;
	mov.b32 	%r524, %f116;
	mov.u32 	%r525, 2;
	shfl.sync.bfly.b32 	%r526|%p55, %r524, %r525, %r514, %r516;
	mov.b32 	%f117, %r526;
	add.f32 	%f118, %f116, %f117;
	mov.b32 	%r527, %f118;
	mov.u32 	%r528, 1;
	shfl.sync.bfly.b32 	%r529|%p56, %r527, %r528, %r514, %r516;
	mov.b32 	%f119, %r529;
	add.f32 	%f120, %f118, %f119;
	st.local.f32 	[%rd3+16], %f120;
	st.shared.f32 	[%r63], %f120;
	bar.sync 	0;
	@%p1 bra 	$L__BB103_19;

	ld.shared.f32 	%f121, [%r4];
	mov.b32 	%r530, %f121;
	shfl.sync.bfly.b32 	%r534|%p57, %r530, %r515, %r514, %r516;
	mov.b32 	%f122, %r534;
	add.f32 	%f123, %f121, %f122;
	mov.b32 	%r535, %f123;
	shfl.sync.bfly.b32 	%r537|%p58, %r535, %r519, %r514, %r516;
	mov.b32 	%f124, %r537;
	add.f32 	%f125, %f123, %f124;
	mov.b32 	%r538, %f125;
	shfl.sync.bfly.b32 	%r540|%p59, %r538, %r522, %r514, %r516;
	mov.b32 	%f126, %r540;
	add.f32 	%f127, %f125, %f126;
	mov.b32 	%r541, %f127;
	shfl.sync.bfly.b32 	%r543|%p60, %r541, %r525, %r514, %r516;
	mov.b32 	%f128, %r543;
	add.f32 	%f129, %f127, %f128;
	mov.b32 	%r544, %f129;
	shfl.sync.bfly.b32 	%r546|%p61, %r544, %r528, %r514, %r516;
	mov.b32 	%f130, %r546;
	add.f32 	%f131, %f129, %f130;
	st.local.f32 	[%rd3+16], %f131;

$L__BB103_19:
	bar.sync 	0;
	mov.b32 	%r547, %f5;
	shfl.sync.bfly.b32 	%r551|%p63, %r547, %r515, %r514, %r516;
	mov.b32 	%f132, %r551;
	add.f32 	%f133, %f5, %f132;
	mov.b32 	%r552, %f133;
	shfl.sync.bfly.b32 	%r554|%p64, %r552, %r519, %r514, %r516;
	mov.b32 	%f134, %r554;
	add.f32 	%f135, %f133, %f134;
	mov.b32 	%r555, %f135;
	shfl.sync.bfly.b32 	%r557|%p65, %r555, %r522, %r514, %r516;
	mov.b32 	%f136, %r557;
	add.f32 	%f137, %f135, %f136;
	mov.b32 	%r558, %f137;
	shfl.sync.bfly.b32 	%r560|%p66, %r558, %r525, %r514, %r516;
	mov.b32 	%f138, %r560;
	add.f32 	%f139, %f137, %f138;
	mov.b32 	%r561, %f139;
	shfl.sync.bfly.b32 	%r563|%p67, %r561, %r528, %r514, %r516;
	mov.b32 	%f140, %r563;
	add.f32 	%f141, %f139, %f140;
	st.local.f32 	[%rd3+20], %f141;
	st.shared.f32 	[%r63], %f141;
	bar.sync 	0;
	@%p1 bra 	$L__BB103_21;

	ld.shared.f32 	%f142, [%r4];
	mov.b32 	%r564, %f142;
	mov.u32 	%r565, 31;
	mov.u32 	%r566, 16;
	mov.u32 	%r567, -1;
	shfl.sync.bfly.b32 	%r568|%p68, %r564, %r566, %r565, %r567;
	mov.b32 	%f143, %r568;
	add.f32 	%f144, %f142, %f143;
	mov.b32 	%r569, %f144;
	mov.u32 	%r570, 8;
	shfl.sync.bfly.b32 	%r571|%p69, %r569, %r570, %r565, %r567;
	mov.b32 	%f145, %r571;
	add.f32 	%f146, %f144, %f145;
	mov.b32 	%r572, %f146;
	mov.u32 	%r573, 4;
	shfl.sync.bfly.b32 	%r574|%p70, %r572, %r573, %r565, %r567;
	mov.b32 	%f147, %r574;
	add.f32 	%f148, %f146, %f147;
	mov.b32 	%r575, %f148;
	mov.u32 	%r576, 2;
	shfl.sync.bfly.b32 	%r577|%p71, %r575, %r576, %r565, %r567;
	mov.b32 	%f149, %r577;
	add.f32 	%f150, %f148, %f149;
	mov.b32 	%r578, %f150;
	mov.u32 	%r579, 1;
	shfl.sync.bfly.b32 	%r580|%p72, %r578, %r579, %r565, %r567;
	mov.b32 	%f151, %r580;
	add.f32 	%f152, %f150, %f151;
	st.local.f32 	[%rd3+20], %f152;

$L__BB103_21:
	bar.sync 	0;
	mov.b32 	%r581, %f6;
	mov.u32 	%r582, 31;
	mov.u32 	%r583, 16;
	mov.u32 	%r584, -1;
	shfl.sync.bfly.b32 	%r585|%p74, %r581, %r583, %r582, %r584;
	mov.b32 	%f153, %r585;
	add.f32 	%f154, %f6, %f153;
	mov.b32 	%r586, %f154;
	mov.u32 	%r587, 8;
	shfl.sync.bfly.b32 	%r588|%p75, %r586, %r587, %r582, %r584;
	mov.b32 	%f155, %r588;
	add.f32 	%f156, %f154, %f155;
	mov.b32 	%r589, %f156;
	mov.u32 	%r590, 4;
	shfl.sync.bfly.b32 	%r591|%p76, %r589, %r590, %r582, %r584;
	mov.b32 	%f157, %r591;
	add.f32 	%f158, %f156, %f157;
	mov.b32 	%r592, %f158;
	mov.u32 	%r593, 2;
	shfl.sync.bfly.b32 	%r594|%p77, %r592, %r593, %r582, %r584;
	mov.b32 	%f159, %r594;
	add.f32 	%f160, %f158, %f159;
	mov.b32 	%r595, %f160;
	mov.u32 	%r596, 1;
	shfl.sync.bfly.b32 	%r597|%p78, %r595, %r596, %r582, %r584;
	mov.b32 	%f161, %r597;
	add.f32 	%f162, %f160, %f161;
	st.local.f32 	[%rd3+24], %f162;
	st.shared.f32 	[%r63], %f162;
	bar.sync 	0;
	@%p1 bra 	$L__BB103_23;

	ld.shared.f32 	%f163, [%r4];
	mov.b32 	%r598, %f163;
	shfl.sync.bfly.b32 	%r602|%p79, %r598, %r583, %r582, %r584;
	mov.b32 	%f164, %r602;
	add.f32 	%f165, %f163, %f164;
	mov.b32 	%r603, %f165;
	shfl.sync.bfly.b32 	%r605|%p80, %r603, %r587, %r582, %r584;
	mov.b32 	%f166, %r605;
	add.f32 	%f167, %f165, %f166;
	mov.b32 	%r606, %f167;
	shfl.sync.bfly.b32 	%r608|%p81, %r606, %r590, %r582, %r584;
	mov.b32 	%f168, %r608;
	add.f32 	%f169, %f167, %f168;
	mov.b32 	%r609, %f169;
	shfl.sync.bfly.b32 	%r611|%p82, %r609, %r593, %r582, %r584;
	mov.b32 	%f170, %r611;
	add.f32 	%f171, %f169, %f170;
	mov.b32 	%r612, %f171;
	shfl.sync.bfly.b32 	%r614|%p83, %r612, %r596, %r582, %r584;
	mov.b32 	%f172, %r614;
	add.f32 	%f173, %f171, %f172;
	st.local.f32 	[%rd3+24], %f173;

$L__BB103_23:
	bar.sync 	0;
	mov.b32 	%r615, %f7;
	shfl.sync.bfly.b32 	%r619|%p85, %r615, %r583, %r582, %r584;
	mov.b32 	%f174, %r619;
	add.f32 	%f175, %f7, %f174;
	mov.b32 	%r620, %f175;
	shfl.sync.bfly.b32 	%r622|%p86, %r620, %r587, %r582, %r584;
	mov.b32 	%f176, %r622;
	add.f32 	%f177, %f175, %f176;
	mov.b32 	%r623, %f177;
	shfl.sync.bfly.b32 	%r625|%p87, %r623, %r590, %r582, %r584;
	mov.b32 	%f178, %r625;
	add.f32 	%f179, %f177, %f178;
	mov.b32 	%r626, %f179;
	shfl.sync.bfly.b32 	%r628|%p88, %r626, %r593, %r582, %r584;
	mov.b32 	%f180, %r628;
	add.f32 	%f181, %f179, %f180;
	mov.b32 	%r629, %f181;
	shfl.sync.bfly.b32 	%r631|%p89, %r629, %r596, %r582, %r584;
	mov.b32 	%f182, %r631;
	add.f32 	%f183, %f181, %f182;
	st.local.f32 	[%rd3+28], %f183;
	st.shared.f32 	[%r63], %f183;
	bar.sync 	0;
	@%p1 bra 	$L__BB103_25;

	ld.shared.f32 	%f184, [%r4];
	mov.b32 	%r632, %f184;
	mov.u32 	%r633, 31;
	mov.u32 	%r634, 16;
	mov.u32 	%r635, -1;
	shfl.sync.bfly.b32 	%r636|%p90, %r632, %r634, %r633, %r635;
	mov.b32 	%f185, %r636;
	add.f32 	%f186, %f184, %f185;
	mov.b32 	%r637, %f186;
	mov.u32 	%r638, 8;
	shfl.sync.bfly.b32 	%r639|%p91, %r637, %r638, %r633, %r635;
	mov.b32 	%f187, %r639;
	add.f32 	%f188, %f186, %f187;
	mov.b32 	%r640, %f188;
	mov.u32 	%r641, 4;
	shfl.sync.bfly.b32 	%r642|%p92, %r640, %r641, %r633, %r635;
	mov.b32 	%f189, %r642;
	add.f32 	%f190, %f188, %f189;
	mov.b32 	%r643, %f190;
	mov.u32 	%r644, 2;
	shfl.sync.bfly.b32 	%r645|%p93, %r643, %r644, %r633, %r635;
	mov.b32 	%f191, %r645;
	add.f32 	%f192, %f190, %f191;
	mov.b32 	%r646, %f192;
	mov.u32 	%r647, 1;
	shfl.sync.bfly.b32 	%r648|%p94, %r646, %r647, %r633, %r635;
	mov.b32 	%f193, %r648;
	add.f32 	%f194, %f192, %f193;
	st.local.f32 	[%rd3+28], %f194;

$L__BB103_25:
	bar.sync 	0;
	setp.gt.s32 	%p95, %r3, 7;
	@%p95 bra 	$L__BB103_27;

	mad.lo.s32 	%r649, %r3, %r66, %r2;
	cvt.s64.s32 	%rd75, %r649;
	mul.lo.s32 	%r650, %r1, %r67;
	cvt.s64.s32 	%rd76, %r650;
	add.s64 	%rd77, %rd76, %rd75;
	mul.wide.s32 	%rd78, %r3, 4;
	add.s64 	%rd79, %rd3, %rd78;
	ld.local.f32 	%f195, [%rd79];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs3, %f195;}

	// end inline asm
	cvta.to.global.u64 	%rd80, %rd30;
	shl.b64 	%rd81, %rd77, 1;
	add.s64 	%rd82, %rd80, %rd81;
	st.global.u16 	[%rd82], %rs3;

$L__BB103_27:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_1_bs_192
.visible .entry ggml_matvec_f16_ncols_1_bs_192(
	.param .u64 ggml_matvec_f16_ncols_1_bs_192_param_0,
	.param .u64 ggml_matvec_f16_ncols_1_bs_192_param_1,
	.param .u64 ggml_matvec_f16_ncols_1_bs_192_param_2,
	.param .u32 ggml_matvec_f16_ncols_1_bs_192_param_3,
	.param .u32 ggml_matvec_f16_ncols_1_bs_192_param_4,
	.param .u32 ggml_matvec_f16_ncols_1_bs_192_param_5,
	.param .u32 ggml_matvec_f16_ncols_1_bs_192_param_6,
	.param .u32 ggml_matvec_f16_ncols_1_bs_192_param_7,
	.param .u32 ggml_matvec_f16_ncols_1_bs_192_param_8,
	.param .u32 ggml_matvec_f16_ncols_1_bs_192_param_9,
	.param .u32 ggml_matvec_f16_ncols_1_bs_192_param_10,
	.param .u32 ggml_matvec_f16_ncols_1_bs_192_param_11
)
{
	.reg .pred 	%p<19>;
	.reg .b16 	%rs<31>;
	.reg .f32 	%f<30>;
	.reg .b32 	%r<127>;
	.reg .b64 	%rd<47>;


	ld.param.u64 	%rd14, [ggml_matvec_f16_ncols_1_bs_192_param_0];
	ld.param.u64 	%rd15, [ggml_matvec_f16_ncols_1_bs_192_param_1];
	ld.param.u64 	%rd16, [ggml_matvec_f16_ncols_1_bs_192_param_2];
	ld.param.u32 	%r13, [ggml_matvec_f16_ncols_1_bs_192_param_3];
	ld.param.u32 	%r17, [ggml_matvec_f16_ncols_1_bs_192_param_5];
	ld.param.u32 	%r14, [ggml_matvec_f16_ncols_1_bs_192_param_7];
	ld.param.u32 	%r18, [ggml_matvec_f16_ncols_1_bs_192_param_8];
	ld.param.u32 	%r19, [ggml_matvec_f16_ncols_1_bs_192_param_9];
	ld.param.u32 	%r15, [ggml_matvec_f16_ncols_1_bs_192_param_10];
	ld.param.u32 	%r16, [ggml_matvec_f16_ncols_1_bs_192_param_11];
	mov.u32 	%r20, %ctaid.x;
	mov.u32 	%r21, %ctaid.y;
	div.s32 	%r22, %r21, %r18;
	mul.lo.s32 	%r23, %r20, %r17;
	mad.lo.s32 	%r24, %r22, %r19, %r23;
	cvt.s64.s32 	%rd1, %r24;
	mov.u32 	%r1, %tid.x;
	setp.gt.s32 	%p1, %r1, 31;
	shl.b32 	%r25, %r1, 2;
	mov.u32 	%r26, data_mmv;
	add.s32 	%r2, %r26, %r25;
	@%p1 bra 	$L__BB104_2;

	mov.u32 	%r27, 0;
	st.shared.u32 	[%r2], %r27;

$L__BB104_2:
	bar.sync 	0;
	mov.f32 	%f5, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs30, %f5;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs29, %f5;}

	// end inline asm
	setp.ge.s32 	%p2, %r1, %r13;
	@%p2 bra 	$L__BB104_9;

	not.b32 	%r28, %r1;
	add.s32 	%r29, %r28, %r13;
	mul.wide.u32 	%rd17, %r29, -1431655765;
	shr.u64 	%rd18, %rd17, 39;
	cvt.u32.u64 	%r30, %rd18;
	add.s32 	%r31, %r30, 1;
	and.b32  	%r124, %r31, 3;
	setp.eq.s32 	%p3, %r124, 0;
	mov.u32 	%r125, %r1;
	@%p3 bra 	$L__BB104_6;

	mov.u32 	%r125, %tid.x;
	mul.wide.s32 	%rd19, %r125, 2;
	mul.lo.s32 	%r33, %r21, %r15;
	cvt.s64.s32 	%rd20, %r33;
	add.s64 	%rd21, %rd19, %rd20;
	cvta.to.global.u64 	%rd22, %rd15;
	shl.b64 	%rd23, %rd21, 1;
	add.s64 	%rd44, %rd22, %rd23;
	add.s64 	%rd24, %rd19, %rd1;
	cvta.to.global.u64 	%rd25, %rd14;
	shl.b64 	%rd26, %rd24, 1;
	add.s64 	%rd43, %rd25, %rd26;

$L__BB104_5:
	.pragma "nounroll";
	ld.global.nc.u32 	%r35, [%rd43];
	ld.global.nc.u32 	%r36, [%rd44];
	// begin inline asm
	{mul.f16x2 %r34,%r35,%r36;
}
	// end inline asm
	mov.b32 	%r38, {%rs30, %rs29};
	// begin inline asm
	{add.f16x2 %r37,%r38,%r34;
}
	// end inline asm
	mov.b32 	{%rs30, %rs29}, %r37;
	add.s32 	%r125, %r125, 192;
	add.s64 	%rd44, %rd44, 768;
	add.s64 	%rd43, %rd43, 768;
	add.s32 	%r124, %r124, -1;
	setp.ne.s32 	%p4, %r124, 0;
	@%p4 bra 	$L__BB104_5;

$L__BB104_6:
	setp.lt.u32 	%p5, %r29, 576;
	@%p5 bra 	$L__BB104_9;

	mul.wide.s32 	%rd27, %r125, 2;
	cvta.to.global.u64 	%rd28, %rd14;
	add.s64 	%rd29, %rd27, %rd1;
	shl.b64 	%rd30, %rd29, 1;
	add.s64 	%rd31, %rd28, %rd30;
	add.s64 	%rd46, %rd31, 1536;
	mul.lo.s32 	%r44, %r21, %r15;
	cvt.s64.s32 	%rd32, %r44;
	cvta.to.global.u64 	%rd33, %rd15;
	add.s64 	%rd34, %rd27, %rd32;
	shl.b64 	%rd35, %rd34, 1;
	add.s64 	%rd36, %rd33, %rd35;
	add.s64 	%rd45, %rd36, 1536;

$L__BB104_8:
	ld.global.nc.u32 	%r46, [%rd46+-1536];
	ld.global.nc.u32 	%r47, [%rd45+-1536];
	// begin inline asm
	{mul.f16x2 %r45,%r46,%r47;
}
	// end inline asm
	mov.b32 	%r49, {%rs30, %rs29};
	// begin inline asm
	{add.f16x2 %r48,%r49,%r45;
}
	// end inline asm
	ld.global.nc.u32 	%r52, [%rd46+-768];
	ld.global.nc.u32 	%r53, [%rd45+-768];
	// begin inline asm
	{mul.f16x2 %r51,%r52,%r53;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r54,%r48,%r51;
}
	// end inline asm
	ld.global.nc.u32 	%r58, [%rd46];
	ld.global.nc.u32 	%r59, [%rd45];
	// begin inline asm
	{mul.f16x2 %r57,%r58,%r59;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r60,%r54,%r57;
}
	// end inline asm
	ld.global.nc.u32 	%r64, [%rd46+768];
	ld.global.nc.u32 	%r65, [%rd45+768];
	// begin inline asm
	{mul.f16x2 %r63,%r64,%r65;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r66,%r60,%r63;
}
	// end inline asm
	mov.b32 	{%rs30, %rs29}, %r66;
	add.s64 	%rd46, %rd46, 3072;
	add.s64 	%rd45, %rd45, 3072;
	add.s32 	%r125, %r125, 768;
	setp.lt.s32 	%p6, %r125, %r13;
	@%p6 bra 	$L__BB104_8;

$L__BB104_9:
	mov.u32 	%r72, 1;
	mov.b32 	%r70, {%rs30, %rs29};
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r70;
  cvt.f32.f16 %f6, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r70;
  cvt.f32.f16 %f7, high;}

	// end inline asm
	add.f32 	%f8, %f6, %f7;
	mov.b32 	%r73, %f8;
	mov.u32 	%r74, 31;
	mov.u32 	%r75, 16;
	mov.u32 	%r76, -1;
	shfl.sync.bfly.b32 	%r77|%p8, %r73, %r75, %r74, %r76;
	mov.b32 	%f9, %r77;
	add.f32 	%f10, %f8, %f9;
	mov.b32 	%r78, %f10;
	mov.u32 	%r79, 8;
	shfl.sync.bfly.b32 	%r80|%p9, %r78, %r79, %r74, %r76;
	mov.b32 	%f11, %r80;
	add.f32 	%f12, %f10, %f11;
	mov.b32 	%r81, %f12;
	mov.u32 	%r82, 4;
	shfl.sync.bfly.b32 	%r83|%p10, %r81, %r82, %r74, %r76;
	mov.b32 	%f13, %r83;
	add.f32 	%f14, %f12, %f13;
	mov.b32 	%r84, %f14;
	mov.u32 	%r85, 2;
	shfl.sync.bfly.b32 	%r86|%p11, %r84, %r85, %r74, %r76;
	mov.b32 	%f15, %r86;
	add.f32 	%f16, %f14, %f15;
	mov.b32 	%r87, %f16;
	shfl.sync.bfly.b32 	%r88|%p12, %r87, %r72, %r74, %r76;
	mov.b32 	%f17, %r88;
	add.f32 	%f29, %f16, %f17;
	shr.s32 	%r89, %r1, 31;
	shr.u32 	%r90, %r89, 27;
	add.s32 	%r91, %r1, %r90;
	shr.s32 	%r92, %r91, 5;
	shl.b32 	%r93, %r92, 2;
	add.s32 	%r95, %r26, %r93;
	st.shared.f32 	[%r95], %f29;
	bar.sync 	0;
	@%p1 bra 	$L__BB104_11;

	ld.shared.f32 	%f18, [%r2];
	mov.b32 	%r101, %f18;
	shfl.sync.bfly.b32 	%r105|%p13, %r101, %r75, %r74, %r76;
	mov.b32 	%f19, %r105;
	add.f32 	%f20, %f18, %f19;
	mov.b32 	%r106, %f20;
	shfl.sync.bfly.b32 	%r108|%p14, %r106, %r79, %r74, %r76;
	mov.b32 	%f21, %r108;
	add.f32 	%f22, %f20, %f21;
	mov.b32 	%r109, %f22;
	shfl.sync.bfly.b32 	%r111|%p15, %r109, %r82, %r74, %r76;
	mov.b32 	%f23, %r111;
	add.f32 	%f24, %f22, %f23;
	mov.b32 	%r112, %f24;
	shfl.sync.bfly.b32 	%r113|%p16, %r112, %r85, %r74, %r76;
	mov.b32 	%f25, %r113;
	add.f32 	%f26, %f24, %f25;
	mov.b32 	%r114, %f26;
	shfl.sync.bfly.b32 	%r116|%p17, %r114, %r72, %r74, %r76;
	mov.b32 	%f27, %r116;
	add.f32 	%f29, %f26, %f27;

$L__BB104_11:
	bar.sync 	0;
	setp.gt.s32 	%p18, %r1, 0;
	@%p18 bra 	$L__BB104_13;

	mad.lo.s32 	%r120, %r1, %r14, %r20;
	cvt.s64.s32 	%rd37, %r120;
	mul.lo.s32 	%r122, %r21, %r16;
	cvt.s64.s32 	%rd38, %r122;
	add.s64 	%rd39, %rd38, %rd37;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs20, %f29;}

	// end inline asm
	cvta.to.global.u64 	%rd40, %rd16;
	shl.b64 	%rd41, %rd39, 1;
	add.s64 	%rd42, %rd40, %rd41;
	st.global.u16 	[%rd42], %rs20;

$L__BB104_13:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_2_bs_192
.visible .entry ggml_matvec_f16_ncols_2_bs_192(
	.param .u64 ggml_matvec_f16_ncols_2_bs_192_param_0,
	.param .u64 ggml_matvec_f16_ncols_2_bs_192_param_1,
	.param .u64 ggml_matvec_f16_ncols_2_bs_192_param_2,
	.param .u32 ggml_matvec_f16_ncols_2_bs_192_param_3,
	.param .u32 ggml_matvec_f16_ncols_2_bs_192_param_4,
	.param .u32 ggml_matvec_f16_ncols_2_bs_192_param_5,
	.param .u32 ggml_matvec_f16_ncols_2_bs_192_param_6,
	.param .u32 ggml_matvec_f16_ncols_2_bs_192_param_7,
	.param .u32 ggml_matvec_f16_ncols_2_bs_192_param_8,
	.param .u32 ggml_matvec_f16_ncols_2_bs_192_param_9,
	.param .u32 ggml_matvec_f16_ncols_2_bs_192_param_10,
	.param .u32 ggml_matvec_f16_ncols_2_bs_192_param_11
)
{
	.local .align 8 .b8 	__local_depot105[8];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<30>;
	.reg .b16 	%rs<4>;
	.reg .f32 	%f<52>;
	.reg .b32 	%r<201>;
	.reg .b64 	%rd<66>;


	mov.u64 	%SPL, __local_depot105;
	ld.param.u64 	%rd27, [ggml_matvec_f16_ncols_2_bs_192_param_0];
	ld.param.u64 	%rd28, [ggml_matvec_f16_ncols_2_bs_192_param_1];
	ld.param.u64 	%rd26, [ggml_matvec_f16_ncols_2_bs_192_param_2];
	ld.param.u32 	%r28, [ggml_matvec_f16_ncols_2_bs_192_param_3];
	ld.param.u32 	%r32, [ggml_matvec_f16_ncols_2_bs_192_param_5];
	ld.param.u32 	%r29, [ggml_matvec_f16_ncols_2_bs_192_param_6];
	ld.param.u32 	%r30, [ggml_matvec_f16_ncols_2_bs_192_param_7];
	ld.param.u32 	%r33, [ggml_matvec_f16_ncols_2_bs_192_param_8];
	ld.param.u32 	%r34, [ggml_matvec_f16_ncols_2_bs_192_param_9];
	ld.param.u32 	%r35, [ggml_matvec_f16_ncols_2_bs_192_param_10];
	ld.param.u32 	%r31, [ggml_matvec_f16_ncols_2_bs_192_param_11];
	cvta.to.global.u64 	%rd1, %rd28;
	cvta.to.global.u64 	%rd2, %rd27;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r36, %r1, %r33;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r37, %r2, %r32;
	mad.lo.s32 	%r38, %r36, %r34, %r37;
	cvt.s64.s32 	%rd4, %r38;
	mul.lo.s32 	%r39, %r1, %r35;
	cvt.s64.s32 	%rd5, %r39;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r40, %r3, 2;
	mov.u32 	%r41, data_mmv;
	add.s32 	%r4, %r41, %r40;
	@%p1 bra 	$L__BB105_2;

	mov.u32 	%r42, 0;
	st.shared.u32 	[%r4], %r42;

$L__BB105_2:
	bar.sync 	0;
	mov.f32 	%f3, 0f00000000;
	st.local.v2.f32 	[%rd3], {%f3, %f3};
	mov.u32 	%r199, 0;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs2, %f3;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f3;}

	// end inline asm
	mov.b32 	%r200, {%rs1, %rs2};
	setp.ge.s32 	%p2, %r3, %r28;
	@%p2 bra 	$L__BB105_9;

	not.b32 	%r45, %r3;
	add.s32 	%r6, %r45, %r28;
	mul.wide.u32 	%rd30, %r6, -1431655765;
	shr.u64 	%rd31, %rd30, 39;
	cvt.u32.u64 	%r46, %rd31;
	add.s32 	%r47, %r46, 1;
	and.b32  	%r192, %r47, 3;
	setp.eq.s32 	%p3, %r192, 0;
	mov.u32 	%r199, 0;
	mov.u32 	%r195, %r3;
	@%p3 bra 	$L__BB105_6;

	mul.wide.s32 	%rd32, %r29, 2;
	mul.wide.s32 	%rd33, %r3, 2;
	add.s64 	%rd34, %rd32, %rd33;
	add.s64 	%rd35, %rd34, %rd5;
	shl.b64 	%rd36, %rd35, 1;
	add.s64 	%rd62, %rd1, %rd36;
	add.s64 	%rd37, %rd33, %rd5;
	shl.b64 	%rd38, %rd37, 1;
	add.s64 	%rd61, %rd1, %rd38;
	add.s64 	%rd39, %rd33, %rd4;
	shl.b64 	%rd40, %rd39, 1;
	add.s64 	%rd60, %rd2, %rd40;
	mov.u32 	%r199, 0;
	mov.u32 	%r195, %r3;

$L__BB105_5:
	.pragma "nounroll";
	ld.global.nc.u32 	%r50, [%rd60];
	ld.global.nc.u32 	%r51, [%rd61];
	// begin inline asm
	{mul.f16x2 %r49,%r50,%r51;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r200,%r200,%r49;
}
	// end inline asm
	ld.global.nc.u32 	%r57, [%rd62];
	// begin inline asm
	{mul.f16x2 %r55,%r50,%r57;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r199,%r199,%r55;
}
	// end inline asm
	add.s32 	%r195, %r195, 192;
	add.s64 	%rd62, %rd62, 768;
	add.s64 	%rd61, %rd61, 768;
	add.s64 	%rd60, %rd60, 768;
	add.s32 	%r192, %r192, -1;
	setp.ne.s32 	%p4, %r192, 0;
	@%p4 bra 	$L__BB105_5;

$L__BB105_6:
	setp.lt.u32 	%p5, %r6, 576;
	@%p5 bra 	$L__BB105_9;

	mul.wide.s32 	%rd41, %r195, 2;
	add.s64 	%rd42, %rd41, %rd4;
	shl.b64 	%rd43, %rd42, 1;
	add.s64 	%rd44, %rd2, %rd43;
	add.s64 	%rd65, %rd44, 1536;
	add.s64 	%rd45, %rd41, %rd5;
	shl.b64 	%rd46, %rd45, 1;
	add.s64 	%rd47, %rd1, %rd46;
	add.s64 	%rd64, %rd47, 2304;
	mul.wide.s32 	%rd48, %r29, 2;
	add.s64 	%rd49, %rd45, %rd48;
	shl.b64 	%rd50, %rd49, 1;
	add.s64 	%rd51, %rd1, %rd50;
	add.s64 	%rd63, %rd51, 1536;

$L__BB105_8:
	ld.global.nc.u32 	%r62, [%rd65+-1536];
	ld.global.nc.u32 	%r63, [%rd64+-2304];
	// begin inline asm
	{mul.f16x2 %r61,%r62,%r63;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r64,%r200,%r61;
}
	// end inline asm
	ld.global.nc.u32 	%r69, [%rd63+-1536];
	// begin inline asm
	{mul.f16x2 %r67,%r62,%r69;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r70,%r199,%r67;
}
	// end inline asm
	ld.global.nc.u32 	%r74, [%rd65+-768];
	ld.global.nc.u32 	%r75, [%rd64+-1536];
	// begin inline asm
	{mul.f16x2 %r73,%r74,%r75;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r76,%r64,%r73;
}
	// end inline asm
	ld.global.nc.u32 	%r81, [%rd63+-768];
	// begin inline asm
	{mul.f16x2 %r79,%r74,%r81;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r82,%r70,%r79;
}
	// end inline asm
	ld.global.nc.u32 	%r86, [%rd65];
	ld.global.nc.u32 	%r87, [%rd64+-768];
	// begin inline asm
	{mul.f16x2 %r85,%r86,%r87;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r88,%r76,%r85;
}
	// end inline asm
	ld.global.nc.u32 	%r93, [%rd63];
	// begin inline asm
	{mul.f16x2 %r91,%r86,%r93;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r94,%r82,%r91;
}
	// end inline asm
	ld.global.nc.u32 	%r98, [%rd65+768];
	ld.global.nc.u32 	%r99, [%rd64];
	// begin inline asm
	{mul.f16x2 %r97,%r98,%r99;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r200,%r88,%r97;
}
	// end inline asm
	ld.global.nc.u32 	%r105, [%rd63+768];
	// begin inline asm
	{mul.f16x2 %r103,%r98,%r105;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r199,%r94,%r103;
}
	// end inline asm
	add.s64 	%rd65, %rd65, 3072;
	add.s64 	%rd64, %rd64, 3072;
	add.s64 	%rd63, %rd63, 3072;
	add.s32 	%r195, %r195, 768;
	setp.lt.s32 	%p6, %r195, %r28;
	@%p6 bra 	$L__BB105_8;

$L__BB105_9:
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r200;
  cvt.f32.f16 %f4, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r200;
  cvt.f32.f16 %f5, high;}

	// end inline asm
	add.f32 	%f8, %f4, %f5;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r199;
  cvt.f32.f16 %f6, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r199;
  cvt.f32.f16 %f7, high;}

	// end inline asm
	add.f32 	%f1, %f6, %f7;
	st.local.f32 	[%rd3+4], %f1;
	shr.s32 	%r113, %r3, 31;
	shr.u32 	%r114, %r113, 27;
	add.s32 	%r115, %r3, %r114;
	shr.s32 	%r116, %r115, 5;
	shl.b32 	%r117, %r116, 2;
	add.s32 	%r27, %r41, %r117;
	mov.u32 	%r119, 2;
	mov.b32 	%r120, %f8;
	mov.u32 	%r121, 31;
	mov.u32 	%r122, 16;
	mov.u32 	%r123, -1;
	shfl.sync.bfly.b32 	%r124|%p7, %r120, %r122, %r121, %r123;
	mov.b32 	%f9, %r124;
	add.f32 	%f10, %f8, %f9;
	mov.b32 	%r125, %f10;
	mov.u32 	%r126, 8;
	shfl.sync.bfly.b32 	%r127|%p8, %r125, %r126, %r121, %r123;
	mov.b32 	%f11, %r127;
	add.f32 	%f12, %f10, %f11;
	mov.b32 	%r128, %f12;
	mov.u32 	%r129, 4;
	shfl.sync.bfly.b32 	%r130|%p9, %r128, %r129, %r121, %r123;
	mov.b32 	%f13, %r130;
	add.f32 	%f14, %f12, %f13;
	mov.b32 	%r131, %f14;
	shfl.sync.bfly.b32 	%r132|%p10, %r131, %r119, %r121, %r123;
	mov.b32 	%f15, %r132;
	add.f32 	%f16, %f14, %f15;
	mov.b32 	%r133, %f16;
	mov.u32 	%r134, 1;
	shfl.sync.bfly.b32 	%r135|%p11, %r133, %r134, %r121, %r123;
	mov.b32 	%f17, %r135;
	add.f32 	%f18, %f16, %f17;
	st.local.f32 	[%rd3], %f18;
	st.shared.f32 	[%r27], %f18;
	bar.sync 	0;
	@%p1 bra 	$L__BB105_11;

	ld.shared.f32 	%f19, [%r4];
	mov.b32 	%r136, %f19;
	shfl.sync.bfly.b32 	%r140|%p13, %r136, %r122, %r121, %r123;
	mov.b32 	%f20, %r140;
	add.f32 	%f21, %f19, %f20;
	mov.b32 	%r141, %f21;
	shfl.sync.bfly.b32 	%r143|%p14, %r141, %r126, %r121, %r123;
	mov.b32 	%f22, %r143;
	add.f32 	%f23, %f21, %f22;
	mov.b32 	%r144, %f23;
	shfl.sync.bfly.b32 	%r146|%p15, %r144, %r129, %r121, %r123;
	mov.b32 	%f24, %r146;
	add.f32 	%f25, %f23, %f24;
	mov.b32 	%r147, %f25;
	shfl.sync.bfly.b32 	%r149|%p16, %r147, %r119, %r121, %r123;
	mov.b32 	%f26, %r149;
	add.f32 	%f27, %f25, %f26;
	mov.b32 	%r150, %f27;
	shfl.sync.bfly.b32 	%r152|%p17, %r150, %r134, %r121, %r123;
	mov.b32 	%f28, %r152;
	add.f32 	%f29, %f27, %f28;
	st.local.f32 	[%rd3], %f29;

$L__BB105_11:
	bar.sync 	0;
	mov.b32 	%r153, %f1;
	shfl.sync.bfly.b32 	%r157|%p19, %r153, %r122, %r121, %r123;
	mov.b32 	%f30, %r157;
	add.f32 	%f31, %f1, %f30;
	mov.b32 	%r158, %f31;
	shfl.sync.bfly.b32 	%r160|%p20, %r158, %r126, %r121, %r123;
	mov.b32 	%f32, %r160;
	add.f32 	%f33, %f31, %f32;
	mov.b32 	%r161, %f33;
	shfl.sync.bfly.b32 	%r163|%p21, %r161, %r129, %r121, %r123;
	mov.b32 	%f34, %r163;
	add.f32 	%f35, %f33, %f34;
	mov.b32 	%r164, %f35;
	shfl.sync.bfly.b32 	%r166|%p22, %r164, %r119, %r121, %r123;
	mov.b32 	%f36, %r166;
	add.f32 	%f37, %f35, %f36;
	mov.b32 	%r167, %f37;
	shfl.sync.bfly.b32 	%r169|%p23, %r167, %r134, %r121, %r123;
	mov.b32 	%f38, %r169;
	add.f32 	%f39, %f37, %f38;
	st.local.f32 	[%rd3+4], %f39;
	st.shared.f32 	[%r27], %f39;
	bar.sync 	0;
	@%p1 bra 	$L__BB105_13;

	ld.shared.f32 	%f40, [%r4];
	mov.b32 	%r170, %f40;
	mov.u32 	%r171, 31;
	mov.u32 	%r172, 16;
	mov.u32 	%r173, -1;
	shfl.sync.bfly.b32 	%r174|%p24, %r170, %r172, %r171, %r173;
	mov.b32 	%f41, %r174;
	add.f32 	%f42, %f40, %f41;
	mov.b32 	%r175, %f42;
	mov.u32 	%r176, 8;
	shfl.sync.bfly.b32 	%r177|%p25, %r175, %r176, %r171, %r173;
	mov.b32 	%f43, %r177;
	add.f32 	%f44, %f42, %f43;
	mov.b32 	%r178, %f44;
	mov.u32 	%r179, 4;
	shfl.sync.bfly.b32 	%r180|%p26, %r178, %r179, %r171, %r173;
	mov.b32 	%f45, %r180;
	add.f32 	%f46, %f44, %f45;
	mov.b32 	%r181, %f46;
	mov.u32 	%r182, 2;
	shfl.sync.bfly.b32 	%r183|%p27, %r181, %r182, %r171, %r173;
	mov.b32 	%f47, %r183;
	add.f32 	%f48, %f46, %f47;
	mov.b32 	%r184, %f48;
	mov.u32 	%r185, 1;
	shfl.sync.bfly.b32 	%r186|%p28, %r184, %r185, %r171, %r173;
	mov.b32 	%f49, %r186;
	add.f32 	%f50, %f48, %f49;
	st.local.f32 	[%rd3+4], %f50;

$L__BB105_13:
	bar.sync 	0;
	setp.gt.s32 	%p29, %r3, 1;
	@%p29 bra 	$L__BB105_15;

	mad.lo.s32 	%r187, %r3, %r30, %r2;
	cvt.s64.s32 	%rd52, %r187;
	mul.lo.s32 	%r188, %r1, %r31;
	cvt.s64.s32 	%rd53, %r188;
	add.s64 	%rd54, %rd53, %rd52;
	mul.wide.s32 	%rd55, %r3, 4;
	add.s64 	%rd56, %rd3, %rd55;
	ld.local.f32 	%f51, [%rd56];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs3, %f51;}

	// end inline asm
	cvta.to.global.u64 	%rd57, %rd26;
	shl.b64 	%rd58, %rd54, 1;
	add.s64 	%rd59, %rd57, %rd58;
	st.global.u16 	[%rd59], %rs3;

$L__BB105_15:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_3_bs_192
.visible .entry ggml_matvec_f16_ncols_3_bs_192(
	.param .u64 ggml_matvec_f16_ncols_3_bs_192_param_0,
	.param .u64 ggml_matvec_f16_ncols_3_bs_192_param_1,
	.param .u64 ggml_matvec_f16_ncols_3_bs_192_param_2,
	.param .u32 ggml_matvec_f16_ncols_3_bs_192_param_3,
	.param .u32 ggml_matvec_f16_ncols_3_bs_192_param_4,
	.param .u32 ggml_matvec_f16_ncols_3_bs_192_param_5,
	.param .u32 ggml_matvec_f16_ncols_3_bs_192_param_6,
	.param .u32 ggml_matvec_f16_ncols_3_bs_192_param_7,
	.param .u32 ggml_matvec_f16_ncols_3_bs_192_param_8,
	.param .u32 ggml_matvec_f16_ncols_3_bs_192_param_9,
	.param .u32 ggml_matvec_f16_ncols_3_bs_192_param_10,
	.param .u32 ggml_matvec_f16_ncols_3_bs_192_param_11
)
{
	.local .align 4 .b8 	__local_depot106[12];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<41>;
	.reg .b16 	%rs<4>;
	.reg .f32 	%f<76>;
	.reg .b32 	%r<286>;
	.reg .b64 	%rd<74>;


	mov.u64 	%SPL, __local_depot106;
	ld.param.u64 	%rd29, [ggml_matvec_f16_ncols_3_bs_192_param_0];
	ld.param.u64 	%rd30, [ggml_matvec_f16_ncols_3_bs_192_param_1];
	ld.param.u64 	%rd28, [ggml_matvec_f16_ncols_3_bs_192_param_2];
	ld.param.u32 	%r34, [ggml_matvec_f16_ncols_3_bs_192_param_3];
	ld.param.u32 	%r38, [ggml_matvec_f16_ncols_3_bs_192_param_5];
	ld.param.u32 	%r35, [ggml_matvec_f16_ncols_3_bs_192_param_6];
	ld.param.u32 	%r36, [ggml_matvec_f16_ncols_3_bs_192_param_7];
	ld.param.u32 	%r39, [ggml_matvec_f16_ncols_3_bs_192_param_8];
	ld.param.u32 	%r40, [ggml_matvec_f16_ncols_3_bs_192_param_9];
	ld.param.u32 	%r41, [ggml_matvec_f16_ncols_3_bs_192_param_10];
	ld.param.u32 	%r37, [ggml_matvec_f16_ncols_3_bs_192_param_11];
	cvta.to.global.u64 	%rd73, %rd30;
	cvta.to.global.u64 	%rd2, %rd29;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r42, %r1, %r39;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r43, %r2, %r38;
	mad.lo.s32 	%r44, %r42, %r40, %r43;
	cvt.s64.s32 	%rd4, %r44;
	mul.lo.s32 	%r45, %r1, %r41;
	cvt.s64.s32 	%rd5, %r45;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r46, %r3, 2;
	mov.u32 	%r47, data_mmv;
	add.s32 	%r4, %r47, %r46;
	@%p1 bra 	$L__BB106_2;

	mov.u32 	%r48, 0;
	st.shared.u32 	[%r4], %r48;

$L__BB106_2:
	bar.sync 	0;
	mov.f32 	%f4, 0f00000000;
	mov.u32 	%r283, 0;
	st.local.u32 	[%rd3], %r283;
	st.local.u32 	[%rd3+4], %r283;
	st.local.u32 	[%rd3+8], %r283;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs2, %f4;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f4;}

	// end inline asm
	mov.b32 	%r285, {%rs1, %rs2};
	setp.ge.s32 	%p2, %r3, %r34;
	mov.u32 	%r284, %r283;
	@%p2 bra 	$L__BB106_9;

	not.b32 	%r53, %r3;
	add.s32 	%r6, %r53, %r34;
	mul.wide.u32 	%rd32, %r6, -1431655765;
	shr.u64 	%rd33, %rd32, 39;
	cvt.u32.u64 	%r54, %rd33;
	add.s32 	%r55, %r54, 1;
	and.b32  	%r274, %r55, 3;
	setp.eq.s32 	%p3, %r274, 0;
	mov.u32 	%r283, 0;
	mov.u32 	%r278, %r3;
	@%p3 bra 	$L__BB106_6;

	shl.b32 	%r58, %r35, 1;
	add.s32 	%r59, %r3, %r58;
	mul.wide.s32 	%rd34, %r59, 2;
	add.s64 	%rd35, %rd34, %rd5;
	shl.b64 	%rd36, %rd35, 1;
	add.s64 	%rd71, %rd73, %rd36;
	mul.wide.s32 	%rd37, %r35, 2;
	mul.wide.s32 	%rd38, %r3, 2;
	add.s64 	%rd39, %rd37, %rd38;
	add.s64 	%rd40, %rd39, %rd5;
	shl.b64 	%rd41, %rd40, 1;
	add.s64 	%rd70, %rd73, %rd41;
	add.s64 	%rd42, %rd38, %rd5;
	shl.b64 	%rd43, %rd42, 1;
	add.s64 	%rd69, %rd73, %rd43;
	add.s64 	%rd44, %rd38, %rd4;
	shl.b64 	%rd45, %rd44, 1;
	add.s64 	%rd68, %rd2, %rd45;
	mov.u32 	%r283, 0;
	mov.u32 	%r284, %r283;
	mov.u32 	%r278, %r3;

$L__BB106_5:
	.pragma "nounroll";
	ld.global.nc.u32 	%r61, [%rd68];
	ld.global.nc.u32 	%r62, [%rd69];
	// begin inline asm
	{mul.f16x2 %r60,%r61,%r62;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r285,%r285,%r60;
}
	// end inline asm
	ld.global.nc.u32 	%r68, [%rd70];
	// begin inline asm
	{mul.f16x2 %r66,%r61,%r68;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r284,%r284,%r66;
}
	// end inline asm
	ld.global.nc.u32 	%r74, [%rd71];
	// begin inline asm
	{mul.f16x2 %r72,%r61,%r74;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r283,%r283,%r72;
}
	// end inline asm
	add.s32 	%r278, %r278, 192;
	add.s64 	%rd71, %rd71, 768;
	add.s64 	%rd70, %rd70, 768;
	add.s64 	%rd69, %rd69, 768;
	add.s64 	%rd68, %rd68, 768;
	add.s32 	%r274, %r274, -1;
	setp.ne.s32 	%p4, %r274, 0;
	@%p4 bra 	$L__BB106_5;

$L__BB106_6:
	setp.lt.u32 	%p5, %r6, 576;
	@%p5 bra 	$L__BB106_9;

	add.s32 	%r78, %r278, %r35;
	shl.b32 	%r79, %r35, 1;
	add.s32 	%r80, %r278, %r79;
	add.s32 	%r81, %r78, 192;
	mul.wide.s32 	%rd46, %r81, 4;
	shl.b64 	%rd47, %rd5, 1;
	add.s64 	%rd19, %rd46, %rd47;
	mul.wide.s32 	%rd48, %r80, 4;
	add.s64 	%rd20, %rd48, %rd47;
	mul.wide.s32 	%rd49, %r278, 2;
	add.s64 	%rd50, %rd49, %rd4;
	shl.b64 	%rd51, %rd50, 1;
	add.s64 	%rd52, %rd2, %rd51;
	add.s64 	%rd72, %rd52, 1536;
	mul.wide.s32 	%rd53, %r278, 4;
	add.s64 	%rd22, %rd53, %rd47;
	mul.wide.s32 	%rd54, %r35, 4;
	add.s64 	%rd55, %rd53, %rd54;
	add.s64 	%rd23, %rd55, %rd47;

$L__BB106_8:
	ld.global.nc.u32 	%r83, [%rd72+-1536];
	add.s64 	%rd56, %rd73, %rd22;
	ld.global.nc.u32 	%r84, [%rd56];
	// begin inline asm
	{mul.f16x2 %r82,%r83,%r84;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r85,%r285,%r82;
}
	// end inline asm
	add.s64 	%rd57, %rd73, %rd23;
	ld.global.nc.u32 	%r90, [%rd57];
	// begin inline asm
	{mul.f16x2 %r88,%r83,%r90;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r91,%r284,%r88;
}
	// end inline asm
	add.s64 	%rd58, %rd73, %rd20;
	ld.global.nc.u32 	%r96, [%rd58];
	// begin inline asm
	{mul.f16x2 %r94,%r83,%r96;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r97,%r283,%r94;
}
	// end inline asm
	ld.global.nc.u32 	%r101, [%rd72+-768];
	ld.global.nc.u32 	%r102, [%rd56+768];
	// begin inline asm
	{mul.f16x2 %r100,%r101,%r102;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r103,%r85,%r100;
}
	// end inline asm
	add.s64 	%rd59, %rd73, %rd19;
	ld.global.nc.u32 	%r108, [%rd59];
	// begin inline asm
	{mul.f16x2 %r106,%r101,%r108;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r109,%r91,%r106;
}
	// end inline asm
	ld.global.nc.u32 	%r114, [%rd58+768];
	// begin inline asm
	{mul.f16x2 %r112,%r101,%r114;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r115,%r97,%r112;
}
	// end inline asm
	ld.global.nc.u32 	%r119, [%rd72];
	ld.global.nc.u32 	%r120, [%rd56+1536];
	// begin inline asm
	{mul.f16x2 %r118,%r119,%r120;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r121,%r103,%r118;
}
	// end inline asm
	ld.global.nc.u32 	%r126, [%rd59+768];
	// begin inline asm
	{mul.f16x2 %r124,%r119,%r126;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r127,%r109,%r124;
}
	// end inline asm
	ld.global.nc.u32 	%r132, [%rd58+1536];
	// begin inline asm
	{mul.f16x2 %r130,%r119,%r132;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r133,%r115,%r130;
}
	// end inline asm
	ld.global.nc.u32 	%r137, [%rd72+768];
	ld.global.nc.u32 	%r138, [%rd56+2304];
	// begin inline asm
	{mul.f16x2 %r136,%r137,%r138;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r285,%r121,%r136;
}
	// end inline asm
	ld.global.nc.u32 	%r144, [%rd59+1536];
	// begin inline asm
	{mul.f16x2 %r142,%r137,%r144;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r284,%r127,%r142;
}
	// end inline asm
	ld.global.nc.u32 	%r150, [%rd58+2304];
	// begin inline asm
	{mul.f16x2 %r148,%r137,%r150;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r283,%r133,%r148;
}
	// end inline asm
	add.s64 	%rd73, %rd73, 3072;
	add.s64 	%rd72, %rd72, 3072;
	add.s32 	%r278, %r278, 768;
	setp.lt.s32 	%p6, %r278, %r34;
	@%p6 bra 	$L__BB106_8;

$L__BB106_9:
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r285;
  cvt.f32.f16 %f5, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r285;
  cvt.f32.f16 %f6, high;}

	// end inline asm
	add.f32 	%f11, %f5, %f6;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r284;
  cvt.f32.f16 %f7, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r284;
  cvt.f32.f16 %f8, high;}

	// end inline asm
	add.f32 	%f1, %f7, %f8;
	st.local.f32 	[%rd3+4], %f1;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r283;
  cvt.f32.f16 %f9, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r283;
  cvt.f32.f16 %f10, high;}

	// end inline asm
	add.f32 	%f2, %f9, %f10;
	st.local.f32 	[%rd3+8], %f2;
	shr.s32 	%r160, %r3, 31;
	shr.u32 	%r161, %r160, 27;
	add.s32 	%r162, %r3, %r161;
	shr.s32 	%r163, %r162, 5;
	shl.b32 	%r164, %r163, 2;
	add.s32 	%r33, %r47, %r164;
	mov.u32 	%r166, 2;
	mov.b32 	%r167, %f11;
	mov.u32 	%r168, 31;
	mov.u32 	%r169, 16;
	mov.u32 	%r170, -1;
	shfl.sync.bfly.b32 	%r171|%p7, %r167, %r169, %r168, %r170;
	mov.b32 	%f12, %r171;
	add.f32 	%f13, %f11, %f12;
	mov.b32 	%r172, %f13;
	mov.u32 	%r173, 8;
	shfl.sync.bfly.b32 	%r174|%p8, %r172, %r173, %r168, %r170;
	mov.b32 	%f14, %r174;
	add.f32 	%f15, %f13, %f14;
	mov.b32 	%r175, %f15;
	mov.u32 	%r176, 4;
	shfl.sync.bfly.b32 	%r177|%p9, %r175, %r176, %r168, %r170;
	mov.b32 	%f16, %r177;
	add.f32 	%f17, %f15, %f16;
	mov.b32 	%r178, %f17;
	shfl.sync.bfly.b32 	%r179|%p10, %r178, %r166, %r168, %r170;
	mov.b32 	%f18, %r179;
	add.f32 	%f19, %f17, %f18;
	mov.b32 	%r180, %f19;
	mov.u32 	%r181, 1;
	shfl.sync.bfly.b32 	%r182|%p11, %r180, %r181, %r168, %r170;
	mov.b32 	%f20, %r182;
	add.f32 	%f21, %f19, %f20;
	st.local.f32 	[%rd3], %f21;
	st.shared.f32 	[%r33], %f21;
	bar.sync 	0;
	@%p1 bra 	$L__BB106_11;

	ld.shared.f32 	%f22, [%r4];
	mov.b32 	%r183, %f22;
	shfl.sync.bfly.b32 	%r187|%p13, %r183, %r169, %r168, %r170;
	mov.b32 	%f23, %r187;
	add.f32 	%f24, %f22, %f23;
	mov.b32 	%r188, %f24;
	shfl.sync.bfly.b32 	%r190|%p14, %r188, %r173, %r168, %r170;
	mov.b32 	%f25, %r190;
	add.f32 	%f26, %f24, %f25;
	mov.b32 	%r191, %f26;
	shfl.sync.bfly.b32 	%r193|%p15, %r191, %r176, %r168, %r170;
	mov.b32 	%f27, %r193;
	add.f32 	%f28, %f26, %f27;
	mov.b32 	%r194, %f28;
	shfl.sync.bfly.b32 	%r196|%p16, %r194, %r166, %r168, %r170;
	mov.b32 	%f29, %r196;
	add.f32 	%f30, %f28, %f29;
	mov.b32 	%r197, %f30;
	shfl.sync.bfly.b32 	%r199|%p17, %r197, %r181, %r168, %r170;
	mov.b32 	%f31, %r199;
	add.f32 	%f32, %f30, %f31;
	st.local.f32 	[%rd3], %f32;

$L__BB106_11:
	bar.sync 	0;
	mov.b32 	%r200, %f1;
	shfl.sync.bfly.b32 	%r204|%p19, %r200, %r169, %r168, %r170;
	mov.b32 	%f33, %r204;
	add.f32 	%f34, %f1, %f33;
	mov.b32 	%r205, %f34;
	shfl.sync.bfly.b32 	%r207|%p20, %r205, %r173, %r168, %r170;
	mov.b32 	%f35, %r207;
	add.f32 	%f36, %f34, %f35;
	mov.b32 	%r208, %f36;
	shfl.sync.bfly.b32 	%r210|%p21, %r208, %r176, %r168, %r170;
	mov.b32 	%f37, %r210;
	add.f32 	%f38, %f36, %f37;
	mov.b32 	%r211, %f38;
	shfl.sync.bfly.b32 	%r213|%p22, %r211, %r166, %r168, %r170;
	mov.b32 	%f39, %r213;
	add.f32 	%f40, %f38, %f39;
	mov.b32 	%r214, %f40;
	shfl.sync.bfly.b32 	%r216|%p23, %r214, %r181, %r168, %r170;
	mov.b32 	%f41, %r216;
	add.f32 	%f42, %f40, %f41;
	st.local.f32 	[%rd3+4], %f42;
	st.shared.f32 	[%r33], %f42;
	bar.sync 	0;
	@%p1 bra 	$L__BB106_13;

	ld.shared.f32 	%f43, [%r4];
	mov.b32 	%r217, %f43;
	mov.u32 	%r218, 31;
	mov.u32 	%r219, 16;
	mov.u32 	%r220, -1;
	shfl.sync.bfly.b32 	%r221|%p24, %r217, %r219, %r218, %r220;
	mov.b32 	%f44, %r221;
	add.f32 	%f45, %f43, %f44;
	mov.b32 	%r222, %f45;
	mov.u32 	%r223, 8;
	shfl.sync.bfly.b32 	%r224|%p25, %r222, %r223, %r218, %r220;
	mov.b32 	%f46, %r224;
	add.f32 	%f47, %f45, %f46;
	mov.b32 	%r225, %f47;
	mov.u32 	%r226, 4;
	shfl.sync.bfly.b32 	%r227|%p26, %r225, %r226, %r218, %r220;
	mov.b32 	%f48, %r227;
	add.f32 	%f49, %f47, %f48;
	mov.b32 	%r228, %f49;
	mov.u32 	%r229, 2;
	shfl.sync.bfly.b32 	%r230|%p27, %r228, %r229, %r218, %r220;
	mov.b32 	%f50, %r230;
	add.f32 	%f51, %f49, %f50;
	mov.b32 	%r231, %f51;
	mov.u32 	%r232, 1;
	shfl.sync.bfly.b32 	%r233|%p28, %r231, %r232, %r218, %r220;
	mov.b32 	%f52, %r233;
	add.f32 	%f53, %f51, %f52;
	st.local.f32 	[%rd3+4], %f53;

$L__BB106_13:
	bar.sync 	0;
	mov.b32 	%r234, %f2;
	mov.u32 	%r235, 31;
	mov.u32 	%r236, 16;
	mov.u32 	%r237, -1;
	shfl.sync.bfly.b32 	%r238|%p30, %r234, %r236, %r235, %r237;
	mov.b32 	%f54, %r238;
	add.f32 	%f55, %f2, %f54;
	mov.b32 	%r239, %f55;
	mov.u32 	%r240, 8;
	shfl.sync.bfly.b32 	%r241|%p31, %r239, %r240, %r235, %r237;
	mov.b32 	%f56, %r241;
	add.f32 	%f57, %f55, %f56;
	mov.b32 	%r242, %f57;
	mov.u32 	%r243, 4;
	shfl.sync.bfly.b32 	%r244|%p32, %r242, %r243, %r235, %r237;
	mov.b32 	%f58, %r244;
	add.f32 	%f59, %f57, %f58;
	mov.b32 	%r245, %f59;
	mov.u32 	%r246, 2;
	shfl.sync.bfly.b32 	%r247|%p33, %r245, %r246, %r235, %r237;
	mov.b32 	%f60, %r247;
	add.f32 	%f61, %f59, %f60;
	mov.b32 	%r248, %f61;
	mov.u32 	%r249, 1;
	shfl.sync.bfly.b32 	%r250|%p34, %r248, %r249, %r235, %r237;
	mov.b32 	%f62, %r250;
	add.f32 	%f63, %f61, %f62;
	st.local.f32 	[%rd3+8], %f63;
	st.shared.f32 	[%r33], %f63;
	bar.sync 	0;
	@%p1 bra 	$L__BB106_15;

	ld.shared.f32 	%f64, [%r4];
	mov.b32 	%r251, %f64;
	shfl.sync.bfly.b32 	%r255|%p35, %r251, %r236, %r235, %r237;
	mov.b32 	%f65, %r255;
	add.f32 	%f66, %f64, %f65;
	mov.b32 	%r256, %f66;
	shfl.sync.bfly.b32 	%r258|%p36, %r256, %r240, %r235, %r237;
	mov.b32 	%f67, %r258;
	add.f32 	%f68, %f66, %f67;
	mov.b32 	%r259, %f68;
	shfl.sync.bfly.b32 	%r261|%p37, %r259, %r243, %r235, %r237;
	mov.b32 	%f69, %r261;
	add.f32 	%f70, %f68, %f69;
	mov.b32 	%r262, %f70;
	shfl.sync.bfly.b32 	%r264|%p38, %r262, %r246, %r235, %r237;
	mov.b32 	%f71, %r264;
	add.f32 	%f72, %f70, %f71;
	mov.b32 	%r265, %f72;
	shfl.sync.bfly.b32 	%r267|%p39, %r265, %r249, %r235, %r237;
	mov.b32 	%f73, %r267;
	add.f32 	%f74, %f72, %f73;
	st.local.f32 	[%rd3+8], %f74;

$L__BB106_15:
	bar.sync 	0;
	setp.gt.s32 	%p40, %r3, 2;
	@%p40 bra 	$L__BB106_17;

	mad.lo.s32 	%r268, %r3, %r36, %r2;
	cvt.s64.s32 	%rd60, %r268;
	mul.lo.s32 	%r269, %r1, %r37;
	cvt.s64.s32 	%rd61, %r269;
	add.s64 	%rd62, %rd61, %rd60;
	mul.wide.s32 	%rd63, %r3, 4;
	add.s64 	%rd64, %rd3, %rd63;
	ld.local.f32 	%f75, [%rd64];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs3, %f75;}

	// end inline asm
	cvta.to.global.u64 	%rd65, %rd28;
	shl.b64 	%rd66, %rd62, 1;
	add.s64 	%rd67, %rd65, %rd66;
	st.global.u16 	[%rd67], %rs3;

$L__BB106_17:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_4_bs_192
.visible .entry ggml_matvec_f16_ncols_4_bs_192(
	.param .u64 ggml_matvec_f16_ncols_4_bs_192_param_0,
	.param .u64 ggml_matvec_f16_ncols_4_bs_192_param_1,
	.param .u64 ggml_matvec_f16_ncols_4_bs_192_param_2,
	.param .u32 ggml_matvec_f16_ncols_4_bs_192_param_3,
	.param .u32 ggml_matvec_f16_ncols_4_bs_192_param_4,
	.param .u32 ggml_matvec_f16_ncols_4_bs_192_param_5,
	.param .u32 ggml_matvec_f16_ncols_4_bs_192_param_6,
	.param .u32 ggml_matvec_f16_ncols_4_bs_192_param_7,
	.param .u32 ggml_matvec_f16_ncols_4_bs_192_param_8,
	.param .u32 ggml_matvec_f16_ncols_4_bs_192_param_9,
	.param .u32 ggml_matvec_f16_ncols_4_bs_192_param_10,
	.param .u32 ggml_matvec_f16_ncols_4_bs_192_param_11
)
{
	.local .align 16 .b8 	__local_depot107[16];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<52>;
	.reg .b16 	%rs<4>;
	.reg .f32 	%f<100>;
	.reg .b32 	%r<367>;
	.reg .b64 	%rd<85>;


	mov.u64 	%SPL, __local_depot107;
	ld.param.u64 	%rd34, [ggml_matvec_f16_ncols_4_bs_192_param_0];
	ld.param.u64 	%rd35, [ggml_matvec_f16_ncols_4_bs_192_param_1];
	ld.param.u64 	%rd33, [ggml_matvec_f16_ncols_4_bs_192_param_2];
	ld.param.u32 	%r40, [ggml_matvec_f16_ncols_4_bs_192_param_3];
	ld.param.u32 	%r44, [ggml_matvec_f16_ncols_4_bs_192_param_5];
	ld.param.u32 	%r41, [ggml_matvec_f16_ncols_4_bs_192_param_6];
	ld.param.u32 	%r42, [ggml_matvec_f16_ncols_4_bs_192_param_7];
	ld.param.u32 	%r45, [ggml_matvec_f16_ncols_4_bs_192_param_8];
	ld.param.u32 	%r46, [ggml_matvec_f16_ncols_4_bs_192_param_9];
	ld.param.u32 	%r47, [ggml_matvec_f16_ncols_4_bs_192_param_10];
	ld.param.u32 	%r43, [ggml_matvec_f16_ncols_4_bs_192_param_11];
	cvta.to.global.u64 	%rd84, %rd35;
	cvta.to.global.u64 	%rd2, %rd34;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r48, %r1, %r45;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r49, %r2, %r44;
	mad.lo.s32 	%r50, %r48, %r46, %r49;
	cvt.s64.s32 	%rd4, %r50;
	mul.lo.s32 	%r51, %r1, %r47;
	cvt.s64.s32 	%rd5, %r51;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r52, %r3, 2;
	mov.u32 	%r53, data_mmv;
	add.s32 	%r4, %r53, %r52;
	@%p1 bra 	$L__BB107_2;

	mov.u32 	%r54, 0;
	st.shared.u32 	[%r4], %r54;

$L__BB107_2:
	bar.sync 	0;
	mov.f32 	%f5, 0f00000000;
	st.local.v4.f32 	[%rd3], {%f5, %f5, %f5, %f5};
	mov.u32 	%r363, 0;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs2, %f5;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f5;}

	// end inline asm
	mov.b32 	%r366, {%rs1, %rs2};
	setp.ge.s32 	%p2, %r3, %r40;
	mov.u32 	%r364, %r363;
	mov.u32 	%r365, %r363;
	@%p2 bra 	$L__BB107_9;

	not.b32 	%r61, %r3;
	add.s32 	%r6, %r61, %r40;
	mul.wide.u32 	%rd37, %r6, -1431655765;
	shr.u64 	%rd38, %rd37, 39;
	cvt.u32.u64 	%r62, %rd38;
	add.s32 	%r63, %r62, 1;
	and.b32  	%r352, %r63, 3;
	setp.eq.s32 	%p3, %r352, 0;
	mov.u32 	%r363, 0;
	mov.u32 	%r357, %r3;
	@%p3 bra 	$L__BB107_6;

	shl.b32 	%r67, %r41, 1;
	add.s32 	%r68, %r3, %r67;
	mul.wide.s32 	%rd39, %r68, 2;
	add.s64 	%rd40, %rd39, %rd5;
	shl.b64 	%rd41, %rd40, 1;
	add.s64 	%rd82, %rd84, %rd41;
	mad.lo.s32 	%r69, %r41, 3, %r3;
	mul.wide.s32 	%rd42, %r69, 2;
	add.s64 	%rd43, %rd42, %rd5;
	shl.b64 	%rd44, %rd43, 1;
	add.s64 	%rd81, %rd84, %rd44;
	mul.wide.s32 	%rd45, %r41, 2;
	mul.wide.s32 	%rd46, %r3, 2;
	add.s64 	%rd47, %rd45, %rd46;
	add.s64 	%rd48, %rd47, %rd5;
	shl.b64 	%rd49, %rd48, 1;
	add.s64 	%rd80, %rd84, %rd49;
	add.s64 	%rd50, %rd46, %rd5;
	shl.b64 	%rd51, %rd50, 1;
	add.s64 	%rd79, %rd84, %rd51;
	add.s64 	%rd52, %rd46, %rd4;
	shl.b64 	%rd53, %rd52, 1;
	add.s64 	%rd78, %rd2, %rd53;
	mov.u32 	%r363, 0;
	mov.u32 	%r364, %r363;
	mov.u32 	%r365, %r363;
	mov.u32 	%r357, %r3;

$L__BB107_5:
	.pragma "nounroll";
	ld.global.nc.u32 	%r71, [%rd78];
	ld.global.nc.u32 	%r72, [%rd79];
	// begin inline asm
	{mul.f16x2 %r70,%r71,%r72;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r366,%r366,%r70;
}
	// end inline asm
	ld.global.nc.u32 	%r78, [%rd80];
	// begin inline asm
	{mul.f16x2 %r76,%r71,%r78;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r365,%r365,%r76;
}
	// end inline asm
	ld.global.nc.u32 	%r84, [%rd82];
	// begin inline asm
	{mul.f16x2 %r82,%r71,%r84;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r364,%r364,%r82;
}
	// end inline asm
	ld.global.nc.u32 	%r90, [%rd81];
	// begin inline asm
	{mul.f16x2 %r88,%r71,%r90;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r363,%r363,%r88;
}
	// end inline asm
	add.s32 	%r357, %r357, 192;
	add.s64 	%rd82, %rd82, 768;
	add.s64 	%rd81, %rd81, 768;
	add.s64 	%rd80, %rd80, 768;
	add.s64 	%rd79, %rd79, 768;
	add.s64 	%rd78, %rd78, 768;
	add.s32 	%r352, %r352, -1;
	setp.ne.s32 	%p4, %r352, 0;
	@%p4 bra 	$L__BB107_5;

$L__BB107_6:
	setp.lt.u32 	%p5, %r6, 576;
	@%p5 bra 	$L__BB107_9;

	add.s32 	%r94, %r357, %r41;
	shl.b32 	%r95, %r41, 1;
	add.s32 	%r96, %r357, %r95;
	mad.lo.s32 	%r97, %r41, 3, %r357;
	add.s32 	%r98, %r94, 192;
	mul.wide.s32 	%rd54, %r98, 4;
	shl.b64 	%rd55, %rd5, 1;
	add.s64 	%rd23, %rd54, %rd55;
	mul.wide.s32 	%rd56, %r96, 4;
	add.s64 	%rd24, %rd56, %rd55;
	mul.wide.s32 	%rd57, %r97, 4;
	add.s64 	%rd25, %rd57, %rd55;
	mul.wide.s32 	%rd58, %r357, 2;
	add.s64 	%rd59, %rd58, %rd4;
	shl.b64 	%rd60, %rd59, 1;
	add.s64 	%rd61, %rd2, %rd60;
	add.s64 	%rd83, %rd61, 1536;
	mul.wide.s32 	%rd62, %r357, 4;
	add.s64 	%rd27, %rd62, %rd55;
	mul.wide.s32 	%rd63, %r41, 4;
	add.s64 	%rd64, %rd62, %rd63;
	add.s64 	%rd28, %rd64, %rd55;

$L__BB107_8:
	ld.global.nc.u32 	%r100, [%rd83+-1536];
	add.s64 	%rd65, %rd84, %rd27;
	ld.global.nc.u32 	%r101, [%rd65];
	// begin inline asm
	{mul.f16x2 %r99,%r100,%r101;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r102,%r366,%r99;
}
	// end inline asm
	add.s64 	%rd66, %rd84, %rd28;
	ld.global.nc.u32 	%r107, [%rd66];
	// begin inline asm
	{mul.f16x2 %r105,%r100,%r107;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r108,%r365,%r105;
}
	// end inline asm
	add.s64 	%rd67, %rd84, %rd24;
	ld.global.nc.u32 	%r113, [%rd67];
	// begin inline asm
	{mul.f16x2 %r111,%r100,%r113;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r114,%r364,%r111;
}
	// end inline asm
	add.s64 	%rd68, %rd84, %rd25;
	ld.global.nc.u32 	%r119, [%rd68];
	// begin inline asm
	{mul.f16x2 %r117,%r100,%r119;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r120,%r363,%r117;
}
	// end inline asm
	ld.global.nc.u32 	%r124, [%rd83+-768];
	ld.global.nc.u32 	%r125, [%rd65+768];
	// begin inline asm
	{mul.f16x2 %r123,%r124,%r125;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r126,%r102,%r123;
}
	// end inline asm
	add.s64 	%rd69, %rd84, %rd23;
	ld.global.nc.u32 	%r131, [%rd69];
	// begin inline asm
	{mul.f16x2 %r129,%r124,%r131;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r132,%r108,%r129;
}
	// end inline asm
	ld.global.nc.u32 	%r137, [%rd67+768];
	// begin inline asm
	{mul.f16x2 %r135,%r124,%r137;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r138,%r114,%r135;
}
	// end inline asm
	ld.global.nc.u32 	%r143, [%rd68+768];
	// begin inline asm
	{mul.f16x2 %r141,%r124,%r143;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r144,%r120,%r141;
}
	// end inline asm
	ld.global.nc.u32 	%r148, [%rd83];
	ld.global.nc.u32 	%r149, [%rd65+1536];
	// begin inline asm
	{mul.f16x2 %r147,%r148,%r149;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r150,%r126,%r147;
}
	// end inline asm
	ld.global.nc.u32 	%r155, [%rd69+768];
	// begin inline asm
	{mul.f16x2 %r153,%r148,%r155;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r156,%r132,%r153;
}
	// end inline asm
	ld.global.nc.u32 	%r161, [%rd67+1536];
	// begin inline asm
	{mul.f16x2 %r159,%r148,%r161;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r162,%r138,%r159;
}
	// end inline asm
	ld.global.nc.u32 	%r167, [%rd68+1536];
	// begin inline asm
	{mul.f16x2 %r165,%r148,%r167;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r168,%r144,%r165;
}
	// end inline asm
	ld.global.nc.u32 	%r172, [%rd83+768];
	ld.global.nc.u32 	%r173, [%rd65+2304];
	// begin inline asm
	{mul.f16x2 %r171,%r172,%r173;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r366,%r150,%r171;
}
	// end inline asm
	ld.global.nc.u32 	%r179, [%rd69+1536];
	// begin inline asm
	{mul.f16x2 %r177,%r172,%r179;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r365,%r156,%r177;
}
	// end inline asm
	ld.global.nc.u32 	%r185, [%rd67+2304];
	// begin inline asm
	{mul.f16x2 %r183,%r172,%r185;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r364,%r162,%r183;
}
	// end inline asm
	ld.global.nc.u32 	%r191, [%rd68+2304];
	// begin inline asm
	{mul.f16x2 %r189,%r172,%r191;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r363,%r168,%r189;
}
	// end inline asm
	add.s64 	%rd84, %rd84, 3072;
	add.s64 	%rd83, %rd83, 3072;
	add.s32 	%r357, %r357, 768;
	setp.lt.s32 	%p6, %r357, %r40;
	@%p6 bra 	$L__BB107_8;

$L__BB107_9:
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r366;
  cvt.f32.f16 %f6, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r366;
  cvt.f32.f16 %f7, high;}

	// end inline asm
	add.f32 	%f14, %f6, %f7;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r365;
  cvt.f32.f16 %f8, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r365;
  cvt.f32.f16 %f9, high;}

	// end inline asm
	add.f32 	%f1, %f8, %f9;
	st.local.f32 	[%rd3+4], %f1;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r364;
  cvt.f32.f16 %f10, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r364;
  cvt.f32.f16 %f11, high;}

	// end inline asm
	add.f32 	%f2, %f10, %f11;
	st.local.f32 	[%rd3+8], %f2;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r363;
  cvt.f32.f16 %f12, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r363;
  cvt.f32.f16 %f13, high;}

	// end inline asm
	add.f32 	%f3, %f12, %f13;
	st.local.f32 	[%rd3+12], %f3;
	shr.s32 	%r203, %r3, 31;
	shr.u32 	%r204, %r203, 27;
	add.s32 	%r205, %r3, %r204;
	shr.s32 	%r206, %r205, 5;
	shl.b32 	%r207, %r206, 2;
	add.s32 	%r39, %r53, %r207;
	mov.u32 	%r209, 2;
	mov.b32 	%r210, %f14;
	mov.u32 	%r211, 31;
	mov.u32 	%r212, 16;
	mov.u32 	%r213, -1;
	shfl.sync.bfly.b32 	%r214|%p7, %r210, %r212, %r211, %r213;
	mov.b32 	%f15, %r214;
	add.f32 	%f16, %f14, %f15;
	mov.b32 	%r215, %f16;
	mov.u32 	%r216, 8;
	shfl.sync.bfly.b32 	%r217|%p8, %r215, %r216, %r211, %r213;
	mov.b32 	%f17, %r217;
	add.f32 	%f18, %f16, %f17;
	mov.b32 	%r218, %f18;
	mov.u32 	%r219, 4;
	shfl.sync.bfly.b32 	%r220|%p9, %r218, %r219, %r211, %r213;
	mov.b32 	%f19, %r220;
	add.f32 	%f20, %f18, %f19;
	mov.b32 	%r221, %f20;
	shfl.sync.bfly.b32 	%r222|%p10, %r221, %r209, %r211, %r213;
	mov.b32 	%f21, %r222;
	add.f32 	%f22, %f20, %f21;
	mov.b32 	%r223, %f22;
	mov.u32 	%r224, 1;
	shfl.sync.bfly.b32 	%r225|%p11, %r223, %r224, %r211, %r213;
	mov.b32 	%f23, %r225;
	add.f32 	%f24, %f22, %f23;
	st.local.f32 	[%rd3], %f24;
	st.shared.f32 	[%r39], %f24;
	bar.sync 	0;
	@%p1 bra 	$L__BB107_11;

	ld.shared.f32 	%f25, [%r4];
	mov.b32 	%r226, %f25;
	shfl.sync.bfly.b32 	%r230|%p13, %r226, %r212, %r211, %r213;
	mov.b32 	%f26, %r230;
	add.f32 	%f27, %f25, %f26;
	mov.b32 	%r231, %f27;
	shfl.sync.bfly.b32 	%r233|%p14, %r231, %r216, %r211, %r213;
	mov.b32 	%f28, %r233;
	add.f32 	%f29, %f27, %f28;
	mov.b32 	%r234, %f29;
	shfl.sync.bfly.b32 	%r236|%p15, %r234, %r219, %r211, %r213;
	mov.b32 	%f30, %r236;
	add.f32 	%f31, %f29, %f30;
	mov.b32 	%r237, %f31;
	shfl.sync.bfly.b32 	%r239|%p16, %r237, %r209, %r211, %r213;
	mov.b32 	%f32, %r239;
	add.f32 	%f33, %f31, %f32;
	mov.b32 	%r240, %f33;
	shfl.sync.bfly.b32 	%r242|%p17, %r240, %r224, %r211, %r213;
	mov.b32 	%f34, %r242;
	add.f32 	%f35, %f33, %f34;
	st.local.f32 	[%rd3], %f35;

$L__BB107_11:
	bar.sync 	0;
	mov.b32 	%r243, %f1;
	shfl.sync.bfly.b32 	%r247|%p19, %r243, %r212, %r211, %r213;
	mov.b32 	%f36, %r247;
	add.f32 	%f37, %f1, %f36;
	mov.b32 	%r248, %f37;
	shfl.sync.bfly.b32 	%r250|%p20, %r248, %r216, %r211, %r213;
	mov.b32 	%f38, %r250;
	add.f32 	%f39, %f37, %f38;
	mov.b32 	%r251, %f39;
	shfl.sync.bfly.b32 	%r253|%p21, %r251, %r219, %r211, %r213;
	mov.b32 	%f40, %r253;
	add.f32 	%f41, %f39, %f40;
	mov.b32 	%r254, %f41;
	shfl.sync.bfly.b32 	%r256|%p22, %r254, %r209, %r211, %r213;
	mov.b32 	%f42, %r256;
	add.f32 	%f43, %f41, %f42;
	mov.b32 	%r257, %f43;
	shfl.sync.bfly.b32 	%r259|%p23, %r257, %r224, %r211, %r213;
	mov.b32 	%f44, %r259;
	add.f32 	%f45, %f43, %f44;
	st.local.f32 	[%rd3+4], %f45;
	st.shared.f32 	[%r39], %f45;
	bar.sync 	0;
	@%p1 bra 	$L__BB107_13;

	ld.shared.f32 	%f46, [%r4];
	mov.b32 	%r260, %f46;
	mov.u32 	%r261, 31;
	mov.u32 	%r262, 16;
	mov.u32 	%r263, -1;
	shfl.sync.bfly.b32 	%r264|%p24, %r260, %r262, %r261, %r263;
	mov.b32 	%f47, %r264;
	add.f32 	%f48, %f46, %f47;
	mov.b32 	%r265, %f48;
	mov.u32 	%r266, 8;
	shfl.sync.bfly.b32 	%r267|%p25, %r265, %r266, %r261, %r263;
	mov.b32 	%f49, %r267;
	add.f32 	%f50, %f48, %f49;
	mov.b32 	%r268, %f50;
	mov.u32 	%r269, 4;
	shfl.sync.bfly.b32 	%r270|%p26, %r268, %r269, %r261, %r263;
	mov.b32 	%f51, %r270;
	add.f32 	%f52, %f50, %f51;
	mov.b32 	%r271, %f52;
	mov.u32 	%r272, 2;
	shfl.sync.bfly.b32 	%r273|%p27, %r271, %r272, %r261, %r263;
	mov.b32 	%f53, %r273;
	add.f32 	%f54, %f52, %f53;
	mov.b32 	%r274, %f54;
	mov.u32 	%r275, 1;
	shfl.sync.bfly.b32 	%r276|%p28, %r274, %r275, %r261, %r263;
	mov.b32 	%f55, %r276;
	add.f32 	%f56, %f54, %f55;
	st.local.f32 	[%rd3+4], %f56;

$L__BB107_13:
	bar.sync 	0;
	mov.b32 	%r277, %f2;
	mov.u32 	%r278, 31;
	mov.u32 	%r279, 16;
	mov.u32 	%r280, -1;
	shfl.sync.bfly.b32 	%r281|%p30, %r277, %r279, %r278, %r280;
	mov.b32 	%f57, %r281;
	add.f32 	%f58, %f2, %f57;
	mov.b32 	%r282, %f58;
	mov.u32 	%r283, 8;
	shfl.sync.bfly.b32 	%r284|%p31, %r282, %r283, %r278, %r280;
	mov.b32 	%f59, %r284;
	add.f32 	%f60, %f58, %f59;
	mov.b32 	%r285, %f60;
	mov.u32 	%r286, 4;
	shfl.sync.bfly.b32 	%r287|%p32, %r285, %r286, %r278, %r280;
	mov.b32 	%f61, %r287;
	add.f32 	%f62, %f60, %f61;
	mov.b32 	%r288, %f62;
	mov.u32 	%r289, 2;
	shfl.sync.bfly.b32 	%r290|%p33, %r288, %r289, %r278, %r280;
	mov.b32 	%f63, %r290;
	add.f32 	%f64, %f62, %f63;
	mov.b32 	%r291, %f64;
	mov.u32 	%r292, 1;
	shfl.sync.bfly.b32 	%r293|%p34, %r291, %r292, %r278, %r280;
	mov.b32 	%f65, %r293;
	add.f32 	%f66, %f64, %f65;
	st.local.f32 	[%rd3+8], %f66;
	st.shared.f32 	[%r39], %f66;
	bar.sync 	0;
	@%p1 bra 	$L__BB107_15;

	ld.shared.f32 	%f67, [%r4];
	mov.b32 	%r294, %f67;
	shfl.sync.bfly.b32 	%r298|%p35, %r294, %r279, %r278, %r280;
	mov.b32 	%f68, %r298;
	add.f32 	%f69, %f67, %f68;
	mov.b32 	%r299, %f69;
	shfl.sync.bfly.b32 	%r301|%p36, %r299, %r283, %r278, %r280;
	mov.b32 	%f70, %r301;
	add.f32 	%f71, %f69, %f70;
	mov.b32 	%r302, %f71;
	shfl.sync.bfly.b32 	%r304|%p37, %r302, %r286, %r278, %r280;
	mov.b32 	%f72, %r304;
	add.f32 	%f73, %f71, %f72;
	mov.b32 	%r305, %f73;
	shfl.sync.bfly.b32 	%r307|%p38, %r305, %r289, %r278, %r280;
	mov.b32 	%f74, %r307;
	add.f32 	%f75, %f73, %f74;
	mov.b32 	%r308, %f75;
	shfl.sync.bfly.b32 	%r310|%p39, %r308, %r292, %r278, %r280;
	mov.b32 	%f76, %r310;
	add.f32 	%f77, %f75, %f76;
	st.local.f32 	[%rd3+8], %f77;

$L__BB107_15:
	bar.sync 	0;
	mov.b32 	%r311, %f3;
	shfl.sync.bfly.b32 	%r315|%p41, %r311, %r279, %r278, %r280;
	mov.b32 	%f78, %r315;
	add.f32 	%f79, %f3, %f78;
	mov.b32 	%r316, %f79;
	shfl.sync.bfly.b32 	%r318|%p42, %r316, %r283, %r278, %r280;
	mov.b32 	%f80, %r318;
	add.f32 	%f81, %f79, %f80;
	mov.b32 	%r319, %f81;
	shfl.sync.bfly.b32 	%r321|%p43, %r319, %r286, %r278, %r280;
	mov.b32 	%f82, %r321;
	add.f32 	%f83, %f81, %f82;
	mov.b32 	%r322, %f83;
	shfl.sync.bfly.b32 	%r324|%p44, %r322, %r289, %r278, %r280;
	mov.b32 	%f84, %r324;
	add.f32 	%f85, %f83, %f84;
	mov.b32 	%r325, %f85;
	shfl.sync.bfly.b32 	%r327|%p45, %r325, %r292, %r278, %r280;
	mov.b32 	%f86, %r327;
	add.f32 	%f87, %f85, %f86;
	st.local.f32 	[%rd3+12], %f87;
	st.shared.f32 	[%r39], %f87;
	bar.sync 	0;
	@%p1 bra 	$L__BB107_17;

	ld.shared.f32 	%f88, [%r4];
	mov.b32 	%r328, %f88;
	mov.u32 	%r329, 31;
	mov.u32 	%r330, 16;
	mov.u32 	%r331, -1;
	shfl.sync.bfly.b32 	%r332|%p46, %r328, %r330, %r329, %r331;
	mov.b32 	%f89, %r332;
	add.f32 	%f90, %f88, %f89;
	mov.b32 	%r333, %f90;
	mov.u32 	%r334, 8;
	shfl.sync.bfly.b32 	%r335|%p47, %r333, %r334, %r329, %r331;
	mov.b32 	%f91, %r335;
	add.f32 	%f92, %f90, %f91;
	mov.b32 	%r336, %f92;
	mov.u32 	%r337, 4;
	shfl.sync.bfly.b32 	%r338|%p48, %r336, %r337, %r329, %r331;
	mov.b32 	%f93, %r338;
	add.f32 	%f94, %f92, %f93;
	mov.b32 	%r339, %f94;
	mov.u32 	%r340, 2;
	shfl.sync.bfly.b32 	%r341|%p49, %r339, %r340, %r329, %r331;
	mov.b32 	%f95, %r341;
	add.f32 	%f96, %f94, %f95;
	mov.b32 	%r342, %f96;
	mov.u32 	%r343, 1;
	shfl.sync.bfly.b32 	%r344|%p50, %r342, %r343, %r329, %r331;
	mov.b32 	%f97, %r344;
	add.f32 	%f98, %f96, %f97;
	st.local.f32 	[%rd3+12], %f98;

$L__BB107_17:
	bar.sync 	0;
	setp.gt.s32 	%p51, %r3, 3;
	@%p51 bra 	$L__BB107_19;

	mad.lo.s32 	%r345, %r3, %r42, %r2;
	cvt.s64.s32 	%rd70, %r345;
	mul.lo.s32 	%r346, %r1, %r43;
	cvt.s64.s32 	%rd71, %r346;
	add.s64 	%rd72, %rd71, %rd70;
	mul.wide.s32 	%rd73, %r3, 4;
	add.s64 	%rd74, %rd3, %rd73;
	ld.local.f32 	%f99, [%rd74];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs3, %f99;}

	// end inline asm
	cvta.to.global.u64 	%rd75, %rd33;
	shl.b64 	%rd76, %rd72, 1;
	add.s64 	%rd77, %rd75, %rd76;
	st.global.u16 	[%rd77], %rs3;

$L__BB107_19:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_5_bs_192
.visible .entry ggml_matvec_f16_ncols_5_bs_192(
	.param .u64 ggml_matvec_f16_ncols_5_bs_192_param_0,
	.param .u64 ggml_matvec_f16_ncols_5_bs_192_param_1,
	.param .u64 ggml_matvec_f16_ncols_5_bs_192_param_2,
	.param .u32 ggml_matvec_f16_ncols_5_bs_192_param_3,
	.param .u32 ggml_matvec_f16_ncols_5_bs_192_param_4,
	.param .u32 ggml_matvec_f16_ncols_5_bs_192_param_5,
	.param .u32 ggml_matvec_f16_ncols_5_bs_192_param_6,
	.param .u32 ggml_matvec_f16_ncols_5_bs_192_param_7,
	.param .u32 ggml_matvec_f16_ncols_5_bs_192_param_8,
	.param .u32 ggml_matvec_f16_ncols_5_bs_192_param_9,
	.param .u32 ggml_matvec_f16_ncols_5_bs_192_param_10,
	.param .u32 ggml_matvec_f16_ncols_5_bs_192_param_11
)
{
	.local .align 4 .b8 	__local_depot108[20];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<63>;
	.reg .b16 	%rs<4>;
	.reg .f32 	%f<124>;
	.reg .b32 	%r<447>;
	.reg .b64 	%rd<76>;


	mov.u64 	%SPL, __local_depot108;
	ld.param.u64 	%rd28, [ggml_matvec_f16_ncols_5_bs_192_param_0];
	ld.param.u64 	%rd29, [ggml_matvec_f16_ncols_5_bs_192_param_1];
	ld.param.u64 	%rd27, [ggml_matvec_f16_ncols_5_bs_192_param_2];
	ld.param.u32 	%r46, [ggml_matvec_f16_ncols_5_bs_192_param_3];
	ld.param.u32 	%r50, [ggml_matvec_f16_ncols_5_bs_192_param_5];
	ld.param.u32 	%r47, [ggml_matvec_f16_ncols_5_bs_192_param_6];
	ld.param.u32 	%r48, [ggml_matvec_f16_ncols_5_bs_192_param_7];
	ld.param.u32 	%r51, [ggml_matvec_f16_ncols_5_bs_192_param_8];
	ld.param.u32 	%r52, [ggml_matvec_f16_ncols_5_bs_192_param_9];
	ld.param.u32 	%r53, [ggml_matvec_f16_ncols_5_bs_192_param_10];
	ld.param.u32 	%r49, [ggml_matvec_f16_ncols_5_bs_192_param_11];
	cvta.to.global.u64 	%rd75, %rd29;
	cvta.to.global.u64 	%rd2, %rd28;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r54, %r1, %r51;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r55, %r2, %r50;
	mad.lo.s32 	%r56, %r54, %r52, %r55;
	cvt.s64.s32 	%rd4, %r56;
	mul.lo.s32 	%r57, %r1, %r53;
	cvt.s64.s32 	%rd5, %r57;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r58, %r3, 2;
	mov.u32 	%r59, data_mmv;
	add.s32 	%r4, %r59, %r58;
	@%p1 bra 	$L__BB108_2;

	mov.u32 	%r60, 0;
	st.shared.u32 	[%r4], %r60;

$L__BB108_2:
	bar.sync 	0;
	mov.f32 	%f6, 0f00000000;
	mov.u32 	%r442, 0;
	st.local.u32 	[%rd3], %r442;
	st.local.u32 	[%rd3+4], %r442;
	st.local.u32 	[%rd3+8], %r442;
	st.local.u32 	[%rd3+12], %r442;
	st.local.u32 	[%rd3+16], %r442;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs2, %f6;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f6;}

	// end inline asm
	mov.b32 	%r446, {%rs1, %rs2};
	setp.ge.s32 	%p2, %r3, %r46;
	mov.u32 	%r443, %r442;
	mov.u32 	%r444, %r442;
	mov.u32 	%r445, %r442;
	@%p2 bra 	$L__BB108_9;

	not.b32 	%r69, %r3;
	add.s32 	%r6, %r69, %r46;
	mul.wide.u32 	%rd31, %r6, -1431655765;
	shr.u64 	%rd32, %rd31, 39;
	cvt.u32.u64 	%r70, %rd32;
	add.s32 	%r71, %r70, 1;
	and.b32  	%r429, %r71, 3;
	setp.eq.s32 	%p3, %r429, 0;
	mov.u32 	%r442, 0;
	mov.u32 	%r435, %r3;
	@%p3 bra 	$L__BB108_6;

	shl.b32 	%r76, %r47, 1;
	mad.lo.s32 	%r77, %r47, 3, %r3;
	mul.wide.s32 	%rd33, %r77, 4;
	shl.b64 	%rd34, %rd5, 1;
	add.s64 	%rd7, %rd33, %rd34;
	mul.wide.s32 	%rd35, %r3, 4;
	mul.wide.s32 	%rd36, %r47, 4;
	add.s64 	%rd37, %rd35, %rd36;
	add.s64 	%rd8, %rd37, %rd34;
	add.s64 	%rd9, %rd35, %rd34;
	mul.wide.s32 	%rd38, %r3, 2;
	add.s64 	%rd39, %rd38, %rd4;
	shl.b64 	%rd40, %rd39, 1;
	add.s64 	%rd72, %rd2, %rd40;
	mul.wide.s32 	%rd11, %r76, 4;
	mov.u32 	%r442, 0;
	mov.u64 	%rd73, %rd75;
	mov.u32 	%r443, %r442;
	mov.u32 	%r444, %r442;
	mov.u32 	%r445, %r442;
	mov.u32 	%r435, %r3;

$L__BB108_5:
	.pragma "nounroll";
	ld.global.nc.u32 	%r79, [%rd72];
	add.s64 	%rd41, %rd73, %rd9;
	ld.global.nc.u32 	%r80, [%rd41];
	// begin inline asm
	{mul.f16x2 %r78,%r79,%r80;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r446,%r446,%r78;
}
	// end inline asm
	add.s64 	%rd42, %rd73, %rd8;
	ld.global.nc.u32 	%r86, [%rd42];
	// begin inline asm
	{mul.f16x2 %r84,%r79,%r86;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r445,%r445,%r84;
}
	// end inline asm
	add.s64 	%rd43, %rd41, %rd11;
	ld.global.nc.u32 	%r92, [%rd43];
	// begin inline asm
	{mul.f16x2 %r90,%r79,%r92;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r444,%r444,%r90;
}
	// end inline asm
	add.s64 	%rd44, %rd73, %rd7;
	ld.global.nc.u32 	%r98, [%rd44];
	// begin inline asm
	{mul.f16x2 %r96,%r79,%r98;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r443,%r443,%r96;
}
	// end inline asm
	add.s64 	%rd45, %rd43, %rd11;
	ld.global.nc.u32 	%r104, [%rd45];
	// begin inline asm
	{mul.f16x2 %r102,%r79,%r104;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r442,%r442,%r102;
}
	// end inline asm
	add.s32 	%r435, %r435, 192;
	add.s64 	%rd73, %rd73, 768;
	add.s64 	%rd72, %rd72, 768;
	add.s32 	%r429, %r429, -1;
	setp.ne.s32 	%p4, %r429, 0;
	@%p4 bra 	$L__BB108_5;

$L__BB108_6:
	setp.lt.u32 	%p5, %r6, 576;
	@%p5 bra 	$L__BB108_9;

	add.s32 	%r108, %r435, %r47;
	shl.b32 	%r109, %r47, 1;
	add.s32 	%r110, %r435, %r109;
	mad.lo.s32 	%r111, %r47, 3, %r435;
	shl.b32 	%r112, %r47, 2;
	add.s32 	%r113, %r435, %r112;
	add.s32 	%r114, %r108, 192;
	mul.wide.s32 	%rd46, %r114, 4;
	shl.b64 	%rd47, %rd5, 1;
	add.s64 	%rd16, %rd46, %rd47;
	mul.wide.s32 	%rd48, %r110, 4;
	add.s64 	%rd17, %rd48, %rd47;
	mul.wide.s32 	%rd49, %r111, 4;
	add.s64 	%rd18, %rd49, %rd47;
	mul.wide.s32 	%rd50, %r113, 4;
	add.s64 	%rd19, %rd50, %rd47;
	mul.wide.s32 	%rd51, %r435, 2;
	add.s64 	%rd52, %rd51, %rd4;
	shl.b64 	%rd53, %rd52, 1;
	add.s64 	%rd54, %rd2, %rd53;
	add.s64 	%rd74, %rd54, 1536;
	mul.wide.s32 	%rd55, %r435, 4;
	add.s64 	%rd21, %rd55, %rd47;
	mul.wide.s32 	%rd56, %r47, 4;
	add.s64 	%rd57, %rd55, %rd56;
	add.s64 	%rd22, %rd57, %rd47;

$L__BB108_8:
	ld.global.nc.u32 	%r116, [%rd74+-1536];
	add.s64 	%rd58, %rd75, %rd21;
	ld.global.nc.u32 	%r117, [%rd58];
	// begin inline asm
	{mul.f16x2 %r115,%r116,%r117;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r118,%r446,%r115;
}
	// end inline asm
	add.s64 	%rd59, %rd75, %rd22;
	ld.global.nc.u32 	%r123, [%rd59];
	// begin inline asm
	{mul.f16x2 %r121,%r116,%r123;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r124,%r445,%r121;
}
	// end inline asm
	add.s64 	%rd60, %rd75, %rd17;
	ld.global.nc.u32 	%r129, [%rd60];
	// begin inline asm
	{mul.f16x2 %r127,%r116,%r129;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r130,%r444,%r127;
}
	// end inline asm
	add.s64 	%rd61, %rd75, %rd18;
	ld.global.nc.u32 	%r135, [%rd61];
	// begin inline asm
	{mul.f16x2 %r133,%r116,%r135;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r136,%r443,%r133;
}
	// end inline asm
	add.s64 	%rd62, %rd75, %rd19;
	ld.global.nc.u32 	%r141, [%rd62];
	// begin inline asm
	{mul.f16x2 %r139,%r116,%r141;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r142,%r442,%r139;
}
	// end inline asm
	ld.global.nc.u32 	%r146, [%rd74+-768];
	ld.global.nc.u32 	%r147, [%rd58+768];
	// begin inline asm
	{mul.f16x2 %r145,%r146,%r147;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r148,%r118,%r145;
}
	// end inline asm
	add.s64 	%rd63, %rd75, %rd16;
	ld.global.nc.u32 	%r153, [%rd63];
	// begin inline asm
	{mul.f16x2 %r151,%r146,%r153;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r154,%r124,%r151;
}
	// end inline asm
	ld.global.nc.u32 	%r159, [%rd60+768];
	// begin inline asm
	{mul.f16x2 %r157,%r146,%r159;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r160,%r130,%r157;
}
	// end inline asm
	ld.global.nc.u32 	%r165, [%rd61+768];
	// begin inline asm
	{mul.f16x2 %r163,%r146,%r165;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r166,%r136,%r163;
}
	// end inline asm
	ld.global.nc.u32 	%r171, [%rd62+768];
	// begin inline asm
	{mul.f16x2 %r169,%r146,%r171;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r172,%r142,%r169;
}
	// end inline asm
	ld.global.nc.u32 	%r176, [%rd74];
	ld.global.nc.u32 	%r177, [%rd58+1536];
	// begin inline asm
	{mul.f16x2 %r175,%r176,%r177;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r178,%r148,%r175;
}
	// end inline asm
	ld.global.nc.u32 	%r183, [%rd63+768];
	// begin inline asm
	{mul.f16x2 %r181,%r176,%r183;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r184,%r154,%r181;
}
	// end inline asm
	ld.global.nc.u32 	%r189, [%rd60+1536];
	// begin inline asm
	{mul.f16x2 %r187,%r176,%r189;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r190,%r160,%r187;
}
	// end inline asm
	ld.global.nc.u32 	%r195, [%rd61+1536];
	// begin inline asm
	{mul.f16x2 %r193,%r176,%r195;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r196,%r166,%r193;
}
	// end inline asm
	ld.global.nc.u32 	%r201, [%rd62+1536];
	// begin inline asm
	{mul.f16x2 %r199,%r176,%r201;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r202,%r172,%r199;
}
	// end inline asm
	ld.global.nc.u32 	%r206, [%rd74+768];
	ld.global.nc.u32 	%r207, [%rd58+2304];
	// begin inline asm
	{mul.f16x2 %r205,%r206,%r207;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r446,%r178,%r205;
}
	// end inline asm
	ld.global.nc.u32 	%r213, [%rd63+1536];
	// begin inline asm
	{mul.f16x2 %r211,%r206,%r213;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r445,%r184,%r211;
}
	// end inline asm
	ld.global.nc.u32 	%r219, [%rd60+2304];
	// begin inline asm
	{mul.f16x2 %r217,%r206,%r219;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r444,%r190,%r217;
}
	// end inline asm
	ld.global.nc.u32 	%r225, [%rd61+2304];
	// begin inline asm
	{mul.f16x2 %r223,%r206,%r225;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r443,%r196,%r223;
}
	// end inline asm
	ld.global.nc.u32 	%r231, [%rd62+2304];
	// begin inline asm
	{mul.f16x2 %r229,%r206,%r231;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r442,%r202,%r229;
}
	// end inline asm
	add.s64 	%rd75, %rd75, 3072;
	add.s64 	%rd74, %rd74, 3072;
	add.s32 	%r435, %r435, 768;
	setp.lt.s32 	%p6, %r435, %r46;
	@%p6 bra 	$L__BB108_8;

$L__BB108_9:
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r446;
  cvt.f32.f16 %f7, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r446;
  cvt.f32.f16 %f8, high;}

	// end inline asm
	add.f32 	%f17, %f7, %f8;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r445;
  cvt.f32.f16 %f9, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r445;
  cvt.f32.f16 %f10, high;}

	// end inline asm
	add.f32 	%f1, %f9, %f10;
	st.local.f32 	[%rd3+4], %f1;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r444;
  cvt.f32.f16 %f11, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r444;
  cvt.f32.f16 %f12, high;}

	// end inline asm
	add.f32 	%f2, %f11, %f12;
	st.local.f32 	[%rd3+8], %f2;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r443;
  cvt.f32.f16 %f13, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r443;
  cvt.f32.f16 %f14, high;}

	// end inline asm
	add.f32 	%f3, %f13, %f14;
	st.local.f32 	[%rd3+12], %f3;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r442;
  cvt.f32.f16 %f15, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r442;
  cvt.f32.f16 %f16, high;}

	// end inline asm
	add.f32 	%f4, %f15, %f16;
	st.local.f32 	[%rd3+16], %f4;
	shr.s32 	%r245, %r3, 31;
	shr.u32 	%r246, %r245, 27;
	add.s32 	%r247, %r3, %r246;
	shr.s32 	%r248, %r247, 5;
	shl.b32 	%r249, %r248, 2;
	add.s32 	%r45, %r59, %r249;
	mov.u32 	%r251, 2;
	mov.b32 	%r252, %f17;
	mov.u32 	%r253, 31;
	mov.u32 	%r254, 16;
	mov.u32 	%r255, -1;
	shfl.sync.bfly.b32 	%r256|%p7, %r252, %r254, %r253, %r255;
	mov.b32 	%f18, %r256;
	add.f32 	%f19, %f17, %f18;
	mov.b32 	%r257, %f19;
	mov.u32 	%r258, 8;
	shfl.sync.bfly.b32 	%r259|%p8, %r257, %r258, %r253, %r255;
	mov.b32 	%f20, %r259;
	add.f32 	%f21, %f19, %f20;
	mov.b32 	%r260, %f21;
	mov.u32 	%r261, 4;
	shfl.sync.bfly.b32 	%r262|%p9, %r260, %r261, %r253, %r255;
	mov.b32 	%f22, %r262;
	add.f32 	%f23, %f21, %f22;
	mov.b32 	%r263, %f23;
	shfl.sync.bfly.b32 	%r264|%p10, %r263, %r251, %r253, %r255;
	mov.b32 	%f24, %r264;
	add.f32 	%f25, %f23, %f24;
	mov.b32 	%r265, %f25;
	mov.u32 	%r266, 1;
	shfl.sync.bfly.b32 	%r267|%p11, %r265, %r266, %r253, %r255;
	mov.b32 	%f26, %r267;
	add.f32 	%f27, %f25, %f26;
	st.local.f32 	[%rd3], %f27;
	st.shared.f32 	[%r45], %f27;
	bar.sync 	0;
	@%p1 bra 	$L__BB108_11;

	ld.shared.f32 	%f28, [%r4];
	mov.b32 	%r268, %f28;
	shfl.sync.bfly.b32 	%r272|%p13, %r268, %r254, %r253, %r255;
	mov.b32 	%f29, %r272;
	add.f32 	%f30, %f28, %f29;
	mov.b32 	%r273, %f30;
	shfl.sync.bfly.b32 	%r275|%p14, %r273, %r258, %r253, %r255;
	mov.b32 	%f31, %r275;
	add.f32 	%f32, %f30, %f31;
	mov.b32 	%r276, %f32;
	shfl.sync.bfly.b32 	%r278|%p15, %r276, %r261, %r253, %r255;
	mov.b32 	%f33, %r278;
	add.f32 	%f34, %f32, %f33;
	mov.b32 	%r279, %f34;
	shfl.sync.bfly.b32 	%r281|%p16, %r279, %r251, %r253, %r255;
	mov.b32 	%f35, %r281;
	add.f32 	%f36, %f34, %f35;
	mov.b32 	%r282, %f36;
	shfl.sync.bfly.b32 	%r284|%p17, %r282, %r266, %r253, %r255;
	mov.b32 	%f37, %r284;
	add.f32 	%f38, %f36, %f37;
	st.local.f32 	[%rd3], %f38;

$L__BB108_11:
	bar.sync 	0;
	mov.b32 	%r285, %f1;
	shfl.sync.bfly.b32 	%r289|%p19, %r285, %r254, %r253, %r255;
	mov.b32 	%f39, %r289;
	add.f32 	%f40, %f1, %f39;
	mov.b32 	%r290, %f40;
	shfl.sync.bfly.b32 	%r292|%p20, %r290, %r258, %r253, %r255;
	mov.b32 	%f41, %r292;
	add.f32 	%f42, %f40, %f41;
	mov.b32 	%r293, %f42;
	shfl.sync.bfly.b32 	%r295|%p21, %r293, %r261, %r253, %r255;
	mov.b32 	%f43, %r295;
	add.f32 	%f44, %f42, %f43;
	mov.b32 	%r296, %f44;
	shfl.sync.bfly.b32 	%r298|%p22, %r296, %r251, %r253, %r255;
	mov.b32 	%f45, %r298;
	add.f32 	%f46, %f44, %f45;
	mov.b32 	%r299, %f46;
	shfl.sync.bfly.b32 	%r301|%p23, %r299, %r266, %r253, %r255;
	mov.b32 	%f47, %r301;
	add.f32 	%f48, %f46, %f47;
	st.local.f32 	[%rd3+4], %f48;
	st.shared.f32 	[%r45], %f48;
	bar.sync 	0;
	@%p1 bra 	$L__BB108_13;

	ld.shared.f32 	%f49, [%r4];
	mov.b32 	%r302, %f49;
	mov.u32 	%r303, 31;
	mov.u32 	%r304, 16;
	mov.u32 	%r305, -1;
	shfl.sync.bfly.b32 	%r306|%p24, %r302, %r304, %r303, %r305;
	mov.b32 	%f50, %r306;
	add.f32 	%f51, %f49, %f50;
	mov.b32 	%r307, %f51;
	mov.u32 	%r308, 8;
	shfl.sync.bfly.b32 	%r309|%p25, %r307, %r308, %r303, %r305;
	mov.b32 	%f52, %r309;
	add.f32 	%f53, %f51, %f52;
	mov.b32 	%r310, %f53;
	mov.u32 	%r311, 4;
	shfl.sync.bfly.b32 	%r312|%p26, %r310, %r311, %r303, %r305;
	mov.b32 	%f54, %r312;
	add.f32 	%f55, %f53, %f54;
	mov.b32 	%r313, %f55;
	mov.u32 	%r314, 2;
	shfl.sync.bfly.b32 	%r315|%p27, %r313, %r314, %r303, %r305;
	mov.b32 	%f56, %r315;
	add.f32 	%f57, %f55, %f56;
	mov.b32 	%r316, %f57;
	mov.u32 	%r317, 1;
	shfl.sync.bfly.b32 	%r318|%p28, %r316, %r317, %r303, %r305;
	mov.b32 	%f58, %r318;
	add.f32 	%f59, %f57, %f58;
	st.local.f32 	[%rd3+4], %f59;

$L__BB108_13:
	bar.sync 	0;
	mov.b32 	%r319, %f2;
	mov.u32 	%r320, 31;
	mov.u32 	%r321, 16;
	mov.u32 	%r322, -1;
	shfl.sync.bfly.b32 	%r323|%p30, %r319, %r321, %r320, %r322;
	mov.b32 	%f60, %r323;
	add.f32 	%f61, %f2, %f60;
	mov.b32 	%r324, %f61;
	mov.u32 	%r325, 8;
	shfl.sync.bfly.b32 	%r326|%p31, %r324, %r325, %r320, %r322;
	mov.b32 	%f62, %r326;
	add.f32 	%f63, %f61, %f62;
	mov.b32 	%r327, %f63;
	mov.u32 	%r328, 4;
	shfl.sync.bfly.b32 	%r329|%p32, %r327, %r328, %r320, %r322;
	mov.b32 	%f64, %r329;
	add.f32 	%f65, %f63, %f64;
	mov.b32 	%r330, %f65;
	mov.u32 	%r331, 2;
	shfl.sync.bfly.b32 	%r332|%p33, %r330, %r331, %r320, %r322;
	mov.b32 	%f66, %r332;
	add.f32 	%f67, %f65, %f66;
	mov.b32 	%r333, %f67;
	mov.u32 	%r334, 1;
	shfl.sync.bfly.b32 	%r335|%p34, %r333, %r334, %r320, %r322;
	mov.b32 	%f68, %r335;
	add.f32 	%f69, %f67, %f68;
	st.local.f32 	[%rd3+8], %f69;
	st.shared.f32 	[%r45], %f69;
	bar.sync 	0;
	@%p1 bra 	$L__BB108_15;

	ld.shared.f32 	%f70, [%r4];
	mov.b32 	%r336, %f70;
	shfl.sync.bfly.b32 	%r340|%p35, %r336, %r321, %r320, %r322;
	mov.b32 	%f71, %r340;
	add.f32 	%f72, %f70, %f71;
	mov.b32 	%r341, %f72;
	shfl.sync.bfly.b32 	%r343|%p36, %r341, %r325, %r320, %r322;
	mov.b32 	%f73, %r343;
	add.f32 	%f74, %f72, %f73;
	mov.b32 	%r344, %f74;
	shfl.sync.bfly.b32 	%r346|%p37, %r344, %r328, %r320, %r322;
	mov.b32 	%f75, %r346;
	add.f32 	%f76, %f74, %f75;
	mov.b32 	%r347, %f76;
	shfl.sync.bfly.b32 	%r349|%p38, %r347, %r331, %r320, %r322;
	mov.b32 	%f77, %r349;
	add.f32 	%f78, %f76, %f77;
	mov.b32 	%r350, %f78;
	shfl.sync.bfly.b32 	%r352|%p39, %r350, %r334, %r320, %r322;
	mov.b32 	%f79, %r352;
	add.f32 	%f80, %f78, %f79;
	st.local.f32 	[%rd3+8], %f80;

$L__BB108_15:
	bar.sync 	0;
	mov.b32 	%r353, %f3;
	shfl.sync.bfly.b32 	%r357|%p41, %r353, %r321, %r320, %r322;
	mov.b32 	%f81, %r357;
	add.f32 	%f82, %f3, %f81;
	mov.b32 	%r358, %f82;
	shfl.sync.bfly.b32 	%r360|%p42, %r358, %r325, %r320, %r322;
	mov.b32 	%f83, %r360;
	add.f32 	%f84, %f82, %f83;
	mov.b32 	%r361, %f84;
	shfl.sync.bfly.b32 	%r363|%p43, %r361, %r328, %r320, %r322;
	mov.b32 	%f85, %r363;
	add.f32 	%f86, %f84, %f85;
	mov.b32 	%r364, %f86;
	shfl.sync.bfly.b32 	%r366|%p44, %r364, %r331, %r320, %r322;
	mov.b32 	%f87, %r366;
	add.f32 	%f88, %f86, %f87;
	mov.b32 	%r367, %f88;
	shfl.sync.bfly.b32 	%r369|%p45, %r367, %r334, %r320, %r322;
	mov.b32 	%f89, %r369;
	add.f32 	%f90, %f88, %f89;
	st.local.f32 	[%rd3+12], %f90;
	st.shared.f32 	[%r45], %f90;
	bar.sync 	0;
	@%p1 bra 	$L__BB108_17;

	ld.shared.f32 	%f91, [%r4];
	mov.b32 	%r370, %f91;
	mov.u32 	%r371, 31;
	mov.u32 	%r372, 16;
	mov.u32 	%r373, -1;
	shfl.sync.bfly.b32 	%r374|%p46, %r370, %r372, %r371, %r373;
	mov.b32 	%f92, %r374;
	add.f32 	%f93, %f91, %f92;
	mov.b32 	%r375, %f93;
	mov.u32 	%r376, 8;
	shfl.sync.bfly.b32 	%r377|%p47, %r375, %r376, %r371, %r373;
	mov.b32 	%f94, %r377;
	add.f32 	%f95, %f93, %f94;
	mov.b32 	%r378, %f95;
	mov.u32 	%r379, 4;
	shfl.sync.bfly.b32 	%r380|%p48, %r378, %r379, %r371, %r373;
	mov.b32 	%f96, %r380;
	add.f32 	%f97, %f95, %f96;
	mov.b32 	%r381, %f97;
	mov.u32 	%r382, 2;
	shfl.sync.bfly.b32 	%r383|%p49, %r381, %r382, %r371, %r373;
	mov.b32 	%f98, %r383;
	add.f32 	%f99, %f97, %f98;
	mov.b32 	%r384, %f99;
	mov.u32 	%r385, 1;
	shfl.sync.bfly.b32 	%r386|%p50, %r384, %r385, %r371, %r373;
	mov.b32 	%f100, %r386;
	add.f32 	%f101, %f99, %f100;
	st.local.f32 	[%rd3+12], %f101;

$L__BB108_17:
	bar.sync 	0;
	mov.b32 	%r387, %f4;
	mov.u32 	%r388, 31;
	mov.u32 	%r389, 16;
	mov.u32 	%r390, -1;
	shfl.sync.bfly.b32 	%r391|%p52, %r387, %r389, %r388, %r390;
	mov.b32 	%f102, %r391;
	add.f32 	%f103, %f4, %f102;
	mov.b32 	%r392, %f103;
	mov.u32 	%r393, 8;
	shfl.sync.bfly.b32 	%r394|%p53, %r392, %r393, %r388, %r390;
	mov.b32 	%f104, %r394;
	add.f32 	%f105, %f103, %f104;
	mov.b32 	%r395, %f105;
	mov.u32 	%r396, 4;
	shfl.sync.bfly.b32 	%r397|%p54, %r395, %r396, %r388, %r390;
	mov.b32 	%f106, %r397;
	add.f32 	%f107, %f105, %f106;
	mov.b32 	%r398, %f107;
	mov.u32 	%r399, 2;
	shfl.sync.bfly.b32 	%r400|%p55, %r398, %r399, %r388, %r390;
	mov.b32 	%f108, %r400;
	add.f32 	%f109, %f107, %f108;
	mov.b32 	%r401, %f109;
	mov.u32 	%r402, 1;
	shfl.sync.bfly.b32 	%r403|%p56, %r401, %r402, %r388, %r390;
	mov.b32 	%f110, %r403;
	add.f32 	%f111, %f109, %f110;
	st.local.f32 	[%rd3+16], %f111;
	st.shared.f32 	[%r45], %f111;
	bar.sync 	0;
	@%p1 bra 	$L__BB108_19;

	ld.shared.f32 	%f112, [%r4];
	mov.b32 	%r404, %f112;
	shfl.sync.bfly.b32 	%r408|%p57, %r404, %r389, %r388, %r390;
	mov.b32 	%f113, %r408;
	add.f32 	%f114, %f112, %f113;
	mov.b32 	%r409, %f114;
	shfl.sync.bfly.b32 	%r411|%p58, %r409, %r393, %r388, %r390;
	mov.b32 	%f115, %r411;
	add.f32 	%f116, %f114, %f115;
	mov.b32 	%r412, %f116;
	shfl.sync.bfly.b32 	%r414|%p59, %r412, %r396, %r388, %r390;
	mov.b32 	%f117, %r414;
	add.f32 	%f118, %f116, %f117;
	mov.b32 	%r415, %f118;
	shfl.sync.bfly.b32 	%r417|%p60, %r415, %r399, %r388, %r390;
	mov.b32 	%f119, %r417;
	add.f32 	%f120, %f118, %f119;
	mov.b32 	%r418, %f120;
	shfl.sync.bfly.b32 	%r420|%p61, %r418, %r402, %r388, %r390;
	mov.b32 	%f121, %r420;
	add.f32 	%f122, %f120, %f121;
	st.local.f32 	[%rd3+16], %f122;

$L__BB108_19:
	bar.sync 	0;
	setp.gt.s32 	%p62, %r3, 4;
	@%p62 bra 	$L__BB108_21;

	mad.lo.s32 	%r421, %r3, %r48, %r2;
	cvt.s64.s32 	%rd64, %r421;
	mul.lo.s32 	%r422, %r1, %r49;
	cvt.s64.s32 	%rd65, %r422;
	add.s64 	%rd66, %rd65, %rd64;
	mul.wide.s32 	%rd67, %r3, 4;
	add.s64 	%rd68, %rd3, %rd67;
	ld.local.f32 	%f123, [%rd68];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs3, %f123;}

	// end inline asm
	cvta.to.global.u64 	%rd69, %rd27;
	shl.b64 	%rd70, %rd66, 1;
	add.s64 	%rd71, %rd69, %rd70;
	st.global.u16 	[%rd71], %rs3;

$L__BB108_21:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_6_bs_192
.visible .entry ggml_matvec_f16_ncols_6_bs_192(
	.param .u64 ggml_matvec_f16_ncols_6_bs_192_param_0,
	.param .u64 ggml_matvec_f16_ncols_6_bs_192_param_1,
	.param .u64 ggml_matvec_f16_ncols_6_bs_192_param_2,
	.param .u32 ggml_matvec_f16_ncols_6_bs_192_param_3,
	.param .u32 ggml_matvec_f16_ncols_6_bs_192_param_4,
	.param .u32 ggml_matvec_f16_ncols_6_bs_192_param_5,
	.param .u32 ggml_matvec_f16_ncols_6_bs_192_param_6,
	.param .u32 ggml_matvec_f16_ncols_6_bs_192_param_7,
	.param .u32 ggml_matvec_f16_ncols_6_bs_192_param_8,
	.param .u32 ggml_matvec_f16_ncols_6_bs_192_param_9,
	.param .u32 ggml_matvec_f16_ncols_6_bs_192_param_10,
	.param .u32 ggml_matvec_f16_ncols_6_bs_192_param_11
)
{
	.local .align 8 .b8 	__local_depot109[24];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<74>;
	.reg .b16 	%rs<4>;
	.reg .f32 	%f<148>;
	.reg .b32 	%r<527>;
	.reg .b64 	%rd<79>;


	mov.u64 	%SPL, __local_depot109;
	ld.param.u64 	%rd29, [ggml_matvec_f16_ncols_6_bs_192_param_0];
	ld.param.u64 	%rd30, [ggml_matvec_f16_ncols_6_bs_192_param_1];
	ld.param.u64 	%rd28, [ggml_matvec_f16_ncols_6_bs_192_param_2];
	ld.param.u32 	%r52, [ggml_matvec_f16_ncols_6_bs_192_param_3];
	ld.param.u32 	%r56, [ggml_matvec_f16_ncols_6_bs_192_param_5];
	ld.param.u32 	%r53, [ggml_matvec_f16_ncols_6_bs_192_param_6];
	ld.param.u32 	%r54, [ggml_matvec_f16_ncols_6_bs_192_param_7];
	ld.param.u32 	%r57, [ggml_matvec_f16_ncols_6_bs_192_param_8];
	ld.param.u32 	%r58, [ggml_matvec_f16_ncols_6_bs_192_param_9];
	ld.param.u32 	%r59, [ggml_matvec_f16_ncols_6_bs_192_param_10];
	ld.param.u32 	%r55, [ggml_matvec_f16_ncols_6_bs_192_param_11];
	cvta.to.global.u64 	%rd78, %rd30;
	cvta.to.global.u64 	%rd2, %rd29;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r60, %r1, %r57;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r61, %r2, %r56;
	mad.lo.s32 	%r62, %r60, %r58, %r61;
	cvt.s64.s32 	%rd4, %r62;
	mul.lo.s32 	%r63, %r1, %r59;
	cvt.s64.s32 	%rd5, %r63;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r64, %r3, 2;
	mov.u32 	%r65, data_mmv;
	add.s32 	%r4, %r65, %r64;
	@%p1 bra 	$L__BB109_2;

	mov.u32 	%r66, 0;
	st.shared.u32 	[%r4], %r66;

$L__BB109_2:
	bar.sync 	0;
	mov.f32 	%f7, 0f00000000;
	st.local.v2.f32 	[%rd3], {%f7, %f7};
	st.local.v2.f32 	[%rd3+8], {%f7, %f7};
	st.local.v2.f32 	[%rd3+16], {%f7, %f7};
	mov.u32 	%r521, 0;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs2, %f7;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f7;}

	// end inline asm
	mov.b32 	%r526, {%rs1, %rs2};
	setp.ge.s32 	%p2, %r3, %r52;
	mov.u32 	%r522, %r521;
	mov.u32 	%r523, %r521;
	mov.u32 	%r524, %r521;
	mov.u32 	%r525, %r521;
	@%p2 bra 	$L__BB109_9;

	not.b32 	%r77, %r3;
	add.s32 	%r6, %r77, %r52;
	mul.wide.u32 	%rd32, %r6, -1431655765;
	shr.u64 	%rd33, %rd32, 39;
	cvt.u32.u64 	%r78, %rd33;
	add.s32 	%r79, %r78, 1;
	and.b32  	%r506, %r79, 3;
	setp.eq.s32 	%p3, %r506, 0;
	mov.u32 	%r521, 0;
	mov.u32 	%r513, %r3;
	@%p3 bra 	$L__BB109_6;

	shl.b32 	%r85, %r53, 1;
	add.s32 	%r86, %r3, %r85;
	mul.wide.s32 	%rd34, %r86, 4;
	shl.b64 	%rd35, %rd5, 1;
	add.s64 	%rd7, %rd34, %rd35;
	mul.wide.s32 	%rd36, %r3, 4;
	mul.wide.s32 	%rd8, %r53, 4;
	add.s64 	%rd37, %rd36, %rd8;
	add.s64 	%rd9, %rd37, %rd35;
	add.s64 	%rd10, %rd36, %rd35;
	mul.wide.s32 	%rd38, %r3, 2;
	add.s64 	%rd39, %rd38, %rd4;
	shl.b64 	%rd40, %rd39, 1;
	add.s64 	%rd75, %rd2, %rd40;
	mov.u32 	%r521, 0;
	mov.u64 	%rd76, %rd78;
	mov.u32 	%r522, %r521;
	mov.u32 	%r523, %r521;
	mov.u32 	%r524, %r521;
	mov.u32 	%r525, %r521;
	mov.u32 	%r513, %r3;

$L__BB109_5:
	.pragma "nounroll";
	ld.global.nc.u32 	%r88, [%rd75];
	add.s64 	%rd41, %rd76, %rd10;
	ld.global.nc.u32 	%r89, [%rd41];
	// begin inline asm
	{mul.f16x2 %r87,%r88,%r89;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r526,%r526,%r87;
}
	// end inline asm
	add.s64 	%rd42, %rd76, %rd9;
	ld.global.nc.u32 	%r95, [%rd42];
	// begin inline asm
	{mul.f16x2 %r93,%r88,%r95;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r525,%r525,%r93;
}
	// end inline asm
	add.s64 	%rd43, %rd76, %rd7;
	ld.global.nc.u32 	%r101, [%rd43];
	// begin inline asm
	{mul.f16x2 %r99,%r88,%r101;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r524,%r524,%r99;
}
	// end inline asm
	add.s64 	%rd44, %rd43, %rd8;
	ld.global.nc.u32 	%r107, [%rd44];
	// begin inline asm
	{mul.f16x2 %r105,%r88,%r107;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r523,%r523,%r105;
}
	// end inline asm
	add.s64 	%rd45, %rd44, %rd8;
	ld.global.nc.u32 	%r113, [%rd45];
	// begin inline asm
	{mul.f16x2 %r111,%r88,%r113;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r522,%r522,%r111;
}
	// end inline asm
	add.s64 	%rd46, %rd45, %rd8;
	ld.global.nc.u32 	%r119, [%rd46];
	// begin inline asm
	{mul.f16x2 %r117,%r88,%r119;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r521,%r521,%r117;
}
	// end inline asm
	add.s32 	%r513, %r513, 192;
	add.s64 	%rd76, %rd76, 768;
	add.s64 	%rd75, %rd75, 768;
	add.s32 	%r506, %r506, -1;
	setp.ne.s32 	%p4, %r506, 0;
	@%p4 bra 	$L__BB109_5;

$L__BB109_6:
	setp.lt.u32 	%p5, %r6, 576;
	@%p5 bra 	$L__BB109_9;

	add.s32 	%r123, %r513, %r53;
	shl.b32 	%r124, %r53, 1;
	add.s32 	%r125, %r513, %r124;
	mad.lo.s32 	%r126, %r53, 3, %r513;
	shl.b32 	%r127, %r53, 2;
	add.s32 	%r128, %r513, %r127;
	mad.lo.s32 	%r129, %r53, 5, %r513;
	add.s32 	%r130, %r123, 192;
	mul.wide.s32 	%rd47, %r130, 4;
	shl.b64 	%rd48, %rd5, 1;
	add.s64 	%rd16, %rd47, %rd48;
	mul.wide.s32 	%rd49, %r125, 4;
	add.s64 	%rd17, %rd49, %rd48;
	mul.wide.s32 	%rd50, %r126, 4;
	add.s64 	%rd18, %rd50, %rd48;
	mul.wide.s32 	%rd51, %r128, 4;
	add.s64 	%rd19, %rd51, %rd48;
	mul.wide.s32 	%rd52, %r129, 4;
	add.s64 	%rd20, %rd52, %rd48;
	mul.wide.s32 	%rd53, %r513, 2;
	add.s64 	%rd54, %rd53, %rd4;
	shl.b64 	%rd55, %rd54, 1;
	add.s64 	%rd56, %rd2, %rd55;
	add.s64 	%rd77, %rd56, 1536;
	mul.wide.s32 	%rd57, %r513, 4;
	add.s64 	%rd22, %rd57, %rd48;
	mul.wide.s32 	%rd58, %r53, 4;
	add.s64 	%rd59, %rd57, %rd58;
	add.s64 	%rd23, %rd59, %rd48;

$L__BB109_8:
	ld.global.nc.u32 	%r132, [%rd77+-1536];
	add.s64 	%rd60, %rd78, %rd22;
	ld.global.nc.u32 	%r133, [%rd60];
	// begin inline asm
	{mul.f16x2 %r131,%r132,%r133;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r134,%r526,%r131;
}
	// end inline asm
	add.s64 	%rd61, %rd78, %rd23;
	ld.global.nc.u32 	%r139, [%rd61];
	// begin inline asm
	{mul.f16x2 %r137,%r132,%r139;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r140,%r525,%r137;
}
	// end inline asm
	add.s64 	%rd62, %rd78, %rd17;
	ld.global.nc.u32 	%r145, [%rd62];
	// begin inline asm
	{mul.f16x2 %r143,%r132,%r145;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r146,%r524,%r143;
}
	// end inline asm
	add.s64 	%rd63, %rd78, %rd18;
	ld.global.nc.u32 	%r151, [%rd63];
	// begin inline asm
	{mul.f16x2 %r149,%r132,%r151;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r152,%r523,%r149;
}
	// end inline asm
	add.s64 	%rd64, %rd78, %rd19;
	ld.global.nc.u32 	%r157, [%rd64];
	// begin inline asm
	{mul.f16x2 %r155,%r132,%r157;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r158,%r522,%r155;
}
	// end inline asm
	add.s64 	%rd65, %rd78, %rd20;
	ld.global.nc.u32 	%r163, [%rd65];
	// begin inline asm
	{mul.f16x2 %r161,%r132,%r163;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r164,%r521,%r161;
}
	// end inline asm
	ld.global.nc.u32 	%r168, [%rd77+-768];
	ld.global.nc.u32 	%r169, [%rd60+768];
	// begin inline asm
	{mul.f16x2 %r167,%r168,%r169;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r170,%r134,%r167;
}
	// end inline asm
	add.s64 	%rd66, %rd78, %rd16;
	ld.global.nc.u32 	%r175, [%rd66];
	// begin inline asm
	{mul.f16x2 %r173,%r168,%r175;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r176,%r140,%r173;
}
	// end inline asm
	ld.global.nc.u32 	%r181, [%rd62+768];
	// begin inline asm
	{mul.f16x2 %r179,%r168,%r181;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r182,%r146,%r179;
}
	// end inline asm
	ld.global.nc.u32 	%r187, [%rd63+768];
	// begin inline asm
	{mul.f16x2 %r185,%r168,%r187;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r188,%r152,%r185;
}
	// end inline asm
	ld.global.nc.u32 	%r193, [%rd64+768];
	// begin inline asm
	{mul.f16x2 %r191,%r168,%r193;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r194,%r158,%r191;
}
	// end inline asm
	ld.global.nc.u32 	%r199, [%rd65+768];
	// begin inline asm
	{mul.f16x2 %r197,%r168,%r199;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r200,%r164,%r197;
}
	// end inline asm
	ld.global.nc.u32 	%r204, [%rd77];
	ld.global.nc.u32 	%r205, [%rd60+1536];
	// begin inline asm
	{mul.f16x2 %r203,%r204,%r205;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r206,%r170,%r203;
}
	// end inline asm
	ld.global.nc.u32 	%r211, [%rd66+768];
	// begin inline asm
	{mul.f16x2 %r209,%r204,%r211;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r212,%r176,%r209;
}
	// end inline asm
	ld.global.nc.u32 	%r217, [%rd62+1536];
	// begin inline asm
	{mul.f16x2 %r215,%r204,%r217;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r218,%r182,%r215;
}
	// end inline asm
	ld.global.nc.u32 	%r223, [%rd63+1536];
	// begin inline asm
	{mul.f16x2 %r221,%r204,%r223;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r224,%r188,%r221;
}
	// end inline asm
	ld.global.nc.u32 	%r229, [%rd64+1536];
	// begin inline asm
	{mul.f16x2 %r227,%r204,%r229;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r230,%r194,%r227;
}
	// end inline asm
	ld.global.nc.u32 	%r235, [%rd65+1536];
	// begin inline asm
	{mul.f16x2 %r233,%r204,%r235;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r236,%r200,%r233;
}
	// end inline asm
	ld.global.nc.u32 	%r240, [%rd77+768];
	ld.global.nc.u32 	%r241, [%rd60+2304];
	// begin inline asm
	{mul.f16x2 %r239,%r240,%r241;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r526,%r206,%r239;
}
	// end inline asm
	ld.global.nc.u32 	%r247, [%rd66+1536];
	// begin inline asm
	{mul.f16x2 %r245,%r240,%r247;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r525,%r212,%r245;
}
	// end inline asm
	ld.global.nc.u32 	%r253, [%rd62+2304];
	// begin inline asm
	{mul.f16x2 %r251,%r240,%r253;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r524,%r218,%r251;
}
	// end inline asm
	ld.global.nc.u32 	%r259, [%rd63+2304];
	// begin inline asm
	{mul.f16x2 %r257,%r240,%r259;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r523,%r224,%r257;
}
	// end inline asm
	ld.global.nc.u32 	%r265, [%rd64+2304];
	// begin inline asm
	{mul.f16x2 %r263,%r240,%r265;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r522,%r230,%r263;
}
	// end inline asm
	ld.global.nc.u32 	%r271, [%rd65+2304];
	// begin inline asm
	{mul.f16x2 %r269,%r240,%r271;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r521,%r236,%r269;
}
	// end inline asm
	add.s64 	%rd78, %rd78, 3072;
	add.s64 	%rd77, %rd77, 3072;
	add.s32 	%r513, %r513, 768;
	setp.lt.s32 	%p6, %r513, %r52;
	@%p6 bra 	$L__BB109_8;

$L__BB109_9:
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r526;
  cvt.f32.f16 %f8, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r526;
  cvt.f32.f16 %f9, high;}

	// end inline asm
	add.f32 	%f20, %f8, %f9;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r525;
  cvt.f32.f16 %f10, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r525;
  cvt.f32.f16 %f11, high;}

	// end inline asm
	add.f32 	%f1, %f10, %f11;
	st.local.f32 	[%rd3+4], %f1;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r524;
  cvt.f32.f16 %f12, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r524;
  cvt.f32.f16 %f13, high;}

	// end inline asm
	add.f32 	%f2, %f12, %f13;
	st.local.f32 	[%rd3+8], %f2;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r523;
  cvt.f32.f16 %f14, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r523;
  cvt.f32.f16 %f15, high;}

	// end inline asm
	add.f32 	%f3, %f14, %f15;
	st.local.f32 	[%rd3+12], %f3;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r522;
  cvt.f32.f16 %f16, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r522;
  cvt.f32.f16 %f17, high;}

	// end inline asm
	add.f32 	%f4, %f16, %f17;
	st.local.f32 	[%rd3+16], %f4;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r521;
  cvt.f32.f16 %f18, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r521;
  cvt.f32.f16 %f19, high;}

	// end inline asm
	add.f32 	%f5, %f18, %f19;
	st.local.f32 	[%rd3+20], %f5;
	shr.s32 	%r287, %r3, 31;
	shr.u32 	%r288, %r287, 27;
	add.s32 	%r289, %r3, %r288;
	shr.s32 	%r290, %r289, 5;
	shl.b32 	%r291, %r290, 2;
	add.s32 	%r51, %r65, %r291;
	mov.u32 	%r293, 2;
	mov.b32 	%r294, %f20;
	mov.u32 	%r295, 31;
	mov.u32 	%r296, 16;
	mov.u32 	%r297, -1;
	shfl.sync.bfly.b32 	%r298|%p7, %r294, %r296, %r295, %r297;
	mov.b32 	%f21, %r298;
	add.f32 	%f22, %f20, %f21;
	mov.b32 	%r299, %f22;
	mov.u32 	%r300, 8;
	shfl.sync.bfly.b32 	%r301|%p8, %r299, %r300, %r295, %r297;
	mov.b32 	%f23, %r301;
	add.f32 	%f24, %f22, %f23;
	mov.b32 	%r302, %f24;
	mov.u32 	%r303, 4;
	shfl.sync.bfly.b32 	%r304|%p9, %r302, %r303, %r295, %r297;
	mov.b32 	%f25, %r304;
	add.f32 	%f26, %f24, %f25;
	mov.b32 	%r305, %f26;
	shfl.sync.bfly.b32 	%r306|%p10, %r305, %r293, %r295, %r297;
	mov.b32 	%f27, %r306;
	add.f32 	%f28, %f26, %f27;
	mov.b32 	%r307, %f28;
	mov.u32 	%r308, 1;
	shfl.sync.bfly.b32 	%r309|%p11, %r307, %r308, %r295, %r297;
	mov.b32 	%f29, %r309;
	add.f32 	%f30, %f28, %f29;
	st.local.f32 	[%rd3], %f30;
	st.shared.f32 	[%r51], %f30;
	bar.sync 	0;
	@%p1 bra 	$L__BB109_11;

	ld.shared.f32 	%f31, [%r4];
	mov.b32 	%r310, %f31;
	shfl.sync.bfly.b32 	%r314|%p13, %r310, %r296, %r295, %r297;
	mov.b32 	%f32, %r314;
	add.f32 	%f33, %f31, %f32;
	mov.b32 	%r315, %f33;
	shfl.sync.bfly.b32 	%r317|%p14, %r315, %r300, %r295, %r297;
	mov.b32 	%f34, %r317;
	add.f32 	%f35, %f33, %f34;
	mov.b32 	%r318, %f35;
	shfl.sync.bfly.b32 	%r320|%p15, %r318, %r303, %r295, %r297;
	mov.b32 	%f36, %r320;
	add.f32 	%f37, %f35, %f36;
	mov.b32 	%r321, %f37;
	shfl.sync.bfly.b32 	%r323|%p16, %r321, %r293, %r295, %r297;
	mov.b32 	%f38, %r323;
	add.f32 	%f39, %f37, %f38;
	mov.b32 	%r324, %f39;
	shfl.sync.bfly.b32 	%r326|%p17, %r324, %r308, %r295, %r297;
	mov.b32 	%f40, %r326;
	add.f32 	%f41, %f39, %f40;
	st.local.f32 	[%rd3], %f41;

$L__BB109_11:
	bar.sync 	0;
	mov.b32 	%r327, %f1;
	shfl.sync.bfly.b32 	%r331|%p19, %r327, %r296, %r295, %r297;
	mov.b32 	%f42, %r331;
	add.f32 	%f43, %f1, %f42;
	mov.b32 	%r332, %f43;
	shfl.sync.bfly.b32 	%r334|%p20, %r332, %r300, %r295, %r297;
	mov.b32 	%f44, %r334;
	add.f32 	%f45, %f43, %f44;
	mov.b32 	%r335, %f45;
	shfl.sync.bfly.b32 	%r337|%p21, %r335, %r303, %r295, %r297;
	mov.b32 	%f46, %r337;
	add.f32 	%f47, %f45, %f46;
	mov.b32 	%r338, %f47;
	shfl.sync.bfly.b32 	%r340|%p22, %r338, %r293, %r295, %r297;
	mov.b32 	%f48, %r340;
	add.f32 	%f49, %f47, %f48;
	mov.b32 	%r341, %f49;
	shfl.sync.bfly.b32 	%r343|%p23, %r341, %r308, %r295, %r297;
	mov.b32 	%f50, %r343;
	add.f32 	%f51, %f49, %f50;
	st.local.f32 	[%rd3+4], %f51;
	st.shared.f32 	[%r51], %f51;
	bar.sync 	0;
	@%p1 bra 	$L__BB109_13;

	ld.shared.f32 	%f52, [%r4];
	mov.b32 	%r344, %f52;
	mov.u32 	%r345, 31;
	mov.u32 	%r346, 16;
	mov.u32 	%r347, -1;
	shfl.sync.bfly.b32 	%r348|%p24, %r344, %r346, %r345, %r347;
	mov.b32 	%f53, %r348;
	add.f32 	%f54, %f52, %f53;
	mov.b32 	%r349, %f54;
	mov.u32 	%r350, 8;
	shfl.sync.bfly.b32 	%r351|%p25, %r349, %r350, %r345, %r347;
	mov.b32 	%f55, %r351;
	add.f32 	%f56, %f54, %f55;
	mov.b32 	%r352, %f56;
	mov.u32 	%r353, 4;
	shfl.sync.bfly.b32 	%r354|%p26, %r352, %r353, %r345, %r347;
	mov.b32 	%f57, %r354;
	add.f32 	%f58, %f56, %f57;
	mov.b32 	%r355, %f58;
	mov.u32 	%r356, 2;
	shfl.sync.bfly.b32 	%r357|%p27, %r355, %r356, %r345, %r347;
	mov.b32 	%f59, %r357;
	add.f32 	%f60, %f58, %f59;
	mov.b32 	%r358, %f60;
	mov.u32 	%r359, 1;
	shfl.sync.bfly.b32 	%r360|%p28, %r358, %r359, %r345, %r347;
	mov.b32 	%f61, %r360;
	add.f32 	%f62, %f60, %f61;
	st.local.f32 	[%rd3+4], %f62;

$L__BB109_13:
	bar.sync 	0;
	mov.b32 	%r361, %f2;
	mov.u32 	%r362, 31;
	mov.u32 	%r363, 16;
	mov.u32 	%r364, -1;
	shfl.sync.bfly.b32 	%r365|%p30, %r361, %r363, %r362, %r364;
	mov.b32 	%f63, %r365;
	add.f32 	%f64, %f2, %f63;
	mov.b32 	%r366, %f64;
	mov.u32 	%r367, 8;
	shfl.sync.bfly.b32 	%r368|%p31, %r366, %r367, %r362, %r364;
	mov.b32 	%f65, %r368;
	add.f32 	%f66, %f64, %f65;
	mov.b32 	%r369, %f66;
	mov.u32 	%r370, 4;
	shfl.sync.bfly.b32 	%r371|%p32, %r369, %r370, %r362, %r364;
	mov.b32 	%f67, %r371;
	add.f32 	%f68, %f66, %f67;
	mov.b32 	%r372, %f68;
	mov.u32 	%r373, 2;
	shfl.sync.bfly.b32 	%r374|%p33, %r372, %r373, %r362, %r364;
	mov.b32 	%f69, %r374;
	add.f32 	%f70, %f68, %f69;
	mov.b32 	%r375, %f70;
	mov.u32 	%r376, 1;
	shfl.sync.bfly.b32 	%r377|%p34, %r375, %r376, %r362, %r364;
	mov.b32 	%f71, %r377;
	add.f32 	%f72, %f70, %f71;
	st.local.f32 	[%rd3+8], %f72;
	st.shared.f32 	[%r51], %f72;
	bar.sync 	0;
	@%p1 bra 	$L__BB109_15;

	ld.shared.f32 	%f73, [%r4];
	mov.b32 	%r378, %f73;
	shfl.sync.bfly.b32 	%r382|%p35, %r378, %r363, %r362, %r364;
	mov.b32 	%f74, %r382;
	add.f32 	%f75, %f73, %f74;
	mov.b32 	%r383, %f75;
	shfl.sync.bfly.b32 	%r385|%p36, %r383, %r367, %r362, %r364;
	mov.b32 	%f76, %r385;
	add.f32 	%f77, %f75, %f76;
	mov.b32 	%r386, %f77;
	shfl.sync.bfly.b32 	%r388|%p37, %r386, %r370, %r362, %r364;
	mov.b32 	%f78, %r388;
	add.f32 	%f79, %f77, %f78;
	mov.b32 	%r389, %f79;
	shfl.sync.bfly.b32 	%r391|%p38, %r389, %r373, %r362, %r364;
	mov.b32 	%f80, %r391;
	add.f32 	%f81, %f79, %f80;
	mov.b32 	%r392, %f81;
	shfl.sync.bfly.b32 	%r394|%p39, %r392, %r376, %r362, %r364;
	mov.b32 	%f82, %r394;
	add.f32 	%f83, %f81, %f82;
	st.local.f32 	[%rd3+8], %f83;

$L__BB109_15:
	bar.sync 	0;
	mov.b32 	%r395, %f3;
	shfl.sync.bfly.b32 	%r399|%p41, %r395, %r363, %r362, %r364;
	mov.b32 	%f84, %r399;
	add.f32 	%f85, %f3, %f84;
	mov.b32 	%r400, %f85;
	shfl.sync.bfly.b32 	%r402|%p42, %r400, %r367, %r362, %r364;
	mov.b32 	%f86, %r402;
	add.f32 	%f87, %f85, %f86;
	mov.b32 	%r403, %f87;
	shfl.sync.bfly.b32 	%r405|%p43, %r403, %r370, %r362, %r364;
	mov.b32 	%f88, %r405;
	add.f32 	%f89, %f87, %f88;
	mov.b32 	%r406, %f89;
	shfl.sync.bfly.b32 	%r408|%p44, %r406, %r373, %r362, %r364;
	mov.b32 	%f90, %r408;
	add.f32 	%f91, %f89, %f90;
	mov.b32 	%r409, %f91;
	shfl.sync.bfly.b32 	%r411|%p45, %r409, %r376, %r362, %r364;
	mov.b32 	%f92, %r411;
	add.f32 	%f93, %f91, %f92;
	st.local.f32 	[%rd3+12], %f93;
	st.shared.f32 	[%r51], %f93;
	bar.sync 	0;
	@%p1 bra 	$L__BB109_17;

	ld.shared.f32 	%f94, [%r4];
	mov.b32 	%r412, %f94;
	mov.u32 	%r413, 31;
	mov.u32 	%r414, 16;
	mov.u32 	%r415, -1;
	shfl.sync.bfly.b32 	%r416|%p46, %r412, %r414, %r413, %r415;
	mov.b32 	%f95, %r416;
	add.f32 	%f96, %f94, %f95;
	mov.b32 	%r417, %f96;
	mov.u32 	%r418, 8;
	shfl.sync.bfly.b32 	%r419|%p47, %r417, %r418, %r413, %r415;
	mov.b32 	%f97, %r419;
	add.f32 	%f98, %f96, %f97;
	mov.b32 	%r420, %f98;
	mov.u32 	%r421, 4;
	shfl.sync.bfly.b32 	%r422|%p48, %r420, %r421, %r413, %r415;
	mov.b32 	%f99, %r422;
	add.f32 	%f100, %f98, %f99;
	mov.b32 	%r423, %f100;
	mov.u32 	%r424, 2;
	shfl.sync.bfly.b32 	%r425|%p49, %r423, %r424, %r413, %r415;
	mov.b32 	%f101, %r425;
	add.f32 	%f102, %f100, %f101;
	mov.b32 	%r426, %f102;
	mov.u32 	%r427, 1;
	shfl.sync.bfly.b32 	%r428|%p50, %r426, %r427, %r413, %r415;
	mov.b32 	%f103, %r428;
	add.f32 	%f104, %f102, %f103;
	st.local.f32 	[%rd3+12], %f104;

$L__BB109_17:
	bar.sync 	0;
	mov.b32 	%r429, %f4;
	mov.u32 	%r430, 31;
	mov.u32 	%r431, 16;
	mov.u32 	%r432, -1;
	shfl.sync.bfly.b32 	%r433|%p52, %r429, %r431, %r430, %r432;
	mov.b32 	%f105, %r433;
	add.f32 	%f106, %f4, %f105;
	mov.b32 	%r434, %f106;
	mov.u32 	%r435, 8;
	shfl.sync.bfly.b32 	%r436|%p53, %r434, %r435, %r430, %r432;
	mov.b32 	%f107, %r436;
	add.f32 	%f108, %f106, %f107;
	mov.b32 	%r437, %f108;
	mov.u32 	%r438, 4;
	shfl.sync.bfly.b32 	%r439|%p54, %r437, %r438, %r430, %r432;
	mov.b32 	%f109, %r439;
	add.f32 	%f110, %f108, %f109;
	mov.b32 	%r440, %f110;
	mov.u32 	%r441, 2;
	shfl.sync.bfly.b32 	%r442|%p55, %r440, %r441, %r430, %r432;
	mov.b32 	%f111, %r442;
	add.f32 	%f112, %f110, %f111;
	mov.b32 	%r443, %f112;
	mov.u32 	%r444, 1;
	shfl.sync.bfly.b32 	%r445|%p56, %r443, %r444, %r430, %r432;
	mov.b32 	%f113, %r445;
	add.f32 	%f114, %f112, %f113;
	st.local.f32 	[%rd3+16], %f114;
	st.shared.f32 	[%r51], %f114;
	bar.sync 	0;
	@%p1 bra 	$L__BB109_19;

	ld.shared.f32 	%f115, [%r4];
	mov.b32 	%r446, %f115;
	shfl.sync.bfly.b32 	%r450|%p57, %r446, %r431, %r430, %r432;
	mov.b32 	%f116, %r450;
	add.f32 	%f117, %f115, %f116;
	mov.b32 	%r451, %f117;
	shfl.sync.bfly.b32 	%r453|%p58, %r451, %r435, %r430, %r432;
	mov.b32 	%f118, %r453;
	add.f32 	%f119, %f117, %f118;
	mov.b32 	%r454, %f119;
	shfl.sync.bfly.b32 	%r456|%p59, %r454, %r438, %r430, %r432;
	mov.b32 	%f120, %r456;
	add.f32 	%f121, %f119, %f120;
	mov.b32 	%r457, %f121;
	shfl.sync.bfly.b32 	%r459|%p60, %r457, %r441, %r430, %r432;
	mov.b32 	%f122, %r459;
	add.f32 	%f123, %f121, %f122;
	mov.b32 	%r460, %f123;
	shfl.sync.bfly.b32 	%r462|%p61, %r460, %r444, %r430, %r432;
	mov.b32 	%f124, %r462;
	add.f32 	%f125, %f123, %f124;
	st.local.f32 	[%rd3+16], %f125;

$L__BB109_19:
	bar.sync 	0;
	mov.b32 	%r463, %f5;
	shfl.sync.bfly.b32 	%r467|%p63, %r463, %r431, %r430, %r432;
	mov.b32 	%f126, %r467;
	add.f32 	%f127, %f5, %f126;
	mov.b32 	%r468, %f127;
	shfl.sync.bfly.b32 	%r470|%p64, %r468, %r435, %r430, %r432;
	mov.b32 	%f128, %r470;
	add.f32 	%f129, %f127, %f128;
	mov.b32 	%r471, %f129;
	shfl.sync.bfly.b32 	%r473|%p65, %r471, %r438, %r430, %r432;
	mov.b32 	%f130, %r473;
	add.f32 	%f131, %f129, %f130;
	mov.b32 	%r474, %f131;
	shfl.sync.bfly.b32 	%r476|%p66, %r474, %r441, %r430, %r432;
	mov.b32 	%f132, %r476;
	add.f32 	%f133, %f131, %f132;
	mov.b32 	%r477, %f133;
	shfl.sync.bfly.b32 	%r479|%p67, %r477, %r444, %r430, %r432;
	mov.b32 	%f134, %r479;
	add.f32 	%f135, %f133, %f134;
	st.local.f32 	[%rd3+20], %f135;
	st.shared.f32 	[%r51], %f135;
	bar.sync 	0;
	@%p1 bra 	$L__BB109_21;

	ld.shared.f32 	%f136, [%r4];
	mov.b32 	%r480, %f136;
	mov.u32 	%r481, 31;
	mov.u32 	%r482, 16;
	mov.u32 	%r483, -1;
	shfl.sync.bfly.b32 	%r484|%p68, %r480, %r482, %r481, %r483;
	mov.b32 	%f137, %r484;
	add.f32 	%f138, %f136, %f137;
	mov.b32 	%r485, %f138;
	mov.u32 	%r486, 8;
	shfl.sync.bfly.b32 	%r487|%p69, %r485, %r486, %r481, %r483;
	mov.b32 	%f139, %r487;
	add.f32 	%f140, %f138, %f139;
	mov.b32 	%r488, %f140;
	mov.u32 	%r489, 4;
	shfl.sync.bfly.b32 	%r490|%p70, %r488, %r489, %r481, %r483;
	mov.b32 	%f141, %r490;
	add.f32 	%f142, %f140, %f141;
	mov.b32 	%r491, %f142;
	mov.u32 	%r492, 2;
	shfl.sync.bfly.b32 	%r493|%p71, %r491, %r492, %r481, %r483;
	mov.b32 	%f143, %r493;
	add.f32 	%f144, %f142, %f143;
	mov.b32 	%r494, %f144;
	mov.u32 	%r495, 1;
	shfl.sync.bfly.b32 	%r496|%p72, %r494, %r495, %r481, %r483;
	mov.b32 	%f145, %r496;
	add.f32 	%f146, %f144, %f145;
	st.local.f32 	[%rd3+20], %f146;

$L__BB109_21:
	bar.sync 	0;
	setp.gt.s32 	%p73, %r3, 5;
	@%p73 bra 	$L__BB109_23;

	mad.lo.s32 	%r497, %r3, %r54, %r2;
	cvt.s64.s32 	%rd67, %r497;
	mul.lo.s32 	%r498, %r1, %r55;
	cvt.s64.s32 	%rd68, %r498;
	add.s64 	%rd69, %rd68, %rd67;
	mul.wide.s32 	%rd70, %r3, 4;
	add.s64 	%rd71, %rd3, %rd70;
	ld.local.f32 	%f147, [%rd71];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs3, %f147;}

	// end inline asm
	cvta.to.global.u64 	%rd72, %rd28;
	shl.b64 	%rd73, %rd69, 1;
	add.s64 	%rd74, %rd72, %rd73;
	st.global.u16 	[%rd74], %rs3;

$L__BB109_23:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_7_bs_192
.visible .entry ggml_matvec_f16_ncols_7_bs_192(
	.param .u64 ggml_matvec_f16_ncols_7_bs_192_param_0,
	.param .u64 ggml_matvec_f16_ncols_7_bs_192_param_1,
	.param .u64 ggml_matvec_f16_ncols_7_bs_192_param_2,
	.param .u32 ggml_matvec_f16_ncols_7_bs_192_param_3,
	.param .u32 ggml_matvec_f16_ncols_7_bs_192_param_4,
	.param .u32 ggml_matvec_f16_ncols_7_bs_192_param_5,
	.param .u32 ggml_matvec_f16_ncols_7_bs_192_param_6,
	.param .u32 ggml_matvec_f16_ncols_7_bs_192_param_7,
	.param .u32 ggml_matvec_f16_ncols_7_bs_192_param_8,
	.param .u32 ggml_matvec_f16_ncols_7_bs_192_param_9,
	.param .u32 ggml_matvec_f16_ncols_7_bs_192_param_10,
	.param .u32 ggml_matvec_f16_ncols_7_bs_192_param_11
)
{
	.local .align 4 .b8 	__local_depot110[28];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<85>;
	.reg .b16 	%rs<4>;
	.reg .f32 	%f<172>;
	.reg .b32 	%r<607>;
	.reg .b64 	%rd<83>;


	mov.u64 	%SPL, __local_depot110;
	ld.param.u64 	%rd30, [ggml_matvec_f16_ncols_7_bs_192_param_0];
	ld.param.u64 	%rd31, [ggml_matvec_f16_ncols_7_bs_192_param_1];
	ld.param.u64 	%rd29, [ggml_matvec_f16_ncols_7_bs_192_param_2];
	ld.param.u32 	%r58, [ggml_matvec_f16_ncols_7_bs_192_param_3];
	ld.param.u32 	%r62, [ggml_matvec_f16_ncols_7_bs_192_param_5];
	ld.param.u32 	%r59, [ggml_matvec_f16_ncols_7_bs_192_param_6];
	ld.param.u32 	%r60, [ggml_matvec_f16_ncols_7_bs_192_param_7];
	ld.param.u32 	%r63, [ggml_matvec_f16_ncols_7_bs_192_param_8];
	ld.param.u32 	%r64, [ggml_matvec_f16_ncols_7_bs_192_param_9];
	ld.param.u32 	%r65, [ggml_matvec_f16_ncols_7_bs_192_param_10];
	ld.param.u32 	%r61, [ggml_matvec_f16_ncols_7_bs_192_param_11];
	cvta.to.global.u64 	%rd82, %rd31;
	cvta.to.global.u64 	%rd2, %rd30;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r66, %r1, %r63;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r67, %r2, %r62;
	mad.lo.s32 	%r68, %r66, %r64, %r67;
	cvt.s64.s32 	%rd4, %r68;
	mul.lo.s32 	%r69, %r1, %r65;
	cvt.s64.s32 	%rd5, %r69;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r70, %r3, 2;
	mov.u32 	%r71, data_mmv;
	add.s32 	%r4, %r71, %r70;
	@%p1 bra 	$L__BB110_2;

	mov.u32 	%r72, 0;
	st.shared.u32 	[%r4], %r72;

$L__BB110_2:
	bar.sync 	0;
	mov.f32 	%f8, 0f00000000;
	mov.u32 	%r600, 0;
	st.local.u32 	[%rd3], %r600;
	st.local.u32 	[%rd3+4], %r600;
	st.local.u32 	[%rd3+8], %r600;
	st.local.u32 	[%rd3+12], %r600;
	st.local.u32 	[%rd3+16], %r600;
	st.local.u32 	[%rd3+20], %r600;
	st.local.u32 	[%rd3+24], %r600;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs2, %f8;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f8;}

	// end inline asm
	mov.b32 	%r606, {%rs1, %rs2};
	setp.ge.s32 	%p2, %r3, %r58;
	mov.u32 	%r601, %r600;
	mov.u32 	%r602, %r600;
	mov.u32 	%r603, %r600;
	mov.u32 	%r604, %r600;
	mov.u32 	%r605, %r600;
	@%p2 bra 	$L__BB110_9;

	not.b32 	%r85, %r3;
	add.s32 	%r6, %r85, %r58;
	mul.wide.u32 	%rd33, %r6, -1431655765;
	shr.u64 	%rd34, %rd33, 39;
	cvt.u32.u64 	%r86, %rd34;
	add.s32 	%r87, %r86, 1;
	and.b32  	%r583, %r87, 3;
	setp.eq.s32 	%p3, %r583, 0;
	mov.u32 	%r600, 0;
	mov.u32 	%r591, %r3;
	@%p3 bra 	$L__BB110_6;

	shl.b32 	%r94, %r59, 1;
	add.s32 	%r95, %r3, %r94;
	mul.wide.s32 	%rd35, %r95, 4;
	shl.b64 	%rd36, %rd5, 1;
	add.s64 	%rd7, %rd35, %rd36;
	mul.wide.s32 	%rd37, %r3, 4;
	mul.wide.s32 	%rd8, %r59, 4;
	add.s64 	%rd38, %rd37, %rd8;
	add.s64 	%rd9, %rd38, %rd36;
	add.s64 	%rd10, %rd37, %rd36;
	mul.wide.s32 	%rd39, %r3, 2;
	add.s64 	%rd40, %rd39, %rd4;
	shl.b64 	%rd41, %rd40, 1;
	add.s64 	%rd79, %rd2, %rd41;
	mov.u32 	%r600, 0;
	mov.u64 	%rd80, %rd82;
	mov.u32 	%r601, %r600;
	mov.u32 	%r602, %r600;
	mov.u32 	%r603, %r600;
	mov.u32 	%r604, %r600;
	mov.u32 	%r605, %r600;
	mov.u32 	%r591, %r3;

$L__BB110_5:
	.pragma "nounroll";
	ld.global.nc.u32 	%r97, [%rd79];
	add.s64 	%rd42, %rd80, %rd10;
	ld.global.nc.u32 	%r98, [%rd42];
	// begin inline asm
	{mul.f16x2 %r96,%r97,%r98;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r606,%r606,%r96;
}
	// end inline asm
	add.s64 	%rd43, %rd80, %rd9;
	ld.global.nc.u32 	%r104, [%rd43];
	// begin inline asm
	{mul.f16x2 %r102,%r97,%r104;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r605,%r605,%r102;
}
	// end inline asm
	add.s64 	%rd44, %rd80, %rd7;
	ld.global.nc.u32 	%r110, [%rd44];
	// begin inline asm
	{mul.f16x2 %r108,%r97,%r110;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r604,%r604,%r108;
}
	// end inline asm
	add.s64 	%rd45, %rd44, %rd8;
	ld.global.nc.u32 	%r116, [%rd45];
	// begin inline asm
	{mul.f16x2 %r114,%r97,%r116;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r603,%r603,%r114;
}
	// end inline asm
	add.s64 	%rd46, %rd45, %rd8;
	ld.global.nc.u32 	%r122, [%rd46];
	// begin inline asm
	{mul.f16x2 %r120,%r97,%r122;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r602,%r602,%r120;
}
	// end inline asm
	add.s64 	%rd47, %rd46, %rd8;
	ld.global.nc.u32 	%r128, [%rd47];
	// begin inline asm
	{mul.f16x2 %r126,%r97,%r128;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r601,%r601,%r126;
}
	// end inline asm
	add.s64 	%rd48, %rd47, %rd8;
	ld.global.nc.u32 	%r134, [%rd48];
	// begin inline asm
	{mul.f16x2 %r132,%r97,%r134;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r600,%r600,%r132;
}
	// end inline asm
	add.s32 	%r591, %r591, 192;
	add.s64 	%rd80, %rd80, 768;
	add.s64 	%rd79, %rd79, 768;
	add.s32 	%r583, %r583, -1;
	setp.ne.s32 	%p4, %r583, 0;
	@%p4 bra 	$L__BB110_5;

$L__BB110_6:
	setp.lt.u32 	%p5, %r6, 576;
	@%p5 bra 	$L__BB110_9;

	add.s32 	%r138, %r591, %r59;
	shl.b32 	%r139, %r59, 1;
	add.s32 	%r140, %r591, %r139;
	mad.lo.s32 	%r141, %r59, 3, %r591;
	shl.b32 	%r142, %r59, 2;
	add.s32 	%r143, %r591, %r142;
	mad.lo.s32 	%r144, %r59, 5, %r591;
	mad.lo.s32 	%r145, %r59, 6, %r591;
	add.s32 	%r146, %r138, 192;
	mul.wide.s32 	%rd49, %r146, 4;
	shl.b64 	%rd50, %rd5, 1;
	add.s64 	%rd16, %rd49, %rd50;
	mul.wide.s32 	%rd51, %r140, 4;
	add.s64 	%rd17, %rd51, %rd50;
	mul.wide.s32 	%rd52, %r141, 4;
	add.s64 	%rd18, %rd52, %rd50;
	mul.wide.s32 	%rd53, %r143, 4;
	add.s64 	%rd19, %rd53, %rd50;
	mul.wide.s32 	%rd54, %r144, 4;
	add.s64 	%rd20, %rd54, %rd50;
	mul.wide.s32 	%rd55, %r145, 4;
	add.s64 	%rd21, %rd55, %rd50;
	mul.wide.s32 	%rd56, %r591, 2;
	add.s64 	%rd57, %rd56, %rd4;
	shl.b64 	%rd58, %rd57, 1;
	add.s64 	%rd59, %rd2, %rd58;
	add.s64 	%rd81, %rd59, 1536;
	mul.wide.s32 	%rd60, %r591, 4;
	add.s64 	%rd23, %rd60, %rd50;
	mul.wide.s32 	%rd61, %r59, 4;
	add.s64 	%rd62, %rd60, %rd61;
	add.s64 	%rd24, %rd62, %rd50;

$L__BB110_8:
	ld.global.nc.u32 	%r148, [%rd81+-1536];
	add.s64 	%rd63, %rd82, %rd23;
	ld.global.nc.u32 	%r149, [%rd63];
	// begin inline asm
	{mul.f16x2 %r147,%r148,%r149;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r150,%r606,%r147;
}
	// end inline asm
	add.s64 	%rd64, %rd82, %rd24;
	ld.global.nc.u32 	%r155, [%rd64];
	// begin inline asm
	{mul.f16x2 %r153,%r148,%r155;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r156,%r605,%r153;
}
	// end inline asm
	add.s64 	%rd65, %rd82, %rd17;
	ld.global.nc.u32 	%r161, [%rd65];
	// begin inline asm
	{mul.f16x2 %r159,%r148,%r161;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r162,%r604,%r159;
}
	// end inline asm
	add.s64 	%rd66, %rd82, %rd18;
	ld.global.nc.u32 	%r167, [%rd66];
	// begin inline asm
	{mul.f16x2 %r165,%r148,%r167;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r168,%r603,%r165;
}
	// end inline asm
	add.s64 	%rd67, %rd82, %rd19;
	ld.global.nc.u32 	%r173, [%rd67];
	// begin inline asm
	{mul.f16x2 %r171,%r148,%r173;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r174,%r602,%r171;
}
	// end inline asm
	add.s64 	%rd68, %rd82, %rd20;
	ld.global.nc.u32 	%r179, [%rd68];
	// begin inline asm
	{mul.f16x2 %r177,%r148,%r179;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r180,%r601,%r177;
}
	// end inline asm
	add.s64 	%rd69, %rd82, %rd21;
	ld.global.nc.u32 	%r185, [%rd69];
	// begin inline asm
	{mul.f16x2 %r183,%r148,%r185;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r186,%r600,%r183;
}
	// end inline asm
	ld.global.nc.u32 	%r190, [%rd81+-768];
	ld.global.nc.u32 	%r191, [%rd63+768];
	// begin inline asm
	{mul.f16x2 %r189,%r190,%r191;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r192,%r150,%r189;
}
	// end inline asm
	add.s64 	%rd70, %rd82, %rd16;
	ld.global.nc.u32 	%r197, [%rd70];
	// begin inline asm
	{mul.f16x2 %r195,%r190,%r197;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r198,%r156,%r195;
}
	// end inline asm
	ld.global.nc.u32 	%r203, [%rd65+768];
	// begin inline asm
	{mul.f16x2 %r201,%r190,%r203;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r204,%r162,%r201;
}
	// end inline asm
	ld.global.nc.u32 	%r209, [%rd66+768];
	// begin inline asm
	{mul.f16x2 %r207,%r190,%r209;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r210,%r168,%r207;
}
	// end inline asm
	ld.global.nc.u32 	%r215, [%rd67+768];
	// begin inline asm
	{mul.f16x2 %r213,%r190,%r215;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r216,%r174,%r213;
}
	// end inline asm
	ld.global.nc.u32 	%r221, [%rd68+768];
	// begin inline asm
	{mul.f16x2 %r219,%r190,%r221;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r222,%r180,%r219;
}
	// end inline asm
	ld.global.nc.u32 	%r227, [%rd69+768];
	// begin inline asm
	{mul.f16x2 %r225,%r190,%r227;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r228,%r186,%r225;
}
	// end inline asm
	ld.global.nc.u32 	%r232, [%rd81];
	ld.global.nc.u32 	%r233, [%rd63+1536];
	// begin inline asm
	{mul.f16x2 %r231,%r232,%r233;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r234,%r192,%r231;
}
	// end inline asm
	ld.global.nc.u32 	%r239, [%rd70+768];
	// begin inline asm
	{mul.f16x2 %r237,%r232,%r239;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r240,%r198,%r237;
}
	// end inline asm
	ld.global.nc.u32 	%r245, [%rd65+1536];
	// begin inline asm
	{mul.f16x2 %r243,%r232,%r245;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r246,%r204,%r243;
}
	// end inline asm
	ld.global.nc.u32 	%r251, [%rd66+1536];
	// begin inline asm
	{mul.f16x2 %r249,%r232,%r251;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r252,%r210,%r249;
}
	// end inline asm
	ld.global.nc.u32 	%r257, [%rd67+1536];
	// begin inline asm
	{mul.f16x2 %r255,%r232,%r257;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r258,%r216,%r255;
}
	// end inline asm
	ld.global.nc.u32 	%r263, [%rd68+1536];
	// begin inline asm
	{mul.f16x2 %r261,%r232,%r263;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r264,%r222,%r261;
}
	// end inline asm
	ld.global.nc.u32 	%r269, [%rd69+1536];
	// begin inline asm
	{mul.f16x2 %r267,%r232,%r269;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r270,%r228,%r267;
}
	// end inline asm
	ld.global.nc.u32 	%r274, [%rd81+768];
	ld.global.nc.u32 	%r275, [%rd63+2304];
	// begin inline asm
	{mul.f16x2 %r273,%r274,%r275;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r606,%r234,%r273;
}
	// end inline asm
	ld.global.nc.u32 	%r281, [%rd70+1536];
	// begin inline asm
	{mul.f16x2 %r279,%r274,%r281;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r605,%r240,%r279;
}
	// end inline asm
	ld.global.nc.u32 	%r287, [%rd65+2304];
	// begin inline asm
	{mul.f16x2 %r285,%r274,%r287;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r604,%r246,%r285;
}
	// end inline asm
	ld.global.nc.u32 	%r293, [%rd66+2304];
	// begin inline asm
	{mul.f16x2 %r291,%r274,%r293;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r603,%r252,%r291;
}
	// end inline asm
	ld.global.nc.u32 	%r299, [%rd67+2304];
	// begin inline asm
	{mul.f16x2 %r297,%r274,%r299;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r602,%r258,%r297;
}
	// end inline asm
	ld.global.nc.u32 	%r305, [%rd68+2304];
	// begin inline asm
	{mul.f16x2 %r303,%r274,%r305;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r601,%r264,%r303;
}
	// end inline asm
	ld.global.nc.u32 	%r311, [%rd69+2304];
	// begin inline asm
	{mul.f16x2 %r309,%r274,%r311;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r600,%r270,%r309;
}
	// end inline asm
	add.s64 	%rd82, %rd82, 3072;
	add.s64 	%rd81, %rd81, 3072;
	add.s32 	%r591, %r591, 768;
	setp.lt.s32 	%p6, %r591, %r58;
	@%p6 bra 	$L__BB110_8;

$L__BB110_9:
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r606;
  cvt.f32.f16 %f9, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r606;
  cvt.f32.f16 %f10, high;}

	// end inline asm
	add.f32 	%f23, %f9, %f10;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r605;
  cvt.f32.f16 %f11, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r605;
  cvt.f32.f16 %f12, high;}

	// end inline asm
	add.f32 	%f1, %f11, %f12;
	st.local.f32 	[%rd3+4], %f1;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r604;
  cvt.f32.f16 %f13, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r604;
  cvt.f32.f16 %f14, high;}

	// end inline asm
	add.f32 	%f2, %f13, %f14;
	st.local.f32 	[%rd3+8], %f2;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r603;
  cvt.f32.f16 %f15, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r603;
  cvt.f32.f16 %f16, high;}

	// end inline asm
	add.f32 	%f3, %f15, %f16;
	st.local.f32 	[%rd3+12], %f3;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r602;
  cvt.f32.f16 %f17, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r602;
  cvt.f32.f16 %f18, high;}

	// end inline asm
	add.f32 	%f4, %f17, %f18;
	st.local.f32 	[%rd3+16], %f4;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r601;
  cvt.f32.f16 %f19, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r601;
  cvt.f32.f16 %f20, high;}

	// end inline asm
	add.f32 	%f5, %f19, %f20;
	st.local.f32 	[%rd3+20], %f5;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r600;
  cvt.f32.f16 %f21, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r600;
  cvt.f32.f16 %f22, high;}

	// end inline asm
	add.f32 	%f6, %f21, %f22;
	st.local.f32 	[%rd3+24], %f6;
	shr.s32 	%r329, %r3, 31;
	shr.u32 	%r330, %r329, 27;
	add.s32 	%r331, %r3, %r330;
	shr.s32 	%r332, %r331, 5;
	shl.b32 	%r333, %r332, 2;
	add.s32 	%r57, %r71, %r333;
	mov.u32 	%r335, 2;
	mov.b32 	%r336, %f23;
	mov.u32 	%r337, 31;
	mov.u32 	%r338, 16;
	mov.u32 	%r339, -1;
	shfl.sync.bfly.b32 	%r340|%p7, %r336, %r338, %r337, %r339;
	mov.b32 	%f24, %r340;
	add.f32 	%f25, %f23, %f24;
	mov.b32 	%r341, %f25;
	mov.u32 	%r342, 8;
	shfl.sync.bfly.b32 	%r343|%p8, %r341, %r342, %r337, %r339;
	mov.b32 	%f26, %r343;
	add.f32 	%f27, %f25, %f26;
	mov.b32 	%r344, %f27;
	mov.u32 	%r345, 4;
	shfl.sync.bfly.b32 	%r346|%p9, %r344, %r345, %r337, %r339;
	mov.b32 	%f28, %r346;
	add.f32 	%f29, %f27, %f28;
	mov.b32 	%r347, %f29;
	shfl.sync.bfly.b32 	%r348|%p10, %r347, %r335, %r337, %r339;
	mov.b32 	%f30, %r348;
	add.f32 	%f31, %f29, %f30;
	mov.b32 	%r349, %f31;
	mov.u32 	%r350, 1;
	shfl.sync.bfly.b32 	%r351|%p11, %r349, %r350, %r337, %r339;
	mov.b32 	%f32, %r351;
	add.f32 	%f33, %f31, %f32;
	st.local.f32 	[%rd3], %f33;
	st.shared.f32 	[%r57], %f33;
	bar.sync 	0;
	@%p1 bra 	$L__BB110_11;

	ld.shared.f32 	%f34, [%r4];
	mov.b32 	%r352, %f34;
	shfl.sync.bfly.b32 	%r356|%p13, %r352, %r338, %r337, %r339;
	mov.b32 	%f35, %r356;
	add.f32 	%f36, %f34, %f35;
	mov.b32 	%r357, %f36;
	shfl.sync.bfly.b32 	%r359|%p14, %r357, %r342, %r337, %r339;
	mov.b32 	%f37, %r359;
	add.f32 	%f38, %f36, %f37;
	mov.b32 	%r360, %f38;
	shfl.sync.bfly.b32 	%r362|%p15, %r360, %r345, %r337, %r339;
	mov.b32 	%f39, %r362;
	add.f32 	%f40, %f38, %f39;
	mov.b32 	%r363, %f40;
	shfl.sync.bfly.b32 	%r365|%p16, %r363, %r335, %r337, %r339;
	mov.b32 	%f41, %r365;
	add.f32 	%f42, %f40, %f41;
	mov.b32 	%r366, %f42;
	shfl.sync.bfly.b32 	%r368|%p17, %r366, %r350, %r337, %r339;
	mov.b32 	%f43, %r368;
	add.f32 	%f44, %f42, %f43;
	st.local.f32 	[%rd3], %f44;

$L__BB110_11:
	bar.sync 	0;
	mov.b32 	%r369, %f1;
	shfl.sync.bfly.b32 	%r373|%p19, %r369, %r338, %r337, %r339;
	mov.b32 	%f45, %r373;
	add.f32 	%f46, %f1, %f45;
	mov.b32 	%r374, %f46;
	shfl.sync.bfly.b32 	%r376|%p20, %r374, %r342, %r337, %r339;
	mov.b32 	%f47, %r376;
	add.f32 	%f48, %f46, %f47;
	mov.b32 	%r377, %f48;
	shfl.sync.bfly.b32 	%r379|%p21, %r377, %r345, %r337, %r339;
	mov.b32 	%f49, %r379;
	add.f32 	%f50, %f48, %f49;
	mov.b32 	%r380, %f50;
	shfl.sync.bfly.b32 	%r382|%p22, %r380, %r335, %r337, %r339;
	mov.b32 	%f51, %r382;
	add.f32 	%f52, %f50, %f51;
	mov.b32 	%r383, %f52;
	shfl.sync.bfly.b32 	%r385|%p23, %r383, %r350, %r337, %r339;
	mov.b32 	%f53, %r385;
	add.f32 	%f54, %f52, %f53;
	st.local.f32 	[%rd3+4], %f54;
	st.shared.f32 	[%r57], %f54;
	bar.sync 	0;
	@%p1 bra 	$L__BB110_13;

	ld.shared.f32 	%f55, [%r4];
	mov.b32 	%r386, %f55;
	mov.u32 	%r387, 31;
	mov.u32 	%r388, 16;
	mov.u32 	%r389, -1;
	shfl.sync.bfly.b32 	%r390|%p24, %r386, %r388, %r387, %r389;
	mov.b32 	%f56, %r390;
	add.f32 	%f57, %f55, %f56;
	mov.b32 	%r391, %f57;
	mov.u32 	%r392, 8;
	shfl.sync.bfly.b32 	%r393|%p25, %r391, %r392, %r387, %r389;
	mov.b32 	%f58, %r393;
	add.f32 	%f59, %f57, %f58;
	mov.b32 	%r394, %f59;
	mov.u32 	%r395, 4;
	shfl.sync.bfly.b32 	%r396|%p26, %r394, %r395, %r387, %r389;
	mov.b32 	%f60, %r396;
	add.f32 	%f61, %f59, %f60;
	mov.b32 	%r397, %f61;
	mov.u32 	%r398, 2;
	shfl.sync.bfly.b32 	%r399|%p27, %r397, %r398, %r387, %r389;
	mov.b32 	%f62, %r399;
	add.f32 	%f63, %f61, %f62;
	mov.b32 	%r400, %f63;
	mov.u32 	%r401, 1;
	shfl.sync.bfly.b32 	%r402|%p28, %r400, %r401, %r387, %r389;
	mov.b32 	%f64, %r402;
	add.f32 	%f65, %f63, %f64;
	st.local.f32 	[%rd3+4], %f65;

$L__BB110_13:
	bar.sync 	0;
	mov.b32 	%r403, %f2;
	mov.u32 	%r404, 31;
	mov.u32 	%r405, 16;
	mov.u32 	%r406, -1;
	shfl.sync.bfly.b32 	%r407|%p30, %r403, %r405, %r404, %r406;
	mov.b32 	%f66, %r407;
	add.f32 	%f67, %f2, %f66;
	mov.b32 	%r408, %f67;
	mov.u32 	%r409, 8;
	shfl.sync.bfly.b32 	%r410|%p31, %r408, %r409, %r404, %r406;
	mov.b32 	%f68, %r410;
	add.f32 	%f69, %f67, %f68;
	mov.b32 	%r411, %f69;
	mov.u32 	%r412, 4;
	shfl.sync.bfly.b32 	%r413|%p32, %r411, %r412, %r404, %r406;
	mov.b32 	%f70, %r413;
	add.f32 	%f71, %f69, %f70;
	mov.b32 	%r414, %f71;
	mov.u32 	%r415, 2;
	shfl.sync.bfly.b32 	%r416|%p33, %r414, %r415, %r404, %r406;
	mov.b32 	%f72, %r416;
	add.f32 	%f73, %f71, %f72;
	mov.b32 	%r417, %f73;
	mov.u32 	%r418, 1;
	shfl.sync.bfly.b32 	%r419|%p34, %r417, %r418, %r404, %r406;
	mov.b32 	%f74, %r419;
	add.f32 	%f75, %f73, %f74;
	st.local.f32 	[%rd3+8], %f75;
	st.shared.f32 	[%r57], %f75;
	bar.sync 	0;
	@%p1 bra 	$L__BB110_15;

	ld.shared.f32 	%f76, [%r4];
	mov.b32 	%r420, %f76;
	shfl.sync.bfly.b32 	%r424|%p35, %r420, %r405, %r404, %r406;
	mov.b32 	%f77, %r424;
	add.f32 	%f78, %f76, %f77;
	mov.b32 	%r425, %f78;
	shfl.sync.bfly.b32 	%r427|%p36, %r425, %r409, %r404, %r406;
	mov.b32 	%f79, %r427;
	add.f32 	%f80, %f78, %f79;
	mov.b32 	%r428, %f80;
	shfl.sync.bfly.b32 	%r430|%p37, %r428, %r412, %r404, %r406;
	mov.b32 	%f81, %r430;
	add.f32 	%f82, %f80, %f81;
	mov.b32 	%r431, %f82;
	shfl.sync.bfly.b32 	%r433|%p38, %r431, %r415, %r404, %r406;
	mov.b32 	%f83, %r433;
	add.f32 	%f84, %f82, %f83;
	mov.b32 	%r434, %f84;
	shfl.sync.bfly.b32 	%r436|%p39, %r434, %r418, %r404, %r406;
	mov.b32 	%f85, %r436;
	add.f32 	%f86, %f84, %f85;
	st.local.f32 	[%rd3+8], %f86;

$L__BB110_15:
	bar.sync 	0;
	mov.b32 	%r437, %f3;
	shfl.sync.bfly.b32 	%r441|%p41, %r437, %r405, %r404, %r406;
	mov.b32 	%f87, %r441;
	add.f32 	%f88, %f3, %f87;
	mov.b32 	%r442, %f88;
	shfl.sync.bfly.b32 	%r444|%p42, %r442, %r409, %r404, %r406;
	mov.b32 	%f89, %r444;
	add.f32 	%f90, %f88, %f89;
	mov.b32 	%r445, %f90;
	shfl.sync.bfly.b32 	%r447|%p43, %r445, %r412, %r404, %r406;
	mov.b32 	%f91, %r447;
	add.f32 	%f92, %f90, %f91;
	mov.b32 	%r448, %f92;
	shfl.sync.bfly.b32 	%r450|%p44, %r448, %r415, %r404, %r406;
	mov.b32 	%f93, %r450;
	add.f32 	%f94, %f92, %f93;
	mov.b32 	%r451, %f94;
	shfl.sync.bfly.b32 	%r453|%p45, %r451, %r418, %r404, %r406;
	mov.b32 	%f95, %r453;
	add.f32 	%f96, %f94, %f95;
	st.local.f32 	[%rd3+12], %f96;
	st.shared.f32 	[%r57], %f96;
	bar.sync 	0;
	@%p1 bra 	$L__BB110_17;

	ld.shared.f32 	%f97, [%r4];
	mov.b32 	%r454, %f97;
	mov.u32 	%r455, 31;
	mov.u32 	%r456, 16;
	mov.u32 	%r457, -1;
	shfl.sync.bfly.b32 	%r458|%p46, %r454, %r456, %r455, %r457;
	mov.b32 	%f98, %r458;
	add.f32 	%f99, %f97, %f98;
	mov.b32 	%r459, %f99;
	mov.u32 	%r460, 8;
	shfl.sync.bfly.b32 	%r461|%p47, %r459, %r460, %r455, %r457;
	mov.b32 	%f100, %r461;
	add.f32 	%f101, %f99, %f100;
	mov.b32 	%r462, %f101;
	mov.u32 	%r463, 4;
	shfl.sync.bfly.b32 	%r464|%p48, %r462, %r463, %r455, %r457;
	mov.b32 	%f102, %r464;
	add.f32 	%f103, %f101, %f102;
	mov.b32 	%r465, %f103;
	mov.u32 	%r466, 2;
	shfl.sync.bfly.b32 	%r467|%p49, %r465, %r466, %r455, %r457;
	mov.b32 	%f104, %r467;
	add.f32 	%f105, %f103, %f104;
	mov.b32 	%r468, %f105;
	mov.u32 	%r469, 1;
	shfl.sync.bfly.b32 	%r470|%p50, %r468, %r469, %r455, %r457;
	mov.b32 	%f106, %r470;
	add.f32 	%f107, %f105, %f106;
	st.local.f32 	[%rd3+12], %f107;

$L__BB110_17:
	bar.sync 	0;
	mov.b32 	%r471, %f4;
	mov.u32 	%r472, 31;
	mov.u32 	%r473, 16;
	mov.u32 	%r474, -1;
	shfl.sync.bfly.b32 	%r475|%p52, %r471, %r473, %r472, %r474;
	mov.b32 	%f108, %r475;
	add.f32 	%f109, %f4, %f108;
	mov.b32 	%r476, %f109;
	mov.u32 	%r477, 8;
	shfl.sync.bfly.b32 	%r478|%p53, %r476, %r477, %r472, %r474;
	mov.b32 	%f110, %r478;
	add.f32 	%f111, %f109, %f110;
	mov.b32 	%r479, %f111;
	mov.u32 	%r480, 4;
	shfl.sync.bfly.b32 	%r481|%p54, %r479, %r480, %r472, %r474;
	mov.b32 	%f112, %r481;
	add.f32 	%f113, %f111, %f112;
	mov.b32 	%r482, %f113;
	mov.u32 	%r483, 2;
	shfl.sync.bfly.b32 	%r484|%p55, %r482, %r483, %r472, %r474;
	mov.b32 	%f114, %r484;
	add.f32 	%f115, %f113, %f114;
	mov.b32 	%r485, %f115;
	mov.u32 	%r486, 1;
	shfl.sync.bfly.b32 	%r487|%p56, %r485, %r486, %r472, %r474;
	mov.b32 	%f116, %r487;
	add.f32 	%f117, %f115, %f116;
	st.local.f32 	[%rd3+16], %f117;
	st.shared.f32 	[%r57], %f117;
	bar.sync 	0;
	@%p1 bra 	$L__BB110_19;

	ld.shared.f32 	%f118, [%r4];
	mov.b32 	%r488, %f118;
	shfl.sync.bfly.b32 	%r492|%p57, %r488, %r473, %r472, %r474;
	mov.b32 	%f119, %r492;
	add.f32 	%f120, %f118, %f119;
	mov.b32 	%r493, %f120;
	shfl.sync.bfly.b32 	%r495|%p58, %r493, %r477, %r472, %r474;
	mov.b32 	%f121, %r495;
	add.f32 	%f122, %f120, %f121;
	mov.b32 	%r496, %f122;
	shfl.sync.bfly.b32 	%r498|%p59, %r496, %r480, %r472, %r474;
	mov.b32 	%f123, %r498;
	add.f32 	%f124, %f122, %f123;
	mov.b32 	%r499, %f124;
	shfl.sync.bfly.b32 	%r501|%p60, %r499, %r483, %r472, %r474;
	mov.b32 	%f125, %r501;
	add.f32 	%f126, %f124, %f125;
	mov.b32 	%r502, %f126;
	shfl.sync.bfly.b32 	%r504|%p61, %r502, %r486, %r472, %r474;
	mov.b32 	%f127, %r504;
	add.f32 	%f128, %f126, %f127;
	st.local.f32 	[%rd3+16], %f128;

$L__BB110_19:
	bar.sync 	0;
	mov.b32 	%r505, %f5;
	shfl.sync.bfly.b32 	%r509|%p63, %r505, %r473, %r472, %r474;
	mov.b32 	%f129, %r509;
	add.f32 	%f130, %f5, %f129;
	mov.b32 	%r510, %f130;
	shfl.sync.bfly.b32 	%r512|%p64, %r510, %r477, %r472, %r474;
	mov.b32 	%f131, %r512;
	add.f32 	%f132, %f130, %f131;
	mov.b32 	%r513, %f132;
	shfl.sync.bfly.b32 	%r515|%p65, %r513, %r480, %r472, %r474;
	mov.b32 	%f133, %r515;
	add.f32 	%f134, %f132, %f133;
	mov.b32 	%r516, %f134;
	shfl.sync.bfly.b32 	%r518|%p66, %r516, %r483, %r472, %r474;
	mov.b32 	%f135, %r518;
	add.f32 	%f136, %f134, %f135;
	mov.b32 	%r519, %f136;
	shfl.sync.bfly.b32 	%r521|%p67, %r519, %r486, %r472, %r474;
	mov.b32 	%f137, %r521;
	add.f32 	%f138, %f136, %f137;
	st.local.f32 	[%rd3+20], %f138;
	st.shared.f32 	[%r57], %f138;
	bar.sync 	0;
	@%p1 bra 	$L__BB110_21;

	ld.shared.f32 	%f139, [%r4];
	mov.b32 	%r522, %f139;
	mov.u32 	%r523, 31;
	mov.u32 	%r524, 16;
	mov.u32 	%r525, -1;
	shfl.sync.bfly.b32 	%r526|%p68, %r522, %r524, %r523, %r525;
	mov.b32 	%f140, %r526;
	add.f32 	%f141, %f139, %f140;
	mov.b32 	%r527, %f141;
	mov.u32 	%r528, 8;
	shfl.sync.bfly.b32 	%r529|%p69, %r527, %r528, %r523, %r525;
	mov.b32 	%f142, %r529;
	add.f32 	%f143, %f141, %f142;
	mov.b32 	%r530, %f143;
	mov.u32 	%r531, 4;
	shfl.sync.bfly.b32 	%r532|%p70, %r530, %r531, %r523, %r525;
	mov.b32 	%f144, %r532;
	add.f32 	%f145, %f143, %f144;
	mov.b32 	%r533, %f145;
	mov.u32 	%r534, 2;
	shfl.sync.bfly.b32 	%r535|%p71, %r533, %r534, %r523, %r525;
	mov.b32 	%f146, %r535;
	add.f32 	%f147, %f145, %f146;
	mov.b32 	%r536, %f147;
	mov.u32 	%r537, 1;
	shfl.sync.bfly.b32 	%r538|%p72, %r536, %r537, %r523, %r525;
	mov.b32 	%f148, %r538;
	add.f32 	%f149, %f147, %f148;
	st.local.f32 	[%rd3+20], %f149;

$L__BB110_21:
	bar.sync 	0;
	mov.b32 	%r539, %f6;
	mov.u32 	%r540, 31;
	mov.u32 	%r541, 16;
	mov.u32 	%r542, -1;
	shfl.sync.bfly.b32 	%r543|%p74, %r539, %r541, %r540, %r542;
	mov.b32 	%f150, %r543;
	add.f32 	%f151, %f6, %f150;
	mov.b32 	%r544, %f151;
	mov.u32 	%r545, 8;
	shfl.sync.bfly.b32 	%r546|%p75, %r544, %r545, %r540, %r542;
	mov.b32 	%f152, %r546;
	add.f32 	%f153, %f151, %f152;
	mov.b32 	%r547, %f153;
	mov.u32 	%r548, 4;
	shfl.sync.bfly.b32 	%r549|%p76, %r547, %r548, %r540, %r542;
	mov.b32 	%f154, %r549;
	add.f32 	%f155, %f153, %f154;
	mov.b32 	%r550, %f155;
	mov.u32 	%r551, 2;
	shfl.sync.bfly.b32 	%r552|%p77, %r550, %r551, %r540, %r542;
	mov.b32 	%f156, %r552;
	add.f32 	%f157, %f155, %f156;
	mov.b32 	%r553, %f157;
	mov.u32 	%r554, 1;
	shfl.sync.bfly.b32 	%r555|%p78, %r553, %r554, %r540, %r542;
	mov.b32 	%f158, %r555;
	add.f32 	%f159, %f157, %f158;
	st.local.f32 	[%rd3+24], %f159;
	st.shared.f32 	[%r57], %f159;
	bar.sync 	0;
	@%p1 bra 	$L__BB110_23;

	ld.shared.f32 	%f160, [%r4];
	mov.b32 	%r556, %f160;
	shfl.sync.bfly.b32 	%r560|%p79, %r556, %r541, %r540, %r542;
	mov.b32 	%f161, %r560;
	add.f32 	%f162, %f160, %f161;
	mov.b32 	%r561, %f162;
	shfl.sync.bfly.b32 	%r563|%p80, %r561, %r545, %r540, %r542;
	mov.b32 	%f163, %r563;
	add.f32 	%f164, %f162, %f163;
	mov.b32 	%r564, %f164;
	shfl.sync.bfly.b32 	%r566|%p81, %r564, %r548, %r540, %r542;
	mov.b32 	%f165, %r566;
	add.f32 	%f166, %f164, %f165;
	mov.b32 	%r567, %f166;
	shfl.sync.bfly.b32 	%r569|%p82, %r567, %r551, %r540, %r542;
	mov.b32 	%f167, %r569;
	add.f32 	%f168, %f166, %f167;
	mov.b32 	%r570, %f168;
	shfl.sync.bfly.b32 	%r572|%p83, %r570, %r554, %r540, %r542;
	mov.b32 	%f169, %r572;
	add.f32 	%f170, %f168, %f169;
	st.local.f32 	[%rd3+24], %f170;

$L__BB110_23:
	bar.sync 	0;
	setp.gt.s32 	%p84, %r3, 6;
	@%p84 bra 	$L__BB110_25;

	mad.lo.s32 	%r573, %r3, %r60, %r2;
	cvt.s64.s32 	%rd71, %r573;
	mul.lo.s32 	%r574, %r1, %r61;
	cvt.s64.s32 	%rd72, %r574;
	add.s64 	%rd73, %rd72, %rd71;
	mul.wide.s32 	%rd74, %r3, 4;
	add.s64 	%rd75, %rd3, %rd74;
	ld.local.f32 	%f171, [%rd75];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs3, %f171;}

	// end inline asm
	cvta.to.global.u64 	%rd76, %rd29;
	shl.b64 	%rd77, %rd73, 1;
	add.s64 	%rd78, %rd76, %rd77;
	st.global.u16 	[%rd78], %rs3;

$L__BB110_25:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_8_bs_192
.visible .entry ggml_matvec_f16_ncols_8_bs_192(
	.param .u64 ggml_matvec_f16_ncols_8_bs_192_param_0,
	.param .u64 ggml_matvec_f16_ncols_8_bs_192_param_1,
	.param .u64 ggml_matvec_f16_ncols_8_bs_192_param_2,
	.param .u32 ggml_matvec_f16_ncols_8_bs_192_param_3,
	.param .u32 ggml_matvec_f16_ncols_8_bs_192_param_4,
	.param .u32 ggml_matvec_f16_ncols_8_bs_192_param_5,
	.param .u32 ggml_matvec_f16_ncols_8_bs_192_param_6,
	.param .u32 ggml_matvec_f16_ncols_8_bs_192_param_7,
	.param .u32 ggml_matvec_f16_ncols_8_bs_192_param_8,
	.param .u32 ggml_matvec_f16_ncols_8_bs_192_param_9,
	.param .u32 ggml_matvec_f16_ncols_8_bs_192_param_10,
	.param .u32 ggml_matvec_f16_ncols_8_bs_192_param_11
)
{
	.local .align 16 .b8 	__local_depot111[32];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<96>;
	.reg .b16 	%rs<4>;
	.reg .f32 	%f<196>;
	.reg .b32 	%r<687>;
	.reg .b64 	%rd<87>;


	mov.u64 	%SPL, __local_depot111;
	ld.param.u64 	%rd31, [ggml_matvec_f16_ncols_8_bs_192_param_0];
	ld.param.u64 	%rd32, [ggml_matvec_f16_ncols_8_bs_192_param_1];
	ld.param.u64 	%rd30, [ggml_matvec_f16_ncols_8_bs_192_param_2];
	ld.param.u32 	%r64, [ggml_matvec_f16_ncols_8_bs_192_param_3];
	ld.param.u32 	%r68, [ggml_matvec_f16_ncols_8_bs_192_param_5];
	ld.param.u32 	%r65, [ggml_matvec_f16_ncols_8_bs_192_param_6];
	ld.param.u32 	%r66, [ggml_matvec_f16_ncols_8_bs_192_param_7];
	ld.param.u32 	%r69, [ggml_matvec_f16_ncols_8_bs_192_param_8];
	ld.param.u32 	%r70, [ggml_matvec_f16_ncols_8_bs_192_param_9];
	ld.param.u32 	%r71, [ggml_matvec_f16_ncols_8_bs_192_param_10];
	ld.param.u32 	%r67, [ggml_matvec_f16_ncols_8_bs_192_param_11];
	cvta.to.global.u64 	%rd86, %rd32;
	cvta.to.global.u64 	%rd2, %rd31;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r72, %r1, %r69;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r73, %r2, %r68;
	mad.lo.s32 	%r74, %r72, %r70, %r73;
	cvt.s64.s32 	%rd4, %r74;
	mul.lo.s32 	%r75, %r1, %r71;
	cvt.s64.s32 	%rd5, %r75;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r76, %r3, 2;
	mov.u32 	%r77, data_mmv;
	add.s32 	%r4, %r77, %r76;
	@%p1 bra 	$L__BB111_2;

	mov.u32 	%r78, 0;
	st.shared.u32 	[%r4], %r78;

$L__BB111_2:
	bar.sync 	0;
	mov.f32 	%f9, 0f00000000;
	st.local.v4.f32 	[%rd3], {%f9, %f9, %f9, %f9};
	st.local.v4.f32 	[%rd3+16], {%f9, %f9, %f9, %f9};
	mov.u32 	%r679, 0;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs2, %f9;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f9;}

	// end inline asm
	mov.b32 	%r686, {%rs1, %rs2};
	setp.ge.s32 	%p2, %r3, %r64;
	mov.u32 	%r680, %r679;
	mov.u32 	%r681, %r679;
	mov.u32 	%r682, %r679;
	mov.u32 	%r683, %r679;
	mov.u32 	%r684, %r679;
	mov.u32 	%r685, %r679;
	@%p2 bra 	$L__BB111_9;

	not.b32 	%r93, %r3;
	add.s32 	%r6, %r93, %r64;
	mul.wide.u32 	%rd34, %r6, -1431655765;
	shr.u64 	%rd35, %rd34, 39;
	cvt.u32.u64 	%r94, %rd35;
	add.s32 	%r95, %r94, 1;
	and.b32  	%r660, %r95, 3;
	setp.eq.s32 	%p3, %r660, 0;
	mov.u32 	%r679, 0;
	mov.u32 	%r669, %r3;
	@%p3 bra 	$L__BB111_6;

	shl.b32 	%r103, %r65, 1;
	add.s32 	%r104, %r3, %r103;
	mul.wide.s32 	%rd36, %r104, 4;
	shl.b64 	%rd37, %rd5, 1;
	add.s64 	%rd7, %rd36, %rd37;
	mul.wide.s32 	%rd38, %r3, 4;
	mul.wide.s32 	%rd8, %r65, 4;
	add.s64 	%rd39, %rd38, %rd8;
	add.s64 	%rd9, %rd39, %rd37;
	add.s64 	%rd10, %rd38, %rd37;
	mul.wide.s32 	%rd40, %r3, 2;
	add.s64 	%rd41, %rd40, %rd4;
	shl.b64 	%rd42, %rd41, 1;
	add.s64 	%rd83, %rd2, %rd42;
	mov.u32 	%r679, 0;
	mov.u64 	%rd84, %rd86;
	mov.u32 	%r680, %r679;
	mov.u32 	%r681, %r679;
	mov.u32 	%r682, %r679;
	mov.u32 	%r683, %r679;
	mov.u32 	%r684, %r679;
	mov.u32 	%r685, %r679;
	mov.u32 	%r669, %r3;

$L__BB111_5:
	.pragma "nounroll";
	ld.global.nc.u32 	%r106, [%rd83];
	add.s64 	%rd43, %rd84, %rd10;
	ld.global.nc.u32 	%r107, [%rd43];
	// begin inline asm
	{mul.f16x2 %r105,%r106,%r107;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r686,%r686,%r105;
}
	// end inline asm
	add.s64 	%rd44, %rd84, %rd9;
	ld.global.nc.u32 	%r113, [%rd44];
	// begin inline asm
	{mul.f16x2 %r111,%r106,%r113;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r685,%r685,%r111;
}
	// end inline asm
	add.s64 	%rd45, %rd84, %rd7;
	ld.global.nc.u32 	%r119, [%rd45];
	// begin inline asm
	{mul.f16x2 %r117,%r106,%r119;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r684,%r684,%r117;
}
	// end inline asm
	add.s64 	%rd46, %rd45, %rd8;
	ld.global.nc.u32 	%r125, [%rd46];
	// begin inline asm
	{mul.f16x2 %r123,%r106,%r125;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r683,%r683,%r123;
}
	// end inline asm
	add.s64 	%rd47, %rd46, %rd8;
	ld.global.nc.u32 	%r131, [%rd47];
	// begin inline asm
	{mul.f16x2 %r129,%r106,%r131;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r682,%r682,%r129;
}
	// end inline asm
	add.s64 	%rd48, %rd47, %rd8;
	ld.global.nc.u32 	%r137, [%rd48];
	// begin inline asm
	{mul.f16x2 %r135,%r106,%r137;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r681,%r681,%r135;
}
	// end inline asm
	add.s64 	%rd49, %rd48, %rd8;
	ld.global.nc.u32 	%r143, [%rd49];
	// begin inline asm
	{mul.f16x2 %r141,%r106,%r143;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r680,%r680,%r141;
}
	// end inline asm
	add.s64 	%rd50, %rd49, %rd8;
	ld.global.nc.u32 	%r149, [%rd50];
	// begin inline asm
	{mul.f16x2 %r147,%r106,%r149;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r679,%r679,%r147;
}
	// end inline asm
	add.s32 	%r669, %r669, 192;
	add.s64 	%rd84, %rd84, 768;
	add.s64 	%rd83, %rd83, 768;
	add.s32 	%r660, %r660, -1;
	setp.ne.s32 	%p4, %r660, 0;
	@%p4 bra 	$L__BB111_5;

$L__BB111_6:
	setp.lt.u32 	%p5, %r6, 576;
	@%p5 bra 	$L__BB111_9;

	add.s32 	%r153, %r669, %r65;
	shl.b32 	%r154, %r65, 1;
	add.s32 	%r155, %r669, %r154;
	mad.lo.s32 	%r156, %r65, 3, %r669;
	shl.b32 	%r157, %r65, 2;
	add.s32 	%r158, %r669, %r157;
	mad.lo.s32 	%r159, %r65, 5, %r669;
	mad.lo.s32 	%r160, %r65, 6, %r669;
	mad.lo.s32 	%r161, %r65, 7, %r669;
	add.s32 	%r162, %r153, 192;
	mul.wide.s32 	%rd51, %r162, 4;
	shl.b64 	%rd52, %rd5, 1;
	add.s64 	%rd16, %rd51, %rd52;
	mul.wide.s32 	%rd53, %r155, 4;
	add.s64 	%rd17, %rd53, %rd52;
	mul.wide.s32 	%rd54, %r156, 4;
	add.s64 	%rd18, %rd54, %rd52;
	mul.wide.s32 	%rd55, %r158, 4;
	add.s64 	%rd19, %rd55, %rd52;
	mul.wide.s32 	%rd56, %r159, 4;
	add.s64 	%rd20, %rd56, %rd52;
	mul.wide.s32 	%rd57, %r160, 4;
	add.s64 	%rd21, %rd57, %rd52;
	mul.wide.s32 	%rd58, %r161, 4;
	add.s64 	%rd22, %rd58, %rd52;
	mul.wide.s32 	%rd59, %r669, 2;
	add.s64 	%rd60, %rd59, %rd4;
	shl.b64 	%rd61, %rd60, 1;
	add.s64 	%rd62, %rd2, %rd61;
	add.s64 	%rd85, %rd62, 1536;
	mul.wide.s32 	%rd63, %r669, 4;
	add.s64 	%rd24, %rd63, %rd52;
	mul.wide.s32 	%rd64, %r65, 4;
	add.s64 	%rd65, %rd63, %rd64;
	add.s64 	%rd25, %rd65, %rd52;

$L__BB111_8:
	ld.global.nc.u32 	%r164, [%rd85+-1536];
	add.s64 	%rd66, %rd86, %rd24;
	ld.global.nc.u32 	%r165, [%rd66];
	// begin inline asm
	{mul.f16x2 %r163,%r164,%r165;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r166,%r686,%r163;
}
	// end inline asm
	add.s64 	%rd67, %rd86, %rd25;
	ld.global.nc.u32 	%r171, [%rd67];
	// begin inline asm
	{mul.f16x2 %r169,%r164,%r171;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r172,%r685,%r169;
}
	// end inline asm
	add.s64 	%rd68, %rd86, %rd17;
	ld.global.nc.u32 	%r177, [%rd68];
	// begin inline asm
	{mul.f16x2 %r175,%r164,%r177;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r178,%r684,%r175;
}
	// end inline asm
	add.s64 	%rd69, %rd86, %rd18;
	ld.global.nc.u32 	%r183, [%rd69];
	// begin inline asm
	{mul.f16x2 %r181,%r164,%r183;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r184,%r683,%r181;
}
	// end inline asm
	add.s64 	%rd70, %rd86, %rd19;
	ld.global.nc.u32 	%r189, [%rd70];
	// begin inline asm
	{mul.f16x2 %r187,%r164,%r189;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r190,%r682,%r187;
}
	// end inline asm
	add.s64 	%rd71, %rd86, %rd20;
	ld.global.nc.u32 	%r195, [%rd71];
	// begin inline asm
	{mul.f16x2 %r193,%r164,%r195;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r196,%r681,%r193;
}
	// end inline asm
	add.s64 	%rd72, %rd86, %rd21;
	ld.global.nc.u32 	%r201, [%rd72];
	// begin inline asm
	{mul.f16x2 %r199,%r164,%r201;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r202,%r680,%r199;
}
	// end inline asm
	add.s64 	%rd73, %rd86, %rd22;
	ld.global.nc.u32 	%r207, [%rd73];
	// begin inline asm
	{mul.f16x2 %r205,%r164,%r207;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r208,%r679,%r205;
}
	// end inline asm
	ld.global.nc.u32 	%r212, [%rd85+-768];
	ld.global.nc.u32 	%r213, [%rd66+768];
	// begin inline asm
	{mul.f16x2 %r211,%r212,%r213;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r214,%r166,%r211;
}
	// end inline asm
	add.s64 	%rd74, %rd86, %rd16;
	ld.global.nc.u32 	%r219, [%rd74];
	// begin inline asm
	{mul.f16x2 %r217,%r212,%r219;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r220,%r172,%r217;
}
	// end inline asm
	ld.global.nc.u32 	%r225, [%rd68+768];
	// begin inline asm
	{mul.f16x2 %r223,%r212,%r225;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r226,%r178,%r223;
}
	// end inline asm
	ld.global.nc.u32 	%r231, [%rd69+768];
	// begin inline asm
	{mul.f16x2 %r229,%r212,%r231;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r232,%r184,%r229;
}
	// end inline asm
	ld.global.nc.u32 	%r237, [%rd70+768];
	// begin inline asm
	{mul.f16x2 %r235,%r212,%r237;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r238,%r190,%r235;
}
	// end inline asm
	ld.global.nc.u32 	%r243, [%rd71+768];
	// begin inline asm
	{mul.f16x2 %r241,%r212,%r243;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r244,%r196,%r241;
}
	// end inline asm
	ld.global.nc.u32 	%r249, [%rd72+768];
	// begin inline asm
	{mul.f16x2 %r247,%r212,%r249;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r250,%r202,%r247;
}
	// end inline asm
	ld.global.nc.u32 	%r255, [%rd73+768];
	// begin inline asm
	{mul.f16x2 %r253,%r212,%r255;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r256,%r208,%r253;
}
	// end inline asm
	ld.global.nc.u32 	%r260, [%rd85];
	ld.global.nc.u32 	%r261, [%rd66+1536];
	// begin inline asm
	{mul.f16x2 %r259,%r260,%r261;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r262,%r214,%r259;
}
	// end inline asm
	ld.global.nc.u32 	%r267, [%rd74+768];
	// begin inline asm
	{mul.f16x2 %r265,%r260,%r267;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r268,%r220,%r265;
}
	// end inline asm
	ld.global.nc.u32 	%r273, [%rd68+1536];
	// begin inline asm
	{mul.f16x2 %r271,%r260,%r273;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r274,%r226,%r271;
}
	// end inline asm
	ld.global.nc.u32 	%r279, [%rd69+1536];
	// begin inline asm
	{mul.f16x2 %r277,%r260,%r279;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r280,%r232,%r277;
}
	// end inline asm
	ld.global.nc.u32 	%r285, [%rd70+1536];
	// begin inline asm
	{mul.f16x2 %r283,%r260,%r285;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r286,%r238,%r283;
}
	// end inline asm
	ld.global.nc.u32 	%r291, [%rd71+1536];
	// begin inline asm
	{mul.f16x2 %r289,%r260,%r291;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r292,%r244,%r289;
}
	// end inline asm
	ld.global.nc.u32 	%r297, [%rd72+1536];
	// begin inline asm
	{mul.f16x2 %r295,%r260,%r297;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r298,%r250,%r295;
}
	// end inline asm
	ld.global.nc.u32 	%r303, [%rd73+1536];
	// begin inline asm
	{mul.f16x2 %r301,%r260,%r303;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r304,%r256,%r301;
}
	// end inline asm
	ld.global.nc.u32 	%r308, [%rd85+768];
	ld.global.nc.u32 	%r309, [%rd66+2304];
	// begin inline asm
	{mul.f16x2 %r307,%r308,%r309;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r686,%r262,%r307;
}
	// end inline asm
	ld.global.nc.u32 	%r315, [%rd74+1536];
	// begin inline asm
	{mul.f16x2 %r313,%r308,%r315;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r685,%r268,%r313;
}
	// end inline asm
	ld.global.nc.u32 	%r321, [%rd68+2304];
	// begin inline asm
	{mul.f16x2 %r319,%r308,%r321;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r684,%r274,%r319;
}
	// end inline asm
	ld.global.nc.u32 	%r327, [%rd69+2304];
	// begin inline asm
	{mul.f16x2 %r325,%r308,%r327;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r683,%r280,%r325;
}
	// end inline asm
	ld.global.nc.u32 	%r333, [%rd70+2304];
	// begin inline asm
	{mul.f16x2 %r331,%r308,%r333;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r682,%r286,%r331;
}
	// end inline asm
	ld.global.nc.u32 	%r339, [%rd71+2304];
	// begin inline asm
	{mul.f16x2 %r337,%r308,%r339;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r681,%r292,%r337;
}
	// end inline asm
	ld.global.nc.u32 	%r345, [%rd72+2304];
	// begin inline asm
	{mul.f16x2 %r343,%r308,%r345;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r680,%r298,%r343;
}
	// end inline asm
	ld.global.nc.u32 	%r351, [%rd73+2304];
	// begin inline asm
	{mul.f16x2 %r349,%r308,%r351;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r679,%r304,%r349;
}
	// end inline asm
	add.s64 	%rd86, %rd86, 3072;
	add.s64 	%rd85, %rd85, 3072;
	add.s32 	%r669, %r669, 768;
	setp.lt.s32 	%p6, %r669, %r64;
	@%p6 bra 	$L__BB111_8;

$L__BB111_9:
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r686;
  cvt.f32.f16 %f10, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r686;
  cvt.f32.f16 %f11, high;}

	// end inline asm
	add.f32 	%f26, %f10, %f11;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r685;
  cvt.f32.f16 %f12, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r685;
  cvt.f32.f16 %f13, high;}

	// end inline asm
	add.f32 	%f1, %f12, %f13;
	st.local.f32 	[%rd3+4], %f1;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r684;
  cvt.f32.f16 %f14, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r684;
  cvt.f32.f16 %f15, high;}

	// end inline asm
	add.f32 	%f2, %f14, %f15;
	st.local.f32 	[%rd3+8], %f2;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r683;
  cvt.f32.f16 %f16, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r683;
  cvt.f32.f16 %f17, high;}

	// end inline asm
	add.f32 	%f3, %f16, %f17;
	st.local.f32 	[%rd3+12], %f3;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r682;
  cvt.f32.f16 %f18, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r682;
  cvt.f32.f16 %f19, high;}

	// end inline asm
	add.f32 	%f4, %f18, %f19;
	st.local.f32 	[%rd3+16], %f4;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r681;
  cvt.f32.f16 %f20, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r681;
  cvt.f32.f16 %f21, high;}

	// end inline asm
	add.f32 	%f5, %f20, %f21;
	st.local.f32 	[%rd3+20], %f5;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r680;
  cvt.f32.f16 %f22, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r680;
  cvt.f32.f16 %f23, high;}

	// end inline asm
	add.f32 	%f6, %f22, %f23;
	st.local.f32 	[%rd3+24], %f6;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r679;
  cvt.f32.f16 %f24, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r679;
  cvt.f32.f16 %f25, high;}

	// end inline asm
	add.f32 	%f7, %f24, %f25;
	st.local.f32 	[%rd3+28], %f7;
	shr.s32 	%r371, %r3, 31;
	shr.u32 	%r372, %r371, 27;
	add.s32 	%r373, %r3, %r372;
	shr.s32 	%r374, %r373, 5;
	shl.b32 	%r375, %r374, 2;
	add.s32 	%r63, %r77, %r375;
	mov.u32 	%r377, 2;
	mov.b32 	%r378, %f26;
	mov.u32 	%r379, 31;
	mov.u32 	%r380, 16;
	mov.u32 	%r381, -1;
	shfl.sync.bfly.b32 	%r382|%p7, %r378, %r380, %r379, %r381;
	mov.b32 	%f27, %r382;
	add.f32 	%f28, %f26, %f27;
	mov.b32 	%r383, %f28;
	mov.u32 	%r384, 8;
	shfl.sync.bfly.b32 	%r385|%p8, %r383, %r384, %r379, %r381;
	mov.b32 	%f29, %r385;
	add.f32 	%f30, %f28, %f29;
	mov.b32 	%r386, %f30;
	mov.u32 	%r387, 4;
	shfl.sync.bfly.b32 	%r388|%p9, %r386, %r387, %r379, %r381;
	mov.b32 	%f31, %r388;
	add.f32 	%f32, %f30, %f31;
	mov.b32 	%r389, %f32;
	shfl.sync.bfly.b32 	%r390|%p10, %r389, %r377, %r379, %r381;
	mov.b32 	%f33, %r390;
	add.f32 	%f34, %f32, %f33;
	mov.b32 	%r391, %f34;
	mov.u32 	%r392, 1;
	shfl.sync.bfly.b32 	%r393|%p11, %r391, %r392, %r379, %r381;
	mov.b32 	%f35, %r393;
	add.f32 	%f36, %f34, %f35;
	st.local.f32 	[%rd3], %f36;
	st.shared.f32 	[%r63], %f36;
	bar.sync 	0;
	@%p1 bra 	$L__BB111_11;

	ld.shared.f32 	%f37, [%r4];
	mov.b32 	%r394, %f37;
	shfl.sync.bfly.b32 	%r398|%p13, %r394, %r380, %r379, %r381;
	mov.b32 	%f38, %r398;
	add.f32 	%f39, %f37, %f38;
	mov.b32 	%r399, %f39;
	shfl.sync.bfly.b32 	%r401|%p14, %r399, %r384, %r379, %r381;
	mov.b32 	%f40, %r401;
	add.f32 	%f41, %f39, %f40;
	mov.b32 	%r402, %f41;
	shfl.sync.bfly.b32 	%r404|%p15, %r402, %r387, %r379, %r381;
	mov.b32 	%f42, %r404;
	add.f32 	%f43, %f41, %f42;
	mov.b32 	%r405, %f43;
	shfl.sync.bfly.b32 	%r407|%p16, %r405, %r377, %r379, %r381;
	mov.b32 	%f44, %r407;
	add.f32 	%f45, %f43, %f44;
	mov.b32 	%r408, %f45;
	shfl.sync.bfly.b32 	%r410|%p17, %r408, %r392, %r379, %r381;
	mov.b32 	%f46, %r410;
	add.f32 	%f47, %f45, %f46;
	st.local.f32 	[%rd3], %f47;

$L__BB111_11:
	bar.sync 	0;
	mov.b32 	%r411, %f1;
	shfl.sync.bfly.b32 	%r415|%p19, %r411, %r380, %r379, %r381;
	mov.b32 	%f48, %r415;
	add.f32 	%f49, %f1, %f48;
	mov.b32 	%r416, %f49;
	shfl.sync.bfly.b32 	%r418|%p20, %r416, %r384, %r379, %r381;
	mov.b32 	%f50, %r418;
	add.f32 	%f51, %f49, %f50;
	mov.b32 	%r419, %f51;
	shfl.sync.bfly.b32 	%r421|%p21, %r419, %r387, %r379, %r381;
	mov.b32 	%f52, %r421;
	add.f32 	%f53, %f51, %f52;
	mov.b32 	%r422, %f53;
	shfl.sync.bfly.b32 	%r424|%p22, %r422, %r377, %r379, %r381;
	mov.b32 	%f54, %r424;
	add.f32 	%f55, %f53, %f54;
	mov.b32 	%r425, %f55;
	shfl.sync.bfly.b32 	%r427|%p23, %r425, %r392, %r379, %r381;
	mov.b32 	%f56, %r427;
	add.f32 	%f57, %f55, %f56;
	st.local.f32 	[%rd3+4], %f57;
	st.shared.f32 	[%r63], %f57;
	bar.sync 	0;
	@%p1 bra 	$L__BB111_13;

	ld.shared.f32 	%f58, [%r4];
	mov.b32 	%r428, %f58;
	mov.u32 	%r429, 31;
	mov.u32 	%r430, 16;
	mov.u32 	%r431, -1;
	shfl.sync.bfly.b32 	%r432|%p24, %r428, %r430, %r429, %r431;
	mov.b32 	%f59, %r432;
	add.f32 	%f60, %f58, %f59;
	mov.b32 	%r433, %f60;
	mov.u32 	%r434, 8;
	shfl.sync.bfly.b32 	%r435|%p25, %r433, %r434, %r429, %r431;
	mov.b32 	%f61, %r435;
	add.f32 	%f62, %f60, %f61;
	mov.b32 	%r436, %f62;
	mov.u32 	%r437, 4;
	shfl.sync.bfly.b32 	%r438|%p26, %r436, %r437, %r429, %r431;
	mov.b32 	%f63, %r438;
	add.f32 	%f64, %f62, %f63;
	mov.b32 	%r439, %f64;
	mov.u32 	%r440, 2;
	shfl.sync.bfly.b32 	%r441|%p27, %r439, %r440, %r429, %r431;
	mov.b32 	%f65, %r441;
	add.f32 	%f66, %f64, %f65;
	mov.b32 	%r442, %f66;
	mov.u32 	%r443, 1;
	shfl.sync.bfly.b32 	%r444|%p28, %r442, %r443, %r429, %r431;
	mov.b32 	%f67, %r444;
	add.f32 	%f68, %f66, %f67;
	st.local.f32 	[%rd3+4], %f68;

$L__BB111_13:
	bar.sync 	0;
	mov.b32 	%r445, %f2;
	mov.u32 	%r446, 31;
	mov.u32 	%r447, 16;
	mov.u32 	%r448, -1;
	shfl.sync.bfly.b32 	%r449|%p30, %r445, %r447, %r446, %r448;
	mov.b32 	%f69, %r449;
	add.f32 	%f70, %f2, %f69;
	mov.b32 	%r450, %f70;
	mov.u32 	%r451, 8;
	shfl.sync.bfly.b32 	%r452|%p31, %r450, %r451, %r446, %r448;
	mov.b32 	%f71, %r452;
	add.f32 	%f72, %f70, %f71;
	mov.b32 	%r453, %f72;
	mov.u32 	%r454, 4;
	shfl.sync.bfly.b32 	%r455|%p32, %r453, %r454, %r446, %r448;
	mov.b32 	%f73, %r455;
	add.f32 	%f74, %f72, %f73;
	mov.b32 	%r456, %f74;
	mov.u32 	%r457, 2;
	shfl.sync.bfly.b32 	%r458|%p33, %r456, %r457, %r446, %r448;
	mov.b32 	%f75, %r458;
	add.f32 	%f76, %f74, %f75;
	mov.b32 	%r459, %f76;
	mov.u32 	%r460, 1;
	shfl.sync.bfly.b32 	%r461|%p34, %r459, %r460, %r446, %r448;
	mov.b32 	%f77, %r461;
	add.f32 	%f78, %f76, %f77;
	st.local.f32 	[%rd3+8], %f78;
	st.shared.f32 	[%r63], %f78;
	bar.sync 	0;
	@%p1 bra 	$L__BB111_15;

	ld.shared.f32 	%f79, [%r4];
	mov.b32 	%r462, %f79;
	shfl.sync.bfly.b32 	%r466|%p35, %r462, %r447, %r446, %r448;
	mov.b32 	%f80, %r466;
	add.f32 	%f81, %f79, %f80;
	mov.b32 	%r467, %f81;
	shfl.sync.bfly.b32 	%r469|%p36, %r467, %r451, %r446, %r448;
	mov.b32 	%f82, %r469;
	add.f32 	%f83, %f81, %f82;
	mov.b32 	%r470, %f83;
	shfl.sync.bfly.b32 	%r472|%p37, %r470, %r454, %r446, %r448;
	mov.b32 	%f84, %r472;
	add.f32 	%f85, %f83, %f84;
	mov.b32 	%r473, %f85;
	shfl.sync.bfly.b32 	%r475|%p38, %r473, %r457, %r446, %r448;
	mov.b32 	%f86, %r475;
	add.f32 	%f87, %f85, %f86;
	mov.b32 	%r476, %f87;
	shfl.sync.bfly.b32 	%r478|%p39, %r476, %r460, %r446, %r448;
	mov.b32 	%f88, %r478;
	add.f32 	%f89, %f87, %f88;
	st.local.f32 	[%rd3+8], %f89;

$L__BB111_15:
	bar.sync 	0;
	mov.b32 	%r479, %f3;
	shfl.sync.bfly.b32 	%r483|%p41, %r479, %r447, %r446, %r448;
	mov.b32 	%f90, %r483;
	add.f32 	%f91, %f3, %f90;
	mov.b32 	%r484, %f91;
	shfl.sync.bfly.b32 	%r486|%p42, %r484, %r451, %r446, %r448;
	mov.b32 	%f92, %r486;
	add.f32 	%f93, %f91, %f92;
	mov.b32 	%r487, %f93;
	shfl.sync.bfly.b32 	%r489|%p43, %r487, %r454, %r446, %r448;
	mov.b32 	%f94, %r489;
	add.f32 	%f95, %f93, %f94;
	mov.b32 	%r490, %f95;
	shfl.sync.bfly.b32 	%r492|%p44, %r490, %r457, %r446, %r448;
	mov.b32 	%f96, %r492;
	add.f32 	%f97, %f95, %f96;
	mov.b32 	%r493, %f97;
	shfl.sync.bfly.b32 	%r495|%p45, %r493, %r460, %r446, %r448;
	mov.b32 	%f98, %r495;
	add.f32 	%f99, %f97, %f98;
	st.local.f32 	[%rd3+12], %f99;
	st.shared.f32 	[%r63], %f99;
	bar.sync 	0;
	@%p1 bra 	$L__BB111_17;

	ld.shared.f32 	%f100, [%r4];
	mov.b32 	%r496, %f100;
	mov.u32 	%r497, 31;
	mov.u32 	%r498, 16;
	mov.u32 	%r499, -1;
	shfl.sync.bfly.b32 	%r500|%p46, %r496, %r498, %r497, %r499;
	mov.b32 	%f101, %r500;
	add.f32 	%f102, %f100, %f101;
	mov.b32 	%r501, %f102;
	mov.u32 	%r502, 8;
	shfl.sync.bfly.b32 	%r503|%p47, %r501, %r502, %r497, %r499;
	mov.b32 	%f103, %r503;
	add.f32 	%f104, %f102, %f103;
	mov.b32 	%r504, %f104;
	mov.u32 	%r505, 4;
	shfl.sync.bfly.b32 	%r506|%p48, %r504, %r505, %r497, %r499;
	mov.b32 	%f105, %r506;
	add.f32 	%f106, %f104, %f105;
	mov.b32 	%r507, %f106;
	mov.u32 	%r508, 2;
	shfl.sync.bfly.b32 	%r509|%p49, %r507, %r508, %r497, %r499;
	mov.b32 	%f107, %r509;
	add.f32 	%f108, %f106, %f107;
	mov.b32 	%r510, %f108;
	mov.u32 	%r511, 1;
	shfl.sync.bfly.b32 	%r512|%p50, %r510, %r511, %r497, %r499;
	mov.b32 	%f109, %r512;
	add.f32 	%f110, %f108, %f109;
	st.local.f32 	[%rd3+12], %f110;

$L__BB111_17:
	bar.sync 	0;
	mov.b32 	%r513, %f4;
	mov.u32 	%r514, 31;
	mov.u32 	%r515, 16;
	mov.u32 	%r516, -1;
	shfl.sync.bfly.b32 	%r517|%p52, %r513, %r515, %r514, %r516;
	mov.b32 	%f111, %r517;
	add.f32 	%f112, %f4, %f111;
	mov.b32 	%r518, %f112;
	mov.u32 	%r519, 8;
	shfl.sync.bfly.b32 	%r520|%p53, %r518, %r519, %r514, %r516;
	mov.b32 	%f113, %r520;
	add.f32 	%f114, %f112, %f113;
	mov.b32 	%r521, %f114;
	mov.u32 	%r522, 4;
	shfl.sync.bfly.b32 	%r523|%p54, %r521, %r522, %r514, %r516;
	mov.b32 	%f115, %r523;
	add.f32 	%f116, %f114, %f115;
	mov.b32 	%r524, %f116;
	mov.u32 	%r525, 2;
	shfl.sync.bfly.b32 	%r526|%p55, %r524, %r525, %r514, %r516;
	mov.b32 	%f117, %r526;
	add.f32 	%f118, %f116, %f117;
	mov.b32 	%r527, %f118;
	mov.u32 	%r528, 1;
	shfl.sync.bfly.b32 	%r529|%p56, %r527, %r528, %r514, %r516;
	mov.b32 	%f119, %r529;
	add.f32 	%f120, %f118, %f119;
	st.local.f32 	[%rd3+16], %f120;
	st.shared.f32 	[%r63], %f120;
	bar.sync 	0;
	@%p1 bra 	$L__BB111_19;

	ld.shared.f32 	%f121, [%r4];
	mov.b32 	%r530, %f121;
	shfl.sync.bfly.b32 	%r534|%p57, %r530, %r515, %r514, %r516;
	mov.b32 	%f122, %r534;
	add.f32 	%f123, %f121, %f122;
	mov.b32 	%r535, %f123;
	shfl.sync.bfly.b32 	%r537|%p58, %r535, %r519, %r514, %r516;
	mov.b32 	%f124, %r537;
	add.f32 	%f125, %f123, %f124;
	mov.b32 	%r538, %f125;
	shfl.sync.bfly.b32 	%r540|%p59, %r538, %r522, %r514, %r516;
	mov.b32 	%f126, %r540;
	add.f32 	%f127, %f125, %f126;
	mov.b32 	%r541, %f127;
	shfl.sync.bfly.b32 	%r543|%p60, %r541, %r525, %r514, %r516;
	mov.b32 	%f128, %r543;
	add.f32 	%f129, %f127, %f128;
	mov.b32 	%r544, %f129;
	shfl.sync.bfly.b32 	%r546|%p61, %r544, %r528, %r514, %r516;
	mov.b32 	%f130, %r546;
	add.f32 	%f131, %f129, %f130;
	st.local.f32 	[%rd3+16], %f131;

$L__BB111_19:
	bar.sync 	0;
	mov.b32 	%r547, %f5;
	shfl.sync.bfly.b32 	%r551|%p63, %r547, %r515, %r514, %r516;
	mov.b32 	%f132, %r551;
	add.f32 	%f133, %f5, %f132;
	mov.b32 	%r552, %f133;
	shfl.sync.bfly.b32 	%r554|%p64, %r552, %r519, %r514, %r516;
	mov.b32 	%f134, %r554;
	add.f32 	%f135, %f133, %f134;
	mov.b32 	%r555, %f135;
	shfl.sync.bfly.b32 	%r557|%p65, %r555, %r522, %r514, %r516;
	mov.b32 	%f136, %r557;
	add.f32 	%f137, %f135, %f136;
	mov.b32 	%r558, %f137;
	shfl.sync.bfly.b32 	%r560|%p66, %r558, %r525, %r514, %r516;
	mov.b32 	%f138, %r560;
	add.f32 	%f139, %f137, %f138;
	mov.b32 	%r561, %f139;
	shfl.sync.bfly.b32 	%r563|%p67, %r561, %r528, %r514, %r516;
	mov.b32 	%f140, %r563;
	add.f32 	%f141, %f139, %f140;
	st.local.f32 	[%rd3+20], %f141;
	st.shared.f32 	[%r63], %f141;
	bar.sync 	0;
	@%p1 bra 	$L__BB111_21;

	ld.shared.f32 	%f142, [%r4];
	mov.b32 	%r564, %f142;
	mov.u32 	%r565, 31;
	mov.u32 	%r566, 16;
	mov.u32 	%r567, -1;
	shfl.sync.bfly.b32 	%r568|%p68, %r564, %r566, %r565, %r567;
	mov.b32 	%f143, %r568;
	add.f32 	%f144, %f142, %f143;
	mov.b32 	%r569, %f144;
	mov.u32 	%r570, 8;
	shfl.sync.bfly.b32 	%r571|%p69, %r569, %r570, %r565, %r567;
	mov.b32 	%f145, %r571;
	add.f32 	%f146, %f144, %f145;
	mov.b32 	%r572, %f146;
	mov.u32 	%r573, 4;
	shfl.sync.bfly.b32 	%r574|%p70, %r572, %r573, %r565, %r567;
	mov.b32 	%f147, %r574;
	add.f32 	%f148, %f146, %f147;
	mov.b32 	%r575, %f148;
	mov.u32 	%r576, 2;
	shfl.sync.bfly.b32 	%r577|%p71, %r575, %r576, %r565, %r567;
	mov.b32 	%f149, %r577;
	add.f32 	%f150, %f148, %f149;
	mov.b32 	%r578, %f150;
	mov.u32 	%r579, 1;
	shfl.sync.bfly.b32 	%r580|%p72, %r578, %r579, %r565, %r567;
	mov.b32 	%f151, %r580;
	add.f32 	%f152, %f150, %f151;
	st.local.f32 	[%rd3+20], %f152;

$L__BB111_21:
	bar.sync 	0;
	mov.b32 	%r581, %f6;
	mov.u32 	%r582, 31;
	mov.u32 	%r583, 16;
	mov.u32 	%r584, -1;
	shfl.sync.bfly.b32 	%r585|%p74, %r581, %r583, %r582, %r584;
	mov.b32 	%f153, %r585;
	add.f32 	%f154, %f6, %f153;
	mov.b32 	%r586, %f154;
	mov.u32 	%r587, 8;
	shfl.sync.bfly.b32 	%r588|%p75, %r586, %r587, %r582, %r584;
	mov.b32 	%f155, %r588;
	add.f32 	%f156, %f154, %f155;
	mov.b32 	%r589, %f156;
	mov.u32 	%r590, 4;
	shfl.sync.bfly.b32 	%r591|%p76, %r589, %r590, %r582, %r584;
	mov.b32 	%f157, %r591;
	add.f32 	%f158, %f156, %f157;
	mov.b32 	%r592, %f158;
	mov.u32 	%r593, 2;
	shfl.sync.bfly.b32 	%r594|%p77, %r592, %r593, %r582, %r584;
	mov.b32 	%f159, %r594;
	add.f32 	%f160, %f158, %f159;
	mov.b32 	%r595, %f160;
	mov.u32 	%r596, 1;
	shfl.sync.bfly.b32 	%r597|%p78, %r595, %r596, %r582, %r584;
	mov.b32 	%f161, %r597;
	add.f32 	%f162, %f160, %f161;
	st.local.f32 	[%rd3+24], %f162;
	st.shared.f32 	[%r63], %f162;
	bar.sync 	0;
	@%p1 bra 	$L__BB111_23;

	ld.shared.f32 	%f163, [%r4];
	mov.b32 	%r598, %f163;
	shfl.sync.bfly.b32 	%r602|%p79, %r598, %r583, %r582, %r584;
	mov.b32 	%f164, %r602;
	add.f32 	%f165, %f163, %f164;
	mov.b32 	%r603, %f165;
	shfl.sync.bfly.b32 	%r605|%p80, %r603, %r587, %r582, %r584;
	mov.b32 	%f166, %r605;
	add.f32 	%f167, %f165, %f166;
	mov.b32 	%r606, %f167;
	shfl.sync.bfly.b32 	%r608|%p81, %r606, %r590, %r582, %r584;
	mov.b32 	%f168, %r608;
	add.f32 	%f169, %f167, %f168;
	mov.b32 	%r609, %f169;
	shfl.sync.bfly.b32 	%r611|%p82, %r609, %r593, %r582, %r584;
	mov.b32 	%f170, %r611;
	add.f32 	%f171, %f169, %f170;
	mov.b32 	%r612, %f171;
	shfl.sync.bfly.b32 	%r614|%p83, %r612, %r596, %r582, %r584;
	mov.b32 	%f172, %r614;
	add.f32 	%f173, %f171, %f172;
	st.local.f32 	[%rd3+24], %f173;

$L__BB111_23:
	bar.sync 	0;
	mov.b32 	%r615, %f7;
	shfl.sync.bfly.b32 	%r619|%p85, %r615, %r583, %r582, %r584;
	mov.b32 	%f174, %r619;
	add.f32 	%f175, %f7, %f174;
	mov.b32 	%r620, %f175;
	shfl.sync.bfly.b32 	%r622|%p86, %r620, %r587, %r582, %r584;
	mov.b32 	%f176, %r622;
	add.f32 	%f177, %f175, %f176;
	mov.b32 	%r623, %f177;
	shfl.sync.bfly.b32 	%r625|%p87, %r623, %r590, %r582, %r584;
	mov.b32 	%f178, %r625;
	add.f32 	%f179, %f177, %f178;
	mov.b32 	%r626, %f179;
	shfl.sync.bfly.b32 	%r628|%p88, %r626, %r593, %r582, %r584;
	mov.b32 	%f180, %r628;
	add.f32 	%f181, %f179, %f180;
	mov.b32 	%r629, %f181;
	shfl.sync.bfly.b32 	%r631|%p89, %r629, %r596, %r582, %r584;
	mov.b32 	%f182, %r631;
	add.f32 	%f183, %f181, %f182;
	st.local.f32 	[%rd3+28], %f183;
	st.shared.f32 	[%r63], %f183;
	bar.sync 	0;
	@%p1 bra 	$L__BB111_25;

	ld.shared.f32 	%f184, [%r4];
	mov.b32 	%r632, %f184;
	mov.u32 	%r633, 31;
	mov.u32 	%r634, 16;
	mov.u32 	%r635, -1;
	shfl.sync.bfly.b32 	%r636|%p90, %r632, %r634, %r633, %r635;
	mov.b32 	%f185, %r636;
	add.f32 	%f186, %f184, %f185;
	mov.b32 	%r637, %f186;
	mov.u32 	%r638, 8;
	shfl.sync.bfly.b32 	%r639|%p91, %r637, %r638, %r633, %r635;
	mov.b32 	%f187, %r639;
	add.f32 	%f188, %f186, %f187;
	mov.b32 	%r640, %f188;
	mov.u32 	%r641, 4;
	shfl.sync.bfly.b32 	%r642|%p92, %r640, %r641, %r633, %r635;
	mov.b32 	%f189, %r642;
	add.f32 	%f190, %f188, %f189;
	mov.b32 	%r643, %f190;
	mov.u32 	%r644, 2;
	shfl.sync.bfly.b32 	%r645|%p93, %r643, %r644, %r633, %r635;
	mov.b32 	%f191, %r645;
	add.f32 	%f192, %f190, %f191;
	mov.b32 	%r646, %f192;
	mov.u32 	%r647, 1;
	shfl.sync.bfly.b32 	%r648|%p94, %r646, %r647, %r633, %r635;
	mov.b32 	%f193, %r648;
	add.f32 	%f194, %f192, %f193;
	st.local.f32 	[%rd3+28], %f194;

$L__BB111_25:
	bar.sync 	0;
	setp.gt.s32 	%p95, %r3, 7;
	@%p95 bra 	$L__BB111_27;

	mad.lo.s32 	%r649, %r3, %r66, %r2;
	cvt.s64.s32 	%rd75, %r649;
	mul.lo.s32 	%r650, %r1, %r67;
	cvt.s64.s32 	%rd76, %r650;
	add.s64 	%rd77, %rd76, %rd75;
	mul.wide.s32 	%rd78, %r3, 4;
	add.s64 	%rd79, %rd3, %rd78;
	ld.local.f32 	%f195, [%rd79];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs3, %f195;}

	// end inline asm
	cvta.to.global.u64 	%rd80, %rd30;
	shl.b64 	%rd81, %rd77, 1;
	add.s64 	%rd82, %rd80, %rd81;
	st.global.u16 	[%rd82], %rs3;

$L__BB111_27:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_1_bs_224
.visible .entry ggml_matvec_f16_ncols_1_bs_224(
	.param .u64 ggml_matvec_f16_ncols_1_bs_224_param_0,
	.param .u64 ggml_matvec_f16_ncols_1_bs_224_param_1,
	.param .u64 ggml_matvec_f16_ncols_1_bs_224_param_2,
	.param .u32 ggml_matvec_f16_ncols_1_bs_224_param_3,
	.param .u32 ggml_matvec_f16_ncols_1_bs_224_param_4,
	.param .u32 ggml_matvec_f16_ncols_1_bs_224_param_5,
	.param .u32 ggml_matvec_f16_ncols_1_bs_224_param_6,
	.param .u32 ggml_matvec_f16_ncols_1_bs_224_param_7,
	.param .u32 ggml_matvec_f16_ncols_1_bs_224_param_8,
	.param .u32 ggml_matvec_f16_ncols_1_bs_224_param_9,
	.param .u32 ggml_matvec_f16_ncols_1_bs_224_param_10,
	.param .u32 ggml_matvec_f16_ncols_1_bs_224_param_11
)
{
	.reg .pred 	%p<19>;
	.reg .b16 	%rs<31>;
	.reg .f32 	%f<30>;
	.reg .b32 	%r<128>;
	.reg .b64 	%rd<47>;


	ld.param.u64 	%rd14, [ggml_matvec_f16_ncols_1_bs_224_param_0];
	ld.param.u64 	%rd15, [ggml_matvec_f16_ncols_1_bs_224_param_1];
	ld.param.u64 	%rd16, [ggml_matvec_f16_ncols_1_bs_224_param_2];
	ld.param.u32 	%r13, [ggml_matvec_f16_ncols_1_bs_224_param_3];
	ld.param.u32 	%r17, [ggml_matvec_f16_ncols_1_bs_224_param_5];
	ld.param.u32 	%r14, [ggml_matvec_f16_ncols_1_bs_224_param_7];
	ld.param.u32 	%r18, [ggml_matvec_f16_ncols_1_bs_224_param_8];
	ld.param.u32 	%r19, [ggml_matvec_f16_ncols_1_bs_224_param_9];
	ld.param.u32 	%r15, [ggml_matvec_f16_ncols_1_bs_224_param_10];
	ld.param.u32 	%r16, [ggml_matvec_f16_ncols_1_bs_224_param_11];
	mov.u32 	%r20, %ctaid.x;
	mov.u32 	%r21, %ctaid.y;
	div.s32 	%r22, %r21, %r18;
	mul.lo.s32 	%r23, %r20, %r17;
	mad.lo.s32 	%r24, %r22, %r19, %r23;
	cvt.s64.s32 	%rd1, %r24;
	mov.u32 	%r1, %tid.x;
	setp.gt.s32 	%p1, %r1, 31;
	shl.b32 	%r25, %r1, 2;
	mov.u32 	%r26, data_mmv;
	add.s32 	%r2, %r26, %r25;
	@%p1 bra 	$L__BB112_2;

	mov.u32 	%r27, 0;
	st.shared.u32 	[%r2], %r27;

$L__BB112_2:
	bar.sync 	0;
	mov.f32 	%f5, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs30, %f5;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs29, %f5;}

	// end inline asm
	setp.ge.s32 	%p2, %r1, %r13;
	@%p2 bra 	$L__BB112_9;

	not.b32 	%r28, %r1;
	add.s32 	%r29, %r28, %r13;
	shr.u32 	%r30, %r29, 5;
	mul.wide.u32 	%rd17, %r30, 613566757;
	shr.u64 	%rd18, %rd17, 32;
	cvt.u32.u64 	%r31, %rd18;
	add.s32 	%r32, %r31, 1;
	and.b32  	%r125, %r32, 3;
	setp.eq.s32 	%p3, %r125, 0;
	mov.u32 	%r126, %r1;
	@%p3 bra 	$L__BB112_6;

	mov.u32 	%r126, %tid.x;
	mul.wide.s32 	%rd19, %r126, 2;
	mul.lo.s32 	%r34, %r21, %r15;
	cvt.s64.s32 	%rd20, %r34;
	add.s64 	%rd21, %rd19, %rd20;
	cvta.to.global.u64 	%rd22, %rd15;
	shl.b64 	%rd23, %rd21, 1;
	add.s64 	%rd44, %rd22, %rd23;
	add.s64 	%rd24, %rd19, %rd1;
	cvta.to.global.u64 	%rd25, %rd14;
	shl.b64 	%rd26, %rd24, 1;
	add.s64 	%rd43, %rd25, %rd26;

$L__BB112_5:
	.pragma "nounroll";
	ld.global.nc.u32 	%r36, [%rd43];
	ld.global.nc.u32 	%r37, [%rd44];
	// begin inline asm
	{mul.f16x2 %r35,%r36,%r37;
}
	// end inline asm
	mov.b32 	%r39, {%rs30, %rs29};
	// begin inline asm
	{add.f16x2 %r38,%r39,%r35;
}
	// end inline asm
	mov.b32 	{%rs30, %rs29}, %r38;
	add.s32 	%r126, %r126, 224;
	add.s64 	%rd44, %rd44, 896;
	add.s64 	%rd43, %rd43, 896;
	add.s32 	%r125, %r125, -1;
	setp.ne.s32 	%p4, %r125, 0;
	@%p4 bra 	$L__BB112_5;

$L__BB112_6:
	setp.lt.u32 	%p5, %r29, 672;
	@%p5 bra 	$L__BB112_9;

	mul.wide.s32 	%rd27, %r126, 2;
	cvta.to.global.u64 	%rd28, %rd14;
	add.s64 	%rd29, %rd27, %rd1;
	shl.b64 	%rd30, %rd29, 1;
	add.s64 	%rd31, %rd28, %rd30;
	add.s64 	%rd46, %rd31, 1792;
	mul.lo.s32 	%r45, %r21, %r15;
	cvt.s64.s32 	%rd32, %r45;
	cvta.to.global.u64 	%rd33, %rd15;
	add.s64 	%rd34, %rd27, %rd32;
	shl.b64 	%rd35, %rd34, 1;
	add.s64 	%rd36, %rd33, %rd35;
	add.s64 	%rd45, %rd36, 1792;

$L__BB112_8:
	ld.global.nc.u32 	%r47, [%rd46+-1792];
	ld.global.nc.u32 	%r48, [%rd45+-1792];
	// begin inline asm
	{mul.f16x2 %r46,%r47,%r48;
}
	// end inline asm
	mov.b32 	%r50, {%rs30, %rs29};
	// begin inline asm
	{add.f16x2 %r49,%r50,%r46;
}
	// end inline asm
	ld.global.nc.u32 	%r53, [%rd46+-896];
	ld.global.nc.u32 	%r54, [%rd45+-896];
	// begin inline asm
	{mul.f16x2 %r52,%r53,%r54;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r55,%r49,%r52;
}
	// end inline asm
	ld.global.nc.u32 	%r59, [%rd46];
	ld.global.nc.u32 	%r60, [%rd45];
	// begin inline asm
	{mul.f16x2 %r58,%r59,%r60;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r61,%r55,%r58;
}
	// end inline asm
	ld.global.nc.u32 	%r65, [%rd46+896];
	ld.global.nc.u32 	%r66, [%rd45+896];
	// begin inline asm
	{mul.f16x2 %r64,%r65,%r66;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r67,%r61,%r64;
}
	// end inline asm
	mov.b32 	{%rs30, %rs29}, %r67;
	add.s64 	%rd46, %rd46, 3584;
	add.s64 	%rd45, %rd45, 3584;
	add.s32 	%r126, %r126, 896;
	setp.lt.s32 	%p6, %r126, %r13;
	@%p6 bra 	$L__BB112_8;

$L__BB112_9:
	mov.u32 	%r73, 1;
	mov.b32 	%r71, {%rs30, %rs29};
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r71;
  cvt.f32.f16 %f6, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r71;
  cvt.f32.f16 %f7, high;}

	// end inline asm
	add.f32 	%f8, %f6, %f7;
	mov.b32 	%r74, %f8;
	mov.u32 	%r75, 31;
	mov.u32 	%r76, 16;
	mov.u32 	%r77, -1;
	shfl.sync.bfly.b32 	%r78|%p8, %r74, %r76, %r75, %r77;
	mov.b32 	%f9, %r78;
	add.f32 	%f10, %f8, %f9;
	mov.b32 	%r79, %f10;
	mov.u32 	%r80, 8;
	shfl.sync.bfly.b32 	%r81|%p9, %r79, %r80, %r75, %r77;
	mov.b32 	%f11, %r81;
	add.f32 	%f12, %f10, %f11;
	mov.b32 	%r82, %f12;
	mov.u32 	%r83, 4;
	shfl.sync.bfly.b32 	%r84|%p10, %r82, %r83, %r75, %r77;
	mov.b32 	%f13, %r84;
	add.f32 	%f14, %f12, %f13;
	mov.b32 	%r85, %f14;
	mov.u32 	%r86, 2;
	shfl.sync.bfly.b32 	%r87|%p11, %r85, %r86, %r75, %r77;
	mov.b32 	%f15, %r87;
	add.f32 	%f16, %f14, %f15;
	mov.b32 	%r88, %f16;
	shfl.sync.bfly.b32 	%r89|%p12, %r88, %r73, %r75, %r77;
	mov.b32 	%f17, %r89;
	add.f32 	%f29, %f16, %f17;
	shr.s32 	%r90, %r1, 31;
	shr.u32 	%r91, %r90, 27;
	add.s32 	%r92, %r1, %r91;
	shr.s32 	%r93, %r92, 5;
	shl.b32 	%r94, %r93, 2;
	add.s32 	%r96, %r26, %r94;
	st.shared.f32 	[%r96], %f29;
	bar.sync 	0;
	@%p1 bra 	$L__BB112_11;

	ld.shared.f32 	%f18, [%r2];
	mov.b32 	%r102, %f18;
	shfl.sync.bfly.b32 	%r106|%p13, %r102, %r76, %r75, %r77;
	mov.b32 	%f19, %r106;
	add.f32 	%f20, %f18, %f19;
	mov.b32 	%r107, %f20;
	shfl.sync.bfly.b32 	%r109|%p14, %r107, %r80, %r75, %r77;
	mov.b32 	%f21, %r109;
	add.f32 	%f22, %f20, %f21;
	mov.b32 	%r110, %f22;
	shfl.sync.bfly.b32 	%r112|%p15, %r110, %r83, %r75, %r77;
	mov.b32 	%f23, %r112;
	add.f32 	%f24, %f22, %f23;
	mov.b32 	%r113, %f24;
	shfl.sync.bfly.b32 	%r114|%p16, %r113, %r86, %r75, %r77;
	mov.b32 	%f25, %r114;
	add.f32 	%f26, %f24, %f25;
	mov.b32 	%r115, %f26;
	shfl.sync.bfly.b32 	%r117|%p17, %r115, %r73, %r75, %r77;
	mov.b32 	%f27, %r117;
	add.f32 	%f29, %f26, %f27;

$L__BB112_11:
	bar.sync 	0;
	setp.gt.s32 	%p18, %r1, 0;
	@%p18 bra 	$L__BB112_13;

	mad.lo.s32 	%r121, %r1, %r14, %r20;
	cvt.s64.s32 	%rd37, %r121;
	mul.lo.s32 	%r123, %r21, %r16;
	cvt.s64.s32 	%rd38, %r123;
	add.s64 	%rd39, %rd38, %rd37;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs20, %f29;}

	// end inline asm
	cvta.to.global.u64 	%rd40, %rd16;
	shl.b64 	%rd41, %rd39, 1;
	add.s64 	%rd42, %rd40, %rd41;
	st.global.u16 	[%rd42], %rs20;

$L__BB112_13:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_2_bs_224
.visible .entry ggml_matvec_f16_ncols_2_bs_224(
	.param .u64 ggml_matvec_f16_ncols_2_bs_224_param_0,
	.param .u64 ggml_matvec_f16_ncols_2_bs_224_param_1,
	.param .u64 ggml_matvec_f16_ncols_2_bs_224_param_2,
	.param .u32 ggml_matvec_f16_ncols_2_bs_224_param_3,
	.param .u32 ggml_matvec_f16_ncols_2_bs_224_param_4,
	.param .u32 ggml_matvec_f16_ncols_2_bs_224_param_5,
	.param .u32 ggml_matvec_f16_ncols_2_bs_224_param_6,
	.param .u32 ggml_matvec_f16_ncols_2_bs_224_param_7,
	.param .u32 ggml_matvec_f16_ncols_2_bs_224_param_8,
	.param .u32 ggml_matvec_f16_ncols_2_bs_224_param_9,
	.param .u32 ggml_matvec_f16_ncols_2_bs_224_param_10,
	.param .u32 ggml_matvec_f16_ncols_2_bs_224_param_11
)
{
	.local .align 8 .b8 	__local_depot113[8];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<30>;
	.reg .b16 	%rs<4>;
	.reg .f32 	%f<52>;
	.reg .b32 	%r<202>;
	.reg .b64 	%rd<66>;


	mov.u64 	%SPL, __local_depot113;
	ld.param.u64 	%rd27, [ggml_matvec_f16_ncols_2_bs_224_param_0];
	ld.param.u64 	%rd28, [ggml_matvec_f16_ncols_2_bs_224_param_1];
	ld.param.u64 	%rd26, [ggml_matvec_f16_ncols_2_bs_224_param_2];
	ld.param.u32 	%r28, [ggml_matvec_f16_ncols_2_bs_224_param_3];
	ld.param.u32 	%r32, [ggml_matvec_f16_ncols_2_bs_224_param_5];
	ld.param.u32 	%r29, [ggml_matvec_f16_ncols_2_bs_224_param_6];
	ld.param.u32 	%r30, [ggml_matvec_f16_ncols_2_bs_224_param_7];
	ld.param.u32 	%r33, [ggml_matvec_f16_ncols_2_bs_224_param_8];
	ld.param.u32 	%r34, [ggml_matvec_f16_ncols_2_bs_224_param_9];
	ld.param.u32 	%r35, [ggml_matvec_f16_ncols_2_bs_224_param_10];
	ld.param.u32 	%r31, [ggml_matvec_f16_ncols_2_bs_224_param_11];
	cvta.to.global.u64 	%rd1, %rd28;
	cvta.to.global.u64 	%rd2, %rd27;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r36, %r1, %r33;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r37, %r2, %r32;
	mad.lo.s32 	%r38, %r36, %r34, %r37;
	cvt.s64.s32 	%rd4, %r38;
	mul.lo.s32 	%r39, %r1, %r35;
	cvt.s64.s32 	%rd5, %r39;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r40, %r3, 2;
	mov.u32 	%r41, data_mmv;
	add.s32 	%r4, %r41, %r40;
	@%p1 bra 	$L__BB113_2;

	mov.u32 	%r42, 0;
	st.shared.u32 	[%r4], %r42;

$L__BB113_2:
	bar.sync 	0;
	mov.f32 	%f3, 0f00000000;
	st.local.v2.f32 	[%rd3], {%f3, %f3};
	mov.u32 	%r200, 0;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs2, %f3;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f3;}

	// end inline asm
	mov.b32 	%r201, {%rs1, %rs2};
	setp.ge.s32 	%p2, %r3, %r28;
	@%p2 bra 	$L__BB113_9;

	not.b32 	%r45, %r3;
	add.s32 	%r6, %r45, %r28;
	shr.u32 	%r46, %r6, 5;
	mul.wide.u32 	%rd30, %r46, 613566757;
	shr.u64 	%rd31, %rd30, 32;
	cvt.u32.u64 	%r47, %rd31;
	add.s32 	%r48, %r47, 1;
	and.b32  	%r193, %r48, 3;
	setp.eq.s32 	%p3, %r193, 0;
	mov.u32 	%r200, 0;
	mov.u32 	%r196, %r3;
	@%p3 bra 	$L__BB113_6;

	mul.wide.s32 	%rd32, %r29, 2;
	mul.wide.s32 	%rd33, %r3, 2;
	add.s64 	%rd34, %rd32, %rd33;
	add.s64 	%rd35, %rd34, %rd5;
	shl.b64 	%rd36, %rd35, 1;
	add.s64 	%rd62, %rd1, %rd36;
	add.s64 	%rd37, %rd33, %rd5;
	shl.b64 	%rd38, %rd37, 1;
	add.s64 	%rd61, %rd1, %rd38;
	add.s64 	%rd39, %rd33, %rd4;
	shl.b64 	%rd40, %rd39, 1;
	add.s64 	%rd60, %rd2, %rd40;
	mov.u32 	%r200, 0;
	mov.u32 	%r196, %r3;

$L__BB113_5:
	.pragma "nounroll";
	ld.global.nc.u32 	%r51, [%rd60];
	ld.global.nc.u32 	%r52, [%rd61];
	// begin inline asm
	{mul.f16x2 %r50,%r51,%r52;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r201,%r201,%r50;
}
	// end inline asm
	ld.global.nc.u32 	%r58, [%rd62];
	// begin inline asm
	{mul.f16x2 %r56,%r51,%r58;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r200,%r200,%r56;
}
	// end inline asm
	add.s32 	%r196, %r196, 224;
	add.s64 	%rd62, %rd62, 896;
	add.s64 	%rd61, %rd61, 896;
	add.s64 	%rd60, %rd60, 896;
	add.s32 	%r193, %r193, -1;
	setp.ne.s32 	%p4, %r193, 0;
	@%p4 bra 	$L__BB113_5;

$L__BB113_6:
	setp.lt.u32 	%p5, %r6, 672;
	@%p5 bra 	$L__BB113_9;

	mul.wide.s32 	%rd41, %r196, 2;
	add.s64 	%rd42, %rd41, %rd4;
	shl.b64 	%rd43, %rd42, 1;
	add.s64 	%rd44, %rd2, %rd43;
	add.s64 	%rd65, %rd44, 1792;
	add.s64 	%rd45, %rd41, %rd5;
	shl.b64 	%rd46, %rd45, 1;
	add.s64 	%rd47, %rd1, %rd46;
	add.s64 	%rd64, %rd47, 2688;
	mul.wide.s32 	%rd48, %r29, 2;
	add.s64 	%rd49, %rd45, %rd48;
	shl.b64 	%rd50, %rd49, 1;
	add.s64 	%rd51, %rd1, %rd50;
	add.s64 	%rd63, %rd51, 1792;

$L__BB113_8:
	ld.global.nc.u32 	%r63, [%rd65+-1792];
	ld.global.nc.u32 	%r64, [%rd64+-2688];
	// begin inline asm
	{mul.f16x2 %r62,%r63,%r64;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r65,%r201,%r62;
}
	// end inline asm
	ld.global.nc.u32 	%r70, [%rd63+-1792];
	// begin inline asm
	{mul.f16x2 %r68,%r63,%r70;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r71,%r200,%r68;
}
	// end inline asm
	ld.global.nc.u32 	%r75, [%rd65+-896];
	ld.global.nc.u32 	%r76, [%rd64+-1792];
	// begin inline asm
	{mul.f16x2 %r74,%r75,%r76;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r77,%r65,%r74;
}
	// end inline asm
	ld.global.nc.u32 	%r82, [%rd63+-896];
	// begin inline asm
	{mul.f16x2 %r80,%r75,%r82;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r83,%r71,%r80;
}
	// end inline asm
	ld.global.nc.u32 	%r87, [%rd65];
	ld.global.nc.u32 	%r88, [%rd64+-896];
	// begin inline asm
	{mul.f16x2 %r86,%r87,%r88;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r89,%r77,%r86;
}
	// end inline asm
	ld.global.nc.u32 	%r94, [%rd63];
	// begin inline asm
	{mul.f16x2 %r92,%r87,%r94;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r95,%r83,%r92;
}
	// end inline asm
	ld.global.nc.u32 	%r99, [%rd65+896];
	ld.global.nc.u32 	%r100, [%rd64];
	// begin inline asm
	{mul.f16x2 %r98,%r99,%r100;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r201,%r89,%r98;
}
	// end inline asm
	ld.global.nc.u32 	%r106, [%rd63+896];
	// begin inline asm
	{mul.f16x2 %r104,%r99,%r106;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r200,%r95,%r104;
}
	// end inline asm
	add.s64 	%rd65, %rd65, 3584;
	add.s64 	%rd64, %rd64, 3584;
	add.s64 	%rd63, %rd63, 3584;
	add.s32 	%r196, %r196, 896;
	setp.lt.s32 	%p6, %r196, %r28;
	@%p6 bra 	$L__BB113_8;

$L__BB113_9:
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r201;
  cvt.f32.f16 %f4, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r201;
  cvt.f32.f16 %f5, high;}

	// end inline asm
	add.f32 	%f8, %f4, %f5;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r200;
  cvt.f32.f16 %f6, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r200;
  cvt.f32.f16 %f7, high;}

	// end inline asm
	add.f32 	%f1, %f6, %f7;
	st.local.f32 	[%rd3+4], %f1;
	shr.s32 	%r114, %r3, 31;
	shr.u32 	%r115, %r114, 27;
	add.s32 	%r116, %r3, %r115;
	shr.s32 	%r117, %r116, 5;
	shl.b32 	%r118, %r117, 2;
	add.s32 	%r27, %r41, %r118;
	mov.u32 	%r120, 2;
	mov.b32 	%r121, %f8;
	mov.u32 	%r122, 31;
	mov.u32 	%r123, 16;
	mov.u32 	%r124, -1;
	shfl.sync.bfly.b32 	%r125|%p7, %r121, %r123, %r122, %r124;
	mov.b32 	%f9, %r125;
	add.f32 	%f10, %f8, %f9;
	mov.b32 	%r126, %f10;
	mov.u32 	%r127, 8;
	shfl.sync.bfly.b32 	%r128|%p8, %r126, %r127, %r122, %r124;
	mov.b32 	%f11, %r128;
	add.f32 	%f12, %f10, %f11;
	mov.b32 	%r129, %f12;
	mov.u32 	%r130, 4;
	shfl.sync.bfly.b32 	%r131|%p9, %r129, %r130, %r122, %r124;
	mov.b32 	%f13, %r131;
	add.f32 	%f14, %f12, %f13;
	mov.b32 	%r132, %f14;
	shfl.sync.bfly.b32 	%r133|%p10, %r132, %r120, %r122, %r124;
	mov.b32 	%f15, %r133;
	add.f32 	%f16, %f14, %f15;
	mov.b32 	%r134, %f16;
	mov.u32 	%r135, 1;
	shfl.sync.bfly.b32 	%r136|%p11, %r134, %r135, %r122, %r124;
	mov.b32 	%f17, %r136;
	add.f32 	%f18, %f16, %f17;
	st.local.f32 	[%rd3], %f18;
	st.shared.f32 	[%r27], %f18;
	bar.sync 	0;
	@%p1 bra 	$L__BB113_11;

	ld.shared.f32 	%f19, [%r4];
	mov.b32 	%r137, %f19;
	shfl.sync.bfly.b32 	%r141|%p13, %r137, %r123, %r122, %r124;
	mov.b32 	%f20, %r141;
	add.f32 	%f21, %f19, %f20;
	mov.b32 	%r142, %f21;
	shfl.sync.bfly.b32 	%r144|%p14, %r142, %r127, %r122, %r124;
	mov.b32 	%f22, %r144;
	add.f32 	%f23, %f21, %f22;
	mov.b32 	%r145, %f23;
	shfl.sync.bfly.b32 	%r147|%p15, %r145, %r130, %r122, %r124;
	mov.b32 	%f24, %r147;
	add.f32 	%f25, %f23, %f24;
	mov.b32 	%r148, %f25;
	shfl.sync.bfly.b32 	%r150|%p16, %r148, %r120, %r122, %r124;
	mov.b32 	%f26, %r150;
	add.f32 	%f27, %f25, %f26;
	mov.b32 	%r151, %f27;
	shfl.sync.bfly.b32 	%r153|%p17, %r151, %r135, %r122, %r124;
	mov.b32 	%f28, %r153;
	add.f32 	%f29, %f27, %f28;
	st.local.f32 	[%rd3], %f29;

$L__BB113_11:
	bar.sync 	0;
	mov.b32 	%r154, %f1;
	shfl.sync.bfly.b32 	%r158|%p19, %r154, %r123, %r122, %r124;
	mov.b32 	%f30, %r158;
	add.f32 	%f31, %f1, %f30;
	mov.b32 	%r159, %f31;
	shfl.sync.bfly.b32 	%r161|%p20, %r159, %r127, %r122, %r124;
	mov.b32 	%f32, %r161;
	add.f32 	%f33, %f31, %f32;
	mov.b32 	%r162, %f33;
	shfl.sync.bfly.b32 	%r164|%p21, %r162, %r130, %r122, %r124;
	mov.b32 	%f34, %r164;
	add.f32 	%f35, %f33, %f34;
	mov.b32 	%r165, %f35;
	shfl.sync.bfly.b32 	%r167|%p22, %r165, %r120, %r122, %r124;
	mov.b32 	%f36, %r167;
	add.f32 	%f37, %f35, %f36;
	mov.b32 	%r168, %f37;
	shfl.sync.bfly.b32 	%r170|%p23, %r168, %r135, %r122, %r124;
	mov.b32 	%f38, %r170;
	add.f32 	%f39, %f37, %f38;
	st.local.f32 	[%rd3+4], %f39;
	st.shared.f32 	[%r27], %f39;
	bar.sync 	0;
	@%p1 bra 	$L__BB113_13;

	ld.shared.f32 	%f40, [%r4];
	mov.b32 	%r171, %f40;
	mov.u32 	%r172, 31;
	mov.u32 	%r173, 16;
	mov.u32 	%r174, -1;
	shfl.sync.bfly.b32 	%r175|%p24, %r171, %r173, %r172, %r174;
	mov.b32 	%f41, %r175;
	add.f32 	%f42, %f40, %f41;
	mov.b32 	%r176, %f42;
	mov.u32 	%r177, 8;
	shfl.sync.bfly.b32 	%r178|%p25, %r176, %r177, %r172, %r174;
	mov.b32 	%f43, %r178;
	add.f32 	%f44, %f42, %f43;
	mov.b32 	%r179, %f44;
	mov.u32 	%r180, 4;
	shfl.sync.bfly.b32 	%r181|%p26, %r179, %r180, %r172, %r174;
	mov.b32 	%f45, %r181;
	add.f32 	%f46, %f44, %f45;
	mov.b32 	%r182, %f46;
	mov.u32 	%r183, 2;
	shfl.sync.bfly.b32 	%r184|%p27, %r182, %r183, %r172, %r174;
	mov.b32 	%f47, %r184;
	add.f32 	%f48, %f46, %f47;
	mov.b32 	%r185, %f48;
	mov.u32 	%r186, 1;
	shfl.sync.bfly.b32 	%r187|%p28, %r185, %r186, %r172, %r174;
	mov.b32 	%f49, %r187;
	add.f32 	%f50, %f48, %f49;
	st.local.f32 	[%rd3+4], %f50;

$L__BB113_13:
	bar.sync 	0;
	setp.gt.s32 	%p29, %r3, 1;
	@%p29 bra 	$L__BB113_15;

	mad.lo.s32 	%r188, %r3, %r30, %r2;
	cvt.s64.s32 	%rd52, %r188;
	mul.lo.s32 	%r189, %r1, %r31;
	cvt.s64.s32 	%rd53, %r189;
	add.s64 	%rd54, %rd53, %rd52;
	mul.wide.s32 	%rd55, %r3, 4;
	add.s64 	%rd56, %rd3, %rd55;
	ld.local.f32 	%f51, [%rd56];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs3, %f51;}

	// end inline asm
	cvta.to.global.u64 	%rd57, %rd26;
	shl.b64 	%rd58, %rd54, 1;
	add.s64 	%rd59, %rd57, %rd58;
	st.global.u16 	[%rd59], %rs3;

$L__BB113_15:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_3_bs_224
.visible .entry ggml_matvec_f16_ncols_3_bs_224(
	.param .u64 ggml_matvec_f16_ncols_3_bs_224_param_0,
	.param .u64 ggml_matvec_f16_ncols_3_bs_224_param_1,
	.param .u64 ggml_matvec_f16_ncols_3_bs_224_param_2,
	.param .u32 ggml_matvec_f16_ncols_3_bs_224_param_3,
	.param .u32 ggml_matvec_f16_ncols_3_bs_224_param_4,
	.param .u32 ggml_matvec_f16_ncols_3_bs_224_param_5,
	.param .u32 ggml_matvec_f16_ncols_3_bs_224_param_6,
	.param .u32 ggml_matvec_f16_ncols_3_bs_224_param_7,
	.param .u32 ggml_matvec_f16_ncols_3_bs_224_param_8,
	.param .u32 ggml_matvec_f16_ncols_3_bs_224_param_9,
	.param .u32 ggml_matvec_f16_ncols_3_bs_224_param_10,
	.param .u32 ggml_matvec_f16_ncols_3_bs_224_param_11
)
{
	.local .align 4 .b8 	__local_depot114[12];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<41>;
	.reg .b16 	%rs<4>;
	.reg .f32 	%f<76>;
	.reg .b32 	%r<287>;
	.reg .b64 	%rd<74>;


	mov.u64 	%SPL, __local_depot114;
	ld.param.u64 	%rd29, [ggml_matvec_f16_ncols_3_bs_224_param_0];
	ld.param.u64 	%rd30, [ggml_matvec_f16_ncols_3_bs_224_param_1];
	ld.param.u64 	%rd28, [ggml_matvec_f16_ncols_3_bs_224_param_2];
	ld.param.u32 	%r34, [ggml_matvec_f16_ncols_3_bs_224_param_3];
	ld.param.u32 	%r38, [ggml_matvec_f16_ncols_3_bs_224_param_5];
	ld.param.u32 	%r35, [ggml_matvec_f16_ncols_3_bs_224_param_6];
	ld.param.u32 	%r36, [ggml_matvec_f16_ncols_3_bs_224_param_7];
	ld.param.u32 	%r39, [ggml_matvec_f16_ncols_3_bs_224_param_8];
	ld.param.u32 	%r40, [ggml_matvec_f16_ncols_3_bs_224_param_9];
	ld.param.u32 	%r41, [ggml_matvec_f16_ncols_3_bs_224_param_10];
	ld.param.u32 	%r37, [ggml_matvec_f16_ncols_3_bs_224_param_11];
	cvta.to.global.u64 	%rd73, %rd30;
	cvta.to.global.u64 	%rd2, %rd29;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r42, %r1, %r39;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r43, %r2, %r38;
	mad.lo.s32 	%r44, %r42, %r40, %r43;
	cvt.s64.s32 	%rd4, %r44;
	mul.lo.s32 	%r45, %r1, %r41;
	cvt.s64.s32 	%rd5, %r45;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r46, %r3, 2;
	mov.u32 	%r47, data_mmv;
	add.s32 	%r4, %r47, %r46;
	@%p1 bra 	$L__BB114_2;

	mov.u32 	%r48, 0;
	st.shared.u32 	[%r4], %r48;

$L__BB114_2:
	bar.sync 	0;
	mov.f32 	%f4, 0f00000000;
	mov.u32 	%r284, 0;
	st.local.u32 	[%rd3], %r284;
	st.local.u32 	[%rd3+4], %r284;
	st.local.u32 	[%rd3+8], %r284;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs2, %f4;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f4;}

	// end inline asm
	mov.b32 	%r286, {%rs1, %rs2};
	setp.ge.s32 	%p2, %r3, %r34;
	mov.u32 	%r285, %r284;
	@%p2 bra 	$L__BB114_9;

	not.b32 	%r53, %r3;
	add.s32 	%r6, %r53, %r34;
	shr.u32 	%r54, %r6, 5;
	mul.wide.u32 	%rd32, %r54, 613566757;
	shr.u64 	%rd33, %rd32, 32;
	cvt.u32.u64 	%r55, %rd33;
	add.s32 	%r56, %r55, 1;
	and.b32  	%r275, %r56, 3;
	setp.eq.s32 	%p3, %r275, 0;
	mov.u32 	%r284, 0;
	mov.u32 	%r279, %r3;
	@%p3 bra 	$L__BB114_6;

	shl.b32 	%r59, %r35, 1;
	add.s32 	%r60, %r3, %r59;
	mul.wide.s32 	%rd34, %r60, 2;
	add.s64 	%rd35, %rd34, %rd5;
	shl.b64 	%rd36, %rd35, 1;
	add.s64 	%rd71, %rd73, %rd36;
	mul.wide.s32 	%rd37, %r35, 2;
	mul.wide.s32 	%rd38, %r3, 2;
	add.s64 	%rd39, %rd37, %rd38;
	add.s64 	%rd40, %rd39, %rd5;
	shl.b64 	%rd41, %rd40, 1;
	add.s64 	%rd70, %rd73, %rd41;
	add.s64 	%rd42, %rd38, %rd5;
	shl.b64 	%rd43, %rd42, 1;
	add.s64 	%rd69, %rd73, %rd43;
	add.s64 	%rd44, %rd38, %rd4;
	shl.b64 	%rd45, %rd44, 1;
	add.s64 	%rd68, %rd2, %rd45;
	mov.u32 	%r284, 0;
	mov.u32 	%r285, %r284;
	mov.u32 	%r279, %r3;

$L__BB114_5:
	.pragma "nounroll";
	ld.global.nc.u32 	%r62, [%rd68];
	ld.global.nc.u32 	%r63, [%rd69];
	// begin inline asm
	{mul.f16x2 %r61,%r62,%r63;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r286,%r286,%r61;
}
	// end inline asm
	ld.global.nc.u32 	%r69, [%rd70];
	// begin inline asm
	{mul.f16x2 %r67,%r62,%r69;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r285,%r285,%r67;
}
	// end inline asm
	ld.global.nc.u32 	%r75, [%rd71];
	// begin inline asm
	{mul.f16x2 %r73,%r62,%r75;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r284,%r284,%r73;
}
	// end inline asm
	add.s32 	%r279, %r279, 224;
	add.s64 	%rd71, %rd71, 896;
	add.s64 	%rd70, %rd70, 896;
	add.s64 	%rd69, %rd69, 896;
	add.s64 	%rd68, %rd68, 896;
	add.s32 	%r275, %r275, -1;
	setp.ne.s32 	%p4, %r275, 0;
	@%p4 bra 	$L__BB114_5;

$L__BB114_6:
	setp.lt.u32 	%p5, %r6, 672;
	@%p5 bra 	$L__BB114_9;

	add.s32 	%r79, %r279, %r35;
	shl.b32 	%r80, %r35, 1;
	add.s32 	%r81, %r279, %r80;
	add.s32 	%r82, %r79, 224;
	mul.wide.s32 	%rd46, %r82, 4;
	shl.b64 	%rd47, %rd5, 1;
	add.s64 	%rd19, %rd46, %rd47;
	mul.wide.s32 	%rd48, %r81, 4;
	add.s64 	%rd20, %rd48, %rd47;
	mul.wide.s32 	%rd49, %r279, 2;
	add.s64 	%rd50, %rd49, %rd4;
	shl.b64 	%rd51, %rd50, 1;
	add.s64 	%rd52, %rd2, %rd51;
	add.s64 	%rd72, %rd52, 1792;
	mul.wide.s32 	%rd53, %r279, 4;
	add.s64 	%rd22, %rd53, %rd47;
	mul.wide.s32 	%rd54, %r35, 4;
	add.s64 	%rd55, %rd53, %rd54;
	add.s64 	%rd23, %rd55, %rd47;

$L__BB114_8:
	ld.global.nc.u32 	%r84, [%rd72+-1792];
	add.s64 	%rd56, %rd73, %rd22;
	ld.global.nc.u32 	%r85, [%rd56];
	// begin inline asm
	{mul.f16x2 %r83,%r84,%r85;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r86,%r286,%r83;
}
	// end inline asm
	add.s64 	%rd57, %rd73, %rd23;
	ld.global.nc.u32 	%r91, [%rd57];
	// begin inline asm
	{mul.f16x2 %r89,%r84,%r91;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r92,%r285,%r89;
}
	// end inline asm
	add.s64 	%rd58, %rd73, %rd20;
	ld.global.nc.u32 	%r97, [%rd58];
	// begin inline asm
	{mul.f16x2 %r95,%r84,%r97;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r98,%r284,%r95;
}
	// end inline asm
	ld.global.nc.u32 	%r102, [%rd72+-896];
	ld.global.nc.u32 	%r103, [%rd56+896];
	// begin inline asm
	{mul.f16x2 %r101,%r102,%r103;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r104,%r86,%r101;
}
	// end inline asm
	add.s64 	%rd59, %rd73, %rd19;
	ld.global.nc.u32 	%r109, [%rd59];
	// begin inline asm
	{mul.f16x2 %r107,%r102,%r109;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r110,%r92,%r107;
}
	// end inline asm
	ld.global.nc.u32 	%r115, [%rd58+896];
	// begin inline asm
	{mul.f16x2 %r113,%r102,%r115;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r116,%r98,%r113;
}
	// end inline asm
	ld.global.nc.u32 	%r120, [%rd72];
	ld.global.nc.u32 	%r121, [%rd56+1792];
	// begin inline asm
	{mul.f16x2 %r119,%r120,%r121;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r122,%r104,%r119;
}
	// end inline asm
	ld.global.nc.u32 	%r127, [%rd59+896];
	// begin inline asm
	{mul.f16x2 %r125,%r120,%r127;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r128,%r110,%r125;
}
	// end inline asm
	ld.global.nc.u32 	%r133, [%rd58+1792];
	// begin inline asm
	{mul.f16x2 %r131,%r120,%r133;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r134,%r116,%r131;
}
	// end inline asm
	ld.global.nc.u32 	%r138, [%rd72+896];
	ld.global.nc.u32 	%r139, [%rd56+2688];
	// begin inline asm
	{mul.f16x2 %r137,%r138,%r139;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r286,%r122,%r137;
}
	// end inline asm
	ld.global.nc.u32 	%r145, [%rd59+1792];
	// begin inline asm
	{mul.f16x2 %r143,%r138,%r145;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r285,%r128,%r143;
}
	// end inline asm
	ld.global.nc.u32 	%r151, [%rd58+2688];
	// begin inline asm
	{mul.f16x2 %r149,%r138,%r151;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r284,%r134,%r149;
}
	// end inline asm
	add.s64 	%rd73, %rd73, 3584;
	add.s64 	%rd72, %rd72, 3584;
	add.s32 	%r279, %r279, 896;
	setp.lt.s32 	%p6, %r279, %r34;
	@%p6 bra 	$L__BB114_8;

$L__BB114_9:
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r286;
  cvt.f32.f16 %f5, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r286;
  cvt.f32.f16 %f6, high;}

	// end inline asm
	add.f32 	%f11, %f5, %f6;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r285;
  cvt.f32.f16 %f7, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r285;
  cvt.f32.f16 %f8, high;}

	// end inline asm
	add.f32 	%f1, %f7, %f8;
	st.local.f32 	[%rd3+4], %f1;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r284;
  cvt.f32.f16 %f9, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r284;
  cvt.f32.f16 %f10, high;}

	// end inline asm
	add.f32 	%f2, %f9, %f10;
	st.local.f32 	[%rd3+8], %f2;
	shr.s32 	%r161, %r3, 31;
	shr.u32 	%r162, %r161, 27;
	add.s32 	%r163, %r3, %r162;
	shr.s32 	%r164, %r163, 5;
	shl.b32 	%r165, %r164, 2;
	add.s32 	%r33, %r47, %r165;
	mov.u32 	%r167, 2;
	mov.b32 	%r168, %f11;
	mov.u32 	%r169, 31;
	mov.u32 	%r170, 16;
	mov.u32 	%r171, -1;
	shfl.sync.bfly.b32 	%r172|%p7, %r168, %r170, %r169, %r171;
	mov.b32 	%f12, %r172;
	add.f32 	%f13, %f11, %f12;
	mov.b32 	%r173, %f13;
	mov.u32 	%r174, 8;
	shfl.sync.bfly.b32 	%r175|%p8, %r173, %r174, %r169, %r171;
	mov.b32 	%f14, %r175;
	add.f32 	%f15, %f13, %f14;
	mov.b32 	%r176, %f15;
	mov.u32 	%r177, 4;
	shfl.sync.bfly.b32 	%r178|%p9, %r176, %r177, %r169, %r171;
	mov.b32 	%f16, %r178;
	add.f32 	%f17, %f15, %f16;
	mov.b32 	%r179, %f17;
	shfl.sync.bfly.b32 	%r180|%p10, %r179, %r167, %r169, %r171;
	mov.b32 	%f18, %r180;
	add.f32 	%f19, %f17, %f18;
	mov.b32 	%r181, %f19;
	mov.u32 	%r182, 1;
	shfl.sync.bfly.b32 	%r183|%p11, %r181, %r182, %r169, %r171;
	mov.b32 	%f20, %r183;
	add.f32 	%f21, %f19, %f20;
	st.local.f32 	[%rd3], %f21;
	st.shared.f32 	[%r33], %f21;
	bar.sync 	0;
	@%p1 bra 	$L__BB114_11;

	ld.shared.f32 	%f22, [%r4];
	mov.b32 	%r184, %f22;
	shfl.sync.bfly.b32 	%r188|%p13, %r184, %r170, %r169, %r171;
	mov.b32 	%f23, %r188;
	add.f32 	%f24, %f22, %f23;
	mov.b32 	%r189, %f24;
	shfl.sync.bfly.b32 	%r191|%p14, %r189, %r174, %r169, %r171;
	mov.b32 	%f25, %r191;
	add.f32 	%f26, %f24, %f25;
	mov.b32 	%r192, %f26;
	shfl.sync.bfly.b32 	%r194|%p15, %r192, %r177, %r169, %r171;
	mov.b32 	%f27, %r194;
	add.f32 	%f28, %f26, %f27;
	mov.b32 	%r195, %f28;
	shfl.sync.bfly.b32 	%r197|%p16, %r195, %r167, %r169, %r171;
	mov.b32 	%f29, %r197;
	add.f32 	%f30, %f28, %f29;
	mov.b32 	%r198, %f30;
	shfl.sync.bfly.b32 	%r200|%p17, %r198, %r182, %r169, %r171;
	mov.b32 	%f31, %r200;
	add.f32 	%f32, %f30, %f31;
	st.local.f32 	[%rd3], %f32;

$L__BB114_11:
	bar.sync 	0;
	mov.b32 	%r201, %f1;
	shfl.sync.bfly.b32 	%r205|%p19, %r201, %r170, %r169, %r171;
	mov.b32 	%f33, %r205;
	add.f32 	%f34, %f1, %f33;
	mov.b32 	%r206, %f34;
	shfl.sync.bfly.b32 	%r208|%p20, %r206, %r174, %r169, %r171;
	mov.b32 	%f35, %r208;
	add.f32 	%f36, %f34, %f35;
	mov.b32 	%r209, %f36;
	shfl.sync.bfly.b32 	%r211|%p21, %r209, %r177, %r169, %r171;
	mov.b32 	%f37, %r211;
	add.f32 	%f38, %f36, %f37;
	mov.b32 	%r212, %f38;
	shfl.sync.bfly.b32 	%r214|%p22, %r212, %r167, %r169, %r171;
	mov.b32 	%f39, %r214;
	add.f32 	%f40, %f38, %f39;
	mov.b32 	%r215, %f40;
	shfl.sync.bfly.b32 	%r217|%p23, %r215, %r182, %r169, %r171;
	mov.b32 	%f41, %r217;
	add.f32 	%f42, %f40, %f41;
	st.local.f32 	[%rd3+4], %f42;
	st.shared.f32 	[%r33], %f42;
	bar.sync 	0;
	@%p1 bra 	$L__BB114_13;

	ld.shared.f32 	%f43, [%r4];
	mov.b32 	%r218, %f43;
	mov.u32 	%r219, 31;
	mov.u32 	%r220, 16;
	mov.u32 	%r221, -1;
	shfl.sync.bfly.b32 	%r222|%p24, %r218, %r220, %r219, %r221;
	mov.b32 	%f44, %r222;
	add.f32 	%f45, %f43, %f44;
	mov.b32 	%r223, %f45;
	mov.u32 	%r224, 8;
	shfl.sync.bfly.b32 	%r225|%p25, %r223, %r224, %r219, %r221;
	mov.b32 	%f46, %r225;
	add.f32 	%f47, %f45, %f46;
	mov.b32 	%r226, %f47;
	mov.u32 	%r227, 4;
	shfl.sync.bfly.b32 	%r228|%p26, %r226, %r227, %r219, %r221;
	mov.b32 	%f48, %r228;
	add.f32 	%f49, %f47, %f48;
	mov.b32 	%r229, %f49;
	mov.u32 	%r230, 2;
	shfl.sync.bfly.b32 	%r231|%p27, %r229, %r230, %r219, %r221;
	mov.b32 	%f50, %r231;
	add.f32 	%f51, %f49, %f50;
	mov.b32 	%r232, %f51;
	mov.u32 	%r233, 1;
	shfl.sync.bfly.b32 	%r234|%p28, %r232, %r233, %r219, %r221;
	mov.b32 	%f52, %r234;
	add.f32 	%f53, %f51, %f52;
	st.local.f32 	[%rd3+4], %f53;

$L__BB114_13:
	bar.sync 	0;
	mov.b32 	%r235, %f2;
	mov.u32 	%r236, 31;
	mov.u32 	%r237, 16;
	mov.u32 	%r238, -1;
	shfl.sync.bfly.b32 	%r239|%p30, %r235, %r237, %r236, %r238;
	mov.b32 	%f54, %r239;
	add.f32 	%f55, %f2, %f54;
	mov.b32 	%r240, %f55;
	mov.u32 	%r241, 8;
	shfl.sync.bfly.b32 	%r242|%p31, %r240, %r241, %r236, %r238;
	mov.b32 	%f56, %r242;
	add.f32 	%f57, %f55, %f56;
	mov.b32 	%r243, %f57;
	mov.u32 	%r244, 4;
	shfl.sync.bfly.b32 	%r245|%p32, %r243, %r244, %r236, %r238;
	mov.b32 	%f58, %r245;
	add.f32 	%f59, %f57, %f58;
	mov.b32 	%r246, %f59;
	mov.u32 	%r247, 2;
	shfl.sync.bfly.b32 	%r248|%p33, %r246, %r247, %r236, %r238;
	mov.b32 	%f60, %r248;
	add.f32 	%f61, %f59, %f60;
	mov.b32 	%r249, %f61;
	mov.u32 	%r250, 1;
	shfl.sync.bfly.b32 	%r251|%p34, %r249, %r250, %r236, %r238;
	mov.b32 	%f62, %r251;
	add.f32 	%f63, %f61, %f62;
	st.local.f32 	[%rd3+8], %f63;
	st.shared.f32 	[%r33], %f63;
	bar.sync 	0;
	@%p1 bra 	$L__BB114_15;

	ld.shared.f32 	%f64, [%r4];
	mov.b32 	%r252, %f64;
	shfl.sync.bfly.b32 	%r256|%p35, %r252, %r237, %r236, %r238;
	mov.b32 	%f65, %r256;
	add.f32 	%f66, %f64, %f65;
	mov.b32 	%r257, %f66;
	shfl.sync.bfly.b32 	%r259|%p36, %r257, %r241, %r236, %r238;
	mov.b32 	%f67, %r259;
	add.f32 	%f68, %f66, %f67;
	mov.b32 	%r260, %f68;
	shfl.sync.bfly.b32 	%r262|%p37, %r260, %r244, %r236, %r238;
	mov.b32 	%f69, %r262;
	add.f32 	%f70, %f68, %f69;
	mov.b32 	%r263, %f70;
	shfl.sync.bfly.b32 	%r265|%p38, %r263, %r247, %r236, %r238;
	mov.b32 	%f71, %r265;
	add.f32 	%f72, %f70, %f71;
	mov.b32 	%r266, %f72;
	shfl.sync.bfly.b32 	%r268|%p39, %r266, %r250, %r236, %r238;
	mov.b32 	%f73, %r268;
	add.f32 	%f74, %f72, %f73;
	st.local.f32 	[%rd3+8], %f74;

$L__BB114_15:
	bar.sync 	0;
	setp.gt.s32 	%p40, %r3, 2;
	@%p40 bra 	$L__BB114_17;

	mad.lo.s32 	%r269, %r3, %r36, %r2;
	cvt.s64.s32 	%rd60, %r269;
	mul.lo.s32 	%r270, %r1, %r37;
	cvt.s64.s32 	%rd61, %r270;
	add.s64 	%rd62, %rd61, %rd60;
	mul.wide.s32 	%rd63, %r3, 4;
	add.s64 	%rd64, %rd3, %rd63;
	ld.local.f32 	%f75, [%rd64];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs3, %f75;}

	// end inline asm
	cvta.to.global.u64 	%rd65, %rd28;
	shl.b64 	%rd66, %rd62, 1;
	add.s64 	%rd67, %rd65, %rd66;
	st.global.u16 	[%rd67], %rs3;

$L__BB114_17:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_4_bs_224
.visible .entry ggml_matvec_f16_ncols_4_bs_224(
	.param .u64 ggml_matvec_f16_ncols_4_bs_224_param_0,
	.param .u64 ggml_matvec_f16_ncols_4_bs_224_param_1,
	.param .u64 ggml_matvec_f16_ncols_4_bs_224_param_2,
	.param .u32 ggml_matvec_f16_ncols_4_bs_224_param_3,
	.param .u32 ggml_matvec_f16_ncols_4_bs_224_param_4,
	.param .u32 ggml_matvec_f16_ncols_4_bs_224_param_5,
	.param .u32 ggml_matvec_f16_ncols_4_bs_224_param_6,
	.param .u32 ggml_matvec_f16_ncols_4_bs_224_param_7,
	.param .u32 ggml_matvec_f16_ncols_4_bs_224_param_8,
	.param .u32 ggml_matvec_f16_ncols_4_bs_224_param_9,
	.param .u32 ggml_matvec_f16_ncols_4_bs_224_param_10,
	.param .u32 ggml_matvec_f16_ncols_4_bs_224_param_11
)
{
	.local .align 16 .b8 	__local_depot115[16];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<52>;
	.reg .b16 	%rs<4>;
	.reg .f32 	%f<100>;
	.reg .b32 	%r<368>;
	.reg .b64 	%rd<85>;


	mov.u64 	%SPL, __local_depot115;
	ld.param.u64 	%rd34, [ggml_matvec_f16_ncols_4_bs_224_param_0];
	ld.param.u64 	%rd35, [ggml_matvec_f16_ncols_4_bs_224_param_1];
	ld.param.u64 	%rd33, [ggml_matvec_f16_ncols_4_bs_224_param_2];
	ld.param.u32 	%r40, [ggml_matvec_f16_ncols_4_bs_224_param_3];
	ld.param.u32 	%r44, [ggml_matvec_f16_ncols_4_bs_224_param_5];
	ld.param.u32 	%r41, [ggml_matvec_f16_ncols_4_bs_224_param_6];
	ld.param.u32 	%r42, [ggml_matvec_f16_ncols_4_bs_224_param_7];
	ld.param.u32 	%r45, [ggml_matvec_f16_ncols_4_bs_224_param_8];
	ld.param.u32 	%r46, [ggml_matvec_f16_ncols_4_bs_224_param_9];
	ld.param.u32 	%r47, [ggml_matvec_f16_ncols_4_bs_224_param_10];
	ld.param.u32 	%r43, [ggml_matvec_f16_ncols_4_bs_224_param_11];
	cvta.to.global.u64 	%rd84, %rd35;
	cvta.to.global.u64 	%rd2, %rd34;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r48, %r1, %r45;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r49, %r2, %r44;
	mad.lo.s32 	%r50, %r48, %r46, %r49;
	cvt.s64.s32 	%rd4, %r50;
	mul.lo.s32 	%r51, %r1, %r47;
	cvt.s64.s32 	%rd5, %r51;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r52, %r3, 2;
	mov.u32 	%r53, data_mmv;
	add.s32 	%r4, %r53, %r52;
	@%p1 bra 	$L__BB115_2;

	mov.u32 	%r54, 0;
	st.shared.u32 	[%r4], %r54;

$L__BB115_2:
	bar.sync 	0;
	mov.f32 	%f5, 0f00000000;
	st.local.v4.f32 	[%rd3], {%f5, %f5, %f5, %f5};
	mov.u32 	%r364, 0;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs2, %f5;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f5;}

	// end inline asm
	mov.b32 	%r367, {%rs1, %rs2};
	setp.ge.s32 	%p2, %r3, %r40;
	mov.u32 	%r365, %r364;
	mov.u32 	%r366, %r364;
	@%p2 bra 	$L__BB115_9;

	not.b32 	%r61, %r3;
	add.s32 	%r6, %r61, %r40;
	shr.u32 	%r62, %r6, 5;
	mul.wide.u32 	%rd37, %r62, 613566757;
	shr.u64 	%rd38, %rd37, 32;
	cvt.u32.u64 	%r63, %rd38;
	add.s32 	%r64, %r63, 1;
	and.b32  	%r353, %r64, 3;
	setp.eq.s32 	%p3, %r353, 0;
	mov.u32 	%r364, 0;
	mov.u32 	%r358, %r3;
	@%p3 bra 	$L__BB115_6;

	shl.b32 	%r68, %r41, 1;
	add.s32 	%r69, %r3, %r68;
	mul.wide.s32 	%rd39, %r69, 2;
	add.s64 	%rd40, %rd39, %rd5;
	shl.b64 	%rd41, %rd40, 1;
	add.s64 	%rd82, %rd84, %rd41;
	mad.lo.s32 	%r70, %r41, 3, %r3;
	mul.wide.s32 	%rd42, %r70, 2;
	add.s64 	%rd43, %rd42, %rd5;
	shl.b64 	%rd44, %rd43, 1;
	add.s64 	%rd81, %rd84, %rd44;
	mul.wide.s32 	%rd45, %r41, 2;
	mul.wide.s32 	%rd46, %r3, 2;
	add.s64 	%rd47, %rd45, %rd46;
	add.s64 	%rd48, %rd47, %rd5;
	shl.b64 	%rd49, %rd48, 1;
	add.s64 	%rd80, %rd84, %rd49;
	add.s64 	%rd50, %rd46, %rd5;
	shl.b64 	%rd51, %rd50, 1;
	add.s64 	%rd79, %rd84, %rd51;
	add.s64 	%rd52, %rd46, %rd4;
	shl.b64 	%rd53, %rd52, 1;
	add.s64 	%rd78, %rd2, %rd53;
	mov.u32 	%r364, 0;
	mov.u32 	%r365, %r364;
	mov.u32 	%r366, %r364;
	mov.u32 	%r358, %r3;

$L__BB115_5:
	.pragma "nounroll";
	ld.global.nc.u32 	%r72, [%rd78];
	ld.global.nc.u32 	%r73, [%rd79];
	// begin inline asm
	{mul.f16x2 %r71,%r72,%r73;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r367,%r367,%r71;
}
	// end inline asm
	ld.global.nc.u32 	%r79, [%rd80];
	// begin inline asm
	{mul.f16x2 %r77,%r72,%r79;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r366,%r366,%r77;
}
	// end inline asm
	ld.global.nc.u32 	%r85, [%rd82];
	// begin inline asm
	{mul.f16x2 %r83,%r72,%r85;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r365,%r365,%r83;
}
	// end inline asm
	ld.global.nc.u32 	%r91, [%rd81];
	// begin inline asm
	{mul.f16x2 %r89,%r72,%r91;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r364,%r364,%r89;
}
	// end inline asm
	add.s32 	%r358, %r358, 224;
	add.s64 	%rd82, %rd82, 896;
	add.s64 	%rd81, %rd81, 896;
	add.s64 	%rd80, %rd80, 896;
	add.s64 	%rd79, %rd79, 896;
	add.s64 	%rd78, %rd78, 896;
	add.s32 	%r353, %r353, -1;
	setp.ne.s32 	%p4, %r353, 0;
	@%p4 bra 	$L__BB115_5;

$L__BB115_6:
	setp.lt.u32 	%p5, %r6, 672;
	@%p5 bra 	$L__BB115_9;

	add.s32 	%r95, %r358, %r41;
	shl.b32 	%r96, %r41, 1;
	add.s32 	%r97, %r358, %r96;
	mad.lo.s32 	%r98, %r41, 3, %r358;
	add.s32 	%r99, %r95, 224;
	mul.wide.s32 	%rd54, %r99, 4;
	shl.b64 	%rd55, %rd5, 1;
	add.s64 	%rd23, %rd54, %rd55;
	mul.wide.s32 	%rd56, %r97, 4;
	add.s64 	%rd24, %rd56, %rd55;
	mul.wide.s32 	%rd57, %r98, 4;
	add.s64 	%rd25, %rd57, %rd55;
	mul.wide.s32 	%rd58, %r358, 2;
	add.s64 	%rd59, %rd58, %rd4;
	shl.b64 	%rd60, %rd59, 1;
	add.s64 	%rd61, %rd2, %rd60;
	add.s64 	%rd83, %rd61, 1792;
	mul.wide.s32 	%rd62, %r358, 4;
	add.s64 	%rd27, %rd62, %rd55;
	mul.wide.s32 	%rd63, %r41, 4;
	add.s64 	%rd64, %rd62, %rd63;
	add.s64 	%rd28, %rd64, %rd55;

$L__BB115_8:
	ld.global.nc.u32 	%r101, [%rd83+-1792];
	add.s64 	%rd65, %rd84, %rd27;
	ld.global.nc.u32 	%r102, [%rd65];
	// begin inline asm
	{mul.f16x2 %r100,%r101,%r102;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r103,%r367,%r100;
}
	// end inline asm
	add.s64 	%rd66, %rd84, %rd28;
	ld.global.nc.u32 	%r108, [%rd66];
	// begin inline asm
	{mul.f16x2 %r106,%r101,%r108;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r109,%r366,%r106;
}
	// end inline asm
	add.s64 	%rd67, %rd84, %rd24;
	ld.global.nc.u32 	%r114, [%rd67];
	// begin inline asm
	{mul.f16x2 %r112,%r101,%r114;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r115,%r365,%r112;
}
	// end inline asm
	add.s64 	%rd68, %rd84, %rd25;
	ld.global.nc.u32 	%r120, [%rd68];
	// begin inline asm
	{mul.f16x2 %r118,%r101,%r120;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r121,%r364,%r118;
}
	// end inline asm
	ld.global.nc.u32 	%r125, [%rd83+-896];
	ld.global.nc.u32 	%r126, [%rd65+896];
	// begin inline asm
	{mul.f16x2 %r124,%r125,%r126;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r127,%r103,%r124;
}
	// end inline asm
	add.s64 	%rd69, %rd84, %rd23;
	ld.global.nc.u32 	%r132, [%rd69];
	// begin inline asm
	{mul.f16x2 %r130,%r125,%r132;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r133,%r109,%r130;
}
	// end inline asm
	ld.global.nc.u32 	%r138, [%rd67+896];
	// begin inline asm
	{mul.f16x2 %r136,%r125,%r138;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r139,%r115,%r136;
}
	// end inline asm
	ld.global.nc.u32 	%r144, [%rd68+896];
	// begin inline asm
	{mul.f16x2 %r142,%r125,%r144;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r145,%r121,%r142;
}
	// end inline asm
	ld.global.nc.u32 	%r149, [%rd83];
	ld.global.nc.u32 	%r150, [%rd65+1792];
	// begin inline asm
	{mul.f16x2 %r148,%r149,%r150;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r151,%r127,%r148;
}
	// end inline asm
	ld.global.nc.u32 	%r156, [%rd69+896];
	// begin inline asm
	{mul.f16x2 %r154,%r149,%r156;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r157,%r133,%r154;
}
	// end inline asm
	ld.global.nc.u32 	%r162, [%rd67+1792];
	// begin inline asm
	{mul.f16x2 %r160,%r149,%r162;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r163,%r139,%r160;
}
	// end inline asm
	ld.global.nc.u32 	%r168, [%rd68+1792];
	// begin inline asm
	{mul.f16x2 %r166,%r149,%r168;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r169,%r145,%r166;
}
	// end inline asm
	ld.global.nc.u32 	%r173, [%rd83+896];
	ld.global.nc.u32 	%r174, [%rd65+2688];
	// begin inline asm
	{mul.f16x2 %r172,%r173,%r174;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r367,%r151,%r172;
}
	// end inline asm
	ld.global.nc.u32 	%r180, [%rd69+1792];
	// begin inline asm
	{mul.f16x2 %r178,%r173,%r180;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r366,%r157,%r178;
}
	// end inline asm
	ld.global.nc.u32 	%r186, [%rd67+2688];
	// begin inline asm
	{mul.f16x2 %r184,%r173,%r186;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r365,%r163,%r184;
}
	// end inline asm
	ld.global.nc.u32 	%r192, [%rd68+2688];
	// begin inline asm
	{mul.f16x2 %r190,%r173,%r192;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r364,%r169,%r190;
}
	// end inline asm
	add.s64 	%rd84, %rd84, 3584;
	add.s64 	%rd83, %rd83, 3584;
	add.s32 	%r358, %r358, 896;
	setp.lt.s32 	%p6, %r358, %r40;
	@%p6 bra 	$L__BB115_8;

$L__BB115_9:
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r367;
  cvt.f32.f16 %f6, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r367;
  cvt.f32.f16 %f7, high;}

	// end inline asm
	add.f32 	%f14, %f6, %f7;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r366;
  cvt.f32.f16 %f8, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r366;
  cvt.f32.f16 %f9, high;}

	// end inline asm
	add.f32 	%f1, %f8, %f9;
	st.local.f32 	[%rd3+4], %f1;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r365;
  cvt.f32.f16 %f10, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r365;
  cvt.f32.f16 %f11, high;}

	// end inline asm
	add.f32 	%f2, %f10, %f11;
	st.local.f32 	[%rd3+8], %f2;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r364;
  cvt.f32.f16 %f12, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r364;
  cvt.f32.f16 %f13, high;}

	// end inline asm
	add.f32 	%f3, %f12, %f13;
	st.local.f32 	[%rd3+12], %f3;
	shr.s32 	%r204, %r3, 31;
	shr.u32 	%r205, %r204, 27;
	add.s32 	%r206, %r3, %r205;
	shr.s32 	%r207, %r206, 5;
	shl.b32 	%r208, %r207, 2;
	add.s32 	%r39, %r53, %r208;
	mov.u32 	%r210, 2;
	mov.b32 	%r211, %f14;
	mov.u32 	%r212, 31;
	mov.u32 	%r213, 16;
	mov.u32 	%r214, -1;
	shfl.sync.bfly.b32 	%r215|%p7, %r211, %r213, %r212, %r214;
	mov.b32 	%f15, %r215;
	add.f32 	%f16, %f14, %f15;
	mov.b32 	%r216, %f16;
	mov.u32 	%r217, 8;
	shfl.sync.bfly.b32 	%r218|%p8, %r216, %r217, %r212, %r214;
	mov.b32 	%f17, %r218;
	add.f32 	%f18, %f16, %f17;
	mov.b32 	%r219, %f18;
	mov.u32 	%r220, 4;
	shfl.sync.bfly.b32 	%r221|%p9, %r219, %r220, %r212, %r214;
	mov.b32 	%f19, %r221;
	add.f32 	%f20, %f18, %f19;
	mov.b32 	%r222, %f20;
	shfl.sync.bfly.b32 	%r223|%p10, %r222, %r210, %r212, %r214;
	mov.b32 	%f21, %r223;
	add.f32 	%f22, %f20, %f21;
	mov.b32 	%r224, %f22;
	mov.u32 	%r225, 1;
	shfl.sync.bfly.b32 	%r226|%p11, %r224, %r225, %r212, %r214;
	mov.b32 	%f23, %r226;
	add.f32 	%f24, %f22, %f23;
	st.local.f32 	[%rd3], %f24;
	st.shared.f32 	[%r39], %f24;
	bar.sync 	0;
	@%p1 bra 	$L__BB115_11;

	ld.shared.f32 	%f25, [%r4];
	mov.b32 	%r227, %f25;
	shfl.sync.bfly.b32 	%r231|%p13, %r227, %r213, %r212, %r214;
	mov.b32 	%f26, %r231;
	add.f32 	%f27, %f25, %f26;
	mov.b32 	%r232, %f27;
	shfl.sync.bfly.b32 	%r234|%p14, %r232, %r217, %r212, %r214;
	mov.b32 	%f28, %r234;
	add.f32 	%f29, %f27, %f28;
	mov.b32 	%r235, %f29;
	shfl.sync.bfly.b32 	%r237|%p15, %r235, %r220, %r212, %r214;
	mov.b32 	%f30, %r237;
	add.f32 	%f31, %f29, %f30;
	mov.b32 	%r238, %f31;
	shfl.sync.bfly.b32 	%r240|%p16, %r238, %r210, %r212, %r214;
	mov.b32 	%f32, %r240;
	add.f32 	%f33, %f31, %f32;
	mov.b32 	%r241, %f33;
	shfl.sync.bfly.b32 	%r243|%p17, %r241, %r225, %r212, %r214;
	mov.b32 	%f34, %r243;
	add.f32 	%f35, %f33, %f34;
	st.local.f32 	[%rd3], %f35;

$L__BB115_11:
	bar.sync 	0;
	mov.b32 	%r244, %f1;
	shfl.sync.bfly.b32 	%r248|%p19, %r244, %r213, %r212, %r214;
	mov.b32 	%f36, %r248;
	add.f32 	%f37, %f1, %f36;
	mov.b32 	%r249, %f37;
	shfl.sync.bfly.b32 	%r251|%p20, %r249, %r217, %r212, %r214;
	mov.b32 	%f38, %r251;
	add.f32 	%f39, %f37, %f38;
	mov.b32 	%r252, %f39;
	shfl.sync.bfly.b32 	%r254|%p21, %r252, %r220, %r212, %r214;
	mov.b32 	%f40, %r254;
	add.f32 	%f41, %f39, %f40;
	mov.b32 	%r255, %f41;
	shfl.sync.bfly.b32 	%r257|%p22, %r255, %r210, %r212, %r214;
	mov.b32 	%f42, %r257;
	add.f32 	%f43, %f41, %f42;
	mov.b32 	%r258, %f43;
	shfl.sync.bfly.b32 	%r260|%p23, %r258, %r225, %r212, %r214;
	mov.b32 	%f44, %r260;
	add.f32 	%f45, %f43, %f44;
	st.local.f32 	[%rd3+4], %f45;
	st.shared.f32 	[%r39], %f45;
	bar.sync 	0;
	@%p1 bra 	$L__BB115_13;

	ld.shared.f32 	%f46, [%r4];
	mov.b32 	%r261, %f46;
	mov.u32 	%r262, 31;
	mov.u32 	%r263, 16;
	mov.u32 	%r264, -1;
	shfl.sync.bfly.b32 	%r265|%p24, %r261, %r263, %r262, %r264;
	mov.b32 	%f47, %r265;
	add.f32 	%f48, %f46, %f47;
	mov.b32 	%r266, %f48;
	mov.u32 	%r267, 8;
	shfl.sync.bfly.b32 	%r268|%p25, %r266, %r267, %r262, %r264;
	mov.b32 	%f49, %r268;
	add.f32 	%f50, %f48, %f49;
	mov.b32 	%r269, %f50;
	mov.u32 	%r270, 4;
	shfl.sync.bfly.b32 	%r271|%p26, %r269, %r270, %r262, %r264;
	mov.b32 	%f51, %r271;
	add.f32 	%f52, %f50, %f51;
	mov.b32 	%r272, %f52;
	mov.u32 	%r273, 2;
	shfl.sync.bfly.b32 	%r274|%p27, %r272, %r273, %r262, %r264;
	mov.b32 	%f53, %r274;
	add.f32 	%f54, %f52, %f53;
	mov.b32 	%r275, %f54;
	mov.u32 	%r276, 1;
	shfl.sync.bfly.b32 	%r277|%p28, %r275, %r276, %r262, %r264;
	mov.b32 	%f55, %r277;
	add.f32 	%f56, %f54, %f55;
	st.local.f32 	[%rd3+4], %f56;

$L__BB115_13:
	bar.sync 	0;
	mov.b32 	%r278, %f2;
	mov.u32 	%r279, 31;
	mov.u32 	%r280, 16;
	mov.u32 	%r281, -1;
	shfl.sync.bfly.b32 	%r282|%p30, %r278, %r280, %r279, %r281;
	mov.b32 	%f57, %r282;
	add.f32 	%f58, %f2, %f57;
	mov.b32 	%r283, %f58;
	mov.u32 	%r284, 8;
	shfl.sync.bfly.b32 	%r285|%p31, %r283, %r284, %r279, %r281;
	mov.b32 	%f59, %r285;
	add.f32 	%f60, %f58, %f59;
	mov.b32 	%r286, %f60;
	mov.u32 	%r287, 4;
	shfl.sync.bfly.b32 	%r288|%p32, %r286, %r287, %r279, %r281;
	mov.b32 	%f61, %r288;
	add.f32 	%f62, %f60, %f61;
	mov.b32 	%r289, %f62;
	mov.u32 	%r290, 2;
	shfl.sync.bfly.b32 	%r291|%p33, %r289, %r290, %r279, %r281;
	mov.b32 	%f63, %r291;
	add.f32 	%f64, %f62, %f63;
	mov.b32 	%r292, %f64;
	mov.u32 	%r293, 1;
	shfl.sync.bfly.b32 	%r294|%p34, %r292, %r293, %r279, %r281;
	mov.b32 	%f65, %r294;
	add.f32 	%f66, %f64, %f65;
	st.local.f32 	[%rd3+8], %f66;
	st.shared.f32 	[%r39], %f66;
	bar.sync 	0;
	@%p1 bra 	$L__BB115_15;

	ld.shared.f32 	%f67, [%r4];
	mov.b32 	%r295, %f67;
	shfl.sync.bfly.b32 	%r299|%p35, %r295, %r280, %r279, %r281;
	mov.b32 	%f68, %r299;
	add.f32 	%f69, %f67, %f68;
	mov.b32 	%r300, %f69;
	shfl.sync.bfly.b32 	%r302|%p36, %r300, %r284, %r279, %r281;
	mov.b32 	%f70, %r302;
	add.f32 	%f71, %f69, %f70;
	mov.b32 	%r303, %f71;
	shfl.sync.bfly.b32 	%r305|%p37, %r303, %r287, %r279, %r281;
	mov.b32 	%f72, %r305;
	add.f32 	%f73, %f71, %f72;
	mov.b32 	%r306, %f73;
	shfl.sync.bfly.b32 	%r308|%p38, %r306, %r290, %r279, %r281;
	mov.b32 	%f74, %r308;
	add.f32 	%f75, %f73, %f74;
	mov.b32 	%r309, %f75;
	shfl.sync.bfly.b32 	%r311|%p39, %r309, %r293, %r279, %r281;
	mov.b32 	%f76, %r311;
	add.f32 	%f77, %f75, %f76;
	st.local.f32 	[%rd3+8], %f77;

$L__BB115_15:
	bar.sync 	0;
	mov.b32 	%r312, %f3;
	shfl.sync.bfly.b32 	%r316|%p41, %r312, %r280, %r279, %r281;
	mov.b32 	%f78, %r316;
	add.f32 	%f79, %f3, %f78;
	mov.b32 	%r317, %f79;
	shfl.sync.bfly.b32 	%r319|%p42, %r317, %r284, %r279, %r281;
	mov.b32 	%f80, %r319;
	add.f32 	%f81, %f79, %f80;
	mov.b32 	%r320, %f81;
	shfl.sync.bfly.b32 	%r322|%p43, %r320, %r287, %r279, %r281;
	mov.b32 	%f82, %r322;
	add.f32 	%f83, %f81, %f82;
	mov.b32 	%r323, %f83;
	shfl.sync.bfly.b32 	%r325|%p44, %r323, %r290, %r279, %r281;
	mov.b32 	%f84, %r325;
	add.f32 	%f85, %f83, %f84;
	mov.b32 	%r326, %f85;
	shfl.sync.bfly.b32 	%r328|%p45, %r326, %r293, %r279, %r281;
	mov.b32 	%f86, %r328;
	add.f32 	%f87, %f85, %f86;
	st.local.f32 	[%rd3+12], %f87;
	st.shared.f32 	[%r39], %f87;
	bar.sync 	0;
	@%p1 bra 	$L__BB115_17;

	ld.shared.f32 	%f88, [%r4];
	mov.b32 	%r329, %f88;
	mov.u32 	%r330, 31;
	mov.u32 	%r331, 16;
	mov.u32 	%r332, -1;
	shfl.sync.bfly.b32 	%r333|%p46, %r329, %r331, %r330, %r332;
	mov.b32 	%f89, %r333;
	add.f32 	%f90, %f88, %f89;
	mov.b32 	%r334, %f90;
	mov.u32 	%r335, 8;
	shfl.sync.bfly.b32 	%r336|%p47, %r334, %r335, %r330, %r332;
	mov.b32 	%f91, %r336;
	add.f32 	%f92, %f90, %f91;
	mov.b32 	%r337, %f92;
	mov.u32 	%r338, 4;
	shfl.sync.bfly.b32 	%r339|%p48, %r337, %r338, %r330, %r332;
	mov.b32 	%f93, %r339;
	add.f32 	%f94, %f92, %f93;
	mov.b32 	%r340, %f94;
	mov.u32 	%r341, 2;
	shfl.sync.bfly.b32 	%r342|%p49, %r340, %r341, %r330, %r332;
	mov.b32 	%f95, %r342;
	add.f32 	%f96, %f94, %f95;
	mov.b32 	%r343, %f96;
	mov.u32 	%r344, 1;
	shfl.sync.bfly.b32 	%r345|%p50, %r343, %r344, %r330, %r332;
	mov.b32 	%f97, %r345;
	add.f32 	%f98, %f96, %f97;
	st.local.f32 	[%rd3+12], %f98;

$L__BB115_17:
	bar.sync 	0;
	setp.gt.s32 	%p51, %r3, 3;
	@%p51 bra 	$L__BB115_19;

	mad.lo.s32 	%r346, %r3, %r42, %r2;
	cvt.s64.s32 	%rd70, %r346;
	mul.lo.s32 	%r347, %r1, %r43;
	cvt.s64.s32 	%rd71, %r347;
	add.s64 	%rd72, %rd71, %rd70;
	mul.wide.s32 	%rd73, %r3, 4;
	add.s64 	%rd74, %rd3, %rd73;
	ld.local.f32 	%f99, [%rd74];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs3, %f99;}

	// end inline asm
	cvta.to.global.u64 	%rd75, %rd33;
	shl.b64 	%rd76, %rd72, 1;
	add.s64 	%rd77, %rd75, %rd76;
	st.global.u16 	[%rd77], %rs3;

$L__BB115_19:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_5_bs_224
.visible .entry ggml_matvec_f16_ncols_5_bs_224(
	.param .u64 ggml_matvec_f16_ncols_5_bs_224_param_0,
	.param .u64 ggml_matvec_f16_ncols_5_bs_224_param_1,
	.param .u64 ggml_matvec_f16_ncols_5_bs_224_param_2,
	.param .u32 ggml_matvec_f16_ncols_5_bs_224_param_3,
	.param .u32 ggml_matvec_f16_ncols_5_bs_224_param_4,
	.param .u32 ggml_matvec_f16_ncols_5_bs_224_param_5,
	.param .u32 ggml_matvec_f16_ncols_5_bs_224_param_6,
	.param .u32 ggml_matvec_f16_ncols_5_bs_224_param_7,
	.param .u32 ggml_matvec_f16_ncols_5_bs_224_param_8,
	.param .u32 ggml_matvec_f16_ncols_5_bs_224_param_9,
	.param .u32 ggml_matvec_f16_ncols_5_bs_224_param_10,
	.param .u32 ggml_matvec_f16_ncols_5_bs_224_param_11
)
{
	.local .align 4 .b8 	__local_depot116[20];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<63>;
	.reg .b16 	%rs<4>;
	.reg .f32 	%f<124>;
	.reg .b32 	%r<448>;
	.reg .b64 	%rd<76>;


	mov.u64 	%SPL, __local_depot116;
	ld.param.u64 	%rd28, [ggml_matvec_f16_ncols_5_bs_224_param_0];
	ld.param.u64 	%rd29, [ggml_matvec_f16_ncols_5_bs_224_param_1];
	ld.param.u64 	%rd27, [ggml_matvec_f16_ncols_5_bs_224_param_2];
	ld.param.u32 	%r46, [ggml_matvec_f16_ncols_5_bs_224_param_3];
	ld.param.u32 	%r50, [ggml_matvec_f16_ncols_5_bs_224_param_5];
	ld.param.u32 	%r47, [ggml_matvec_f16_ncols_5_bs_224_param_6];
	ld.param.u32 	%r48, [ggml_matvec_f16_ncols_5_bs_224_param_7];
	ld.param.u32 	%r51, [ggml_matvec_f16_ncols_5_bs_224_param_8];
	ld.param.u32 	%r52, [ggml_matvec_f16_ncols_5_bs_224_param_9];
	ld.param.u32 	%r53, [ggml_matvec_f16_ncols_5_bs_224_param_10];
	ld.param.u32 	%r49, [ggml_matvec_f16_ncols_5_bs_224_param_11];
	cvta.to.global.u64 	%rd75, %rd29;
	cvta.to.global.u64 	%rd2, %rd28;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r54, %r1, %r51;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r55, %r2, %r50;
	mad.lo.s32 	%r56, %r54, %r52, %r55;
	cvt.s64.s32 	%rd4, %r56;
	mul.lo.s32 	%r57, %r1, %r53;
	cvt.s64.s32 	%rd5, %r57;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r58, %r3, 2;
	mov.u32 	%r59, data_mmv;
	add.s32 	%r4, %r59, %r58;
	@%p1 bra 	$L__BB116_2;

	mov.u32 	%r60, 0;
	st.shared.u32 	[%r4], %r60;

$L__BB116_2:
	bar.sync 	0;
	mov.f32 	%f6, 0f00000000;
	mov.u32 	%r443, 0;
	st.local.u32 	[%rd3], %r443;
	st.local.u32 	[%rd3+4], %r443;
	st.local.u32 	[%rd3+8], %r443;
	st.local.u32 	[%rd3+12], %r443;
	st.local.u32 	[%rd3+16], %r443;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs2, %f6;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f6;}

	// end inline asm
	mov.b32 	%r447, {%rs1, %rs2};
	setp.ge.s32 	%p2, %r3, %r46;
	mov.u32 	%r444, %r443;
	mov.u32 	%r445, %r443;
	mov.u32 	%r446, %r443;
	@%p2 bra 	$L__BB116_9;

	not.b32 	%r69, %r3;
	add.s32 	%r6, %r69, %r46;
	shr.u32 	%r70, %r6, 5;
	mul.wide.u32 	%rd31, %r70, 613566757;
	shr.u64 	%rd32, %rd31, 32;
	cvt.u32.u64 	%r71, %rd32;
	add.s32 	%r72, %r71, 1;
	and.b32  	%r430, %r72, 3;
	setp.eq.s32 	%p3, %r430, 0;
	mov.u32 	%r443, 0;
	mov.u32 	%r436, %r3;
	@%p3 bra 	$L__BB116_6;

	shl.b32 	%r77, %r47, 1;
	mad.lo.s32 	%r78, %r47, 3, %r3;
	mul.wide.s32 	%rd33, %r78, 4;
	shl.b64 	%rd34, %rd5, 1;
	add.s64 	%rd7, %rd33, %rd34;
	mul.wide.s32 	%rd35, %r3, 4;
	mul.wide.s32 	%rd36, %r47, 4;
	add.s64 	%rd37, %rd35, %rd36;
	add.s64 	%rd8, %rd37, %rd34;
	add.s64 	%rd9, %rd35, %rd34;
	mul.wide.s32 	%rd38, %r3, 2;
	add.s64 	%rd39, %rd38, %rd4;
	shl.b64 	%rd40, %rd39, 1;
	add.s64 	%rd72, %rd2, %rd40;
	mul.wide.s32 	%rd11, %r77, 4;
	mov.u32 	%r443, 0;
	mov.u64 	%rd73, %rd75;
	mov.u32 	%r444, %r443;
	mov.u32 	%r445, %r443;
	mov.u32 	%r446, %r443;
	mov.u32 	%r436, %r3;

$L__BB116_5:
	.pragma "nounroll";
	ld.global.nc.u32 	%r80, [%rd72];
	add.s64 	%rd41, %rd73, %rd9;
	ld.global.nc.u32 	%r81, [%rd41];
	// begin inline asm
	{mul.f16x2 %r79,%r80,%r81;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r447,%r447,%r79;
}
	// end inline asm
	add.s64 	%rd42, %rd73, %rd8;
	ld.global.nc.u32 	%r87, [%rd42];
	// begin inline asm
	{mul.f16x2 %r85,%r80,%r87;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r446,%r446,%r85;
}
	// end inline asm
	add.s64 	%rd43, %rd41, %rd11;
	ld.global.nc.u32 	%r93, [%rd43];
	// begin inline asm
	{mul.f16x2 %r91,%r80,%r93;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r445,%r445,%r91;
}
	// end inline asm
	add.s64 	%rd44, %rd73, %rd7;
	ld.global.nc.u32 	%r99, [%rd44];
	// begin inline asm
	{mul.f16x2 %r97,%r80,%r99;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r444,%r444,%r97;
}
	// end inline asm
	add.s64 	%rd45, %rd43, %rd11;
	ld.global.nc.u32 	%r105, [%rd45];
	// begin inline asm
	{mul.f16x2 %r103,%r80,%r105;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r443,%r443,%r103;
}
	// end inline asm
	add.s32 	%r436, %r436, 224;
	add.s64 	%rd73, %rd73, 896;
	add.s64 	%rd72, %rd72, 896;
	add.s32 	%r430, %r430, -1;
	setp.ne.s32 	%p4, %r430, 0;
	@%p4 bra 	$L__BB116_5;

$L__BB116_6:
	setp.lt.u32 	%p5, %r6, 672;
	@%p5 bra 	$L__BB116_9;

	add.s32 	%r109, %r436, %r47;
	shl.b32 	%r110, %r47, 1;
	add.s32 	%r111, %r436, %r110;
	mad.lo.s32 	%r112, %r47, 3, %r436;
	shl.b32 	%r113, %r47, 2;
	add.s32 	%r114, %r436, %r113;
	add.s32 	%r115, %r109, 224;
	mul.wide.s32 	%rd46, %r115, 4;
	shl.b64 	%rd47, %rd5, 1;
	add.s64 	%rd16, %rd46, %rd47;
	mul.wide.s32 	%rd48, %r111, 4;
	add.s64 	%rd17, %rd48, %rd47;
	mul.wide.s32 	%rd49, %r112, 4;
	add.s64 	%rd18, %rd49, %rd47;
	mul.wide.s32 	%rd50, %r114, 4;
	add.s64 	%rd19, %rd50, %rd47;
	mul.wide.s32 	%rd51, %r436, 2;
	add.s64 	%rd52, %rd51, %rd4;
	shl.b64 	%rd53, %rd52, 1;
	add.s64 	%rd54, %rd2, %rd53;
	add.s64 	%rd74, %rd54, 1792;
	mul.wide.s32 	%rd55, %r436, 4;
	add.s64 	%rd21, %rd55, %rd47;
	mul.wide.s32 	%rd56, %r47, 4;
	add.s64 	%rd57, %rd55, %rd56;
	add.s64 	%rd22, %rd57, %rd47;

$L__BB116_8:
	ld.global.nc.u32 	%r117, [%rd74+-1792];
	add.s64 	%rd58, %rd75, %rd21;
	ld.global.nc.u32 	%r118, [%rd58];
	// begin inline asm
	{mul.f16x2 %r116,%r117,%r118;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r119,%r447,%r116;
}
	// end inline asm
	add.s64 	%rd59, %rd75, %rd22;
	ld.global.nc.u32 	%r124, [%rd59];
	// begin inline asm
	{mul.f16x2 %r122,%r117,%r124;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r125,%r446,%r122;
}
	// end inline asm
	add.s64 	%rd60, %rd75, %rd17;
	ld.global.nc.u32 	%r130, [%rd60];
	// begin inline asm
	{mul.f16x2 %r128,%r117,%r130;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r131,%r445,%r128;
}
	// end inline asm
	add.s64 	%rd61, %rd75, %rd18;
	ld.global.nc.u32 	%r136, [%rd61];
	// begin inline asm
	{mul.f16x2 %r134,%r117,%r136;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r137,%r444,%r134;
}
	// end inline asm
	add.s64 	%rd62, %rd75, %rd19;
	ld.global.nc.u32 	%r142, [%rd62];
	// begin inline asm
	{mul.f16x2 %r140,%r117,%r142;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r143,%r443,%r140;
}
	// end inline asm
	ld.global.nc.u32 	%r147, [%rd74+-896];
	ld.global.nc.u32 	%r148, [%rd58+896];
	// begin inline asm
	{mul.f16x2 %r146,%r147,%r148;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r149,%r119,%r146;
}
	// end inline asm
	add.s64 	%rd63, %rd75, %rd16;
	ld.global.nc.u32 	%r154, [%rd63];
	// begin inline asm
	{mul.f16x2 %r152,%r147,%r154;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r155,%r125,%r152;
}
	// end inline asm
	ld.global.nc.u32 	%r160, [%rd60+896];
	// begin inline asm
	{mul.f16x2 %r158,%r147,%r160;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r161,%r131,%r158;
}
	// end inline asm
	ld.global.nc.u32 	%r166, [%rd61+896];
	// begin inline asm
	{mul.f16x2 %r164,%r147,%r166;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r167,%r137,%r164;
}
	// end inline asm
	ld.global.nc.u32 	%r172, [%rd62+896];
	// begin inline asm
	{mul.f16x2 %r170,%r147,%r172;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r173,%r143,%r170;
}
	// end inline asm
	ld.global.nc.u32 	%r177, [%rd74];
	ld.global.nc.u32 	%r178, [%rd58+1792];
	// begin inline asm
	{mul.f16x2 %r176,%r177,%r178;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r179,%r149,%r176;
}
	// end inline asm
	ld.global.nc.u32 	%r184, [%rd63+896];
	// begin inline asm
	{mul.f16x2 %r182,%r177,%r184;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r185,%r155,%r182;
}
	// end inline asm
	ld.global.nc.u32 	%r190, [%rd60+1792];
	// begin inline asm
	{mul.f16x2 %r188,%r177,%r190;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r191,%r161,%r188;
}
	// end inline asm
	ld.global.nc.u32 	%r196, [%rd61+1792];
	// begin inline asm
	{mul.f16x2 %r194,%r177,%r196;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r197,%r167,%r194;
}
	// end inline asm
	ld.global.nc.u32 	%r202, [%rd62+1792];
	// begin inline asm
	{mul.f16x2 %r200,%r177,%r202;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r203,%r173,%r200;
}
	// end inline asm
	ld.global.nc.u32 	%r207, [%rd74+896];
	ld.global.nc.u32 	%r208, [%rd58+2688];
	// begin inline asm
	{mul.f16x2 %r206,%r207,%r208;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r447,%r179,%r206;
}
	// end inline asm
	ld.global.nc.u32 	%r214, [%rd63+1792];
	// begin inline asm
	{mul.f16x2 %r212,%r207,%r214;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r446,%r185,%r212;
}
	// end inline asm
	ld.global.nc.u32 	%r220, [%rd60+2688];
	// begin inline asm
	{mul.f16x2 %r218,%r207,%r220;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r445,%r191,%r218;
}
	// end inline asm
	ld.global.nc.u32 	%r226, [%rd61+2688];
	// begin inline asm
	{mul.f16x2 %r224,%r207,%r226;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r444,%r197,%r224;
}
	// end inline asm
	ld.global.nc.u32 	%r232, [%rd62+2688];
	// begin inline asm
	{mul.f16x2 %r230,%r207,%r232;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r443,%r203,%r230;
}
	// end inline asm
	add.s64 	%rd75, %rd75, 3584;
	add.s64 	%rd74, %rd74, 3584;
	add.s32 	%r436, %r436, 896;
	setp.lt.s32 	%p6, %r436, %r46;
	@%p6 bra 	$L__BB116_8;

$L__BB116_9:
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r447;
  cvt.f32.f16 %f7, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r447;
  cvt.f32.f16 %f8, high;}

	// end inline asm
	add.f32 	%f17, %f7, %f8;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r446;
  cvt.f32.f16 %f9, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r446;
  cvt.f32.f16 %f10, high;}

	// end inline asm
	add.f32 	%f1, %f9, %f10;
	st.local.f32 	[%rd3+4], %f1;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r445;
  cvt.f32.f16 %f11, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r445;
  cvt.f32.f16 %f12, high;}

	// end inline asm
	add.f32 	%f2, %f11, %f12;
	st.local.f32 	[%rd3+8], %f2;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r444;
  cvt.f32.f16 %f13, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r444;
  cvt.f32.f16 %f14, high;}

	// end inline asm
	add.f32 	%f3, %f13, %f14;
	st.local.f32 	[%rd3+12], %f3;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r443;
  cvt.f32.f16 %f15, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r443;
  cvt.f32.f16 %f16, high;}

	// end inline asm
	add.f32 	%f4, %f15, %f16;
	st.local.f32 	[%rd3+16], %f4;
	shr.s32 	%r246, %r3, 31;
	shr.u32 	%r247, %r246, 27;
	add.s32 	%r248, %r3, %r247;
	shr.s32 	%r249, %r248, 5;
	shl.b32 	%r250, %r249, 2;
	add.s32 	%r45, %r59, %r250;
	mov.u32 	%r252, 2;
	mov.b32 	%r253, %f17;
	mov.u32 	%r254, 31;
	mov.u32 	%r255, 16;
	mov.u32 	%r256, -1;
	shfl.sync.bfly.b32 	%r257|%p7, %r253, %r255, %r254, %r256;
	mov.b32 	%f18, %r257;
	add.f32 	%f19, %f17, %f18;
	mov.b32 	%r258, %f19;
	mov.u32 	%r259, 8;
	shfl.sync.bfly.b32 	%r260|%p8, %r258, %r259, %r254, %r256;
	mov.b32 	%f20, %r260;
	add.f32 	%f21, %f19, %f20;
	mov.b32 	%r261, %f21;
	mov.u32 	%r262, 4;
	shfl.sync.bfly.b32 	%r263|%p9, %r261, %r262, %r254, %r256;
	mov.b32 	%f22, %r263;
	add.f32 	%f23, %f21, %f22;
	mov.b32 	%r264, %f23;
	shfl.sync.bfly.b32 	%r265|%p10, %r264, %r252, %r254, %r256;
	mov.b32 	%f24, %r265;
	add.f32 	%f25, %f23, %f24;
	mov.b32 	%r266, %f25;
	mov.u32 	%r267, 1;
	shfl.sync.bfly.b32 	%r268|%p11, %r266, %r267, %r254, %r256;
	mov.b32 	%f26, %r268;
	add.f32 	%f27, %f25, %f26;
	st.local.f32 	[%rd3], %f27;
	st.shared.f32 	[%r45], %f27;
	bar.sync 	0;
	@%p1 bra 	$L__BB116_11;

	ld.shared.f32 	%f28, [%r4];
	mov.b32 	%r269, %f28;
	shfl.sync.bfly.b32 	%r273|%p13, %r269, %r255, %r254, %r256;
	mov.b32 	%f29, %r273;
	add.f32 	%f30, %f28, %f29;
	mov.b32 	%r274, %f30;
	shfl.sync.bfly.b32 	%r276|%p14, %r274, %r259, %r254, %r256;
	mov.b32 	%f31, %r276;
	add.f32 	%f32, %f30, %f31;
	mov.b32 	%r277, %f32;
	shfl.sync.bfly.b32 	%r279|%p15, %r277, %r262, %r254, %r256;
	mov.b32 	%f33, %r279;
	add.f32 	%f34, %f32, %f33;
	mov.b32 	%r280, %f34;
	shfl.sync.bfly.b32 	%r282|%p16, %r280, %r252, %r254, %r256;
	mov.b32 	%f35, %r282;
	add.f32 	%f36, %f34, %f35;
	mov.b32 	%r283, %f36;
	shfl.sync.bfly.b32 	%r285|%p17, %r283, %r267, %r254, %r256;
	mov.b32 	%f37, %r285;
	add.f32 	%f38, %f36, %f37;
	st.local.f32 	[%rd3], %f38;

$L__BB116_11:
	bar.sync 	0;
	mov.b32 	%r286, %f1;
	shfl.sync.bfly.b32 	%r290|%p19, %r286, %r255, %r254, %r256;
	mov.b32 	%f39, %r290;
	add.f32 	%f40, %f1, %f39;
	mov.b32 	%r291, %f40;
	shfl.sync.bfly.b32 	%r293|%p20, %r291, %r259, %r254, %r256;
	mov.b32 	%f41, %r293;
	add.f32 	%f42, %f40, %f41;
	mov.b32 	%r294, %f42;
	shfl.sync.bfly.b32 	%r296|%p21, %r294, %r262, %r254, %r256;
	mov.b32 	%f43, %r296;
	add.f32 	%f44, %f42, %f43;
	mov.b32 	%r297, %f44;
	shfl.sync.bfly.b32 	%r299|%p22, %r297, %r252, %r254, %r256;
	mov.b32 	%f45, %r299;
	add.f32 	%f46, %f44, %f45;
	mov.b32 	%r300, %f46;
	shfl.sync.bfly.b32 	%r302|%p23, %r300, %r267, %r254, %r256;
	mov.b32 	%f47, %r302;
	add.f32 	%f48, %f46, %f47;
	st.local.f32 	[%rd3+4], %f48;
	st.shared.f32 	[%r45], %f48;
	bar.sync 	0;
	@%p1 bra 	$L__BB116_13;

	ld.shared.f32 	%f49, [%r4];
	mov.b32 	%r303, %f49;
	mov.u32 	%r304, 31;
	mov.u32 	%r305, 16;
	mov.u32 	%r306, -1;
	shfl.sync.bfly.b32 	%r307|%p24, %r303, %r305, %r304, %r306;
	mov.b32 	%f50, %r307;
	add.f32 	%f51, %f49, %f50;
	mov.b32 	%r308, %f51;
	mov.u32 	%r309, 8;
	shfl.sync.bfly.b32 	%r310|%p25, %r308, %r309, %r304, %r306;
	mov.b32 	%f52, %r310;
	add.f32 	%f53, %f51, %f52;
	mov.b32 	%r311, %f53;
	mov.u32 	%r312, 4;
	shfl.sync.bfly.b32 	%r313|%p26, %r311, %r312, %r304, %r306;
	mov.b32 	%f54, %r313;
	add.f32 	%f55, %f53, %f54;
	mov.b32 	%r314, %f55;
	mov.u32 	%r315, 2;
	shfl.sync.bfly.b32 	%r316|%p27, %r314, %r315, %r304, %r306;
	mov.b32 	%f56, %r316;
	add.f32 	%f57, %f55, %f56;
	mov.b32 	%r317, %f57;
	mov.u32 	%r318, 1;
	shfl.sync.bfly.b32 	%r319|%p28, %r317, %r318, %r304, %r306;
	mov.b32 	%f58, %r319;
	add.f32 	%f59, %f57, %f58;
	st.local.f32 	[%rd3+4], %f59;

$L__BB116_13:
	bar.sync 	0;
	mov.b32 	%r320, %f2;
	mov.u32 	%r321, 31;
	mov.u32 	%r322, 16;
	mov.u32 	%r323, -1;
	shfl.sync.bfly.b32 	%r324|%p30, %r320, %r322, %r321, %r323;
	mov.b32 	%f60, %r324;
	add.f32 	%f61, %f2, %f60;
	mov.b32 	%r325, %f61;
	mov.u32 	%r326, 8;
	shfl.sync.bfly.b32 	%r327|%p31, %r325, %r326, %r321, %r323;
	mov.b32 	%f62, %r327;
	add.f32 	%f63, %f61, %f62;
	mov.b32 	%r328, %f63;
	mov.u32 	%r329, 4;
	shfl.sync.bfly.b32 	%r330|%p32, %r328, %r329, %r321, %r323;
	mov.b32 	%f64, %r330;
	add.f32 	%f65, %f63, %f64;
	mov.b32 	%r331, %f65;
	mov.u32 	%r332, 2;
	shfl.sync.bfly.b32 	%r333|%p33, %r331, %r332, %r321, %r323;
	mov.b32 	%f66, %r333;
	add.f32 	%f67, %f65, %f66;
	mov.b32 	%r334, %f67;
	mov.u32 	%r335, 1;
	shfl.sync.bfly.b32 	%r336|%p34, %r334, %r335, %r321, %r323;
	mov.b32 	%f68, %r336;
	add.f32 	%f69, %f67, %f68;
	st.local.f32 	[%rd3+8], %f69;
	st.shared.f32 	[%r45], %f69;
	bar.sync 	0;
	@%p1 bra 	$L__BB116_15;

	ld.shared.f32 	%f70, [%r4];
	mov.b32 	%r337, %f70;
	shfl.sync.bfly.b32 	%r341|%p35, %r337, %r322, %r321, %r323;
	mov.b32 	%f71, %r341;
	add.f32 	%f72, %f70, %f71;
	mov.b32 	%r342, %f72;
	shfl.sync.bfly.b32 	%r344|%p36, %r342, %r326, %r321, %r323;
	mov.b32 	%f73, %r344;
	add.f32 	%f74, %f72, %f73;
	mov.b32 	%r345, %f74;
	shfl.sync.bfly.b32 	%r347|%p37, %r345, %r329, %r321, %r323;
	mov.b32 	%f75, %r347;
	add.f32 	%f76, %f74, %f75;
	mov.b32 	%r348, %f76;
	shfl.sync.bfly.b32 	%r350|%p38, %r348, %r332, %r321, %r323;
	mov.b32 	%f77, %r350;
	add.f32 	%f78, %f76, %f77;
	mov.b32 	%r351, %f78;
	shfl.sync.bfly.b32 	%r353|%p39, %r351, %r335, %r321, %r323;
	mov.b32 	%f79, %r353;
	add.f32 	%f80, %f78, %f79;
	st.local.f32 	[%rd3+8], %f80;

$L__BB116_15:
	bar.sync 	0;
	mov.b32 	%r354, %f3;
	shfl.sync.bfly.b32 	%r358|%p41, %r354, %r322, %r321, %r323;
	mov.b32 	%f81, %r358;
	add.f32 	%f82, %f3, %f81;
	mov.b32 	%r359, %f82;
	shfl.sync.bfly.b32 	%r361|%p42, %r359, %r326, %r321, %r323;
	mov.b32 	%f83, %r361;
	add.f32 	%f84, %f82, %f83;
	mov.b32 	%r362, %f84;
	shfl.sync.bfly.b32 	%r364|%p43, %r362, %r329, %r321, %r323;
	mov.b32 	%f85, %r364;
	add.f32 	%f86, %f84, %f85;
	mov.b32 	%r365, %f86;
	shfl.sync.bfly.b32 	%r367|%p44, %r365, %r332, %r321, %r323;
	mov.b32 	%f87, %r367;
	add.f32 	%f88, %f86, %f87;
	mov.b32 	%r368, %f88;
	shfl.sync.bfly.b32 	%r370|%p45, %r368, %r335, %r321, %r323;
	mov.b32 	%f89, %r370;
	add.f32 	%f90, %f88, %f89;
	st.local.f32 	[%rd3+12], %f90;
	st.shared.f32 	[%r45], %f90;
	bar.sync 	0;
	@%p1 bra 	$L__BB116_17;

	ld.shared.f32 	%f91, [%r4];
	mov.b32 	%r371, %f91;
	mov.u32 	%r372, 31;
	mov.u32 	%r373, 16;
	mov.u32 	%r374, -1;
	shfl.sync.bfly.b32 	%r375|%p46, %r371, %r373, %r372, %r374;
	mov.b32 	%f92, %r375;
	add.f32 	%f93, %f91, %f92;
	mov.b32 	%r376, %f93;
	mov.u32 	%r377, 8;
	shfl.sync.bfly.b32 	%r378|%p47, %r376, %r377, %r372, %r374;
	mov.b32 	%f94, %r378;
	add.f32 	%f95, %f93, %f94;
	mov.b32 	%r379, %f95;
	mov.u32 	%r380, 4;
	shfl.sync.bfly.b32 	%r381|%p48, %r379, %r380, %r372, %r374;
	mov.b32 	%f96, %r381;
	add.f32 	%f97, %f95, %f96;
	mov.b32 	%r382, %f97;
	mov.u32 	%r383, 2;
	shfl.sync.bfly.b32 	%r384|%p49, %r382, %r383, %r372, %r374;
	mov.b32 	%f98, %r384;
	add.f32 	%f99, %f97, %f98;
	mov.b32 	%r385, %f99;
	mov.u32 	%r386, 1;
	shfl.sync.bfly.b32 	%r387|%p50, %r385, %r386, %r372, %r374;
	mov.b32 	%f100, %r387;
	add.f32 	%f101, %f99, %f100;
	st.local.f32 	[%rd3+12], %f101;

$L__BB116_17:
	bar.sync 	0;
	mov.b32 	%r388, %f4;
	mov.u32 	%r389, 31;
	mov.u32 	%r390, 16;
	mov.u32 	%r391, -1;
	shfl.sync.bfly.b32 	%r392|%p52, %r388, %r390, %r389, %r391;
	mov.b32 	%f102, %r392;
	add.f32 	%f103, %f4, %f102;
	mov.b32 	%r393, %f103;
	mov.u32 	%r394, 8;
	shfl.sync.bfly.b32 	%r395|%p53, %r393, %r394, %r389, %r391;
	mov.b32 	%f104, %r395;
	add.f32 	%f105, %f103, %f104;
	mov.b32 	%r396, %f105;
	mov.u32 	%r397, 4;
	shfl.sync.bfly.b32 	%r398|%p54, %r396, %r397, %r389, %r391;
	mov.b32 	%f106, %r398;
	add.f32 	%f107, %f105, %f106;
	mov.b32 	%r399, %f107;
	mov.u32 	%r400, 2;
	shfl.sync.bfly.b32 	%r401|%p55, %r399, %r400, %r389, %r391;
	mov.b32 	%f108, %r401;
	add.f32 	%f109, %f107, %f108;
	mov.b32 	%r402, %f109;
	mov.u32 	%r403, 1;
	shfl.sync.bfly.b32 	%r404|%p56, %r402, %r403, %r389, %r391;
	mov.b32 	%f110, %r404;
	add.f32 	%f111, %f109, %f110;
	st.local.f32 	[%rd3+16], %f111;
	st.shared.f32 	[%r45], %f111;
	bar.sync 	0;
	@%p1 bra 	$L__BB116_19;

	ld.shared.f32 	%f112, [%r4];
	mov.b32 	%r405, %f112;
	shfl.sync.bfly.b32 	%r409|%p57, %r405, %r390, %r389, %r391;
	mov.b32 	%f113, %r409;
	add.f32 	%f114, %f112, %f113;
	mov.b32 	%r410, %f114;
	shfl.sync.bfly.b32 	%r412|%p58, %r410, %r394, %r389, %r391;
	mov.b32 	%f115, %r412;
	add.f32 	%f116, %f114, %f115;
	mov.b32 	%r413, %f116;
	shfl.sync.bfly.b32 	%r415|%p59, %r413, %r397, %r389, %r391;
	mov.b32 	%f117, %r415;
	add.f32 	%f118, %f116, %f117;
	mov.b32 	%r416, %f118;
	shfl.sync.bfly.b32 	%r418|%p60, %r416, %r400, %r389, %r391;
	mov.b32 	%f119, %r418;
	add.f32 	%f120, %f118, %f119;
	mov.b32 	%r419, %f120;
	shfl.sync.bfly.b32 	%r421|%p61, %r419, %r403, %r389, %r391;
	mov.b32 	%f121, %r421;
	add.f32 	%f122, %f120, %f121;
	st.local.f32 	[%rd3+16], %f122;

$L__BB116_19:
	bar.sync 	0;
	setp.gt.s32 	%p62, %r3, 4;
	@%p62 bra 	$L__BB116_21;

	mad.lo.s32 	%r422, %r3, %r48, %r2;
	cvt.s64.s32 	%rd64, %r422;
	mul.lo.s32 	%r423, %r1, %r49;
	cvt.s64.s32 	%rd65, %r423;
	add.s64 	%rd66, %rd65, %rd64;
	mul.wide.s32 	%rd67, %r3, 4;
	add.s64 	%rd68, %rd3, %rd67;
	ld.local.f32 	%f123, [%rd68];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs3, %f123;}

	// end inline asm
	cvta.to.global.u64 	%rd69, %rd27;
	shl.b64 	%rd70, %rd66, 1;
	add.s64 	%rd71, %rd69, %rd70;
	st.global.u16 	[%rd71], %rs3;

$L__BB116_21:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_6_bs_224
.visible .entry ggml_matvec_f16_ncols_6_bs_224(
	.param .u64 ggml_matvec_f16_ncols_6_bs_224_param_0,
	.param .u64 ggml_matvec_f16_ncols_6_bs_224_param_1,
	.param .u64 ggml_matvec_f16_ncols_6_bs_224_param_2,
	.param .u32 ggml_matvec_f16_ncols_6_bs_224_param_3,
	.param .u32 ggml_matvec_f16_ncols_6_bs_224_param_4,
	.param .u32 ggml_matvec_f16_ncols_6_bs_224_param_5,
	.param .u32 ggml_matvec_f16_ncols_6_bs_224_param_6,
	.param .u32 ggml_matvec_f16_ncols_6_bs_224_param_7,
	.param .u32 ggml_matvec_f16_ncols_6_bs_224_param_8,
	.param .u32 ggml_matvec_f16_ncols_6_bs_224_param_9,
	.param .u32 ggml_matvec_f16_ncols_6_bs_224_param_10,
	.param .u32 ggml_matvec_f16_ncols_6_bs_224_param_11
)
{
	.local .align 8 .b8 	__local_depot117[24];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<74>;
	.reg .b16 	%rs<4>;
	.reg .f32 	%f<148>;
	.reg .b32 	%r<528>;
	.reg .b64 	%rd<79>;


	mov.u64 	%SPL, __local_depot117;
	ld.param.u64 	%rd29, [ggml_matvec_f16_ncols_6_bs_224_param_0];
	ld.param.u64 	%rd30, [ggml_matvec_f16_ncols_6_bs_224_param_1];
	ld.param.u64 	%rd28, [ggml_matvec_f16_ncols_6_bs_224_param_2];
	ld.param.u32 	%r52, [ggml_matvec_f16_ncols_6_bs_224_param_3];
	ld.param.u32 	%r56, [ggml_matvec_f16_ncols_6_bs_224_param_5];
	ld.param.u32 	%r53, [ggml_matvec_f16_ncols_6_bs_224_param_6];
	ld.param.u32 	%r54, [ggml_matvec_f16_ncols_6_bs_224_param_7];
	ld.param.u32 	%r57, [ggml_matvec_f16_ncols_6_bs_224_param_8];
	ld.param.u32 	%r58, [ggml_matvec_f16_ncols_6_bs_224_param_9];
	ld.param.u32 	%r59, [ggml_matvec_f16_ncols_6_bs_224_param_10];
	ld.param.u32 	%r55, [ggml_matvec_f16_ncols_6_bs_224_param_11];
	cvta.to.global.u64 	%rd78, %rd30;
	cvta.to.global.u64 	%rd2, %rd29;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r60, %r1, %r57;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r61, %r2, %r56;
	mad.lo.s32 	%r62, %r60, %r58, %r61;
	cvt.s64.s32 	%rd4, %r62;
	mul.lo.s32 	%r63, %r1, %r59;
	cvt.s64.s32 	%rd5, %r63;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r64, %r3, 2;
	mov.u32 	%r65, data_mmv;
	add.s32 	%r4, %r65, %r64;
	@%p1 bra 	$L__BB117_2;

	mov.u32 	%r66, 0;
	st.shared.u32 	[%r4], %r66;

$L__BB117_2:
	bar.sync 	0;
	mov.f32 	%f7, 0f00000000;
	st.local.v2.f32 	[%rd3], {%f7, %f7};
	st.local.v2.f32 	[%rd3+8], {%f7, %f7};
	st.local.v2.f32 	[%rd3+16], {%f7, %f7};
	mov.u32 	%r522, 0;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs2, %f7;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f7;}

	// end inline asm
	mov.b32 	%r527, {%rs1, %rs2};
	setp.ge.s32 	%p2, %r3, %r52;
	mov.u32 	%r523, %r522;
	mov.u32 	%r524, %r522;
	mov.u32 	%r525, %r522;
	mov.u32 	%r526, %r522;
	@%p2 bra 	$L__BB117_9;

	not.b32 	%r77, %r3;
	add.s32 	%r6, %r77, %r52;
	shr.u32 	%r78, %r6, 5;
	mul.wide.u32 	%rd32, %r78, 613566757;
	shr.u64 	%rd33, %rd32, 32;
	cvt.u32.u64 	%r79, %rd33;
	add.s32 	%r80, %r79, 1;
	and.b32  	%r507, %r80, 3;
	setp.eq.s32 	%p3, %r507, 0;
	mov.u32 	%r522, 0;
	mov.u32 	%r514, %r3;
	@%p3 bra 	$L__BB117_6;

	shl.b32 	%r86, %r53, 1;
	add.s32 	%r87, %r3, %r86;
	mul.wide.s32 	%rd34, %r87, 4;
	shl.b64 	%rd35, %rd5, 1;
	add.s64 	%rd7, %rd34, %rd35;
	mul.wide.s32 	%rd36, %r3, 4;
	mul.wide.s32 	%rd8, %r53, 4;
	add.s64 	%rd37, %rd36, %rd8;
	add.s64 	%rd9, %rd37, %rd35;
	add.s64 	%rd10, %rd36, %rd35;
	mul.wide.s32 	%rd38, %r3, 2;
	add.s64 	%rd39, %rd38, %rd4;
	shl.b64 	%rd40, %rd39, 1;
	add.s64 	%rd75, %rd2, %rd40;
	mov.u32 	%r522, 0;
	mov.u64 	%rd76, %rd78;
	mov.u32 	%r523, %r522;
	mov.u32 	%r524, %r522;
	mov.u32 	%r525, %r522;
	mov.u32 	%r526, %r522;
	mov.u32 	%r514, %r3;

$L__BB117_5:
	.pragma "nounroll";
	ld.global.nc.u32 	%r89, [%rd75];
	add.s64 	%rd41, %rd76, %rd10;
	ld.global.nc.u32 	%r90, [%rd41];
	// begin inline asm
	{mul.f16x2 %r88,%r89,%r90;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r527,%r527,%r88;
}
	// end inline asm
	add.s64 	%rd42, %rd76, %rd9;
	ld.global.nc.u32 	%r96, [%rd42];
	// begin inline asm
	{mul.f16x2 %r94,%r89,%r96;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r526,%r526,%r94;
}
	// end inline asm
	add.s64 	%rd43, %rd76, %rd7;
	ld.global.nc.u32 	%r102, [%rd43];
	// begin inline asm
	{mul.f16x2 %r100,%r89,%r102;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r525,%r525,%r100;
}
	// end inline asm
	add.s64 	%rd44, %rd43, %rd8;
	ld.global.nc.u32 	%r108, [%rd44];
	// begin inline asm
	{mul.f16x2 %r106,%r89,%r108;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r524,%r524,%r106;
}
	// end inline asm
	add.s64 	%rd45, %rd44, %rd8;
	ld.global.nc.u32 	%r114, [%rd45];
	// begin inline asm
	{mul.f16x2 %r112,%r89,%r114;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r523,%r523,%r112;
}
	// end inline asm
	add.s64 	%rd46, %rd45, %rd8;
	ld.global.nc.u32 	%r120, [%rd46];
	// begin inline asm
	{mul.f16x2 %r118,%r89,%r120;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r522,%r522,%r118;
}
	// end inline asm
	add.s32 	%r514, %r514, 224;
	add.s64 	%rd76, %rd76, 896;
	add.s64 	%rd75, %rd75, 896;
	add.s32 	%r507, %r507, -1;
	setp.ne.s32 	%p4, %r507, 0;
	@%p4 bra 	$L__BB117_5;

$L__BB117_6:
	setp.lt.u32 	%p5, %r6, 672;
	@%p5 bra 	$L__BB117_9;

	add.s32 	%r124, %r514, %r53;
	shl.b32 	%r125, %r53, 1;
	add.s32 	%r126, %r514, %r125;
	mad.lo.s32 	%r127, %r53, 3, %r514;
	shl.b32 	%r128, %r53, 2;
	add.s32 	%r129, %r514, %r128;
	mad.lo.s32 	%r130, %r53, 5, %r514;
	add.s32 	%r131, %r124, 224;
	mul.wide.s32 	%rd47, %r131, 4;
	shl.b64 	%rd48, %rd5, 1;
	add.s64 	%rd16, %rd47, %rd48;
	mul.wide.s32 	%rd49, %r126, 4;
	add.s64 	%rd17, %rd49, %rd48;
	mul.wide.s32 	%rd50, %r127, 4;
	add.s64 	%rd18, %rd50, %rd48;
	mul.wide.s32 	%rd51, %r129, 4;
	add.s64 	%rd19, %rd51, %rd48;
	mul.wide.s32 	%rd52, %r130, 4;
	add.s64 	%rd20, %rd52, %rd48;
	mul.wide.s32 	%rd53, %r514, 2;
	add.s64 	%rd54, %rd53, %rd4;
	shl.b64 	%rd55, %rd54, 1;
	add.s64 	%rd56, %rd2, %rd55;
	add.s64 	%rd77, %rd56, 1792;
	mul.wide.s32 	%rd57, %r514, 4;
	add.s64 	%rd22, %rd57, %rd48;
	mul.wide.s32 	%rd58, %r53, 4;
	add.s64 	%rd59, %rd57, %rd58;
	add.s64 	%rd23, %rd59, %rd48;

$L__BB117_8:
	ld.global.nc.u32 	%r133, [%rd77+-1792];
	add.s64 	%rd60, %rd78, %rd22;
	ld.global.nc.u32 	%r134, [%rd60];
	// begin inline asm
	{mul.f16x2 %r132,%r133,%r134;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r135,%r527,%r132;
}
	// end inline asm
	add.s64 	%rd61, %rd78, %rd23;
	ld.global.nc.u32 	%r140, [%rd61];
	// begin inline asm
	{mul.f16x2 %r138,%r133,%r140;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r141,%r526,%r138;
}
	// end inline asm
	add.s64 	%rd62, %rd78, %rd17;
	ld.global.nc.u32 	%r146, [%rd62];
	// begin inline asm
	{mul.f16x2 %r144,%r133,%r146;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r147,%r525,%r144;
}
	// end inline asm
	add.s64 	%rd63, %rd78, %rd18;
	ld.global.nc.u32 	%r152, [%rd63];
	// begin inline asm
	{mul.f16x2 %r150,%r133,%r152;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r153,%r524,%r150;
}
	// end inline asm
	add.s64 	%rd64, %rd78, %rd19;
	ld.global.nc.u32 	%r158, [%rd64];
	// begin inline asm
	{mul.f16x2 %r156,%r133,%r158;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r159,%r523,%r156;
}
	// end inline asm
	add.s64 	%rd65, %rd78, %rd20;
	ld.global.nc.u32 	%r164, [%rd65];
	// begin inline asm
	{mul.f16x2 %r162,%r133,%r164;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r165,%r522,%r162;
}
	// end inline asm
	ld.global.nc.u32 	%r169, [%rd77+-896];
	ld.global.nc.u32 	%r170, [%rd60+896];
	// begin inline asm
	{mul.f16x2 %r168,%r169,%r170;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r171,%r135,%r168;
}
	// end inline asm
	add.s64 	%rd66, %rd78, %rd16;
	ld.global.nc.u32 	%r176, [%rd66];
	// begin inline asm
	{mul.f16x2 %r174,%r169,%r176;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r177,%r141,%r174;
}
	// end inline asm
	ld.global.nc.u32 	%r182, [%rd62+896];
	// begin inline asm
	{mul.f16x2 %r180,%r169,%r182;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r183,%r147,%r180;
}
	// end inline asm
	ld.global.nc.u32 	%r188, [%rd63+896];
	// begin inline asm
	{mul.f16x2 %r186,%r169,%r188;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r189,%r153,%r186;
}
	// end inline asm
	ld.global.nc.u32 	%r194, [%rd64+896];
	// begin inline asm
	{mul.f16x2 %r192,%r169,%r194;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r195,%r159,%r192;
}
	// end inline asm
	ld.global.nc.u32 	%r200, [%rd65+896];
	// begin inline asm
	{mul.f16x2 %r198,%r169,%r200;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r201,%r165,%r198;
}
	// end inline asm
	ld.global.nc.u32 	%r205, [%rd77];
	ld.global.nc.u32 	%r206, [%rd60+1792];
	// begin inline asm
	{mul.f16x2 %r204,%r205,%r206;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r207,%r171,%r204;
}
	// end inline asm
	ld.global.nc.u32 	%r212, [%rd66+896];
	// begin inline asm
	{mul.f16x2 %r210,%r205,%r212;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r213,%r177,%r210;
}
	// end inline asm
	ld.global.nc.u32 	%r218, [%rd62+1792];
	// begin inline asm
	{mul.f16x2 %r216,%r205,%r218;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r219,%r183,%r216;
}
	// end inline asm
	ld.global.nc.u32 	%r224, [%rd63+1792];
	// begin inline asm
	{mul.f16x2 %r222,%r205,%r224;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r225,%r189,%r222;
}
	// end inline asm
	ld.global.nc.u32 	%r230, [%rd64+1792];
	// begin inline asm
	{mul.f16x2 %r228,%r205,%r230;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r231,%r195,%r228;
}
	// end inline asm
	ld.global.nc.u32 	%r236, [%rd65+1792];
	// begin inline asm
	{mul.f16x2 %r234,%r205,%r236;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r237,%r201,%r234;
}
	// end inline asm
	ld.global.nc.u32 	%r241, [%rd77+896];
	ld.global.nc.u32 	%r242, [%rd60+2688];
	// begin inline asm
	{mul.f16x2 %r240,%r241,%r242;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r527,%r207,%r240;
}
	// end inline asm
	ld.global.nc.u32 	%r248, [%rd66+1792];
	// begin inline asm
	{mul.f16x2 %r246,%r241,%r248;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r526,%r213,%r246;
}
	// end inline asm
	ld.global.nc.u32 	%r254, [%rd62+2688];
	// begin inline asm
	{mul.f16x2 %r252,%r241,%r254;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r525,%r219,%r252;
}
	// end inline asm
	ld.global.nc.u32 	%r260, [%rd63+2688];
	// begin inline asm
	{mul.f16x2 %r258,%r241,%r260;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r524,%r225,%r258;
}
	// end inline asm
	ld.global.nc.u32 	%r266, [%rd64+2688];
	// begin inline asm
	{mul.f16x2 %r264,%r241,%r266;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r523,%r231,%r264;
}
	// end inline asm
	ld.global.nc.u32 	%r272, [%rd65+2688];
	// begin inline asm
	{mul.f16x2 %r270,%r241,%r272;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r522,%r237,%r270;
}
	// end inline asm
	add.s64 	%rd78, %rd78, 3584;
	add.s64 	%rd77, %rd77, 3584;
	add.s32 	%r514, %r514, 896;
	setp.lt.s32 	%p6, %r514, %r52;
	@%p6 bra 	$L__BB117_8;

$L__BB117_9:
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r527;
  cvt.f32.f16 %f8, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r527;
  cvt.f32.f16 %f9, high;}

	// end inline asm
	add.f32 	%f20, %f8, %f9;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r526;
  cvt.f32.f16 %f10, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r526;
  cvt.f32.f16 %f11, high;}

	// end inline asm
	add.f32 	%f1, %f10, %f11;
	st.local.f32 	[%rd3+4], %f1;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r525;
  cvt.f32.f16 %f12, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r525;
  cvt.f32.f16 %f13, high;}

	// end inline asm
	add.f32 	%f2, %f12, %f13;
	st.local.f32 	[%rd3+8], %f2;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r524;
  cvt.f32.f16 %f14, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r524;
  cvt.f32.f16 %f15, high;}

	// end inline asm
	add.f32 	%f3, %f14, %f15;
	st.local.f32 	[%rd3+12], %f3;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r523;
  cvt.f32.f16 %f16, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r523;
  cvt.f32.f16 %f17, high;}

	// end inline asm
	add.f32 	%f4, %f16, %f17;
	st.local.f32 	[%rd3+16], %f4;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r522;
  cvt.f32.f16 %f18, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r522;
  cvt.f32.f16 %f19, high;}

	// end inline asm
	add.f32 	%f5, %f18, %f19;
	st.local.f32 	[%rd3+20], %f5;
	shr.s32 	%r288, %r3, 31;
	shr.u32 	%r289, %r288, 27;
	add.s32 	%r290, %r3, %r289;
	shr.s32 	%r291, %r290, 5;
	shl.b32 	%r292, %r291, 2;
	add.s32 	%r51, %r65, %r292;
	mov.u32 	%r294, 2;
	mov.b32 	%r295, %f20;
	mov.u32 	%r296, 31;
	mov.u32 	%r297, 16;
	mov.u32 	%r298, -1;
	shfl.sync.bfly.b32 	%r299|%p7, %r295, %r297, %r296, %r298;
	mov.b32 	%f21, %r299;
	add.f32 	%f22, %f20, %f21;
	mov.b32 	%r300, %f22;
	mov.u32 	%r301, 8;
	shfl.sync.bfly.b32 	%r302|%p8, %r300, %r301, %r296, %r298;
	mov.b32 	%f23, %r302;
	add.f32 	%f24, %f22, %f23;
	mov.b32 	%r303, %f24;
	mov.u32 	%r304, 4;
	shfl.sync.bfly.b32 	%r305|%p9, %r303, %r304, %r296, %r298;
	mov.b32 	%f25, %r305;
	add.f32 	%f26, %f24, %f25;
	mov.b32 	%r306, %f26;
	shfl.sync.bfly.b32 	%r307|%p10, %r306, %r294, %r296, %r298;
	mov.b32 	%f27, %r307;
	add.f32 	%f28, %f26, %f27;
	mov.b32 	%r308, %f28;
	mov.u32 	%r309, 1;
	shfl.sync.bfly.b32 	%r310|%p11, %r308, %r309, %r296, %r298;
	mov.b32 	%f29, %r310;
	add.f32 	%f30, %f28, %f29;
	st.local.f32 	[%rd3], %f30;
	st.shared.f32 	[%r51], %f30;
	bar.sync 	0;
	@%p1 bra 	$L__BB117_11;

	ld.shared.f32 	%f31, [%r4];
	mov.b32 	%r311, %f31;
	shfl.sync.bfly.b32 	%r315|%p13, %r311, %r297, %r296, %r298;
	mov.b32 	%f32, %r315;
	add.f32 	%f33, %f31, %f32;
	mov.b32 	%r316, %f33;
	shfl.sync.bfly.b32 	%r318|%p14, %r316, %r301, %r296, %r298;
	mov.b32 	%f34, %r318;
	add.f32 	%f35, %f33, %f34;
	mov.b32 	%r319, %f35;
	shfl.sync.bfly.b32 	%r321|%p15, %r319, %r304, %r296, %r298;
	mov.b32 	%f36, %r321;
	add.f32 	%f37, %f35, %f36;
	mov.b32 	%r322, %f37;
	shfl.sync.bfly.b32 	%r324|%p16, %r322, %r294, %r296, %r298;
	mov.b32 	%f38, %r324;
	add.f32 	%f39, %f37, %f38;
	mov.b32 	%r325, %f39;
	shfl.sync.bfly.b32 	%r327|%p17, %r325, %r309, %r296, %r298;
	mov.b32 	%f40, %r327;
	add.f32 	%f41, %f39, %f40;
	st.local.f32 	[%rd3], %f41;

$L__BB117_11:
	bar.sync 	0;
	mov.b32 	%r328, %f1;
	shfl.sync.bfly.b32 	%r332|%p19, %r328, %r297, %r296, %r298;
	mov.b32 	%f42, %r332;
	add.f32 	%f43, %f1, %f42;
	mov.b32 	%r333, %f43;
	shfl.sync.bfly.b32 	%r335|%p20, %r333, %r301, %r296, %r298;
	mov.b32 	%f44, %r335;
	add.f32 	%f45, %f43, %f44;
	mov.b32 	%r336, %f45;
	shfl.sync.bfly.b32 	%r338|%p21, %r336, %r304, %r296, %r298;
	mov.b32 	%f46, %r338;
	add.f32 	%f47, %f45, %f46;
	mov.b32 	%r339, %f47;
	shfl.sync.bfly.b32 	%r341|%p22, %r339, %r294, %r296, %r298;
	mov.b32 	%f48, %r341;
	add.f32 	%f49, %f47, %f48;
	mov.b32 	%r342, %f49;
	shfl.sync.bfly.b32 	%r344|%p23, %r342, %r309, %r296, %r298;
	mov.b32 	%f50, %r344;
	add.f32 	%f51, %f49, %f50;
	st.local.f32 	[%rd3+4], %f51;
	st.shared.f32 	[%r51], %f51;
	bar.sync 	0;
	@%p1 bra 	$L__BB117_13;

	ld.shared.f32 	%f52, [%r4];
	mov.b32 	%r345, %f52;
	mov.u32 	%r346, 31;
	mov.u32 	%r347, 16;
	mov.u32 	%r348, -1;
	shfl.sync.bfly.b32 	%r349|%p24, %r345, %r347, %r346, %r348;
	mov.b32 	%f53, %r349;
	add.f32 	%f54, %f52, %f53;
	mov.b32 	%r350, %f54;
	mov.u32 	%r351, 8;
	shfl.sync.bfly.b32 	%r352|%p25, %r350, %r351, %r346, %r348;
	mov.b32 	%f55, %r352;
	add.f32 	%f56, %f54, %f55;
	mov.b32 	%r353, %f56;
	mov.u32 	%r354, 4;
	shfl.sync.bfly.b32 	%r355|%p26, %r353, %r354, %r346, %r348;
	mov.b32 	%f57, %r355;
	add.f32 	%f58, %f56, %f57;
	mov.b32 	%r356, %f58;
	mov.u32 	%r357, 2;
	shfl.sync.bfly.b32 	%r358|%p27, %r356, %r357, %r346, %r348;
	mov.b32 	%f59, %r358;
	add.f32 	%f60, %f58, %f59;
	mov.b32 	%r359, %f60;
	mov.u32 	%r360, 1;
	shfl.sync.bfly.b32 	%r361|%p28, %r359, %r360, %r346, %r348;
	mov.b32 	%f61, %r361;
	add.f32 	%f62, %f60, %f61;
	st.local.f32 	[%rd3+4], %f62;

$L__BB117_13:
	bar.sync 	0;
	mov.b32 	%r362, %f2;
	mov.u32 	%r363, 31;
	mov.u32 	%r364, 16;
	mov.u32 	%r365, -1;
	shfl.sync.bfly.b32 	%r366|%p30, %r362, %r364, %r363, %r365;
	mov.b32 	%f63, %r366;
	add.f32 	%f64, %f2, %f63;
	mov.b32 	%r367, %f64;
	mov.u32 	%r368, 8;
	shfl.sync.bfly.b32 	%r369|%p31, %r367, %r368, %r363, %r365;
	mov.b32 	%f65, %r369;
	add.f32 	%f66, %f64, %f65;
	mov.b32 	%r370, %f66;
	mov.u32 	%r371, 4;
	shfl.sync.bfly.b32 	%r372|%p32, %r370, %r371, %r363, %r365;
	mov.b32 	%f67, %r372;
	add.f32 	%f68, %f66, %f67;
	mov.b32 	%r373, %f68;
	mov.u32 	%r374, 2;
	shfl.sync.bfly.b32 	%r375|%p33, %r373, %r374, %r363, %r365;
	mov.b32 	%f69, %r375;
	add.f32 	%f70, %f68, %f69;
	mov.b32 	%r376, %f70;
	mov.u32 	%r377, 1;
	shfl.sync.bfly.b32 	%r378|%p34, %r376, %r377, %r363, %r365;
	mov.b32 	%f71, %r378;
	add.f32 	%f72, %f70, %f71;
	st.local.f32 	[%rd3+8], %f72;
	st.shared.f32 	[%r51], %f72;
	bar.sync 	0;
	@%p1 bra 	$L__BB117_15;

	ld.shared.f32 	%f73, [%r4];
	mov.b32 	%r379, %f73;
	shfl.sync.bfly.b32 	%r383|%p35, %r379, %r364, %r363, %r365;
	mov.b32 	%f74, %r383;
	add.f32 	%f75, %f73, %f74;
	mov.b32 	%r384, %f75;
	shfl.sync.bfly.b32 	%r386|%p36, %r384, %r368, %r363, %r365;
	mov.b32 	%f76, %r386;
	add.f32 	%f77, %f75, %f76;
	mov.b32 	%r387, %f77;
	shfl.sync.bfly.b32 	%r389|%p37, %r387, %r371, %r363, %r365;
	mov.b32 	%f78, %r389;
	add.f32 	%f79, %f77, %f78;
	mov.b32 	%r390, %f79;
	shfl.sync.bfly.b32 	%r392|%p38, %r390, %r374, %r363, %r365;
	mov.b32 	%f80, %r392;
	add.f32 	%f81, %f79, %f80;
	mov.b32 	%r393, %f81;
	shfl.sync.bfly.b32 	%r395|%p39, %r393, %r377, %r363, %r365;
	mov.b32 	%f82, %r395;
	add.f32 	%f83, %f81, %f82;
	st.local.f32 	[%rd3+8], %f83;

$L__BB117_15:
	bar.sync 	0;
	mov.b32 	%r396, %f3;
	shfl.sync.bfly.b32 	%r400|%p41, %r396, %r364, %r363, %r365;
	mov.b32 	%f84, %r400;
	add.f32 	%f85, %f3, %f84;
	mov.b32 	%r401, %f85;
	shfl.sync.bfly.b32 	%r403|%p42, %r401, %r368, %r363, %r365;
	mov.b32 	%f86, %r403;
	add.f32 	%f87, %f85, %f86;
	mov.b32 	%r404, %f87;
	shfl.sync.bfly.b32 	%r406|%p43, %r404, %r371, %r363, %r365;
	mov.b32 	%f88, %r406;
	add.f32 	%f89, %f87, %f88;
	mov.b32 	%r407, %f89;
	shfl.sync.bfly.b32 	%r409|%p44, %r407, %r374, %r363, %r365;
	mov.b32 	%f90, %r409;
	add.f32 	%f91, %f89, %f90;
	mov.b32 	%r410, %f91;
	shfl.sync.bfly.b32 	%r412|%p45, %r410, %r377, %r363, %r365;
	mov.b32 	%f92, %r412;
	add.f32 	%f93, %f91, %f92;
	st.local.f32 	[%rd3+12], %f93;
	st.shared.f32 	[%r51], %f93;
	bar.sync 	0;
	@%p1 bra 	$L__BB117_17;

	ld.shared.f32 	%f94, [%r4];
	mov.b32 	%r413, %f94;
	mov.u32 	%r414, 31;
	mov.u32 	%r415, 16;
	mov.u32 	%r416, -1;
	shfl.sync.bfly.b32 	%r417|%p46, %r413, %r415, %r414, %r416;
	mov.b32 	%f95, %r417;
	add.f32 	%f96, %f94, %f95;
	mov.b32 	%r418, %f96;
	mov.u32 	%r419, 8;
	shfl.sync.bfly.b32 	%r420|%p47, %r418, %r419, %r414, %r416;
	mov.b32 	%f97, %r420;
	add.f32 	%f98, %f96, %f97;
	mov.b32 	%r421, %f98;
	mov.u32 	%r422, 4;
	shfl.sync.bfly.b32 	%r423|%p48, %r421, %r422, %r414, %r416;
	mov.b32 	%f99, %r423;
	add.f32 	%f100, %f98, %f99;
	mov.b32 	%r424, %f100;
	mov.u32 	%r425, 2;
	shfl.sync.bfly.b32 	%r426|%p49, %r424, %r425, %r414, %r416;
	mov.b32 	%f101, %r426;
	add.f32 	%f102, %f100, %f101;
	mov.b32 	%r427, %f102;
	mov.u32 	%r428, 1;
	shfl.sync.bfly.b32 	%r429|%p50, %r427, %r428, %r414, %r416;
	mov.b32 	%f103, %r429;
	add.f32 	%f104, %f102, %f103;
	st.local.f32 	[%rd3+12], %f104;

$L__BB117_17:
	bar.sync 	0;
	mov.b32 	%r430, %f4;
	mov.u32 	%r431, 31;
	mov.u32 	%r432, 16;
	mov.u32 	%r433, -1;
	shfl.sync.bfly.b32 	%r434|%p52, %r430, %r432, %r431, %r433;
	mov.b32 	%f105, %r434;
	add.f32 	%f106, %f4, %f105;
	mov.b32 	%r435, %f106;
	mov.u32 	%r436, 8;
	shfl.sync.bfly.b32 	%r437|%p53, %r435, %r436, %r431, %r433;
	mov.b32 	%f107, %r437;
	add.f32 	%f108, %f106, %f107;
	mov.b32 	%r438, %f108;
	mov.u32 	%r439, 4;
	shfl.sync.bfly.b32 	%r440|%p54, %r438, %r439, %r431, %r433;
	mov.b32 	%f109, %r440;
	add.f32 	%f110, %f108, %f109;
	mov.b32 	%r441, %f110;
	mov.u32 	%r442, 2;
	shfl.sync.bfly.b32 	%r443|%p55, %r441, %r442, %r431, %r433;
	mov.b32 	%f111, %r443;
	add.f32 	%f112, %f110, %f111;
	mov.b32 	%r444, %f112;
	mov.u32 	%r445, 1;
	shfl.sync.bfly.b32 	%r446|%p56, %r444, %r445, %r431, %r433;
	mov.b32 	%f113, %r446;
	add.f32 	%f114, %f112, %f113;
	st.local.f32 	[%rd3+16], %f114;
	st.shared.f32 	[%r51], %f114;
	bar.sync 	0;
	@%p1 bra 	$L__BB117_19;

	ld.shared.f32 	%f115, [%r4];
	mov.b32 	%r447, %f115;
	shfl.sync.bfly.b32 	%r451|%p57, %r447, %r432, %r431, %r433;
	mov.b32 	%f116, %r451;
	add.f32 	%f117, %f115, %f116;
	mov.b32 	%r452, %f117;
	shfl.sync.bfly.b32 	%r454|%p58, %r452, %r436, %r431, %r433;
	mov.b32 	%f118, %r454;
	add.f32 	%f119, %f117, %f118;
	mov.b32 	%r455, %f119;
	shfl.sync.bfly.b32 	%r457|%p59, %r455, %r439, %r431, %r433;
	mov.b32 	%f120, %r457;
	add.f32 	%f121, %f119, %f120;
	mov.b32 	%r458, %f121;
	shfl.sync.bfly.b32 	%r460|%p60, %r458, %r442, %r431, %r433;
	mov.b32 	%f122, %r460;
	add.f32 	%f123, %f121, %f122;
	mov.b32 	%r461, %f123;
	shfl.sync.bfly.b32 	%r463|%p61, %r461, %r445, %r431, %r433;
	mov.b32 	%f124, %r463;
	add.f32 	%f125, %f123, %f124;
	st.local.f32 	[%rd3+16], %f125;

$L__BB117_19:
	bar.sync 	0;
	mov.b32 	%r464, %f5;
	shfl.sync.bfly.b32 	%r468|%p63, %r464, %r432, %r431, %r433;
	mov.b32 	%f126, %r468;
	add.f32 	%f127, %f5, %f126;
	mov.b32 	%r469, %f127;
	shfl.sync.bfly.b32 	%r471|%p64, %r469, %r436, %r431, %r433;
	mov.b32 	%f128, %r471;
	add.f32 	%f129, %f127, %f128;
	mov.b32 	%r472, %f129;
	shfl.sync.bfly.b32 	%r474|%p65, %r472, %r439, %r431, %r433;
	mov.b32 	%f130, %r474;
	add.f32 	%f131, %f129, %f130;
	mov.b32 	%r475, %f131;
	shfl.sync.bfly.b32 	%r477|%p66, %r475, %r442, %r431, %r433;
	mov.b32 	%f132, %r477;
	add.f32 	%f133, %f131, %f132;
	mov.b32 	%r478, %f133;
	shfl.sync.bfly.b32 	%r480|%p67, %r478, %r445, %r431, %r433;
	mov.b32 	%f134, %r480;
	add.f32 	%f135, %f133, %f134;
	st.local.f32 	[%rd3+20], %f135;
	st.shared.f32 	[%r51], %f135;
	bar.sync 	0;
	@%p1 bra 	$L__BB117_21;

	ld.shared.f32 	%f136, [%r4];
	mov.b32 	%r481, %f136;
	mov.u32 	%r482, 31;
	mov.u32 	%r483, 16;
	mov.u32 	%r484, -1;
	shfl.sync.bfly.b32 	%r485|%p68, %r481, %r483, %r482, %r484;
	mov.b32 	%f137, %r485;
	add.f32 	%f138, %f136, %f137;
	mov.b32 	%r486, %f138;
	mov.u32 	%r487, 8;
	shfl.sync.bfly.b32 	%r488|%p69, %r486, %r487, %r482, %r484;
	mov.b32 	%f139, %r488;
	add.f32 	%f140, %f138, %f139;
	mov.b32 	%r489, %f140;
	mov.u32 	%r490, 4;
	shfl.sync.bfly.b32 	%r491|%p70, %r489, %r490, %r482, %r484;
	mov.b32 	%f141, %r491;
	add.f32 	%f142, %f140, %f141;
	mov.b32 	%r492, %f142;
	mov.u32 	%r493, 2;
	shfl.sync.bfly.b32 	%r494|%p71, %r492, %r493, %r482, %r484;
	mov.b32 	%f143, %r494;
	add.f32 	%f144, %f142, %f143;
	mov.b32 	%r495, %f144;
	mov.u32 	%r496, 1;
	shfl.sync.bfly.b32 	%r497|%p72, %r495, %r496, %r482, %r484;
	mov.b32 	%f145, %r497;
	add.f32 	%f146, %f144, %f145;
	st.local.f32 	[%rd3+20], %f146;

$L__BB117_21:
	bar.sync 	0;
	setp.gt.s32 	%p73, %r3, 5;
	@%p73 bra 	$L__BB117_23;

	mad.lo.s32 	%r498, %r3, %r54, %r2;
	cvt.s64.s32 	%rd67, %r498;
	mul.lo.s32 	%r499, %r1, %r55;
	cvt.s64.s32 	%rd68, %r499;
	add.s64 	%rd69, %rd68, %rd67;
	mul.wide.s32 	%rd70, %r3, 4;
	add.s64 	%rd71, %rd3, %rd70;
	ld.local.f32 	%f147, [%rd71];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs3, %f147;}

	// end inline asm
	cvta.to.global.u64 	%rd72, %rd28;
	shl.b64 	%rd73, %rd69, 1;
	add.s64 	%rd74, %rd72, %rd73;
	st.global.u16 	[%rd74], %rs3;

$L__BB117_23:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_7_bs_224
.visible .entry ggml_matvec_f16_ncols_7_bs_224(
	.param .u64 ggml_matvec_f16_ncols_7_bs_224_param_0,
	.param .u64 ggml_matvec_f16_ncols_7_bs_224_param_1,
	.param .u64 ggml_matvec_f16_ncols_7_bs_224_param_2,
	.param .u32 ggml_matvec_f16_ncols_7_bs_224_param_3,
	.param .u32 ggml_matvec_f16_ncols_7_bs_224_param_4,
	.param .u32 ggml_matvec_f16_ncols_7_bs_224_param_5,
	.param .u32 ggml_matvec_f16_ncols_7_bs_224_param_6,
	.param .u32 ggml_matvec_f16_ncols_7_bs_224_param_7,
	.param .u32 ggml_matvec_f16_ncols_7_bs_224_param_8,
	.param .u32 ggml_matvec_f16_ncols_7_bs_224_param_9,
	.param .u32 ggml_matvec_f16_ncols_7_bs_224_param_10,
	.param .u32 ggml_matvec_f16_ncols_7_bs_224_param_11
)
{
	.local .align 4 .b8 	__local_depot118[28];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<85>;
	.reg .b16 	%rs<4>;
	.reg .f32 	%f<172>;
	.reg .b32 	%r<608>;
	.reg .b64 	%rd<83>;


	mov.u64 	%SPL, __local_depot118;
	ld.param.u64 	%rd30, [ggml_matvec_f16_ncols_7_bs_224_param_0];
	ld.param.u64 	%rd31, [ggml_matvec_f16_ncols_7_bs_224_param_1];
	ld.param.u64 	%rd29, [ggml_matvec_f16_ncols_7_bs_224_param_2];
	ld.param.u32 	%r58, [ggml_matvec_f16_ncols_7_bs_224_param_3];
	ld.param.u32 	%r62, [ggml_matvec_f16_ncols_7_bs_224_param_5];
	ld.param.u32 	%r59, [ggml_matvec_f16_ncols_7_bs_224_param_6];
	ld.param.u32 	%r60, [ggml_matvec_f16_ncols_7_bs_224_param_7];
	ld.param.u32 	%r63, [ggml_matvec_f16_ncols_7_bs_224_param_8];
	ld.param.u32 	%r64, [ggml_matvec_f16_ncols_7_bs_224_param_9];
	ld.param.u32 	%r65, [ggml_matvec_f16_ncols_7_bs_224_param_10];
	ld.param.u32 	%r61, [ggml_matvec_f16_ncols_7_bs_224_param_11];
	cvta.to.global.u64 	%rd82, %rd31;
	cvta.to.global.u64 	%rd2, %rd30;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r66, %r1, %r63;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r67, %r2, %r62;
	mad.lo.s32 	%r68, %r66, %r64, %r67;
	cvt.s64.s32 	%rd4, %r68;
	mul.lo.s32 	%r69, %r1, %r65;
	cvt.s64.s32 	%rd5, %r69;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r70, %r3, 2;
	mov.u32 	%r71, data_mmv;
	add.s32 	%r4, %r71, %r70;
	@%p1 bra 	$L__BB118_2;

	mov.u32 	%r72, 0;
	st.shared.u32 	[%r4], %r72;

$L__BB118_2:
	bar.sync 	0;
	mov.f32 	%f8, 0f00000000;
	mov.u32 	%r601, 0;
	st.local.u32 	[%rd3], %r601;
	st.local.u32 	[%rd3+4], %r601;
	st.local.u32 	[%rd3+8], %r601;
	st.local.u32 	[%rd3+12], %r601;
	st.local.u32 	[%rd3+16], %r601;
	st.local.u32 	[%rd3+20], %r601;
	st.local.u32 	[%rd3+24], %r601;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs2, %f8;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f8;}

	// end inline asm
	mov.b32 	%r607, {%rs1, %rs2};
	setp.ge.s32 	%p2, %r3, %r58;
	mov.u32 	%r602, %r601;
	mov.u32 	%r603, %r601;
	mov.u32 	%r604, %r601;
	mov.u32 	%r605, %r601;
	mov.u32 	%r606, %r601;
	@%p2 bra 	$L__BB118_9;

	not.b32 	%r85, %r3;
	add.s32 	%r6, %r85, %r58;
	shr.u32 	%r86, %r6, 5;
	mul.wide.u32 	%rd33, %r86, 613566757;
	shr.u64 	%rd34, %rd33, 32;
	cvt.u32.u64 	%r87, %rd34;
	add.s32 	%r88, %r87, 1;
	and.b32  	%r584, %r88, 3;
	setp.eq.s32 	%p3, %r584, 0;
	mov.u32 	%r601, 0;
	mov.u32 	%r592, %r3;
	@%p3 bra 	$L__BB118_6;

	shl.b32 	%r95, %r59, 1;
	add.s32 	%r96, %r3, %r95;
	mul.wide.s32 	%rd35, %r96, 4;
	shl.b64 	%rd36, %rd5, 1;
	add.s64 	%rd7, %rd35, %rd36;
	mul.wide.s32 	%rd37, %r3, 4;
	mul.wide.s32 	%rd8, %r59, 4;
	add.s64 	%rd38, %rd37, %rd8;
	add.s64 	%rd9, %rd38, %rd36;
	add.s64 	%rd10, %rd37, %rd36;
	mul.wide.s32 	%rd39, %r3, 2;
	add.s64 	%rd40, %rd39, %rd4;
	shl.b64 	%rd41, %rd40, 1;
	add.s64 	%rd79, %rd2, %rd41;
	mov.u32 	%r601, 0;
	mov.u64 	%rd80, %rd82;
	mov.u32 	%r602, %r601;
	mov.u32 	%r603, %r601;
	mov.u32 	%r604, %r601;
	mov.u32 	%r605, %r601;
	mov.u32 	%r606, %r601;
	mov.u32 	%r592, %r3;

$L__BB118_5:
	.pragma "nounroll";
	ld.global.nc.u32 	%r98, [%rd79];
	add.s64 	%rd42, %rd80, %rd10;
	ld.global.nc.u32 	%r99, [%rd42];
	// begin inline asm
	{mul.f16x2 %r97,%r98,%r99;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r607,%r607,%r97;
}
	// end inline asm
	add.s64 	%rd43, %rd80, %rd9;
	ld.global.nc.u32 	%r105, [%rd43];
	// begin inline asm
	{mul.f16x2 %r103,%r98,%r105;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r606,%r606,%r103;
}
	// end inline asm
	add.s64 	%rd44, %rd80, %rd7;
	ld.global.nc.u32 	%r111, [%rd44];
	// begin inline asm
	{mul.f16x2 %r109,%r98,%r111;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r605,%r605,%r109;
}
	// end inline asm
	add.s64 	%rd45, %rd44, %rd8;
	ld.global.nc.u32 	%r117, [%rd45];
	// begin inline asm
	{mul.f16x2 %r115,%r98,%r117;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r604,%r604,%r115;
}
	// end inline asm
	add.s64 	%rd46, %rd45, %rd8;
	ld.global.nc.u32 	%r123, [%rd46];
	// begin inline asm
	{mul.f16x2 %r121,%r98,%r123;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r603,%r603,%r121;
}
	// end inline asm
	add.s64 	%rd47, %rd46, %rd8;
	ld.global.nc.u32 	%r129, [%rd47];
	// begin inline asm
	{mul.f16x2 %r127,%r98,%r129;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r602,%r602,%r127;
}
	// end inline asm
	add.s64 	%rd48, %rd47, %rd8;
	ld.global.nc.u32 	%r135, [%rd48];
	// begin inline asm
	{mul.f16x2 %r133,%r98,%r135;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r601,%r601,%r133;
}
	// end inline asm
	add.s32 	%r592, %r592, 224;
	add.s64 	%rd80, %rd80, 896;
	add.s64 	%rd79, %rd79, 896;
	add.s32 	%r584, %r584, -1;
	setp.ne.s32 	%p4, %r584, 0;
	@%p4 bra 	$L__BB118_5;

$L__BB118_6:
	setp.lt.u32 	%p5, %r6, 672;
	@%p5 bra 	$L__BB118_9;

	add.s32 	%r139, %r592, %r59;
	shl.b32 	%r140, %r59, 1;
	add.s32 	%r141, %r592, %r140;
	mad.lo.s32 	%r142, %r59, 3, %r592;
	shl.b32 	%r143, %r59, 2;
	add.s32 	%r144, %r592, %r143;
	mad.lo.s32 	%r145, %r59, 5, %r592;
	mad.lo.s32 	%r146, %r59, 6, %r592;
	add.s32 	%r147, %r139, 224;
	mul.wide.s32 	%rd49, %r147, 4;
	shl.b64 	%rd50, %rd5, 1;
	add.s64 	%rd16, %rd49, %rd50;
	mul.wide.s32 	%rd51, %r141, 4;
	add.s64 	%rd17, %rd51, %rd50;
	mul.wide.s32 	%rd52, %r142, 4;
	add.s64 	%rd18, %rd52, %rd50;
	mul.wide.s32 	%rd53, %r144, 4;
	add.s64 	%rd19, %rd53, %rd50;
	mul.wide.s32 	%rd54, %r145, 4;
	add.s64 	%rd20, %rd54, %rd50;
	mul.wide.s32 	%rd55, %r146, 4;
	add.s64 	%rd21, %rd55, %rd50;
	mul.wide.s32 	%rd56, %r592, 2;
	add.s64 	%rd57, %rd56, %rd4;
	shl.b64 	%rd58, %rd57, 1;
	add.s64 	%rd59, %rd2, %rd58;
	add.s64 	%rd81, %rd59, 1792;
	mul.wide.s32 	%rd60, %r592, 4;
	add.s64 	%rd23, %rd60, %rd50;
	mul.wide.s32 	%rd61, %r59, 4;
	add.s64 	%rd62, %rd60, %rd61;
	add.s64 	%rd24, %rd62, %rd50;

$L__BB118_8:
	ld.global.nc.u32 	%r149, [%rd81+-1792];
	add.s64 	%rd63, %rd82, %rd23;
	ld.global.nc.u32 	%r150, [%rd63];
	// begin inline asm
	{mul.f16x2 %r148,%r149,%r150;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r151,%r607,%r148;
}
	// end inline asm
	add.s64 	%rd64, %rd82, %rd24;
	ld.global.nc.u32 	%r156, [%rd64];
	// begin inline asm
	{mul.f16x2 %r154,%r149,%r156;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r157,%r606,%r154;
}
	// end inline asm
	add.s64 	%rd65, %rd82, %rd17;
	ld.global.nc.u32 	%r162, [%rd65];
	// begin inline asm
	{mul.f16x2 %r160,%r149,%r162;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r163,%r605,%r160;
}
	// end inline asm
	add.s64 	%rd66, %rd82, %rd18;
	ld.global.nc.u32 	%r168, [%rd66];
	// begin inline asm
	{mul.f16x2 %r166,%r149,%r168;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r169,%r604,%r166;
}
	// end inline asm
	add.s64 	%rd67, %rd82, %rd19;
	ld.global.nc.u32 	%r174, [%rd67];
	// begin inline asm
	{mul.f16x2 %r172,%r149,%r174;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r175,%r603,%r172;
}
	// end inline asm
	add.s64 	%rd68, %rd82, %rd20;
	ld.global.nc.u32 	%r180, [%rd68];
	// begin inline asm
	{mul.f16x2 %r178,%r149,%r180;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r181,%r602,%r178;
}
	// end inline asm
	add.s64 	%rd69, %rd82, %rd21;
	ld.global.nc.u32 	%r186, [%rd69];
	// begin inline asm
	{mul.f16x2 %r184,%r149,%r186;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r187,%r601,%r184;
}
	// end inline asm
	ld.global.nc.u32 	%r191, [%rd81+-896];
	ld.global.nc.u32 	%r192, [%rd63+896];
	// begin inline asm
	{mul.f16x2 %r190,%r191,%r192;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r193,%r151,%r190;
}
	// end inline asm
	add.s64 	%rd70, %rd82, %rd16;
	ld.global.nc.u32 	%r198, [%rd70];
	// begin inline asm
	{mul.f16x2 %r196,%r191,%r198;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r199,%r157,%r196;
}
	// end inline asm
	ld.global.nc.u32 	%r204, [%rd65+896];
	// begin inline asm
	{mul.f16x2 %r202,%r191,%r204;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r205,%r163,%r202;
}
	// end inline asm
	ld.global.nc.u32 	%r210, [%rd66+896];
	// begin inline asm
	{mul.f16x2 %r208,%r191,%r210;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r211,%r169,%r208;
}
	// end inline asm
	ld.global.nc.u32 	%r216, [%rd67+896];
	// begin inline asm
	{mul.f16x2 %r214,%r191,%r216;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r217,%r175,%r214;
}
	// end inline asm
	ld.global.nc.u32 	%r222, [%rd68+896];
	// begin inline asm
	{mul.f16x2 %r220,%r191,%r222;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r223,%r181,%r220;
}
	// end inline asm
	ld.global.nc.u32 	%r228, [%rd69+896];
	// begin inline asm
	{mul.f16x2 %r226,%r191,%r228;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r229,%r187,%r226;
}
	// end inline asm
	ld.global.nc.u32 	%r233, [%rd81];
	ld.global.nc.u32 	%r234, [%rd63+1792];
	// begin inline asm
	{mul.f16x2 %r232,%r233,%r234;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r235,%r193,%r232;
}
	// end inline asm
	ld.global.nc.u32 	%r240, [%rd70+896];
	// begin inline asm
	{mul.f16x2 %r238,%r233,%r240;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r241,%r199,%r238;
}
	// end inline asm
	ld.global.nc.u32 	%r246, [%rd65+1792];
	// begin inline asm
	{mul.f16x2 %r244,%r233,%r246;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r247,%r205,%r244;
}
	// end inline asm
	ld.global.nc.u32 	%r252, [%rd66+1792];
	// begin inline asm
	{mul.f16x2 %r250,%r233,%r252;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r253,%r211,%r250;
}
	// end inline asm
	ld.global.nc.u32 	%r258, [%rd67+1792];
	// begin inline asm
	{mul.f16x2 %r256,%r233,%r258;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r259,%r217,%r256;
}
	// end inline asm
	ld.global.nc.u32 	%r264, [%rd68+1792];
	// begin inline asm
	{mul.f16x2 %r262,%r233,%r264;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r265,%r223,%r262;
}
	// end inline asm
	ld.global.nc.u32 	%r270, [%rd69+1792];
	// begin inline asm
	{mul.f16x2 %r268,%r233,%r270;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r271,%r229,%r268;
}
	// end inline asm
	ld.global.nc.u32 	%r275, [%rd81+896];
	ld.global.nc.u32 	%r276, [%rd63+2688];
	// begin inline asm
	{mul.f16x2 %r274,%r275,%r276;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r607,%r235,%r274;
}
	// end inline asm
	ld.global.nc.u32 	%r282, [%rd70+1792];
	// begin inline asm
	{mul.f16x2 %r280,%r275,%r282;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r606,%r241,%r280;
}
	// end inline asm
	ld.global.nc.u32 	%r288, [%rd65+2688];
	// begin inline asm
	{mul.f16x2 %r286,%r275,%r288;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r605,%r247,%r286;
}
	// end inline asm
	ld.global.nc.u32 	%r294, [%rd66+2688];
	// begin inline asm
	{mul.f16x2 %r292,%r275,%r294;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r604,%r253,%r292;
}
	// end inline asm
	ld.global.nc.u32 	%r300, [%rd67+2688];
	// begin inline asm
	{mul.f16x2 %r298,%r275,%r300;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r603,%r259,%r298;
}
	// end inline asm
	ld.global.nc.u32 	%r306, [%rd68+2688];
	// begin inline asm
	{mul.f16x2 %r304,%r275,%r306;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r602,%r265,%r304;
}
	// end inline asm
	ld.global.nc.u32 	%r312, [%rd69+2688];
	// begin inline asm
	{mul.f16x2 %r310,%r275,%r312;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r601,%r271,%r310;
}
	// end inline asm
	add.s64 	%rd82, %rd82, 3584;
	add.s64 	%rd81, %rd81, 3584;
	add.s32 	%r592, %r592, 896;
	setp.lt.s32 	%p6, %r592, %r58;
	@%p6 bra 	$L__BB118_8;

$L__BB118_9:
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r607;
  cvt.f32.f16 %f9, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r607;
  cvt.f32.f16 %f10, high;}

	// end inline asm
	add.f32 	%f23, %f9, %f10;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r606;
  cvt.f32.f16 %f11, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r606;
  cvt.f32.f16 %f12, high;}

	// end inline asm
	add.f32 	%f1, %f11, %f12;
	st.local.f32 	[%rd3+4], %f1;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r605;
  cvt.f32.f16 %f13, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r605;
  cvt.f32.f16 %f14, high;}

	// end inline asm
	add.f32 	%f2, %f13, %f14;
	st.local.f32 	[%rd3+8], %f2;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r604;
  cvt.f32.f16 %f15, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r604;
  cvt.f32.f16 %f16, high;}

	// end inline asm
	add.f32 	%f3, %f15, %f16;
	st.local.f32 	[%rd3+12], %f3;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r603;
  cvt.f32.f16 %f17, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r603;
  cvt.f32.f16 %f18, high;}

	// end inline asm
	add.f32 	%f4, %f17, %f18;
	st.local.f32 	[%rd3+16], %f4;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r602;
  cvt.f32.f16 %f19, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r602;
  cvt.f32.f16 %f20, high;}

	// end inline asm
	add.f32 	%f5, %f19, %f20;
	st.local.f32 	[%rd3+20], %f5;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r601;
  cvt.f32.f16 %f21, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r601;
  cvt.f32.f16 %f22, high;}

	// end inline asm
	add.f32 	%f6, %f21, %f22;
	st.local.f32 	[%rd3+24], %f6;
	shr.s32 	%r330, %r3, 31;
	shr.u32 	%r331, %r330, 27;
	add.s32 	%r332, %r3, %r331;
	shr.s32 	%r333, %r332, 5;
	shl.b32 	%r334, %r333, 2;
	add.s32 	%r57, %r71, %r334;
	mov.u32 	%r336, 2;
	mov.b32 	%r337, %f23;
	mov.u32 	%r338, 31;
	mov.u32 	%r339, 16;
	mov.u32 	%r340, -1;
	shfl.sync.bfly.b32 	%r341|%p7, %r337, %r339, %r338, %r340;
	mov.b32 	%f24, %r341;
	add.f32 	%f25, %f23, %f24;
	mov.b32 	%r342, %f25;
	mov.u32 	%r343, 8;
	shfl.sync.bfly.b32 	%r344|%p8, %r342, %r343, %r338, %r340;
	mov.b32 	%f26, %r344;
	add.f32 	%f27, %f25, %f26;
	mov.b32 	%r345, %f27;
	mov.u32 	%r346, 4;
	shfl.sync.bfly.b32 	%r347|%p9, %r345, %r346, %r338, %r340;
	mov.b32 	%f28, %r347;
	add.f32 	%f29, %f27, %f28;
	mov.b32 	%r348, %f29;
	shfl.sync.bfly.b32 	%r349|%p10, %r348, %r336, %r338, %r340;
	mov.b32 	%f30, %r349;
	add.f32 	%f31, %f29, %f30;
	mov.b32 	%r350, %f31;
	mov.u32 	%r351, 1;
	shfl.sync.bfly.b32 	%r352|%p11, %r350, %r351, %r338, %r340;
	mov.b32 	%f32, %r352;
	add.f32 	%f33, %f31, %f32;
	st.local.f32 	[%rd3], %f33;
	st.shared.f32 	[%r57], %f33;
	bar.sync 	0;
	@%p1 bra 	$L__BB118_11;

	ld.shared.f32 	%f34, [%r4];
	mov.b32 	%r353, %f34;
	shfl.sync.bfly.b32 	%r357|%p13, %r353, %r339, %r338, %r340;
	mov.b32 	%f35, %r357;
	add.f32 	%f36, %f34, %f35;
	mov.b32 	%r358, %f36;
	shfl.sync.bfly.b32 	%r360|%p14, %r358, %r343, %r338, %r340;
	mov.b32 	%f37, %r360;
	add.f32 	%f38, %f36, %f37;
	mov.b32 	%r361, %f38;
	shfl.sync.bfly.b32 	%r363|%p15, %r361, %r346, %r338, %r340;
	mov.b32 	%f39, %r363;
	add.f32 	%f40, %f38, %f39;
	mov.b32 	%r364, %f40;
	shfl.sync.bfly.b32 	%r366|%p16, %r364, %r336, %r338, %r340;
	mov.b32 	%f41, %r366;
	add.f32 	%f42, %f40, %f41;
	mov.b32 	%r367, %f42;
	shfl.sync.bfly.b32 	%r369|%p17, %r367, %r351, %r338, %r340;
	mov.b32 	%f43, %r369;
	add.f32 	%f44, %f42, %f43;
	st.local.f32 	[%rd3], %f44;

$L__BB118_11:
	bar.sync 	0;
	mov.b32 	%r370, %f1;
	shfl.sync.bfly.b32 	%r374|%p19, %r370, %r339, %r338, %r340;
	mov.b32 	%f45, %r374;
	add.f32 	%f46, %f1, %f45;
	mov.b32 	%r375, %f46;
	shfl.sync.bfly.b32 	%r377|%p20, %r375, %r343, %r338, %r340;
	mov.b32 	%f47, %r377;
	add.f32 	%f48, %f46, %f47;
	mov.b32 	%r378, %f48;
	shfl.sync.bfly.b32 	%r380|%p21, %r378, %r346, %r338, %r340;
	mov.b32 	%f49, %r380;
	add.f32 	%f50, %f48, %f49;
	mov.b32 	%r381, %f50;
	shfl.sync.bfly.b32 	%r383|%p22, %r381, %r336, %r338, %r340;
	mov.b32 	%f51, %r383;
	add.f32 	%f52, %f50, %f51;
	mov.b32 	%r384, %f52;
	shfl.sync.bfly.b32 	%r386|%p23, %r384, %r351, %r338, %r340;
	mov.b32 	%f53, %r386;
	add.f32 	%f54, %f52, %f53;
	st.local.f32 	[%rd3+4], %f54;
	st.shared.f32 	[%r57], %f54;
	bar.sync 	0;
	@%p1 bra 	$L__BB118_13;

	ld.shared.f32 	%f55, [%r4];
	mov.b32 	%r387, %f55;
	mov.u32 	%r388, 31;
	mov.u32 	%r389, 16;
	mov.u32 	%r390, -1;
	shfl.sync.bfly.b32 	%r391|%p24, %r387, %r389, %r388, %r390;
	mov.b32 	%f56, %r391;
	add.f32 	%f57, %f55, %f56;
	mov.b32 	%r392, %f57;
	mov.u32 	%r393, 8;
	shfl.sync.bfly.b32 	%r394|%p25, %r392, %r393, %r388, %r390;
	mov.b32 	%f58, %r394;
	add.f32 	%f59, %f57, %f58;
	mov.b32 	%r395, %f59;
	mov.u32 	%r396, 4;
	shfl.sync.bfly.b32 	%r397|%p26, %r395, %r396, %r388, %r390;
	mov.b32 	%f60, %r397;
	add.f32 	%f61, %f59, %f60;
	mov.b32 	%r398, %f61;
	mov.u32 	%r399, 2;
	shfl.sync.bfly.b32 	%r400|%p27, %r398, %r399, %r388, %r390;
	mov.b32 	%f62, %r400;
	add.f32 	%f63, %f61, %f62;
	mov.b32 	%r401, %f63;
	mov.u32 	%r402, 1;
	shfl.sync.bfly.b32 	%r403|%p28, %r401, %r402, %r388, %r390;
	mov.b32 	%f64, %r403;
	add.f32 	%f65, %f63, %f64;
	st.local.f32 	[%rd3+4], %f65;

$L__BB118_13:
	bar.sync 	0;
	mov.b32 	%r404, %f2;
	mov.u32 	%r405, 31;
	mov.u32 	%r406, 16;
	mov.u32 	%r407, -1;
	shfl.sync.bfly.b32 	%r408|%p30, %r404, %r406, %r405, %r407;
	mov.b32 	%f66, %r408;
	add.f32 	%f67, %f2, %f66;
	mov.b32 	%r409, %f67;
	mov.u32 	%r410, 8;
	shfl.sync.bfly.b32 	%r411|%p31, %r409, %r410, %r405, %r407;
	mov.b32 	%f68, %r411;
	add.f32 	%f69, %f67, %f68;
	mov.b32 	%r412, %f69;
	mov.u32 	%r413, 4;
	shfl.sync.bfly.b32 	%r414|%p32, %r412, %r413, %r405, %r407;
	mov.b32 	%f70, %r414;
	add.f32 	%f71, %f69, %f70;
	mov.b32 	%r415, %f71;
	mov.u32 	%r416, 2;
	shfl.sync.bfly.b32 	%r417|%p33, %r415, %r416, %r405, %r407;
	mov.b32 	%f72, %r417;
	add.f32 	%f73, %f71, %f72;
	mov.b32 	%r418, %f73;
	mov.u32 	%r419, 1;
	shfl.sync.bfly.b32 	%r420|%p34, %r418, %r419, %r405, %r407;
	mov.b32 	%f74, %r420;
	add.f32 	%f75, %f73, %f74;
	st.local.f32 	[%rd3+8], %f75;
	st.shared.f32 	[%r57], %f75;
	bar.sync 	0;
	@%p1 bra 	$L__BB118_15;

	ld.shared.f32 	%f76, [%r4];
	mov.b32 	%r421, %f76;
	shfl.sync.bfly.b32 	%r425|%p35, %r421, %r406, %r405, %r407;
	mov.b32 	%f77, %r425;
	add.f32 	%f78, %f76, %f77;
	mov.b32 	%r426, %f78;
	shfl.sync.bfly.b32 	%r428|%p36, %r426, %r410, %r405, %r407;
	mov.b32 	%f79, %r428;
	add.f32 	%f80, %f78, %f79;
	mov.b32 	%r429, %f80;
	shfl.sync.bfly.b32 	%r431|%p37, %r429, %r413, %r405, %r407;
	mov.b32 	%f81, %r431;
	add.f32 	%f82, %f80, %f81;
	mov.b32 	%r432, %f82;
	shfl.sync.bfly.b32 	%r434|%p38, %r432, %r416, %r405, %r407;
	mov.b32 	%f83, %r434;
	add.f32 	%f84, %f82, %f83;
	mov.b32 	%r435, %f84;
	shfl.sync.bfly.b32 	%r437|%p39, %r435, %r419, %r405, %r407;
	mov.b32 	%f85, %r437;
	add.f32 	%f86, %f84, %f85;
	st.local.f32 	[%rd3+8], %f86;

$L__BB118_15:
	bar.sync 	0;
	mov.b32 	%r438, %f3;
	shfl.sync.bfly.b32 	%r442|%p41, %r438, %r406, %r405, %r407;
	mov.b32 	%f87, %r442;
	add.f32 	%f88, %f3, %f87;
	mov.b32 	%r443, %f88;
	shfl.sync.bfly.b32 	%r445|%p42, %r443, %r410, %r405, %r407;
	mov.b32 	%f89, %r445;
	add.f32 	%f90, %f88, %f89;
	mov.b32 	%r446, %f90;
	shfl.sync.bfly.b32 	%r448|%p43, %r446, %r413, %r405, %r407;
	mov.b32 	%f91, %r448;
	add.f32 	%f92, %f90, %f91;
	mov.b32 	%r449, %f92;
	shfl.sync.bfly.b32 	%r451|%p44, %r449, %r416, %r405, %r407;
	mov.b32 	%f93, %r451;
	add.f32 	%f94, %f92, %f93;
	mov.b32 	%r452, %f94;
	shfl.sync.bfly.b32 	%r454|%p45, %r452, %r419, %r405, %r407;
	mov.b32 	%f95, %r454;
	add.f32 	%f96, %f94, %f95;
	st.local.f32 	[%rd3+12], %f96;
	st.shared.f32 	[%r57], %f96;
	bar.sync 	0;
	@%p1 bra 	$L__BB118_17;

	ld.shared.f32 	%f97, [%r4];
	mov.b32 	%r455, %f97;
	mov.u32 	%r456, 31;
	mov.u32 	%r457, 16;
	mov.u32 	%r458, -1;
	shfl.sync.bfly.b32 	%r459|%p46, %r455, %r457, %r456, %r458;
	mov.b32 	%f98, %r459;
	add.f32 	%f99, %f97, %f98;
	mov.b32 	%r460, %f99;
	mov.u32 	%r461, 8;
	shfl.sync.bfly.b32 	%r462|%p47, %r460, %r461, %r456, %r458;
	mov.b32 	%f100, %r462;
	add.f32 	%f101, %f99, %f100;
	mov.b32 	%r463, %f101;
	mov.u32 	%r464, 4;
	shfl.sync.bfly.b32 	%r465|%p48, %r463, %r464, %r456, %r458;
	mov.b32 	%f102, %r465;
	add.f32 	%f103, %f101, %f102;
	mov.b32 	%r466, %f103;
	mov.u32 	%r467, 2;
	shfl.sync.bfly.b32 	%r468|%p49, %r466, %r467, %r456, %r458;
	mov.b32 	%f104, %r468;
	add.f32 	%f105, %f103, %f104;
	mov.b32 	%r469, %f105;
	mov.u32 	%r470, 1;
	shfl.sync.bfly.b32 	%r471|%p50, %r469, %r470, %r456, %r458;
	mov.b32 	%f106, %r471;
	add.f32 	%f107, %f105, %f106;
	st.local.f32 	[%rd3+12], %f107;

$L__BB118_17:
	bar.sync 	0;
	mov.b32 	%r472, %f4;
	mov.u32 	%r473, 31;
	mov.u32 	%r474, 16;
	mov.u32 	%r475, -1;
	shfl.sync.bfly.b32 	%r476|%p52, %r472, %r474, %r473, %r475;
	mov.b32 	%f108, %r476;
	add.f32 	%f109, %f4, %f108;
	mov.b32 	%r477, %f109;
	mov.u32 	%r478, 8;
	shfl.sync.bfly.b32 	%r479|%p53, %r477, %r478, %r473, %r475;
	mov.b32 	%f110, %r479;
	add.f32 	%f111, %f109, %f110;
	mov.b32 	%r480, %f111;
	mov.u32 	%r481, 4;
	shfl.sync.bfly.b32 	%r482|%p54, %r480, %r481, %r473, %r475;
	mov.b32 	%f112, %r482;
	add.f32 	%f113, %f111, %f112;
	mov.b32 	%r483, %f113;
	mov.u32 	%r484, 2;
	shfl.sync.bfly.b32 	%r485|%p55, %r483, %r484, %r473, %r475;
	mov.b32 	%f114, %r485;
	add.f32 	%f115, %f113, %f114;
	mov.b32 	%r486, %f115;
	mov.u32 	%r487, 1;
	shfl.sync.bfly.b32 	%r488|%p56, %r486, %r487, %r473, %r475;
	mov.b32 	%f116, %r488;
	add.f32 	%f117, %f115, %f116;
	st.local.f32 	[%rd3+16], %f117;
	st.shared.f32 	[%r57], %f117;
	bar.sync 	0;
	@%p1 bra 	$L__BB118_19;

	ld.shared.f32 	%f118, [%r4];
	mov.b32 	%r489, %f118;
	shfl.sync.bfly.b32 	%r493|%p57, %r489, %r474, %r473, %r475;
	mov.b32 	%f119, %r493;
	add.f32 	%f120, %f118, %f119;
	mov.b32 	%r494, %f120;
	shfl.sync.bfly.b32 	%r496|%p58, %r494, %r478, %r473, %r475;
	mov.b32 	%f121, %r496;
	add.f32 	%f122, %f120, %f121;
	mov.b32 	%r497, %f122;
	shfl.sync.bfly.b32 	%r499|%p59, %r497, %r481, %r473, %r475;
	mov.b32 	%f123, %r499;
	add.f32 	%f124, %f122, %f123;
	mov.b32 	%r500, %f124;
	shfl.sync.bfly.b32 	%r502|%p60, %r500, %r484, %r473, %r475;
	mov.b32 	%f125, %r502;
	add.f32 	%f126, %f124, %f125;
	mov.b32 	%r503, %f126;
	shfl.sync.bfly.b32 	%r505|%p61, %r503, %r487, %r473, %r475;
	mov.b32 	%f127, %r505;
	add.f32 	%f128, %f126, %f127;
	st.local.f32 	[%rd3+16], %f128;

$L__BB118_19:
	bar.sync 	0;
	mov.b32 	%r506, %f5;
	shfl.sync.bfly.b32 	%r510|%p63, %r506, %r474, %r473, %r475;
	mov.b32 	%f129, %r510;
	add.f32 	%f130, %f5, %f129;
	mov.b32 	%r511, %f130;
	shfl.sync.bfly.b32 	%r513|%p64, %r511, %r478, %r473, %r475;
	mov.b32 	%f131, %r513;
	add.f32 	%f132, %f130, %f131;
	mov.b32 	%r514, %f132;
	shfl.sync.bfly.b32 	%r516|%p65, %r514, %r481, %r473, %r475;
	mov.b32 	%f133, %r516;
	add.f32 	%f134, %f132, %f133;
	mov.b32 	%r517, %f134;
	shfl.sync.bfly.b32 	%r519|%p66, %r517, %r484, %r473, %r475;
	mov.b32 	%f135, %r519;
	add.f32 	%f136, %f134, %f135;
	mov.b32 	%r520, %f136;
	shfl.sync.bfly.b32 	%r522|%p67, %r520, %r487, %r473, %r475;
	mov.b32 	%f137, %r522;
	add.f32 	%f138, %f136, %f137;
	st.local.f32 	[%rd3+20], %f138;
	st.shared.f32 	[%r57], %f138;
	bar.sync 	0;
	@%p1 bra 	$L__BB118_21;

	ld.shared.f32 	%f139, [%r4];
	mov.b32 	%r523, %f139;
	mov.u32 	%r524, 31;
	mov.u32 	%r525, 16;
	mov.u32 	%r526, -1;
	shfl.sync.bfly.b32 	%r527|%p68, %r523, %r525, %r524, %r526;
	mov.b32 	%f140, %r527;
	add.f32 	%f141, %f139, %f140;
	mov.b32 	%r528, %f141;
	mov.u32 	%r529, 8;
	shfl.sync.bfly.b32 	%r530|%p69, %r528, %r529, %r524, %r526;
	mov.b32 	%f142, %r530;
	add.f32 	%f143, %f141, %f142;
	mov.b32 	%r531, %f143;
	mov.u32 	%r532, 4;
	shfl.sync.bfly.b32 	%r533|%p70, %r531, %r532, %r524, %r526;
	mov.b32 	%f144, %r533;
	add.f32 	%f145, %f143, %f144;
	mov.b32 	%r534, %f145;
	mov.u32 	%r535, 2;
	shfl.sync.bfly.b32 	%r536|%p71, %r534, %r535, %r524, %r526;
	mov.b32 	%f146, %r536;
	add.f32 	%f147, %f145, %f146;
	mov.b32 	%r537, %f147;
	mov.u32 	%r538, 1;
	shfl.sync.bfly.b32 	%r539|%p72, %r537, %r538, %r524, %r526;
	mov.b32 	%f148, %r539;
	add.f32 	%f149, %f147, %f148;
	st.local.f32 	[%rd3+20], %f149;

$L__BB118_21:
	bar.sync 	0;
	mov.b32 	%r540, %f6;
	mov.u32 	%r541, 31;
	mov.u32 	%r542, 16;
	mov.u32 	%r543, -1;
	shfl.sync.bfly.b32 	%r544|%p74, %r540, %r542, %r541, %r543;
	mov.b32 	%f150, %r544;
	add.f32 	%f151, %f6, %f150;
	mov.b32 	%r545, %f151;
	mov.u32 	%r546, 8;
	shfl.sync.bfly.b32 	%r547|%p75, %r545, %r546, %r541, %r543;
	mov.b32 	%f152, %r547;
	add.f32 	%f153, %f151, %f152;
	mov.b32 	%r548, %f153;
	mov.u32 	%r549, 4;
	shfl.sync.bfly.b32 	%r550|%p76, %r548, %r549, %r541, %r543;
	mov.b32 	%f154, %r550;
	add.f32 	%f155, %f153, %f154;
	mov.b32 	%r551, %f155;
	mov.u32 	%r552, 2;
	shfl.sync.bfly.b32 	%r553|%p77, %r551, %r552, %r541, %r543;
	mov.b32 	%f156, %r553;
	add.f32 	%f157, %f155, %f156;
	mov.b32 	%r554, %f157;
	mov.u32 	%r555, 1;
	shfl.sync.bfly.b32 	%r556|%p78, %r554, %r555, %r541, %r543;
	mov.b32 	%f158, %r556;
	add.f32 	%f159, %f157, %f158;
	st.local.f32 	[%rd3+24], %f159;
	st.shared.f32 	[%r57], %f159;
	bar.sync 	0;
	@%p1 bra 	$L__BB118_23;

	ld.shared.f32 	%f160, [%r4];
	mov.b32 	%r557, %f160;
	shfl.sync.bfly.b32 	%r561|%p79, %r557, %r542, %r541, %r543;
	mov.b32 	%f161, %r561;
	add.f32 	%f162, %f160, %f161;
	mov.b32 	%r562, %f162;
	shfl.sync.bfly.b32 	%r564|%p80, %r562, %r546, %r541, %r543;
	mov.b32 	%f163, %r564;
	add.f32 	%f164, %f162, %f163;
	mov.b32 	%r565, %f164;
	shfl.sync.bfly.b32 	%r567|%p81, %r565, %r549, %r541, %r543;
	mov.b32 	%f165, %r567;
	add.f32 	%f166, %f164, %f165;
	mov.b32 	%r568, %f166;
	shfl.sync.bfly.b32 	%r570|%p82, %r568, %r552, %r541, %r543;
	mov.b32 	%f167, %r570;
	add.f32 	%f168, %f166, %f167;
	mov.b32 	%r571, %f168;
	shfl.sync.bfly.b32 	%r573|%p83, %r571, %r555, %r541, %r543;
	mov.b32 	%f169, %r573;
	add.f32 	%f170, %f168, %f169;
	st.local.f32 	[%rd3+24], %f170;

$L__BB118_23:
	bar.sync 	0;
	setp.gt.s32 	%p84, %r3, 6;
	@%p84 bra 	$L__BB118_25;

	mad.lo.s32 	%r574, %r3, %r60, %r2;
	cvt.s64.s32 	%rd71, %r574;
	mul.lo.s32 	%r575, %r1, %r61;
	cvt.s64.s32 	%rd72, %r575;
	add.s64 	%rd73, %rd72, %rd71;
	mul.wide.s32 	%rd74, %r3, 4;
	add.s64 	%rd75, %rd3, %rd74;
	ld.local.f32 	%f171, [%rd75];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs3, %f171;}

	// end inline asm
	cvta.to.global.u64 	%rd76, %rd29;
	shl.b64 	%rd77, %rd73, 1;
	add.s64 	%rd78, %rd76, %rd77;
	st.global.u16 	[%rd78], %rs3;

$L__BB118_25:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_8_bs_224
.visible .entry ggml_matvec_f16_ncols_8_bs_224(
	.param .u64 ggml_matvec_f16_ncols_8_bs_224_param_0,
	.param .u64 ggml_matvec_f16_ncols_8_bs_224_param_1,
	.param .u64 ggml_matvec_f16_ncols_8_bs_224_param_2,
	.param .u32 ggml_matvec_f16_ncols_8_bs_224_param_3,
	.param .u32 ggml_matvec_f16_ncols_8_bs_224_param_4,
	.param .u32 ggml_matvec_f16_ncols_8_bs_224_param_5,
	.param .u32 ggml_matvec_f16_ncols_8_bs_224_param_6,
	.param .u32 ggml_matvec_f16_ncols_8_bs_224_param_7,
	.param .u32 ggml_matvec_f16_ncols_8_bs_224_param_8,
	.param .u32 ggml_matvec_f16_ncols_8_bs_224_param_9,
	.param .u32 ggml_matvec_f16_ncols_8_bs_224_param_10,
	.param .u32 ggml_matvec_f16_ncols_8_bs_224_param_11
)
{
	.local .align 16 .b8 	__local_depot119[32];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<96>;
	.reg .b16 	%rs<4>;
	.reg .f32 	%f<196>;
	.reg .b32 	%r<688>;
	.reg .b64 	%rd<87>;


	mov.u64 	%SPL, __local_depot119;
	ld.param.u64 	%rd31, [ggml_matvec_f16_ncols_8_bs_224_param_0];
	ld.param.u64 	%rd32, [ggml_matvec_f16_ncols_8_bs_224_param_1];
	ld.param.u64 	%rd30, [ggml_matvec_f16_ncols_8_bs_224_param_2];
	ld.param.u32 	%r64, [ggml_matvec_f16_ncols_8_bs_224_param_3];
	ld.param.u32 	%r68, [ggml_matvec_f16_ncols_8_bs_224_param_5];
	ld.param.u32 	%r65, [ggml_matvec_f16_ncols_8_bs_224_param_6];
	ld.param.u32 	%r66, [ggml_matvec_f16_ncols_8_bs_224_param_7];
	ld.param.u32 	%r69, [ggml_matvec_f16_ncols_8_bs_224_param_8];
	ld.param.u32 	%r70, [ggml_matvec_f16_ncols_8_bs_224_param_9];
	ld.param.u32 	%r71, [ggml_matvec_f16_ncols_8_bs_224_param_10];
	ld.param.u32 	%r67, [ggml_matvec_f16_ncols_8_bs_224_param_11];
	cvta.to.global.u64 	%rd86, %rd32;
	cvta.to.global.u64 	%rd2, %rd31;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r72, %r1, %r69;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r73, %r2, %r68;
	mad.lo.s32 	%r74, %r72, %r70, %r73;
	cvt.s64.s32 	%rd4, %r74;
	mul.lo.s32 	%r75, %r1, %r71;
	cvt.s64.s32 	%rd5, %r75;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r76, %r3, 2;
	mov.u32 	%r77, data_mmv;
	add.s32 	%r4, %r77, %r76;
	@%p1 bra 	$L__BB119_2;

	mov.u32 	%r78, 0;
	st.shared.u32 	[%r4], %r78;

$L__BB119_2:
	bar.sync 	0;
	mov.f32 	%f9, 0f00000000;
	st.local.v4.f32 	[%rd3], {%f9, %f9, %f9, %f9};
	st.local.v4.f32 	[%rd3+16], {%f9, %f9, %f9, %f9};
	mov.u32 	%r680, 0;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs2, %f9;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f9;}

	// end inline asm
	mov.b32 	%r687, {%rs1, %rs2};
	setp.ge.s32 	%p2, %r3, %r64;
	mov.u32 	%r681, %r680;
	mov.u32 	%r682, %r680;
	mov.u32 	%r683, %r680;
	mov.u32 	%r684, %r680;
	mov.u32 	%r685, %r680;
	mov.u32 	%r686, %r680;
	@%p2 bra 	$L__BB119_9;

	not.b32 	%r93, %r3;
	add.s32 	%r6, %r93, %r64;
	shr.u32 	%r94, %r6, 5;
	mul.wide.u32 	%rd34, %r94, 613566757;
	shr.u64 	%rd35, %rd34, 32;
	cvt.u32.u64 	%r95, %rd35;
	add.s32 	%r96, %r95, 1;
	and.b32  	%r661, %r96, 3;
	setp.eq.s32 	%p3, %r661, 0;
	mov.u32 	%r680, 0;
	mov.u32 	%r670, %r3;
	@%p3 bra 	$L__BB119_6;

	shl.b32 	%r104, %r65, 1;
	add.s32 	%r105, %r3, %r104;
	mul.wide.s32 	%rd36, %r105, 4;
	shl.b64 	%rd37, %rd5, 1;
	add.s64 	%rd7, %rd36, %rd37;
	mul.wide.s32 	%rd38, %r3, 4;
	mul.wide.s32 	%rd8, %r65, 4;
	add.s64 	%rd39, %rd38, %rd8;
	add.s64 	%rd9, %rd39, %rd37;
	add.s64 	%rd10, %rd38, %rd37;
	mul.wide.s32 	%rd40, %r3, 2;
	add.s64 	%rd41, %rd40, %rd4;
	shl.b64 	%rd42, %rd41, 1;
	add.s64 	%rd83, %rd2, %rd42;
	mov.u32 	%r680, 0;
	mov.u64 	%rd84, %rd86;
	mov.u32 	%r681, %r680;
	mov.u32 	%r682, %r680;
	mov.u32 	%r683, %r680;
	mov.u32 	%r684, %r680;
	mov.u32 	%r685, %r680;
	mov.u32 	%r686, %r680;
	mov.u32 	%r670, %r3;

$L__BB119_5:
	.pragma "nounroll";
	ld.global.nc.u32 	%r107, [%rd83];
	add.s64 	%rd43, %rd84, %rd10;
	ld.global.nc.u32 	%r108, [%rd43];
	// begin inline asm
	{mul.f16x2 %r106,%r107,%r108;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r687,%r687,%r106;
}
	// end inline asm
	add.s64 	%rd44, %rd84, %rd9;
	ld.global.nc.u32 	%r114, [%rd44];
	// begin inline asm
	{mul.f16x2 %r112,%r107,%r114;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r686,%r686,%r112;
}
	// end inline asm
	add.s64 	%rd45, %rd84, %rd7;
	ld.global.nc.u32 	%r120, [%rd45];
	// begin inline asm
	{mul.f16x2 %r118,%r107,%r120;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r685,%r685,%r118;
}
	// end inline asm
	add.s64 	%rd46, %rd45, %rd8;
	ld.global.nc.u32 	%r126, [%rd46];
	// begin inline asm
	{mul.f16x2 %r124,%r107,%r126;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r684,%r684,%r124;
}
	// end inline asm
	add.s64 	%rd47, %rd46, %rd8;
	ld.global.nc.u32 	%r132, [%rd47];
	// begin inline asm
	{mul.f16x2 %r130,%r107,%r132;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r683,%r683,%r130;
}
	// end inline asm
	add.s64 	%rd48, %rd47, %rd8;
	ld.global.nc.u32 	%r138, [%rd48];
	// begin inline asm
	{mul.f16x2 %r136,%r107,%r138;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r682,%r682,%r136;
}
	// end inline asm
	add.s64 	%rd49, %rd48, %rd8;
	ld.global.nc.u32 	%r144, [%rd49];
	// begin inline asm
	{mul.f16x2 %r142,%r107,%r144;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r681,%r681,%r142;
}
	// end inline asm
	add.s64 	%rd50, %rd49, %rd8;
	ld.global.nc.u32 	%r150, [%rd50];
	// begin inline asm
	{mul.f16x2 %r148,%r107,%r150;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r680,%r680,%r148;
}
	// end inline asm
	add.s32 	%r670, %r670, 224;
	add.s64 	%rd84, %rd84, 896;
	add.s64 	%rd83, %rd83, 896;
	add.s32 	%r661, %r661, -1;
	setp.ne.s32 	%p4, %r661, 0;
	@%p4 bra 	$L__BB119_5;

$L__BB119_6:
	setp.lt.u32 	%p5, %r6, 672;
	@%p5 bra 	$L__BB119_9;

	add.s32 	%r154, %r670, %r65;
	shl.b32 	%r155, %r65, 1;
	add.s32 	%r156, %r670, %r155;
	mad.lo.s32 	%r157, %r65, 3, %r670;
	shl.b32 	%r158, %r65, 2;
	add.s32 	%r159, %r670, %r158;
	mad.lo.s32 	%r160, %r65, 5, %r670;
	mad.lo.s32 	%r161, %r65, 6, %r670;
	mad.lo.s32 	%r162, %r65, 7, %r670;
	add.s32 	%r163, %r154, 224;
	mul.wide.s32 	%rd51, %r163, 4;
	shl.b64 	%rd52, %rd5, 1;
	add.s64 	%rd16, %rd51, %rd52;
	mul.wide.s32 	%rd53, %r156, 4;
	add.s64 	%rd17, %rd53, %rd52;
	mul.wide.s32 	%rd54, %r157, 4;
	add.s64 	%rd18, %rd54, %rd52;
	mul.wide.s32 	%rd55, %r159, 4;
	add.s64 	%rd19, %rd55, %rd52;
	mul.wide.s32 	%rd56, %r160, 4;
	add.s64 	%rd20, %rd56, %rd52;
	mul.wide.s32 	%rd57, %r161, 4;
	add.s64 	%rd21, %rd57, %rd52;
	mul.wide.s32 	%rd58, %r162, 4;
	add.s64 	%rd22, %rd58, %rd52;
	mul.wide.s32 	%rd59, %r670, 2;
	add.s64 	%rd60, %rd59, %rd4;
	shl.b64 	%rd61, %rd60, 1;
	add.s64 	%rd62, %rd2, %rd61;
	add.s64 	%rd85, %rd62, 1792;
	mul.wide.s32 	%rd63, %r670, 4;
	add.s64 	%rd24, %rd63, %rd52;
	mul.wide.s32 	%rd64, %r65, 4;
	add.s64 	%rd65, %rd63, %rd64;
	add.s64 	%rd25, %rd65, %rd52;

$L__BB119_8:
	ld.global.nc.u32 	%r165, [%rd85+-1792];
	add.s64 	%rd66, %rd86, %rd24;
	ld.global.nc.u32 	%r166, [%rd66];
	// begin inline asm
	{mul.f16x2 %r164,%r165,%r166;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r167,%r687,%r164;
}
	// end inline asm
	add.s64 	%rd67, %rd86, %rd25;
	ld.global.nc.u32 	%r172, [%rd67];
	// begin inline asm
	{mul.f16x2 %r170,%r165,%r172;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r173,%r686,%r170;
}
	// end inline asm
	add.s64 	%rd68, %rd86, %rd17;
	ld.global.nc.u32 	%r178, [%rd68];
	// begin inline asm
	{mul.f16x2 %r176,%r165,%r178;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r179,%r685,%r176;
}
	// end inline asm
	add.s64 	%rd69, %rd86, %rd18;
	ld.global.nc.u32 	%r184, [%rd69];
	// begin inline asm
	{mul.f16x2 %r182,%r165,%r184;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r185,%r684,%r182;
}
	// end inline asm
	add.s64 	%rd70, %rd86, %rd19;
	ld.global.nc.u32 	%r190, [%rd70];
	// begin inline asm
	{mul.f16x2 %r188,%r165,%r190;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r191,%r683,%r188;
}
	// end inline asm
	add.s64 	%rd71, %rd86, %rd20;
	ld.global.nc.u32 	%r196, [%rd71];
	// begin inline asm
	{mul.f16x2 %r194,%r165,%r196;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r197,%r682,%r194;
}
	// end inline asm
	add.s64 	%rd72, %rd86, %rd21;
	ld.global.nc.u32 	%r202, [%rd72];
	// begin inline asm
	{mul.f16x2 %r200,%r165,%r202;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r203,%r681,%r200;
}
	// end inline asm
	add.s64 	%rd73, %rd86, %rd22;
	ld.global.nc.u32 	%r208, [%rd73];
	// begin inline asm
	{mul.f16x2 %r206,%r165,%r208;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r209,%r680,%r206;
}
	// end inline asm
	ld.global.nc.u32 	%r213, [%rd85+-896];
	ld.global.nc.u32 	%r214, [%rd66+896];
	// begin inline asm
	{mul.f16x2 %r212,%r213,%r214;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r215,%r167,%r212;
}
	// end inline asm
	add.s64 	%rd74, %rd86, %rd16;
	ld.global.nc.u32 	%r220, [%rd74];
	// begin inline asm
	{mul.f16x2 %r218,%r213,%r220;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r221,%r173,%r218;
}
	// end inline asm
	ld.global.nc.u32 	%r226, [%rd68+896];
	// begin inline asm
	{mul.f16x2 %r224,%r213,%r226;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r227,%r179,%r224;
}
	// end inline asm
	ld.global.nc.u32 	%r232, [%rd69+896];
	// begin inline asm
	{mul.f16x2 %r230,%r213,%r232;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r233,%r185,%r230;
}
	// end inline asm
	ld.global.nc.u32 	%r238, [%rd70+896];
	// begin inline asm
	{mul.f16x2 %r236,%r213,%r238;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r239,%r191,%r236;
}
	// end inline asm
	ld.global.nc.u32 	%r244, [%rd71+896];
	// begin inline asm
	{mul.f16x2 %r242,%r213,%r244;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r245,%r197,%r242;
}
	// end inline asm
	ld.global.nc.u32 	%r250, [%rd72+896];
	// begin inline asm
	{mul.f16x2 %r248,%r213,%r250;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r251,%r203,%r248;
}
	// end inline asm
	ld.global.nc.u32 	%r256, [%rd73+896];
	// begin inline asm
	{mul.f16x2 %r254,%r213,%r256;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r257,%r209,%r254;
}
	// end inline asm
	ld.global.nc.u32 	%r261, [%rd85];
	ld.global.nc.u32 	%r262, [%rd66+1792];
	// begin inline asm
	{mul.f16x2 %r260,%r261,%r262;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r263,%r215,%r260;
}
	// end inline asm
	ld.global.nc.u32 	%r268, [%rd74+896];
	// begin inline asm
	{mul.f16x2 %r266,%r261,%r268;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r269,%r221,%r266;
}
	// end inline asm
	ld.global.nc.u32 	%r274, [%rd68+1792];
	// begin inline asm
	{mul.f16x2 %r272,%r261,%r274;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r275,%r227,%r272;
}
	// end inline asm
	ld.global.nc.u32 	%r280, [%rd69+1792];
	// begin inline asm
	{mul.f16x2 %r278,%r261,%r280;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r281,%r233,%r278;
}
	// end inline asm
	ld.global.nc.u32 	%r286, [%rd70+1792];
	// begin inline asm
	{mul.f16x2 %r284,%r261,%r286;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r287,%r239,%r284;
}
	// end inline asm
	ld.global.nc.u32 	%r292, [%rd71+1792];
	// begin inline asm
	{mul.f16x2 %r290,%r261,%r292;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r293,%r245,%r290;
}
	// end inline asm
	ld.global.nc.u32 	%r298, [%rd72+1792];
	// begin inline asm
	{mul.f16x2 %r296,%r261,%r298;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r299,%r251,%r296;
}
	// end inline asm
	ld.global.nc.u32 	%r304, [%rd73+1792];
	// begin inline asm
	{mul.f16x2 %r302,%r261,%r304;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r305,%r257,%r302;
}
	// end inline asm
	ld.global.nc.u32 	%r309, [%rd85+896];
	ld.global.nc.u32 	%r310, [%rd66+2688];
	// begin inline asm
	{mul.f16x2 %r308,%r309,%r310;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r687,%r263,%r308;
}
	// end inline asm
	ld.global.nc.u32 	%r316, [%rd74+1792];
	// begin inline asm
	{mul.f16x2 %r314,%r309,%r316;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r686,%r269,%r314;
}
	// end inline asm
	ld.global.nc.u32 	%r322, [%rd68+2688];
	// begin inline asm
	{mul.f16x2 %r320,%r309,%r322;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r685,%r275,%r320;
}
	// end inline asm
	ld.global.nc.u32 	%r328, [%rd69+2688];
	// begin inline asm
	{mul.f16x2 %r326,%r309,%r328;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r684,%r281,%r326;
}
	// end inline asm
	ld.global.nc.u32 	%r334, [%rd70+2688];
	// begin inline asm
	{mul.f16x2 %r332,%r309,%r334;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r683,%r287,%r332;
}
	// end inline asm
	ld.global.nc.u32 	%r340, [%rd71+2688];
	// begin inline asm
	{mul.f16x2 %r338,%r309,%r340;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r682,%r293,%r338;
}
	// end inline asm
	ld.global.nc.u32 	%r346, [%rd72+2688];
	// begin inline asm
	{mul.f16x2 %r344,%r309,%r346;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r681,%r299,%r344;
}
	// end inline asm
	ld.global.nc.u32 	%r352, [%rd73+2688];
	// begin inline asm
	{mul.f16x2 %r350,%r309,%r352;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r680,%r305,%r350;
}
	// end inline asm
	add.s64 	%rd86, %rd86, 3584;
	add.s64 	%rd85, %rd85, 3584;
	add.s32 	%r670, %r670, 896;
	setp.lt.s32 	%p6, %r670, %r64;
	@%p6 bra 	$L__BB119_8;

$L__BB119_9:
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r687;
  cvt.f32.f16 %f10, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r687;
  cvt.f32.f16 %f11, high;}

	// end inline asm
	add.f32 	%f26, %f10, %f11;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r686;
  cvt.f32.f16 %f12, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r686;
  cvt.f32.f16 %f13, high;}

	// end inline asm
	add.f32 	%f1, %f12, %f13;
	st.local.f32 	[%rd3+4], %f1;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r685;
  cvt.f32.f16 %f14, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r685;
  cvt.f32.f16 %f15, high;}

	// end inline asm
	add.f32 	%f2, %f14, %f15;
	st.local.f32 	[%rd3+8], %f2;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r684;
  cvt.f32.f16 %f16, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r684;
  cvt.f32.f16 %f17, high;}

	// end inline asm
	add.f32 	%f3, %f16, %f17;
	st.local.f32 	[%rd3+12], %f3;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r683;
  cvt.f32.f16 %f18, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r683;
  cvt.f32.f16 %f19, high;}

	// end inline asm
	add.f32 	%f4, %f18, %f19;
	st.local.f32 	[%rd3+16], %f4;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r682;
  cvt.f32.f16 %f20, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r682;
  cvt.f32.f16 %f21, high;}

	// end inline asm
	add.f32 	%f5, %f20, %f21;
	st.local.f32 	[%rd3+20], %f5;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r681;
  cvt.f32.f16 %f22, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r681;
  cvt.f32.f16 %f23, high;}

	// end inline asm
	add.f32 	%f6, %f22, %f23;
	st.local.f32 	[%rd3+24], %f6;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r680;
  cvt.f32.f16 %f24, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r680;
  cvt.f32.f16 %f25, high;}

	// end inline asm
	add.f32 	%f7, %f24, %f25;
	st.local.f32 	[%rd3+28], %f7;
	shr.s32 	%r372, %r3, 31;
	shr.u32 	%r373, %r372, 27;
	add.s32 	%r374, %r3, %r373;
	shr.s32 	%r375, %r374, 5;
	shl.b32 	%r376, %r375, 2;
	add.s32 	%r63, %r77, %r376;
	mov.u32 	%r378, 2;
	mov.b32 	%r379, %f26;
	mov.u32 	%r380, 31;
	mov.u32 	%r381, 16;
	mov.u32 	%r382, -1;
	shfl.sync.bfly.b32 	%r383|%p7, %r379, %r381, %r380, %r382;
	mov.b32 	%f27, %r383;
	add.f32 	%f28, %f26, %f27;
	mov.b32 	%r384, %f28;
	mov.u32 	%r385, 8;
	shfl.sync.bfly.b32 	%r386|%p8, %r384, %r385, %r380, %r382;
	mov.b32 	%f29, %r386;
	add.f32 	%f30, %f28, %f29;
	mov.b32 	%r387, %f30;
	mov.u32 	%r388, 4;
	shfl.sync.bfly.b32 	%r389|%p9, %r387, %r388, %r380, %r382;
	mov.b32 	%f31, %r389;
	add.f32 	%f32, %f30, %f31;
	mov.b32 	%r390, %f32;
	shfl.sync.bfly.b32 	%r391|%p10, %r390, %r378, %r380, %r382;
	mov.b32 	%f33, %r391;
	add.f32 	%f34, %f32, %f33;
	mov.b32 	%r392, %f34;
	mov.u32 	%r393, 1;
	shfl.sync.bfly.b32 	%r394|%p11, %r392, %r393, %r380, %r382;
	mov.b32 	%f35, %r394;
	add.f32 	%f36, %f34, %f35;
	st.local.f32 	[%rd3], %f36;
	st.shared.f32 	[%r63], %f36;
	bar.sync 	0;
	@%p1 bra 	$L__BB119_11;

	ld.shared.f32 	%f37, [%r4];
	mov.b32 	%r395, %f37;
	shfl.sync.bfly.b32 	%r399|%p13, %r395, %r381, %r380, %r382;
	mov.b32 	%f38, %r399;
	add.f32 	%f39, %f37, %f38;
	mov.b32 	%r400, %f39;
	shfl.sync.bfly.b32 	%r402|%p14, %r400, %r385, %r380, %r382;
	mov.b32 	%f40, %r402;
	add.f32 	%f41, %f39, %f40;
	mov.b32 	%r403, %f41;
	shfl.sync.bfly.b32 	%r405|%p15, %r403, %r388, %r380, %r382;
	mov.b32 	%f42, %r405;
	add.f32 	%f43, %f41, %f42;
	mov.b32 	%r406, %f43;
	shfl.sync.bfly.b32 	%r408|%p16, %r406, %r378, %r380, %r382;
	mov.b32 	%f44, %r408;
	add.f32 	%f45, %f43, %f44;
	mov.b32 	%r409, %f45;
	shfl.sync.bfly.b32 	%r411|%p17, %r409, %r393, %r380, %r382;
	mov.b32 	%f46, %r411;
	add.f32 	%f47, %f45, %f46;
	st.local.f32 	[%rd3], %f47;

$L__BB119_11:
	bar.sync 	0;
	mov.b32 	%r412, %f1;
	shfl.sync.bfly.b32 	%r416|%p19, %r412, %r381, %r380, %r382;
	mov.b32 	%f48, %r416;
	add.f32 	%f49, %f1, %f48;
	mov.b32 	%r417, %f49;
	shfl.sync.bfly.b32 	%r419|%p20, %r417, %r385, %r380, %r382;
	mov.b32 	%f50, %r419;
	add.f32 	%f51, %f49, %f50;
	mov.b32 	%r420, %f51;
	shfl.sync.bfly.b32 	%r422|%p21, %r420, %r388, %r380, %r382;
	mov.b32 	%f52, %r422;
	add.f32 	%f53, %f51, %f52;
	mov.b32 	%r423, %f53;
	shfl.sync.bfly.b32 	%r425|%p22, %r423, %r378, %r380, %r382;
	mov.b32 	%f54, %r425;
	add.f32 	%f55, %f53, %f54;
	mov.b32 	%r426, %f55;
	shfl.sync.bfly.b32 	%r428|%p23, %r426, %r393, %r380, %r382;
	mov.b32 	%f56, %r428;
	add.f32 	%f57, %f55, %f56;
	st.local.f32 	[%rd3+4], %f57;
	st.shared.f32 	[%r63], %f57;
	bar.sync 	0;
	@%p1 bra 	$L__BB119_13;

	ld.shared.f32 	%f58, [%r4];
	mov.b32 	%r429, %f58;
	mov.u32 	%r430, 31;
	mov.u32 	%r431, 16;
	mov.u32 	%r432, -1;
	shfl.sync.bfly.b32 	%r433|%p24, %r429, %r431, %r430, %r432;
	mov.b32 	%f59, %r433;
	add.f32 	%f60, %f58, %f59;
	mov.b32 	%r434, %f60;
	mov.u32 	%r435, 8;
	shfl.sync.bfly.b32 	%r436|%p25, %r434, %r435, %r430, %r432;
	mov.b32 	%f61, %r436;
	add.f32 	%f62, %f60, %f61;
	mov.b32 	%r437, %f62;
	mov.u32 	%r438, 4;
	shfl.sync.bfly.b32 	%r439|%p26, %r437, %r438, %r430, %r432;
	mov.b32 	%f63, %r439;
	add.f32 	%f64, %f62, %f63;
	mov.b32 	%r440, %f64;
	mov.u32 	%r441, 2;
	shfl.sync.bfly.b32 	%r442|%p27, %r440, %r441, %r430, %r432;
	mov.b32 	%f65, %r442;
	add.f32 	%f66, %f64, %f65;
	mov.b32 	%r443, %f66;
	mov.u32 	%r444, 1;
	shfl.sync.bfly.b32 	%r445|%p28, %r443, %r444, %r430, %r432;
	mov.b32 	%f67, %r445;
	add.f32 	%f68, %f66, %f67;
	st.local.f32 	[%rd3+4], %f68;

$L__BB119_13:
	bar.sync 	0;
	mov.b32 	%r446, %f2;
	mov.u32 	%r447, 31;
	mov.u32 	%r448, 16;
	mov.u32 	%r449, -1;
	shfl.sync.bfly.b32 	%r450|%p30, %r446, %r448, %r447, %r449;
	mov.b32 	%f69, %r450;
	add.f32 	%f70, %f2, %f69;
	mov.b32 	%r451, %f70;
	mov.u32 	%r452, 8;
	shfl.sync.bfly.b32 	%r453|%p31, %r451, %r452, %r447, %r449;
	mov.b32 	%f71, %r453;
	add.f32 	%f72, %f70, %f71;
	mov.b32 	%r454, %f72;
	mov.u32 	%r455, 4;
	shfl.sync.bfly.b32 	%r456|%p32, %r454, %r455, %r447, %r449;
	mov.b32 	%f73, %r456;
	add.f32 	%f74, %f72, %f73;
	mov.b32 	%r457, %f74;
	mov.u32 	%r458, 2;
	shfl.sync.bfly.b32 	%r459|%p33, %r457, %r458, %r447, %r449;
	mov.b32 	%f75, %r459;
	add.f32 	%f76, %f74, %f75;
	mov.b32 	%r460, %f76;
	mov.u32 	%r461, 1;
	shfl.sync.bfly.b32 	%r462|%p34, %r460, %r461, %r447, %r449;
	mov.b32 	%f77, %r462;
	add.f32 	%f78, %f76, %f77;
	st.local.f32 	[%rd3+8], %f78;
	st.shared.f32 	[%r63], %f78;
	bar.sync 	0;
	@%p1 bra 	$L__BB119_15;

	ld.shared.f32 	%f79, [%r4];
	mov.b32 	%r463, %f79;
	shfl.sync.bfly.b32 	%r467|%p35, %r463, %r448, %r447, %r449;
	mov.b32 	%f80, %r467;
	add.f32 	%f81, %f79, %f80;
	mov.b32 	%r468, %f81;
	shfl.sync.bfly.b32 	%r470|%p36, %r468, %r452, %r447, %r449;
	mov.b32 	%f82, %r470;
	add.f32 	%f83, %f81, %f82;
	mov.b32 	%r471, %f83;
	shfl.sync.bfly.b32 	%r473|%p37, %r471, %r455, %r447, %r449;
	mov.b32 	%f84, %r473;
	add.f32 	%f85, %f83, %f84;
	mov.b32 	%r474, %f85;
	shfl.sync.bfly.b32 	%r476|%p38, %r474, %r458, %r447, %r449;
	mov.b32 	%f86, %r476;
	add.f32 	%f87, %f85, %f86;
	mov.b32 	%r477, %f87;
	shfl.sync.bfly.b32 	%r479|%p39, %r477, %r461, %r447, %r449;
	mov.b32 	%f88, %r479;
	add.f32 	%f89, %f87, %f88;
	st.local.f32 	[%rd3+8], %f89;

$L__BB119_15:
	bar.sync 	0;
	mov.b32 	%r480, %f3;
	shfl.sync.bfly.b32 	%r484|%p41, %r480, %r448, %r447, %r449;
	mov.b32 	%f90, %r484;
	add.f32 	%f91, %f3, %f90;
	mov.b32 	%r485, %f91;
	shfl.sync.bfly.b32 	%r487|%p42, %r485, %r452, %r447, %r449;
	mov.b32 	%f92, %r487;
	add.f32 	%f93, %f91, %f92;
	mov.b32 	%r488, %f93;
	shfl.sync.bfly.b32 	%r490|%p43, %r488, %r455, %r447, %r449;
	mov.b32 	%f94, %r490;
	add.f32 	%f95, %f93, %f94;
	mov.b32 	%r491, %f95;
	shfl.sync.bfly.b32 	%r493|%p44, %r491, %r458, %r447, %r449;
	mov.b32 	%f96, %r493;
	add.f32 	%f97, %f95, %f96;
	mov.b32 	%r494, %f97;
	shfl.sync.bfly.b32 	%r496|%p45, %r494, %r461, %r447, %r449;
	mov.b32 	%f98, %r496;
	add.f32 	%f99, %f97, %f98;
	st.local.f32 	[%rd3+12], %f99;
	st.shared.f32 	[%r63], %f99;
	bar.sync 	0;
	@%p1 bra 	$L__BB119_17;

	ld.shared.f32 	%f100, [%r4];
	mov.b32 	%r497, %f100;
	mov.u32 	%r498, 31;
	mov.u32 	%r499, 16;
	mov.u32 	%r500, -1;
	shfl.sync.bfly.b32 	%r501|%p46, %r497, %r499, %r498, %r500;
	mov.b32 	%f101, %r501;
	add.f32 	%f102, %f100, %f101;
	mov.b32 	%r502, %f102;
	mov.u32 	%r503, 8;
	shfl.sync.bfly.b32 	%r504|%p47, %r502, %r503, %r498, %r500;
	mov.b32 	%f103, %r504;
	add.f32 	%f104, %f102, %f103;
	mov.b32 	%r505, %f104;
	mov.u32 	%r506, 4;
	shfl.sync.bfly.b32 	%r507|%p48, %r505, %r506, %r498, %r500;
	mov.b32 	%f105, %r507;
	add.f32 	%f106, %f104, %f105;
	mov.b32 	%r508, %f106;
	mov.u32 	%r509, 2;
	shfl.sync.bfly.b32 	%r510|%p49, %r508, %r509, %r498, %r500;
	mov.b32 	%f107, %r510;
	add.f32 	%f108, %f106, %f107;
	mov.b32 	%r511, %f108;
	mov.u32 	%r512, 1;
	shfl.sync.bfly.b32 	%r513|%p50, %r511, %r512, %r498, %r500;
	mov.b32 	%f109, %r513;
	add.f32 	%f110, %f108, %f109;
	st.local.f32 	[%rd3+12], %f110;

$L__BB119_17:
	bar.sync 	0;
	mov.b32 	%r514, %f4;
	mov.u32 	%r515, 31;
	mov.u32 	%r516, 16;
	mov.u32 	%r517, -1;
	shfl.sync.bfly.b32 	%r518|%p52, %r514, %r516, %r515, %r517;
	mov.b32 	%f111, %r518;
	add.f32 	%f112, %f4, %f111;
	mov.b32 	%r519, %f112;
	mov.u32 	%r520, 8;
	shfl.sync.bfly.b32 	%r521|%p53, %r519, %r520, %r515, %r517;
	mov.b32 	%f113, %r521;
	add.f32 	%f114, %f112, %f113;
	mov.b32 	%r522, %f114;
	mov.u32 	%r523, 4;
	shfl.sync.bfly.b32 	%r524|%p54, %r522, %r523, %r515, %r517;
	mov.b32 	%f115, %r524;
	add.f32 	%f116, %f114, %f115;
	mov.b32 	%r525, %f116;
	mov.u32 	%r526, 2;
	shfl.sync.bfly.b32 	%r527|%p55, %r525, %r526, %r515, %r517;
	mov.b32 	%f117, %r527;
	add.f32 	%f118, %f116, %f117;
	mov.b32 	%r528, %f118;
	mov.u32 	%r529, 1;
	shfl.sync.bfly.b32 	%r530|%p56, %r528, %r529, %r515, %r517;
	mov.b32 	%f119, %r530;
	add.f32 	%f120, %f118, %f119;
	st.local.f32 	[%rd3+16], %f120;
	st.shared.f32 	[%r63], %f120;
	bar.sync 	0;
	@%p1 bra 	$L__BB119_19;

	ld.shared.f32 	%f121, [%r4];
	mov.b32 	%r531, %f121;
	shfl.sync.bfly.b32 	%r535|%p57, %r531, %r516, %r515, %r517;
	mov.b32 	%f122, %r535;
	add.f32 	%f123, %f121, %f122;
	mov.b32 	%r536, %f123;
	shfl.sync.bfly.b32 	%r538|%p58, %r536, %r520, %r515, %r517;
	mov.b32 	%f124, %r538;
	add.f32 	%f125, %f123, %f124;
	mov.b32 	%r539, %f125;
	shfl.sync.bfly.b32 	%r541|%p59, %r539, %r523, %r515, %r517;
	mov.b32 	%f126, %r541;
	add.f32 	%f127, %f125, %f126;
	mov.b32 	%r542, %f127;
	shfl.sync.bfly.b32 	%r544|%p60, %r542, %r526, %r515, %r517;
	mov.b32 	%f128, %r544;
	add.f32 	%f129, %f127, %f128;
	mov.b32 	%r545, %f129;
	shfl.sync.bfly.b32 	%r547|%p61, %r545, %r529, %r515, %r517;
	mov.b32 	%f130, %r547;
	add.f32 	%f131, %f129, %f130;
	st.local.f32 	[%rd3+16], %f131;

$L__BB119_19:
	bar.sync 	0;
	mov.b32 	%r548, %f5;
	shfl.sync.bfly.b32 	%r552|%p63, %r548, %r516, %r515, %r517;
	mov.b32 	%f132, %r552;
	add.f32 	%f133, %f5, %f132;
	mov.b32 	%r553, %f133;
	shfl.sync.bfly.b32 	%r555|%p64, %r553, %r520, %r515, %r517;
	mov.b32 	%f134, %r555;
	add.f32 	%f135, %f133, %f134;
	mov.b32 	%r556, %f135;
	shfl.sync.bfly.b32 	%r558|%p65, %r556, %r523, %r515, %r517;
	mov.b32 	%f136, %r558;
	add.f32 	%f137, %f135, %f136;
	mov.b32 	%r559, %f137;
	shfl.sync.bfly.b32 	%r561|%p66, %r559, %r526, %r515, %r517;
	mov.b32 	%f138, %r561;
	add.f32 	%f139, %f137, %f138;
	mov.b32 	%r562, %f139;
	shfl.sync.bfly.b32 	%r564|%p67, %r562, %r529, %r515, %r517;
	mov.b32 	%f140, %r564;
	add.f32 	%f141, %f139, %f140;
	st.local.f32 	[%rd3+20], %f141;
	st.shared.f32 	[%r63], %f141;
	bar.sync 	0;
	@%p1 bra 	$L__BB119_21;

	ld.shared.f32 	%f142, [%r4];
	mov.b32 	%r565, %f142;
	mov.u32 	%r566, 31;
	mov.u32 	%r567, 16;
	mov.u32 	%r568, -1;
	shfl.sync.bfly.b32 	%r569|%p68, %r565, %r567, %r566, %r568;
	mov.b32 	%f143, %r569;
	add.f32 	%f144, %f142, %f143;
	mov.b32 	%r570, %f144;
	mov.u32 	%r571, 8;
	shfl.sync.bfly.b32 	%r572|%p69, %r570, %r571, %r566, %r568;
	mov.b32 	%f145, %r572;
	add.f32 	%f146, %f144, %f145;
	mov.b32 	%r573, %f146;
	mov.u32 	%r574, 4;
	shfl.sync.bfly.b32 	%r575|%p70, %r573, %r574, %r566, %r568;
	mov.b32 	%f147, %r575;
	add.f32 	%f148, %f146, %f147;
	mov.b32 	%r576, %f148;
	mov.u32 	%r577, 2;
	shfl.sync.bfly.b32 	%r578|%p71, %r576, %r577, %r566, %r568;
	mov.b32 	%f149, %r578;
	add.f32 	%f150, %f148, %f149;
	mov.b32 	%r579, %f150;
	mov.u32 	%r580, 1;
	shfl.sync.bfly.b32 	%r581|%p72, %r579, %r580, %r566, %r568;
	mov.b32 	%f151, %r581;
	add.f32 	%f152, %f150, %f151;
	st.local.f32 	[%rd3+20], %f152;

$L__BB119_21:
	bar.sync 	0;
	mov.b32 	%r582, %f6;
	mov.u32 	%r583, 31;
	mov.u32 	%r584, 16;
	mov.u32 	%r585, -1;
	shfl.sync.bfly.b32 	%r586|%p74, %r582, %r584, %r583, %r585;
	mov.b32 	%f153, %r586;
	add.f32 	%f154, %f6, %f153;
	mov.b32 	%r587, %f154;
	mov.u32 	%r588, 8;
	shfl.sync.bfly.b32 	%r589|%p75, %r587, %r588, %r583, %r585;
	mov.b32 	%f155, %r589;
	add.f32 	%f156, %f154, %f155;
	mov.b32 	%r590, %f156;
	mov.u32 	%r591, 4;
	shfl.sync.bfly.b32 	%r592|%p76, %r590, %r591, %r583, %r585;
	mov.b32 	%f157, %r592;
	add.f32 	%f158, %f156, %f157;
	mov.b32 	%r593, %f158;
	mov.u32 	%r594, 2;
	shfl.sync.bfly.b32 	%r595|%p77, %r593, %r594, %r583, %r585;
	mov.b32 	%f159, %r595;
	add.f32 	%f160, %f158, %f159;
	mov.b32 	%r596, %f160;
	mov.u32 	%r597, 1;
	shfl.sync.bfly.b32 	%r598|%p78, %r596, %r597, %r583, %r585;
	mov.b32 	%f161, %r598;
	add.f32 	%f162, %f160, %f161;
	st.local.f32 	[%rd3+24], %f162;
	st.shared.f32 	[%r63], %f162;
	bar.sync 	0;
	@%p1 bra 	$L__BB119_23;

	ld.shared.f32 	%f163, [%r4];
	mov.b32 	%r599, %f163;
	shfl.sync.bfly.b32 	%r603|%p79, %r599, %r584, %r583, %r585;
	mov.b32 	%f164, %r603;
	add.f32 	%f165, %f163, %f164;
	mov.b32 	%r604, %f165;
	shfl.sync.bfly.b32 	%r606|%p80, %r604, %r588, %r583, %r585;
	mov.b32 	%f166, %r606;
	add.f32 	%f167, %f165, %f166;
	mov.b32 	%r607, %f167;
	shfl.sync.bfly.b32 	%r609|%p81, %r607, %r591, %r583, %r585;
	mov.b32 	%f168, %r609;
	add.f32 	%f169, %f167, %f168;
	mov.b32 	%r610, %f169;
	shfl.sync.bfly.b32 	%r612|%p82, %r610, %r594, %r583, %r585;
	mov.b32 	%f170, %r612;
	add.f32 	%f171, %f169, %f170;
	mov.b32 	%r613, %f171;
	shfl.sync.bfly.b32 	%r615|%p83, %r613, %r597, %r583, %r585;
	mov.b32 	%f172, %r615;
	add.f32 	%f173, %f171, %f172;
	st.local.f32 	[%rd3+24], %f173;

$L__BB119_23:
	bar.sync 	0;
	mov.b32 	%r616, %f7;
	shfl.sync.bfly.b32 	%r620|%p85, %r616, %r584, %r583, %r585;
	mov.b32 	%f174, %r620;
	add.f32 	%f175, %f7, %f174;
	mov.b32 	%r621, %f175;
	shfl.sync.bfly.b32 	%r623|%p86, %r621, %r588, %r583, %r585;
	mov.b32 	%f176, %r623;
	add.f32 	%f177, %f175, %f176;
	mov.b32 	%r624, %f177;
	shfl.sync.bfly.b32 	%r626|%p87, %r624, %r591, %r583, %r585;
	mov.b32 	%f178, %r626;
	add.f32 	%f179, %f177, %f178;
	mov.b32 	%r627, %f179;
	shfl.sync.bfly.b32 	%r629|%p88, %r627, %r594, %r583, %r585;
	mov.b32 	%f180, %r629;
	add.f32 	%f181, %f179, %f180;
	mov.b32 	%r630, %f181;
	shfl.sync.bfly.b32 	%r632|%p89, %r630, %r597, %r583, %r585;
	mov.b32 	%f182, %r632;
	add.f32 	%f183, %f181, %f182;
	st.local.f32 	[%rd3+28], %f183;
	st.shared.f32 	[%r63], %f183;
	bar.sync 	0;
	@%p1 bra 	$L__BB119_25;

	ld.shared.f32 	%f184, [%r4];
	mov.b32 	%r633, %f184;
	mov.u32 	%r634, 31;
	mov.u32 	%r635, 16;
	mov.u32 	%r636, -1;
	shfl.sync.bfly.b32 	%r637|%p90, %r633, %r635, %r634, %r636;
	mov.b32 	%f185, %r637;
	add.f32 	%f186, %f184, %f185;
	mov.b32 	%r638, %f186;
	mov.u32 	%r639, 8;
	shfl.sync.bfly.b32 	%r640|%p91, %r638, %r639, %r634, %r636;
	mov.b32 	%f187, %r640;
	add.f32 	%f188, %f186, %f187;
	mov.b32 	%r641, %f188;
	mov.u32 	%r642, 4;
	shfl.sync.bfly.b32 	%r643|%p92, %r641, %r642, %r634, %r636;
	mov.b32 	%f189, %r643;
	add.f32 	%f190, %f188, %f189;
	mov.b32 	%r644, %f190;
	mov.u32 	%r645, 2;
	shfl.sync.bfly.b32 	%r646|%p93, %r644, %r645, %r634, %r636;
	mov.b32 	%f191, %r646;
	add.f32 	%f192, %f190, %f191;
	mov.b32 	%r647, %f192;
	mov.u32 	%r648, 1;
	shfl.sync.bfly.b32 	%r649|%p94, %r647, %r648, %r634, %r636;
	mov.b32 	%f193, %r649;
	add.f32 	%f194, %f192, %f193;
	st.local.f32 	[%rd3+28], %f194;

$L__BB119_25:
	bar.sync 	0;
	setp.gt.s32 	%p95, %r3, 7;
	@%p95 bra 	$L__BB119_27;

	mad.lo.s32 	%r650, %r3, %r66, %r2;
	cvt.s64.s32 	%rd75, %r650;
	mul.lo.s32 	%r651, %r1, %r67;
	cvt.s64.s32 	%rd76, %r651;
	add.s64 	%rd77, %rd76, %rd75;
	mul.wide.s32 	%rd78, %r3, 4;
	add.s64 	%rd79, %rd3, %rd78;
	ld.local.f32 	%f195, [%rd79];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs3, %f195;}

	// end inline asm
	cvta.to.global.u64 	%rd80, %rd30;
	shl.b64 	%rd81, %rd77, 1;
	add.s64 	%rd82, %rd80, %rd81;
	st.global.u16 	[%rd82], %rs3;

$L__BB119_27:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_1_bs_256
.visible .entry ggml_matvec_f16_ncols_1_bs_256(
	.param .u64 ggml_matvec_f16_ncols_1_bs_256_param_0,
	.param .u64 ggml_matvec_f16_ncols_1_bs_256_param_1,
	.param .u64 ggml_matvec_f16_ncols_1_bs_256_param_2,
	.param .u32 ggml_matvec_f16_ncols_1_bs_256_param_3,
	.param .u32 ggml_matvec_f16_ncols_1_bs_256_param_4,
	.param .u32 ggml_matvec_f16_ncols_1_bs_256_param_5,
	.param .u32 ggml_matvec_f16_ncols_1_bs_256_param_6,
	.param .u32 ggml_matvec_f16_ncols_1_bs_256_param_7,
	.param .u32 ggml_matvec_f16_ncols_1_bs_256_param_8,
	.param .u32 ggml_matvec_f16_ncols_1_bs_256_param_9,
	.param .u32 ggml_matvec_f16_ncols_1_bs_256_param_10,
	.param .u32 ggml_matvec_f16_ncols_1_bs_256_param_11
)
{
	.reg .pred 	%p<19>;
	.reg .b16 	%rs<31>;
	.reg .f32 	%f<30>;
	.reg .b32 	%r<127>;
	.reg .b64 	%rd<45>;


	ld.param.u64 	%rd14, [ggml_matvec_f16_ncols_1_bs_256_param_0];
	ld.param.u64 	%rd15, [ggml_matvec_f16_ncols_1_bs_256_param_1];
	ld.param.u64 	%rd16, [ggml_matvec_f16_ncols_1_bs_256_param_2];
	ld.param.u32 	%r13, [ggml_matvec_f16_ncols_1_bs_256_param_3];
	ld.param.u32 	%r17, [ggml_matvec_f16_ncols_1_bs_256_param_5];
	ld.param.u32 	%r14, [ggml_matvec_f16_ncols_1_bs_256_param_7];
	ld.param.u32 	%r18, [ggml_matvec_f16_ncols_1_bs_256_param_8];
	ld.param.u32 	%r19, [ggml_matvec_f16_ncols_1_bs_256_param_9];
	ld.param.u32 	%r15, [ggml_matvec_f16_ncols_1_bs_256_param_10];
	ld.param.u32 	%r16, [ggml_matvec_f16_ncols_1_bs_256_param_11];
	mov.u32 	%r20, %ctaid.x;
	mov.u32 	%r21, %ctaid.y;
	div.s32 	%r22, %r21, %r18;
	mul.lo.s32 	%r23, %r20, %r17;
	mad.lo.s32 	%r24, %r22, %r19, %r23;
	cvt.s64.s32 	%rd1, %r24;
	mov.u32 	%r1, %tid.x;
	setp.gt.s32 	%p1, %r1, 31;
	shl.b32 	%r25, %r1, 2;
	mov.u32 	%r26, data_mmv;
	add.s32 	%r2, %r26, %r25;
	@%p1 bra 	$L__BB120_2;

	mov.u32 	%r27, 0;
	st.shared.u32 	[%r2], %r27;

$L__BB120_2:
	bar.sync 	0;
	mov.f32 	%f5, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs30, %f5;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs29, %f5;}

	// end inline asm
	setp.ge.s32 	%p2, %r1, %r13;
	@%p2 bra 	$L__BB120_9;

	not.b32 	%r28, %r1;
	add.s32 	%r29, %r28, %r13;
	shr.u32 	%r30, %r29, 8;
	add.s32 	%r31, %r30, 1;
	and.b32  	%r124, %r31, 3;
	setp.eq.s32 	%p3, %r124, 0;
	mov.u32 	%r125, %r1;
	@%p3 bra 	$L__BB120_6;

	mov.u32 	%r125, %tid.x;
	mul.wide.s32 	%rd17, %r125, 2;
	mul.lo.s32 	%r33, %r21, %r15;
	cvt.s64.s32 	%rd18, %r33;
	add.s64 	%rd19, %rd17, %rd18;
	cvta.to.global.u64 	%rd20, %rd15;
	shl.b64 	%rd21, %rd19, 1;
	add.s64 	%rd42, %rd20, %rd21;
	add.s64 	%rd22, %rd17, %rd1;
	cvta.to.global.u64 	%rd23, %rd14;
	shl.b64 	%rd24, %rd22, 1;
	add.s64 	%rd41, %rd23, %rd24;

$L__BB120_5:
	.pragma "nounroll";
	ld.global.nc.u32 	%r35, [%rd41];
	ld.global.nc.u32 	%r36, [%rd42];
	// begin inline asm
	{mul.f16x2 %r34,%r35,%r36;
}
	// end inline asm
	mov.b32 	%r38, {%rs30, %rs29};
	// begin inline asm
	{add.f16x2 %r37,%r38,%r34;
}
	// end inline asm
	mov.b32 	{%rs30, %rs29}, %r37;
	add.s32 	%r125, %r125, 256;
	add.s64 	%rd42, %rd42, 1024;
	add.s64 	%rd41, %rd41, 1024;
	add.s32 	%r124, %r124, -1;
	setp.ne.s32 	%p4, %r124, 0;
	@%p4 bra 	$L__BB120_5;

$L__BB120_6:
	setp.lt.u32 	%p5, %r29, 768;
	@%p5 bra 	$L__BB120_9;

	mul.wide.s32 	%rd25, %r125, 2;
	cvta.to.global.u64 	%rd26, %rd14;
	add.s64 	%rd27, %rd25, %rd1;
	shl.b64 	%rd28, %rd27, 1;
	add.s64 	%rd29, %rd26, %rd28;
	add.s64 	%rd44, %rd29, 2048;
	mul.lo.s32 	%r44, %r21, %r15;
	cvt.s64.s32 	%rd30, %r44;
	cvta.to.global.u64 	%rd31, %rd15;
	add.s64 	%rd32, %rd25, %rd30;
	shl.b64 	%rd33, %rd32, 1;
	add.s64 	%rd34, %rd31, %rd33;
	add.s64 	%rd43, %rd34, 2048;

$L__BB120_8:
	ld.global.nc.u32 	%r46, [%rd44+-2048];
	ld.global.nc.u32 	%r47, [%rd43+-2048];
	// begin inline asm
	{mul.f16x2 %r45,%r46,%r47;
}
	// end inline asm
	mov.b32 	%r49, {%rs30, %rs29};
	// begin inline asm
	{add.f16x2 %r48,%r49,%r45;
}
	// end inline asm
	ld.global.nc.u32 	%r52, [%rd44+-1024];
	ld.global.nc.u32 	%r53, [%rd43+-1024];
	// begin inline asm
	{mul.f16x2 %r51,%r52,%r53;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r54,%r48,%r51;
}
	// end inline asm
	ld.global.nc.u32 	%r58, [%rd44];
	ld.global.nc.u32 	%r59, [%rd43];
	// begin inline asm
	{mul.f16x2 %r57,%r58,%r59;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r60,%r54,%r57;
}
	// end inline asm
	ld.global.nc.u32 	%r64, [%rd44+1024];
	ld.global.nc.u32 	%r65, [%rd43+1024];
	// begin inline asm
	{mul.f16x2 %r63,%r64,%r65;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r66,%r60,%r63;
}
	// end inline asm
	mov.b32 	{%rs30, %rs29}, %r66;
	add.s64 	%rd44, %rd44, 4096;
	add.s64 	%rd43, %rd43, 4096;
	add.s32 	%r125, %r125, 1024;
	setp.lt.s32 	%p6, %r125, %r13;
	@%p6 bra 	$L__BB120_8;

$L__BB120_9:
	mov.u32 	%r72, 1;
	mov.b32 	%r70, {%rs30, %rs29};
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r70;
  cvt.f32.f16 %f6, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r70;
  cvt.f32.f16 %f7, high;}

	// end inline asm
	add.f32 	%f8, %f6, %f7;
	mov.b32 	%r73, %f8;
	mov.u32 	%r74, 31;
	mov.u32 	%r75, 16;
	mov.u32 	%r76, -1;
	shfl.sync.bfly.b32 	%r77|%p8, %r73, %r75, %r74, %r76;
	mov.b32 	%f9, %r77;
	add.f32 	%f10, %f8, %f9;
	mov.b32 	%r78, %f10;
	mov.u32 	%r79, 8;
	shfl.sync.bfly.b32 	%r80|%p9, %r78, %r79, %r74, %r76;
	mov.b32 	%f11, %r80;
	add.f32 	%f12, %f10, %f11;
	mov.b32 	%r81, %f12;
	mov.u32 	%r82, 4;
	shfl.sync.bfly.b32 	%r83|%p10, %r81, %r82, %r74, %r76;
	mov.b32 	%f13, %r83;
	add.f32 	%f14, %f12, %f13;
	mov.b32 	%r84, %f14;
	mov.u32 	%r85, 2;
	shfl.sync.bfly.b32 	%r86|%p11, %r84, %r85, %r74, %r76;
	mov.b32 	%f15, %r86;
	add.f32 	%f16, %f14, %f15;
	mov.b32 	%r87, %f16;
	shfl.sync.bfly.b32 	%r88|%p12, %r87, %r72, %r74, %r76;
	mov.b32 	%f17, %r88;
	add.f32 	%f29, %f16, %f17;
	shr.s32 	%r89, %r1, 31;
	shr.u32 	%r90, %r89, 27;
	add.s32 	%r91, %r1, %r90;
	shr.s32 	%r92, %r91, 5;
	shl.b32 	%r93, %r92, 2;
	add.s32 	%r95, %r26, %r93;
	st.shared.f32 	[%r95], %f29;
	bar.sync 	0;
	@%p1 bra 	$L__BB120_11;

	ld.shared.f32 	%f18, [%r2];
	mov.b32 	%r101, %f18;
	shfl.sync.bfly.b32 	%r105|%p13, %r101, %r75, %r74, %r76;
	mov.b32 	%f19, %r105;
	add.f32 	%f20, %f18, %f19;
	mov.b32 	%r106, %f20;
	shfl.sync.bfly.b32 	%r108|%p14, %r106, %r79, %r74, %r76;
	mov.b32 	%f21, %r108;
	add.f32 	%f22, %f20, %f21;
	mov.b32 	%r109, %f22;
	shfl.sync.bfly.b32 	%r111|%p15, %r109, %r82, %r74, %r76;
	mov.b32 	%f23, %r111;
	add.f32 	%f24, %f22, %f23;
	mov.b32 	%r112, %f24;
	shfl.sync.bfly.b32 	%r113|%p16, %r112, %r85, %r74, %r76;
	mov.b32 	%f25, %r113;
	add.f32 	%f26, %f24, %f25;
	mov.b32 	%r114, %f26;
	shfl.sync.bfly.b32 	%r116|%p17, %r114, %r72, %r74, %r76;
	mov.b32 	%f27, %r116;
	add.f32 	%f29, %f26, %f27;

$L__BB120_11:
	bar.sync 	0;
	setp.gt.s32 	%p18, %r1, 0;
	@%p18 bra 	$L__BB120_13;

	mad.lo.s32 	%r120, %r1, %r14, %r20;
	cvt.s64.s32 	%rd35, %r120;
	mul.lo.s32 	%r122, %r21, %r16;
	cvt.s64.s32 	%rd36, %r122;
	add.s64 	%rd37, %rd36, %rd35;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs20, %f29;}

	// end inline asm
	cvta.to.global.u64 	%rd38, %rd16;
	shl.b64 	%rd39, %rd37, 1;
	add.s64 	%rd40, %rd38, %rd39;
	st.global.u16 	[%rd40], %rs20;

$L__BB120_13:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_2_bs_256
.visible .entry ggml_matvec_f16_ncols_2_bs_256(
	.param .u64 ggml_matvec_f16_ncols_2_bs_256_param_0,
	.param .u64 ggml_matvec_f16_ncols_2_bs_256_param_1,
	.param .u64 ggml_matvec_f16_ncols_2_bs_256_param_2,
	.param .u32 ggml_matvec_f16_ncols_2_bs_256_param_3,
	.param .u32 ggml_matvec_f16_ncols_2_bs_256_param_4,
	.param .u32 ggml_matvec_f16_ncols_2_bs_256_param_5,
	.param .u32 ggml_matvec_f16_ncols_2_bs_256_param_6,
	.param .u32 ggml_matvec_f16_ncols_2_bs_256_param_7,
	.param .u32 ggml_matvec_f16_ncols_2_bs_256_param_8,
	.param .u32 ggml_matvec_f16_ncols_2_bs_256_param_9,
	.param .u32 ggml_matvec_f16_ncols_2_bs_256_param_10,
	.param .u32 ggml_matvec_f16_ncols_2_bs_256_param_11
)
{
	.local .align 8 .b8 	__local_depot121[8];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<30>;
	.reg .b16 	%rs<4>;
	.reg .f32 	%f<52>;
	.reg .b32 	%r<201>;
	.reg .b64 	%rd<64>;


	mov.u64 	%SPL, __local_depot121;
	ld.param.u64 	%rd27, [ggml_matvec_f16_ncols_2_bs_256_param_0];
	ld.param.u64 	%rd28, [ggml_matvec_f16_ncols_2_bs_256_param_1];
	ld.param.u64 	%rd26, [ggml_matvec_f16_ncols_2_bs_256_param_2];
	ld.param.u32 	%r28, [ggml_matvec_f16_ncols_2_bs_256_param_3];
	ld.param.u32 	%r32, [ggml_matvec_f16_ncols_2_bs_256_param_5];
	ld.param.u32 	%r29, [ggml_matvec_f16_ncols_2_bs_256_param_6];
	ld.param.u32 	%r30, [ggml_matvec_f16_ncols_2_bs_256_param_7];
	ld.param.u32 	%r33, [ggml_matvec_f16_ncols_2_bs_256_param_8];
	ld.param.u32 	%r34, [ggml_matvec_f16_ncols_2_bs_256_param_9];
	ld.param.u32 	%r35, [ggml_matvec_f16_ncols_2_bs_256_param_10];
	ld.param.u32 	%r31, [ggml_matvec_f16_ncols_2_bs_256_param_11];
	cvta.to.global.u64 	%rd1, %rd28;
	cvta.to.global.u64 	%rd2, %rd27;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r36, %r1, %r33;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r37, %r2, %r32;
	mad.lo.s32 	%r38, %r36, %r34, %r37;
	cvt.s64.s32 	%rd4, %r38;
	mul.lo.s32 	%r39, %r1, %r35;
	cvt.s64.s32 	%rd5, %r39;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r40, %r3, 2;
	mov.u32 	%r41, data_mmv;
	add.s32 	%r4, %r41, %r40;
	@%p1 bra 	$L__BB121_2;

	mov.u32 	%r42, 0;
	st.shared.u32 	[%r4], %r42;

$L__BB121_2:
	bar.sync 	0;
	mov.f32 	%f3, 0f00000000;
	st.local.v2.f32 	[%rd3], {%f3, %f3};
	mov.u32 	%r199, 0;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs2, %f3;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f3;}

	// end inline asm
	mov.b32 	%r200, {%rs1, %rs2};
	setp.ge.s32 	%p2, %r3, %r28;
	@%p2 bra 	$L__BB121_9;

	not.b32 	%r45, %r3;
	add.s32 	%r6, %r45, %r28;
	shr.u32 	%r46, %r6, 8;
	add.s32 	%r47, %r46, 1;
	and.b32  	%r192, %r47, 3;
	setp.eq.s32 	%p3, %r192, 0;
	mov.u32 	%r199, 0;
	mov.u32 	%r195, %r3;
	@%p3 bra 	$L__BB121_6;

	mul.wide.s32 	%rd30, %r29, 2;
	mul.wide.s32 	%rd31, %r3, 2;
	add.s64 	%rd32, %rd30, %rd31;
	add.s64 	%rd33, %rd32, %rd5;
	shl.b64 	%rd34, %rd33, 1;
	add.s64 	%rd60, %rd1, %rd34;
	add.s64 	%rd35, %rd31, %rd5;
	shl.b64 	%rd36, %rd35, 1;
	add.s64 	%rd59, %rd1, %rd36;
	add.s64 	%rd37, %rd31, %rd4;
	shl.b64 	%rd38, %rd37, 1;
	add.s64 	%rd58, %rd2, %rd38;
	mov.u32 	%r199, 0;
	mov.u32 	%r195, %r3;

$L__BB121_5:
	.pragma "nounroll";
	ld.global.nc.u32 	%r50, [%rd58];
	ld.global.nc.u32 	%r51, [%rd59];
	// begin inline asm
	{mul.f16x2 %r49,%r50,%r51;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r200,%r200,%r49;
}
	// end inline asm
	ld.global.nc.u32 	%r57, [%rd60];
	// begin inline asm
	{mul.f16x2 %r55,%r50,%r57;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r199,%r199,%r55;
}
	// end inline asm
	add.s32 	%r195, %r195, 256;
	add.s64 	%rd60, %rd60, 1024;
	add.s64 	%rd59, %rd59, 1024;
	add.s64 	%rd58, %rd58, 1024;
	add.s32 	%r192, %r192, -1;
	setp.ne.s32 	%p4, %r192, 0;
	@%p4 bra 	$L__BB121_5;

$L__BB121_6:
	setp.lt.u32 	%p5, %r6, 768;
	@%p5 bra 	$L__BB121_9;

	mul.wide.s32 	%rd39, %r195, 2;
	add.s64 	%rd40, %rd39, %rd4;
	shl.b64 	%rd41, %rd40, 1;
	add.s64 	%rd42, %rd2, %rd41;
	add.s64 	%rd63, %rd42, 2048;
	add.s64 	%rd43, %rd39, %rd5;
	shl.b64 	%rd44, %rd43, 1;
	add.s64 	%rd45, %rd1, %rd44;
	add.s64 	%rd62, %rd45, 3072;
	mul.wide.s32 	%rd46, %r29, 2;
	add.s64 	%rd47, %rd43, %rd46;
	shl.b64 	%rd48, %rd47, 1;
	add.s64 	%rd49, %rd1, %rd48;
	add.s64 	%rd61, %rd49, 2048;

$L__BB121_8:
	ld.global.nc.u32 	%r62, [%rd63+-2048];
	ld.global.nc.u32 	%r63, [%rd62+-3072];
	// begin inline asm
	{mul.f16x2 %r61,%r62,%r63;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r64,%r200,%r61;
}
	// end inline asm
	ld.global.nc.u32 	%r69, [%rd61+-2048];
	// begin inline asm
	{mul.f16x2 %r67,%r62,%r69;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r70,%r199,%r67;
}
	// end inline asm
	ld.global.nc.u32 	%r74, [%rd63+-1024];
	ld.global.nc.u32 	%r75, [%rd62+-2048];
	// begin inline asm
	{mul.f16x2 %r73,%r74,%r75;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r76,%r64,%r73;
}
	// end inline asm
	ld.global.nc.u32 	%r81, [%rd61+-1024];
	// begin inline asm
	{mul.f16x2 %r79,%r74,%r81;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r82,%r70,%r79;
}
	// end inline asm
	ld.global.nc.u32 	%r86, [%rd63];
	ld.global.nc.u32 	%r87, [%rd62+-1024];
	// begin inline asm
	{mul.f16x2 %r85,%r86,%r87;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r88,%r76,%r85;
}
	// end inline asm
	ld.global.nc.u32 	%r93, [%rd61];
	// begin inline asm
	{mul.f16x2 %r91,%r86,%r93;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r94,%r82,%r91;
}
	// end inline asm
	ld.global.nc.u32 	%r98, [%rd63+1024];
	ld.global.nc.u32 	%r99, [%rd62];
	// begin inline asm
	{mul.f16x2 %r97,%r98,%r99;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r200,%r88,%r97;
}
	// end inline asm
	ld.global.nc.u32 	%r105, [%rd61+1024];
	// begin inline asm
	{mul.f16x2 %r103,%r98,%r105;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r199,%r94,%r103;
}
	// end inline asm
	add.s64 	%rd63, %rd63, 4096;
	add.s64 	%rd62, %rd62, 4096;
	add.s64 	%rd61, %rd61, 4096;
	add.s32 	%r195, %r195, 1024;
	setp.lt.s32 	%p6, %r195, %r28;
	@%p6 bra 	$L__BB121_8;

$L__BB121_9:
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r200;
  cvt.f32.f16 %f4, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r200;
  cvt.f32.f16 %f5, high;}

	// end inline asm
	add.f32 	%f8, %f4, %f5;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r199;
  cvt.f32.f16 %f6, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r199;
  cvt.f32.f16 %f7, high;}

	// end inline asm
	add.f32 	%f1, %f6, %f7;
	st.local.f32 	[%rd3+4], %f1;
	shr.s32 	%r113, %r3, 31;
	shr.u32 	%r114, %r113, 27;
	add.s32 	%r115, %r3, %r114;
	shr.s32 	%r116, %r115, 5;
	shl.b32 	%r117, %r116, 2;
	add.s32 	%r27, %r41, %r117;
	mov.u32 	%r119, 2;
	mov.b32 	%r120, %f8;
	mov.u32 	%r121, 31;
	mov.u32 	%r122, 16;
	mov.u32 	%r123, -1;
	shfl.sync.bfly.b32 	%r124|%p7, %r120, %r122, %r121, %r123;
	mov.b32 	%f9, %r124;
	add.f32 	%f10, %f8, %f9;
	mov.b32 	%r125, %f10;
	mov.u32 	%r126, 8;
	shfl.sync.bfly.b32 	%r127|%p8, %r125, %r126, %r121, %r123;
	mov.b32 	%f11, %r127;
	add.f32 	%f12, %f10, %f11;
	mov.b32 	%r128, %f12;
	mov.u32 	%r129, 4;
	shfl.sync.bfly.b32 	%r130|%p9, %r128, %r129, %r121, %r123;
	mov.b32 	%f13, %r130;
	add.f32 	%f14, %f12, %f13;
	mov.b32 	%r131, %f14;
	shfl.sync.bfly.b32 	%r132|%p10, %r131, %r119, %r121, %r123;
	mov.b32 	%f15, %r132;
	add.f32 	%f16, %f14, %f15;
	mov.b32 	%r133, %f16;
	mov.u32 	%r134, 1;
	shfl.sync.bfly.b32 	%r135|%p11, %r133, %r134, %r121, %r123;
	mov.b32 	%f17, %r135;
	add.f32 	%f18, %f16, %f17;
	st.local.f32 	[%rd3], %f18;
	st.shared.f32 	[%r27], %f18;
	bar.sync 	0;
	@%p1 bra 	$L__BB121_11;

	ld.shared.f32 	%f19, [%r4];
	mov.b32 	%r136, %f19;
	shfl.sync.bfly.b32 	%r140|%p13, %r136, %r122, %r121, %r123;
	mov.b32 	%f20, %r140;
	add.f32 	%f21, %f19, %f20;
	mov.b32 	%r141, %f21;
	shfl.sync.bfly.b32 	%r143|%p14, %r141, %r126, %r121, %r123;
	mov.b32 	%f22, %r143;
	add.f32 	%f23, %f21, %f22;
	mov.b32 	%r144, %f23;
	shfl.sync.bfly.b32 	%r146|%p15, %r144, %r129, %r121, %r123;
	mov.b32 	%f24, %r146;
	add.f32 	%f25, %f23, %f24;
	mov.b32 	%r147, %f25;
	shfl.sync.bfly.b32 	%r149|%p16, %r147, %r119, %r121, %r123;
	mov.b32 	%f26, %r149;
	add.f32 	%f27, %f25, %f26;
	mov.b32 	%r150, %f27;
	shfl.sync.bfly.b32 	%r152|%p17, %r150, %r134, %r121, %r123;
	mov.b32 	%f28, %r152;
	add.f32 	%f29, %f27, %f28;
	st.local.f32 	[%rd3], %f29;

$L__BB121_11:
	bar.sync 	0;
	mov.b32 	%r153, %f1;
	shfl.sync.bfly.b32 	%r157|%p19, %r153, %r122, %r121, %r123;
	mov.b32 	%f30, %r157;
	add.f32 	%f31, %f1, %f30;
	mov.b32 	%r158, %f31;
	shfl.sync.bfly.b32 	%r160|%p20, %r158, %r126, %r121, %r123;
	mov.b32 	%f32, %r160;
	add.f32 	%f33, %f31, %f32;
	mov.b32 	%r161, %f33;
	shfl.sync.bfly.b32 	%r163|%p21, %r161, %r129, %r121, %r123;
	mov.b32 	%f34, %r163;
	add.f32 	%f35, %f33, %f34;
	mov.b32 	%r164, %f35;
	shfl.sync.bfly.b32 	%r166|%p22, %r164, %r119, %r121, %r123;
	mov.b32 	%f36, %r166;
	add.f32 	%f37, %f35, %f36;
	mov.b32 	%r167, %f37;
	shfl.sync.bfly.b32 	%r169|%p23, %r167, %r134, %r121, %r123;
	mov.b32 	%f38, %r169;
	add.f32 	%f39, %f37, %f38;
	st.local.f32 	[%rd3+4], %f39;
	st.shared.f32 	[%r27], %f39;
	bar.sync 	0;
	@%p1 bra 	$L__BB121_13;

	ld.shared.f32 	%f40, [%r4];
	mov.b32 	%r170, %f40;
	mov.u32 	%r171, 31;
	mov.u32 	%r172, 16;
	mov.u32 	%r173, -1;
	shfl.sync.bfly.b32 	%r174|%p24, %r170, %r172, %r171, %r173;
	mov.b32 	%f41, %r174;
	add.f32 	%f42, %f40, %f41;
	mov.b32 	%r175, %f42;
	mov.u32 	%r176, 8;
	shfl.sync.bfly.b32 	%r177|%p25, %r175, %r176, %r171, %r173;
	mov.b32 	%f43, %r177;
	add.f32 	%f44, %f42, %f43;
	mov.b32 	%r178, %f44;
	mov.u32 	%r179, 4;
	shfl.sync.bfly.b32 	%r180|%p26, %r178, %r179, %r171, %r173;
	mov.b32 	%f45, %r180;
	add.f32 	%f46, %f44, %f45;
	mov.b32 	%r181, %f46;
	mov.u32 	%r182, 2;
	shfl.sync.bfly.b32 	%r183|%p27, %r181, %r182, %r171, %r173;
	mov.b32 	%f47, %r183;
	add.f32 	%f48, %f46, %f47;
	mov.b32 	%r184, %f48;
	mov.u32 	%r185, 1;
	shfl.sync.bfly.b32 	%r186|%p28, %r184, %r185, %r171, %r173;
	mov.b32 	%f49, %r186;
	add.f32 	%f50, %f48, %f49;
	st.local.f32 	[%rd3+4], %f50;

$L__BB121_13:
	bar.sync 	0;
	setp.gt.s32 	%p29, %r3, 1;
	@%p29 bra 	$L__BB121_15;

	mad.lo.s32 	%r187, %r3, %r30, %r2;
	cvt.s64.s32 	%rd50, %r187;
	mul.lo.s32 	%r188, %r1, %r31;
	cvt.s64.s32 	%rd51, %r188;
	add.s64 	%rd52, %rd51, %rd50;
	mul.wide.s32 	%rd53, %r3, 4;
	add.s64 	%rd54, %rd3, %rd53;
	ld.local.f32 	%f51, [%rd54];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs3, %f51;}

	// end inline asm
	cvta.to.global.u64 	%rd55, %rd26;
	shl.b64 	%rd56, %rd52, 1;
	add.s64 	%rd57, %rd55, %rd56;
	st.global.u16 	[%rd57], %rs3;

$L__BB121_15:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_3_bs_256
.visible .entry ggml_matvec_f16_ncols_3_bs_256(
	.param .u64 ggml_matvec_f16_ncols_3_bs_256_param_0,
	.param .u64 ggml_matvec_f16_ncols_3_bs_256_param_1,
	.param .u64 ggml_matvec_f16_ncols_3_bs_256_param_2,
	.param .u32 ggml_matvec_f16_ncols_3_bs_256_param_3,
	.param .u32 ggml_matvec_f16_ncols_3_bs_256_param_4,
	.param .u32 ggml_matvec_f16_ncols_3_bs_256_param_5,
	.param .u32 ggml_matvec_f16_ncols_3_bs_256_param_6,
	.param .u32 ggml_matvec_f16_ncols_3_bs_256_param_7,
	.param .u32 ggml_matvec_f16_ncols_3_bs_256_param_8,
	.param .u32 ggml_matvec_f16_ncols_3_bs_256_param_9,
	.param .u32 ggml_matvec_f16_ncols_3_bs_256_param_10,
	.param .u32 ggml_matvec_f16_ncols_3_bs_256_param_11
)
{
	.local .align 4 .b8 	__local_depot122[12];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<41>;
	.reg .b16 	%rs<4>;
	.reg .f32 	%f<76>;
	.reg .b32 	%r<286>;
	.reg .b64 	%rd<72>;


	mov.u64 	%SPL, __local_depot122;
	ld.param.u64 	%rd29, [ggml_matvec_f16_ncols_3_bs_256_param_0];
	ld.param.u64 	%rd30, [ggml_matvec_f16_ncols_3_bs_256_param_1];
	ld.param.u64 	%rd28, [ggml_matvec_f16_ncols_3_bs_256_param_2];
	ld.param.u32 	%r34, [ggml_matvec_f16_ncols_3_bs_256_param_3];
	ld.param.u32 	%r38, [ggml_matvec_f16_ncols_3_bs_256_param_5];
	ld.param.u32 	%r35, [ggml_matvec_f16_ncols_3_bs_256_param_6];
	ld.param.u32 	%r36, [ggml_matvec_f16_ncols_3_bs_256_param_7];
	ld.param.u32 	%r39, [ggml_matvec_f16_ncols_3_bs_256_param_8];
	ld.param.u32 	%r40, [ggml_matvec_f16_ncols_3_bs_256_param_9];
	ld.param.u32 	%r41, [ggml_matvec_f16_ncols_3_bs_256_param_10];
	ld.param.u32 	%r37, [ggml_matvec_f16_ncols_3_bs_256_param_11];
	cvta.to.global.u64 	%rd71, %rd30;
	cvta.to.global.u64 	%rd2, %rd29;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r42, %r1, %r39;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r43, %r2, %r38;
	mad.lo.s32 	%r44, %r42, %r40, %r43;
	cvt.s64.s32 	%rd4, %r44;
	mul.lo.s32 	%r45, %r1, %r41;
	cvt.s64.s32 	%rd5, %r45;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r46, %r3, 2;
	mov.u32 	%r47, data_mmv;
	add.s32 	%r4, %r47, %r46;
	@%p1 bra 	$L__BB122_2;

	mov.u32 	%r48, 0;
	st.shared.u32 	[%r4], %r48;

$L__BB122_2:
	bar.sync 	0;
	mov.f32 	%f4, 0f00000000;
	mov.u32 	%r283, 0;
	st.local.u32 	[%rd3], %r283;
	st.local.u32 	[%rd3+4], %r283;
	st.local.u32 	[%rd3+8], %r283;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs2, %f4;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f4;}

	// end inline asm
	mov.b32 	%r285, {%rs1, %rs2};
	setp.ge.s32 	%p2, %r3, %r34;
	mov.u32 	%r284, %r283;
	@%p2 bra 	$L__BB122_9;

	not.b32 	%r53, %r3;
	add.s32 	%r6, %r53, %r34;
	shr.u32 	%r54, %r6, 8;
	add.s32 	%r55, %r54, 1;
	and.b32  	%r274, %r55, 3;
	setp.eq.s32 	%p3, %r274, 0;
	mov.u32 	%r283, 0;
	mov.u32 	%r278, %r3;
	@%p3 bra 	$L__BB122_6;

	shl.b32 	%r58, %r35, 1;
	add.s32 	%r59, %r3, %r58;
	mul.wide.s32 	%rd32, %r59, 2;
	add.s64 	%rd33, %rd32, %rd5;
	shl.b64 	%rd34, %rd33, 1;
	add.s64 	%rd69, %rd71, %rd34;
	mul.wide.s32 	%rd35, %r35, 2;
	mul.wide.s32 	%rd36, %r3, 2;
	add.s64 	%rd37, %rd35, %rd36;
	add.s64 	%rd38, %rd37, %rd5;
	shl.b64 	%rd39, %rd38, 1;
	add.s64 	%rd68, %rd71, %rd39;
	add.s64 	%rd40, %rd36, %rd5;
	shl.b64 	%rd41, %rd40, 1;
	add.s64 	%rd67, %rd71, %rd41;
	add.s64 	%rd42, %rd36, %rd4;
	shl.b64 	%rd43, %rd42, 1;
	add.s64 	%rd66, %rd2, %rd43;
	mov.u32 	%r283, 0;
	mov.u32 	%r284, %r283;
	mov.u32 	%r278, %r3;

$L__BB122_5:
	.pragma "nounroll";
	ld.global.nc.u32 	%r61, [%rd66];
	ld.global.nc.u32 	%r62, [%rd67];
	// begin inline asm
	{mul.f16x2 %r60,%r61,%r62;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r285,%r285,%r60;
}
	// end inline asm
	ld.global.nc.u32 	%r68, [%rd68];
	// begin inline asm
	{mul.f16x2 %r66,%r61,%r68;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r284,%r284,%r66;
}
	// end inline asm
	ld.global.nc.u32 	%r74, [%rd69];
	// begin inline asm
	{mul.f16x2 %r72,%r61,%r74;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r283,%r283,%r72;
}
	// end inline asm
	add.s32 	%r278, %r278, 256;
	add.s64 	%rd69, %rd69, 1024;
	add.s64 	%rd68, %rd68, 1024;
	add.s64 	%rd67, %rd67, 1024;
	add.s64 	%rd66, %rd66, 1024;
	add.s32 	%r274, %r274, -1;
	setp.ne.s32 	%p4, %r274, 0;
	@%p4 bra 	$L__BB122_5;

$L__BB122_6:
	setp.lt.u32 	%p5, %r6, 768;
	@%p5 bra 	$L__BB122_9;

	add.s32 	%r78, %r278, %r35;
	shl.b32 	%r79, %r35, 1;
	add.s32 	%r80, %r278, %r79;
	add.s32 	%r81, %r78, 256;
	mul.wide.s32 	%rd44, %r81, 4;
	shl.b64 	%rd45, %rd5, 1;
	add.s64 	%rd19, %rd44, %rd45;
	mul.wide.s32 	%rd46, %r80, 4;
	add.s64 	%rd20, %rd46, %rd45;
	mul.wide.s32 	%rd47, %r278, 2;
	add.s64 	%rd48, %rd47, %rd4;
	shl.b64 	%rd49, %rd48, 1;
	add.s64 	%rd50, %rd2, %rd49;
	add.s64 	%rd70, %rd50, 2048;
	mul.wide.s32 	%rd51, %r278, 4;
	add.s64 	%rd22, %rd51, %rd45;
	mul.wide.s32 	%rd52, %r35, 4;
	add.s64 	%rd53, %rd51, %rd52;
	add.s64 	%rd23, %rd53, %rd45;

$L__BB122_8:
	ld.global.nc.u32 	%r83, [%rd70+-2048];
	add.s64 	%rd54, %rd71, %rd22;
	ld.global.nc.u32 	%r84, [%rd54];
	// begin inline asm
	{mul.f16x2 %r82,%r83,%r84;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r85,%r285,%r82;
}
	// end inline asm
	add.s64 	%rd55, %rd71, %rd23;
	ld.global.nc.u32 	%r90, [%rd55];
	// begin inline asm
	{mul.f16x2 %r88,%r83,%r90;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r91,%r284,%r88;
}
	// end inline asm
	add.s64 	%rd56, %rd71, %rd20;
	ld.global.nc.u32 	%r96, [%rd56];
	// begin inline asm
	{mul.f16x2 %r94,%r83,%r96;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r97,%r283,%r94;
}
	// end inline asm
	ld.global.nc.u32 	%r101, [%rd70+-1024];
	ld.global.nc.u32 	%r102, [%rd54+1024];
	// begin inline asm
	{mul.f16x2 %r100,%r101,%r102;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r103,%r85,%r100;
}
	// end inline asm
	add.s64 	%rd57, %rd71, %rd19;
	ld.global.nc.u32 	%r108, [%rd57];
	// begin inline asm
	{mul.f16x2 %r106,%r101,%r108;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r109,%r91,%r106;
}
	// end inline asm
	ld.global.nc.u32 	%r114, [%rd56+1024];
	// begin inline asm
	{mul.f16x2 %r112,%r101,%r114;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r115,%r97,%r112;
}
	// end inline asm
	ld.global.nc.u32 	%r119, [%rd70];
	ld.global.nc.u32 	%r120, [%rd54+2048];
	// begin inline asm
	{mul.f16x2 %r118,%r119,%r120;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r121,%r103,%r118;
}
	// end inline asm
	ld.global.nc.u32 	%r126, [%rd57+1024];
	// begin inline asm
	{mul.f16x2 %r124,%r119,%r126;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r127,%r109,%r124;
}
	// end inline asm
	ld.global.nc.u32 	%r132, [%rd56+2048];
	// begin inline asm
	{mul.f16x2 %r130,%r119,%r132;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r133,%r115,%r130;
}
	// end inline asm
	ld.global.nc.u32 	%r137, [%rd70+1024];
	ld.global.nc.u32 	%r138, [%rd54+3072];
	// begin inline asm
	{mul.f16x2 %r136,%r137,%r138;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r285,%r121,%r136;
}
	// end inline asm
	ld.global.nc.u32 	%r144, [%rd57+2048];
	// begin inline asm
	{mul.f16x2 %r142,%r137,%r144;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r284,%r127,%r142;
}
	// end inline asm
	ld.global.nc.u32 	%r150, [%rd56+3072];
	// begin inline asm
	{mul.f16x2 %r148,%r137,%r150;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r283,%r133,%r148;
}
	// end inline asm
	add.s64 	%rd71, %rd71, 4096;
	add.s64 	%rd70, %rd70, 4096;
	add.s32 	%r278, %r278, 1024;
	setp.lt.s32 	%p6, %r278, %r34;
	@%p6 bra 	$L__BB122_8;

$L__BB122_9:
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r285;
  cvt.f32.f16 %f5, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r285;
  cvt.f32.f16 %f6, high;}

	// end inline asm
	add.f32 	%f11, %f5, %f6;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r284;
  cvt.f32.f16 %f7, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r284;
  cvt.f32.f16 %f8, high;}

	// end inline asm
	add.f32 	%f1, %f7, %f8;
	st.local.f32 	[%rd3+4], %f1;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r283;
  cvt.f32.f16 %f9, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r283;
  cvt.f32.f16 %f10, high;}

	// end inline asm
	add.f32 	%f2, %f9, %f10;
	st.local.f32 	[%rd3+8], %f2;
	shr.s32 	%r160, %r3, 31;
	shr.u32 	%r161, %r160, 27;
	add.s32 	%r162, %r3, %r161;
	shr.s32 	%r163, %r162, 5;
	shl.b32 	%r164, %r163, 2;
	add.s32 	%r33, %r47, %r164;
	mov.u32 	%r166, 2;
	mov.b32 	%r167, %f11;
	mov.u32 	%r168, 31;
	mov.u32 	%r169, 16;
	mov.u32 	%r170, -1;
	shfl.sync.bfly.b32 	%r171|%p7, %r167, %r169, %r168, %r170;
	mov.b32 	%f12, %r171;
	add.f32 	%f13, %f11, %f12;
	mov.b32 	%r172, %f13;
	mov.u32 	%r173, 8;
	shfl.sync.bfly.b32 	%r174|%p8, %r172, %r173, %r168, %r170;
	mov.b32 	%f14, %r174;
	add.f32 	%f15, %f13, %f14;
	mov.b32 	%r175, %f15;
	mov.u32 	%r176, 4;
	shfl.sync.bfly.b32 	%r177|%p9, %r175, %r176, %r168, %r170;
	mov.b32 	%f16, %r177;
	add.f32 	%f17, %f15, %f16;
	mov.b32 	%r178, %f17;
	shfl.sync.bfly.b32 	%r179|%p10, %r178, %r166, %r168, %r170;
	mov.b32 	%f18, %r179;
	add.f32 	%f19, %f17, %f18;
	mov.b32 	%r180, %f19;
	mov.u32 	%r181, 1;
	shfl.sync.bfly.b32 	%r182|%p11, %r180, %r181, %r168, %r170;
	mov.b32 	%f20, %r182;
	add.f32 	%f21, %f19, %f20;
	st.local.f32 	[%rd3], %f21;
	st.shared.f32 	[%r33], %f21;
	bar.sync 	0;
	@%p1 bra 	$L__BB122_11;

	ld.shared.f32 	%f22, [%r4];
	mov.b32 	%r183, %f22;
	shfl.sync.bfly.b32 	%r187|%p13, %r183, %r169, %r168, %r170;
	mov.b32 	%f23, %r187;
	add.f32 	%f24, %f22, %f23;
	mov.b32 	%r188, %f24;
	shfl.sync.bfly.b32 	%r190|%p14, %r188, %r173, %r168, %r170;
	mov.b32 	%f25, %r190;
	add.f32 	%f26, %f24, %f25;
	mov.b32 	%r191, %f26;
	shfl.sync.bfly.b32 	%r193|%p15, %r191, %r176, %r168, %r170;
	mov.b32 	%f27, %r193;
	add.f32 	%f28, %f26, %f27;
	mov.b32 	%r194, %f28;
	shfl.sync.bfly.b32 	%r196|%p16, %r194, %r166, %r168, %r170;
	mov.b32 	%f29, %r196;
	add.f32 	%f30, %f28, %f29;
	mov.b32 	%r197, %f30;
	shfl.sync.bfly.b32 	%r199|%p17, %r197, %r181, %r168, %r170;
	mov.b32 	%f31, %r199;
	add.f32 	%f32, %f30, %f31;
	st.local.f32 	[%rd3], %f32;

$L__BB122_11:
	bar.sync 	0;
	mov.b32 	%r200, %f1;
	shfl.sync.bfly.b32 	%r204|%p19, %r200, %r169, %r168, %r170;
	mov.b32 	%f33, %r204;
	add.f32 	%f34, %f1, %f33;
	mov.b32 	%r205, %f34;
	shfl.sync.bfly.b32 	%r207|%p20, %r205, %r173, %r168, %r170;
	mov.b32 	%f35, %r207;
	add.f32 	%f36, %f34, %f35;
	mov.b32 	%r208, %f36;
	shfl.sync.bfly.b32 	%r210|%p21, %r208, %r176, %r168, %r170;
	mov.b32 	%f37, %r210;
	add.f32 	%f38, %f36, %f37;
	mov.b32 	%r211, %f38;
	shfl.sync.bfly.b32 	%r213|%p22, %r211, %r166, %r168, %r170;
	mov.b32 	%f39, %r213;
	add.f32 	%f40, %f38, %f39;
	mov.b32 	%r214, %f40;
	shfl.sync.bfly.b32 	%r216|%p23, %r214, %r181, %r168, %r170;
	mov.b32 	%f41, %r216;
	add.f32 	%f42, %f40, %f41;
	st.local.f32 	[%rd3+4], %f42;
	st.shared.f32 	[%r33], %f42;
	bar.sync 	0;
	@%p1 bra 	$L__BB122_13;

	ld.shared.f32 	%f43, [%r4];
	mov.b32 	%r217, %f43;
	mov.u32 	%r218, 31;
	mov.u32 	%r219, 16;
	mov.u32 	%r220, -1;
	shfl.sync.bfly.b32 	%r221|%p24, %r217, %r219, %r218, %r220;
	mov.b32 	%f44, %r221;
	add.f32 	%f45, %f43, %f44;
	mov.b32 	%r222, %f45;
	mov.u32 	%r223, 8;
	shfl.sync.bfly.b32 	%r224|%p25, %r222, %r223, %r218, %r220;
	mov.b32 	%f46, %r224;
	add.f32 	%f47, %f45, %f46;
	mov.b32 	%r225, %f47;
	mov.u32 	%r226, 4;
	shfl.sync.bfly.b32 	%r227|%p26, %r225, %r226, %r218, %r220;
	mov.b32 	%f48, %r227;
	add.f32 	%f49, %f47, %f48;
	mov.b32 	%r228, %f49;
	mov.u32 	%r229, 2;
	shfl.sync.bfly.b32 	%r230|%p27, %r228, %r229, %r218, %r220;
	mov.b32 	%f50, %r230;
	add.f32 	%f51, %f49, %f50;
	mov.b32 	%r231, %f51;
	mov.u32 	%r232, 1;
	shfl.sync.bfly.b32 	%r233|%p28, %r231, %r232, %r218, %r220;
	mov.b32 	%f52, %r233;
	add.f32 	%f53, %f51, %f52;
	st.local.f32 	[%rd3+4], %f53;

$L__BB122_13:
	bar.sync 	0;
	mov.b32 	%r234, %f2;
	mov.u32 	%r235, 31;
	mov.u32 	%r236, 16;
	mov.u32 	%r237, -1;
	shfl.sync.bfly.b32 	%r238|%p30, %r234, %r236, %r235, %r237;
	mov.b32 	%f54, %r238;
	add.f32 	%f55, %f2, %f54;
	mov.b32 	%r239, %f55;
	mov.u32 	%r240, 8;
	shfl.sync.bfly.b32 	%r241|%p31, %r239, %r240, %r235, %r237;
	mov.b32 	%f56, %r241;
	add.f32 	%f57, %f55, %f56;
	mov.b32 	%r242, %f57;
	mov.u32 	%r243, 4;
	shfl.sync.bfly.b32 	%r244|%p32, %r242, %r243, %r235, %r237;
	mov.b32 	%f58, %r244;
	add.f32 	%f59, %f57, %f58;
	mov.b32 	%r245, %f59;
	mov.u32 	%r246, 2;
	shfl.sync.bfly.b32 	%r247|%p33, %r245, %r246, %r235, %r237;
	mov.b32 	%f60, %r247;
	add.f32 	%f61, %f59, %f60;
	mov.b32 	%r248, %f61;
	mov.u32 	%r249, 1;
	shfl.sync.bfly.b32 	%r250|%p34, %r248, %r249, %r235, %r237;
	mov.b32 	%f62, %r250;
	add.f32 	%f63, %f61, %f62;
	st.local.f32 	[%rd3+8], %f63;
	st.shared.f32 	[%r33], %f63;
	bar.sync 	0;
	@%p1 bra 	$L__BB122_15;

	ld.shared.f32 	%f64, [%r4];
	mov.b32 	%r251, %f64;
	shfl.sync.bfly.b32 	%r255|%p35, %r251, %r236, %r235, %r237;
	mov.b32 	%f65, %r255;
	add.f32 	%f66, %f64, %f65;
	mov.b32 	%r256, %f66;
	shfl.sync.bfly.b32 	%r258|%p36, %r256, %r240, %r235, %r237;
	mov.b32 	%f67, %r258;
	add.f32 	%f68, %f66, %f67;
	mov.b32 	%r259, %f68;
	shfl.sync.bfly.b32 	%r261|%p37, %r259, %r243, %r235, %r237;
	mov.b32 	%f69, %r261;
	add.f32 	%f70, %f68, %f69;
	mov.b32 	%r262, %f70;
	shfl.sync.bfly.b32 	%r264|%p38, %r262, %r246, %r235, %r237;
	mov.b32 	%f71, %r264;
	add.f32 	%f72, %f70, %f71;
	mov.b32 	%r265, %f72;
	shfl.sync.bfly.b32 	%r267|%p39, %r265, %r249, %r235, %r237;
	mov.b32 	%f73, %r267;
	add.f32 	%f74, %f72, %f73;
	st.local.f32 	[%rd3+8], %f74;

$L__BB122_15:
	bar.sync 	0;
	setp.gt.s32 	%p40, %r3, 2;
	@%p40 bra 	$L__BB122_17;

	mad.lo.s32 	%r268, %r3, %r36, %r2;
	cvt.s64.s32 	%rd58, %r268;
	mul.lo.s32 	%r269, %r1, %r37;
	cvt.s64.s32 	%rd59, %r269;
	add.s64 	%rd60, %rd59, %rd58;
	mul.wide.s32 	%rd61, %r3, 4;
	add.s64 	%rd62, %rd3, %rd61;
	ld.local.f32 	%f75, [%rd62];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs3, %f75;}

	// end inline asm
	cvta.to.global.u64 	%rd63, %rd28;
	shl.b64 	%rd64, %rd60, 1;
	add.s64 	%rd65, %rd63, %rd64;
	st.global.u16 	[%rd65], %rs3;

$L__BB122_17:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_4_bs_256
.visible .entry ggml_matvec_f16_ncols_4_bs_256(
	.param .u64 ggml_matvec_f16_ncols_4_bs_256_param_0,
	.param .u64 ggml_matvec_f16_ncols_4_bs_256_param_1,
	.param .u64 ggml_matvec_f16_ncols_4_bs_256_param_2,
	.param .u32 ggml_matvec_f16_ncols_4_bs_256_param_3,
	.param .u32 ggml_matvec_f16_ncols_4_bs_256_param_4,
	.param .u32 ggml_matvec_f16_ncols_4_bs_256_param_5,
	.param .u32 ggml_matvec_f16_ncols_4_bs_256_param_6,
	.param .u32 ggml_matvec_f16_ncols_4_bs_256_param_7,
	.param .u32 ggml_matvec_f16_ncols_4_bs_256_param_8,
	.param .u32 ggml_matvec_f16_ncols_4_bs_256_param_9,
	.param .u32 ggml_matvec_f16_ncols_4_bs_256_param_10,
	.param .u32 ggml_matvec_f16_ncols_4_bs_256_param_11
)
{
	.local .align 16 .b8 	__local_depot123[16];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<52>;
	.reg .b16 	%rs<4>;
	.reg .f32 	%f<100>;
	.reg .b32 	%r<367>;
	.reg .b64 	%rd<83>;


	mov.u64 	%SPL, __local_depot123;
	ld.param.u64 	%rd34, [ggml_matvec_f16_ncols_4_bs_256_param_0];
	ld.param.u64 	%rd35, [ggml_matvec_f16_ncols_4_bs_256_param_1];
	ld.param.u64 	%rd33, [ggml_matvec_f16_ncols_4_bs_256_param_2];
	ld.param.u32 	%r40, [ggml_matvec_f16_ncols_4_bs_256_param_3];
	ld.param.u32 	%r44, [ggml_matvec_f16_ncols_4_bs_256_param_5];
	ld.param.u32 	%r41, [ggml_matvec_f16_ncols_4_bs_256_param_6];
	ld.param.u32 	%r42, [ggml_matvec_f16_ncols_4_bs_256_param_7];
	ld.param.u32 	%r45, [ggml_matvec_f16_ncols_4_bs_256_param_8];
	ld.param.u32 	%r46, [ggml_matvec_f16_ncols_4_bs_256_param_9];
	ld.param.u32 	%r47, [ggml_matvec_f16_ncols_4_bs_256_param_10];
	ld.param.u32 	%r43, [ggml_matvec_f16_ncols_4_bs_256_param_11];
	cvta.to.global.u64 	%rd82, %rd35;
	cvta.to.global.u64 	%rd2, %rd34;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r48, %r1, %r45;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r49, %r2, %r44;
	mad.lo.s32 	%r50, %r48, %r46, %r49;
	cvt.s64.s32 	%rd4, %r50;
	mul.lo.s32 	%r51, %r1, %r47;
	cvt.s64.s32 	%rd5, %r51;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r52, %r3, 2;
	mov.u32 	%r53, data_mmv;
	add.s32 	%r4, %r53, %r52;
	@%p1 bra 	$L__BB123_2;

	mov.u32 	%r54, 0;
	st.shared.u32 	[%r4], %r54;

$L__BB123_2:
	bar.sync 	0;
	mov.f32 	%f5, 0f00000000;
	st.local.v4.f32 	[%rd3], {%f5, %f5, %f5, %f5};
	mov.u32 	%r363, 0;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs2, %f5;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f5;}

	// end inline asm
	mov.b32 	%r366, {%rs1, %rs2};
	setp.ge.s32 	%p2, %r3, %r40;
	mov.u32 	%r364, %r363;
	mov.u32 	%r365, %r363;
	@%p2 bra 	$L__BB123_9;

	not.b32 	%r61, %r3;
	add.s32 	%r6, %r61, %r40;
	shr.u32 	%r62, %r6, 8;
	add.s32 	%r63, %r62, 1;
	and.b32  	%r352, %r63, 3;
	setp.eq.s32 	%p3, %r352, 0;
	mov.u32 	%r363, 0;
	mov.u32 	%r357, %r3;
	@%p3 bra 	$L__BB123_6;

	shl.b32 	%r67, %r41, 1;
	add.s32 	%r68, %r3, %r67;
	mul.wide.s32 	%rd37, %r68, 2;
	add.s64 	%rd38, %rd37, %rd5;
	shl.b64 	%rd39, %rd38, 1;
	add.s64 	%rd80, %rd82, %rd39;
	mad.lo.s32 	%r69, %r41, 3, %r3;
	mul.wide.s32 	%rd40, %r69, 2;
	add.s64 	%rd41, %rd40, %rd5;
	shl.b64 	%rd42, %rd41, 1;
	add.s64 	%rd79, %rd82, %rd42;
	mul.wide.s32 	%rd43, %r41, 2;
	mul.wide.s32 	%rd44, %r3, 2;
	add.s64 	%rd45, %rd43, %rd44;
	add.s64 	%rd46, %rd45, %rd5;
	shl.b64 	%rd47, %rd46, 1;
	add.s64 	%rd78, %rd82, %rd47;
	add.s64 	%rd48, %rd44, %rd5;
	shl.b64 	%rd49, %rd48, 1;
	add.s64 	%rd77, %rd82, %rd49;
	add.s64 	%rd50, %rd44, %rd4;
	shl.b64 	%rd51, %rd50, 1;
	add.s64 	%rd76, %rd2, %rd51;
	mov.u32 	%r363, 0;
	mov.u32 	%r364, %r363;
	mov.u32 	%r365, %r363;
	mov.u32 	%r357, %r3;

$L__BB123_5:
	.pragma "nounroll";
	ld.global.nc.u32 	%r71, [%rd76];
	ld.global.nc.u32 	%r72, [%rd77];
	// begin inline asm
	{mul.f16x2 %r70,%r71,%r72;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r366,%r366,%r70;
}
	// end inline asm
	ld.global.nc.u32 	%r78, [%rd78];
	// begin inline asm
	{mul.f16x2 %r76,%r71,%r78;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r365,%r365,%r76;
}
	// end inline asm
	ld.global.nc.u32 	%r84, [%rd80];
	// begin inline asm
	{mul.f16x2 %r82,%r71,%r84;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r364,%r364,%r82;
}
	// end inline asm
	ld.global.nc.u32 	%r90, [%rd79];
	// begin inline asm
	{mul.f16x2 %r88,%r71,%r90;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r363,%r363,%r88;
}
	// end inline asm
	add.s32 	%r357, %r357, 256;
	add.s64 	%rd80, %rd80, 1024;
	add.s64 	%rd79, %rd79, 1024;
	add.s64 	%rd78, %rd78, 1024;
	add.s64 	%rd77, %rd77, 1024;
	add.s64 	%rd76, %rd76, 1024;
	add.s32 	%r352, %r352, -1;
	setp.ne.s32 	%p4, %r352, 0;
	@%p4 bra 	$L__BB123_5;

$L__BB123_6:
	setp.lt.u32 	%p5, %r6, 768;
	@%p5 bra 	$L__BB123_9;

	add.s32 	%r94, %r357, %r41;
	shl.b32 	%r95, %r41, 1;
	add.s32 	%r96, %r357, %r95;
	mad.lo.s32 	%r97, %r41, 3, %r357;
	add.s32 	%r98, %r94, 256;
	mul.wide.s32 	%rd52, %r98, 4;
	shl.b64 	%rd53, %rd5, 1;
	add.s64 	%rd23, %rd52, %rd53;
	mul.wide.s32 	%rd54, %r96, 4;
	add.s64 	%rd24, %rd54, %rd53;
	mul.wide.s32 	%rd55, %r97, 4;
	add.s64 	%rd25, %rd55, %rd53;
	mul.wide.s32 	%rd56, %r357, 2;
	add.s64 	%rd57, %rd56, %rd4;
	shl.b64 	%rd58, %rd57, 1;
	add.s64 	%rd59, %rd2, %rd58;
	add.s64 	%rd81, %rd59, 2048;
	mul.wide.s32 	%rd60, %r357, 4;
	add.s64 	%rd27, %rd60, %rd53;
	mul.wide.s32 	%rd61, %r41, 4;
	add.s64 	%rd62, %rd60, %rd61;
	add.s64 	%rd28, %rd62, %rd53;

$L__BB123_8:
	ld.global.nc.u32 	%r100, [%rd81+-2048];
	add.s64 	%rd63, %rd82, %rd27;
	ld.global.nc.u32 	%r101, [%rd63];
	// begin inline asm
	{mul.f16x2 %r99,%r100,%r101;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r102,%r366,%r99;
}
	// end inline asm
	add.s64 	%rd64, %rd82, %rd28;
	ld.global.nc.u32 	%r107, [%rd64];
	// begin inline asm
	{mul.f16x2 %r105,%r100,%r107;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r108,%r365,%r105;
}
	// end inline asm
	add.s64 	%rd65, %rd82, %rd24;
	ld.global.nc.u32 	%r113, [%rd65];
	// begin inline asm
	{mul.f16x2 %r111,%r100,%r113;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r114,%r364,%r111;
}
	// end inline asm
	add.s64 	%rd66, %rd82, %rd25;
	ld.global.nc.u32 	%r119, [%rd66];
	// begin inline asm
	{mul.f16x2 %r117,%r100,%r119;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r120,%r363,%r117;
}
	// end inline asm
	ld.global.nc.u32 	%r124, [%rd81+-1024];
	ld.global.nc.u32 	%r125, [%rd63+1024];
	// begin inline asm
	{mul.f16x2 %r123,%r124,%r125;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r126,%r102,%r123;
}
	// end inline asm
	add.s64 	%rd67, %rd82, %rd23;
	ld.global.nc.u32 	%r131, [%rd67];
	// begin inline asm
	{mul.f16x2 %r129,%r124,%r131;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r132,%r108,%r129;
}
	// end inline asm
	ld.global.nc.u32 	%r137, [%rd65+1024];
	// begin inline asm
	{mul.f16x2 %r135,%r124,%r137;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r138,%r114,%r135;
}
	// end inline asm
	ld.global.nc.u32 	%r143, [%rd66+1024];
	// begin inline asm
	{mul.f16x2 %r141,%r124,%r143;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r144,%r120,%r141;
}
	// end inline asm
	ld.global.nc.u32 	%r148, [%rd81];
	ld.global.nc.u32 	%r149, [%rd63+2048];
	// begin inline asm
	{mul.f16x2 %r147,%r148,%r149;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r150,%r126,%r147;
}
	// end inline asm
	ld.global.nc.u32 	%r155, [%rd67+1024];
	// begin inline asm
	{mul.f16x2 %r153,%r148,%r155;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r156,%r132,%r153;
}
	// end inline asm
	ld.global.nc.u32 	%r161, [%rd65+2048];
	// begin inline asm
	{mul.f16x2 %r159,%r148,%r161;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r162,%r138,%r159;
}
	// end inline asm
	ld.global.nc.u32 	%r167, [%rd66+2048];
	// begin inline asm
	{mul.f16x2 %r165,%r148,%r167;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r168,%r144,%r165;
}
	// end inline asm
	ld.global.nc.u32 	%r172, [%rd81+1024];
	ld.global.nc.u32 	%r173, [%rd63+3072];
	// begin inline asm
	{mul.f16x2 %r171,%r172,%r173;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r366,%r150,%r171;
}
	// end inline asm
	ld.global.nc.u32 	%r179, [%rd67+2048];
	// begin inline asm
	{mul.f16x2 %r177,%r172,%r179;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r365,%r156,%r177;
}
	// end inline asm
	ld.global.nc.u32 	%r185, [%rd65+3072];
	// begin inline asm
	{mul.f16x2 %r183,%r172,%r185;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r364,%r162,%r183;
}
	// end inline asm
	ld.global.nc.u32 	%r191, [%rd66+3072];
	// begin inline asm
	{mul.f16x2 %r189,%r172,%r191;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r363,%r168,%r189;
}
	// end inline asm
	add.s64 	%rd82, %rd82, 4096;
	add.s64 	%rd81, %rd81, 4096;
	add.s32 	%r357, %r357, 1024;
	setp.lt.s32 	%p6, %r357, %r40;
	@%p6 bra 	$L__BB123_8;

$L__BB123_9:
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r366;
  cvt.f32.f16 %f6, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r366;
  cvt.f32.f16 %f7, high;}

	// end inline asm
	add.f32 	%f14, %f6, %f7;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r365;
  cvt.f32.f16 %f8, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r365;
  cvt.f32.f16 %f9, high;}

	// end inline asm
	add.f32 	%f1, %f8, %f9;
	st.local.f32 	[%rd3+4], %f1;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r364;
  cvt.f32.f16 %f10, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r364;
  cvt.f32.f16 %f11, high;}

	// end inline asm
	add.f32 	%f2, %f10, %f11;
	st.local.f32 	[%rd3+8], %f2;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r363;
  cvt.f32.f16 %f12, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r363;
  cvt.f32.f16 %f13, high;}

	// end inline asm
	add.f32 	%f3, %f12, %f13;
	st.local.f32 	[%rd3+12], %f3;
	shr.s32 	%r203, %r3, 31;
	shr.u32 	%r204, %r203, 27;
	add.s32 	%r205, %r3, %r204;
	shr.s32 	%r206, %r205, 5;
	shl.b32 	%r207, %r206, 2;
	add.s32 	%r39, %r53, %r207;
	mov.u32 	%r209, 2;
	mov.b32 	%r210, %f14;
	mov.u32 	%r211, 31;
	mov.u32 	%r212, 16;
	mov.u32 	%r213, -1;
	shfl.sync.bfly.b32 	%r214|%p7, %r210, %r212, %r211, %r213;
	mov.b32 	%f15, %r214;
	add.f32 	%f16, %f14, %f15;
	mov.b32 	%r215, %f16;
	mov.u32 	%r216, 8;
	shfl.sync.bfly.b32 	%r217|%p8, %r215, %r216, %r211, %r213;
	mov.b32 	%f17, %r217;
	add.f32 	%f18, %f16, %f17;
	mov.b32 	%r218, %f18;
	mov.u32 	%r219, 4;
	shfl.sync.bfly.b32 	%r220|%p9, %r218, %r219, %r211, %r213;
	mov.b32 	%f19, %r220;
	add.f32 	%f20, %f18, %f19;
	mov.b32 	%r221, %f20;
	shfl.sync.bfly.b32 	%r222|%p10, %r221, %r209, %r211, %r213;
	mov.b32 	%f21, %r222;
	add.f32 	%f22, %f20, %f21;
	mov.b32 	%r223, %f22;
	mov.u32 	%r224, 1;
	shfl.sync.bfly.b32 	%r225|%p11, %r223, %r224, %r211, %r213;
	mov.b32 	%f23, %r225;
	add.f32 	%f24, %f22, %f23;
	st.local.f32 	[%rd3], %f24;
	st.shared.f32 	[%r39], %f24;
	bar.sync 	0;
	@%p1 bra 	$L__BB123_11;

	ld.shared.f32 	%f25, [%r4];
	mov.b32 	%r226, %f25;
	shfl.sync.bfly.b32 	%r230|%p13, %r226, %r212, %r211, %r213;
	mov.b32 	%f26, %r230;
	add.f32 	%f27, %f25, %f26;
	mov.b32 	%r231, %f27;
	shfl.sync.bfly.b32 	%r233|%p14, %r231, %r216, %r211, %r213;
	mov.b32 	%f28, %r233;
	add.f32 	%f29, %f27, %f28;
	mov.b32 	%r234, %f29;
	shfl.sync.bfly.b32 	%r236|%p15, %r234, %r219, %r211, %r213;
	mov.b32 	%f30, %r236;
	add.f32 	%f31, %f29, %f30;
	mov.b32 	%r237, %f31;
	shfl.sync.bfly.b32 	%r239|%p16, %r237, %r209, %r211, %r213;
	mov.b32 	%f32, %r239;
	add.f32 	%f33, %f31, %f32;
	mov.b32 	%r240, %f33;
	shfl.sync.bfly.b32 	%r242|%p17, %r240, %r224, %r211, %r213;
	mov.b32 	%f34, %r242;
	add.f32 	%f35, %f33, %f34;
	st.local.f32 	[%rd3], %f35;

$L__BB123_11:
	bar.sync 	0;
	mov.b32 	%r243, %f1;
	shfl.sync.bfly.b32 	%r247|%p19, %r243, %r212, %r211, %r213;
	mov.b32 	%f36, %r247;
	add.f32 	%f37, %f1, %f36;
	mov.b32 	%r248, %f37;
	shfl.sync.bfly.b32 	%r250|%p20, %r248, %r216, %r211, %r213;
	mov.b32 	%f38, %r250;
	add.f32 	%f39, %f37, %f38;
	mov.b32 	%r251, %f39;
	shfl.sync.bfly.b32 	%r253|%p21, %r251, %r219, %r211, %r213;
	mov.b32 	%f40, %r253;
	add.f32 	%f41, %f39, %f40;
	mov.b32 	%r254, %f41;
	shfl.sync.bfly.b32 	%r256|%p22, %r254, %r209, %r211, %r213;
	mov.b32 	%f42, %r256;
	add.f32 	%f43, %f41, %f42;
	mov.b32 	%r257, %f43;
	shfl.sync.bfly.b32 	%r259|%p23, %r257, %r224, %r211, %r213;
	mov.b32 	%f44, %r259;
	add.f32 	%f45, %f43, %f44;
	st.local.f32 	[%rd3+4], %f45;
	st.shared.f32 	[%r39], %f45;
	bar.sync 	0;
	@%p1 bra 	$L__BB123_13;

	ld.shared.f32 	%f46, [%r4];
	mov.b32 	%r260, %f46;
	mov.u32 	%r261, 31;
	mov.u32 	%r262, 16;
	mov.u32 	%r263, -1;
	shfl.sync.bfly.b32 	%r264|%p24, %r260, %r262, %r261, %r263;
	mov.b32 	%f47, %r264;
	add.f32 	%f48, %f46, %f47;
	mov.b32 	%r265, %f48;
	mov.u32 	%r266, 8;
	shfl.sync.bfly.b32 	%r267|%p25, %r265, %r266, %r261, %r263;
	mov.b32 	%f49, %r267;
	add.f32 	%f50, %f48, %f49;
	mov.b32 	%r268, %f50;
	mov.u32 	%r269, 4;
	shfl.sync.bfly.b32 	%r270|%p26, %r268, %r269, %r261, %r263;
	mov.b32 	%f51, %r270;
	add.f32 	%f52, %f50, %f51;
	mov.b32 	%r271, %f52;
	mov.u32 	%r272, 2;
	shfl.sync.bfly.b32 	%r273|%p27, %r271, %r272, %r261, %r263;
	mov.b32 	%f53, %r273;
	add.f32 	%f54, %f52, %f53;
	mov.b32 	%r274, %f54;
	mov.u32 	%r275, 1;
	shfl.sync.bfly.b32 	%r276|%p28, %r274, %r275, %r261, %r263;
	mov.b32 	%f55, %r276;
	add.f32 	%f56, %f54, %f55;
	st.local.f32 	[%rd3+4], %f56;

$L__BB123_13:
	bar.sync 	0;
	mov.b32 	%r277, %f2;
	mov.u32 	%r278, 31;
	mov.u32 	%r279, 16;
	mov.u32 	%r280, -1;
	shfl.sync.bfly.b32 	%r281|%p30, %r277, %r279, %r278, %r280;
	mov.b32 	%f57, %r281;
	add.f32 	%f58, %f2, %f57;
	mov.b32 	%r282, %f58;
	mov.u32 	%r283, 8;
	shfl.sync.bfly.b32 	%r284|%p31, %r282, %r283, %r278, %r280;
	mov.b32 	%f59, %r284;
	add.f32 	%f60, %f58, %f59;
	mov.b32 	%r285, %f60;
	mov.u32 	%r286, 4;
	shfl.sync.bfly.b32 	%r287|%p32, %r285, %r286, %r278, %r280;
	mov.b32 	%f61, %r287;
	add.f32 	%f62, %f60, %f61;
	mov.b32 	%r288, %f62;
	mov.u32 	%r289, 2;
	shfl.sync.bfly.b32 	%r290|%p33, %r288, %r289, %r278, %r280;
	mov.b32 	%f63, %r290;
	add.f32 	%f64, %f62, %f63;
	mov.b32 	%r291, %f64;
	mov.u32 	%r292, 1;
	shfl.sync.bfly.b32 	%r293|%p34, %r291, %r292, %r278, %r280;
	mov.b32 	%f65, %r293;
	add.f32 	%f66, %f64, %f65;
	st.local.f32 	[%rd3+8], %f66;
	st.shared.f32 	[%r39], %f66;
	bar.sync 	0;
	@%p1 bra 	$L__BB123_15;

	ld.shared.f32 	%f67, [%r4];
	mov.b32 	%r294, %f67;
	shfl.sync.bfly.b32 	%r298|%p35, %r294, %r279, %r278, %r280;
	mov.b32 	%f68, %r298;
	add.f32 	%f69, %f67, %f68;
	mov.b32 	%r299, %f69;
	shfl.sync.bfly.b32 	%r301|%p36, %r299, %r283, %r278, %r280;
	mov.b32 	%f70, %r301;
	add.f32 	%f71, %f69, %f70;
	mov.b32 	%r302, %f71;
	shfl.sync.bfly.b32 	%r304|%p37, %r302, %r286, %r278, %r280;
	mov.b32 	%f72, %r304;
	add.f32 	%f73, %f71, %f72;
	mov.b32 	%r305, %f73;
	shfl.sync.bfly.b32 	%r307|%p38, %r305, %r289, %r278, %r280;
	mov.b32 	%f74, %r307;
	add.f32 	%f75, %f73, %f74;
	mov.b32 	%r308, %f75;
	shfl.sync.bfly.b32 	%r310|%p39, %r308, %r292, %r278, %r280;
	mov.b32 	%f76, %r310;
	add.f32 	%f77, %f75, %f76;
	st.local.f32 	[%rd3+8], %f77;

$L__BB123_15:
	bar.sync 	0;
	mov.b32 	%r311, %f3;
	shfl.sync.bfly.b32 	%r315|%p41, %r311, %r279, %r278, %r280;
	mov.b32 	%f78, %r315;
	add.f32 	%f79, %f3, %f78;
	mov.b32 	%r316, %f79;
	shfl.sync.bfly.b32 	%r318|%p42, %r316, %r283, %r278, %r280;
	mov.b32 	%f80, %r318;
	add.f32 	%f81, %f79, %f80;
	mov.b32 	%r319, %f81;
	shfl.sync.bfly.b32 	%r321|%p43, %r319, %r286, %r278, %r280;
	mov.b32 	%f82, %r321;
	add.f32 	%f83, %f81, %f82;
	mov.b32 	%r322, %f83;
	shfl.sync.bfly.b32 	%r324|%p44, %r322, %r289, %r278, %r280;
	mov.b32 	%f84, %r324;
	add.f32 	%f85, %f83, %f84;
	mov.b32 	%r325, %f85;
	shfl.sync.bfly.b32 	%r327|%p45, %r325, %r292, %r278, %r280;
	mov.b32 	%f86, %r327;
	add.f32 	%f87, %f85, %f86;
	st.local.f32 	[%rd3+12], %f87;
	st.shared.f32 	[%r39], %f87;
	bar.sync 	0;
	@%p1 bra 	$L__BB123_17;

	ld.shared.f32 	%f88, [%r4];
	mov.b32 	%r328, %f88;
	mov.u32 	%r329, 31;
	mov.u32 	%r330, 16;
	mov.u32 	%r331, -1;
	shfl.sync.bfly.b32 	%r332|%p46, %r328, %r330, %r329, %r331;
	mov.b32 	%f89, %r332;
	add.f32 	%f90, %f88, %f89;
	mov.b32 	%r333, %f90;
	mov.u32 	%r334, 8;
	shfl.sync.bfly.b32 	%r335|%p47, %r333, %r334, %r329, %r331;
	mov.b32 	%f91, %r335;
	add.f32 	%f92, %f90, %f91;
	mov.b32 	%r336, %f92;
	mov.u32 	%r337, 4;
	shfl.sync.bfly.b32 	%r338|%p48, %r336, %r337, %r329, %r331;
	mov.b32 	%f93, %r338;
	add.f32 	%f94, %f92, %f93;
	mov.b32 	%r339, %f94;
	mov.u32 	%r340, 2;
	shfl.sync.bfly.b32 	%r341|%p49, %r339, %r340, %r329, %r331;
	mov.b32 	%f95, %r341;
	add.f32 	%f96, %f94, %f95;
	mov.b32 	%r342, %f96;
	mov.u32 	%r343, 1;
	shfl.sync.bfly.b32 	%r344|%p50, %r342, %r343, %r329, %r331;
	mov.b32 	%f97, %r344;
	add.f32 	%f98, %f96, %f97;
	st.local.f32 	[%rd3+12], %f98;

$L__BB123_17:
	bar.sync 	0;
	setp.gt.s32 	%p51, %r3, 3;
	@%p51 bra 	$L__BB123_19;

	mad.lo.s32 	%r345, %r3, %r42, %r2;
	cvt.s64.s32 	%rd68, %r345;
	mul.lo.s32 	%r346, %r1, %r43;
	cvt.s64.s32 	%rd69, %r346;
	add.s64 	%rd70, %rd69, %rd68;
	mul.wide.s32 	%rd71, %r3, 4;
	add.s64 	%rd72, %rd3, %rd71;
	ld.local.f32 	%f99, [%rd72];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs3, %f99;}

	// end inline asm
	cvta.to.global.u64 	%rd73, %rd33;
	shl.b64 	%rd74, %rd70, 1;
	add.s64 	%rd75, %rd73, %rd74;
	st.global.u16 	[%rd75], %rs3;

$L__BB123_19:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_5_bs_256
.visible .entry ggml_matvec_f16_ncols_5_bs_256(
	.param .u64 ggml_matvec_f16_ncols_5_bs_256_param_0,
	.param .u64 ggml_matvec_f16_ncols_5_bs_256_param_1,
	.param .u64 ggml_matvec_f16_ncols_5_bs_256_param_2,
	.param .u32 ggml_matvec_f16_ncols_5_bs_256_param_3,
	.param .u32 ggml_matvec_f16_ncols_5_bs_256_param_4,
	.param .u32 ggml_matvec_f16_ncols_5_bs_256_param_5,
	.param .u32 ggml_matvec_f16_ncols_5_bs_256_param_6,
	.param .u32 ggml_matvec_f16_ncols_5_bs_256_param_7,
	.param .u32 ggml_matvec_f16_ncols_5_bs_256_param_8,
	.param .u32 ggml_matvec_f16_ncols_5_bs_256_param_9,
	.param .u32 ggml_matvec_f16_ncols_5_bs_256_param_10,
	.param .u32 ggml_matvec_f16_ncols_5_bs_256_param_11
)
{
	.local .align 4 .b8 	__local_depot124[20];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<63>;
	.reg .b16 	%rs<4>;
	.reg .f32 	%f<124>;
	.reg .b32 	%r<447>;
	.reg .b64 	%rd<74>;


	mov.u64 	%SPL, __local_depot124;
	ld.param.u64 	%rd28, [ggml_matvec_f16_ncols_5_bs_256_param_0];
	ld.param.u64 	%rd29, [ggml_matvec_f16_ncols_5_bs_256_param_1];
	ld.param.u64 	%rd27, [ggml_matvec_f16_ncols_5_bs_256_param_2];
	ld.param.u32 	%r46, [ggml_matvec_f16_ncols_5_bs_256_param_3];
	ld.param.u32 	%r50, [ggml_matvec_f16_ncols_5_bs_256_param_5];
	ld.param.u32 	%r47, [ggml_matvec_f16_ncols_5_bs_256_param_6];
	ld.param.u32 	%r48, [ggml_matvec_f16_ncols_5_bs_256_param_7];
	ld.param.u32 	%r51, [ggml_matvec_f16_ncols_5_bs_256_param_8];
	ld.param.u32 	%r52, [ggml_matvec_f16_ncols_5_bs_256_param_9];
	ld.param.u32 	%r53, [ggml_matvec_f16_ncols_5_bs_256_param_10];
	ld.param.u32 	%r49, [ggml_matvec_f16_ncols_5_bs_256_param_11];
	cvta.to.global.u64 	%rd73, %rd29;
	cvta.to.global.u64 	%rd2, %rd28;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r54, %r1, %r51;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r55, %r2, %r50;
	mad.lo.s32 	%r56, %r54, %r52, %r55;
	cvt.s64.s32 	%rd4, %r56;
	mul.lo.s32 	%r57, %r1, %r53;
	cvt.s64.s32 	%rd5, %r57;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r58, %r3, 2;
	mov.u32 	%r59, data_mmv;
	add.s32 	%r4, %r59, %r58;
	@%p1 bra 	$L__BB124_2;

	mov.u32 	%r60, 0;
	st.shared.u32 	[%r4], %r60;

$L__BB124_2:
	bar.sync 	0;
	mov.f32 	%f6, 0f00000000;
	mov.u32 	%r442, 0;
	st.local.u32 	[%rd3], %r442;
	st.local.u32 	[%rd3+4], %r442;
	st.local.u32 	[%rd3+8], %r442;
	st.local.u32 	[%rd3+12], %r442;
	st.local.u32 	[%rd3+16], %r442;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs2, %f6;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f6;}

	// end inline asm
	mov.b32 	%r446, {%rs1, %rs2};
	setp.ge.s32 	%p2, %r3, %r46;
	mov.u32 	%r443, %r442;
	mov.u32 	%r444, %r442;
	mov.u32 	%r445, %r442;
	@%p2 bra 	$L__BB124_9;

	not.b32 	%r69, %r3;
	add.s32 	%r6, %r69, %r46;
	shr.u32 	%r70, %r6, 8;
	add.s32 	%r71, %r70, 1;
	and.b32  	%r429, %r71, 3;
	setp.eq.s32 	%p3, %r429, 0;
	mov.u32 	%r442, 0;
	mov.u32 	%r435, %r3;
	@%p3 bra 	$L__BB124_6;

	shl.b32 	%r76, %r47, 1;
	mad.lo.s32 	%r77, %r47, 3, %r3;
	mul.wide.s32 	%rd31, %r77, 4;
	shl.b64 	%rd32, %rd5, 1;
	add.s64 	%rd7, %rd31, %rd32;
	mul.wide.s32 	%rd33, %r3, 4;
	mul.wide.s32 	%rd34, %r47, 4;
	add.s64 	%rd35, %rd33, %rd34;
	add.s64 	%rd8, %rd35, %rd32;
	add.s64 	%rd9, %rd33, %rd32;
	mul.wide.s32 	%rd36, %r3, 2;
	add.s64 	%rd37, %rd36, %rd4;
	shl.b64 	%rd38, %rd37, 1;
	add.s64 	%rd70, %rd2, %rd38;
	mul.wide.s32 	%rd11, %r76, 4;
	mov.u32 	%r442, 0;
	mov.u64 	%rd71, %rd73;
	mov.u32 	%r443, %r442;
	mov.u32 	%r444, %r442;
	mov.u32 	%r445, %r442;
	mov.u32 	%r435, %r3;

$L__BB124_5:
	.pragma "nounroll";
	ld.global.nc.u32 	%r79, [%rd70];
	add.s64 	%rd39, %rd71, %rd9;
	ld.global.nc.u32 	%r80, [%rd39];
	// begin inline asm
	{mul.f16x2 %r78,%r79,%r80;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r446,%r446,%r78;
}
	// end inline asm
	add.s64 	%rd40, %rd71, %rd8;
	ld.global.nc.u32 	%r86, [%rd40];
	// begin inline asm
	{mul.f16x2 %r84,%r79,%r86;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r445,%r445,%r84;
}
	// end inline asm
	add.s64 	%rd41, %rd39, %rd11;
	ld.global.nc.u32 	%r92, [%rd41];
	// begin inline asm
	{mul.f16x2 %r90,%r79,%r92;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r444,%r444,%r90;
}
	// end inline asm
	add.s64 	%rd42, %rd71, %rd7;
	ld.global.nc.u32 	%r98, [%rd42];
	// begin inline asm
	{mul.f16x2 %r96,%r79,%r98;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r443,%r443,%r96;
}
	// end inline asm
	add.s64 	%rd43, %rd41, %rd11;
	ld.global.nc.u32 	%r104, [%rd43];
	// begin inline asm
	{mul.f16x2 %r102,%r79,%r104;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r442,%r442,%r102;
}
	// end inline asm
	add.s32 	%r435, %r435, 256;
	add.s64 	%rd71, %rd71, 1024;
	add.s64 	%rd70, %rd70, 1024;
	add.s32 	%r429, %r429, -1;
	setp.ne.s32 	%p4, %r429, 0;
	@%p4 bra 	$L__BB124_5;

$L__BB124_6:
	setp.lt.u32 	%p5, %r6, 768;
	@%p5 bra 	$L__BB124_9;

	add.s32 	%r108, %r435, %r47;
	shl.b32 	%r109, %r47, 1;
	add.s32 	%r110, %r435, %r109;
	mad.lo.s32 	%r111, %r47, 3, %r435;
	shl.b32 	%r112, %r47, 2;
	add.s32 	%r113, %r435, %r112;
	add.s32 	%r114, %r108, 256;
	mul.wide.s32 	%rd44, %r114, 4;
	shl.b64 	%rd45, %rd5, 1;
	add.s64 	%rd16, %rd44, %rd45;
	mul.wide.s32 	%rd46, %r110, 4;
	add.s64 	%rd17, %rd46, %rd45;
	mul.wide.s32 	%rd47, %r111, 4;
	add.s64 	%rd18, %rd47, %rd45;
	mul.wide.s32 	%rd48, %r113, 4;
	add.s64 	%rd19, %rd48, %rd45;
	mul.wide.s32 	%rd49, %r435, 2;
	add.s64 	%rd50, %rd49, %rd4;
	shl.b64 	%rd51, %rd50, 1;
	add.s64 	%rd52, %rd2, %rd51;
	add.s64 	%rd72, %rd52, 2048;
	mul.wide.s32 	%rd53, %r435, 4;
	add.s64 	%rd21, %rd53, %rd45;
	mul.wide.s32 	%rd54, %r47, 4;
	add.s64 	%rd55, %rd53, %rd54;
	add.s64 	%rd22, %rd55, %rd45;

$L__BB124_8:
	ld.global.nc.u32 	%r116, [%rd72+-2048];
	add.s64 	%rd56, %rd73, %rd21;
	ld.global.nc.u32 	%r117, [%rd56];
	// begin inline asm
	{mul.f16x2 %r115,%r116,%r117;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r118,%r446,%r115;
}
	// end inline asm
	add.s64 	%rd57, %rd73, %rd22;
	ld.global.nc.u32 	%r123, [%rd57];
	// begin inline asm
	{mul.f16x2 %r121,%r116,%r123;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r124,%r445,%r121;
}
	// end inline asm
	add.s64 	%rd58, %rd73, %rd17;
	ld.global.nc.u32 	%r129, [%rd58];
	// begin inline asm
	{mul.f16x2 %r127,%r116,%r129;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r130,%r444,%r127;
}
	// end inline asm
	add.s64 	%rd59, %rd73, %rd18;
	ld.global.nc.u32 	%r135, [%rd59];
	// begin inline asm
	{mul.f16x2 %r133,%r116,%r135;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r136,%r443,%r133;
}
	// end inline asm
	add.s64 	%rd60, %rd73, %rd19;
	ld.global.nc.u32 	%r141, [%rd60];
	// begin inline asm
	{mul.f16x2 %r139,%r116,%r141;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r142,%r442,%r139;
}
	// end inline asm
	ld.global.nc.u32 	%r146, [%rd72+-1024];
	ld.global.nc.u32 	%r147, [%rd56+1024];
	// begin inline asm
	{mul.f16x2 %r145,%r146,%r147;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r148,%r118,%r145;
}
	// end inline asm
	add.s64 	%rd61, %rd73, %rd16;
	ld.global.nc.u32 	%r153, [%rd61];
	// begin inline asm
	{mul.f16x2 %r151,%r146,%r153;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r154,%r124,%r151;
}
	// end inline asm
	ld.global.nc.u32 	%r159, [%rd58+1024];
	// begin inline asm
	{mul.f16x2 %r157,%r146,%r159;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r160,%r130,%r157;
}
	// end inline asm
	ld.global.nc.u32 	%r165, [%rd59+1024];
	// begin inline asm
	{mul.f16x2 %r163,%r146,%r165;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r166,%r136,%r163;
}
	// end inline asm
	ld.global.nc.u32 	%r171, [%rd60+1024];
	// begin inline asm
	{mul.f16x2 %r169,%r146,%r171;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r172,%r142,%r169;
}
	// end inline asm
	ld.global.nc.u32 	%r176, [%rd72];
	ld.global.nc.u32 	%r177, [%rd56+2048];
	// begin inline asm
	{mul.f16x2 %r175,%r176,%r177;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r178,%r148,%r175;
}
	// end inline asm
	ld.global.nc.u32 	%r183, [%rd61+1024];
	// begin inline asm
	{mul.f16x2 %r181,%r176,%r183;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r184,%r154,%r181;
}
	// end inline asm
	ld.global.nc.u32 	%r189, [%rd58+2048];
	// begin inline asm
	{mul.f16x2 %r187,%r176,%r189;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r190,%r160,%r187;
}
	// end inline asm
	ld.global.nc.u32 	%r195, [%rd59+2048];
	// begin inline asm
	{mul.f16x2 %r193,%r176,%r195;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r196,%r166,%r193;
}
	// end inline asm
	ld.global.nc.u32 	%r201, [%rd60+2048];
	// begin inline asm
	{mul.f16x2 %r199,%r176,%r201;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r202,%r172,%r199;
}
	// end inline asm
	ld.global.nc.u32 	%r206, [%rd72+1024];
	ld.global.nc.u32 	%r207, [%rd56+3072];
	// begin inline asm
	{mul.f16x2 %r205,%r206,%r207;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r446,%r178,%r205;
}
	// end inline asm
	ld.global.nc.u32 	%r213, [%rd61+2048];
	// begin inline asm
	{mul.f16x2 %r211,%r206,%r213;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r445,%r184,%r211;
}
	// end inline asm
	ld.global.nc.u32 	%r219, [%rd58+3072];
	// begin inline asm
	{mul.f16x2 %r217,%r206,%r219;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r444,%r190,%r217;
}
	// end inline asm
	ld.global.nc.u32 	%r225, [%rd59+3072];
	// begin inline asm
	{mul.f16x2 %r223,%r206,%r225;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r443,%r196,%r223;
}
	// end inline asm
	ld.global.nc.u32 	%r231, [%rd60+3072];
	// begin inline asm
	{mul.f16x2 %r229,%r206,%r231;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r442,%r202,%r229;
}
	// end inline asm
	add.s64 	%rd73, %rd73, 4096;
	add.s64 	%rd72, %rd72, 4096;
	add.s32 	%r435, %r435, 1024;
	setp.lt.s32 	%p6, %r435, %r46;
	@%p6 bra 	$L__BB124_8;

$L__BB124_9:
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r446;
  cvt.f32.f16 %f7, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r446;
  cvt.f32.f16 %f8, high;}

	// end inline asm
	add.f32 	%f17, %f7, %f8;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r445;
  cvt.f32.f16 %f9, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r445;
  cvt.f32.f16 %f10, high;}

	// end inline asm
	add.f32 	%f1, %f9, %f10;
	st.local.f32 	[%rd3+4], %f1;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r444;
  cvt.f32.f16 %f11, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r444;
  cvt.f32.f16 %f12, high;}

	// end inline asm
	add.f32 	%f2, %f11, %f12;
	st.local.f32 	[%rd3+8], %f2;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r443;
  cvt.f32.f16 %f13, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r443;
  cvt.f32.f16 %f14, high;}

	// end inline asm
	add.f32 	%f3, %f13, %f14;
	st.local.f32 	[%rd3+12], %f3;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r442;
  cvt.f32.f16 %f15, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r442;
  cvt.f32.f16 %f16, high;}

	// end inline asm
	add.f32 	%f4, %f15, %f16;
	st.local.f32 	[%rd3+16], %f4;
	shr.s32 	%r245, %r3, 31;
	shr.u32 	%r246, %r245, 27;
	add.s32 	%r247, %r3, %r246;
	shr.s32 	%r248, %r247, 5;
	shl.b32 	%r249, %r248, 2;
	add.s32 	%r45, %r59, %r249;
	mov.u32 	%r251, 2;
	mov.b32 	%r252, %f17;
	mov.u32 	%r253, 31;
	mov.u32 	%r254, 16;
	mov.u32 	%r255, -1;
	shfl.sync.bfly.b32 	%r256|%p7, %r252, %r254, %r253, %r255;
	mov.b32 	%f18, %r256;
	add.f32 	%f19, %f17, %f18;
	mov.b32 	%r257, %f19;
	mov.u32 	%r258, 8;
	shfl.sync.bfly.b32 	%r259|%p8, %r257, %r258, %r253, %r255;
	mov.b32 	%f20, %r259;
	add.f32 	%f21, %f19, %f20;
	mov.b32 	%r260, %f21;
	mov.u32 	%r261, 4;
	shfl.sync.bfly.b32 	%r262|%p9, %r260, %r261, %r253, %r255;
	mov.b32 	%f22, %r262;
	add.f32 	%f23, %f21, %f22;
	mov.b32 	%r263, %f23;
	shfl.sync.bfly.b32 	%r264|%p10, %r263, %r251, %r253, %r255;
	mov.b32 	%f24, %r264;
	add.f32 	%f25, %f23, %f24;
	mov.b32 	%r265, %f25;
	mov.u32 	%r266, 1;
	shfl.sync.bfly.b32 	%r267|%p11, %r265, %r266, %r253, %r255;
	mov.b32 	%f26, %r267;
	add.f32 	%f27, %f25, %f26;
	st.local.f32 	[%rd3], %f27;
	st.shared.f32 	[%r45], %f27;
	bar.sync 	0;
	@%p1 bra 	$L__BB124_11;

	ld.shared.f32 	%f28, [%r4];
	mov.b32 	%r268, %f28;
	shfl.sync.bfly.b32 	%r272|%p13, %r268, %r254, %r253, %r255;
	mov.b32 	%f29, %r272;
	add.f32 	%f30, %f28, %f29;
	mov.b32 	%r273, %f30;
	shfl.sync.bfly.b32 	%r275|%p14, %r273, %r258, %r253, %r255;
	mov.b32 	%f31, %r275;
	add.f32 	%f32, %f30, %f31;
	mov.b32 	%r276, %f32;
	shfl.sync.bfly.b32 	%r278|%p15, %r276, %r261, %r253, %r255;
	mov.b32 	%f33, %r278;
	add.f32 	%f34, %f32, %f33;
	mov.b32 	%r279, %f34;
	shfl.sync.bfly.b32 	%r281|%p16, %r279, %r251, %r253, %r255;
	mov.b32 	%f35, %r281;
	add.f32 	%f36, %f34, %f35;
	mov.b32 	%r282, %f36;
	shfl.sync.bfly.b32 	%r284|%p17, %r282, %r266, %r253, %r255;
	mov.b32 	%f37, %r284;
	add.f32 	%f38, %f36, %f37;
	st.local.f32 	[%rd3], %f38;

$L__BB124_11:
	bar.sync 	0;
	mov.b32 	%r285, %f1;
	shfl.sync.bfly.b32 	%r289|%p19, %r285, %r254, %r253, %r255;
	mov.b32 	%f39, %r289;
	add.f32 	%f40, %f1, %f39;
	mov.b32 	%r290, %f40;
	shfl.sync.bfly.b32 	%r292|%p20, %r290, %r258, %r253, %r255;
	mov.b32 	%f41, %r292;
	add.f32 	%f42, %f40, %f41;
	mov.b32 	%r293, %f42;
	shfl.sync.bfly.b32 	%r295|%p21, %r293, %r261, %r253, %r255;
	mov.b32 	%f43, %r295;
	add.f32 	%f44, %f42, %f43;
	mov.b32 	%r296, %f44;
	shfl.sync.bfly.b32 	%r298|%p22, %r296, %r251, %r253, %r255;
	mov.b32 	%f45, %r298;
	add.f32 	%f46, %f44, %f45;
	mov.b32 	%r299, %f46;
	shfl.sync.bfly.b32 	%r301|%p23, %r299, %r266, %r253, %r255;
	mov.b32 	%f47, %r301;
	add.f32 	%f48, %f46, %f47;
	st.local.f32 	[%rd3+4], %f48;
	st.shared.f32 	[%r45], %f48;
	bar.sync 	0;
	@%p1 bra 	$L__BB124_13;

	ld.shared.f32 	%f49, [%r4];
	mov.b32 	%r302, %f49;
	mov.u32 	%r303, 31;
	mov.u32 	%r304, 16;
	mov.u32 	%r305, -1;
	shfl.sync.bfly.b32 	%r306|%p24, %r302, %r304, %r303, %r305;
	mov.b32 	%f50, %r306;
	add.f32 	%f51, %f49, %f50;
	mov.b32 	%r307, %f51;
	mov.u32 	%r308, 8;
	shfl.sync.bfly.b32 	%r309|%p25, %r307, %r308, %r303, %r305;
	mov.b32 	%f52, %r309;
	add.f32 	%f53, %f51, %f52;
	mov.b32 	%r310, %f53;
	mov.u32 	%r311, 4;
	shfl.sync.bfly.b32 	%r312|%p26, %r310, %r311, %r303, %r305;
	mov.b32 	%f54, %r312;
	add.f32 	%f55, %f53, %f54;
	mov.b32 	%r313, %f55;
	mov.u32 	%r314, 2;
	shfl.sync.bfly.b32 	%r315|%p27, %r313, %r314, %r303, %r305;
	mov.b32 	%f56, %r315;
	add.f32 	%f57, %f55, %f56;
	mov.b32 	%r316, %f57;
	mov.u32 	%r317, 1;
	shfl.sync.bfly.b32 	%r318|%p28, %r316, %r317, %r303, %r305;
	mov.b32 	%f58, %r318;
	add.f32 	%f59, %f57, %f58;
	st.local.f32 	[%rd3+4], %f59;

$L__BB124_13:
	bar.sync 	0;
	mov.b32 	%r319, %f2;
	mov.u32 	%r320, 31;
	mov.u32 	%r321, 16;
	mov.u32 	%r322, -1;
	shfl.sync.bfly.b32 	%r323|%p30, %r319, %r321, %r320, %r322;
	mov.b32 	%f60, %r323;
	add.f32 	%f61, %f2, %f60;
	mov.b32 	%r324, %f61;
	mov.u32 	%r325, 8;
	shfl.sync.bfly.b32 	%r326|%p31, %r324, %r325, %r320, %r322;
	mov.b32 	%f62, %r326;
	add.f32 	%f63, %f61, %f62;
	mov.b32 	%r327, %f63;
	mov.u32 	%r328, 4;
	shfl.sync.bfly.b32 	%r329|%p32, %r327, %r328, %r320, %r322;
	mov.b32 	%f64, %r329;
	add.f32 	%f65, %f63, %f64;
	mov.b32 	%r330, %f65;
	mov.u32 	%r331, 2;
	shfl.sync.bfly.b32 	%r332|%p33, %r330, %r331, %r320, %r322;
	mov.b32 	%f66, %r332;
	add.f32 	%f67, %f65, %f66;
	mov.b32 	%r333, %f67;
	mov.u32 	%r334, 1;
	shfl.sync.bfly.b32 	%r335|%p34, %r333, %r334, %r320, %r322;
	mov.b32 	%f68, %r335;
	add.f32 	%f69, %f67, %f68;
	st.local.f32 	[%rd3+8], %f69;
	st.shared.f32 	[%r45], %f69;
	bar.sync 	0;
	@%p1 bra 	$L__BB124_15;

	ld.shared.f32 	%f70, [%r4];
	mov.b32 	%r336, %f70;
	shfl.sync.bfly.b32 	%r340|%p35, %r336, %r321, %r320, %r322;
	mov.b32 	%f71, %r340;
	add.f32 	%f72, %f70, %f71;
	mov.b32 	%r341, %f72;
	shfl.sync.bfly.b32 	%r343|%p36, %r341, %r325, %r320, %r322;
	mov.b32 	%f73, %r343;
	add.f32 	%f74, %f72, %f73;
	mov.b32 	%r344, %f74;
	shfl.sync.bfly.b32 	%r346|%p37, %r344, %r328, %r320, %r322;
	mov.b32 	%f75, %r346;
	add.f32 	%f76, %f74, %f75;
	mov.b32 	%r347, %f76;
	shfl.sync.bfly.b32 	%r349|%p38, %r347, %r331, %r320, %r322;
	mov.b32 	%f77, %r349;
	add.f32 	%f78, %f76, %f77;
	mov.b32 	%r350, %f78;
	shfl.sync.bfly.b32 	%r352|%p39, %r350, %r334, %r320, %r322;
	mov.b32 	%f79, %r352;
	add.f32 	%f80, %f78, %f79;
	st.local.f32 	[%rd3+8], %f80;

$L__BB124_15:
	bar.sync 	0;
	mov.b32 	%r353, %f3;
	shfl.sync.bfly.b32 	%r357|%p41, %r353, %r321, %r320, %r322;
	mov.b32 	%f81, %r357;
	add.f32 	%f82, %f3, %f81;
	mov.b32 	%r358, %f82;
	shfl.sync.bfly.b32 	%r360|%p42, %r358, %r325, %r320, %r322;
	mov.b32 	%f83, %r360;
	add.f32 	%f84, %f82, %f83;
	mov.b32 	%r361, %f84;
	shfl.sync.bfly.b32 	%r363|%p43, %r361, %r328, %r320, %r322;
	mov.b32 	%f85, %r363;
	add.f32 	%f86, %f84, %f85;
	mov.b32 	%r364, %f86;
	shfl.sync.bfly.b32 	%r366|%p44, %r364, %r331, %r320, %r322;
	mov.b32 	%f87, %r366;
	add.f32 	%f88, %f86, %f87;
	mov.b32 	%r367, %f88;
	shfl.sync.bfly.b32 	%r369|%p45, %r367, %r334, %r320, %r322;
	mov.b32 	%f89, %r369;
	add.f32 	%f90, %f88, %f89;
	st.local.f32 	[%rd3+12], %f90;
	st.shared.f32 	[%r45], %f90;
	bar.sync 	0;
	@%p1 bra 	$L__BB124_17;

	ld.shared.f32 	%f91, [%r4];
	mov.b32 	%r370, %f91;
	mov.u32 	%r371, 31;
	mov.u32 	%r372, 16;
	mov.u32 	%r373, -1;
	shfl.sync.bfly.b32 	%r374|%p46, %r370, %r372, %r371, %r373;
	mov.b32 	%f92, %r374;
	add.f32 	%f93, %f91, %f92;
	mov.b32 	%r375, %f93;
	mov.u32 	%r376, 8;
	shfl.sync.bfly.b32 	%r377|%p47, %r375, %r376, %r371, %r373;
	mov.b32 	%f94, %r377;
	add.f32 	%f95, %f93, %f94;
	mov.b32 	%r378, %f95;
	mov.u32 	%r379, 4;
	shfl.sync.bfly.b32 	%r380|%p48, %r378, %r379, %r371, %r373;
	mov.b32 	%f96, %r380;
	add.f32 	%f97, %f95, %f96;
	mov.b32 	%r381, %f97;
	mov.u32 	%r382, 2;
	shfl.sync.bfly.b32 	%r383|%p49, %r381, %r382, %r371, %r373;
	mov.b32 	%f98, %r383;
	add.f32 	%f99, %f97, %f98;
	mov.b32 	%r384, %f99;
	mov.u32 	%r385, 1;
	shfl.sync.bfly.b32 	%r386|%p50, %r384, %r385, %r371, %r373;
	mov.b32 	%f100, %r386;
	add.f32 	%f101, %f99, %f100;
	st.local.f32 	[%rd3+12], %f101;

$L__BB124_17:
	bar.sync 	0;
	mov.b32 	%r387, %f4;
	mov.u32 	%r388, 31;
	mov.u32 	%r389, 16;
	mov.u32 	%r390, -1;
	shfl.sync.bfly.b32 	%r391|%p52, %r387, %r389, %r388, %r390;
	mov.b32 	%f102, %r391;
	add.f32 	%f103, %f4, %f102;
	mov.b32 	%r392, %f103;
	mov.u32 	%r393, 8;
	shfl.sync.bfly.b32 	%r394|%p53, %r392, %r393, %r388, %r390;
	mov.b32 	%f104, %r394;
	add.f32 	%f105, %f103, %f104;
	mov.b32 	%r395, %f105;
	mov.u32 	%r396, 4;
	shfl.sync.bfly.b32 	%r397|%p54, %r395, %r396, %r388, %r390;
	mov.b32 	%f106, %r397;
	add.f32 	%f107, %f105, %f106;
	mov.b32 	%r398, %f107;
	mov.u32 	%r399, 2;
	shfl.sync.bfly.b32 	%r400|%p55, %r398, %r399, %r388, %r390;
	mov.b32 	%f108, %r400;
	add.f32 	%f109, %f107, %f108;
	mov.b32 	%r401, %f109;
	mov.u32 	%r402, 1;
	shfl.sync.bfly.b32 	%r403|%p56, %r401, %r402, %r388, %r390;
	mov.b32 	%f110, %r403;
	add.f32 	%f111, %f109, %f110;
	st.local.f32 	[%rd3+16], %f111;
	st.shared.f32 	[%r45], %f111;
	bar.sync 	0;
	@%p1 bra 	$L__BB124_19;

	ld.shared.f32 	%f112, [%r4];
	mov.b32 	%r404, %f112;
	shfl.sync.bfly.b32 	%r408|%p57, %r404, %r389, %r388, %r390;
	mov.b32 	%f113, %r408;
	add.f32 	%f114, %f112, %f113;
	mov.b32 	%r409, %f114;
	shfl.sync.bfly.b32 	%r411|%p58, %r409, %r393, %r388, %r390;
	mov.b32 	%f115, %r411;
	add.f32 	%f116, %f114, %f115;
	mov.b32 	%r412, %f116;
	shfl.sync.bfly.b32 	%r414|%p59, %r412, %r396, %r388, %r390;
	mov.b32 	%f117, %r414;
	add.f32 	%f118, %f116, %f117;
	mov.b32 	%r415, %f118;
	shfl.sync.bfly.b32 	%r417|%p60, %r415, %r399, %r388, %r390;
	mov.b32 	%f119, %r417;
	add.f32 	%f120, %f118, %f119;
	mov.b32 	%r418, %f120;
	shfl.sync.bfly.b32 	%r420|%p61, %r418, %r402, %r388, %r390;
	mov.b32 	%f121, %r420;
	add.f32 	%f122, %f120, %f121;
	st.local.f32 	[%rd3+16], %f122;

$L__BB124_19:
	bar.sync 	0;
	setp.gt.s32 	%p62, %r3, 4;
	@%p62 bra 	$L__BB124_21;

	mad.lo.s32 	%r421, %r3, %r48, %r2;
	cvt.s64.s32 	%rd62, %r421;
	mul.lo.s32 	%r422, %r1, %r49;
	cvt.s64.s32 	%rd63, %r422;
	add.s64 	%rd64, %rd63, %rd62;
	mul.wide.s32 	%rd65, %r3, 4;
	add.s64 	%rd66, %rd3, %rd65;
	ld.local.f32 	%f123, [%rd66];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs3, %f123;}

	// end inline asm
	cvta.to.global.u64 	%rd67, %rd27;
	shl.b64 	%rd68, %rd64, 1;
	add.s64 	%rd69, %rd67, %rd68;
	st.global.u16 	[%rd69], %rs3;

$L__BB124_21:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_6_bs_256
.visible .entry ggml_matvec_f16_ncols_6_bs_256(
	.param .u64 ggml_matvec_f16_ncols_6_bs_256_param_0,
	.param .u64 ggml_matvec_f16_ncols_6_bs_256_param_1,
	.param .u64 ggml_matvec_f16_ncols_6_bs_256_param_2,
	.param .u32 ggml_matvec_f16_ncols_6_bs_256_param_3,
	.param .u32 ggml_matvec_f16_ncols_6_bs_256_param_4,
	.param .u32 ggml_matvec_f16_ncols_6_bs_256_param_5,
	.param .u32 ggml_matvec_f16_ncols_6_bs_256_param_6,
	.param .u32 ggml_matvec_f16_ncols_6_bs_256_param_7,
	.param .u32 ggml_matvec_f16_ncols_6_bs_256_param_8,
	.param .u32 ggml_matvec_f16_ncols_6_bs_256_param_9,
	.param .u32 ggml_matvec_f16_ncols_6_bs_256_param_10,
	.param .u32 ggml_matvec_f16_ncols_6_bs_256_param_11
)
{
	.local .align 8 .b8 	__local_depot125[24];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<74>;
	.reg .b16 	%rs<4>;
	.reg .f32 	%f<148>;
	.reg .b32 	%r<527>;
	.reg .b64 	%rd<77>;


	mov.u64 	%SPL, __local_depot125;
	ld.param.u64 	%rd29, [ggml_matvec_f16_ncols_6_bs_256_param_0];
	ld.param.u64 	%rd30, [ggml_matvec_f16_ncols_6_bs_256_param_1];
	ld.param.u64 	%rd28, [ggml_matvec_f16_ncols_6_bs_256_param_2];
	ld.param.u32 	%r52, [ggml_matvec_f16_ncols_6_bs_256_param_3];
	ld.param.u32 	%r56, [ggml_matvec_f16_ncols_6_bs_256_param_5];
	ld.param.u32 	%r53, [ggml_matvec_f16_ncols_6_bs_256_param_6];
	ld.param.u32 	%r54, [ggml_matvec_f16_ncols_6_bs_256_param_7];
	ld.param.u32 	%r57, [ggml_matvec_f16_ncols_6_bs_256_param_8];
	ld.param.u32 	%r58, [ggml_matvec_f16_ncols_6_bs_256_param_9];
	ld.param.u32 	%r59, [ggml_matvec_f16_ncols_6_bs_256_param_10];
	ld.param.u32 	%r55, [ggml_matvec_f16_ncols_6_bs_256_param_11];
	cvta.to.global.u64 	%rd76, %rd30;
	cvta.to.global.u64 	%rd2, %rd29;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r60, %r1, %r57;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r61, %r2, %r56;
	mad.lo.s32 	%r62, %r60, %r58, %r61;
	cvt.s64.s32 	%rd4, %r62;
	mul.lo.s32 	%r63, %r1, %r59;
	cvt.s64.s32 	%rd5, %r63;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r64, %r3, 2;
	mov.u32 	%r65, data_mmv;
	add.s32 	%r4, %r65, %r64;
	@%p1 bra 	$L__BB125_2;

	mov.u32 	%r66, 0;
	st.shared.u32 	[%r4], %r66;

$L__BB125_2:
	bar.sync 	0;
	mov.f32 	%f7, 0f00000000;
	st.local.v2.f32 	[%rd3], {%f7, %f7};
	st.local.v2.f32 	[%rd3+8], {%f7, %f7};
	st.local.v2.f32 	[%rd3+16], {%f7, %f7};
	mov.u32 	%r521, 0;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs2, %f7;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f7;}

	// end inline asm
	mov.b32 	%r526, {%rs1, %rs2};
	setp.ge.s32 	%p2, %r3, %r52;
	mov.u32 	%r522, %r521;
	mov.u32 	%r523, %r521;
	mov.u32 	%r524, %r521;
	mov.u32 	%r525, %r521;
	@%p2 bra 	$L__BB125_9;

	not.b32 	%r77, %r3;
	add.s32 	%r6, %r77, %r52;
	shr.u32 	%r78, %r6, 8;
	add.s32 	%r79, %r78, 1;
	and.b32  	%r506, %r79, 3;
	setp.eq.s32 	%p3, %r506, 0;
	mov.u32 	%r521, 0;
	mov.u32 	%r513, %r3;
	@%p3 bra 	$L__BB125_6;

	shl.b32 	%r85, %r53, 1;
	add.s32 	%r86, %r3, %r85;
	mul.wide.s32 	%rd32, %r86, 4;
	shl.b64 	%rd33, %rd5, 1;
	add.s64 	%rd7, %rd32, %rd33;
	mul.wide.s32 	%rd34, %r3, 4;
	mul.wide.s32 	%rd8, %r53, 4;
	add.s64 	%rd35, %rd34, %rd8;
	add.s64 	%rd9, %rd35, %rd33;
	add.s64 	%rd10, %rd34, %rd33;
	mul.wide.s32 	%rd36, %r3, 2;
	add.s64 	%rd37, %rd36, %rd4;
	shl.b64 	%rd38, %rd37, 1;
	add.s64 	%rd73, %rd2, %rd38;
	mov.u32 	%r521, 0;
	mov.u64 	%rd74, %rd76;
	mov.u32 	%r522, %r521;
	mov.u32 	%r523, %r521;
	mov.u32 	%r524, %r521;
	mov.u32 	%r525, %r521;
	mov.u32 	%r513, %r3;

$L__BB125_5:
	.pragma "nounroll";
	ld.global.nc.u32 	%r88, [%rd73];
	add.s64 	%rd39, %rd74, %rd10;
	ld.global.nc.u32 	%r89, [%rd39];
	// begin inline asm
	{mul.f16x2 %r87,%r88,%r89;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r526,%r526,%r87;
}
	// end inline asm
	add.s64 	%rd40, %rd74, %rd9;
	ld.global.nc.u32 	%r95, [%rd40];
	// begin inline asm
	{mul.f16x2 %r93,%r88,%r95;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r525,%r525,%r93;
}
	// end inline asm
	add.s64 	%rd41, %rd74, %rd7;
	ld.global.nc.u32 	%r101, [%rd41];
	// begin inline asm
	{mul.f16x2 %r99,%r88,%r101;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r524,%r524,%r99;
}
	// end inline asm
	add.s64 	%rd42, %rd41, %rd8;
	ld.global.nc.u32 	%r107, [%rd42];
	// begin inline asm
	{mul.f16x2 %r105,%r88,%r107;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r523,%r523,%r105;
}
	// end inline asm
	add.s64 	%rd43, %rd42, %rd8;
	ld.global.nc.u32 	%r113, [%rd43];
	// begin inline asm
	{mul.f16x2 %r111,%r88,%r113;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r522,%r522,%r111;
}
	// end inline asm
	add.s64 	%rd44, %rd43, %rd8;
	ld.global.nc.u32 	%r119, [%rd44];
	// begin inline asm
	{mul.f16x2 %r117,%r88,%r119;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r521,%r521,%r117;
}
	// end inline asm
	add.s32 	%r513, %r513, 256;
	add.s64 	%rd74, %rd74, 1024;
	add.s64 	%rd73, %rd73, 1024;
	add.s32 	%r506, %r506, -1;
	setp.ne.s32 	%p4, %r506, 0;
	@%p4 bra 	$L__BB125_5;

$L__BB125_6:
	setp.lt.u32 	%p5, %r6, 768;
	@%p5 bra 	$L__BB125_9;

	add.s32 	%r123, %r513, %r53;
	shl.b32 	%r124, %r53, 1;
	add.s32 	%r125, %r513, %r124;
	mad.lo.s32 	%r126, %r53, 3, %r513;
	shl.b32 	%r127, %r53, 2;
	add.s32 	%r128, %r513, %r127;
	mad.lo.s32 	%r129, %r53, 5, %r513;
	add.s32 	%r130, %r123, 256;
	mul.wide.s32 	%rd45, %r130, 4;
	shl.b64 	%rd46, %rd5, 1;
	add.s64 	%rd16, %rd45, %rd46;
	mul.wide.s32 	%rd47, %r125, 4;
	add.s64 	%rd17, %rd47, %rd46;
	mul.wide.s32 	%rd48, %r126, 4;
	add.s64 	%rd18, %rd48, %rd46;
	mul.wide.s32 	%rd49, %r128, 4;
	add.s64 	%rd19, %rd49, %rd46;
	mul.wide.s32 	%rd50, %r129, 4;
	add.s64 	%rd20, %rd50, %rd46;
	mul.wide.s32 	%rd51, %r513, 2;
	add.s64 	%rd52, %rd51, %rd4;
	shl.b64 	%rd53, %rd52, 1;
	add.s64 	%rd54, %rd2, %rd53;
	add.s64 	%rd75, %rd54, 2048;
	mul.wide.s32 	%rd55, %r513, 4;
	add.s64 	%rd22, %rd55, %rd46;
	mul.wide.s32 	%rd56, %r53, 4;
	add.s64 	%rd57, %rd55, %rd56;
	add.s64 	%rd23, %rd57, %rd46;

$L__BB125_8:
	ld.global.nc.u32 	%r132, [%rd75+-2048];
	add.s64 	%rd58, %rd76, %rd22;
	ld.global.nc.u32 	%r133, [%rd58];
	// begin inline asm
	{mul.f16x2 %r131,%r132,%r133;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r134,%r526,%r131;
}
	// end inline asm
	add.s64 	%rd59, %rd76, %rd23;
	ld.global.nc.u32 	%r139, [%rd59];
	// begin inline asm
	{mul.f16x2 %r137,%r132,%r139;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r140,%r525,%r137;
}
	// end inline asm
	add.s64 	%rd60, %rd76, %rd17;
	ld.global.nc.u32 	%r145, [%rd60];
	// begin inline asm
	{mul.f16x2 %r143,%r132,%r145;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r146,%r524,%r143;
}
	// end inline asm
	add.s64 	%rd61, %rd76, %rd18;
	ld.global.nc.u32 	%r151, [%rd61];
	// begin inline asm
	{mul.f16x2 %r149,%r132,%r151;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r152,%r523,%r149;
}
	// end inline asm
	add.s64 	%rd62, %rd76, %rd19;
	ld.global.nc.u32 	%r157, [%rd62];
	// begin inline asm
	{mul.f16x2 %r155,%r132,%r157;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r158,%r522,%r155;
}
	// end inline asm
	add.s64 	%rd63, %rd76, %rd20;
	ld.global.nc.u32 	%r163, [%rd63];
	// begin inline asm
	{mul.f16x2 %r161,%r132,%r163;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r164,%r521,%r161;
}
	// end inline asm
	ld.global.nc.u32 	%r168, [%rd75+-1024];
	ld.global.nc.u32 	%r169, [%rd58+1024];
	// begin inline asm
	{mul.f16x2 %r167,%r168,%r169;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r170,%r134,%r167;
}
	// end inline asm
	add.s64 	%rd64, %rd76, %rd16;
	ld.global.nc.u32 	%r175, [%rd64];
	// begin inline asm
	{mul.f16x2 %r173,%r168,%r175;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r176,%r140,%r173;
}
	// end inline asm
	ld.global.nc.u32 	%r181, [%rd60+1024];
	// begin inline asm
	{mul.f16x2 %r179,%r168,%r181;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r182,%r146,%r179;
}
	// end inline asm
	ld.global.nc.u32 	%r187, [%rd61+1024];
	// begin inline asm
	{mul.f16x2 %r185,%r168,%r187;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r188,%r152,%r185;
}
	// end inline asm
	ld.global.nc.u32 	%r193, [%rd62+1024];
	// begin inline asm
	{mul.f16x2 %r191,%r168,%r193;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r194,%r158,%r191;
}
	// end inline asm
	ld.global.nc.u32 	%r199, [%rd63+1024];
	// begin inline asm
	{mul.f16x2 %r197,%r168,%r199;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r200,%r164,%r197;
}
	// end inline asm
	ld.global.nc.u32 	%r204, [%rd75];
	ld.global.nc.u32 	%r205, [%rd58+2048];
	// begin inline asm
	{mul.f16x2 %r203,%r204,%r205;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r206,%r170,%r203;
}
	// end inline asm
	ld.global.nc.u32 	%r211, [%rd64+1024];
	// begin inline asm
	{mul.f16x2 %r209,%r204,%r211;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r212,%r176,%r209;
}
	// end inline asm
	ld.global.nc.u32 	%r217, [%rd60+2048];
	// begin inline asm
	{mul.f16x2 %r215,%r204,%r217;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r218,%r182,%r215;
}
	// end inline asm
	ld.global.nc.u32 	%r223, [%rd61+2048];
	// begin inline asm
	{mul.f16x2 %r221,%r204,%r223;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r224,%r188,%r221;
}
	// end inline asm
	ld.global.nc.u32 	%r229, [%rd62+2048];
	// begin inline asm
	{mul.f16x2 %r227,%r204,%r229;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r230,%r194,%r227;
}
	// end inline asm
	ld.global.nc.u32 	%r235, [%rd63+2048];
	// begin inline asm
	{mul.f16x2 %r233,%r204,%r235;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r236,%r200,%r233;
}
	// end inline asm
	ld.global.nc.u32 	%r240, [%rd75+1024];
	ld.global.nc.u32 	%r241, [%rd58+3072];
	// begin inline asm
	{mul.f16x2 %r239,%r240,%r241;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r526,%r206,%r239;
}
	// end inline asm
	ld.global.nc.u32 	%r247, [%rd64+2048];
	// begin inline asm
	{mul.f16x2 %r245,%r240,%r247;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r525,%r212,%r245;
}
	// end inline asm
	ld.global.nc.u32 	%r253, [%rd60+3072];
	// begin inline asm
	{mul.f16x2 %r251,%r240,%r253;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r524,%r218,%r251;
}
	// end inline asm
	ld.global.nc.u32 	%r259, [%rd61+3072];
	// begin inline asm
	{mul.f16x2 %r257,%r240,%r259;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r523,%r224,%r257;
}
	// end inline asm
	ld.global.nc.u32 	%r265, [%rd62+3072];
	// begin inline asm
	{mul.f16x2 %r263,%r240,%r265;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r522,%r230,%r263;
}
	// end inline asm
	ld.global.nc.u32 	%r271, [%rd63+3072];
	// begin inline asm
	{mul.f16x2 %r269,%r240,%r271;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r521,%r236,%r269;
}
	// end inline asm
	add.s64 	%rd76, %rd76, 4096;
	add.s64 	%rd75, %rd75, 4096;
	add.s32 	%r513, %r513, 1024;
	setp.lt.s32 	%p6, %r513, %r52;
	@%p6 bra 	$L__BB125_8;

$L__BB125_9:
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r526;
  cvt.f32.f16 %f8, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r526;
  cvt.f32.f16 %f9, high;}

	// end inline asm
	add.f32 	%f20, %f8, %f9;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r525;
  cvt.f32.f16 %f10, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r525;
  cvt.f32.f16 %f11, high;}

	// end inline asm
	add.f32 	%f1, %f10, %f11;
	st.local.f32 	[%rd3+4], %f1;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r524;
  cvt.f32.f16 %f12, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r524;
  cvt.f32.f16 %f13, high;}

	// end inline asm
	add.f32 	%f2, %f12, %f13;
	st.local.f32 	[%rd3+8], %f2;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r523;
  cvt.f32.f16 %f14, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r523;
  cvt.f32.f16 %f15, high;}

	// end inline asm
	add.f32 	%f3, %f14, %f15;
	st.local.f32 	[%rd3+12], %f3;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r522;
  cvt.f32.f16 %f16, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r522;
  cvt.f32.f16 %f17, high;}

	// end inline asm
	add.f32 	%f4, %f16, %f17;
	st.local.f32 	[%rd3+16], %f4;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r521;
  cvt.f32.f16 %f18, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r521;
  cvt.f32.f16 %f19, high;}

	// end inline asm
	add.f32 	%f5, %f18, %f19;
	st.local.f32 	[%rd3+20], %f5;
	shr.s32 	%r287, %r3, 31;
	shr.u32 	%r288, %r287, 27;
	add.s32 	%r289, %r3, %r288;
	shr.s32 	%r290, %r289, 5;
	shl.b32 	%r291, %r290, 2;
	add.s32 	%r51, %r65, %r291;
	mov.u32 	%r293, 2;
	mov.b32 	%r294, %f20;
	mov.u32 	%r295, 31;
	mov.u32 	%r296, 16;
	mov.u32 	%r297, -1;
	shfl.sync.bfly.b32 	%r298|%p7, %r294, %r296, %r295, %r297;
	mov.b32 	%f21, %r298;
	add.f32 	%f22, %f20, %f21;
	mov.b32 	%r299, %f22;
	mov.u32 	%r300, 8;
	shfl.sync.bfly.b32 	%r301|%p8, %r299, %r300, %r295, %r297;
	mov.b32 	%f23, %r301;
	add.f32 	%f24, %f22, %f23;
	mov.b32 	%r302, %f24;
	mov.u32 	%r303, 4;
	shfl.sync.bfly.b32 	%r304|%p9, %r302, %r303, %r295, %r297;
	mov.b32 	%f25, %r304;
	add.f32 	%f26, %f24, %f25;
	mov.b32 	%r305, %f26;
	shfl.sync.bfly.b32 	%r306|%p10, %r305, %r293, %r295, %r297;
	mov.b32 	%f27, %r306;
	add.f32 	%f28, %f26, %f27;
	mov.b32 	%r307, %f28;
	mov.u32 	%r308, 1;
	shfl.sync.bfly.b32 	%r309|%p11, %r307, %r308, %r295, %r297;
	mov.b32 	%f29, %r309;
	add.f32 	%f30, %f28, %f29;
	st.local.f32 	[%rd3], %f30;
	st.shared.f32 	[%r51], %f30;
	bar.sync 	0;
	@%p1 bra 	$L__BB125_11;

	ld.shared.f32 	%f31, [%r4];
	mov.b32 	%r310, %f31;
	shfl.sync.bfly.b32 	%r314|%p13, %r310, %r296, %r295, %r297;
	mov.b32 	%f32, %r314;
	add.f32 	%f33, %f31, %f32;
	mov.b32 	%r315, %f33;
	shfl.sync.bfly.b32 	%r317|%p14, %r315, %r300, %r295, %r297;
	mov.b32 	%f34, %r317;
	add.f32 	%f35, %f33, %f34;
	mov.b32 	%r318, %f35;
	shfl.sync.bfly.b32 	%r320|%p15, %r318, %r303, %r295, %r297;
	mov.b32 	%f36, %r320;
	add.f32 	%f37, %f35, %f36;
	mov.b32 	%r321, %f37;
	shfl.sync.bfly.b32 	%r323|%p16, %r321, %r293, %r295, %r297;
	mov.b32 	%f38, %r323;
	add.f32 	%f39, %f37, %f38;
	mov.b32 	%r324, %f39;
	shfl.sync.bfly.b32 	%r326|%p17, %r324, %r308, %r295, %r297;
	mov.b32 	%f40, %r326;
	add.f32 	%f41, %f39, %f40;
	st.local.f32 	[%rd3], %f41;

$L__BB125_11:
	bar.sync 	0;
	mov.b32 	%r327, %f1;
	shfl.sync.bfly.b32 	%r331|%p19, %r327, %r296, %r295, %r297;
	mov.b32 	%f42, %r331;
	add.f32 	%f43, %f1, %f42;
	mov.b32 	%r332, %f43;
	shfl.sync.bfly.b32 	%r334|%p20, %r332, %r300, %r295, %r297;
	mov.b32 	%f44, %r334;
	add.f32 	%f45, %f43, %f44;
	mov.b32 	%r335, %f45;
	shfl.sync.bfly.b32 	%r337|%p21, %r335, %r303, %r295, %r297;
	mov.b32 	%f46, %r337;
	add.f32 	%f47, %f45, %f46;
	mov.b32 	%r338, %f47;
	shfl.sync.bfly.b32 	%r340|%p22, %r338, %r293, %r295, %r297;
	mov.b32 	%f48, %r340;
	add.f32 	%f49, %f47, %f48;
	mov.b32 	%r341, %f49;
	shfl.sync.bfly.b32 	%r343|%p23, %r341, %r308, %r295, %r297;
	mov.b32 	%f50, %r343;
	add.f32 	%f51, %f49, %f50;
	st.local.f32 	[%rd3+4], %f51;
	st.shared.f32 	[%r51], %f51;
	bar.sync 	0;
	@%p1 bra 	$L__BB125_13;

	ld.shared.f32 	%f52, [%r4];
	mov.b32 	%r344, %f52;
	mov.u32 	%r345, 31;
	mov.u32 	%r346, 16;
	mov.u32 	%r347, -1;
	shfl.sync.bfly.b32 	%r348|%p24, %r344, %r346, %r345, %r347;
	mov.b32 	%f53, %r348;
	add.f32 	%f54, %f52, %f53;
	mov.b32 	%r349, %f54;
	mov.u32 	%r350, 8;
	shfl.sync.bfly.b32 	%r351|%p25, %r349, %r350, %r345, %r347;
	mov.b32 	%f55, %r351;
	add.f32 	%f56, %f54, %f55;
	mov.b32 	%r352, %f56;
	mov.u32 	%r353, 4;
	shfl.sync.bfly.b32 	%r354|%p26, %r352, %r353, %r345, %r347;
	mov.b32 	%f57, %r354;
	add.f32 	%f58, %f56, %f57;
	mov.b32 	%r355, %f58;
	mov.u32 	%r356, 2;
	shfl.sync.bfly.b32 	%r357|%p27, %r355, %r356, %r345, %r347;
	mov.b32 	%f59, %r357;
	add.f32 	%f60, %f58, %f59;
	mov.b32 	%r358, %f60;
	mov.u32 	%r359, 1;
	shfl.sync.bfly.b32 	%r360|%p28, %r358, %r359, %r345, %r347;
	mov.b32 	%f61, %r360;
	add.f32 	%f62, %f60, %f61;
	st.local.f32 	[%rd3+4], %f62;

$L__BB125_13:
	bar.sync 	0;
	mov.b32 	%r361, %f2;
	mov.u32 	%r362, 31;
	mov.u32 	%r363, 16;
	mov.u32 	%r364, -1;
	shfl.sync.bfly.b32 	%r365|%p30, %r361, %r363, %r362, %r364;
	mov.b32 	%f63, %r365;
	add.f32 	%f64, %f2, %f63;
	mov.b32 	%r366, %f64;
	mov.u32 	%r367, 8;
	shfl.sync.bfly.b32 	%r368|%p31, %r366, %r367, %r362, %r364;
	mov.b32 	%f65, %r368;
	add.f32 	%f66, %f64, %f65;
	mov.b32 	%r369, %f66;
	mov.u32 	%r370, 4;
	shfl.sync.bfly.b32 	%r371|%p32, %r369, %r370, %r362, %r364;
	mov.b32 	%f67, %r371;
	add.f32 	%f68, %f66, %f67;
	mov.b32 	%r372, %f68;
	mov.u32 	%r373, 2;
	shfl.sync.bfly.b32 	%r374|%p33, %r372, %r373, %r362, %r364;
	mov.b32 	%f69, %r374;
	add.f32 	%f70, %f68, %f69;
	mov.b32 	%r375, %f70;
	mov.u32 	%r376, 1;
	shfl.sync.bfly.b32 	%r377|%p34, %r375, %r376, %r362, %r364;
	mov.b32 	%f71, %r377;
	add.f32 	%f72, %f70, %f71;
	st.local.f32 	[%rd3+8], %f72;
	st.shared.f32 	[%r51], %f72;
	bar.sync 	0;
	@%p1 bra 	$L__BB125_15;

	ld.shared.f32 	%f73, [%r4];
	mov.b32 	%r378, %f73;
	shfl.sync.bfly.b32 	%r382|%p35, %r378, %r363, %r362, %r364;
	mov.b32 	%f74, %r382;
	add.f32 	%f75, %f73, %f74;
	mov.b32 	%r383, %f75;
	shfl.sync.bfly.b32 	%r385|%p36, %r383, %r367, %r362, %r364;
	mov.b32 	%f76, %r385;
	add.f32 	%f77, %f75, %f76;
	mov.b32 	%r386, %f77;
	shfl.sync.bfly.b32 	%r388|%p37, %r386, %r370, %r362, %r364;
	mov.b32 	%f78, %r388;
	add.f32 	%f79, %f77, %f78;
	mov.b32 	%r389, %f79;
	shfl.sync.bfly.b32 	%r391|%p38, %r389, %r373, %r362, %r364;
	mov.b32 	%f80, %r391;
	add.f32 	%f81, %f79, %f80;
	mov.b32 	%r392, %f81;
	shfl.sync.bfly.b32 	%r394|%p39, %r392, %r376, %r362, %r364;
	mov.b32 	%f82, %r394;
	add.f32 	%f83, %f81, %f82;
	st.local.f32 	[%rd3+8], %f83;

$L__BB125_15:
	bar.sync 	0;
	mov.b32 	%r395, %f3;
	shfl.sync.bfly.b32 	%r399|%p41, %r395, %r363, %r362, %r364;
	mov.b32 	%f84, %r399;
	add.f32 	%f85, %f3, %f84;
	mov.b32 	%r400, %f85;
	shfl.sync.bfly.b32 	%r402|%p42, %r400, %r367, %r362, %r364;
	mov.b32 	%f86, %r402;
	add.f32 	%f87, %f85, %f86;
	mov.b32 	%r403, %f87;
	shfl.sync.bfly.b32 	%r405|%p43, %r403, %r370, %r362, %r364;
	mov.b32 	%f88, %r405;
	add.f32 	%f89, %f87, %f88;
	mov.b32 	%r406, %f89;
	shfl.sync.bfly.b32 	%r408|%p44, %r406, %r373, %r362, %r364;
	mov.b32 	%f90, %r408;
	add.f32 	%f91, %f89, %f90;
	mov.b32 	%r409, %f91;
	shfl.sync.bfly.b32 	%r411|%p45, %r409, %r376, %r362, %r364;
	mov.b32 	%f92, %r411;
	add.f32 	%f93, %f91, %f92;
	st.local.f32 	[%rd3+12], %f93;
	st.shared.f32 	[%r51], %f93;
	bar.sync 	0;
	@%p1 bra 	$L__BB125_17;

	ld.shared.f32 	%f94, [%r4];
	mov.b32 	%r412, %f94;
	mov.u32 	%r413, 31;
	mov.u32 	%r414, 16;
	mov.u32 	%r415, -1;
	shfl.sync.bfly.b32 	%r416|%p46, %r412, %r414, %r413, %r415;
	mov.b32 	%f95, %r416;
	add.f32 	%f96, %f94, %f95;
	mov.b32 	%r417, %f96;
	mov.u32 	%r418, 8;
	shfl.sync.bfly.b32 	%r419|%p47, %r417, %r418, %r413, %r415;
	mov.b32 	%f97, %r419;
	add.f32 	%f98, %f96, %f97;
	mov.b32 	%r420, %f98;
	mov.u32 	%r421, 4;
	shfl.sync.bfly.b32 	%r422|%p48, %r420, %r421, %r413, %r415;
	mov.b32 	%f99, %r422;
	add.f32 	%f100, %f98, %f99;
	mov.b32 	%r423, %f100;
	mov.u32 	%r424, 2;
	shfl.sync.bfly.b32 	%r425|%p49, %r423, %r424, %r413, %r415;
	mov.b32 	%f101, %r425;
	add.f32 	%f102, %f100, %f101;
	mov.b32 	%r426, %f102;
	mov.u32 	%r427, 1;
	shfl.sync.bfly.b32 	%r428|%p50, %r426, %r427, %r413, %r415;
	mov.b32 	%f103, %r428;
	add.f32 	%f104, %f102, %f103;
	st.local.f32 	[%rd3+12], %f104;

$L__BB125_17:
	bar.sync 	0;
	mov.b32 	%r429, %f4;
	mov.u32 	%r430, 31;
	mov.u32 	%r431, 16;
	mov.u32 	%r432, -1;
	shfl.sync.bfly.b32 	%r433|%p52, %r429, %r431, %r430, %r432;
	mov.b32 	%f105, %r433;
	add.f32 	%f106, %f4, %f105;
	mov.b32 	%r434, %f106;
	mov.u32 	%r435, 8;
	shfl.sync.bfly.b32 	%r436|%p53, %r434, %r435, %r430, %r432;
	mov.b32 	%f107, %r436;
	add.f32 	%f108, %f106, %f107;
	mov.b32 	%r437, %f108;
	mov.u32 	%r438, 4;
	shfl.sync.bfly.b32 	%r439|%p54, %r437, %r438, %r430, %r432;
	mov.b32 	%f109, %r439;
	add.f32 	%f110, %f108, %f109;
	mov.b32 	%r440, %f110;
	mov.u32 	%r441, 2;
	shfl.sync.bfly.b32 	%r442|%p55, %r440, %r441, %r430, %r432;
	mov.b32 	%f111, %r442;
	add.f32 	%f112, %f110, %f111;
	mov.b32 	%r443, %f112;
	mov.u32 	%r444, 1;
	shfl.sync.bfly.b32 	%r445|%p56, %r443, %r444, %r430, %r432;
	mov.b32 	%f113, %r445;
	add.f32 	%f114, %f112, %f113;
	st.local.f32 	[%rd3+16], %f114;
	st.shared.f32 	[%r51], %f114;
	bar.sync 	0;
	@%p1 bra 	$L__BB125_19;

	ld.shared.f32 	%f115, [%r4];
	mov.b32 	%r446, %f115;
	shfl.sync.bfly.b32 	%r450|%p57, %r446, %r431, %r430, %r432;
	mov.b32 	%f116, %r450;
	add.f32 	%f117, %f115, %f116;
	mov.b32 	%r451, %f117;
	shfl.sync.bfly.b32 	%r453|%p58, %r451, %r435, %r430, %r432;
	mov.b32 	%f118, %r453;
	add.f32 	%f119, %f117, %f118;
	mov.b32 	%r454, %f119;
	shfl.sync.bfly.b32 	%r456|%p59, %r454, %r438, %r430, %r432;
	mov.b32 	%f120, %r456;
	add.f32 	%f121, %f119, %f120;
	mov.b32 	%r457, %f121;
	shfl.sync.bfly.b32 	%r459|%p60, %r457, %r441, %r430, %r432;
	mov.b32 	%f122, %r459;
	add.f32 	%f123, %f121, %f122;
	mov.b32 	%r460, %f123;
	shfl.sync.bfly.b32 	%r462|%p61, %r460, %r444, %r430, %r432;
	mov.b32 	%f124, %r462;
	add.f32 	%f125, %f123, %f124;
	st.local.f32 	[%rd3+16], %f125;

$L__BB125_19:
	bar.sync 	0;
	mov.b32 	%r463, %f5;
	shfl.sync.bfly.b32 	%r467|%p63, %r463, %r431, %r430, %r432;
	mov.b32 	%f126, %r467;
	add.f32 	%f127, %f5, %f126;
	mov.b32 	%r468, %f127;
	shfl.sync.bfly.b32 	%r470|%p64, %r468, %r435, %r430, %r432;
	mov.b32 	%f128, %r470;
	add.f32 	%f129, %f127, %f128;
	mov.b32 	%r471, %f129;
	shfl.sync.bfly.b32 	%r473|%p65, %r471, %r438, %r430, %r432;
	mov.b32 	%f130, %r473;
	add.f32 	%f131, %f129, %f130;
	mov.b32 	%r474, %f131;
	shfl.sync.bfly.b32 	%r476|%p66, %r474, %r441, %r430, %r432;
	mov.b32 	%f132, %r476;
	add.f32 	%f133, %f131, %f132;
	mov.b32 	%r477, %f133;
	shfl.sync.bfly.b32 	%r479|%p67, %r477, %r444, %r430, %r432;
	mov.b32 	%f134, %r479;
	add.f32 	%f135, %f133, %f134;
	st.local.f32 	[%rd3+20], %f135;
	st.shared.f32 	[%r51], %f135;
	bar.sync 	0;
	@%p1 bra 	$L__BB125_21;

	ld.shared.f32 	%f136, [%r4];
	mov.b32 	%r480, %f136;
	mov.u32 	%r481, 31;
	mov.u32 	%r482, 16;
	mov.u32 	%r483, -1;
	shfl.sync.bfly.b32 	%r484|%p68, %r480, %r482, %r481, %r483;
	mov.b32 	%f137, %r484;
	add.f32 	%f138, %f136, %f137;
	mov.b32 	%r485, %f138;
	mov.u32 	%r486, 8;
	shfl.sync.bfly.b32 	%r487|%p69, %r485, %r486, %r481, %r483;
	mov.b32 	%f139, %r487;
	add.f32 	%f140, %f138, %f139;
	mov.b32 	%r488, %f140;
	mov.u32 	%r489, 4;
	shfl.sync.bfly.b32 	%r490|%p70, %r488, %r489, %r481, %r483;
	mov.b32 	%f141, %r490;
	add.f32 	%f142, %f140, %f141;
	mov.b32 	%r491, %f142;
	mov.u32 	%r492, 2;
	shfl.sync.bfly.b32 	%r493|%p71, %r491, %r492, %r481, %r483;
	mov.b32 	%f143, %r493;
	add.f32 	%f144, %f142, %f143;
	mov.b32 	%r494, %f144;
	mov.u32 	%r495, 1;
	shfl.sync.bfly.b32 	%r496|%p72, %r494, %r495, %r481, %r483;
	mov.b32 	%f145, %r496;
	add.f32 	%f146, %f144, %f145;
	st.local.f32 	[%rd3+20], %f146;

$L__BB125_21:
	bar.sync 	0;
	setp.gt.s32 	%p73, %r3, 5;
	@%p73 bra 	$L__BB125_23;

	mad.lo.s32 	%r497, %r3, %r54, %r2;
	cvt.s64.s32 	%rd65, %r497;
	mul.lo.s32 	%r498, %r1, %r55;
	cvt.s64.s32 	%rd66, %r498;
	add.s64 	%rd67, %rd66, %rd65;
	mul.wide.s32 	%rd68, %r3, 4;
	add.s64 	%rd69, %rd3, %rd68;
	ld.local.f32 	%f147, [%rd69];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs3, %f147;}

	// end inline asm
	cvta.to.global.u64 	%rd70, %rd28;
	shl.b64 	%rd71, %rd67, 1;
	add.s64 	%rd72, %rd70, %rd71;
	st.global.u16 	[%rd72], %rs3;

$L__BB125_23:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_7_bs_256
.visible .entry ggml_matvec_f16_ncols_7_bs_256(
	.param .u64 ggml_matvec_f16_ncols_7_bs_256_param_0,
	.param .u64 ggml_matvec_f16_ncols_7_bs_256_param_1,
	.param .u64 ggml_matvec_f16_ncols_7_bs_256_param_2,
	.param .u32 ggml_matvec_f16_ncols_7_bs_256_param_3,
	.param .u32 ggml_matvec_f16_ncols_7_bs_256_param_4,
	.param .u32 ggml_matvec_f16_ncols_7_bs_256_param_5,
	.param .u32 ggml_matvec_f16_ncols_7_bs_256_param_6,
	.param .u32 ggml_matvec_f16_ncols_7_bs_256_param_7,
	.param .u32 ggml_matvec_f16_ncols_7_bs_256_param_8,
	.param .u32 ggml_matvec_f16_ncols_7_bs_256_param_9,
	.param .u32 ggml_matvec_f16_ncols_7_bs_256_param_10,
	.param .u32 ggml_matvec_f16_ncols_7_bs_256_param_11
)
{
	.local .align 4 .b8 	__local_depot126[28];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<85>;
	.reg .b16 	%rs<4>;
	.reg .f32 	%f<172>;
	.reg .b32 	%r<607>;
	.reg .b64 	%rd<81>;


	mov.u64 	%SPL, __local_depot126;
	ld.param.u64 	%rd30, [ggml_matvec_f16_ncols_7_bs_256_param_0];
	ld.param.u64 	%rd31, [ggml_matvec_f16_ncols_7_bs_256_param_1];
	ld.param.u64 	%rd29, [ggml_matvec_f16_ncols_7_bs_256_param_2];
	ld.param.u32 	%r58, [ggml_matvec_f16_ncols_7_bs_256_param_3];
	ld.param.u32 	%r62, [ggml_matvec_f16_ncols_7_bs_256_param_5];
	ld.param.u32 	%r59, [ggml_matvec_f16_ncols_7_bs_256_param_6];
	ld.param.u32 	%r60, [ggml_matvec_f16_ncols_7_bs_256_param_7];
	ld.param.u32 	%r63, [ggml_matvec_f16_ncols_7_bs_256_param_8];
	ld.param.u32 	%r64, [ggml_matvec_f16_ncols_7_bs_256_param_9];
	ld.param.u32 	%r65, [ggml_matvec_f16_ncols_7_bs_256_param_10];
	ld.param.u32 	%r61, [ggml_matvec_f16_ncols_7_bs_256_param_11];
	cvta.to.global.u64 	%rd80, %rd31;
	cvta.to.global.u64 	%rd2, %rd30;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r66, %r1, %r63;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r67, %r2, %r62;
	mad.lo.s32 	%r68, %r66, %r64, %r67;
	cvt.s64.s32 	%rd4, %r68;
	mul.lo.s32 	%r69, %r1, %r65;
	cvt.s64.s32 	%rd5, %r69;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r70, %r3, 2;
	mov.u32 	%r71, data_mmv;
	add.s32 	%r4, %r71, %r70;
	@%p1 bra 	$L__BB126_2;

	mov.u32 	%r72, 0;
	st.shared.u32 	[%r4], %r72;

$L__BB126_2:
	bar.sync 	0;
	mov.f32 	%f8, 0f00000000;
	mov.u32 	%r600, 0;
	st.local.u32 	[%rd3], %r600;
	st.local.u32 	[%rd3+4], %r600;
	st.local.u32 	[%rd3+8], %r600;
	st.local.u32 	[%rd3+12], %r600;
	st.local.u32 	[%rd3+16], %r600;
	st.local.u32 	[%rd3+20], %r600;
	st.local.u32 	[%rd3+24], %r600;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs2, %f8;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f8;}

	// end inline asm
	mov.b32 	%r606, {%rs1, %rs2};
	setp.ge.s32 	%p2, %r3, %r58;
	mov.u32 	%r601, %r600;
	mov.u32 	%r602, %r600;
	mov.u32 	%r603, %r600;
	mov.u32 	%r604, %r600;
	mov.u32 	%r605, %r600;
	@%p2 bra 	$L__BB126_9;

	not.b32 	%r85, %r3;
	add.s32 	%r6, %r85, %r58;
	shr.u32 	%r86, %r6, 8;
	add.s32 	%r87, %r86, 1;
	and.b32  	%r583, %r87, 3;
	setp.eq.s32 	%p3, %r583, 0;
	mov.u32 	%r600, 0;
	mov.u32 	%r591, %r3;
	@%p3 bra 	$L__BB126_6;

	shl.b32 	%r94, %r59, 1;
	add.s32 	%r95, %r3, %r94;
	mul.wide.s32 	%rd33, %r95, 4;
	shl.b64 	%rd34, %rd5, 1;
	add.s64 	%rd7, %rd33, %rd34;
	mul.wide.s32 	%rd35, %r3, 4;
	mul.wide.s32 	%rd8, %r59, 4;
	add.s64 	%rd36, %rd35, %rd8;
	add.s64 	%rd9, %rd36, %rd34;
	add.s64 	%rd10, %rd35, %rd34;
	mul.wide.s32 	%rd37, %r3, 2;
	add.s64 	%rd38, %rd37, %rd4;
	shl.b64 	%rd39, %rd38, 1;
	add.s64 	%rd77, %rd2, %rd39;
	mov.u32 	%r600, 0;
	mov.u64 	%rd78, %rd80;
	mov.u32 	%r601, %r600;
	mov.u32 	%r602, %r600;
	mov.u32 	%r603, %r600;
	mov.u32 	%r604, %r600;
	mov.u32 	%r605, %r600;
	mov.u32 	%r591, %r3;

$L__BB126_5:
	.pragma "nounroll";
	ld.global.nc.u32 	%r97, [%rd77];
	add.s64 	%rd40, %rd78, %rd10;
	ld.global.nc.u32 	%r98, [%rd40];
	// begin inline asm
	{mul.f16x2 %r96,%r97,%r98;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r606,%r606,%r96;
}
	// end inline asm
	add.s64 	%rd41, %rd78, %rd9;
	ld.global.nc.u32 	%r104, [%rd41];
	// begin inline asm
	{mul.f16x2 %r102,%r97,%r104;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r605,%r605,%r102;
}
	// end inline asm
	add.s64 	%rd42, %rd78, %rd7;
	ld.global.nc.u32 	%r110, [%rd42];
	// begin inline asm
	{mul.f16x2 %r108,%r97,%r110;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r604,%r604,%r108;
}
	// end inline asm
	add.s64 	%rd43, %rd42, %rd8;
	ld.global.nc.u32 	%r116, [%rd43];
	// begin inline asm
	{mul.f16x2 %r114,%r97,%r116;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r603,%r603,%r114;
}
	// end inline asm
	add.s64 	%rd44, %rd43, %rd8;
	ld.global.nc.u32 	%r122, [%rd44];
	// begin inline asm
	{mul.f16x2 %r120,%r97,%r122;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r602,%r602,%r120;
}
	// end inline asm
	add.s64 	%rd45, %rd44, %rd8;
	ld.global.nc.u32 	%r128, [%rd45];
	// begin inline asm
	{mul.f16x2 %r126,%r97,%r128;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r601,%r601,%r126;
}
	// end inline asm
	add.s64 	%rd46, %rd45, %rd8;
	ld.global.nc.u32 	%r134, [%rd46];
	// begin inline asm
	{mul.f16x2 %r132,%r97,%r134;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r600,%r600,%r132;
}
	// end inline asm
	add.s32 	%r591, %r591, 256;
	add.s64 	%rd78, %rd78, 1024;
	add.s64 	%rd77, %rd77, 1024;
	add.s32 	%r583, %r583, -1;
	setp.ne.s32 	%p4, %r583, 0;
	@%p4 bra 	$L__BB126_5;

$L__BB126_6:
	setp.lt.u32 	%p5, %r6, 768;
	@%p5 bra 	$L__BB126_9;

	add.s32 	%r138, %r591, %r59;
	shl.b32 	%r139, %r59, 1;
	add.s32 	%r140, %r591, %r139;
	mad.lo.s32 	%r141, %r59, 3, %r591;
	shl.b32 	%r142, %r59, 2;
	add.s32 	%r143, %r591, %r142;
	mad.lo.s32 	%r144, %r59, 5, %r591;
	mad.lo.s32 	%r145, %r59, 6, %r591;
	add.s32 	%r146, %r138, 256;
	mul.wide.s32 	%rd47, %r146, 4;
	shl.b64 	%rd48, %rd5, 1;
	add.s64 	%rd16, %rd47, %rd48;
	mul.wide.s32 	%rd49, %r140, 4;
	add.s64 	%rd17, %rd49, %rd48;
	mul.wide.s32 	%rd50, %r141, 4;
	add.s64 	%rd18, %rd50, %rd48;
	mul.wide.s32 	%rd51, %r143, 4;
	add.s64 	%rd19, %rd51, %rd48;
	mul.wide.s32 	%rd52, %r144, 4;
	add.s64 	%rd20, %rd52, %rd48;
	mul.wide.s32 	%rd53, %r145, 4;
	add.s64 	%rd21, %rd53, %rd48;
	mul.wide.s32 	%rd54, %r591, 2;
	add.s64 	%rd55, %rd54, %rd4;
	shl.b64 	%rd56, %rd55, 1;
	add.s64 	%rd57, %rd2, %rd56;
	add.s64 	%rd79, %rd57, 2048;
	mul.wide.s32 	%rd58, %r591, 4;
	add.s64 	%rd23, %rd58, %rd48;
	mul.wide.s32 	%rd59, %r59, 4;
	add.s64 	%rd60, %rd58, %rd59;
	add.s64 	%rd24, %rd60, %rd48;

$L__BB126_8:
	ld.global.nc.u32 	%r148, [%rd79+-2048];
	add.s64 	%rd61, %rd80, %rd23;
	ld.global.nc.u32 	%r149, [%rd61];
	// begin inline asm
	{mul.f16x2 %r147,%r148,%r149;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r150,%r606,%r147;
}
	// end inline asm
	add.s64 	%rd62, %rd80, %rd24;
	ld.global.nc.u32 	%r155, [%rd62];
	// begin inline asm
	{mul.f16x2 %r153,%r148,%r155;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r156,%r605,%r153;
}
	// end inline asm
	add.s64 	%rd63, %rd80, %rd17;
	ld.global.nc.u32 	%r161, [%rd63];
	// begin inline asm
	{mul.f16x2 %r159,%r148,%r161;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r162,%r604,%r159;
}
	// end inline asm
	add.s64 	%rd64, %rd80, %rd18;
	ld.global.nc.u32 	%r167, [%rd64];
	// begin inline asm
	{mul.f16x2 %r165,%r148,%r167;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r168,%r603,%r165;
}
	// end inline asm
	add.s64 	%rd65, %rd80, %rd19;
	ld.global.nc.u32 	%r173, [%rd65];
	// begin inline asm
	{mul.f16x2 %r171,%r148,%r173;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r174,%r602,%r171;
}
	// end inline asm
	add.s64 	%rd66, %rd80, %rd20;
	ld.global.nc.u32 	%r179, [%rd66];
	// begin inline asm
	{mul.f16x2 %r177,%r148,%r179;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r180,%r601,%r177;
}
	// end inline asm
	add.s64 	%rd67, %rd80, %rd21;
	ld.global.nc.u32 	%r185, [%rd67];
	// begin inline asm
	{mul.f16x2 %r183,%r148,%r185;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r186,%r600,%r183;
}
	// end inline asm
	ld.global.nc.u32 	%r190, [%rd79+-1024];
	ld.global.nc.u32 	%r191, [%rd61+1024];
	// begin inline asm
	{mul.f16x2 %r189,%r190,%r191;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r192,%r150,%r189;
}
	// end inline asm
	add.s64 	%rd68, %rd80, %rd16;
	ld.global.nc.u32 	%r197, [%rd68];
	// begin inline asm
	{mul.f16x2 %r195,%r190,%r197;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r198,%r156,%r195;
}
	// end inline asm
	ld.global.nc.u32 	%r203, [%rd63+1024];
	// begin inline asm
	{mul.f16x2 %r201,%r190,%r203;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r204,%r162,%r201;
}
	// end inline asm
	ld.global.nc.u32 	%r209, [%rd64+1024];
	// begin inline asm
	{mul.f16x2 %r207,%r190,%r209;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r210,%r168,%r207;
}
	// end inline asm
	ld.global.nc.u32 	%r215, [%rd65+1024];
	// begin inline asm
	{mul.f16x2 %r213,%r190,%r215;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r216,%r174,%r213;
}
	// end inline asm
	ld.global.nc.u32 	%r221, [%rd66+1024];
	// begin inline asm
	{mul.f16x2 %r219,%r190,%r221;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r222,%r180,%r219;
}
	// end inline asm
	ld.global.nc.u32 	%r227, [%rd67+1024];
	// begin inline asm
	{mul.f16x2 %r225,%r190,%r227;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r228,%r186,%r225;
}
	// end inline asm
	ld.global.nc.u32 	%r232, [%rd79];
	ld.global.nc.u32 	%r233, [%rd61+2048];
	// begin inline asm
	{mul.f16x2 %r231,%r232,%r233;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r234,%r192,%r231;
}
	// end inline asm
	ld.global.nc.u32 	%r239, [%rd68+1024];
	// begin inline asm
	{mul.f16x2 %r237,%r232,%r239;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r240,%r198,%r237;
}
	// end inline asm
	ld.global.nc.u32 	%r245, [%rd63+2048];
	// begin inline asm
	{mul.f16x2 %r243,%r232,%r245;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r246,%r204,%r243;
}
	// end inline asm
	ld.global.nc.u32 	%r251, [%rd64+2048];
	// begin inline asm
	{mul.f16x2 %r249,%r232,%r251;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r252,%r210,%r249;
}
	// end inline asm
	ld.global.nc.u32 	%r257, [%rd65+2048];
	// begin inline asm
	{mul.f16x2 %r255,%r232,%r257;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r258,%r216,%r255;
}
	// end inline asm
	ld.global.nc.u32 	%r263, [%rd66+2048];
	// begin inline asm
	{mul.f16x2 %r261,%r232,%r263;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r264,%r222,%r261;
}
	// end inline asm
	ld.global.nc.u32 	%r269, [%rd67+2048];
	// begin inline asm
	{mul.f16x2 %r267,%r232,%r269;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r270,%r228,%r267;
}
	// end inline asm
	ld.global.nc.u32 	%r274, [%rd79+1024];
	ld.global.nc.u32 	%r275, [%rd61+3072];
	// begin inline asm
	{mul.f16x2 %r273,%r274,%r275;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r606,%r234,%r273;
}
	// end inline asm
	ld.global.nc.u32 	%r281, [%rd68+2048];
	// begin inline asm
	{mul.f16x2 %r279,%r274,%r281;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r605,%r240,%r279;
}
	// end inline asm
	ld.global.nc.u32 	%r287, [%rd63+3072];
	// begin inline asm
	{mul.f16x2 %r285,%r274,%r287;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r604,%r246,%r285;
}
	// end inline asm
	ld.global.nc.u32 	%r293, [%rd64+3072];
	// begin inline asm
	{mul.f16x2 %r291,%r274,%r293;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r603,%r252,%r291;
}
	// end inline asm
	ld.global.nc.u32 	%r299, [%rd65+3072];
	// begin inline asm
	{mul.f16x2 %r297,%r274,%r299;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r602,%r258,%r297;
}
	// end inline asm
	ld.global.nc.u32 	%r305, [%rd66+3072];
	// begin inline asm
	{mul.f16x2 %r303,%r274,%r305;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r601,%r264,%r303;
}
	// end inline asm
	ld.global.nc.u32 	%r311, [%rd67+3072];
	// begin inline asm
	{mul.f16x2 %r309,%r274,%r311;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r600,%r270,%r309;
}
	// end inline asm
	add.s64 	%rd80, %rd80, 4096;
	add.s64 	%rd79, %rd79, 4096;
	add.s32 	%r591, %r591, 1024;
	setp.lt.s32 	%p6, %r591, %r58;
	@%p6 bra 	$L__BB126_8;

$L__BB126_9:
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r606;
  cvt.f32.f16 %f9, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r606;
  cvt.f32.f16 %f10, high;}

	// end inline asm
	add.f32 	%f23, %f9, %f10;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r605;
  cvt.f32.f16 %f11, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r605;
  cvt.f32.f16 %f12, high;}

	// end inline asm
	add.f32 	%f1, %f11, %f12;
	st.local.f32 	[%rd3+4], %f1;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r604;
  cvt.f32.f16 %f13, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r604;
  cvt.f32.f16 %f14, high;}

	// end inline asm
	add.f32 	%f2, %f13, %f14;
	st.local.f32 	[%rd3+8], %f2;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r603;
  cvt.f32.f16 %f15, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r603;
  cvt.f32.f16 %f16, high;}

	// end inline asm
	add.f32 	%f3, %f15, %f16;
	st.local.f32 	[%rd3+12], %f3;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r602;
  cvt.f32.f16 %f17, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r602;
  cvt.f32.f16 %f18, high;}

	// end inline asm
	add.f32 	%f4, %f17, %f18;
	st.local.f32 	[%rd3+16], %f4;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r601;
  cvt.f32.f16 %f19, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r601;
  cvt.f32.f16 %f20, high;}

	// end inline asm
	add.f32 	%f5, %f19, %f20;
	st.local.f32 	[%rd3+20], %f5;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r600;
  cvt.f32.f16 %f21, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r600;
  cvt.f32.f16 %f22, high;}

	// end inline asm
	add.f32 	%f6, %f21, %f22;
	st.local.f32 	[%rd3+24], %f6;
	shr.s32 	%r329, %r3, 31;
	shr.u32 	%r330, %r329, 27;
	add.s32 	%r331, %r3, %r330;
	shr.s32 	%r332, %r331, 5;
	shl.b32 	%r333, %r332, 2;
	add.s32 	%r57, %r71, %r333;
	mov.u32 	%r335, 2;
	mov.b32 	%r336, %f23;
	mov.u32 	%r337, 31;
	mov.u32 	%r338, 16;
	mov.u32 	%r339, -1;
	shfl.sync.bfly.b32 	%r340|%p7, %r336, %r338, %r337, %r339;
	mov.b32 	%f24, %r340;
	add.f32 	%f25, %f23, %f24;
	mov.b32 	%r341, %f25;
	mov.u32 	%r342, 8;
	shfl.sync.bfly.b32 	%r343|%p8, %r341, %r342, %r337, %r339;
	mov.b32 	%f26, %r343;
	add.f32 	%f27, %f25, %f26;
	mov.b32 	%r344, %f27;
	mov.u32 	%r345, 4;
	shfl.sync.bfly.b32 	%r346|%p9, %r344, %r345, %r337, %r339;
	mov.b32 	%f28, %r346;
	add.f32 	%f29, %f27, %f28;
	mov.b32 	%r347, %f29;
	shfl.sync.bfly.b32 	%r348|%p10, %r347, %r335, %r337, %r339;
	mov.b32 	%f30, %r348;
	add.f32 	%f31, %f29, %f30;
	mov.b32 	%r349, %f31;
	mov.u32 	%r350, 1;
	shfl.sync.bfly.b32 	%r351|%p11, %r349, %r350, %r337, %r339;
	mov.b32 	%f32, %r351;
	add.f32 	%f33, %f31, %f32;
	st.local.f32 	[%rd3], %f33;
	st.shared.f32 	[%r57], %f33;
	bar.sync 	0;
	@%p1 bra 	$L__BB126_11;

	ld.shared.f32 	%f34, [%r4];
	mov.b32 	%r352, %f34;
	shfl.sync.bfly.b32 	%r356|%p13, %r352, %r338, %r337, %r339;
	mov.b32 	%f35, %r356;
	add.f32 	%f36, %f34, %f35;
	mov.b32 	%r357, %f36;
	shfl.sync.bfly.b32 	%r359|%p14, %r357, %r342, %r337, %r339;
	mov.b32 	%f37, %r359;
	add.f32 	%f38, %f36, %f37;
	mov.b32 	%r360, %f38;
	shfl.sync.bfly.b32 	%r362|%p15, %r360, %r345, %r337, %r339;
	mov.b32 	%f39, %r362;
	add.f32 	%f40, %f38, %f39;
	mov.b32 	%r363, %f40;
	shfl.sync.bfly.b32 	%r365|%p16, %r363, %r335, %r337, %r339;
	mov.b32 	%f41, %r365;
	add.f32 	%f42, %f40, %f41;
	mov.b32 	%r366, %f42;
	shfl.sync.bfly.b32 	%r368|%p17, %r366, %r350, %r337, %r339;
	mov.b32 	%f43, %r368;
	add.f32 	%f44, %f42, %f43;
	st.local.f32 	[%rd3], %f44;

$L__BB126_11:
	bar.sync 	0;
	mov.b32 	%r369, %f1;
	shfl.sync.bfly.b32 	%r373|%p19, %r369, %r338, %r337, %r339;
	mov.b32 	%f45, %r373;
	add.f32 	%f46, %f1, %f45;
	mov.b32 	%r374, %f46;
	shfl.sync.bfly.b32 	%r376|%p20, %r374, %r342, %r337, %r339;
	mov.b32 	%f47, %r376;
	add.f32 	%f48, %f46, %f47;
	mov.b32 	%r377, %f48;
	shfl.sync.bfly.b32 	%r379|%p21, %r377, %r345, %r337, %r339;
	mov.b32 	%f49, %r379;
	add.f32 	%f50, %f48, %f49;
	mov.b32 	%r380, %f50;
	shfl.sync.bfly.b32 	%r382|%p22, %r380, %r335, %r337, %r339;
	mov.b32 	%f51, %r382;
	add.f32 	%f52, %f50, %f51;
	mov.b32 	%r383, %f52;
	shfl.sync.bfly.b32 	%r385|%p23, %r383, %r350, %r337, %r339;
	mov.b32 	%f53, %r385;
	add.f32 	%f54, %f52, %f53;
	st.local.f32 	[%rd3+4], %f54;
	st.shared.f32 	[%r57], %f54;
	bar.sync 	0;
	@%p1 bra 	$L__BB126_13;

	ld.shared.f32 	%f55, [%r4];
	mov.b32 	%r386, %f55;
	mov.u32 	%r387, 31;
	mov.u32 	%r388, 16;
	mov.u32 	%r389, -1;
	shfl.sync.bfly.b32 	%r390|%p24, %r386, %r388, %r387, %r389;
	mov.b32 	%f56, %r390;
	add.f32 	%f57, %f55, %f56;
	mov.b32 	%r391, %f57;
	mov.u32 	%r392, 8;
	shfl.sync.bfly.b32 	%r393|%p25, %r391, %r392, %r387, %r389;
	mov.b32 	%f58, %r393;
	add.f32 	%f59, %f57, %f58;
	mov.b32 	%r394, %f59;
	mov.u32 	%r395, 4;
	shfl.sync.bfly.b32 	%r396|%p26, %r394, %r395, %r387, %r389;
	mov.b32 	%f60, %r396;
	add.f32 	%f61, %f59, %f60;
	mov.b32 	%r397, %f61;
	mov.u32 	%r398, 2;
	shfl.sync.bfly.b32 	%r399|%p27, %r397, %r398, %r387, %r389;
	mov.b32 	%f62, %r399;
	add.f32 	%f63, %f61, %f62;
	mov.b32 	%r400, %f63;
	mov.u32 	%r401, 1;
	shfl.sync.bfly.b32 	%r402|%p28, %r400, %r401, %r387, %r389;
	mov.b32 	%f64, %r402;
	add.f32 	%f65, %f63, %f64;
	st.local.f32 	[%rd3+4], %f65;

$L__BB126_13:
	bar.sync 	0;
	mov.b32 	%r403, %f2;
	mov.u32 	%r404, 31;
	mov.u32 	%r405, 16;
	mov.u32 	%r406, -1;
	shfl.sync.bfly.b32 	%r407|%p30, %r403, %r405, %r404, %r406;
	mov.b32 	%f66, %r407;
	add.f32 	%f67, %f2, %f66;
	mov.b32 	%r408, %f67;
	mov.u32 	%r409, 8;
	shfl.sync.bfly.b32 	%r410|%p31, %r408, %r409, %r404, %r406;
	mov.b32 	%f68, %r410;
	add.f32 	%f69, %f67, %f68;
	mov.b32 	%r411, %f69;
	mov.u32 	%r412, 4;
	shfl.sync.bfly.b32 	%r413|%p32, %r411, %r412, %r404, %r406;
	mov.b32 	%f70, %r413;
	add.f32 	%f71, %f69, %f70;
	mov.b32 	%r414, %f71;
	mov.u32 	%r415, 2;
	shfl.sync.bfly.b32 	%r416|%p33, %r414, %r415, %r404, %r406;
	mov.b32 	%f72, %r416;
	add.f32 	%f73, %f71, %f72;
	mov.b32 	%r417, %f73;
	mov.u32 	%r418, 1;
	shfl.sync.bfly.b32 	%r419|%p34, %r417, %r418, %r404, %r406;
	mov.b32 	%f74, %r419;
	add.f32 	%f75, %f73, %f74;
	st.local.f32 	[%rd3+8], %f75;
	st.shared.f32 	[%r57], %f75;
	bar.sync 	0;
	@%p1 bra 	$L__BB126_15;

	ld.shared.f32 	%f76, [%r4];
	mov.b32 	%r420, %f76;
	shfl.sync.bfly.b32 	%r424|%p35, %r420, %r405, %r404, %r406;
	mov.b32 	%f77, %r424;
	add.f32 	%f78, %f76, %f77;
	mov.b32 	%r425, %f78;
	shfl.sync.bfly.b32 	%r427|%p36, %r425, %r409, %r404, %r406;
	mov.b32 	%f79, %r427;
	add.f32 	%f80, %f78, %f79;
	mov.b32 	%r428, %f80;
	shfl.sync.bfly.b32 	%r430|%p37, %r428, %r412, %r404, %r406;
	mov.b32 	%f81, %r430;
	add.f32 	%f82, %f80, %f81;
	mov.b32 	%r431, %f82;
	shfl.sync.bfly.b32 	%r433|%p38, %r431, %r415, %r404, %r406;
	mov.b32 	%f83, %r433;
	add.f32 	%f84, %f82, %f83;
	mov.b32 	%r434, %f84;
	shfl.sync.bfly.b32 	%r436|%p39, %r434, %r418, %r404, %r406;
	mov.b32 	%f85, %r436;
	add.f32 	%f86, %f84, %f85;
	st.local.f32 	[%rd3+8], %f86;

$L__BB126_15:
	bar.sync 	0;
	mov.b32 	%r437, %f3;
	shfl.sync.bfly.b32 	%r441|%p41, %r437, %r405, %r404, %r406;
	mov.b32 	%f87, %r441;
	add.f32 	%f88, %f3, %f87;
	mov.b32 	%r442, %f88;
	shfl.sync.bfly.b32 	%r444|%p42, %r442, %r409, %r404, %r406;
	mov.b32 	%f89, %r444;
	add.f32 	%f90, %f88, %f89;
	mov.b32 	%r445, %f90;
	shfl.sync.bfly.b32 	%r447|%p43, %r445, %r412, %r404, %r406;
	mov.b32 	%f91, %r447;
	add.f32 	%f92, %f90, %f91;
	mov.b32 	%r448, %f92;
	shfl.sync.bfly.b32 	%r450|%p44, %r448, %r415, %r404, %r406;
	mov.b32 	%f93, %r450;
	add.f32 	%f94, %f92, %f93;
	mov.b32 	%r451, %f94;
	shfl.sync.bfly.b32 	%r453|%p45, %r451, %r418, %r404, %r406;
	mov.b32 	%f95, %r453;
	add.f32 	%f96, %f94, %f95;
	st.local.f32 	[%rd3+12], %f96;
	st.shared.f32 	[%r57], %f96;
	bar.sync 	0;
	@%p1 bra 	$L__BB126_17;

	ld.shared.f32 	%f97, [%r4];
	mov.b32 	%r454, %f97;
	mov.u32 	%r455, 31;
	mov.u32 	%r456, 16;
	mov.u32 	%r457, -1;
	shfl.sync.bfly.b32 	%r458|%p46, %r454, %r456, %r455, %r457;
	mov.b32 	%f98, %r458;
	add.f32 	%f99, %f97, %f98;
	mov.b32 	%r459, %f99;
	mov.u32 	%r460, 8;
	shfl.sync.bfly.b32 	%r461|%p47, %r459, %r460, %r455, %r457;
	mov.b32 	%f100, %r461;
	add.f32 	%f101, %f99, %f100;
	mov.b32 	%r462, %f101;
	mov.u32 	%r463, 4;
	shfl.sync.bfly.b32 	%r464|%p48, %r462, %r463, %r455, %r457;
	mov.b32 	%f102, %r464;
	add.f32 	%f103, %f101, %f102;
	mov.b32 	%r465, %f103;
	mov.u32 	%r466, 2;
	shfl.sync.bfly.b32 	%r467|%p49, %r465, %r466, %r455, %r457;
	mov.b32 	%f104, %r467;
	add.f32 	%f105, %f103, %f104;
	mov.b32 	%r468, %f105;
	mov.u32 	%r469, 1;
	shfl.sync.bfly.b32 	%r470|%p50, %r468, %r469, %r455, %r457;
	mov.b32 	%f106, %r470;
	add.f32 	%f107, %f105, %f106;
	st.local.f32 	[%rd3+12], %f107;

$L__BB126_17:
	bar.sync 	0;
	mov.b32 	%r471, %f4;
	mov.u32 	%r472, 31;
	mov.u32 	%r473, 16;
	mov.u32 	%r474, -1;
	shfl.sync.bfly.b32 	%r475|%p52, %r471, %r473, %r472, %r474;
	mov.b32 	%f108, %r475;
	add.f32 	%f109, %f4, %f108;
	mov.b32 	%r476, %f109;
	mov.u32 	%r477, 8;
	shfl.sync.bfly.b32 	%r478|%p53, %r476, %r477, %r472, %r474;
	mov.b32 	%f110, %r478;
	add.f32 	%f111, %f109, %f110;
	mov.b32 	%r479, %f111;
	mov.u32 	%r480, 4;
	shfl.sync.bfly.b32 	%r481|%p54, %r479, %r480, %r472, %r474;
	mov.b32 	%f112, %r481;
	add.f32 	%f113, %f111, %f112;
	mov.b32 	%r482, %f113;
	mov.u32 	%r483, 2;
	shfl.sync.bfly.b32 	%r484|%p55, %r482, %r483, %r472, %r474;
	mov.b32 	%f114, %r484;
	add.f32 	%f115, %f113, %f114;
	mov.b32 	%r485, %f115;
	mov.u32 	%r486, 1;
	shfl.sync.bfly.b32 	%r487|%p56, %r485, %r486, %r472, %r474;
	mov.b32 	%f116, %r487;
	add.f32 	%f117, %f115, %f116;
	st.local.f32 	[%rd3+16], %f117;
	st.shared.f32 	[%r57], %f117;
	bar.sync 	0;
	@%p1 bra 	$L__BB126_19;

	ld.shared.f32 	%f118, [%r4];
	mov.b32 	%r488, %f118;
	shfl.sync.bfly.b32 	%r492|%p57, %r488, %r473, %r472, %r474;
	mov.b32 	%f119, %r492;
	add.f32 	%f120, %f118, %f119;
	mov.b32 	%r493, %f120;
	shfl.sync.bfly.b32 	%r495|%p58, %r493, %r477, %r472, %r474;
	mov.b32 	%f121, %r495;
	add.f32 	%f122, %f120, %f121;
	mov.b32 	%r496, %f122;
	shfl.sync.bfly.b32 	%r498|%p59, %r496, %r480, %r472, %r474;
	mov.b32 	%f123, %r498;
	add.f32 	%f124, %f122, %f123;
	mov.b32 	%r499, %f124;
	shfl.sync.bfly.b32 	%r501|%p60, %r499, %r483, %r472, %r474;
	mov.b32 	%f125, %r501;
	add.f32 	%f126, %f124, %f125;
	mov.b32 	%r502, %f126;
	shfl.sync.bfly.b32 	%r504|%p61, %r502, %r486, %r472, %r474;
	mov.b32 	%f127, %r504;
	add.f32 	%f128, %f126, %f127;
	st.local.f32 	[%rd3+16], %f128;

$L__BB126_19:
	bar.sync 	0;
	mov.b32 	%r505, %f5;
	shfl.sync.bfly.b32 	%r509|%p63, %r505, %r473, %r472, %r474;
	mov.b32 	%f129, %r509;
	add.f32 	%f130, %f5, %f129;
	mov.b32 	%r510, %f130;
	shfl.sync.bfly.b32 	%r512|%p64, %r510, %r477, %r472, %r474;
	mov.b32 	%f131, %r512;
	add.f32 	%f132, %f130, %f131;
	mov.b32 	%r513, %f132;
	shfl.sync.bfly.b32 	%r515|%p65, %r513, %r480, %r472, %r474;
	mov.b32 	%f133, %r515;
	add.f32 	%f134, %f132, %f133;
	mov.b32 	%r516, %f134;
	shfl.sync.bfly.b32 	%r518|%p66, %r516, %r483, %r472, %r474;
	mov.b32 	%f135, %r518;
	add.f32 	%f136, %f134, %f135;
	mov.b32 	%r519, %f136;
	shfl.sync.bfly.b32 	%r521|%p67, %r519, %r486, %r472, %r474;
	mov.b32 	%f137, %r521;
	add.f32 	%f138, %f136, %f137;
	st.local.f32 	[%rd3+20], %f138;
	st.shared.f32 	[%r57], %f138;
	bar.sync 	0;
	@%p1 bra 	$L__BB126_21;

	ld.shared.f32 	%f139, [%r4];
	mov.b32 	%r522, %f139;
	mov.u32 	%r523, 31;
	mov.u32 	%r524, 16;
	mov.u32 	%r525, -1;
	shfl.sync.bfly.b32 	%r526|%p68, %r522, %r524, %r523, %r525;
	mov.b32 	%f140, %r526;
	add.f32 	%f141, %f139, %f140;
	mov.b32 	%r527, %f141;
	mov.u32 	%r528, 8;
	shfl.sync.bfly.b32 	%r529|%p69, %r527, %r528, %r523, %r525;
	mov.b32 	%f142, %r529;
	add.f32 	%f143, %f141, %f142;
	mov.b32 	%r530, %f143;
	mov.u32 	%r531, 4;
	shfl.sync.bfly.b32 	%r532|%p70, %r530, %r531, %r523, %r525;
	mov.b32 	%f144, %r532;
	add.f32 	%f145, %f143, %f144;
	mov.b32 	%r533, %f145;
	mov.u32 	%r534, 2;
	shfl.sync.bfly.b32 	%r535|%p71, %r533, %r534, %r523, %r525;
	mov.b32 	%f146, %r535;
	add.f32 	%f147, %f145, %f146;
	mov.b32 	%r536, %f147;
	mov.u32 	%r537, 1;
	shfl.sync.bfly.b32 	%r538|%p72, %r536, %r537, %r523, %r525;
	mov.b32 	%f148, %r538;
	add.f32 	%f149, %f147, %f148;
	st.local.f32 	[%rd3+20], %f149;

$L__BB126_21:
	bar.sync 	0;
	mov.b32 	%r539, %f6;
	mov.u32 	%r540, 31;
	mov.u32 	%r541, 16;
	mov.u32 	%r542, -1;
	shfl.sync.bfly.b32 	%r543|%p74, %r539, %r541, %r540, %r542;
	mov.b32 	%f150, %r543;
	add.f32 	%f151, %f6, %f150;
	mov.b32 	%r544, %f151;
	mov.u32 	%r545, 8;
	shfl.sync.bfly.b32 	%r546|%p75, %r544, %r545, %r540, %r542;
	mov.b32 	%f152, %r546;
	add.f32 	%f153, %f151, %f152;
	mov.b32 	%r547, %f153;
	mov.u32 	%r548, 4;
	shfl.sync.bfly.b32 	%r549|%p76, %r547, %r548, %r540, %r542;
	mov.b32 	%f154, %r549;
	add.f32 	%f155, %f153, %f154;
	mov.b32 	%r550, %f155;
	mov.u32 	%r551, 2;
	shfl.sync.bfly.b32 	%r552|%p77, %r550, %r551, %r540, %r542;
	mov.b32 	%f156, %r552;
	add.f32 	%f157, %f155, %f156;
	mov.b32 	%r553, %f157;
	mov.u32 	%r554, 1;
	shfl.sync.bfly.b32 	%r555|%p78, %r553, %r554, %r540, %r542;
	mov.b32 	%f158, %r555;
	add.f32 	%f159, %f157, %f158;
	st.local.f32 	[%rd3+24], %f159;
	st.shared.f32 	[%r57], %f159;
	bar.sync 	0;
	@%p1 bra 	$L__BB126_23;

	ld.shared.f32 	%f160, [%r4];
	mov.b32 	%r556, %f160;
	shfl.sync.bfly.b32 	%r560|%p79, %r556, %r541, %r540, %r542;
	mov.b32 	%f161, %r560;
	add.f32 	%f162, %f160, %f161;
	mov.b32 	%r561, %f162;
	shfl.sync.bfly.b32 	%r563|%p80, %r561, %r545, %r540, %r542;
	mov.b32 	%f163, %r563;
	add.f32 	%f164, %f162, %f163;
	mov.b32 	%r564, %f164;
	shfl.sync.bfly.b32 	%r566|%p81, %r564, %r548, %r540, %r542;
	mov.b32 	%f165, %r566;
	add.f32 	%f166, %f164, %f165;
	mov.b32 	%r567, %f166;
	shfl.sync.bfly.b32 	%r569|%p82, %r567, %r551, %r540, %r542;
	mov.b32 	%f167, %r569;
	add.f32 	%f168, %f166, %f167;
	mov.b32 	%r570, %f168;
	shfl.sync.bfly.b32 	%r572|%p83, %r570, %r554, %r540, %r542;
	mov.b32 	%f169, %r572;
	add.f32 	%f170, %f168, %f169;
	st.local.f32 	[%rd3+24], %f170;

$L__BB126_23:
	bar.sync 	0;
	setp.gt.s32 	%p84, %r3, 6;
	@%p84 bra 	$L__BB126_25;

	mad.lo.s32 	%r573, %r3, %r60, %r2;
	cvt.s64.s32 	%rd69, %r573;
	mul.lo.s32 	%r574, %r1, %r61;
	cvt.s64.s32 	%rd70, %r574;
	add.s64 	%rd71, %rd70, %rd69;
	mul.wide.s32 	%rd72, %r3, 4;
	add.s64 	%rd73, %rd3, %rd72;
	ld.local.f32 	%f171, [%rd73];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs3, %f171;}

	// end inline asm
	cvta.to.global.u64 	%rd74, %rd29;
	shl.b64 	%rd75, %rd71, 1;
	add.s64 	%rd76, %rd74, %rd75;
	st.global.u16 	[%rd76], %rs3;

$L__BB126_25:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_8_bs_256
.visible .entry ggml_matvec_f16_ncols_8_bs_256(
	.param .u64 ggml_matvec_f16_ncols_8_bs_256_param_0,
	.param .u64 ggml_matvec_f16_ncols_8_bs_256_param_1,
	.param .u64 ggml_matvec_f16_ncols_8_bs_256_param_2,
	.param .u32 ggml_matvec_f16_ncols_8_bs_256_param_3,
	.param .u32 ggml_matvec_f16_ncols_8_bs_256_param_4,
	.param .u32 ggml_matvec_f16_ncols_8_bs_256_param_5,
	.param .u32 ggml_matvec_f16_ncols_8_bs_256_param_6,
	.param .u32 ggml_matvec_f16_ncols_8_bs_256_param_7,
	.param .u32 ggml_matvec_f16_ncols_8_bs_256_param_8,
	.param .u32 ggml_matvec_f16_ncols_8_bs_256_param_9,
	.param .u32 ggml_matvec_f16_ncols_8_bs_256_param_10,
	.param .u32 ggml_matvec_f16_ncols_8_bs_256_param_11
)
{
	.local .align 16 .b8 	__local_depot127[32];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<96>;
	.reg .b16 	%rs<4>;
	.reg .f32 	%f<196>;
	.reg .b32 	%r<687>;
	.reg .b64 	%rd<85>;


	mov.u64 	%SPL, __local_depot127;
	ld.param.u64 	%rd31, [ggml_matvec_f16_ncols_8_bs_256_param_0];
	ld.param.u64 	%rd32, [ggml_matvec_f16_ncols_8_bs_256_param_1];
	ld.param.u64 	%rd30, [ggml_matvec_f16_ncols_8_bs_256_param_2];
	ld.param.u32 	%r64, [ggml_matvec_f16_ncols_8_bs_256_param_3];
	ld.param.u32 	%r68, [ggml_matvec_f16_ncols_8_bs_256_param_5];
	ld.param.u32 	%r65, [ggml_matvec_f16_ncols_8_bs_256_param_6];
	ld.param.u32 	%r66, [ggml_matvec_f16_ncols_8_bs_256_param_7];
	ld.param.u32 	%r69, [ggml_matvec_f16_ncols_8_bs_256_param_8];
	ld.param.u32 	%r70, [ggml_matvec_f16_ncols_8_bs_256_param_9];
	ld.param.u32 	%r71, [ggml_matvec_f16_ncols_8_bs_256_param_10];
	ld.param.u32 	%r67, [ggml_matvec_f16_ncols_8_bs_256_param_11];
	cvta.to.global.u64 	%rd84, %rd32;
	cvta.to.global.u64 	%rd2, %rd31;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r72, %r1, %r69;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r73, %r2, %r68;
	mad.lo.s32 	%r74, %r72, %r70, %r73;
	cvt.s64.s32 	%rd4, %r74;
	mul.lo.s32 	%r75, %r1, %r71;
	cvt.s64.s32 	%rd5, %r75;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r76, %r3, 2;
	mov.u32 	%r77, data_mmv;
	add.s32 	%r4, %r77, %r76;
	@%p1 bra 	$L__BB127_2;

	mov.u32 	%r78, 0;
	st.shared.u32 	[%r4], %r78;

$L__BB127_2:
	bar.sync 	0;
	mov.f32 	%f9, 0f00000000;
	st.local.v4.f32 	[%rd3], {%f9, %f9, %f9, %f9};
	st.local.v4.f32 	[%rd3+16], {%f9, %f9, %f9, %f9};
	mov.u32 	%r679, 0;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs2, %f9;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f9;}

	// end inline asm
	mov.b32 	%r686, {%rs1, %rs2};
	setp.ge.s32 	%p2, %r3, %r64;
	mov.u32 	%r680, %r679;
	mov.u32 	%r681, %r679;
	mov.u32 	%r682, %r679;
	mov.u32 	%r683, %r679;
	mov.u32 	%r684, %r679;
	mov.u32 	%r685, %r679;
	@%p2 bra 	$L__BB127_9;

	not.b32 	%r93, %r3;
	add.s32 	%r6, %r93, %r64;
	shr.u32 	%r94, %r6, 8;
	add.s32 	%r95, %r94, 1;
	and.b32  	%r660, %r95, 3;
	setp.eq.s32 	%p3, %r660, 0;
	mov.u32 	%r679, 0;
	mov.u32 	%r669, %r3;
	@%p3 bra 	$L__BB127_6;

	shl.b32 	%r103, %r65, 1;
	add.s32 	%r104, %r3, %r103;
	mul.wide.s32 	%rd34, %r104, 4;
	shl.b64 	%rd35, %rd5, 1;
	add.s64 	%rd7, %rd34, %rd35;
	mul.wide.s32 	%rd36, %r3, 4;
	mul.wide.s32 	%rd8, %r65, 4;
	add.s64 	%rd37, %rd36, %rd8;
	add.s64 	%rd9, %rd37, %rd35;
	add.s64 	%rd10, %rd36, %rd35;
	mul.wide.s32 	%rd38, %r3, 2;
	add.s64 	%rd39, %rd38, %rd4;
	shl.b64 	%rd40, %rd39, 1;
	add.s64 	%rd81, %rd2, %rd40;
	mov.u32 	%r679, 0;
	mov.u64 	%rd82, %rd84;
	mov.u32 	%r680, %r679;
	mov.u32 	%r681, %r679;
	mov.u32 	%r682, %r679;
	mov.u32 	%r683, %r679;
	mov.u32 	%r684, %r679;
	mov.u32 	%r685, %r679;
	mov.u32 	%r669, %r3;

$L__BB127_5:
	.pragma "nounroll";
	ld.global.nc.u32 	%r106, [%rd81];
	add.s64 	%rd41, %rd82, %rd10;
	ld.global.nc.u32 	%r107, [%rd41];
	// begin inline asm
	{mul.f16x2 %r105,%r106,%r107;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r686,%r686,%r105;
}
	// end inline asm
	add.s64 	%rd42, %rd82, %rd9;
	ld.global.nc.u32 	%r113, [%rd42];
	// begin inline asm
	{mul.f16x2 %r111,%r106,%r113;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r685,%r685,%r111;
}
	// end inline asm
	add.s64 	%rd43, %rd82, %rd7;
	ld.global.nc.u32 	%r119, [%rd43];
	// begin inline asm
	{mul.f16x2 %r117,%r106,%r119;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r684,%r684,%r117;
}
	// end inline asm
	add.s64 	%rd44, %rd43, %rd8;
	ld.global.nc.u32 	%r125, [%rd44];
	// begin inline asm
	{mul.f16x2 %r123,%r106,%r125;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r683,%r683,%r123;
}
	// end inline asm
	add.s64 	%rd45, %rd44, %rd8;
	ld.global.nc.u32 	%r131, [%rd45];
	// begin inline asm
	{mul.f16x2 %r129,%r106,%r131;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r682,%r682,%r129;
}
	// end inline asm
	add.s64 	%rd46, %rd45, %rd8;
	ld.global.nc.u32 	%r137, [%rd46];
	// begin inline asm
	{mul.f16x2 %r135,%r106,%r137;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r681,%r681,%r135;
}
	// end inline asm
	add.s64 	%rd47, %rd46, %rd8;
	ld.global.nc.u32 	%r143, [%rd47];
	// begin inline asm
	{mul.f16x2 %r141,%r106,%r143;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r680,%r680,%r141;
}
	// end inline asm
	add.s64 	%rd48, %rd47, %rd8;
	ld.global.nc.u32 	%r149, [%rd48];
	// begin inline asm
	{mul.f16x2 %r147,%r106,%r149;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r679,%r679,%r147;
}
	// end inline asm
	add.s32 	%r669, %r669, 256;
	add.s64 	%rd82, %rd82, 1024;
	add.s64 	%rd81, %rd81, 1024;
	add.s32 	%r660, %r660, -1;
	setp.ne.s32 	%p4, %r660, 0;
	@%p4 bra 	$L__BB127_5;

$L__BB127_6:
	setp.lt.u32 	%p5, %r6, 768;
	@%p5 bra 	$L__BB127_9;

	add.s32 	%r153, %r669, %r65;
	shl.b32 	%r154, %r65, 1;
	add.s32 	%r155, %r669, %r154;
	mad.lo.s32 	%r156, %r65, 3, %r669;
	shl.b32 	%r157, %r65, 2;
	add.s32 	%r158, %r669, %r157;
	mad.lo.s32 	%r159, %r65, 5, %r669;
	mad.lo.s32 	%r160, %r65, 6, %r669;
	mad.lo.s32 	%r161, %r65, 7, %r669;
	add.s32 	%r162, %r153, 256;
	mul.wide.s32 	%rd49, %r162, 4;
	shl.b64 	%rd50, %rd5, 1;
	add.s64 	%rd16, %rd49, %rd50;
	mul.wide.s32 	%rd51, %r155, 4;
	add.s64 	%rd17, %rd51, %rd50;
	mul.wide.s32 	%rd52, %r156, 4;
	add.s64 	%rd18, %rd52, %rd50;
	mul.wide.s32 	%rd53, %r158, 4;
	add.s64 	%rd19, %rd53, %rd50;
	mul.wide.s32 	%rd54, %r159, 4;
	add.s64 	%rd20, %rd54, %rd50;
	mul.wide.s32 	%rd55, %r160, 4;
	add.s64 	%rd21, %rd55, %rd50;
	mul.wide.s32 	%rd56, %r161, 4;
	add.s64 	%rd22, %rd56, %rd50;
	mul.wide.s32 	%rd57, %r669, 2;
	add.s64 	%rd58, %rd57, %rd4;
	shl.b64 	%rd59, %rd58, 1;
	add.s64 	%rd60, %rd2, %rd59;
	add.s64 	%rd83, %rd60, 2048;
	mul.wide.s32 	%rd61, %r669, 4;
	add.s64 	%rd24, %rd61, %rd50;
	mul.wide.s32 	%rd62, %r65, 4;
	add.s64 	%rd63, %rd61, %rd62;
	add.s64 	%rd25, %rd63, %rd50;

$L__BB127_8:
	ld.global.nc.u32 	%r164, [%rd83+-2048];
	add.s64 	%rd64, %rd84, %rd24;
	ld.global.nc.u32 	%r165, [%rd64];
	// begin inline asm
	{mul.f16x2 %r163,%r164,%r165;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r166,%r686,%r163;
}
	// end inline asm
	add.s64 	%rd65, %rd84, %rd25;
	ld.global.nc.u32 	%r171, [%rd65];
	// begin inline asm
	{mul.f16x2 %r169,%r164,%r171;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r172,%r685,%r169;
}
	// end inline asm
	add.s64 	%rd66, %rd84, %rd17;
	ld.global.nc.u32 	%r177, [%rd66];
	// begin inline asm
	{mul.f16x2 %r175,%r164,%r177;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r178,%r684,%r175;
}
	// end inline asm
	add.s64 	%rd67, %rd84, %rd18;
	ld.global.nc.u32 	%r183, [%rd67];
	// begin inline asm
	{mul.f16x2 %r181,%r164,%r183;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r184,%r683,%r181;
}
	// end inline asm
	add.s64 	%rd68, %rd84, %rd19;
	ld.global.nc.u32 	%r189, [%rd68];
	// begin inline asm
	{mul.f16x2 %r187,%r164,%r189;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r190,%r682,%r187;
}
	// end inline asm
	add.s64 	%rd69, %rd84, %rd20;
	ld.global.nc.u32 	%r195, [%rd69];
	// begin inline asm
	{mul.f16x2 %r193,%r164,%r195;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r196,%r681,%r193;
}
	// end inline asm
	add.s64 	%rd70, %rd84, %rd21;
	ld.global.nc.u32 	%r201, [%rd70];
	// begin inline asm
	{mul.f16x2 %r199,%r164,%r201;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r202,%r680,%r199;
}
	// end inline asm
	add.s64 	%rd71, %rd84, %rd22;
	ld.global.nc.u32 	%r207, [%rd71];
	// begin inline asm
	{mul.f16x2 %r205,%r164,%r207;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r208,%r679,%r205;
}
	// end inline asm
	ld.global.nc.u32 	%r212, [%rd83+-1024];
	ld.global.nc.u32 	%r213, [%rd64+1024];
	// begin inline asm
	{mul.f16x2 %r211,%r212,%r213;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r214,%r166,%r211;
}
	// end inline asm
	add.s64 	%rd72, %rd84, %rd16;
	ld.global.nc.u32 	%r219, [%rd72];
	// begin inline asm
	{mul.f16x2 %r217,%r212,%r219;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r220,%r172,%r217;
}
	// end inline asm
	ld.global.nc.u32 	%r225, [%rd66+1024];
	// begin inline asm
	{mul.f16x2 %r223,%r212,%r225;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r226,%r178,%r223;
}
	// end inline asm
	ld.global.nc.u32 	%r231, [%rd67+1024];
	// begin inline asm
	{mul.f16x2 %r229,%r212,%r231;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r232,%r184,%r229;
}
	// end inline asm
	ld.global.nc.u32 	%r237, [%rd68+1024];
	// begin inline asm
	{mul.f16x2 %r235,%r212,%r237;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r238,%r190,%r235;
}
	// end inline asm
	ld.global.nc.u32 	%r243, [%rd69+1024];
	// begin inline asm
	{mul.f16x2 %r241,%r212,%r243;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r244,%r196,%r241;
}
	// end inline asm
	ld.global.nc.u32 	%r249, [%rd70+1024];
	// begin inline asm
	{mul.f16x2 %r247,%r212,%r249;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r250,%r202,%r247;
}
	// end inline asm
	ld.global.nc.u32 	%r255, [%rd71+1024];
	// begin inline asm
	{mul.f16x2 %r253,%r212,%r255;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r256,%r208,%r253;
}
	// end inline asm
	ld.global.nc.u32 	%r260, [%rd83];
	ld.global.nc.u32 	%r261, [%rd64+2048];
	// begin inline asm
	{mul.f16x2 %r259,%r260,%r261;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r262,%r214,%r259;
}
	// end inline asm
	ld.global.nc.u32 	%r267, [%rd72+1024];
	// begin inline asm
	{mul.f16x2 %r265,%r260,%r267;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r268,%r220,%r265;
}
	// end inline asm
	ld.global.nc.u32 	%r273, [%rd66+2048];
	// begin inline asm
	{mul.f16x2 %r271,%r260,%r273;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r274,%r226,%r271;
}
	// end inline asm
	ld.global.nc.u32 	%r279, [%rd67+2048];
	// begin inline asm
	{mul.f16x2 %r277,%r260,%r279;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r280,%r232,%r277;
}
	// end inline asm
	ld.global.nc.u32 	%r285, [%rd68+2048];
	// begin inline asm
	{mul.f16x2 %r283,%r260,%r285;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r286,%r238,%r283;
}
	// end inline asm
	ld.global.nc.u32 	%r291, [%rd69+2048];
	// begin inline asm
	{mul.f16x2 %r289,%r260,%r291;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r292,%r244,%r289;
}
	// end inline asm
	ld.global.nc.u32 	%r297, [%rd70+2048];
	// begin inline asm
	{mul.f16x2 %r295,%r260,%r297;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r298,%r250,%r295;
}
	// end inline asm
	ld.global.nc.u32 	%r303, [%rd71+2048];
	// begin inline asm
	{mul.f16x2 %r301,%r260,%r303;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r304,%r256,%r301;
}
	// end inline asm
	ld.global.nc.u32 	%r308, [%rd83+1024];
	ld.global.nc.u32 	%r309, [%rd64+3072];
	// begin inline asm
	{mul.f16x2 %r307,%r308,%r309;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r686,%r262,%r307;
}
	// end inline asm
	ld.global.nc.u32 	%r315, [%rd72+2048];
	// begin inline asm
	{mul.f16x2 %r313,%r308,%r315;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r685,%r268,%r313;
}
	// end inline asm
	ld.global.nc.u32 	%r321, [%rd66+3072];
	// begin inline asm
	{mul.f16x2 %r319,%r308,%r321;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r684,%r274,%r319;
}
	// end inline asm
	ld.global.nc.u32 	%r327, [%rd67+3072];
	// begin inline asm
	{mul.f16x2 %r325,%r308,%r327;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r683,%r280,%r325;
}
	// end inline asm
	ld.global.nc.u32 	%r333, [%rd68+3072];
	// begin inline asm
	{mul.f16x2 %r331,%r308,%r333;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r682,%r286,%r331;
}
	// end inline asm
	ld.global.nc.u32 	%r339, [%rd69+3072];
	// begin inline asm
	{mul.f16x2 %r337,%r308,%r339;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r681,%r292,%r337;
}
	// end inline asm
	ld.global.nc.u32 	%r345, [%rd70+3072];
	// begin inline asm
	{mul.f16x2 %r343,%r308,%r345;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r680,%r298,%r343;
}
	// end inline asm
	ld.global.nc.u32 	%r351, [%rd71+3072];
	// begin inline asm
	{mul.f16x2 %r349,%r308,%r351;
}
	// end inline asm
	// begin inline asm
	{add.f16x2 %r679,%r304,%r349;
}
	// end inline asm
	add.s64 	%rd84, %rd84, 4096;
	add.s64 	%rd83, %rd83, 4096;
	add.s32 	%r669, %r669, 1024;
	setp.lt.s32 	%p6, %r669, %r64;
	@%p6 bra 	$L__BB127_8;

$L__BB127_9:
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r686;
  cvt.f32.f16 %f10, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r686;
  cvt.f32.f16 %f11, high;}

	// end inline asm
	add.f32 	%f26, %f10, %f11;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r685;
  cvt.f32.f16 %f12, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r685;
  cvt.f32.f16 %f13, high;}

	// end inline asm
	add.f32 	%f1, %f12, %f13;
	st.local.f32 	[%rd3+4], %f1;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r684;
  cvt.f32.f16 %f14, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r684;
  cvt.f32.f16 %f15, high;}

	// end inline asm
	add.f32 	%f2, %f14, %f15;
	st.local.f32 	[%rd3+8], %f2;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r683;
  cvt.f32.f16 %f16, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r683;
  cvt.f32.f16 %f17, high;}

	// end inline asm
	add.f32 	%f3, %f16, %f17;
	st.local.f32 	[%rd3+12], %f3;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r682;
  cvt.f32.f16 %f18, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r682;
  cvt.f32.f16 %f19, high;}

	// end inline asm
	add.f32 	%f4, %f18, %f19;
	st.local.f32 	[%rd3+16], %f4;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r681;
  cvt.f32.f16 %f20, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r681;
  cvt.f32.f16 %f21, high;}

	// end inline asm
	add.f32 	%f5, %f20, %f21;
	st.local.f32 	[%rd3+20], %f5;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r680;
  cvt.f32.f16 %f22, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r680;
  cvt.f32.f16 %f23, high;}

	// end inline asm
	add.f32 	%f6, %f22, %f23;
	st.local.f32 	[%rd3+24], %f6;
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r679;
  cvt.f32.f16 %f24, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r679;
  cvt.f32.f16 %f25, high;}

	// end inline asm
	add.f32 	%f7, %f24, %f25;
	st.local.f32 	[%rd3+28], %f7;
	shr.s32 	%r371, %r3, 31;
	shr.u32 	%r372, %r371, 27;
	add.s32 	%r373, %r3, %r372;
	shr.s32 	%r374, %r373, 5;
	shl.b32 	%r375, %r374, 2;
	add.s32 	%r63, %r77, %r375;
	mov.u32 	%r377, 2;
	mov.b32 	%r378, %f26;
	mov.u32 	%r379, 31;
	mov.u32 	%r380, 16;
	mov.u32 	%r381, -1;
	shfl.sync.bfly.b32 	%r382|%p7, %r378, %r380, %r379, %r381;
	mov.b32 	%f27, %r382;
	add.f32 	%f28, %f26, %f27;
	mov.b32 	%r383, %f28;
	mov.u32 	%r384, 8;
	shfl.sync.bfly.b32 	%r385|%p8, %r383, %r384, %r379, %r381;
	mov.b32 	%f29, %r385;
	add.f32 	%f30, %f28, %f29;
	mov.b32 	%r386, %f30;
	mov.u32 	%r387, 4;
	shfl.sync.bfly.b32 	%r388|%p9, %r386, %r387, %r379, %r381;
	mov.b32 	%f31, %r388;
	add.f32 	%f32, %f30, %f31;
	mov.b32 	%r389, %f32;
	shfl.sync.bfly.b32 	%r390|%p10, %r389, %r377, %r379, %r381;
	mov.b32 	%f33, %r390;
	add.f32 	%f34, %f32, %f33;
	mov.b32 	%r391, %f34;
	mov.u32 	%r392, 1;
	shfl.sync.bfly.b32 	%r393|%p11, %r391, %r392, %r379, %r381;
	mov.b32 	%f35, %r393;
	add.f32 	%f36, %f34, %f35;
	st.local.f32 	[%rd3], %f36;
	st.shared.f32 	[%r63], %f36;
	bar.sync 	0;
	@%p1 bra 	$L__BB127_11;

	ld.shared.f32 	%f37, [%r4];
	mov.b32 	%r394, %f37;
	shfl.sync.bfly.b32 	%r398|%p13, %r394, %r380, %r379, %r381;
	mov.b32 	%f38, %r398;
	add.f32 	%f39, %f37, %f38;
	mov.b32 	%r399, %f39;
	shfl.sync.bfly.b32 	%r401|%p14, %r399, %r384, %r379, %r381;
	mov.b32 	%f40, %r401;
	add.f32 	%f41, %f39, %f40;
	mov.b32 	%r402, %f41;
	shfl.sync.bfly.b32 	%r404|%p15, %r402, %r387, %r379, %r381;
	mov.b32 	%f42, %r404;
	add.f32 	%f43, %f41, %f42;
	mov.b32 	%r405, %f43;
	shfl.sync.bfly.b32 	%r407|%p16, %r405, %r377, %r379, %r381;
	mov.b32 	%f44, %r407;
	add.f32 	%f45, %f43, %f44;
	mov.b32 	%r408, %f45;
	shfl.sync.bfly.b32 	%r410|%p17, %r408, %r392, %r379, %r381;
	mov.b32 	%f46, %r410;
	add.f32 	%f47, %f45, %f46;
	st.local.f32 	[%rd3], %f47;

$L__BB127_11:
	bar.sync 	0;
	mov.b32 	%r411, %f1;
	shfl.sync.bfly.b32 	%r415|%p19, %r411, %r380, %r379, %r381;
	mov.b32 	%f48, %r415;
	add.f32 	%f49, %f1, %f48;
	mov.b32 	%r416, %f49;
	shfl.sync.bfly.b32 	%r418|%p20, %r416, %r384, %r379, %r381;
	mov.b32 	%f50, %r418;
	add.f32 	%f51, %f49, %f50;
	mov.b32 	%r419, %f51;
	shfl.sync.bfly.b32 	%r421|%p21, %r419, %r387, %r379, %r381;
	mov.b32 	%f52, %r421;
	add.f32 	%f53, %f51, %f52;
	mov.b32 	%r422, %f53;
	shfl.sync.bfly.b32 	%r424|%p22, %r422, %r377, %r379, %r381;
	mov.b32 	%f54, %r424;
	add.f32 	%f55, %f53, %f54;
	mov.b32 	%r425, %f55;
	shfl.sync.bfly.b32 	%r427|%p23, %r425, %r392, %r379, %r381;
	mov.b32 	%f56, %r427;
	add.f32 	%f57, %f55, %f56;
	st.local.f32 	[%rd3+4], %f57;
	st.shared.f32 	[%r63], %f57;
	bar.sync 	0;
	@%p1 bra 	$L__BB127_13;

	ld.shared.f32 	%f58, [%r4];
	mov.b32 	%r428, %f58;
	mov.u32 	%r429, 31;
	mov.u32 	%r430, 16;
	mov.u32 	%r431, -1;
	shfl.sync.bfly.b32 	%r432|%p24, %r428, %r430, %r429, %r431;
	mov.b32 	%f59, %r432;
	add.f32 	%f60, %f58, %f59;
	mov.b32 	%r433, %f60;
	mov.u32 	%r434, 8;
	shfl.sync.bfly.b32 	%r435|%p25, %r433, %r434, %r429, %r431;
	mov.b32 	%f61, %r435;
	add.f32 	%f62, %f60, %f61;
	mov.b32 	%r436, %f62;
	mov.u32 	%r437, 4;
	shfl.sync.bfly.b32 	%r438|%p26, %r436, %r437, %r429, %r431;
	mov.b32 	%f63, %r438;
	add.f32 	%f64, %f62, %f63;
	mov.b32 	%r439, %f64;
	mov.u32 	%r440, 2;
	shfl.sync.bfly.b32 	%r441|%p27, %r439, %r440, %r429, %r431;
	mov.b32 	%f65, %r441;
	add.f32 	%f66, %f64, %f65;
	mov.b32 	%r442, %f66;
	mov.u32 	%r443, 1;
	shfl.sync.bfly.b32 	%r444|%p28, %r442, %r443, %r429, %r431;
	mov.b32 	%f67, %r444;
	add.f32 	%f68, %f66, %f67;
	st.local.f32 	[%rd3+4], %f68;

$L__BB127_13:
	bar.sync 	0;
	mov.b32 	%r445, %f2;
	mov.u32 	%r446, 31;
	mov.u32 	%r447, 16;
	mov.u32 	%r448, -1;
	shfl.sync.bfly.b32 	%r449|%p30, %r445, %r447, %r446, %r448;
	mov.b32 	%f69, %r449;
	add.f32 	%f70, %f2, %f69;
	mov.b32 	%r450, %f70;
	mov.u32 	%r451, 8;
	shfl.sync.bfly.b32 	%r452|%p31, %r450, %r451, %r446, %r448;
	mov.b32 	%f71, %r452;
	add.f32 	%f72, %f70, %f71;
	mov.b32 	%r453, %f72;
	mov.u32 	%r454, 4;
	shfl.sync.bfly.b32 	%r455|%p32, %r453, %r454, %r446, %r448;
	mov.b32 	%f73, %r455;
	add.f32 	%f74, %f72, %f73;
	mov.b32 	%r456, %f74;
	mov.u32 	%r457, 2;
	shfl.sync.bfly.b32 	%r458|%p33, %r456, %r457, %r446, %r448;
	mov.b32 	%f75, %r458;
	add.f32 	%f76, %f74, %f75;
	mov.b32 	%r459, %f76;
	mov.u32 	%r460, 1;
	shfl.sync.bfly.b32 	%r461|%p34, %r459, %r460, %r446, %r448;
	mov.b32 	%f77, %r461;
	add.f32 	%f78, %f76, %f77;
	st.local.f32 	[%rd3+8], %f78;
	st.shared.f32 	[%r63], %f78;
	bar.sync 	0;
	@%p1 bra 	$L__BB127_15;

	ld.shared.f32 	%f79, [%r4];
	mov.b32 	%r462, %f79;
	shfl.sync.bfly.b32 	%r466|%p35, %r462, %r447, %r446, %r448;
	mov.b32 	%f80, %r466;
	add.f32 	%f81, %f79, %f80;
	mov.b32 	%r467, %f81;
	shfl.sync.bfly.b32 	%r469|%p36, %r467, %r451, %r446, %r448;
	mov.b32 	%f82, %r469;
	add.f32 	%f83, %f81, %f82;
	mov.b32 	%r470, %f83;
	shfl.sync.bfly.b32 	%r472|%p37, %r470, %r454, %r446, %r448;
	mov.b32 	%f84, %r472;
	add.f32 	%f85, %f83, %f84;
	mov.b32 	%r473, %f85;
	shfl.sync.bfly.b32 	%r475|%p38, %r473, %r457, %r446, %r448;
	mov.b32 	%f86, %r475;
	add.f32 	%f87, %f85, %f86;
	mov.b32 	%r476, %f87;
	shfl.sync.bfly.b32 	%r478|%p39, %r476, %r460, %r446, %r448;
	mov.b32 	%f88, %r478;
	add.f32 	%f89, %f87, %f88;
	st.local.f32 	[%rd3+8], %f89;

$L__BB127_15:
	bar.sync 	0;
	mov.b32 	%r479, %f3;
	shfl.sync.bfly.b32 	%r483|%p41, %r479, %r447, %r446, %r448;
	mov.b32 	%f90, %r483;
	add.f32 	%f91, %f3, %f90;
	mov.b32 	%r484, %f91;
	shfl.sync.bfly.b32 	%r486|%p42, %r484, %r451, %r446, %r448;
	mov.b32 	%f92, %r486;
	add.f32 	%f93, %f91, %f92;
	mov.b32 	%r487, %f93;
	shfl.sync.bfly.b32 	%r489|%p43, %r487, %r454, %r446, %r448;
	mov.b32 	%f94, %r489;
	add.f32 	%f95, %f93, %f94;
	mov.b32 	%r490, %f95;
	shfl.sync.bfly.b32 	%r492|%p44, %r490, %r457, %r446, %r448;
	mov.b32 	%f96, %r492;
	add.f32 	%f97, %f95, %f96;
	mov.b32 	%r493, %f97;
	shfl.sync.bfly.b32 	%r495|%p45, %r493, %r460, %r446, %r448;
	mov.b32 	%f98, %r495;
	add.f32 	%f99, %f97, %f98;
	st.local.f32 	[%rd3+12], %f99;
	st.shared.f32 	[%r63], %f99;
	bar.sync 	0;
	@%p1 bra 	$L__BB127_17;

	ld.shared.f32 	%f100, [%r4];
	mov.b32 	%r496, %f100;
	mov.u32 	%r497, 31;
	mov.u32 	%r498, 16;
	mov.u32 	%r499, -1;
	shfl.sync.bfly.b32 	%r500|%p46, %r496, %r498, %r497, %r499;
	mov.b32 	%f101, %r500;
	add.f32 	%f102, %f100, %f101;
	mov.b32 	%r501, %f102;
	mov.u32 	%r502, 8;
	shfl.sync.bfly.b32 	%r503|%p47, %r501, %r502, %r497, %r499;
	mov.b32 	%f103, %r503;
	add.f32 	%f104, %f102, %f103;
	mov.b32 	%r504, %f104;
	mov.u32 	%r505, 4;
	shfl.sync.bfly.b32 	%r506|%p48, %r504, %r505, %r497, %r499;
	mov.b32 	%f105, %r506;
	add.f32 	%f106, %f104, %f105;
	mov.b32 	%r507, %f106;
	mov.u32 	%r508, 2;
	shfl.sync.bfly.b32 	%r509|%p49, %r507, %r508, %r497, %r499;
	mov.b32 	%f107, %r509;
	add.f32 	%f108, %f106, %f107;
	mov.b32 	%r510, %f108;
	mov.u32 	%r511, 1;
	shfl.sync.bfly.b32 	%r512|%p50, %r510, %r511, %r497, %r499;
	mov.b32 	%f109, %r512;
	add.f32 	%f110, %f108, %f109;
	st.local.f32 	[%rd3+12], %f110;

$L__BB127_17:
	bar.sync 	0;
	mov.b32 	%r513, %f4;
	mov.u32 	%r514, 31;
	mov.u32 	%r515, 16;
	mov.u32 	%r516, -1;
	shfl.sync.bfly.b32 	%r517|%p52, %r513, %r515, %r514, %r516;
	mov.b32 	%f111, %r517;
	add.f32 	%f112, %f4, %f111;
	mov.b32 	%r518, %f112;
	mov.u32 	%r519, 8;
	shfl.sync.bfly.b32 	%r520|%p53, %r518, %r519, %r514, %r516;
	mov.b32 	%f113, %r520;
	add.f32 	%f114, %f112, %f113;
	mov.b32 	%r521, %f114;
	mov.u32 	%r522, 4;
	shfl.sync.bfly.b32 	%r523|%p54, %r521, %r522, %r514, %r516;
	mov.b32 	%f115, %r523;
	add.f32 	%f116, %f114, %f115;
	mov.b32 	%r524, %f116;
	mov.u32 	%r525, 2;
	shfl.sync.bfly.b32 	%r526|%p55, %r524, %r525, %r514, %r516;
	mov.b32 	%f117, %r526;
	add.f32 	%f118, %f116, %f117;
	mov.b32 	%r527, %f118;
	mov.u32 	%r528, 1;
	shfl.sync.bfly.b32 	%r529|%p56, %r527, %r528, %r514, %r516;
	mov.b32 	%f119, %r529;
	add.f32 	%f120, %f118, %f119;
	st.local.f32 	[%rd3+16], %f120;
	st.shared.f32 	[%r63], %f120;
	bar.sync 	0;
	@%p1 bra 	$L__BB127_19;

	ld.shared.f32 	%f121, [%r4];
	mov.b32 	%r530, %f121;
	shfl.sync.bfly.b32 	%r534|%p57, %r530, %r515, %r514, %r516;
	mov.b32 	%f122, %r534;
	add.f32 	%f123, %f121, %f122;
	mov.b32 	%r535, %f123;
	shfl.sync.bfly.b32 	%r537|%p58, %r535, %r519, %r514, %r516;
	mov.b32 	%f124, %r537;
	add.f32 	%f125, %f123, %f124;
	mov.b32 	%r538, %f125;
	shfl.sync.bfly.b32 	%r540|%p59, %r538, %r522, %r514, %r516;
	mov.b32 	%f126, %r540;
	add.f32 	%f127, %f125, %f126;
	mov.b32 	%r541, %f127;
	shfl.sync.bfly.b32 	%r543|%p60, %r541, %r525, %r514, %r516;
	mov.b32 	%f128, %r543;
	add.f32 	%f129, %f127, %f128;
	mov.b32 	%r544, %f129;
	shfl.sync.bfly.b32 	%r546|%p61, %r544, %r528, %r514, %r516;
	mov.b32 	%f130, %r546;
	add.f32 	%f131, %f129, %f130;
	st.local.f32 	[%rd3+16], %f131;

$L__BB127_19:
	bar.sync 	0;
	mov.b32 	%r547, %f5;
	shfl.sync.bfly.b32 	%r551|%p63, %r547, %r515, %r514, %r516;
	mov.b32 	%f132, %r551;
	add.f32 	%f133, %f5, %f132;
	mov.b32 	%r552, %f133;
	shfl.sync.bfly.b32 	%r554|%p64, %r552, %r519, %r514, %r516;
	mov.b32 	%f134, %r554;
	add.f32 	%f135, %f133, %f134;
	mov.b32 	%r555, %f135;
	shfl.sync.bfly.b32 	%r557|%p65, %r555, %r522, %r514, %r516;
	mov.b32 	%f136, %r557;
	add.f32 	%f137, %f135, %f136;
	mov.b32 	%r558, %f137;
	shfl.sync.bfly.b32 	%r560|%p66, %r558, %r525, %r514, %r516;
	mov.b32 	%f138, %r560;
	add.f32 	%f139, %f137, %f138;
	mov.b32 	%r561, %f139;
	shfl.sync.bfly.b32 	%r563|%p67, %r561, %r528, %r514, %r516;
	mov.b32 	%f140, %r563;
	add.f32 	%f141, %f139, %f140;
	st.local.f32 	[%rd3+20], %f141;
	st.shared.f32 	[%r63], %f141;
	bar.sync 	0;
	@%p1 bra 	$L__BB127_21;

	ld.shared.f32 	%f142, [%r4];
	mov.b32 	%r564, %f142;
	mov.u32 	%r565, 31;
	mov.u32 	%r566, 16;
	mov.u32 	%r567, -1;
	shfl.sync.bfly.b32 	%r568|%p68, %r564, %r566, %r565, %r567;
	mov.b32 	%f143, %r568;
	add.f32 	%f144, %f142, %f143;
	mov.b32 	%r569, %f144;
	mov.u32 	%r570, 8;
	shfl.sync.bfly.b32 	%r571|%p69, %r569, %r570, %r565, %r567;
	mov.b32 	%f145, %r571;
	add.f32 	%f146, %f144, %f145;
	mov.b32 	%r572, %f146;
	mov.u32 	%r573, 4;
	shfl.sync.bfly.b32 	%r574|%p70, %r572, %r573, %r565, %r567;
	mov.b32 	%f147, %r574;
	add.f32 	%f148, %f146, %f147;
	mov.b32 	%r575, %f148;
	mov.u32 	%r576, 2;
	shfl.sync.bfly.b32 	%r577|%p71, %r575, %r576, %r565, %r567;
	mov.b32 	%f149, %r577;
	add.f32 	%f150, %f148, %f149;
	mov.b32 	%r578, %f150;
	mov.u32 	%r579, 1;
	shfl.sync.bfly.b32 	%r580|%p72, %r578, %r579, %r565, %r567;
	mov.b32 	%f151, %r580;
	add.f32 	%f152, %f150, %f151;
	st.local.f32 	[%rd3+20], %f152;

$L__BB127_21:
	bar.sync 	0;
	mov.b32 	%r581, %f6;
	mov.u32 	%r582, 31;
	mov.u32 	%r583, 16;
	mov.u32 	%r584, -1;
	shfl.sync.bfly.b32 	%r585|%p74, %r581, %r583, %r582, %r584;
	mov.b32 	%f153, %r585;
	add.f32 	%f154, %f6, %f153;
	mov.b32 	%r586, %f154;
	mov.u32 	%r587, 8;
	shfl.sync.bfly.b32 	%r588|%p75, %r586, %r587, %r582, %r584;
	mov.b32 	%f155, %r588;
	add.f32 	%f156, %f154, %f155;
	mov.b32 	%r589, %f156;
	mov.u32 	%r590, 4;
	shfl.sync.bfly.b32 	%r591|%p76, %r589, %r590, %r582, %r584;
	mov.b32 	%f157, %r591;
	add.f32 	%f158, %f156, %f157;
	mov.b32 	%r592, %f158;
	mov.u32 	%r593, 2;
	shfl.sync.bfly.b32 	%r594|%p77, %r592, %r593, %r582, %r584;
	mov.b32 	%f159, %r594;
	add.f32 	%f160, %f158, %f159;
	mov.b32 	%r595, %f160;
	mov.u32 	%r596, 1;
	shfl.sync.bfly.b32 	%r597|%p78, %r595, %r596, %r582, %r584;
	mov.b32 	%f161, %r597;
	add.f32 	%f162, %f160, %f161;
	st.local.f32 	[%rd3+24], %f162;
	st.shared.f32 	[%r63], %f162;
	bar.sync 	0;
	@%p1 bra 	$L__BB127_23;

	ld.shared.f32 	%f163, [%r4];
	mov.b32 	%r598, %f163;
	shfl.sync.bfly.b32 	%r602|%p79, %r598, %r583, %r582, %r584;
	mov.b32 	%f164, %r602;
	add.f32 	%f165, %f163, %f164;
	mov.b32 	%r603, %f165;
	shfl.sync.bfly.b32 	%r605|%p80, %r603, %r587, %r582, %r584;
	mov.b32 	%f166, %r605;
	add.f32 	%f167, %f165, %f166;
	mov.b32 	%r606, %f167;
	shfl.sync.bfly.b32 	%r608|%p81, %r606, %r590, %r582, %r584;
	mov.b32 	%f168, %r608;
	add.f32 	%f169, %f167, %f168;
	mov.b32 	%r609, %f169;
	shfl.sync.bfly.b32 	%r611|%p82, %r609, %r593, %r582, %r584;
	mov.b32 	%f170, %r611;
	add.f32 	%f171, %f169, %f170;
	mov.b32 	%r612, %f171;
	shfl.sync.bfly.b32 	%r614|%p83, %r612, %r596, %r582, %r584;
	mov.b32 	%f172, %r614;
	add.f32 	%f173, %f171, %f172;
	st.local.f32 	[%rd3+24], %f173;

$L__BB127_23:
	bar.sync 	0;
	mov.b32 	%r615, %f7;
	shfl.sync.bfly.b32 	%r619|%p85, %r615, %r583, %r582, %r584;
	mov.b32 	%f174, %r619;
	add.f32 	%f175, %f7, %f174;
	mov.b32 	%r620, %f175;
	shfl.sync.bfly.b32 	%r622|%p86, %r620, %r587, %r582, %r584;
	mov.b32 	%f176, %r622;
	add.f32 	%f177, %f175, %f176;
	mov.b32 	%r623, %f177;
	shfl.sync.bfly.b32 	%r625|%p87, %r623, %r590, %r582, %r584;
	mov.b32 	%f178, %r625;
	add.f32 	%f179, %f177, %f178;
	mov.b32 	%r626, %f179;
	shfl.sync.bfly.b32 	%r628|%p88, %r626, %r593, %r582, %r584;
	mov.b32 	%f180, %r628;
	add.f32 	%f181, %f179, %f180;
	mov.b32 	%r629, %f181;
	shfl.sync.bfly.b32 	%r631|%p89, %r629, %r596, %r582, %r584;
	mov.b32 	%f182, %r631;
	add.f32 	%f183, %f181, %f182;
	st.local.f32 	[%rd3+28], %f183;
	st.shared.f32 	[%r63], %f183;
	bar.sync 	0;
	@%p1 bra 	$L__BB127_25;

	ld.shared.f32 	%f184, [%r4];
	mov.b32 	%r632, %f184;
	mov.u32 	%r633, 31;
	mov.u32 	%r634, 16;
	mov.u32 	%r635, -1;
	shfl.sync.bfly.b32 	%r636|%p90, %r632, %r634, %r633, %r635;
	mov.b32 	%f185, %r636;
	add.f32 	%f186, %f184, %f185;
	mov.b32 	%r637, %f186;
	mov.u32 	%r638, 8;
	shfl.sync.bfly.b32 	%r639|%p91, %r637, %r638, %r633, %r635;
	mov.b32 	%f187, %r639;
	add.f32 	%f188, %f186, %f187;
	mov.b32 	%r640, %f188;
	mov.u32 	%r641, 4;
	shfl.sync.bfly.b32 	%r642|%p92, %r640, %r641, %r633, %r635;
	mov.b32 	%f189, %r642;
	add.f32 	%f190, %f188, %f189;
	mov.b32 	%r643, %f190;
	mov.u32 	%r644, 2;
	shfl.sync.bfly.b32 	%r645|%p93, %r643, %r644, %r633, %r635;
	mov.b32 	%f191, %r645;
	add.f32 	%f192, %f190, %f191;
	mov.b32 	%r646, %f192;
	mov.u32 	%r647, 1;
	shfl.sync.bfly.b32 	%r648|%p94, %r646, %r647, %r633, %r635;
	mov.b32 	%f193, %r648;
	add.f32 	%f194, %f192, %f193;
	st.local.f32 	[%rd3+28], %f194;

$L__BB127_25:
	bar.sync 	0;
	setp.gt.s32 	%p95, %r3, 7;
	@%p95 bra 	$L__BB127_27;

	mad.lo.s32 	%r649, %r3, %r66, %r2;
	cvt.s64.s32 	%rd73, %r649;
	mul.lo.s32 	%r650, %r1, %r67;
	cvt.s64.s32 	%rd74, %r650;
	add.s64 	%rd75, %rd74, %rd73;
	mul.wide.s32 	%rd76, %r3, 4;
	add.s64 	%rd77, %rd3, %rd76;
	ld.local.f32 	%f195, [%rd77];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs3, %f195;}

	// end inline asm
	cvta.to.global.u64 	%rd78, %rd30;
	shl.b64 	%rd79, %rd75, 1;
	add.s64 	%rd80, %rd78, %rd79;
	st.global.u16 	[%rd80], %rs3;

$L__BB127_27:
	ret;

}

