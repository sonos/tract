//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-35583870
// Cuda compilation tools, release 12.8, V12.8.93
// Based on NVVM 7.0.1
//

.version 8.7
.target sm_87
.address_size 64

	// .globl	ggml_matvec_f32_ncols_1_bs_32
.extern .shared .align 16 .b8 data_mmv[];

.visible .entry ggml_matvec_f32_ncols_1_bs_32(
	.param .u64 ggml_matvec_f32_ncols_1_bs_32_param_0,
	.param .u64 ggml_matvec_f32_ncols_1_bs_32_param_1,
	.param .u64 ggml_matvec_f32_ncols_1_bs_32_param_2,
	.param .u32 ggml_matvec_f32_ncols_1_bs_32_param_3,
	.param .u32 ggml_matvec_f32_ncols_1_bs_32_param_4,
	.param .u32 ggml_matvec_f32_ncols_1_bs_32_param_5,
	.param .u32 ggml_matvec_f32_ncols_1_bs_32_param_6,
	.param .u32 ggml_matvec_f32_ncols_1_bs_32_param_7,
	.param .u32 ggml_matvec_f32_ncols_1_bs_32_param_8,
	.param .u32 ggml_matvec_f32_ncols_1_bs_32_param_9,
	.param .u32 ggml_matvec_f32_ncols_1_bs_32_param_10,
	.param .u32 ggml_matvec_f32_ncols_1_bs_32_param_11
)
{
	.reg .pred 	%p<12>;
	.reg .f32 	%f<75>;
	.reg .b32 	%r<51>;
	.reg .b64 	%rd<42>;


	ld.param.u64 	%rd18, [ggml_matvec_f32_ncols_1_bs_32_param_0];
	ld.param.u64 	%rd19, [ggml_matvec_f32_ncols_1_bs_32_param_1];
	ld.param.u64 	%rd17, [ggml_matvec_f32_ncols_1_bs_32_param_2];
	ld.param.u32 	%r12, [ggml_matvec_f32_ncols_1_bs_32_param_3];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_1_bs_32_param_5];
	ld.param.u32 	%r13, [ggml_matvec_f32_ncols_1_bs_32_param_7];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_1_bs_32_param_8];
	ld.param.u32 	%r17, [ggml_matvec_f32_ncols_1_bs_32_param_9];
	ld.param.u32 	%r18, [ggml_matvec_f32_ncols_1_bs_32_param_10];
	ld.param.u32 	%r14, [ggml_matvec_f32_ncols_1_bs_32_param_11];
	cvta.to.global.u64 	%rd1, %rd19;
	cvta.to.global.u64 	%rd2, %rd18;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r19, %r1, %r16;
	mov.u32 	%r20, %ctaid.x;
	mul.lo.s32 	%r21, %r20, %r15;
	mad.lo.s32 	%r22, %r19, %r17, %r21;
	cvt.s64.s32 	%rd3, %r22;
	mul.lo.s32 	%r23, %r1, %r18;
	cvt.s64.s32 	%rd4, %r23;
	mov.u32 	%r2, %tid.x;
	setp.ge.s32 	%p1, %r2, %r12;
	mov.f32 	%f74, 0f00000000;
	@%p1 bra 	$L__BB0_7;

	not.b32 	%r24, %r2;
	add.s32 	%r3, %r24, %r12;
	shr.u32 	%r25, %r3, 5;
	add.s32 	%r26, %r25, 1;
	and.b32  	%r48, %r26, 3;
	setp.eq.s32 	%p2, %r48, 0;
	mov.f32 	%f74, 0f00000000;
	mov.u32 	%r49, %r2;
	@%p2 bra 	$L__BB0_4;

	mul.wide.s32 	%rd20, %r2, 2;
	add.s64 	%rd21, %rd20, %rd4;
	shl.b64 	%rd22, %rd21, 2;
	add.s64 	%rd39, %rd1, %rd22;
	add.s64 	%rd23, %rd20, %rd3;
	shl.b64 	%rd24, %rd23, 2;
	add.s64 	%rd38, %rd2, %rd24;
	mov.f32 	%f74, 0f00000000;
	mov.u32 	%r49, %r2;

$L__BB0_3:
	.pragma "nounroll";
	ld.global.nc.v2.f32 	{%f13, %f14}, [%rd38];
	ld.global.nc.v2.f32 	{%f17, %f18}, [%rd39];
	fma.rn.f32 	%f21, %f13, %f17, %f74;
	fma.rn.f32 	%f74, %f14, %f18, %f21;
	add.s32 	%r49, %r49, 32;
	add.s64 	%rd39, %rd39, 256;
	add.s64 	%rd38, %rd38, 256;
	add.s32 	%r48, %r48, -1;
	setp.ne.s32 	%p3, %r48, 0;
	@%p3 bra 	$L__BB0_3;

$L__BB0_4:
	setp.lt.u32 	%p4, %r3, 96;
	@%p4 bra 	$L__BB0_7;

	mul.wide.s32 	%rd25, %r49, 2;
	add.s64 	%rd26, %rd25, %rd3;
	shl.b64 	%rd27, %rd26, 2;
	add.s64 	%rd28, %rd2, %rd27;
	add.s64 	%rd41, %rd28, 512;
	add.s64 	%rd29, %rd25, %rd4;
	shl.b64 	%rd30, %rd29, 2;
	add.s64 	%rd31, %rd1, %rd30;
	add.s64 	%rd40, %rd31, 512;

$L__BB0_6:
	ld.global.nc.v2.f32 	{%f22, %f23}, [%rd41+-512];
	ld.global.nc.v2.f32 	{%f26, %f27}, [%rd40+-512];
	fma.rn.f32 	%f30, %f22, %f26, %f74;
	fma.rn.f32 	%f31, %f23, %f27, %f30;
	ld.global.nc.v2.f32 	{%f32, %f33}, [%rd41+-256];
	ld.global.nc.v2.f32 	{%f36, %f37}, [%rd40+-256];
	fma.rn.f32 	%f40, %f32, %f36, %f31;
	fma.rn.f32 	%f41, %f33, %f37, %f40;
	ld.global.nc.v2.f32 	{%f42, %f43}, [%rd41];
	ld.global.nc.v2.f32 	{%f46, %f47}, [%rd40];
	fma.rn.f32 	%f50, %f42, %f46, %f41;
	fma.rn.f32 	%f51, %f43, %f47, %f50;
	ld.global.nc.v2.f32 	{%f52, %f53}, [%rd41+256];
	ld.global.nc.v2.f32 	{%f56, %f57}, [%rd40+256];
	fma.rn.f32 	%f60, %f52, %f56, %f51;
	fma.rn.f32 	%f74, %f53, %f57, %f60;
	add.s64 	%rd41, %rd41, 1024;
	add.s64 	%rd40, %rd40, 1024;
	add.s32 	%r49, %r49, 128;
	setp.lt.s32 	%p5, %r49, %r12;
	@%p5 bra 	$L__BB0_6;

$L__BB0_7:
	mov.b32 	%r27, %f74;
	mov.u32 	%r28, 31;
	mov.u32 	%r29, 16;
	mov.u32 	%r30, -1;
	shfl.sync.bfly.b32 	%r31|%p6, %r27, %r29, %r28, %r30;
	mov.b32 	%f61, %r31;
	add.f32 	%f62, %f74, %f61;
	mov.b32 	%r32, %f62;
	mov.u32 	%r33, 8;
	shfl.sync.bfly.b32 	%r34|%p7, %r32, %r33, %r28, %r30;
	mov.b32 	%f63, %r34;
	add.f32 	%f64, %f62, %f63;
	mov.b32 	%r35, %f64;
	mov.u32 	%r36, 4;
	shfl.sync.bfly.b32 	%r37|%p8, %r35, %r36, %r28, %r30;
	mov.b32 	%f65, %r37;
	add.f32 	%f66, %f64, %f65;
	mov.b32 	%r38, %f66;
	mov.u32 	%r39, 2;
	shfl.sync.bfly.b32 	%r40|%p9, %r38, %r39, %r28, %r30;
	mov.b32 	%f67, %r40;
	add.f32 	%f68, %f66, %f67;
	mov.b32 	%r41, %f68;
	mov.u32 	%r42, 1;
	shfl.sync.bfly.b32 	%r43|%p10, %r41, %r42, %r28, %r30;
	mov.b32 	%f69, %r43;
	add.f32 	%f8, %f68, %f69;
	setp.gt.s32 	%p11, %r2, 0;
	@%p11 bra 	$L__BB0_9;

	mad.lo.s32 	%r45, %r2, %r13, %r20;
	cvt.s64.s32 	%rd32, %r45;
	mul.lo.s32 	%r46, %r1, %r14;
	cvt.s64.s32 	%rd33, %r46;
	add.s64 	%rd34, %rd33, %rd32;
	cvta.to.global.u64 	%rd35, %rd17;
	shl.b64 	%rd36, %rd34, 2;
	add.s64 	%rd37, %rd35, %rd36;
	st.global.f32 	[%rd37], %f8;

$L__BB0_9:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_2_bs_32
.visible .entry ggml_matvec_f32_ncols_2_bs_32(
	.param .u64 ggml_matvec_f32_ncols_2_bs_32_param_0,
	.param .u64 ggml_matvec_f32_ncols_2_bs_32_param_1,
	.param .u64 ggml_matvec_f32_ncols_2_bs_32_param_2,
	.param .u32 ggml_matvec_f32_ncols_2_bs_32_param_3,
	.param .u32 ggml_matvec_f32_ncols_2_bs_32_param_4,
	.param .u32 ggml_matvec_f32_ncols_2_bs_32_param_5,
	.param .u32 ggml_matvec_f32_ncols_2_bs_32_param_6,
	.param .u32 ggml_matvec_f32_ncols_2_bs_32_param_7,
	.param .u32 ggml_matvec_f32_ncols_2_bs_32_param_8,
	.param .u32 ggml_matvec_f32_ncols_2_bs_32_param_9,
	.param .u32 ggml_matvec_f32_ncols_2_bs_32_param_10,
	.param .u32 ggml_matvec_f32_ncols_2_bs_32_param_11
)
{
	.local .align 8 .b8 	__local_depot1[8];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<17>;
	.reg .f32 	%f<124>;
	.reg .b32 	%r<61>;
	.reg .b64 	%rd<63>;


	mov.u64 	%SPL, __local_depot1;
	ld.param.u64 	%rd26, [ggml_matvec_f32_ncols_2_bs_32_param_0];
	ld.param.u64 	%rd27, [ggml_matvec_f32_ncols_2_bs_32_param_1];
	ld.param.u64 	%rd25, [ggml_matvec_f32_ncols_2_bs_32_param_2];
	ld.param.u32 	%r13, [ggml_matvec_f32_ncols_2_bs_32_param_3];
	ld.param.u32 	%r17, [ggml_matvec_f32_ncols_2_bs_32_param_5];
	ld.param.u32 	%r14, [ggml_matvec_f32_ncols_2_bs_32_param_6];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_2_bs_32_param_7];
	ld.param.u32 	%r18, [ggml_matvec_f32_ncols_2_bs_32_param_8];
	ld.param.u32 	%r19, [ggml_matvec_f32_ncols_2_bs_32_param_9];
	ld.param.u32 	%r20, [ggml_matvec_f32_ncols_2_bs_32_param_10];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_2_bs_32_param_11];
	cvta.to.global.u64 	%rd1, %rd27;
	cvta.to.global.u64 	%rd2, %rd26;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r21, %r1, %r18;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r22, %r2, %r17;
	mad.lo.s32 	%r23, %r21, %r19, %r22;
	cvt.s64.s32 	%rd4, %r23;
	mul.lo.s32 	%r24, %r1, %r20;
	cvt.s64.s32 	%rd5, %r24;
	mov.f32 	%f122, 0f00000000;
	st.local.v2.f32 	[%rd3], {%f122, %f122};
	mov.u32 	%r3, %tid.x;
	setp.ge.s32 	%p1, %r3, %r13;
	mov.f32 	%f123, %f122;
	@%p1 bra 	$L__BB1_9;

	not.b32 	%r25, %r3;
	add.s32 	%r4, %r25, %r13;
	shr.u32 	%r26, %r4, 5;
	add.s32 	%r27, %r26, 1;
	and.b32  	%r58, %r27, 3;
	setp.eq.s32 	%p2, %r58, 0;
	mov.f32 	%f122, 0f00000000;
	mov.u32 	%r59, %r3;
	@%p2 bra 	$L__BB1_5;

	mul.wide.s32 	%rd29, %r14, 2;
	mul.wide.s32 	%rd30, %r3, 2;
	add.s64 	%rd31, %rd29, %rd30;
	add.s64 	%rd32, %rd31, %rd5;
	shl.b64 	%rd33, %rd32, 2;
	add.s64 	%rd59, %rd1, %rd33;
	add.s64 	%rd34, %rd30, %rd5;
	shl.b64 	%rd35, %rd34, 2;
	add.s64 	%rd58, %rd1, %rd35;
	add.s64 	%rd36, %rd30, %rd4;
	shl.b64 	%rd37, %rd36, 2;
	add.s64 	%rd57, %rd2, %rd37;
	mov.f32 	%f122, 0f00000000;
	mov.f32 	%f123, %f122;
	mov.u32 	%r59, %r3;

$L__BB1_3:
	.pragma "nounroll";
	ld.global.nc.v2.f32 	{%f19, %f20}, [%rd57];
	ld.global.nc.v2.f32 	{%f23, %f24}, [%rd58];
	fma.rn.f32 	%f27, %f19, %f23, %f123;
	fma.rn.f32 	%f123, %f20, %f24, %f27;
	ld.global.nc.v2.f32 	{%f28, %f29}, [%rd59];
	fma.rn.f32 	%f32, %f19, %f28, %f122;
	fma.rn.f32 	%f122, %f20, %f29, %f32;
	add.s32 	%r59, %r59, 32;
	add.s64 	%rd59, %rd59, 256;
	add.s64 	%rd58, %rd58, 256;
	add.s64 	%rd57, %rd57, 256;
	add.s32 	%r58, %r58, -1;
	setp.ne.s32 	%p3, %r58, 0;
	@%p3 bra 	$L__BB1_3;

	st.local.v2.f32 	[%rd3], {%f123, %f122};

$L__BB1_5:
	setp.lt.u32 	%p4, %r4, 96;
	@%p4 bra 	$L__BB1_9;

	mul.wide.s32 	%rd38, %r59, 2;
	add.s64 	%rd39, %rd38, %rd4;
	shl.b64 	%rd40, %rd39, 2;
	add.s64 	%rd41, %rd2, %rd40;
	add.s64 	%rd62, %rd41, 512;
	add.s64 	%rd42, %rd38, %rd5;
	shl.b64 	%rd43, %rd42, 2;
	add.s64 	%rd44, %rd1, %rd43;
	add.s64 	%rd61, %rd44, 768;
	mul.wide.s32 	%rd45, %r14, 2;
	add.s64 	%rd46, %rd42, %rd45;
	shl.b64 	%rd47, %rd46, 2;
	add.s64 	%rd48, %rd1, %rd47;
	add.s64 	%rd60, %rd48, 512;

$L__BB1_7:
	ld.global.nc.v2.f32 	{%f33, %f34}, [%rd62+-512];
	ld.global.nc.v2.f32 	{%f37, %f38}, [%rd61+-768];
	fma.rn.f32 	%f41, %f33, %f37, %f123;
	fma.rn.f32 	%f42, %f34, %f38, %f41;
	ld.global.nc.v2.f32 	{%f43, %f44}, [%rd60+-512];
	fma.rn.f32 	%f47, %f33, %f43, %f122;
	fma.rn.f32 	%f48, %f34, %f44, %f47;
	ld.global.nc.v2.f32 	{%f49, %f50}, [%rd62+-256];
	ld.global.nc.v2.f32 	{%f53, %f54}, [%rd61+-512];
	fma.rn.f32 	%f57, %f49, %f53, %f42;
	fma.rn.f32 	%f58, %f50, %f54, %f57;
	ld.global.nc.v2.f32 	{%f59, %f60}, [%rd60+-256];
	fma.rn.f32 	%f63, %f49, %f59, %f48;
	fma.rn.f32 	%f64, %f50, %f60, %f63;
	ld.global.nc.v2.f32 	{%f65, %f66}, [%rd62];
	ld.global.nc.v2.f32 	{%f69, %f70}, [%rd61+-256];
	fma.rn.f32 	%f73, %f65, %f69, %f58;
	fma.rn.f32 	%f74, %f66, %f70, %f73;
	ld.global.nc.v2.f32 	{%f75, %f76}, [%rd60];
	fma.rn.f32 	%f79, %f65, %f75, %f64;
	fma.rn.f32 	%f80, %f66, %f76, %f79;
	ld.global.nc.v2.f32 	{%f81, %f82}, [%rd62+256];
	ld.global.nc.v2.f32 	{%f85, %f86}, [%rd61];
	fma.rn.f32 	%f89, %f81, %f85, %f74;
	fma.rn.f32 	%f123, %f82, %f86, %f89;
	ld.global.nc.v2.f32 	{%f90, %f91}, [%rd60+256];
	fma.rn.f32 	%f94, %f81, %f90, %f80;
	fma.rn.f32 	%f122, %f82, %f91, %f94;
	add.s64 	%rd62, %rd62, 1024;
	add.s64 	%rd61, %rd61, 1024;
	add.s64 	%rd60, %rd60, 1024;
	add.s32 	%r59, %r59, 128;
	setp.lt.s32 	%p5, %r59, %r13;
	@%p5 bra 	$L__BB1_7;

	st.local.v2.f32 	[%rd3], {%f123, %f122};

$L__BB1_9:
	mov.b32 	%r28, %f123;
	mov.u32 	%r29, 31;
	mov.u32 	%r30, 16;
	mov.u32 	%r31, -1;
	shfl.sync.bfly.b32 	%r32|%p6, %r28, %r30, %r29, %r31;
	mov.b32 	%f95, %r32;
	add.f32 	%f96, %f123, %f95;
	mov.b32 	%r33, %f96;
	mov.u32 	%r34, 8;
	shfl.sync.bfly.b32 	%r35|%p7, %r33, %r34, %r29, %r31;
	mov.b32 	%f97, %r35;
	add.f32 	%f98, %f96, %f97;
	mov.b32 	%r36, %f98;
	mov.u32 	%r37, 4;
	shfl.sync.bfly.b32 	%r38|%p8, %r36, %r37, %r29, %r31;
	mov.b32 	%f99, %r38;
	add.f32 	%f100, %f98, %f99;
	mov.b32 	%r39, %f100;
	mov.u32 	%r40, 2;
	shfl.sync.bfly.b32 	%r41|%p9, %r39, %r40, %r29, %r31;
	mov.b32 	%f101, %r41;
	add.f32 	%f102, %f100, %f101;
	mov.b32 	%r42, %f102;
	mov.u32 	%r43, 1;
	shfl.sync.bfly.b32 	%r44|%p10, %r42, %r43, %r29, %r31;
	mov.b32 	%f103, %r44;
	add.f32 	%f104, %f102, %f103;
	st.local.f32 	[%rd3], %f104;
	mov.b32 	%r45, %f122;
	shfl.sync.bfly.b32 	%r46|%p11, %r45, %r30, %r29, %r31;
	mov.b32 	%f105, %r46;
	add.f32 	%f106, %f122, %f105;
	mov.b32 	%r47, %f106;
	shfl.sync.bfly.b32 	%r48|%p12, %r47, %r34, %r29, %r31;
	mov.b32 	%f107, %r48;
	add.f32 	%f108, %f106, %f107;
	mov.b32 	%r49, %f108;
	shfl.sync.bfly.b32 	%r50|%p13, %r49, %r37, %r29, %r31;
	mov.b32 	%f109, %r50;
	add.f32 	%f110, %f108, %f109;
	mov.b32 	%r51, %f110;
	shfl.sync.bfly.b32 	%r52|%p14, %r51, %r40, %r29, %r31;
	mov.b32 	%f111, %r52;
	add.f32 	%f112, %f110, %f111;
	mov.b32 	%r53, %f112;
	shfl.sync.bfly.b32 	%r54|%p15, %r53, %r43, %r29, %r31;
	mov.b32 	%f113, %r54;
	add.f32 	%f114, %f112, %f113;
	st.local.f32 	[%rd3+4], %f114;
	setp.gt.s32 	%p16, %r3, 1;
	@%p16 bra 	$L__BB1_11;

	mul.wide.s32 	%rd49, %r3, 4;
	add.s64 	%rd50, %rd3, %rd49;
	ld.local.f32 	%f115, [%rd50];
	mad.lo.s32 	%r55, %r3, %r15, %r2;
	cvt.s64.s32 	%rd51, %r55;
	mul.lo.s32 	%r56, %r1, %r16;
	cvt.s64.s32 	%rd52, %r56;
	add.s64 	%rd53, %rd52, %rd51;
	cvta.to.global.u64 	%rd54, %rd25;
	shl.b64 	%rd55, %rd53, 2;
	add.s64 	%rd56, %rd54, %rd55;
	st.global.f32 	[%rd56], %f115;

$L__BB1_11:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_3_bs_32
.visible .entry ggml_matvec_f32_ncols_3_bs_32(
	.param .u64 ggml_matvec_f32_ncols_3_bs_32_param_0,
	.param .u64 ggml_matvec_f32_ncols_3_bs_32_param_1,
	.param .u64 ggml_matvec_f32_ncols_3_bs_32_param_2,
	.param .u32 ggml_matvec_f32_ncols_3_bs_32_param_3,
	.param .u32 ggml_matvec_f32_ncols_3_bs_32_param_4,
	.param .u32 ggml_matvec_f32_ncols_3_bs_32_param_5,
	.param .u32 ggml_matvec_f32_ncols_3_bs_32_param_6,
	.param .u32 ggml_matvec_f32_ncols_3_bs_32_param_7,
	.param .u32 ggml_matvec_f32_ncols_3_bs_32_param_8,
	.param .u32 ggml_matvec_f32_ncols_3_bs_32_param_9,
	.param .u32 ggml_matvec_f32_ncols_3_bs_32_param_10,
	.param .u32 ggml_matvec_f32_ncols_3_bs_32_param_11
)
{
	.local .align 4 .b8 	__local_depot2[12];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<22>;
	.reg .f32 	%f<175>;
	.reg .b32 	%r<78>;
	.reg .b64 	%rd<72>;


	mov.u64 	%SPL, __local_depot2;
	ld.param.u64 	%rd29, [ggml_matvec_f32_ncols_3_bs_32_param_0];
	ld.param.u64 	%rd30, [ggml_matvec_f32_ncols_3_bs_32_param_1];
	ld.param.u64 	%rd28, [ggml_matvec_f32_ncols_3_bs_32_param_2];
	ld.param.u32 	%r13, [ggml_matvec_f32_ncols_3_bs_32_param_3];
	ld.param.u32 	%r17, [ggml_matvec_f32_ncols_3_bs_32_param_5];
	ld.param.u32 	%r14, [ggml_matvec_f32_ncols_3_bs_32_param_6];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_3_bs_32_param_7];
	ld.param.u32 	%r18, [ggml_matvec_f32_ncols_3_bs_32_param_8];
	ld.param.u32 	%r19, [ggml_matvec_f32_ncols_3_bs_32_param_9];
	ld.param.u32 	%r20, [ggml_matvec_f32_ncols_3_bs_32_param_10];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_3_bs_32_param_11];
	cvta.to.global.u64 	%rd71, %rd30;
	cvta.to.global.u64 	%rd2, %rd29;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r21, %r1, %r18;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r22, %r2, %r17;
	mad.lo.s32 	%r23, %r21, %r19, %r22;
	cvt.s64.s32 	%rd4, %r23;
	mul.lo.s32 	%r24, %r1, %r20;
	cvt.s64.s32 	%rd5, %r24;
	mov.f32 	%f172, 0f00000000;
	mov.u32 	%r25, 0;
	st.local.u32 	[%rd3], %r25;
	st.local.u32 	[%rd3+4], %r25;
	st.local.u32 	[%rd3+8], %r25;
	mov.u32 	%r3, %tid.x;
	setp.ge.s32 	%p1, %r3, %r13;
	mov.f32 	%f173, %f172;
	mov.f32 	%f174, %f172;
	@%p1 bra 	$L__BB2_9;

	not.b32 	%r26, %r3;
	add.s32 	%r4, %r26, %r13;
	shr.u32 	%r27, %r4, 5;
	add.s32 	%r28, %r27, 1;
	and.b32  	%r75, %r28, 3;
	setp.eq.s32 	%p2, %r75, 0;
	mov.f32 	%f172, 0f00000000;
	mov.u32 	%r76, %r3;
	@%p2 bra 	$L__BB2_5;

	shl.b32 	%r29, %r14, 1;
	add.s32 	%r30, %r3, %r29;
	mul.wide.s32 	%rd32, %r30, 2;
	add.s64 	%rd33, %rd32, %rd5;
	shl.b64 	%rd34, %rd33, 2;
	add.s64 	%rd69, %rd71, %rd34;
	mul.wide.s32 	%rd35, %r14, 2;
	mul.wide.s32 	%rd36, %r3, 2;
	add.s64 	%rd37, %rd35, %rd36;
	add.s64 	%rd38, %rd37, %rd5;
	shl.b64 	%rd39, %rd38, 2;
	add.s64 	%rd68, %rd71, %rd39;
	add.s64 	%rd40, %rd36, %rd5;
	shl.b64 	%rd41, %rd40, 2;
	add.s64 	%rd67, %rd71, %rd41;
	add.s64 	%rd42, %rd36, %rd4;
	shl.b64 	%rd43, %rd42, 2;
	add.s64 	%rd66, %rd2, %rd43;
	mov.f32 	%f172, 0f00000000;
	mov.f32 	%f173, %f172;
	mov.f32 	%f174, %f172;
	mov.u32 	%r76, %r3;

$L__BB2_3:
	.pragma "nounroll";
	ld.global.nc.v2.f32 	{%f28, %f29}, [%rd66];
	ld.global.nc.v2.f32 	{%f32, %f33}, [%rd67];
	fma.rn.f32 	%f36, %f28, %f32, %f174;
	fma.rn.f32 	%f174, %f29, %f33, %f36;
	ld.global.nc.v2.f32 	{%f37, %f38}, [%rd68];
	fma.rn.f32 	%f41, %f28, %f37, %f173;
	fma.rn.f32 	%f173, %f29, %f38, %f41;
	ld.global.nc.v2.f32 	{%f42, %f43}, [%rd69];
	fma.rn.f32 	%f46, %f28, %f42, %f172;
	fma.rn.f32 	%f172, %f29, %f43, %f46;
	add.s32 	%r76, %r76, 32;
	add.s64 	%rd69, %rd69, 256;
	add.s64 	%rd68, %rd68, 256;
	add.s64 	%rd67, %rd67, 256;
	add.s64 	%rd66, %rd66, 256;
	add.s32 	%r75, %r75, -1;
	setp.ne.s32 	%p3, %r75, 0;
	@%p3 bra 	$L__BB2_3;

	st.local.f32 	[%rd3], %f174;
	st.local.f32 	[%rd3+4], %f173;
	st.local.f32 	[%rd3+8], %f172;

$L__BB2_5:
	setp.lt.u32 	%p4, %r4, 96;
	@%p4 bra 	$L__BB2_9;

	add.s32 	%r31, %r76, %r14;
	shl.b32 	%r32, %r14, 1;
	add.s32 	%r33, %r76, %r32;
	add.s32 	%r34, %r31, 32;
	mul.wide.s32 	%rd44, %r34, 8;
	shl.b64 	%rd45, %rd5, 2;
	add.s64 	%rd19, %rd44, %rd45;
	mul.wide.s32 	%rd46, %r33, 8;
	add.s64 	%rd20, %rd46, %rd45;
	mul.wide.s32 	%rd47, %r76, 2;
	add.s64 	%rd48, %rd47, %rd4;
	shl.b64 	%rd49, %rd48, 2;
	add.s64 	%rd50, %rd2, %rd49;
	add.s64 	%rd70, %rd50, 512;
	mul.wide.s32 	%rd51, %r76, 8;
	add.s64 	%rd22, %rd51, %rd45;
	mul.wide.s32 	%rd52, %r14, 8;
	add.s64 	%rd53, %rd51, %rd52;
	add.s64 	%rd23, %rd53, %rd45;

$L__BB2_7:
	ld.global.nc.v2.f32 	{%f47, %f48}, [%rd70+-512];
	add.s64 	%rd54, %rd71, %rd22;
	ld.global.nc.v2.f32 	{%f51, %f52}, [%rd54];
	fma.rn.f32 	%f55, %f47, %f51, %f174;
	fma.rn.f32 	%f56, %f48, %f52, %f55;
	add.s64 	%rd55, %rd71, %rd23;
	ld.global.nc.v2.f32 	{%f57, %f58}, [%rd55];
	fma.rn.f32 	%f61, %f47, %f57, %f173;
	fma.rn.f32 	%f62, %f48, %f58, %f61;
	add.s64 	%rd56, %rd71, %rd20;
	ld.global.nc.v2.f32 	{%f63, %f64}, [%rd56];
	fma.rn.f32 	%f67, %f47, %f63, %f172;
	fma.rn.f32 	%f68, %f48, %f64, %f67;
	ld.global.nc.v2.f32 	{%f69, %f70}, [%rd70+-256];
	ld.global.nc.v2.f32 	{%f73, %f74}, [%rd54+256];
	fma.rn.f32 	%f77, %f69, %f73, %f56;
	fma.rn.f32 	%f78, %f70, %f74, %f77;
	add.s64 	%rd57, %rd71, %rd19;
	ld.global.nc.v2.f32 	{%f79, %f80}, [%rd57];
	fma.rn.f32 	%f83, %f69, %f79, %f62;
	fma.rn.f32 	%f84, %f70, %f80, %f83;
	ld.global.nc.v2.f32 	{%f85, %f86}, [%rd56+256];
	fma.rn.f32 	%f89, %f69, %f85, %f68;
	fma.rn.f32 	%f90, %f70, %f86, %f89;
	ld.global.nc.v2.f32 	{%f91, %f92}, [%rd70];
	ld.global.nc.v2.f32 	{%f95, %f96}, [%rd54+512];
	fma.rn.f32 	%f99, %f91, %f95, %f78;
	fma.rn.f32 	%f100, %f92, %f96, %f99;
	ld.global.nc.v2.f32 	{%f101, %f102}, [%rd57+256];
	fma.rn.f32 	%f105, %f91, %f101, %f84;
	fma.rn.f32 	%f106, %f92, %f102, %f105;
	ld.global.nc.v2.f32 	{%f107, %f108}, [%rd56+512];
	fma.rn.f32 	%f111, %f91, %f107, %f90;
	fma.rn.f32 	%f112, %f92, %f108, %f111;
	ld.global.nc.v2.f32 	{%f113, %f114}, [%rd70+256];
	ld.global.nc.v2.f32 	{%f117, %f118}, [%rd54+768];
	fma.rn.f32 	%f121, %f113, %f117, %f100;
	fma.rn.f32 	%f174, %f114, %f118, %f121;
	ld.global.nc.v2.f32 	{%f122, %f123}, [%rd57+512];
	fma.rn.f32 	%f126, %f113, %f122, %f106;
	fma.rn.f32 	%f173, %f114, %f123, %f126;
	ld.global.nc.v2.f32 	{%f127, %f128}, [%rd56+768];
	fma.rn.f32 	%f131, %f113, %f127, %f112;
	fma.rn.f32 	%f172, %f114, %f128, %f131;
	add.s64 	%rd71, %rd71, 1024;
	add.s64 	%rd70, %rd70, 1024;
	add.s32 	%r76, %r76, 128;
	setp.lt.s32 	%p5, %r76, %r13;
	@%p5 bra 	$L__BB2_7;

	st.local.f32 	[%rd3], %f174;
	st.local.f32 	[%rd3+4], %f173;
	st.local.f32 	[%rd3+8], %f172;

$L__BB2_9:
	mov.b32 	%r35, %f174;
	mov.u32 	%r36, 31;
	mov.u32 	%r37, 16;
	mov.u32 	%r38, -1;
	shfl.sync.bfly.b32 	%r39|%p6, %r35, %r37, %r36, %r38;
	mov.b32 	%f132, %r39;
	add.f32 	%f133, %f174, %f132;
	mov.b32 	%r40, %f133;
	mov.u32 	%r41, 8;
	shfl.sync.bfly.b32 	%r42|%p7, %r40, %r41, %r36, %r38;
	mov.b32 	%f134, %r42;
	add.f32 	%f135, %f133, %f134;
	mov.b32 	%r43, %f135;
	mov.u32 	%r44, 4;
	shfl.sync.bfly.b32 	%r45|%p8, %r43, %r44, %r36, %r38;
	mov.b32 	%f136, %r45;
	add.f32 	%f137, %f135, %f136;
	mov.b32 	%r46, %f137;
	mov.u32 	%r47, 2;
	shfl.sync.bfly.b32 	%r48|%p9, %r46, %r47, %r36, %r38;
	mov.b32 	%f138, %r48;
	add.f32 	%f139, %f137, %f138;
	mov.b32 	%r49, %f139;
	mov.u32 	%r50, 1;
	shfl.sync.bfly.b32 	%r51|%p10, %r49, %r50, %r36, %r38;
	mov.b32 	%f140, %r51;
	add.f32 	%f141, %f139, %f140;
	st.local.f32 	[%rd3], %f141;
	mov.b32 	%r52, %f173;
	shfl.sync.bfly.b32 	%r53|%p11, %r52, %r37, %r36, %r38;
	mov.b32 	%f142, %r53;
	add.f32 	%f143, %f173, %f142;
	mov.b32 	%r54, %f143;
	shfl.sync.bfly.b32 	%r55|%p12, %r54, %r41, %r36, %r38;
	mov.b32 	%f144, %r55;
	add.f32 	%f145, %f143, %f144;
	mov.b32 	%r56, %f145;
	shfl.sync.bfly.b32 	%r57|%p13, %r56, %r44, %r36, %r38;
	mov.b32 	%f146, %r57;
	add.f32 	%f147, %f145, %f146;
	mov.b32 	%r58, %f147;
	shfl.sync.bfly.b32 	%r59|%p14, %r58, %r47, %r36, %r38;
	mov.b32 	%f148, %r59;
	add.f32 	%f149, %f147, %f148;
	mov.b32 	%r60, %f149;
	shfl.sync.bfly.b32 	%r61|%p15, %r60, %r50, %r36, %r38;
	mov.b32 	%f150, %r61;
	add.f32 	%f151, %f149, %f150;
	st.local.f32 	[%rd3+4], %f151;
	mov.b32 	%r62, %f172;
	shfl.sync.bfly.b32 	%r63|%p16, %r62, %r37, %r36, %r38;
	mov.b32 	%f152, %r63;
	add.f32 	%f153, %f172, %f152;
	mov.b32 	%r64, %f153;
	shfl.sync.bfly.b32 	%r65|%p17, %r64, %r41, %r36, %r38;
	mov.b32 	%f154, %r65;
	add.f32 	%f155, %f153, %f154;
	mov.b32 	%r66, %f155;
	shfl.sync.bfly.b32 	%r67|%p18, %r66, %r44, %r36, %r38;
	mov.b32 	%f156, %r67;
	add.f32 	%f157, %f155, %f156;
	mov.b32 	%r68, %f157;
	shfl.sync.bfly.b32 	%r69|%p19, %r68, %r47, %r36, %r38;
	mov.b32 	%f158, %r69;
	add.f32 	%f159, %f157, %f158;
	mov.b32 	%r70, %f159;
	shfl.sync.bfly.b32 	%r71|%p20, %r70, %r50, %r36, %r38;
	mov.b32 	%f160, %r71;
	add.f32 	%f161, %f159, %f160;
	st.local.f32 	[%rd3+8], %f161;
	setp.gt.s32 	%p21, %r3, 2;
	@%p21 bra 	$L__BB2_11;

	mul.wide.s32 	%rd58, %r3, 4;
	add.s64 	%rd59, %rd3, %rd58;
	ld.local.f32 	%f162, [%rd59];
	mad.lo.s32 	%r72, %r3, %r15, %r2;
	cvt.s64.s32 	%rd60, %r72;
	mul.lo.s32 	%r73, %r1, %r16;
	cvt.s64.s32 	%rd61, %r73;
	add.s64 	%rd62, %rd61, %rd60;
	cvta.to.global.u64 	%rd63, %rd28;
	shl.b64 	%rd64, %rd62, 2;
	add.s64 	%rd65, %rd63, %rd64;
	st.global.f32 	[%rd65], %f162;

$L__BB2_11:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_4_bs_32
.visible .entry ggml_matvec_f32_ncols_4_bs_32(
	.param .u64 ggml_matvec_f32_ncols_4_bs_32_param_0,
	.param .u64 ggml_matvec_f32_ncols_4_bs_32_param_1,
	.param .u64 ggml_matvec_f32_ncols_4_bs_32_param_2,
	.param .u32 ggml_matvec_f32_ncols_4_bs_32_param_3,
	.param .u32 ggml_matvec_f32_ncols_4_bs_32_param_4,
	.param .u32 ggml_matvec_f32_ncols_4_bs_32_param_5,
	.param .u32 ggml_matvec_f32_ncols_4_bs_32_param_6,
	.param .u32 ggml_matvec_f32_ncols_4_bs_32_param_7,
	.param .u32 ggml_matvec_f32_ncols_4_bs_32_param_8,
	.param .u32 ggml_matvec_f32_ncols_4_bs_32_param_9,
	.param .u32 ggml_matvec_f32_ncols_4_bs_32_param_10,
	.param .u32 ggml_matvec_f32_ncols_4_bs_32_param_11
)
{
	.local .align 16 .b8 	__local_depot3[16];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<27>;
	.reg .f32 	%f<226>;
	.reg .b32 	%r<89>;
	.reg .b64 	%rd<82>;


	mov.u64 	%SPL, __local_depot3;
	ld.param.u64 	%rd33, [ggml_matvec_f32_ncols_4_bs_32_param_0];
	ld.param.u64 	%rd34, [ggml_matvec_f32_ncols_4_bs_32_param_1];
	ld.param.u64 	%rd32, [ggml_matvec_f32_ncols_4_bs_32_param_2];
	ld.param.u32 	%r13, [ggml_matvec_f32_ncols_4_bs_32_param_3];
	ld.param.u32 	%r17, [ggml_matvec_f32_ncols_4_bs_32_param_5];
	ld.param.u32 	%r14, [ggml_matvec_f32_ncols_4_bs_32_param_6];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_4_bs_32_param_7];
	ld.param.u32 	%r18, [ggml_matvec_f32_ncols_4_bs_32_param_8];
	ld.param.u32 	%r19, [ggml_matvec_f32_ncols_4_bs_32_param_9];
	ld.param.u32 	%r20, [ggml_matvec_f32_ncols_4_bs_32_param_10];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_4_bs_32_param_11];
	cvta.to.global.u64 	%rd81, %rd34;
	cvta.to.global.u64 	%rd2, %rd33;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r21, %r1, %r18;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r22, %r2, %r17;
	mad.lo.s32 	%r23, %r21, %r19, %r22;
	cvt.s64.s32 	%rd4, %r23;
	mul.lo.s32 	%r24, %r1, %r20;
	cvt.s64.s32 	%rd5, %r24;
	mov.f32 	%f222, 0f00000000;
	st.local.v4.f32 	[%rd3], {%f222, %f222, %f222, %f222};
	mov.u32 	%r3, %tid.x;
	setp.ge.s32 	%p1, %r3, %r13;
	mov.f32 	%f223, %f222;
	mov.f32 	%f224, %f222;
	mov.f32 	%f225, %f222;
	@%p1 bra 	$L__BB3_9;

	not.b32 	%r25, %r3;
	add.s32 	%r4, %r25, %r13;
	shr.u32 	%r26, %r4, 5;
	add.s32 	%r27, %r26, 1;
	and.b32  	%r86, %r27, 3;
	setp.eq.s32 	%p2, %r86, 0;
	mov.f32 	%f222, 0f00000000;
	mov.u32 	%r87, %r3;
	@%p2 bra 	$L__BB3_5;

	shl.b32 	%r28, %r14, 1;
	add.s32 	%r29, %r3, %r28;
	mul.wide.s32 	%rd36, %r29, 2;
	add.s64 	%rd37, %rd36, %rd5;
	shl.b64 	%rd38, %rd37, 2;
	add.s64 	%rd79, %rd81, %rd38;
	mad.lo.s32 	%r30, %r14, 3, %r3;
	mul.wide.s32 	%rd39, %r30, 2;
	add.s64 	%rd40, %rd39, %rd5;
	shl.b64 	%rd41, %rd40, 2;
	add.s64 	%rd78, %rd81, %rd41;
	mul.wide.s32 	%rd42, %r14, 2;
	mul.wide.s32 	%rd43, %r3, 2;
	add.s64 	%rd44, %rd42, %rd43;
	add.s64 	%rd45, %rd44, %rd5;
	shl.b64 	%rd46, %rd45, 2;
	add.s64 	%rd77, %rd81, %rd46;
	add.s64 	%rd47, %rd43, %rd5;
	shl.b64 	%rd48, %rd47, 2;
	add.s64 	%rd76, %rd81, %rd48;
	add.s64 	%rd49, %rd43, %rd4;
	shl.b64 	%rd50, %rd49, 2;
	add.s64 	%rd75, %rd2, %rd50;
	mov.f32 	%f222, 0f00000000;
	mov.f32 	%f223, %f222;
	mov.f32 	%f224, %f222;
	mov.f32 	%f225, %f222;
	mov.u32 	%r87, %r3;

$L__BB3_3:
	.pragma "nounroll";
	ld.global.nc.v2.f32 	{%f37, %f38}, [%rd75];
	ld.global.nc.v2.f32 	{%f41, %f42}, [%rd76];
	fma.rn.f32 	%f45, %f37, %f41, %f225;
	fma.rn.f32 	%f225, %f38, %f42, %f45;
	ld.global.nc.v2.f32 	{%f46, %f47}, [%rd77];
	fma.rn.f32 	%f50, %f37, %f46, %f224;
	fma.rn.f32 	%f224, %f38, %f47, %f50;
	ld.global.nc.v2.f32 	{%f51, %f52}, [%rd79];
	fma.rn.f32 	%f55, %f37, %f51, %f223;
	fma.rn.f32 	%f223, %f38, %f52, %f55;
	ld.global.nc.v2.f32 	{%f56, %f57}, [%rd78];
	fma.rn.f32 	%f60, %f37, %f56, %f222;
	fma.rn.f32 	%f222, %f38, %f57, %f60;
	add.s32 	%r87, %r87, 32;
	add.s64 	%rd79, %rd79, 256;
	add.s64 	%rd78, %rd78, 256;
	add.s64 	%rd77, %rd77, 256;
	add.s64 	%rd76, %rd76, 256;
	add.s64 	%rd75, %rd75, 256;
	add.s32 	%r86, %r86, -1;
	setp.ne.s32 	%p3, %r86, 0;
	@%p3 bra 	$L__BB3_3;

	st.local.v4.f32 	[%rd3], {%f225, %f224, %f223, %f222};

$L__BB3_5:
	setp.lt.u32 	%p4, %r4, 96;
	@%p4 bra 	$L__BB3_9;

	add.s32 	%r31, %r87, %r14;
	shl.b32 	%r32, %r14, 1;
	add.s32 	%r33, %r87, %r32;
	mad.lo.s32 	%r34, %r14, 3, %r87;
	add.s32 	%r35, %r31, 32;
	mul.wide.s32 	%rd51, %r35, 8;
	shl.b64 	%rd52, %rd5, 2;
	add.s64 	%rd22, %rd51, %rd52;
	mul.wide.s32 	%rd53, %r33, 8;
	add.s64 	%rd23, %rd53, %rd52;
	mul.wide.s32 	%rd54, %r34, 8;
	add.s64 	%rd24, %rd54, %rd52;
	mul.wide.s32 	%rd55, %r87, 2;
	add.s64 	%rd56, %rd55, %rd4;
	shl.b64 	%rd57, %rd56, 2;
	add.s64 	%rd58, %rd2, %rd57;
	add.s64 	%rd80, %rd58, 512;
	mul.wide.s32 	%rd59, %r87, 8;
	add.s64 	%rd26, %rd59, %rd52;
	mul.wide.s32 	%rd60, %r14, 8;
	add.s64 	%rd61, %rd59, %rd60;
	add.s64 	%rd27, %rd61, %rd52;

$L__BB3_7:
	ld.global.nc.v2.f32 	{%f61, %f62}, [%rd80+-512];
	add.s64 	%rd62, %rd81, %rd26;
	ld.global.nc.v2.f32 	{%f65, %f66}, [%rd62];
	fma.rn.f32 	%f69, %f61, %f65, %f225;
	fma.rn.f32 	%f70, %f62, %f66, %f69;
	add.s64 	%rd63, %rd81, %rd27;
	ld.global.nc.v2.f32 	{%f71, %f72}, [%rd63];
	fma.rn.f32 	%f75, %f61, %f71, %f224;
	fma.rn.f32 	%f76, %f62, %f72, %f75;
	add.s64 	%rd64, %rd81, %rd23;
	ld.global.nc.v2.f32 	{%f77, %f78}, [%rd64];
	fma.rn.f32 	%f81, %f61, %f77, %f223;
	fma.rn.f32 	%f82, %f62, %f78, %f81;
	add.s64 	%rd65, %rd81, %rd24;
	ld.global.nc.v2.f32 	{%f83, %f84}, [%rd65];
	fma.rn.f32 	%f87, %f61, %f83, %f222;
	fma.rn.f32 	%f88, %f62, %f84, %f87;
	ld.global.nc.v2.f32 	{%f89, %f90}, [%rd80+-256];
	ld.global.nc.v2.f32 	{%f93, %f94}, [%rd62+256];
	fma.rn.f32 	%f97, %f89, %f93, %f70;
	fma.rn.f32 	%f98, %f90, %f94, %f97;
	add.s64 	%rd66, %rd81, %rd22;
	ld.global.nc.v2.f32 	{%f99, %f100}, [%rd66];
	fma.rn.f32 	%f103, %f89, %f99, %f76;
	fma.rn.f32 	%f104, %f90, %f100, %f103;
	ld.global.nc.v2.f32 	{%f105, %f106}, [%rd64+256];
	fma.rn.f32 	%f109, %f89, %f105, %f82;
	fma.rn.f32 	%f110, %f90, %f106, %f109;
	ld.global.nc.v2.f32 	{%f111, %f112}, [%rd65+256];
	fma.rn.f32 	%f115, %f89, %f111, %f88;
	fma.rn.f32 	%f116, %f90, %f112, %f115;
	ld.global.nc.v2.f32 	{%f117, %f118}, [%rd80];
	ld.global.nc.v2.f32 	{%f121, %f122}, [%rd62+512];
	fma.rn.f32 	%f125, %f117, %f121, %f98;
	fma.rn.f32 	%f126, %f118, %f122, %f125;
	ld.global.nc.v2.f32 	{%f127, %f128}, [%rd66+256];
	fma.rn.f32 	%f131, %f117, %f127, %f104;
	fma.rn.f32 	%f132, %f118, %f128, %f131;
	ld.global.nc.v2.f32 	{%f133, %f134}, [%rd64+512];
	fma.rn.f32 	%f137, %f117, %f133, %f110;
	fma.rn.f32 	%f138, %f118, %f134, %f137;
	ld.global.nc.v2.f32 	{%f139, %f140}, [%rd65+512];
	fma.rn.f32 	%f143, %f117, %f139, %f116;
	fma.rn.f32 	%f144, %f118, %f140, %f143;
	ld.global.nc.v2.f32 	{%f145, %f146}, [%rd80+256];
	ld.global.nc.v2.f32 	{%f149, %f150}, [%rd62+768];
	fma.rn.f32 	%f153, %f145, %f149, %f126;
	fma.rn.f32 	%f225, %f146, %f150, %f153;
	ld.global.nc.v2.f32 	{%f154, %f155}, [%rd66+512];
	fma.rn.f32 	%f158, %f145, %f154, %f132;
	fma.rn.f32 	%f224, %f146, %f155, %f158;
	ld.global.nc.v2.f32 	{%f159, %f160}, [%rd64+768];
	fma.rn.f32 	%f163, %f145, %f159, %f138;
	fma.rn.f32 	%f223, %f146, %f160, %f163;
	ld.global.nc.v2.f32 	{%f164, %f165}, [%rd65+768];
	fma.rn.f32 	%f168, %f145, %f164, %f144;
	fma.rn.f32 	%f222, %f146, %f165, %f168;
	add.s64 	%rd81, %rd81, 1024;
	add.s64 	%rd80, %rd80, 1024;
	add.s32 	%r87, %r87, 128;
	setp.lt.s32 	%p5, %r87, %r13;
	@%p5 bra 	$L__BB3_7;

	st.local.v4.f32 	[%rd3], {%f225, %f224, %f223, %f222};

$L__BB3_9:
	mov.b32 	%r36, %f225;
	mov.u32 	%r37, 31;
	mov.u32 	%r38, 16;
	mov.u32 	%r39, -1;
	shfl.sync.bfly.b32 	%r40|%p6, %r36, %r38, %r37, %r39;
	mov.b32 	%f169, %r40;
	add.f32 	%f170, %f225, %f169;
	mov.b32 	%r41, %f170;
	mov.u32 	%r42, 8;
	shfl.sync.bfly.b32 	%r43|%p7, %r41, %r42, %r37, %r39;
	mov.b32 	%f171, %r43;
	add.f32 	%f172, %f170, %f171;
	mov.b32 	%r44, %f172;
	mov.u32 	%r45, 4;
	shfl.sync.bfly.b32 	%r46|%p8, %r44, %r45, %r37, %r39;
	mov.b32 	%f173, %r46;
	add.f32 	%f174, %f172, %f173;
	mov.b32 	%r47, %f174;
	mov.u32 	%r48, 2;
	shfl.sync.bfly.b32 	%r49|%p9, %r47, %r48, %r37, %r39;
	mov.b32 	%f175, %r49;
	add.f32 	%f176, %f174, %f175;
	mov.b32 	%r50, %f176;
	mov.u32 	%r51, 1;
	shfl.sync.bfly.b32 	%r52|%p10, %r50, %r51, %r37, %r39;
	mov.b32 	%f177, %r52;
	add.f32 	%f178, %f176, %f177;
	st.local.f32 	[%rd3], %f178;
	mov.b32 	%r53, %f224;
	shfl.sync.bfly.b32 	%r54|%p11, %r53, %r38, %r37, %r39;
	mov.b32 	%f179, %r54;
	add.f32 	%f180, %f224, %f179;
	mov.b32 	%r55, %f180;
	shfl.sync.bfly.b32 	%r56|%p12, %r55, %r42, %r37, %r39;
	mov.b32 	%f181, %r56;
	add.f32 	%f182, %f180, %f181;
	mov.b32 	%r57, %f182;
	shfl.sync.bfly.b32 	%r58|%p13, %r57, %r45, %r37, %r39;
	mov.b32 	%f183, %r58;
	add.f32 	%f184, %f182, %f183;
	mov.b32 	%r59, %f184;
	shfl.sync.bfly.b32 	%r60|%p14, %r59, %r48, %r37, %r39;
	mov.b32 	%f185, %r60;
	add.f32 	%f186, %f184, %f185;
	mov.b32 	%r61, %f186;
	shfl.sync.bfly.b32 	%r62|%p15, %r61, %r51, %r37, %r39;
	mov.b32 	%f187, %r62;
	add.f32 	%f188, %f186, %f187;
	st.local.f32 	[%rd3+4], %f188;
	mov.b32 	%r63, %f223;
	shfl.sync.bfly.b32 	%r64|%p16, %r63, %r38, %r37, %r39;
	mov.b32 	%f189, %r64;
	add.f32 	%f190, %f223, %f189;
	mov.b32 	%r65, %f190;
	shfl.sync.bfly.b32 	%r66|%p17, %r65, %r42, %r37, %r39;
	mov.b32 	%f191, %r66;
	add.f32 	%f192, %f190, %f191;
	mov.b32 	%r67, %f192;
	shfl.sync.bfly.b32 	%r68|%p18, %r67, %r45, %r37, %r39;
	mov.b32 	%f193, %r68;
	add.f32 	%f194, %f192, %f193;
	mov.b32 	%r69, %f194;
	shfl.sync.bfly.b32 	%r70|%p19, %r69, %r48, %r37, %r39;
	mov.b32 	%f195, %r70;
	add.f32 	%f196, %f194, %f195;
	mov.b32 	%r71, %f196;
	shfl.sync.bfly.b32 	%r72|%p20, %r71, %r51, %r37, %r39;
	mov.b32 	%f197, %r72;
	add.f32 	%f198, %f196, %f197;
	st.local.f32 	[%rd3+8], %f198;
	mov.b32 	%r73, %f222;
	shfl.sync.bfly.b32 	%r74|%p21, %r73, %r38, %r37, %r39;
	mov.b32 	%f199, %r74;
	add.f32 	%f200, %f222, %f199;
	mov.b32 	%r75, %f200;
	shfl.sync.bfly.b32 	%r76|%p22, %r75, %r42, %r37, %r39;
	mov.b32 	%f201, %r76;
	add.f32 	%f202, %f200, %f201;
	mov.b32 	%r77, %f202;
	shfl.sync.bfly.b32 	%r78|%p23, %r77, %r45, %r37, %r39;
	mov.b32 	%f203, %r78;
	add.f32 	%f204, %f202, %f203;
	mov.b32 	%r79, %f204;
	shfl.sync.bfly.b32 	%r80|%p24, %r79, %r48, %r37, %r39;
	mov.b32 	%f205, %r80;
	add.f32 	%f206, %f204, %f205;
	mov.b32 	%r81, %f206;
	shfl.sync.bfly.b32 	%r82|%p25, %r81, %r51, %r37, %r39;
	mov.b32 	%f207, %r82;
	add.f32 	%f208, %f206, %f207;
	st.local.f32 	[%rd3+12], %f208;
	setp.gt.s32 	%p26, %r3, 3;
	@%p26 bra 	$L__BB3_11;

	mul.wide.s32 	%rd67, %r3, 4;
	add.s64 	%rd68, %rd3, %rd67;
	ld.local.f32 	%f209, [%rd68];
	mad.lo.s32 	%r83, %r3, %r15, %r2;
	cvt.s64.s32 	%rd69, %r83;
	mul.lo.s32 	%r84, %r1, %r16;
	cvt.s64.s32 	%rd70, %r84;
	add.s64 	%rd71, %rd70, %rd69;
	cvta.to.global.u64 	%rd72, %rd32;
	shl.b64 	%rd73, %rd71, 2;
	add.s64 	%rd74, %rd72, %rd73;
	st.global.f32 	[%rd74], %f209;

$L__BB3_11:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_5_bs_32
.visible .entry ggml_matvec_f32_ncols_5_bs_32(
	.param .u64 ggml_matvec_f32_ncols_5_bs_32_param_0,
	.param .u64 ggml_matvec_f32_ncols_5_bs_32_param_1,
	.param .u64 ggml_matvec_f32_ncols_5_bs_32_param_2,
	.param .u32 ggml_matvec_f32_ncols_5_bs_32_param_3,
	.param .u32 ggml_matvec_f32_ncols_5_bs_32_param_4,
	.param .u32 ggml_matvec_f32_ncols_5_bs_32_param_5,
	.param .u32 ggml_matvec_f32_ncols_5_bs_32_param_6,
	.param .u32 ggml_matvec_f32_ncols_5_bs_32_param_7,
	.param .u32 ggml_matvec_f32_ncols_5_bs_32_param_8,
	.param .u32 ggml_matvec_f32_ncols_5_bs_32_param_9,
	.param .u32 ggml_matvec_f32_ncols_5_bs_32_param_10,
	.param .u32 ggml_matvec_f32_ncols_5_bs_32_param_11
)
{
	.local .align 4 .b8 	__local_depot4[20];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<32>;
	.reg .f32 	%f<277>;
	.reg .b32 	%r<101>;
	.reg .b64 	%rd<74>;


	mov.u64 	%SPL, __local_depot4;
	ld.param.u64 	%rd28, [ggml_matvec_f32_ncols_5_bs_32_param_0];
	ld.param.u64 	%rd29, [ggml_matvec_f32_ncols_5_bs_32_param_1];
	ld.param.u64 	%rd27, [ggml_matvec_f32_ncols_5_bs_32_param_2];
	ld.param.u32 	%r13, [ggml_matvec_f32_ncols_5_bs_32_param_3];
	ld.param.u32 	%r17, [ggml_matvec_f32_ncols_5_bs_32_param_5];
	ld.param.u32 	%r14, [ggml_matvec_f32_ncols_5_bs_32_param_6];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_5_bs_32_param_7];
	ld.param.u32 	%r18, [ggml_matvec_f32_ncols_5_bs_32_param_8];
	ld.param.u32 	%r19, [ggml_matvec_f32_ncols_5_bs_32_param_9];
	ld.param.u32 	%r20, [ggml_matvec_f32_ncols_5_bs_32_param_10];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_5_bs_32_param_11];
	cvta.to.global.u64 	%rd73, %rd29;
	cvta.to.global.u64 	%rd2, %rd28;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r21, %r1, %r18;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r22, %r2, %r17;
	mad.lo.s32 	%r23, %r21, %r19, %r22;
	cvt.s64.s32 	%rd4, %r23;
	mul.lo.s32 	%r24, %r1, %r20;
	cvt.s64.s32 	%rd5, %r24;
	mov.f32 	%f272, 0f00000000;
	mov.u32 	%r25, 0;
	st.local.u32 	[%rd3], %r25;
	st.local.u32 	[%rd3+4], %r25;
	st.local.u32 	[%rd3+8], %r25;
	st.local.u32 	[%rd3+12], %r25;
	st.local.u32 	[%rd3+16], %r25;
	mov.u32 	%r3, %tid.x;
	setp.ge.s32 	%p1, %r3, %r13;
	mov.f32 	%f273, %f272;
	mov.f32 	%f274, %f272;
	mov.f32 	%f275, %f272;
	mov.f32 	%f276, %f272;
	@%p1 bra 	$L__BB4_9;

	not.b32 	%r26, %r3;
	add.s32 	%r4, %r26, %r13;
	shr.u32 	%r27, %r4, 5;
	add.s32 	%r28, %r27, 1;
	and.b32  	%r98, %r28, 3;
	setp.eq.s32 	%p2, %r98, 0;
	mov.f32 	%f272, 0f00000000;
	mov.u32 	%r99, %r3;
	@%p2 bra 	$L__BB4_5;

	shl.b32 	%r29, %r14, 1;
	mad.lo.s32 	%r30, %r14, 3, %r3;
	mul.wide.s32 	%rd31, %r30, 8;
	shl.b64 	%rd32, %rd5, 2;
	add.s64 	%rd7, %rd31, %rd32;
	mul.wide.s32 	%rd33, %r3, 8;
	mul.wide.s32 	%rd34, %r14, 8;
	add.s64 	%rd35, %rd33, %rd34;
	add.s64 	%rd8, %rd35, %rd32;
	add.s64 	%rd9, %rd33, %rd32;
	mul.wide.s32 	%rd36, %r3, 2;
	add.s64 	%rd37, %rd36, %rd4;
	shl.b64 	%rd38, %rd37, 2;
	add.s64 	%rd70, %rd2, %rd38;
	mul.wide.s32 	%rd11, %r29, 8;
	mov.f32 	%f272, 0f00000000;
	mov.u64 	%rd71, %rd73;
	mov.f32 	%f273, %f272;
	mov.f32 	%f274, %f272;
	mov.f32 	%f275, %f272;
	mov.f32 	%f276, %f272;
	mov.u32 	%r99, %r3;

$L__BB4_3:
	.pragma "nounroll";
	ld.global.nc.v2.f32 	{%f46, %f47}, [%rd70];
	add.s64 	%rd39, %rd71, %rd9;
	ld.global.nc.v2.f32 	{%f50, %f51}, [%rd39];
	fma.rn.f32 	%f54, %f46, %f50, %f276;
	fma.rn.f32 	%f276, %f47, %f51, %f54;
	add.s64 	%rd40, %rd71, %rd8;
	ld.global.nc.v2.f32 	{%f55, %f56}, [%rd40];
	fma.rn.f32 	%f59, %f46, %f55, %f275;
	fma.rn.f32 	%f275, %f47, %f56, %f59;
	add.s64 	%rd41, %rd39, %rd11;
	ld.global.nc.v2.f32 	{%f60, %f61}, [%rd41];
	fma.rn.f32 	%f64, %f46, %f60, %f274;
	fma.rn.f32 	%f274, %f47, %f61, %f64;
	add.s64 	%rd42, %rd71, %rd7;
	ld.global.nc.v2.f32 	{%f65, %f66}, [%rd42];
	fma.rn.f32 	%f69, %f46, %f65, %f273;
	fma.rn.f32 	%f273, %f47, %f66, %f69;
	add.s64 	%rd43, %rd41, %rd11;
	ld.global.nc.v2.f32 	{%f70, %f71}, [%rd43];
	fma.rn.f32 	%f74, %f46, %f70, %f272;
	fma.rn.f32 	%f272, %f47, %f71, %f74;
	add.s32 	%r99, %r99, 32;
	add.s64 	%rd71, %rd71, 256;
	add.s64 	%rd70, %rd70, 256;
	add.s32 	%r98, %r98, -1;
	setp.ne.s32 	%p3, %r98, 0;
	@%p3 bra 	$L__BB4_3;

	st.local.f32 	[%rd3], %f276;
	st.local.f32 	[%rd3+4], %f275;
	st.local.f32 	[%rd3+8], %f274;
	st.local.f32 	[%rd3+12], %f273;
	st.local.f32 	[%rd3+16], %f272;

$L__BB4_5:
	setp.lt.u32 	%p4, %r4, 96;
	@%p4 bra 	$L__BB4_9;

	add.s32 	%r31, %r99, %r14;
	shl.b32 	%r32, %r14, 1;
	add.s32 	%r33, %r99, %r32;
	mad.lo.s32 	%r34, %r14, 3, %r99;
	shl.b32 	%r35, %r14, 2;
	add.s32 	%r36, %r99, %r35;
	add.s32 	%r37, %r31, 32;
	mul.wide.s32 	%rd44, %r37, 8;
	shl.b64 	%rd45, %rd5, 2;
	add.s64 	%rd16, %rd44, %rd45;
	mul.wide.s32 	%rd46, %r33, 8;
	add.s64 	%rd17, %rd46, %rd45;
	mul.wide.s32 	%rd47, %r34, 8;
	add.s64 	%rd18, %rd47, %rd45;
	mul.wide.s32 	%rd48, %r36, 8;
	add.s64 	%rd19, %rd48, %rd45;
	mul.wide.s32 	%rd49, %r99, 2;
	add.s64 	%rd50, %rd49, %rd4;
	shl.b64 	%rd51, %rd50, 2;
	add.s64 	%rd52, %rd2, %rd51;
	add.s64 	%rd72, %rd52, 512;
	mul.wide.s32 	%rd53, %r99, 8;
	add.s64 	%rd21, %rd53, %rd45;
	mul.wide.s32 	%rd54, %r14, 8;
	add.s64 	%rd55, %rd53, %rd54;
	add.s64 	%rd22, %rd55, %rd45;

$L__BB4_7:
	ld.global.nc.v2.f32 	{%f75, %f76}, [%rd72+-512];
	add.s64 	%rd56, %rd73, %rd21;
	ld.global.nc.v2.f32 	{%f79, %f80}, [%rd56];
	fma.rn.f32 	%f83, %f75, %f79, %f276;
	fma.rn.f32 	%f84, %f76, %f80, %f83;
	add.s64 	%rd57, %rd73, %rd22;
	ld.global.nc.v2.f32 	{%f85, %f86}, [%rd57];
	fma.rn.f32 	%f89, %f75, %f85, %f275;
	fma.rn.f32 	%f90, %f76, %f86, %f89;
	add.s64 	%rd58, %rd73, %rd17;
	ld.global.nc.v2.f32 	{%f91, %f92}, [%rd58];
	fma.rn.f32 	%f95, %f75, %f91, %f274;
	fma.rn.f32 	%f96, %f76, %f92, %f95;
	add.s64 	%rd59, %rd73, %rd18;
	ld.global.nc.v2.f32 	{%f97, %f98}, [%rd59];
	fma.rn.f32 	%f101, %f75, %f97, %f273;
	fma.rn.f32 	%f102, %f76, %f98, %f101;
	add.s64 	%rd60, %rd73, %rd19;
	ld.global.nc.v2.f32 	{%f103, %f104}, [%rd60];
	fma.rn.f32 	%f107, %f75, %f103, %f272;
	fma.rn.f32 	%f108, %f76, %f104, %f107;
	ld.global.nc.v2.f32 	{%f109, %f110}, [%rd72+-256];
	ld.global.nc.v2.f32 	{%f113, %f114}, [%rd56+256];
	fma.rn.f32 	%f117, %f109, %f113, %f84;
	fma.rn.f32 	%f118, %f110, %f114, %f117;
	add.s64 	%rd61, %rd73, %rd16;
	ld.global.nc.v2.f32 	{%f119, %f120}, [%rd61];
	fma.rn.f32 	%f123, %f109, %f119, %f90;
	fma.rn.f32 	%f124, %f110, %f120, %f123;
	ld.global.nc.v2.f32 	{%f125, %f126}, [%rd58+256];
	fma.rn.f32 	%f129, %f109, %f125, %f96;
	fma.rn.f32 	%f130, %f110, %f126, %f129;
	ld.global.nc.v2.f32 	{%f131, %f132}, [%rd59+256];
	fma.rn.f32 	%f135, %f109, %f131, %f102;
	fma.rn.f32 	%f136, %f110, %f132, %f135;
	ld.global.nc.v2.f32 	{%f137, %f138}, [%rd60+256];
	fma.rn.f32 	%f141, %f109, %f137, %f108;
	fma.rn.f32 	%f142, %f110, %f138, %f141;
	ld.global.nc.v2.f32 	{%f143, %f144}, [%rd72];
	ld.global.nc.v2.f32 	{%f147, %f148}, [%rd56+512];
	fma.rn.f32 	%f151, %f143, %f147, %f118;
	fma.rn.f32 	%f152, %f144, %f148, %f151;
	ld.global.nc.v2.f32 	{%f153, %f154}, [%rd61+256];
	fma.rn.f32 	%f157, %f143, %f153, %f124;
	fma.rn.f32 	%f158, %f144, %f154, %f157;
	ld.global.nc.v2.f32 	{%f159, %f160}, [%rd58+512];
	fma.rn.f32 	%f163, %f143, %f159, %f130;
	fma.rn.f32 	%f164, %f144, %f160, %f163;
	ld.global.nc.v2.f32 	{%f165, %f166}, [%rd59+512];
	fma.rn.f32 	%f169, %f143, %f165, %f136;
	fma.rn.f32 	%f170, %f144, %f166, %f169;
	ld.global.nc.v2.f32 	{%f171, %f172}, [%rd60+512];
	fma.rn.f32 	%f175, %f143, %f171, %f142;
	fma.rn.f32 	%f176, %f144, %f172, %f175;
	ld.global.nc.v2.f32 	{%f177, %f178}, [%rd72+256];
	ld.global.nc.v2.f32 	{%f181, %f182}, [%rd56+768];
	fma.rn.f32 	%f185, %f177, %f181, %f152;
	fma.rn.f32 	%f276, %f178, %f182, %f185;
	ld.global.nc.v2.f32 	{%f186, %f187}, [%rd61+512];
	fma.rn.f32 	%f190, %f177, %f186, %f158;
	fma.rn.f32 	%f275, %f178, %f187, %f190;
	ld.global.nc.v2.f32 	{%f191, %f192}, [%rd58+768];
	fma.rn.f32 	%f195, %f177, %f191, %f164;
	fma.rn.f32 	%f274, %f178, %f192, %f195;
	ld.global.nc.v2.f32 	{%f196, %f197}, [%rd59+768];
	fma.rn.f32 	%f200, %f177, %f196, %f170;
	fma.rn.f32 	%f273, %f178, %f197, %f200;
	ld.global.nc.v2.f32 	{%f201, %f202}, [%rd60+768];
	fma.rn.f32 	%f205, %f177, %f201, %f176;
	fma.rn.f32 	%f272, %f178, %f202, %f205;
	add.s64 	%rd73, %rd73, 1024;
	add.s64 	%rd72, %rd72, 1024;
	add.s32 	%r99, %r99, 128;
	setp.lt.s32 	%p5, %r99, %r13;
	@%p5 bra 	$L__BB4_7;

	st.local.f32 	[%rd3], %f276;
	st.local.f32 	[%rd3+4], %f275;
	st.local.f32 	[%rd3+8], %f274;
	st.local.f32 	[%rd3+12], %f273;
	st.local.f32 	[%rd3+16], %f272;

$L__BB4_9:
	mov.b32 	%r38, %f276;
	mov.u32 	%r39, 31;
	mov.u32 	%r40, 16;
	mov.u32 	%r41, -1;
	shfl.sync.bfly.b32 	%r42|%p6, %r38, %r40, %r39, %r41;
	mov.b32 	%f206, %r42;
	add.f32 	%f207, %f276, %f206;
	mov.b32 	%r43, %f207;
	mov.u32 	%r44, 8;
	shfl.sync.bfly.b32 	%r45|%p7, %r43, %r44, %r39, %r41;
	mov.b32 	%f208, %r45;
	add.f32 	%f209, %f207, %f208;
	mov.b32 	%r46, %f209;
	mov.u32 	%r47, 4;
	shfl.sync.bfly.b32 	%r48|%p8, %r46, %r47, %r39, %r41;
	mov.b32 	%f210, %r48;
	add.f32 	%f211, %f209, %f210;
	mov.b32 	%r49, %f211;
	mov.u32 	%r50, 2;
	shfl.sync.bfly.b32 	%r51|%p9, %r49, %r50, %r39, %r41;
	mov.b32 	%f212, %r51;
	add.f32 	%f213, %f211, %f212;
	mov.b32 	%r52, %f213;
	mov.u32 	%r53, 1;
	shfl.sync.bfly.b32 	%r54|%p10, %r52, %r53, %r39, %r41;
	mov.b32 	%f214, %r54;
	add.f32 	%f215, %f213, %f214;
	st.local.f32 	[%rd3], %f215;
	mov.b32 	%r55, %f275;
	shfl.sync.bfly.b32 	%r56|%p11, %r55, %r40, %r39, %r41;
	mov.b32 	%f216, %r56;
	add.f32 	%f217, %f275, %f216;
	mov.b32 	%r57, %f217;
	shfl.sync.bfly.b32 	%r58|%p12, %r57, %r44, %r39, %r41;
	mov.b32 	%f218, %r58;
	add.f32 	%f219, %f217, %f218;
	mov.b32 	%r59, %f219;
	shfl.sync.bfly.b32 	%r60|%p13, %r59, %r47, %r39, %r41;
	mov.b32 	%f220, %r60;
	add.f32 	%f221, %f219, %f220;
	mov.b32 	%r61, %f221;
	shfl.sync.bfly.b32 	%r62|%p14, %r61, %r50, %r39, %r41;
	mov.b32 	%f222, %r62;
	add.f32 	%f223, %f221, %f222;
	mov.b32 	%r63, %f223;
	shfl.sync.bfly.b32 	%r64|%p15, %r63, %r53, %r39, %r41;
	mov.b32 	%f224, %r64;
	add.f32 	%f225, %f223, %f224;
	st.local.f32 	[%rd3+4], %f225;
	mov.b32 	%r65, %f274;
	shfl.sync.bfly.b32 	%r66|%p16, %r65, %r40, %r39, %r41;
	mov.b32 	%f226, %r66;
	add.f32 	%f227, %f274, %f226;
	mov.b32 	%r67, %f227;
	shfl.sync.bfly.b32 	%r68|%p17, %r67, %r44, %r39, %r41;
	mov.b32 	%f228, %r68;
	add.f32 	%f229, %f227, %f228;
	mov.b32 	%r69, %f229;
	shfl.sync.bfly.b32 	%r70|%p18, %r69, %r47, %r39, %r41;
	mov.b32 	%f230, %r70;
	add.f32 	%f231, %f229, %f230;
	mov.b32 	%r71, %f231;
	shfl.sync.bfly.b32 	%r72|%p19, %r71, %r50, %r39, %r41;
	mov.b32 	%f232, %r72;
	add.f32 	%f233, %f231, %f232;
	mov.b32 	%r73, %f233;
	shfl.sync.bfly.b32 	%r74|%p20, %r73, %r53, %r39, %r41;
	mov.b32 	%f234, %r74;
	add.f32 	%f235, %f233, %f234;
	st.local.f32 	[%rd3+8], %f235;
	mov.b32 	%r75, %f273;
	shfl.sync.bfly.b32 	%r76|%p21, %r75, %r40, %r39, %r41;
	mov.b32 	%f236, %r76;
	add.f32 	%f237, %f273, %f236;
	mov.b32 	%r77, %f237;
	shfl.sync.bfly.b32 	%r78|%p22, %r77, %r44, %r39, %r41;
	mov.b32 	%f238, %r78;
	add.f32 	%f239, %f237, %f238;
	mov.b32 	%r79, %f239;
	shfl.sync.bfly.b32 	%r80|%p23, %r79, %r47, %r39, %r41;
	mov.b32 	%f240, %r80;
	add.f32 	%f241, %f239, %f240;
	mov.b32 	%r81, %f241;
	shfl.sync.bfly.b32 	%r82|%p24, %r81, %r50, %r39, %r41;
	mov.b32 	%f242, %r82;
	add.f32 	%f243, %f241, %f242;
	mov.b32 	%r83, %f243;
	shfl.sync.bfly.b32 	%r84|%p25, %r83, %r53, %r39, %r41;
	mov.b32 	%f244, %r84;
	add.f32 	%f245, %f243, %f244;
	st.local.f32 	[%rd3+12], %f245;
	mov.b32 	%r85, %f272;
	shfl.sync.bfly.b32 	%r86|%p26, %r85, %r40, %r39, %r41;
	mov.b32 	%f246, %r86;
	add.f32 	%f247, %f272, %f246;
	mov.b32 	%r87, %f247;
	shfl.sync.bfly.b32 	%r88|%p27, %r87, %r44, %r39, %r41;
	mov.b32 	%f248, %r88;
	add.f32 	%f249, %f247, %f248;
	mov.b32 	%r89, %f249;
	shfl.sync.bfly.b32 	%r90|%p28, %r89, %r47, %r39, %r41;
	mov.b32 	%f250, %r90;
	add.f32 	%f251, %f249, %f250;
	mov.b32 	%r91, %f251;
	shfl.sync.bfly.b32 	%r92|%p29, %r91, %r50, %r39, %r41;
	mov.b32 	%f252, %r92;
	add.f32 	%f253, %f251, %f252;
	mov.b32 	%r93, %f253;
	shfl.sync.bfly.b32 	%r94|%p30, %r93, %r53, %r39, %r41;
	mov.b32 	%f254, %r94;
	add.f32 	%f255, %f253, %f254;
	st.local.f32 	[%rd3+16], %f255;
	setp.gt.s32 	%p31, %r3, 4;
	@%p31 bra 	$L__BB4_11;

	mul.wide.s32 	%rd62, %r3, 4;
	add.s64 	%rd63, %rd3, %rd62;
	ld.local.f32 	%f256, [%rd63];
	mad.lo.s32 	%r95, %r3, %r15, %r2;
	cvt.s64.s32 	%rd64, %r95;
	mul.lo.s32 	%r96, %r1, %r16;
	cvt.s64.s32 	%rd65, %r96;
	add.s64 	%rd66, %rd65, %rd64;
	cvta.to.global.u64 	%rd67, %rd27;
	shl.b64 	%rd68, %rd66, 2;
	add.s64 	%rd69, %rd67, %rd68;
	st.global.f32 	[%rd69], %f256;

$L__BB4_11:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_6_bs_32
.visible .entry ggml_matvec_f32_ncols_6_bs_32(
	.param .u64 ggml_matvec_f32_ncols_6_bs_32_param_0,
	.param .u64 ggml_matvec_f32_ncols_6_bs_32_param_1,
	.param .u64 ggml_matvec_f32_ncols_6_bs_32_param_2,
	.param .u32 ggml_matvec_f32_ncols_6_bs_32_param_3,
	.param .u32 ggml_matvec_f32_ncols_6_bs_32_param_4,
	.param .u32 ggml_matvec_f32_ncols_6_bs_32_param_5,
	.param .u32 ggml_matvec_f32_ncols_6_bs_32_param_6,
	.param .u32 ggml_matvec_f32_ncols_6_bs_32_param_7,
	.param .u32 ggml_matvec_f32_ncols_6_bs_32_param_8,
	.param .u32 ggml_matvec_f32_ncols_6_bs_32_param_9,
	.param .u32 ggml_matvec_f32_ncols_6_bs_32_param_10,
	.param .u32 ggml_matvec_f32_ncols_6_bs_32_param_11
)
{
	.local .align 8 .b8 	__local_depot5[24];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<36>;
	.reg .f32 	%f<230>;
	.reg .b32 	%r<105>;
	.reg .b64 	%rd<67>;


	mov.u64 	%SPL, __local_depot5;
	ld.param.u64 	%rd20, [ggml_matvec_f32_ncols_6_bs_32_param_0];
	ld.param.u64 	%rd21, [ggml_matvec_f32_ncols_6_bs_32_param_1];
	ld.param.u64 	%rd19, [ggml_matvec_f32_ncols_6_bs_32_param_2];
	ld.param.u32 	%r9, [ggml_matvec_f32_ncols_6_bs_32_param_3];
	ld.param.u32 	%r13, [ggml_matvec_f32_ncols_6_bs_32_param_5];
	ld.param.u32 	%r10, [ggml_matvec_f32_ncols_6_bs_32_param_6];
	ld.param.u32 	%r11, [ggml_matvec_f32_ncols_6_bs_32_param_7];
	ld.param.u32 	%r14, [ggml_matvec_f32_ncols_6_bs_32_param_8];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_6_bs_32_param_9];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_6_bs_32_param_10];
	ld.param.u32 	%r12, [ggml_matvec_f32_ncols_6_bs_32_param_11];
	cvta.to.global.u64 	%rd66, %rd21;
	add.u64 	%rd2, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r17, %r1, %r14;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r18, %r2, %r13;
	mad.lo.s32 	%r19, %r17, %r15, %r18;
	cvt.s64.s32 	%rd3, %r19;
	cvta.to.global.u64 	%rd4, %rd20;
	mul.lo.s32 	%r20, %r1, %r16;
	cvt.s64.s32 	%rd5, %r20;
	mov.f32 	%f224, 0f00000000;
	st.local.v2.f32 	[%rd2], {%f224, %f224};
	st.local.v2.f32 	[%rd2+8], {%f224, %f224};
	st.local.v2.f32 	[%rd2+16], {%f224, %f224};
	mov.u32 	%r3, %tid.x;
	setp.ge.s32 	%p1, %r3, %r9;
	mov.f32 	%f225, %f224;
	mov.f32 	%f226, %f224;
	mov.f32 	%f227, %f224;
	mov.f32 	%f228, %f224;
	mov.f32 	%f229, %f224;
	@%p1 bra 	$L__BB5_7;

	not.b32 	%r21, %r3;
	add.s32 	%r4, %r21, %r9;
	and.b32  	%r22, %r4, 32;
	setp.ne.s32 	%p2, %r22, 0;
	mov.f32 	%f224, 0f00000000;
	mov.u32 	%r104, %r3;
	@%p2 bra 	$L__BB5_3;

	shl.b64 	%rd23, %rd5, 2;
	add.s64 	%rd24, %rd66, %rd23;
	shl.b64 	%rd25, %rd3, 2;
	add.s64 	%rd26, %rd4, %rd25;
	mul.wide.s32 	%rd27, %r3, 8;
	add.s64 	%rd28, %rd26, %rd27;
	ld.global.nc.v2.f32 	{%f43, %f44}, [%rd28];
	add.s64 	%rd29, %rd24, %rd27;
	ld.global.nc.v2.f32 	{%f47, %f48}, [%rd29];
	fma.rn.f32 	%f51, %f43, %f47, 0f00000000;
	fma.rn.f32 	%f229, %f44, %f48, %f51;
	mul.wide.s32 	%rd30, %r10, 8;
	add.s64 	%rd31, %rd29, %rd30;
	ld.global.nc.v2.f32 	{%f52, %f53}, [%rd31];
	fma.rn.f32 	%f56, %f43, %f52, 0f00000000;
	fma.rn.f32 	%f228, %f44, %f53, %f56;
	st.local.v2.f32 	[%rd2], {%f229, %f228};
	add.s32 	%r23, %r3, %r10;
	add.s32 	%r24, %r23, %r10;
	mul.wide.s32 	%rd32, %r24, 8;
	add.s64 	%rd33, %rd24, %rd32;
	ld.global.nc.v2.f32 	{%f57, %f58}, [%rd33];
	fma.rn.f32 	%f61, %f43, %f57, 0f00000000;
	fma.rn.f32 	%f227, %f44, %f58, %f61;
	add.s64 	%rd34, %rd33, %rd30;
	ld.global.nc.v2.f32 	{%f62, %f63}, [%rd34];
	fma.rn.f32 	%f66, %f43, %f62, 0f00000000;
	fma.rn.f32 	%f226, %f44, %f63, %f66;
	st.local.v2.f32 	[%rd2+8], {%f227, %f226};
	add.s64 	%rd35, %rd34, %rd30;
	ld.global.nc.v2.f32 	{%f67, %f68}, [%rd35];
	fma.rn.f32 	%f71, %f43, %f67, 0f00000000;
	fma.rn.f32 	%f225, %f44, %f68, %f71;
	add.s64 	%rd36, %rd35, %rd30;
	ld.global.nc.v2.f32 	{%f72, %f73}, [%rd36];
	fma.rn.f32 	%f76, %f43, %f72, 0f00000000;
	fma.rn.f32 	%f224, %f44, %f73, %f76;
	st.local.v2.f32 	[%rd2+16], {%f225, %f224};
	add.s32 	%r104, %r3, 32;

$L__BB5_3:
	and.b32  	%r25, %r4, -32;
	setp.eq.s32 	%p3, %r25, 0;
	@%p3 bra 	$L__BB5_7;

	add.s32 	%r26, %r104, %r10;
	add.s32 	%r27, %r26, 32;
	mul.wide.s32 	%rd37, %r27, 8;
	shl.b64 	%rd38, %rd5, 2;
	add.s64 	%rd7, %rd37, %rd38;
	shl.b32 	%r28, %r10, 1;
	add.s32 	%r29, %r104, %r28;
	mad.lo.s32 	%r30, %r10, 3, %r104;
	shl.b32 	%r31, %r10, 2;
	add.s32 	%r32, %r104, %r31;
	mad.lo.s32 	%r33, %r10, 5, %r104;
	mul.wide.s32 	%rd39, %r29, 8;
	add.s64 	%rd8, %rd39, %rd38;
	mul.wide.s32 	%rd40, %r30, 8;
	add.s64 	%rd9, %rd40, %rd38;
	mul.wide.s32 	%rd41, %r32, 8;
	add.s64 	%rd10, %rd41, %rd38;
	mul.wide.s32 	%rd42, %r33, 8;
	add.s64 	%rd11, %rd42, %rd38;
	mul.wide.s32 	%rd43, %r104, 2;
	add.s64 	%rd44, %rd43, %rd3;
	shl.b64 	%rd45, %rd44, 2;
	add.s64 	%rd46, %rd4, %rd45;
	add.s64 	%rd65, %rd46, 256;
	mul.wide.s32 	%rd47, %r104, 8;
	mul.wide.s32 	%rd48, %r10, 8;
	add.s64 	%rd49, %rd47, %rd48;
	add.s64 	%rd13, %rd49, %rd38;
	add.s64 	%rd14, %rd47, %rd38;

$L__BB5_5:
	ld.global.nc.v2.f32 	{%f77, %f78}, [%rd65+-256];
	add.s64 	%rd50, %rd66, %rd14;
	ld.global.nc.v2.f32 	{%f81, %f82}, [%rd50];
	fma.rn.f32 	%f85, %f77, %f81, %f229;
	fma.rn.f32 	%f86, %f78, %f82, %f85;
	add.s64 	%rd51, %rd66, %rd13;
	ld.global.nc.v2.f32 	{%f87, %f88}, [%rd51];
	fma.rn.f32 	%f91, %f77, %f87, %f228;
	fma.rn.f32 	%f92, %f78, %f88, %f91;
	add.s64 	%rd52, %rd66, %rd8;
	ld.global.nc.v2.f32 	{%f93, %f94}, [%rd52];
	fma.rn.f32 	%f97, %f77, %f93, %f227;
	fma.rn.f32 	%f98, %f78, %f94, %f97;
	add.s64 	%rd53, %rd66, %rd9;
	ld.global.nc.v2.f32 	{%f99, %f100}, [%rd53];
	fma.rn.f32 	%f103, %f77, %f99, %f226;
	fma.rn.f32 	%f104, %f78, %f100, %f103;
	add.s64 	%rd54, %rd66, %rd10;
	ld.global.nc.v2.f32 	{%f105, %f106}, [%rd54];
	fma.rn.f32 	%f109, %f77, %f105, %f225;
	fma.rn.f32 	%f110, %f78, %f106, %f109;
	add.s64 	%rd55, %rd66, %rd11;
	ld.global.nc.v2.f32 	{%f111, %f112}, [%rd55];
	fma.rn.f32 	%f115, %f77, %f111, %f224;
	fma.rn.f32 	%f116, %f78, %f112, %f115;
	ld.global.nc.v2.f32 	{%f117, %f118}, [%rd65];
	ld.global.nc.v2.f32 	{%f121, %f122}, [%rd50+256];
	fma.rn.f32 	%f125, %f117, %f121, %f86;
	fma.rn.f32 	%f229, %f118, %f122, %f125;
	add.s64 	%rd56, %rd66, %rd7;
	ld.global.nc.v2.f32 	{%f126, %f127}, [%rd56];
	fma.rn.f32 	%f130, %f117, %f126, %f92;
	fma.rn.f32 	%f228, %f118, %f127, %f130;
	ld.global.nc.v2.f32 	{%f131, %f132}, [%rd52+256];
	fma.rn.f32 	%f135, %f117, %f131, %f98;
	fma.rn.f32 	%f227, %f118, %f132, %f135;
	ld.global.nc.v2.f32 	{%f136, %f137}, [%rd53+256];
	fma.rn.f32 	%f140, %f117, %f136, %f104;
	fma.rn.f32 	%f226, %f118, %f137, %f140;
	ld.global.nc.v2.f32 	{%f141, %f142}, [%rd54+256];
	fma.rn.f32 	%f145, %f117, %f141, %f110;
	fma.rn.f32 	%f225, %f118, %f142, %f145;
	ld.global.nc.v2.f32 	{%f146, %f147}, [%rd55+256];
	fma.rn.f32 	%f150, %f117, %f146, %f116;
	fma.rn.f32 	%f224, %f118, %f147, %f150;
	add.s64 	%rd66, %rd66, 512;
	add.s64 	%rd65, %rd65, 512;
	add.s32 	%r104, %r104, 64;
	setp.lt.s32 	%p4, %r104, %r9;
	@%p4 bra 	$L__BB5_5;

	st.local.v2.f32 	[%rd2], {%f229, %f228};
	st.local.v2.f32 	[%rd2+8], {%f227, %f226};
	st.local.v2.f32 	[%rd2+16], {%f225, %f224};

$L__BB5_7:
	mov.b32 	%r34, %f229;
	mov.u32 	%r35, 31;
	mov.u32 	%r36, 16;
	mov.u32 	%r37, -1;
	shfl.sync.bfly.b32 	%r38|%p5, %r34, %r36, %r35, %r37;
	mov.b32 	%f151, %r38;
	add.f32 	%f152, %f229, %f151;
	mov.b32 	%r39, %f152;
	mov.u32 	%r40, 8;
	shfl.sync.bfly.b32 	%r41|%p6, %r39, %r40, %r35, %r37;
	mov.b32 	%f153, %r41;
	add.f32 	%f154, %f152, %f153;
	mov.b32 	%r42, %f154;
	mov.u32 	%r43, 4;
	shfl.sync.bfly.b32 	%r44|%p7, %r42, %r43, %r35, %r37;
	mov.b32 	%f155, %r44;
	add.f32 	%f156, %f154, %f155;
	mov.b32 	%r45, %f156;
	mov.u32 	%r46, 2;
	shfl.sync.bfly.b32 	%r47|%p8, %r45, %r46, %r35, %r37;
	mov.b32 	%f157, %r47;
	add.f32 	%f158, %f156, %f157;
	mov.b32 	%r48, %f158;
	mov.u32 	%r49, 1;
	shfl.sync.bfly.b32 	%r50|%p9, %r48, %r49, %r35, %r37;
	mov.b32 	%f159, %r50;
	add.f32 	%f160, %f158, %f159;
	st.local.f32 	[%rd2], %f160;
	mov.b32 	%r51, %f228;
	shfl.sync.bfly.b32 	%r52|%p10, %r51, %r36, %r35, %r37;
	mov.b32 	%f161, %r52;
	add.f32 	%f162, %f228, %f161;
	mov.b32 	%r53, %f162;
	shfl.sync.bfly.b32 	%r54|%p11, %r53, %r40, %r35, %r37;
	mov.b32 	%f163, %r54;
	add.f32 	%f164, %f162, %f163;
	mov.b32 	%r55, %f164;
	shfl.sync.bfly.b32 	%r56|%p12, %r55, %r43, %r35, %r37;
	mov.b32 	%f165, %r56;
	add.f32 	%f166, %f164, %f165;
	mov.b32 	%r57, %f166;
	shfl.sync.bfly.b32 	%r58|%p13, %r57, %r46, %r35, %r37;
	mov.b32 	%f167, %r58;
	add.f32 	%f168, %f166, %f167;
	mov.b32 	%r59, %f168;
	shfl.sync.bfly.b32 	%r60|%p14, %r59, %r49, %r35, %r37;
	mov.b32 	%f169, %r60;
	add.f32 	%f170, %f168, %f169;
	st.local.f32 	[%rd2+4], %f170;
	mov.b32 	%r61, %f227;
	shfl.sync.bfly.b32 	%r62|%p15, %r61, %r36, %r35, %r37;
	mov.b32 	%f171, %r62;
	add.f32 	%f172, %f227, %f171;
	mov.b32 	%r63, %f172;
	shfl.sync.bfly.b32 	%r64|%p16, %r63, %r40, %r35, %r37;
	mov.b32 	%f173, %r64;
	add.f32 	%f174, %f172, %f173;
	mov.b32 	%r65, %f174;
	shfl.sync.bfly.b32 	%r66|%p17, %r65, %r43, %r35, %r37;
	mov.b32 	%f175, %r66;
	add.f32 	%f176, %f174, %f175;
	mov.b32 	%r67, %f176;
	shfl.sync.bfly.b32 	%r68|%p18, %r67, %r46, %r35, %r37;
	mov.b32 	%f177, %r68;
	add.f32 	%f178, %f176, %f177;
	mov.b32 	%r69, %f178;
	shfl.sync.bfly.b32 	%r70|%p19, %r69, %r49, %r35, %r37;
	mov.b32 	%f179, %r70;
	add.f32 	%f180, %f178, %f179;
	st.local.f32 	[%rd2+8], %f180;
	mov.b32 	%r71, %f226;
	shfl.sync.bfly.b32 	%r72|%p20, %r71, %r36, %r35, %r37;
	mov.b32 	%f181, %r72;
	add.f32 	%f182, %f226, %f181;
	mov.b32 	%r73, %f182;
	shfl.sync.bfly.b32 	%r74|%p21, %r73, %r40, %r35, %r37;
	mov.b32 	%f183, %r74;
	add.f32 	%f184, %f182, %f183;
	mov.b32 	%r75, %f184;
	shfl.sync.bfly.b32 	%r76|%p22, %r75, %r43, %r35, %r37;
	mov.b32 	%f185, %r76;
	add.f32 	%f186, %f184, %f185;
	mov.b32 	%r77, %f186;
	shfl.sync.bfly.b32 	%r78|%p23, %r77, %r46, %r35, %r37;
	mov.b32 	%f187, %r78;
	add.f32 	%f188, %f186, %f187;
	mov.b32 	%r79, %f188;
	shfl.sync.bfly.b32 	%r80|%p24, %r79, %r49, %r35, %r37;
	mov.b32 	%f189, %r80;
	add.f32 	%f190, %f188, %f189;
	st.local.f32 	[%rd2+12], %f190;
	mov.b32 	%r81, %f225;
	shfl.sync.bfly.b32 	%r82|%p25, %r81, %r36, %r35, %r37;
	mov.b32 	%f191, %r82;
	add.f32 	%f192, %f225, %f191;
	mov.b32 	%r83, %f192;
	shfl.sync.bfly.b32 	%r84|%p26, %r83, %r40, %r35, %r37;
	mov.b32 	%f193, %r84;
	add.f32 	%f194, %f192, %f193;
	mov.b32 	%r85, %f194;
	shfl.sync.bfly.b32 	%r86|%p27, %r85, %r43, %r35, %r37;
	mov.b32 	%f195, %r86;
	add.f32 	%f196, %f194, %f195;
	mov.b32 	%r87, %f196;
	shfl.sync.bfly.b32 	%r88|%p28, %r87, %r46, %r35, %r37;
	mov.b32 	%f197, %r88;
	add.f32 	%f198, %f196, %f197;
	mov.b32 	%r89, %f198;
	shfl.sync.bfly.b32 	%r90|%p29, %r89, %r49, %r35, %r37;
	mov.b32 	%f199, %r90;
	add.f32 	%f200, %f198, %f199;
	st.local.f32 	[%rd2+16], %f200;
	mov.b32 	%r91, %f224;
	shfl.sync.bfly.b32 	%r92|%p30, %r91, %r36, %r35, %r37;
	mov.b32 	%f201, %r92;
	add.f32 	%f202, %f224, %f201;
	mov.b32 	%r93, %f202;
	shfl.sync.bfly.b32 	%r94|%p31, %r93, %r40, %r35, %r37;
	mov.b32 	%f203, %r94;
	add.f32 	%f204, %f202, %f203;
	mov.b32 	%r95, %f204;
	shfl.sync.bfly.b32 	%r96|%p32, %r95, %r43, %r35, %r37;
	mov.b32 	%f205, %r96;
	add.f32 	%f206, %f204, %f205;
	mov.b32 	%r97, %f206;
	shfl.sync.bfly.b32 	%r98|%p33, %r97, %r46, %r35, %r37;
	mov.b32 	%f207, %r98;
	add.f32 	%f208, %f206, %f207;
	mov.b32 	%r99, %f208;
	shfl.sync.bfly.b32 	%r100|%p34, %r99, %r49, %r35, %r37;
	mov.b32 	%f209, %r100;
	add.f32 	%f210, %f208, %f209;
	st.local.f32 	[%rd2+20], %f210;
	setp.gt.s32 	%p35, %r3, 5;
	@%p35 bra 	$L__BB5_9;

	mul.wide.s32 	%rd57, %r3, 4;
	add.s64 	%rd58, %rd2, %rd57;
	ld.local.f32 	%f211, [%rd58];
	mad.lo.s32 	%r101, %r3, %r11, %r2;
	cvt.s64.s32 	%rd59, %r101;
	mul.lo.s32 	%r102, %r1, %r12;
	cvt.s64.s32 	%rd60, %r102;
	add.s64 	%rd61, %rd60, %rd59;
	cvta.to.global.u64 	%rd62, %rd19;
	shl.b64 	%rd63, %rd61, 2;
	add.s64 	%rd64, %rd62, %rd63;
	st.global.f32 	[%rd64], %f211;

$L__BB5_9:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_7_bs_32
.visible .entry ggml_matvec_f32_ncols_7_bs_32(
	.param .u64 ggml_matvec_f32_ncols_7_bs_32_param_0,
	.param .u64 ggml_matvec_f32_ncols_7_bs_32_param_1,
	.param .u64 ggml_matvec_f32_ncols_7_bs_32_param_2,
	.param .u32 ggml_matvec_f32_ncols_7_bs_32_param_3,
	.param .u32 ggml_matvec_f32_ncols_7_bs_32_param_4,
	.param .u32 ggml_matvec_f32_ncols_7_bs_32_param_5,
	.param .u32 ggml_matvec_f32_ncols_7_bs_32_param_6,
	.param .u32 ggml_matvec_f32_ncols_7_bs_32_param_7,
	.param .u32 ggml_matvec_f32_ncols_7_bs_32_param_8,
	.param .u32 ggml_matvec_f32_ncols_7_bs_32_param_9,
	.param .u32 ggml_matvec_f32_ncols_7_bs_32_param_10,
	.param .u32 ggml_matvec_f32_ncols_7_bs_32_param_11
)
{
	.local .align 4 .b8 	__local_depot6[28];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<41>;
	.reg .f32 	%f<266>;
	.reg .b32 	%r<117>;
	.reg .b64 	%rd<70>;


	mov.u64 	%SPL, __local_depot6;
	ld.param.u64 	%rd20, [ggml_matvec_f32_ncols_7_bs_32_param_0];
	ld.param.u64 	%rd21, [ggml_matvec_f32_ncols_7_bs_32_param_1];
	ld.param.u64 	%rd19, [ggml_matvec_f32_ncols_7_bs_32_param_2];
	ld.param.u32 	%r9, [ggml_matvec_f32_ncols_7_bs_32_param_3];
	ld.param.u32 	%r13, [ggml_matvec_f32_ncols_7_bs_32_param_5];
	ld.param.u32 	%r10, [ggml_matvec_f32_ncols_7_bs_32_param_6];
	ld.param.u32 	%r11, [ggml_matvec_f32_ncols_7_bs_32_param_7];
	ld.param.u32 	%r14, [ggml_matvec_f32_ncols_7_bs_32_param_8];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_7_bs_32_param_9];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_7_bs_32_param_10];
	ld.param.u32 	%r12, [ggml_matvec_f32_ncols_7_bs_32_param_11];
	cvta.to.global.u64 	%rd69, %rd21;
	add.u64 	%rd2, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r17, %r1, %r14;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r18, %r2, %r13;
	mad.lo.s32 	%r19, %r17, %r15, %r18;
	cvt.s64.s32 	%rd3, %r19;
	cvta.to.global.u64 	%rd4, %rd20;
	mul.lo.s32 	%r20, %r1, %r16;
	cvt.s64.s32 	%rd5, %r20;
	mov.f32 	%f259, 0f00000000;
	mov.u32 	%r21, 0;
	st.local.u32 	[%rd2], %r21;
	st.local.u32 	[%rd2+4], %r21;
	st.local.u32 	[%rd2+8], %r21;
	st.local.u32 	[%rd2+12], %r21;
	st.local.u32 	[%rd2+16], %r21;
	st.local.u32 	[%rd2+20], %r21;
	st.local.u32 	[%rd2+24], %r21;
	mov.u32 	%r3, %tid.x;
	setp.ge.s32 	%p1, %r3, %r9;
	mov.f32 	%f260, %f259;
	mov.f32 	%f261, %f259;
	mov.f32 	%f262, %f259;
	mov.f32 	%f263, %f259;
	mov.f32 	%f264, %f259;
	mov.f32 	%f265, %f259;
	@%p1 bra 	$L__BB6_7;

	not.b32 	%r22, %r3;
	add.s32 	%r4, %r22, %r9;
	and.b32  	%r23, %r4, 32;
	setp.ne.s32 	%p2, %r23, 0;
	mov.f32 	%f259, 0f00000000;
	mov.u32 	%r116, %r3;
	@%p2 bra 	$L__BB6_3;

	shl.b64 	%rd23, %rd5, 2;
	add.s64 	%rd24, %rd69, %rd23;
	shl.b64 	%rd25, %rd3, 2;
	add.s64 	%rd26, %rd4, %rd25;
	mul.wide.s32 	%rd27, %r3, 8;
	add.s64 	%rd28, %rd26, %rd27;
	ld.global.nc.v2.f32 	{%f50, %f51}, [%rd28];
	add.s64 	%rd29, %rd24, %rd27;
	ld.global.nc.v2.f32 	{%f54, %f55}, [%rd29];
	fma.rn.f32 	%f58, %f50, %f54, 0f00000000;
	fma.rn.f32 	%f265, %f51, %f55, %f58;
	st.local.f32 	[%rd2], %f265;
	mul.wide.s32 	%rd30, %r10, 8;
	add.s64 	%rd31, %rd29, %rd30;
	ld.global.nc.v2.f32 	{%f59, %f60}, [%rd31];
	fma.rn.f32 	%f63, %f50, %f59, 0f00000000;
	fma.rn.f32 	%f264, %f51, %f60, %f63;
	st.local.f32 	[%rd2+4], %f264;
	add.s32 	%r24, %r3, %r10;
	add.s32 	%r25, %r24, %r10;
	mul.wide.s32 	%rd32, %r25, 8;
	add.s64 	%rd33, %rd24, %rd32;
	ld.global.nc.v2.f32 	{%f64, %f65}, [%rd33];
	fma.rn.f32 	%f68, %f50, %f64, 0f00000000;
	fma.rn.f32 	%f263, %f51, %f65, %f68;
	st.local.f32 	[%rd2+8], %f263;
	add.s64 	%rd34, %rd33, %rd30;
	ld.global.nc.v2.f32 	{%f69, %f70}, [%rd34];
	fma.rn.f32 	%f73, %f50, %f69, 0f00000000;
	fma.rn.f32 	%f262, %f51, %f70, %f73;
	st.local.f32 	[%rd2+12], %f262;
	add.s64 	%rd35, %rd34, %rd30;
	ld.global.nc.v2.f32 	{%f74, %f75}, [%rd35];
	fma.rn.f32 	%f78, %f50, %f74, 0f00000000;
	fma.rn.f32 	%f261, %f51, %f75, %f78;
	st.local.f32 	[%rd2+16], %f261;
	add.s64 	%rd36, %rd35, %rd30;
	ld.global.nc.v2.f32 	{%f79, %f80}, [%rd36];
	fma.rn.f32 	%f83, %f50, %f79, 0f00000000;
	fma.rn.f32 	%f260, %f51, %f80, %f83;
	st.local.f32 	[%rd2+20], %f260;
	add.s64 	%rd37, %rd36, %rd30;
	ld.global.nc.v2.f32 	{%f84, %f85}, [%rd37];
	fma.rn.f32 	%f88, %f50, %f84, 0f00000000;
	fma.rn.f32 	%f259, %f51, %f85, %f88;
	st.local.f32 	[%rd2+24], %f259;
	add.s32 	%r116, %r3, 32;

$L__BB6_3:
	and.b32  	%r26, %r4, -32;
	setp.eq.s32 	%p3, %r26, 0;
	@%p3 bra 	$L__BB6_7;

	add.s32 	%r27, %r116, %r10;
	add.s32 	%r28, %r27, 32;
	mul.wide.s32 	%rd38, %r28, 8;
	shl.b64 	%rd39, %rd5, 2;
	add.s64 	%rd6, %rd38, %rd39;
	shl.b32 	%r29, %r10, 1;
	add.s32 	%r30, %r116, %r29;
	mad.lo.s32 	%r31, %r10, 3, %r116;
	shl.b32 	%r32, %r10, 2;
	add.s32 	%r33, %r116, %r32;
	mad.lo.s32 	%r34, %r10, 5, %r116;
	mad.lo.s32 	%r35, %r10, 6, %r116;
	mul.wide.s32 	%rd40, %r30, 8;
	add.s64 	%rd7, %rd40, %rd39;
	mul.wide.s32 	%rd41, %r31, 8;
	add.s64 	%rd8, %rd41, %rd39;
	mul.wide.s32 	%rd42, %r33, 8;
	add.s64 	%rd9, %rd42, %rd39;
	mul.wide.s32 	%rd43, %r34, 8;
	add.s64 	%rd10, %rd43, %rd39;
	mul.wide.s32 	%rd44, %r35, 8;
	add.s64 	%rd11, %rd44, %rd39;
	mul.wide.s32 	%rd45, %r116, 2;
	add.s64 	%rd46, %rd45, %rd3;
	shl.b64 	%rd47, %rd46, 2;
	add.s64 	%rd48, %rd4, %rd47;
	add.s64 	%rd68, %rd48, 256;
	mul.wide.s32 	%rd49, %r116, 8;
	mul.wide.s32 	%rd50, %r10, 8;
	add.s64 	%rd51, %rd49, %rd50;
	add.s64 	%rd13, %rd51, %rd39;
	add.s64 	%rd14, %rd49, %rd39;

$L__BB6_5:
	ld.global.nc.v2.f32 	{%f89, %f90}, [%rd68+-256];
	add.s64 	%rd52, %rd69, %rd14;
	ld.global.nc.v2.f32 	{%f93, %f94}, [%rd52];
	fma.rn.f32 	%f97, %f89, %f93, %f265;
	fma.rn.f32 	%f98, %f90, %f94, %f97;
	add.s64 	%rd53, %rd69, %rd13;
	ld.global.nc.v2.f32 	{%f99, %f100}, [%rd53];
	fma.rn.f32 	%f103, %f89, %f99, %f264;
	fma.rn.f32 	%f104, %f90, %f100, %f103;
	add.s64 	%rd54, %rd69, %rd7;
	ld.global.nc.v2.f32 	{%f105, %f106}, [%rd54];
	fma.rn.f32 	%f109, %f89, %f105, %f263;
	fma.rn.f32 	%f110, %f90, %f106, %f109;
	add.s64 	%rd55, %rd69, %rd8;
	ld.global.nc.v2.f32 	{%f111, %f112}, [%rd55];
	fma.rn.f32 	%f115, %f89, %f111, %f262;
	fma.rn.f32 	%f116, %f90, %f112, %f115;
	add.s64 	%rd56, %rd69, %rd9;
	ld.global.nc.v2.f32 	{%f117, %f118}, [%rd56];
	fma.rn.f32 	%f121, %f89, %f117, %f261;
	fma.rn.f32 	%f122, %f90, %f118, %f121;
	add.s64 	%rd57, %rd69, %rd10;
	ld.global.nc.v2.f32 	{%f123, %f124}, [%rd57];
	fma.rn.f32 	%f127, %f89, %f123, %f260;
	fma.rn.f32 	%f128, %f90, %f124, %f127;
	add.s64 	%rd58, %rd69, %rd11;
	ld.global.nc.v2.f32 	{%f129, %f130}, [%rd58];
	fma.rn.f32 	%f133, %f89, %f129, %f259;
	fma.rn.f32 	%f134, %f90, %f130, %f133;
	ld.global.nc.v2.f32 	{%f135, %f136}, [%rd68];
	ld.global.nc.v2.f32 	{%f139, %f140}, [%rd52+256];
	fma.rn.f32 	%f143, %f135, %f139, %f98;
	fma.rn.f32 	%f265, %f136, %f140, %f143;
	add.s64 	%rd59, %rd69, %rd6;
	ld.global.nc.v2.f32 	{%f144, %f145}, [%rd59];
	fma.rn.f32 	%f148, %f135, %f144, %f104;
	fma.rn.f32 	%f264, %f136, %f145, %f148;
	ld.global.nc.v2.f32 	{%f149, %f150}, [%rd54+256];
	fma.rn.f32 	%f153, %f135, %f149, %f110;
	fma.rn.f32 	%f263, %f136, %f150, %f153;
	ld.global.nc.v2.f32 	{%f154, %f155}, [%rd55+256];
	fma.rn.f32 	%f158, %f135, %f154, %f116;
	fma.rn.f32 	%f262, %f136, %f155, %f158;
	ld.global.nc.v2.f32 	{%f159, %f160}, [%rd56+256];
	fma.rn.f32 	%f163, %f135, %f159, %f122;
	fma.rn.f32 	%f261, %f136, %f160, %f163;
	ld.global.nc.v2.f32 	{%f164, %f165}, [%rd57+256];
	fma.rn.f32 	%f168, %f135, %f164, %f128;
	fma.rn.f32 	%f260, %f136, %f165, %f168;
	ld.global.nc.v2.f32 	{%f169, %f170}, [%rd58+256];
	fma.rn.f32 	%f173, %f135, %f169, %f134;
	fma.rn.f32 	%f259, %f136, %f170, %f173;
	add.s64 	%rd69, %rd69, 512;
	add.s64 	%rd68, %rd68, 512;
	add.s32 	%r116, %r116, 64;
	setp.lt.s32 	%p4, %r116, %r9;
	@%p4 bra 	$L__BB6_5;

	st.local.f32 	[%rd2], %f265;
	st.local.f32 	[%rd2+4], %f264;
	st.local.f32 	[%rd2+8], %f263;
	st.local.f32 	[%rd2+12], %f262;
	st.local.f32 	[%rd2+16], %f261;
	st.local.f32 	[%rd2+20], %f260;
	st.local.f32 	[%rd2+24], %f259;

$L__BB6_7:
	mov.b32 	%r36, %f265;
	mov.u32 	%r37, 31;
	mov.u32 	%r38, 16;
	mov.u32 	%r39, -1;
	shfl.sync.bfly.b32 	%r40|%p5, %r36, %r38, %r37, %r39;
	mov.b32 	%f174, %r40;
	add.f32 	%f175, %f265, %f174;
	mov.b32 	%r41, %f175;
	mov.u32 	%r42, 8;
	shfl.sync.bfly.b32 	%r43|%p6, %r41, %r42, %r37, %r39;
	mov.b32 	%f176, %r43;
	add.f32 	%f177, %f175, %f176;
	mov.b32 	%r44, %f177;
	mov.u32 	%r45, 4;
	shfl.sync.bfly.b32 	%r46|%p7, %r44, %r45, %r37, %r39;
	mov.b32 	%f178, %r46;
	add.f32 	%f179, %f177, %f178;
	mov.b32 	%r47, %f179;
	mov.u32 	%r48, 2;
	shfl.sync.bfly.b32 	%r49|%p8, %r47, %r48, %r37, %r39;
	mov.b32 	%f180, %r49;
	add.f32 	%f181, %f179, %f180;
	mov.b32 	%r50, %f181;
	mov.u32 	%r51, 1;
	shfl.sync.bfly.b32 	%r52|%p9, %r50, %r51, %r37, %r39;
	mov.b32 	%f182, %r52;
	add.f32 	%f183, %f181, %f182;
	st.local.f32 	[%rd2], %f183;
	mov.b32 	%r53, %f264;
	shfl.sync.bfly.b32 	%r54|%p10, %r53, %r38, %r37, %r39;
	mov.b32 	%f184, %r54;
	add.f32 	%f185, %f264, %f184;
	mov.b32 	%r55, %f185;
	shfl.sync.bfly.b32 	%r56|%p11, %r55, %r42, %r37, %r39;
	mov.b32 	%f186, %r56;
	add.f32 	%f187, %f185, %f186;
	mov.b32 	%r57, %f187;
	shfl.sync.bfly.b32 	%r58|%p12, %r57, %r45, %r37, %r39;
	mov.b32 	%f188, %r58;
	add.f32 	%f189, %f187, %f188;
	mov.b32 	%r59, %f189;
	shfl.sync.bfly.b32 	%r60|%p13, %r59, %r48, %r37, %r39;
	mov.b32 	%f190, %r60;
	add.f32 	%f191, %f189, %f190;
	mov.b32 	%r61, %f191;
	shfl.sync.bfly.b32 	%r62|%p14, %r61, %r51, %r37, %r39;
	mov.b32 	%f192, %r62;
	add.f32 	%f193, %f191, %f192;
	st.local.f32 	[%rd2+4], %f193;
	mov.b32 	%r63, %f263;
	shfl.sync.bfly.b32 	%r64|%p15, %r63, %r38, %r37, %r39;
	mov.b32 	%f194, %r64;
	add.f32 	%f195, %f263, %f194;
	mov.b32 	%r65, %f195;
	shfl.sync.bfly.b32 	%r66|%p16, %r65, %r42, %r37, %r39;
	mov.b32 	%f196, %r66;
	add.f32 	%f197, %f195, %f196;
	mov.b32 	%r67, %f197;
	shfl.sync.bfly.b32 	%r68|%p17, %r67, %r45, %r37, %r39;
	mov.b32 	%f198, %r68;
	add.f32 	%f199, %f197, %f198;
	mov.b32 	%r69, %f199;
	shfl.sync.bfly.b32 	%r70|%p18, %r69, %r48, %r37, %r39;
	mov.b32 	%f200, %r70;
	add.f32 	%f201, %f199, %f200;
	mov.b32 	%r71, %f201;
	shfl.sync.bfly.b32 	%r72|%p19, %r71, %r51, %r37, %r39;
	mov.b32 	%f202, %r72;
	add.f32 	%f203, %f201, %f202;
	st.local.f32 	[%rd2+8], %f203;
	mov.b32 	%r73, %f262;
	shfl.sync.bfly.b32 	%r74|%p20, %r73, %r38, %r37, %r39;
	mov.b32 	%f204, %r74;
	add.f32 	%f205, %f262, %f204;
	mov.b32 	%r75, %f205;
	shfl.sync.bfly.b32 	%r76|%p21, %r75, %r42, %r37, %r39;
	mov.b32 	%f206, %r76;
	add.f32 	%f207, %f205, %f206;
	mov.b32 	%r77, %f207;
	shfl.sync.bfly.b32 	%r78|%p22, %r77, %r45, %r37, %r39;
	mov.b32 	%f208, %r78;
	add.f32 	%f209, %f207, %f208;
	mov.b32 	%r79, %f209;
	shfl.sync.bfly.b32 	%r80|%p23, %r79, %r48, %r37, %r39;
	mov.b32 	%f210, %r80;
	add.f32 	%f211, %f209, %f210;
	mov.b32 	%r81, %f211;
	shfl.sync.bfly.b32 	%r82|%p24, %r81, %r51, %r37, %r39;
	mov.b32 	%f212, %r82;
	add.f32 	%f213, %f211, %f212;
	st.local.f32 	[%rd2+12], %f213;
	mov.b32 	%r83, %f261;
	shfl.sync.bfly.b32 	%r84|%p25, %r83, %r38, %r37, %r39;
	mov.b32 	%f214, %r84;
	add.f32 	%f215, %f261, %f214;
	mov.b32 	%r85, %f215;
	shfl.sync.bfly.b32 	%r86|%p26, %r85, %r42, %r37, %r39;
	mov.b32 	%f216, %r86;
	add.f32 	%f217, %f215, %f216;
	mov.b32 	%r87, %f217;
	shfl.sync.bfly.b32 	%r88|%p27, %r87, %r45, %r37, %r39;
	mov.b32 	%f218, %r88;
	add.f32 	%f219, %f217, %f218;
	mov.b32 	%r89, %f219;
	shfl.sync.bfly.b32 	%r90|%p28, %r89, %r48, %r37, %r39;
	mov.b32 	%f220, %r90;
	add.f32 	%f221, %f219, %f220;
	mov.b32 	%r91, %f221;
	shfl.sync.bfly.b32 	%r92|%p29, %r91, %r51, %r37, %r39;
	mov.b32 	%f222, %r92;
	add.f32 	%f223, %f221, %f222;
	st.local.f32 	[%rd2+16], %f223;
	mov.b32 	%r93, %f260;
	shfl.sync.bfly.b32 	%r94|%p30, %r93, %r38, %r37, %r39;
	mov.b32 	%f224, %r94;
	add.f32 	%f225, %f260, %f224;
	mov.b32 	%r95, %f225;
	shfl.sync.bfly.b32 	%r96|%p31, %r95, %r42, %r37, %r39;
	mov.b32 	%f226, %r96;
	add.f32 	%f227, %f225, %f226;
	mov.b32 	%r97, %f227;
	shfl.sync.bfly.b32 	%r98|%p32, %r97, %r45, %r37, %r39;
	mov.b32 	%f228, %r98;
	add.f32 	%f229, %f227, %f228;
	mov.b32 	%r99, %f229;
	shfl.sync.bfly.b32 	%r100|%p33, %r99, %r48, %r37, %r39;
	mov.b32 	%f230, %r100;
	add.f32 	%f231, %f229, %f230;
	mov.b32 	%r101, %f231;
	shfl.sync.bfly.b32 	%r102|%p34, %r101, %r51, %r37, %r39;
	mov.b32 	%f232, %r102;
	add.f32 	%f233, %f231, %f232;
	st.local.f32 	[%rd2+20], %f233;
	mov.b32 	%r103, %f259;
	shfl.sync.bfly.b32 	%r104|%p35, %r103, %r38, %r37, %r39;
	mov.b32 	%f234, %r104;
	add.f32 	%f235, %f259, %f234;
	mov.b32 	%r105, %f235;
	shfl.sync.bfly.b32 	%r106|%p36, %r105, %r42, %r37, %r39;
	mov.b32 	%f236, %r106;
	add.f32 	%f237, %f235, %f236;
	mov.b32 	%r107, %f237;
	shfl.sync.bfly.b32 	%r108|%p37, %r107, %r45, %r37, %r39;
	mov.b32 	%f238, %r108;
	add.f32 	%f239, %f237, %f238;
	mov.b32 	%r109, %f239;
	shfl.sync.bfly.b32 	%r110|%p38, %r109, %r48, %r37, %r39;
	mov.b32 	%f240, %r110;
	add.f32 	%f241, %f239, %f240;
	mov.b32 	%r111, %f241;
	shfl.sync.bfly.b32 	%r112|%p39, %r111, %r51, %r37, %r39;
	mov.b32 	%f242, %r112;
	add.f32 	%f243, %f241, %f242;
	st.local.f32 	[%rd2+24], %f243;
	setp.gt.s32 	%p40, %r3, 6;
	@%p40 bra 	$L__BB6_9;

	mul.wide.s32 	%rd60, %r3, 4;
	add.s64 	%rd61, %rd2, %rd60;
	ld.local.f32 	%f244, [%rd61];
	mad.lo.s32 	%r113, %r3, %r11, %r2;
	cvt.s64.s32 	%rd62, %r113;
	mul.lo.s32 	%r114, %r1, %r12;
	cvt.s64.s32 	%rd63, %r114;
	add.s64 	%rd64, %rd63, %rd62;
	cvta.to.global.u64 	%rd65, %rd19;
	shl.b64 	%rd66, %rd64, 2;
	add.s64 	%rd67, %rd65, %rd66;
	st.global.f32 	[%rd67], %f244;

$L__BB6_9:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_8_bs_32
.visible .entry ggml_matvec_f32_ncols_8_bs_32(
	.param .u64 ggml_matvec_f32_ncols_8_bs_32_param_0,
	.param .u64 ggml_matvec_f32_ncols_8_bs_32_param_1,
	.param .u64 ggml_matvec_f32_ncols_8_bs_32_param_2,
	.param .u32 ggml_matvec_f32_ncols_8_bs_32_param_3,
	.param .u32 ggml_matvec_f32_ncols_8_bs_32_param_4,
	.param .u32 ggml_matvec_f32_ncols_8_bs_32_param_5,
	.param .u32 ggml_matvec_f32_ncols_8_bs_32_param_6,
	.param .u32 ggml_matvec_f32_ncols_8_bs_32_param_7,
	.param .u32 ggml_matvec_f32_ncols_8_bs_32_param_8,
	.param .u32 ggml_matvec_f32_ncols_8_bs_32_param_9,
	.param .u32 ggml_matvec_f32_ncols_8_bs_32_param_10,
	.param .u32 ggml_matvec_f32_ncols_8_bs_32_param_11
)
{
	.local .align 16 .b8 	__local_depot7[32];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<46>;
	.reg .f32 	%f<302>;
	.reg .b32 	%r<127>;
	.reg .b64 	%rd<74>;


	mov.u64 	%SPL, __local_depot7;
	ld.param.u64 	%rd21, [ggml_matvec_f32_ncols_8_bs_32_param_0];
	ld.param.u64 	%rd22, [ggml_matvec_f32_ncols_8_bs_32_param_1];
	ld.param.u64 	%rd20, [ggml_matvec_f32_ncols_8_bs_32_param_2];
	ld.param.u32 	%r9, [ggml_matvec_f32_ncols_8_bs_32_param_3];
	ld.param.u32 	%r13, [ggml_matvec_f32_ncols_8_bs_32_param_5];
	ld.param.u32 	%r10, [ggml_matvec_f32_ncols_8_bs_32_param_6];
	ld.param.u32 	%r11, [ggml_matvec_f32_ncols_8_bs_32_param_7];
	ld.param.u32 	%r14, [ggml_matvec_f32_ncols_8_bs_32_param_8];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_8_bs_32_param_9];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_8_bs_32_param_10];
	ld.param.u32 	%r12, [ggml_matvec_f32_ncols_8_bs_32_param_11];
	cvta.to.global.u64 	%rd73, %rd22;
	add.u64 	%rd2, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r17, %r1, %r14;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r18, %r2, %r13;
	mad.lo.s32 	%r19, %r17, %r15, %r18;
	cvt.s64.s32 	%rd3, %r19;
	cvta.to.global.u64 	%rd4, %rd21;
	mul.lo.s32 	%r20, %r1, %r16;
	cvt.s64.s32 	%rd5, %r20;
	mov.f32 	%f294, 0f00000000;
	st.local.v4.f32 	[%rd2], {%f294, %f294, %f294, %f294};
	st.local.v4.f32 	[%rd2+16], {%f294, %f294, %f294, %f294};
	mov.u32 	%r3, %tid.x;
	setp.ge.s32 	%p1, %r3, %r9;
	mov.f32 	%f295, %f294;
	mov.f32 	%f296, %f294;
	mov.f32 	%f297, %f294;
	mov.f32 	%f298, %f294;
	mov.f32 	%f299, %f294;
	mov.f32 	%f300, %f294;
	mov.f32 	%f301, %f294;
	@%p1 bra 	$L__BB7_7;

	not.b32 	%r21, %r3;
	add.s32 	%r4, %r21, %r9;
	and.b32  	%r22, %r4, 32;
	setp.ne.s32 	%p2, %r22, 0;
	mov.f32 	%f294, 0f00000000;
	mov.u32 	%r126, %r3;
	@%p2 bra 	$L__BB7_3;

	shl.b64 	%rd24, %rd5, 2;
	add.s64 	%rd25, %rd73, %rd24;
	shl.b64 	%rd26, %rd3, 2;
	add.s64 	%rd27, %rd4, %rd26;
	mul.wide.s32 	%rd28, %r3, 8;
	add.s64 	%rd29, %rd27, %rd28;
	ld.global.nc.v2.f32 	{%f57, %f58}, [%rd29];
	add.s64 	%rd30, %rd25, %rd28;
	ld.global.nc.v2.f32 	{%f61, %f62}, [%rd30];
	fma.rn.f32 	%f65, %f57, %f61, 0f00000000;
	fma.rn.f32 	%f301, %f58, %f62, %f65;
	mul.wide.s32 	%rd31, %r10, 8;
	add.s64 	%rd32, %rd30, %rd31;
	ld.global.nc.v2.f32 	{%f66, %f67}, [%rd32];
	fma.rn.f32 	%f70, %f57, %f66, 0f00000000;
	fma.rn.f32 	%f300, %f58, %f67, %f70;
	add.s32 	%r23, %r3, %r10;
	add.s32 	%r24, %r23, %r10;
	mul.wide.s32 	%rd33, %r24, 8;
	add.s64 	%rd34, %rd25, %rd33;
	ld.global.nc.v2.f32 	{%f71, %f72}, [%rd34];
	fma.rn.f32 	%f75, %f57, %f71, 0f00000000;
	fma.rn.f32 	%f299, %f58, %f72, %f75;
	add.s64 	%rd35, %rd34, %rd31;
	ld.global.nc.v2.f32 	{%f76, %f77}, [%rd35];
	fma.rn.f32 	%f80, %f57, %f76, 0f00000000;
	fma.rn.f32 	%f298, %f58, %f77, %f80;
	st.local.v4.f32 	[%rd2], {%f301, %f300, %f299, %f298};
	add.s64 	%rd36, %rd35, %rd31;
	ld.global.nc.v2.f32 	{%f81, %f82}, [%rd36];
	fma.rn.f32 	%f85, %f57, %f81, 0f00000000;
	fma.rn.f32 	%f297, %f58, %f82, %f85;
	add.s64 	%rd37, %rd36, %rd31;
	ld.global.nc.v2.f32 	{%f86, %f87}, [%rd37];
	fma.rn.f32 	%f90, %f57, %f86, 0f00000000;
	fma.rn.f32 	%f296, %f58, %f87, %f90;
	add.s64 	%rd38, %rd37, %rd31;
	ld.global.nc.v2.f32 	{%f91, %f92}, [%rd38];
	fma.rn.f32 	%f95, %f57, %f91, 0f00000000;
	fma.rn.f32 	%f295, %f58, %f92, %f95;
	add.s64 	%rd39, %rd38, %rd31;
	ld.global.nc.v2.f32 	{%f96, %f97}, [%rd39];
	fma.rn.f32 	%f100, %f57, %f96, 0f00000000;
	fma.rn.f32 	%f294, %f58, %f97, %f100;
	st.local.v4.f32 	[%rd2+16], {%f297, %f296, %f295, %f294};
	add.s32 	%r126, %r3, 32;

$L__BB7_3:
	and.b32  	%r25, %r4, -32;
	setp.eq.s32 	%p3, %r25, 0;
	@%p3 bra 	$L__BB7_7;

	add.s32 	%r26, %r126, %r10;
	add.s32 	%r27, %r26, 32;
	mul.wide.s32 	%rd40, %r27, 8;
	shl.b64 	%rd41, %rd5, 2;
	add.s64 	%rd6, %rd40, %rd41;
	shl.b32 	%r28, %r10, 1;
	add.s32 	%r29, %r126, %r28;
	mad.lo.s32 	%r30, %r10, 3, %r126;
	shl.b32 	%r31, %r10, 2;
	add.s32 	%r32, %r126, %r31;
	mad.lo.s32 	%r33, %r10, 5, %r126;
	mad.lo.s32 	%r34, %r10, 6, %r126;
	mad.lo.s32 	%r35, %r10, 7, %r126;
	mul.wide.s32 	%rd42, %r29, 8;
	add.s64 	%rd7, %rd42, %rd41;
	mul.wide.s32 	%rd43, %r30, 8;
	add.s64 	%rd8, %rd43, %rd41;
	mul.wide.s32 	%rd44, %r32, 8;
	add.s64 	%rd9, %rd44, %rd41;
	mul.wide.s32 	%rd45, %r33, 8;
	add.s64 	%rd10, %rd45, %rd41;
	mul.wide.s32 	%rd46, %r34, 8;
	add.s64 	%rd11, %rd46, %rd41;
	mul.wide.s32 	%rd47, %r35, 8;
	add.s64 	%rd12, %rd47, %rd41;
	mul.wide.s32 	%rd48, %r126, 2;
	add.s64 	%rd49, %rd48, %rd3;
	shl.b64 	%rd50, %rd49, 2;
	add.s64 	%rd51, %rd4, %rd50;
	add.s64 	%rd72, %rd51, 256;
	mul.wide.s32 	%rd52, %r126, 8;
	mul.wide.s32 	%rd53, %r10, 8;
	add.s64 	%rd54, %rd52, %rd53;
	add.s64 	%rd14, %rd54, %rd41;
	add.s64 	%rd15, %rd52, %rd41;

$L__BB7_5:
	ld.global.nc.v2.f32 	{%f101, %f102}, [%rd72+-256];
	add.s64 	%rd55, %rd73, %rd15;
	ld.global.nc.v2.f32 	{%f105, %f106}, [%rd55];
	fma.rn.f32 	%f109, %f101, %f105, %f301;
	fma.rn.f32 	%f110, %f102, %f106, %f109;
	add.s64 	%rd56, %rd73, %rd14;
	ld.global.nc.v2.f32 	{%f111, %f112}, [%rd56];
	fma.rn.f32 	%f115, %f101, %f111, %f300;
	fma.rn.f32 	%f116, %f102, %f112, %f115;
	add.s64 	%rd57, %rd73, %rd7;
	ld.global.nc.v2.f32 	{%f117, %f118}, [%rd57];
	fma.rn.f32 	%f121, %f101, %f117, %f299;
	fma.rn.f32 	%f122, %f102, %f118, %f121;
	add.s64 	%rd58, %rd73, %rd8;
	ld.global.nc.v2.f32 	{%f123, %f124}, [%rd58];
	fma.rn.f32 	%f127, %f101, %f123, %f298;
	fma.rn.f32 	%f128, %f102, %f124, %f127;
	add.s64 	%rd59, %rd73, %rd9;
	ld.global.nc.v2.f32 	{%f129, %f130}, [%rd59];
	fma.rn.f32 	%f133, %f101, %f129, %f297;
	fma.rn.f32 	%f134, %f102, %f130, %f133;
	add.s64 	%rd60, %rd73, %rd10;
	ld.global.nc.v2.f32 	{%f135, %f136}, [%rd60];
	fma.rn.f32 	%f139, %f101, %f135, %f296;
	fma.rn.f32 	%f140, %f102, %f136, %f139;
	add.s64 	%rd61, %rd73, %rd11;
	ld.global.nc.v2.f32 	{%f141, %f142}, [%rd61];
	fma.rn.f32 	%f145, %f101, %f141, %f295;
	fma.rn.f32 	%f146, %f102, %f142, %f145;
	add.s64 	%rd62, %rd73, %rd12;
	ld.global.nc.v2.f32 	{%f147, %f148}, [%rd62];
	fma.rn.f32 	%f151, %f101, %f147, %f294;
	fma.rn.f32 	%f152, %f102, %f148, %f151;
	ld.global.nc.v2.f32 	{%f153, %f154}, [%rd72];
	ld.global.nc.v2.f32 	{%f157, %f158}, [%rd55+256];
	fma.rn.f32 	%f161, %f153, %f157, %f110;
	fma.rn.f32 	%f301, %f154, %f158, %f161;
	add.s64 	%rd63, %rd73, %rd6;
	ld.global.nc.v2.f32 	{%f162, %f163}, [%rd63];
	fma.rn.f32 	%f166, %f153, %f162, %f116;
	fma.rn.f32 	%f300, %f154, %f163, %f166;
	ld.global.nc.v2.f32 	{%f167, %f168}, [%rd57+256];
	fma.rn.f32 	%f171, %f153, %f167, %f122;
	fma.rn.f32 	%f299, %f154, %f168, %f171;
	ld.global.nc.v2.f32 	{%f172, %f173}, [%rd58+256];
	fma.rn.f32 	%f176, %f153, %f172, %f128;
	fma.rn.f32 	%f298, %f154, %f173, %f176;
	ld.global.nc.v2.f32 	{%f177, %f178}, [%rd59+256];
	fma.rn.f32 	%f181, %f153, %f177, %f134;
	fma.rn.f32 	%f297, %f154, %f178, %f181;
	ld.global.nc.v2.f32 	{%f182, %f183}, [%rd60+256];
	fma.rn.f32 	%f186, %f153, %f182, %f140;
	fma.rn.f32 	%f296, %f154, %f183, %f186;
	ld.global.nc.v2.f32 	{%f187, %f188}, [%rd61+256];
	fma.rn.f32 	%f191, %f153, %f187, %f146;
	fma.rn.f32 	%f295, %f154, %f188, %f191;
	ld.global.nc.v2.f32 	{%f192, %f193}, [%rd62+256];
	fma.rn.f32 	%f196, %f153, %f192, %f152;
	fma.rn.f32 	%f294, %f154, %f193, %f196;
	add.s64 	%rd73, %rd73, 512;
	add.s64 	%rd72, %rd72, 512;
	add.s32 	%r126, %r126, 64;
	setp.lt.s32 	%p4, %r126, %r9;
	@%p4 bra 	$L__BB7_5;

	st.local.v4.f32 	[%rd2], {%f301, %f300, %f299, %f298};
	st.local.v4.f32 	[%rd2+16], {%f297, %f296, %f295, %f294};

$L__BB7_7:
	mov.b32 	%r36, %f301;
	mov.u32 	%r37, 31;
	mov.u32 	%r38, 16;
	mov.u32 	%r39, -1;
	shfl.sync.bfly.b32 	%r40|%p5, %r36, %r38, %r37, %r39;
	mov.b32 	%f197, %r40;
	add.f32 	%f198, %f301, %f197;
	mov.b32 	%r41, %f198;
	mov.u32 	%r42, 8;
	shfl.sync.bfly.b32 	%r43|%p6, %r41, %r42, %r37, %r39;
	mov.b32 	%f199, %r43;
	add.f32 	%f200, %f198, %f199;
	mov.b32 	%r44, %f200;
	mov.u32 	%r45, 4;
	shfl.sync.bfly.b32 	%r46|%p7, %r44, %r45, %r37, %r39;
	mov.b32 	%f201, %r46;
	add.f32 	%f202, %f200, %f201;
	mov.b32 	%r47, %f202;
	mov.u32 	%r48, 2;
	shfl.sync.bfly.b32 	%r49|%p8, %r47, %r48, %r37, %r39;
	mov.b32 	%f203, %r49;
	add.f32 	%f204, %f202, %f203;
	mov.b32 	%r50, %f204;
	mov.u32 	%r51, 1;
	shfl.sync.bfly.b32 	%r52|%p9, %r50, %r51, %r37, %r39;
	mov.b32 	%f205, %r52;
	add.f32 	%f206, %f204, %f205;
	st.local.f32 	[%rd2], %f206;
	mov.b32 	%r53, %f300;
	shfl.sync.bfly.b32 	%r54|%p10, %r53, %r38, %r37, %r39;
	mov.b32 	%f207, %r54;
	add.f32 	%f208, %f300, %f207;
	mov.b32 	%r55, %f208;
	shfl.sync.bfly.b32 	%r56|%p11, %r55, %r42, %r37, %r39;
	mov.b32 	%f209, %r56;
	add.f32 	%f210, %f208, %f209;
	mov.b32 	%r57, %f210;
	shfl.sync.bfly.b32 	%r58|%p12, %r57, %r45, %r37, %r39;
	mov.b32 	%f211, %r58;
	add.f32 	%f212, %f210, %f211;
	mov.b32 	%r59, %f212;
	shfl.sync.bfly.b32 	%r60|%p13, %r59, %r48, %r37, %r39;
	mov.b32 	%f213, %r60;
	add.f32 	%f214, %f212, %f213;
	mov.b32 	%r61, %f214;
	shfl.sync.bfly.b32 	%r62|%p14, %r61, %r51, %r37, %r39;
	mov.b32 	%f215, %r62;
	add.f32 	%f216, %f214, %f215;
	st.local.f32 	[%rd2+4], %f216;
	mov.b32 	%r63, %f299;
	shfl.sync.bfly.b32 	%r64|%p15, %r63, %r38, %r37, %r39;
	mov.b32 	%f217, %r64;
	add.f32 	%f218, %f299, %f217;
	mov.b32 	%r65, %f218;
	shfl.sync.bfly.b32 	%r66|%p16, %r65, %r42, %r37, %r39;
	mov.b32 	%f219, %r66;
	add.f32 	%f220, %f218, %f219;
	mov.b32 	%r67, %f220;
	shfl.sync.bfly.b32 	%r68|%p17, %r67, %r45, %r37, %r39;
	mov.b32 	%f221, %r68;
	add.f32 	%f222, %f220, %f221;
	mov.b32 	%r69, %f222;
	shfl.sync.bfly.b32 	%r70|%p18, %r69, %r48, %r37, %r39;
	mov.b32 	%f223, %r70;
	add.f32 	%f224, %f222, %f223;
	mov.b32 	%r71, %f224;
	shfl.sync.bfly.b32 	%r72|%p19, %r71, %r51, %r37, %r39;
	mov.b32 	%f225, %r72;
	add.f32 	%f226, %f224, %f225;
	st.local.f32 	[%rd2+8], %f226;
	mov.b32 	%r73, %f298;
	shfl.sync.bfly.b32 	%r74|%p20, %r73, %r38, %r37, %r39;
	mov.b32 	%f227, %r74;
	add.f32 	%f228, %f298, %f227;
	mov.b32 	%r75, %f228;
	shfl.sync.bfly.b32 	%r76|%p21, %r75, %r42, %r37, %r39;
	mov.b32 	%f229, %r76;
	add.f32 	%f230, %f228, %f229;
	mov.b32 	%r77, %f230;
	shfl.sync.bfly.b32 	%r78|%p22, %r77, %r45, %r37, %r39;
	mov.b32 	%f231, %r78;
	add.f32 	%f232, %f230, %f231;
	mov.b32 	%r79, %f232;
	shfl.sync.bfly.b32 	%r80|%p23, %r79, %r48, %r37, %r39;
	mov.b32 	%f233, %r80;
	add.f32 	%f234, %f232, %f233;
	mov.b32 	%r81, %f234;
	shfl.sync.bfly.b32 	%r82|%p24, %r81, %r51, %r37, %r39;
	mov.b32 	%f235, %r82;
	add.f32 	%f236, %f234, %f235;
	st.local.f32 	[%rd2+12], %f236;
	mov.b32 	%r83, %f297;
	shfl.sync.bfly.b32 	%r84|%p25, %r83, %r38, %r37, %r39;
	mov.b32 	%f237, %r84;
	add.f32 	%f238, %f297, %f237;
	mov.b32 	%r85, %f238;
	shfl.sync.bfly.b32 	%r86|%p26, %r85, %r42, %r37, %r39;
	mov.b32 	%f239, %r86;
	add.f32 	%f240, %f238, %f239;
	mov.b32 	%r87, %f240;
	shfl.sync.bfly.b32 	%r88|%p27, %r87, %r45, %r37, %r39;
	mov.b32 	%f241, %r88;
	add.f32 	%f242, %f240, %f241;
	mov.b32 	%r89, %f242;
	shfl.sync.bfly.b32 	%r90|%p28, %r89, %r48, %r37, %r39;
	mov.b32 	%f243, %r90;
	add.f32 	%f244, %f242, %f243;
	mov.b32 	%r91, %f244;
	shfl.sync.bfly.b32 	%r92|%p29, %r91, %r51, %r37, %r39;
	mov.b32 	%f245, %r92;
	add.f32 	%f246, %f244, %f245;
	st.local.f32 	[%rd2+16], %f246;
	mov.b32 	%r93, %f296;
	shfl.sync.bfly.b32 	%r94|%p30, %r93, %r38, %r37, %r39;
	mov.b32 	%f247, %r94;
	add.f32 	%f248, %f296, %f247;
	mov.b32 	%r95, %f248;
	shfl.sync.bfly.b32 	%r96|%p31, %r95, %r42, %r37, %r39;
	mov.b32 	%f249, %r96;
	add.f32 	%f250, %f248, %f249;
	mov.b32 	%r97, %f250;
	shfl.sync.bfly.b32 	%r98|%p32, %r97, %r45, %r37, %r39;
	mov.b32 	%f251, %r98;
	add.f32 	%f252, %f250, %f251;
	mov.b32 	%r99, %f252;
	shfl.sync.bfly.b32 	%r100|%p33, %r99, %r48, %r37, %r39;
	mov.b32 	%f253, %r100;
	add.f32 	%f254, %f252, %f253;
	mov.b32 	%r101, %f254;
	shfl.sync.bfly.b32 	%r102|%p34, %r101, %r51, %r37, %r39;
	mov.b32 	%f255, %r102;
	add.f32 	%f256, %f254, %f255;
	st.local.f32 	[%rd2+20], %f256;
	mov.b32 	%r103, %f295;
	shfl.sync.bfly.b32 	%r104|%p35, %r103, %r38, %r37, %r39;
	mov.b32 	%f257, %r104;
	add.f32 	%f258, %f295, %f257;
	mov.b32 	%r105, %f258;
	shfl.sync.bfly.b32 	%r106|%p36, %r105, %r42, %r37, %r39;
	mov.b32 	%f259, %r106;
	add.f32 	%f260, %f258, %f259;
	mov.b32 	%r107, %f260;
	shfl.sync.bfly.b32 	%r108|%p37, %r107, %r45, %r37, %r39;
	mov.b32 	%f261, %r108;
	add.f32 	%f262, %f260, %f261;
	mov.b32 	%r109, %f262;
	shfl.sync.bfly.b32 	%r110|%p38, %r109, %r48, %r37, %r39;
	mov.b32 	%f263, %r110;
	add.f32 	%f264, %f262, %f263;
	mov.b32 	%r111, %f264;
	shfl.sync.bfly.b32 	%r112|%p39, %r111, %r51, %r37, %r39;
	mov.b32 	%f265, %r112;
	add.f32 	%f266, %f264, %f265;
	st.local.f32 	[%rd2+24], %f266;
	mov.b32 	%r113, %f294;
	shfl.sync.bfly.b32 	%r114|%p40, %r113, %r38, %r37, %r39;
	mov.b32 	%f267, %r114;
	add.f32 	%f268, %f294, %f267;
	mov.b32 	%r115, %f268;
	shfl.sync.bfly.b32 	%r116|%p41, %r115, %r42, %r37, %r39;
	mov.b32 	%f269, %r116;
	add.f32 	%f270, %f268, %f269;
	mov.b32 	%r117, %f270;
	shfl.sync.bfly.b32 	%r118|%p42, %r117, %r45, %r37, %r39;
	mov.b32 	%f271, %r118;
	add.f32 	%f272, %f270, %f271;
	mov.b32 	%r119, %f272;
	shfl.sync.bfly.b32 	%r120|%p43, %r119, %r48, %r37, %r39;
	mov.b32 	%f273, %r120;
	add.f32 	%f274, %f272, %f273;
	mov.b32 	%r121, %f274;
	shfl.sync.bfly.b32 	%r122|%p44, %r121, %r51, %r37, %r39;
	mov.b32 	%f275, %r122;
	add.f32 	%f276, %f274, %f275;
	st.local.f32 	[%rd2+28], %f276;
	setp.gt.s32 	%p45, %r3, 7;
	@%p45 bra 	$L__BB7_9;

	mul.wide.s32 	%rd64, %r3, 4;
	add.s64 	%rd65, %rd2, %rd64;
	ld.local.f32 	%f277, [%rd65];
	mad.lo.s32 	%r123, %r3, %r11, %r2;
	cvt.s64.s32 	%rd66, %r123;
	mul.lo.s32 	%r124, %r1, %r12;
	cvt.s64.s32 	%rd67, %r124;
	add.s64 	%rd68, %rd67, %rd66;
	cvta.to.global.u64 	%rd69, %rd20;
	shl.b64 	%rd70, %rd68, 2;
	add.s64 	%rd71, %rd69, %rd70;
	st.global.f32 	[%rd71], %f277;

$L__BB7_9:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_1_bs_64
.visible .entry ggml_matvec_f32_ncols_1_bs_64(
	.param .u64 ggml_matvec_f32_ncols_1_bs_64_param_0,
	.param .u64 ggml_matvec_f32_ncols_1_bs_64_param_1,
	.param .u64 ggml_matvec_f32_ncols_1_bs_64_param_2,
	.param .u32 ggml_matvec_f32_ncols_1_bs_64_param_3,
	.param .u32 ggml_matvec_f32_ncols_1_bs_64_param_4,
	.param .u32 ggml_matvec_f32_ncols_1_bs_64_param_5,
	.param .u32 ggml_matvec_f32_ncols_1_bs_64_param_6,
	.param .u32 ggml_matvec_f32_ncols_1_bs_64_param_7,
	.param .u32 ggml_matvec_f32_ncols_1_bs_64_param_8,
	.param .u32 ggml_matvec_f32_ncols_1_bs_64_param_9,
	.param .u32 ggml_matvec_f32_ncols_1_bs_64_param_10,
	.param .u32 ggml_matvec_f32_ncols_1_bs_64_param_11
)
{
	.reg .pred 	%p<19>;
	.reg .f32 	%f<88>;
	.reg .b32 	%r<79>;
	.reg .b64 	%rd<42>;


	ld.param.u64 	%rd18, [ggml_matvec_f32_ncols_1_bs_64_param_0];
	ld.param.u64 	%rd19, [ggml_matvec_f32_ncols_1_bs_64_param_1];
	ld.param.u64 	%rd17, [ggml_matvec_f32_ncols_1_bs_64_param_2];
	ld.param.u32 	%r13, [ggml_matvec_f32_ncols_1_bs_64_param_3];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_1_bs_64_param_5];
	ld.param.u32 	%r14, [ggml_matvec_f32_ncols_1_bs_64_param_7];
	ld.param.u32 	%r17, [ggml_matvec_f32_ncols_1_bs_64_param_8];
	ld.param.u32 	%r18, [ggml_matvec_f32_ncols_1_bs_64_param_9];
	ld.param.u32 	%r19, [ggml_matvec_f32_ncols_1_bs_64_param_10];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_1_bs_64_param_11];
	cvta.to.global.u64 	%rd1, %rd19;
	cvta.to.global.u64 	%rd2, %rd18;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r20, %r1, %r17;
	mov.u32 	%r21, %ctaid.x;
	mul.lo.s32 	%r22, %r21, %r16;
	mad.lo.s32 	%r23, %r20, %r18, %r22;
	cvt.s64.s32 	%rd3, %r23;
	mul.lo.s32 	%r24, %r1, %r19;
	cvt.s64.s32 	%rd4, %r24;
	mov.u32 	%r2, %tid.x;
	setp.gt.s32 	%p1, %r2, 31;
	shl.b32 	%r25, %r2, 2;
	mov.u32 	%r26, data_mmv;
	add.s32 	%r3, %r26, %r25;
	@%p1 bra 	$L__BB8_2;

	mov.u32 	%r27, 0;
	st.shared.u32 	[%r3], %r27;

$L__BB8_2:
	bar.sync 	0;
	setp.ge.s32 	%p2, %r2, %r13;
	mov.f32 	%f86, 0f00000000;
	@%p2 bra 	$L__BB8_9;

	not.b32 	%r28, %r2;
	add.s32 	%r4, %r28, %r13;
	shr.u32 	%r29, %r4, 6;
	add.s32 	%r30, %r29, 1;
	and.b32  	%r76, %r30, 3;
	setp.eq.s32 	%p3, %r76, 0;
	mov.f32 	%f86, 0f00000000;
	mov.u32 	%r77, %r2;
	@%p3 bra 	$L__BB8_6;

	mul.wide.s32 	%rd20, %r2, 2;
	add.s64 	%rd21, %rd20, %rd4;
	shl.b64 	%rd22, %rd21, 2;
	add.s64 	%rd39, %rd1, %rd22;
	add.s64 	%rd23, %rd20, %rd3;
	shl.b64 	%rd24, %rd23, 2;
	add.s64 	%rd38, %rd2, %rd24;
	mov.f32 	%f86, 0f00000000;
	mov.u32 	%r77, %r2;

$L__BB8_5:
	.pragma "nounroll";
	ld.global.nc.v2.f32 	{%f15, %f16}, [%rd38];
	ld.global.nc.v2.f32 	{%f19, %f20}, [%rd39];
	fma.rn.f32 	%f23, %f15, %f19, %f86;
	fma.rn.f32 	%f86, %f16, %f20, %f23;
	add.s32 	%r77, %r77, 64;
	add.s64 	%rd39, %rd39, 512;
	add.s64 	%rd38, %rd38, 512;
	add.s32 	%r76, %r76, -1;
	setp.ne.s32 	%p4, %r76, 0;
	@%p4 bra 	$L__BB8_5;

$L__BB8_6:
	setp.lt.u32 	%p5, %r4, 192;
	@%p5 bra 	$L__BB8_9;

	mul.wide.s32 	%rd25, %r77, 2;
	add.s64 	%rd26, %rd25, %rd3;
	shl.b64 	%rd27, %rd26, 2;
	add.s64 	%rd28, %rd2, %rd27;
	add.s64 	%rd41, %rd28, 1024;
	add.s64 	%rd29, %rd25, %rd4;
	shl.b64 	%rd30, %rd29, 2;
	add.s64 	%rd31, %rd1, %rd30;
	add.s64 	%rd40, %rd31, 1024;

$L__BB8_8:
	ld.global.nc.v2.f32 	{%f24, %f25}, [%rd41+-1024];
	ld.global.nc.v2.f32 	{%f28, %f29}, [%rd40+-1024];
	fma.rn.f32 	%f32, %f24, %f28, %f86;
	fma.rn.f32 	%f33, %f25, %f29, %f32;
	ld.global.nc.v2.f32 	{%f34, %f35}, [%rd41+-512];
	ld.global.nc.v2.f32 	{%f38, %f39}, [%rd40+-512];
	fma.rn.f32 	%f42, %f34, %f38, %f33;
	fma.rn.f32 	%f43, %f35, %f39, %f42;
	ld.global.nc.v2.f32 	{%f44, %f45}, [%rd41];
	ld.global.nc.v2.f32 	{%f48, %f49}, [%rd40];
	fma.rn.f32 	%f52, %f44, %f48, %f43;
	fma.rn.f32 	%f53, %f45, %f49, %f52;
	ld.global.nc.v2.f32 	{%f54, %f55}, [%rd41+512];
	ld.global.nc.v2.f32 	{%f58, %f59}, [%rd40+512];
	fma.rn.f32 	%f62, %f54, %f58, %f53;
	fma.rn.f32 	%f86, %f55, %f59, %f62;
	add.s64 	%rd41, %rd41, 2048;
	add.s64 	%rd40, %rd40, 2048;
	add.s32 	%r77, %r77, 256;
	setp.lt.s32 	%p6, %r77, %r13;
	@%p6 bra 	$L__BB8_8;

$L__BB8_9:
	mov.b32 	%r31, %f86;
	mov.u32 	%r32, 31;
	mov.u32 	%r33, 16;
	mov.u32 	%r34, -1;
	shfl.sync.bfly.b32 	%r35|%p7, %r31, %r33, %r32, %r34;
	mov.b32 	%f63, %r35;
	add.f32 	%f64, %f86, %f63;
	mov.b32 	%r36, %f64;
	mov.u32 	%r37, 8;
	shfl.sync.bfly.b32 	%r38|%p8, %r36, %r37, %r32, %r34;
	mov.b32 	%f65, %r38;
	add.f32 	%f66, %f64, %f65;
	mov.b32 	%r39, %f66;
	mov.u32 	%r40, 4;
	shfl.sync.bfly.b32 	%r41|%p9, %r39, %r40, %r32, %r34;
	mov.b32 	%f67, %r41;
	add.f32 	%f68, %f66, %f67;
	mov.b32 	%r42, %f68;
	mov.u32 	%r43, 2;
	shfl.sync.bfly.b32 	%r44|%p10, %r42, %r43, %r32, %r34;
	mov.b32 	%f69, %r44;
	add.f32 	%f70, %f68, %f69;
	mov.b32 	%r45, %f70;
	mov.u32 	%r46, 1;
	shfl.sync.bfly.b32 	%r47|%p11, %r45, %r46, %r32, %r34;
	mov.b32 	%f71, %r47;
	add.f32 	%f87, %f70, %f71;
	shr.s32 	%r48, %r2, 31;
	shr.u32 	%r49, %r48, 27;
	add.s32 	%r50, %r2, %r49;
	shr.s32 	%r51, %r50, 5;
	shl.b32 	%r52, %r51, 2;
	add.s32 	%r54, %r26, %r52;
	st.shared.f32 	[%r54], %f87;
	bar.sync 	0;
	@%p1 bra 	$L__BB8_11;

	ld.shared.f32 	%f72, [%r3];
	mov.b32 	%r55, %f72;
	shfl.sync.bfly.b32 	%r59|%p13, %r55, %r33, %r32, %r34;
	mov.b32 	%f73, %r59;
	add.f32 	%f74, %f72, %f73;
	mov.b32 	%r60, %f74;
	shfl.sync.bfly.b32 	%r62|%p14, %r60, %r37, %r32, %r34;
	mov.b32 	%f75, %r62;
	add.f32 	%f76, %f74, %f75;
	mov.b32 	%r63, %f76;
	shfl.sync.bfly.b32 	%r65|%p15, %r63, %r40, %r32, %r34;
	mov.b32 	%f77, %r65;
	add.f32 	%f78, %f76, %f77;
	mov.b32 	%r66, %f78;
	shfl.sync.bfly.b32 	%r68|%p16, %r66, %r43, %r32, %r34;
	mov.b32 	%f79, %r68;
	add.f32 	%f80, %f78, %f79;
	mov.b32 	%r69, %f80;
	shfl.sync.bfly.b32 	%r71|%p17, %r69, %r46, %r32, %r34;
	mov.b32 	%f81, %r71;
	add.f32 	%f87, %f80, %f81;

$L__BB8_11:
	bar.sync 	0;
	setp.gt.s32 	%p18, %r2, 0;
	@%p18 bra 	$L__BB8_13;

	mad.lo.s32 	%r73, %r2, %r14, %r21;
	cvt.s64.s32 	%rd32, %r73;
	mul.lo.s32 	%r74, %r1, %r15;
	cvt.s64.s32 	%rd33, %r74;
	add.s64 	%rd34, %rd33, %rd32;
	cvta.to.global.u64 	%rd35, %rd17;
	shl.b64 	%rd36, %rd34, 2;
	add.s64 	%rd37, %rd35, %rd36;
	st.global.f32 	[%rd37], %f87;

$L__BB8_13:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_2_bs_64
.visible .entry ggml_matvec_f32_ncols_2_bs_64(
	.param .u64 ggml_matvec_f32_ncols_2_bs_64_param_0,
	.param .u64 ggml_matvec_f32_ncols_2_bs_64_param_1,
	.param .u64 ggml_matvec_f32_ncols_2_bs_64_param_2,
	.param .u32 ggml_matvec_f32_ncols_2_bs_64_param_3,
	.param .u32 ggml_matvec_f32_ncols_2_bs_64_param_4,
	.param .u32 ggml_matvec_f32_ncols_2_bs_64_param_5,
	.param .u32 ggml_matvec_f32_ncols_2_bs_64_param_6,
	.param .u32 ggml_matvec_f32_ncols_2_bs_64_param_7,
	.param .u32 ggml_matvec_f32_ncols_2_bs_64_param_8,
	.param .u32 ggml_matvec_f32_ncols_2_bs_64_param_9,
	.param .u32 ggml_matvec_f32_ncols_2_bs_64_param_10,
	.param .u32 ggml_matvec_f32_ncols_2_bs_64_param_11
)
{
	.local .align 8 .b8 	__local_depot9[8];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<30>;
	.reg .f32 	%f<146>;
	.reg .b32 	%r<113>;
	.reg .b64 	%rd<64>;


	mov.u64 	%SPL, __local_depot9;
	ld.param.u64 	%rd27, [ggml_matvec_f32_ncols_2_bs_64_param_0];
	ld.param.u64 	%rd28, [ggml_matvec_f32_ncols_2_bs_64_param_1];
	ld.param.u64 	%rd26, [ggml_matvec_f32_ncols_2_bs_64_param_2];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_2_bs_64_param_3];
	ld.param.u32 	%r19, [ggml_matvec_f32_ncols_2_bs_64_param_5];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_2_bs_64_param_6];
	ld.param.u32 	%r17, [ggml_matvec_f32_ncols_2_bs_64_param_7];
	ld.param.u32 	%r20, [ggml_matvec_f32_ncols_2_bs_64_param_8];
	ld.param.u32 	%r21, [ggml_matvec_f32_ncols_2_bs_64_param_9];
	ld.param.u32 	%r22, [ggml_matvec_f32_ncols_2_bs_64_param_10];
	ld.param.u32 	%r18, [ggml_matvec_f32_ncols_2_bs_64_param_11];
	cvta.to.global.u64 	%rd1, %rd28;
	cvta.to.global.u64 	%rd2, %rd27;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r23, %r1, %r20;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r24, %r2, %r19;
	mad.lo.s32 	%r25, %r23, %r21, %r24;
	cvt.s64.s32 	%rd4, %r25;
	mul.lo.s32 	%r26, %r1, %r22;
	cvt.s64.s32 	%rd5, %r26;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r27, %r3, 2;
	mov.u32 	%r28, data_mmv;
	add.s32 	%r4, %r28, %r27;
	@%p1 bra 	$L__BB9_2;

	mov.u32 	%r29, 0;
	st.shared.u32 	[%r4], %r29;

$L__BB9_2:
	bar.sync 	0;
	mov.f32 	%f144, 0f00000000;
	st.local.v2.f32 	[%rd3], {%f144, %f144};
	setp.ge.s32 	%p2, %r3, %r15;
	mov.f32 	%f145, %f144;
	@%p2 bra 	$L__BB9_11;

	not.b32 	%r30, %r3;
	add.s32 	%r5, %r30, %r15;
	shr.u32 	%r31, %r5, 6;
	add.s32 	%r32, %r31, 1;
	and.b32  	%r110, %r32, 3;
	setp.eq.s32 	%p3, %r110, 0;
	mov.f32 	%f144, 0f00000000;
	mov.u32 	%r111, %r3;
	@%p3 bra 	$L__BB9_7;

	mul.wide.s32 	%rd30, %r16, 2;
	mul.wide.s32 	%rd31, %r3, 2;
	add.s64 	%rd32, %rd30, %rd31;
	add.s64 	%rd33, %rd32, %rd5;
	shl.b64 	%rd34, %rd33, 2;
	add.s64 	%rd60, %rd1, %rd34;
	add.s64 	%rd35, %rd31, %rd5;
	shl.b64 	%rd36, %rd35, 2;
	add.s64 	%rd59, %rd1, %rd36;
	add.s64 	%rd37, %rd31, %rd4;
	shl.b64 	%rd38, %rd37, 2;
	add.s64 	%rd58, %rd2, %rd38;
	mov.f32 	%f144, 0f00000000;
	mov.f32 	%f145, %f144;
	mov.u32 	%r111, %r3;

$L__BB9_5:
	.pragma "nounroll";
	ld.global.nc.v2.f32 	{%f19, %f20}, [%rd58];
	ld.global.nc.v2.f32 	{%f23, %f24}, [%rd59];
	fma.rn.f32 	%f27, %f19, %f23, %f145;
	fma.rn.f32 	%f145, %f20, %f24, %f27;
	ld.global.nc.v2.f32 	{%f28, %f29}, [%rd60];
	fma.rn.f32 	%f32, %f19, %f28, %f144;
	fma.rn.f32 	%f144, %f20, %f29, %f32;
	add.s32 	%r111, %r111, 64;
	add.s64 	%rd60, %rd60, 512;
	add.s64 	%rd59, %rd59, 512;
	add.s64 	%rd58, %rd58, 512;
	add.s32 	%r110, %r110, -1;
	setp.ne.s32 	%p4, %r110, 0;
	@%p4 bra 	$L__BB9_5;

	st.local.v2.f32 	[%rd3], {%f145, %f144};

$L__BB9_7:
	setp.lt.u32 	%p5, %r5, 192;
	@%p5 bra 	$L__BB9_11;

	mul.wide.s32 	%rd39, %r111, 2;
	add.s64 	%rd40, %rd39, %rd4;
	shl.b64 	%rd41, %rd40, 2;
	add.s64 	%rd42, %rd2, %rd41;
	add.s64 	%rd63, %rd42, 1024;
	add.s64 	%rd43, %rd39, %rd5;
	shl.b64 	%rd44, %rd43, 2;
	add.s64 	%rd45, %rd1, %rd44;
	add.s64 	%rd62, %rd45, 1536;
	mul.wide.s32 	%rd46, %r16, 2;
	add.s64 	%rd47, %rd43, %rd46;
	shl.b64 	%rd48, %rd47, 2;
	add.s64 	%rd49, %rd1, %rd48;
	add.s64 	%rd61, %rd49, 1024;

$L__BB9_9:
	ld.global.nc.v2.f32 	{%f33, %f34}, [%rd63+-1024];
	ld.global.nc.v2.f32 	{%f37, %f38}, [%rd62+-1536];
	fma.rn.f32 	%f41, %f33, %f37, %f145;
	fma.rn.f32 	%f42, %f34, %f38, %f41;
	ld.global.nc.v2.f32 	{%f43, %f44}, [%rd61+-1024];
	fma.rn.f32 	%f47, %f33, %f43, %f144;
	fma.rn.f32 	%f48, %f34, %f44, %f47;
	ld.global.nc.v2.f32 	{%f49, %f50}, [%rd63+-512];
	ld.global.nc.v2.f32 	{%f53, %f54}, [%rd62+-1024];
	fma.rn.f32 	%f57, %f49, %f53, %f42;
	fma.rn.f32 	%f58, %f50, %f54, %f57;
	ld.global.nc.v2.f32 	{%f59, %f60}, [%rd61+-512];
	fma.rn.f32 	%f63, %f49, %f59, %f48;
	fma.rn.f32 	%f64, %f50, %f60, %f63;
	ld.global.nc.v2.f32 	{%f65, %f66}, [%rd63];
	ld.global.nc.v2.f32 	{%f69, %f70}, [%rd62+-512];
	fma.rn.f32 	%f73, %f65, %f69, %f58;
	fma.rn.f32 	%f74, %f66, %f70, %f73;
	ld.global.nc.v2.f32 	{%f75, %f76}, [%rd61];
	fma.rn.f32 	%f79, %f65, %f75, %f64;
	fma.rn.f32 	%f80, %f66, %f76, %f79;
	ld.global.nc.v2.f32 	{%f81, %f82}, [%rd63+512];
	ld.global.nc.v2.f32 	{%f85, %f86}, [%rd62];
	fma.rn.f32 	%f89, %f81, %f85, %f74;
	fma.rn.f32 	%f145, %f82, %f86, %f89;
	ld.global.nc.v2.f32 	{%f90, %f91}, [%rd61+512];
	fma.rn.f32 	%f94, %f81, %f90, %f80;
	fma.rn.f32 	%f144, %f82, %f91, %f94;
	add.s64 	%rd63, %rd63, 2048;
	add.s64 	%rd62, %rd62, 2048;
	add.s64 	%rd61, %rd61, 2048;
	add.s32 	%r111, %r111, 256;
	setp.lt.s32 	%p6, %r111, %r15;
	@%p6 bra 	$L__BB9_9;

	st.local.v2.f32 	[%rd3], {%f145, %f144};

$L__BB9_11:
	shr.s32 	%r33, %r3, 31;
	shr.u32 	%r34, %r33, 27;
	add.s32 	%r35, %r3, %r34;
	shr.s32 	%r36, %r35, 5;
	shl.b32 	%r37, %r36, 2;
	add.s32 	%r14, %r28, %r37;
	mov.u32 	%r39, 2;
	mov.b32 	%r40, %f145;
	mov.u32 	%r41, 31;
	mov.u32 	%r42, 16;
	mov.u32 	%r43, -1;
	shfl.sync.bfly.b32 	%r44|%p7, %r40, %r42, %r41, %r43;
	mov.b32 	%f95, %r44;
	add.f32 	%f96, %f145, %f95;
	mov.b32 	%r45, %f96;
	mov.u32 	%r46, 8;
	shfl.sync.bfly.b32 	%r47|%p8, %r45, %r46, %r41, %r43;
	mov.b32 	%f97, %r47;
	add.f32 	%f98, %f96, %f97;
	mov.b32 	%r48, %f98;
	mov.u32 	%r49, 4;
	shfl.sync.bfly.b32 	%r50|%p9, %r48, %r49, %r41, %r43;
	mov.b32 	%f99, %r50;
	add.f32 	%f100, %f98, %f99;
	mov.b32 	%r51, %f100;
	shfl.sync.bfly.b32 	%r52|%p10, %r51, %r39, %r41, %r43;
	mov.b32 	%f101, %r52;
	add.f32 	%f102, %f100, %f101;
	mov.b32 	%r53, %f102;
	mov.u32 	%r54, 1;
	shfl.sync.bfly.b32 	%r55|%p11, %r53, %r54, %r41, %r43;
	mov.b32 	%f103, %r55;
	add.f32 	%f104, %f102, %f103;
	st.local.f32 	[%rd3], %f104;
	st.shared.f32 	[%r14], %f104;
	bar.sync 	0;
	@%p1 bra 	$L__BB9_13;

	ld.shared.f32 	%f105, [%r4];
	mov.b32 	%r56, %f105;
	shfl.sync.bfly.b32 	%r60|%p13, %r56, %r42, %r41, %r43;
	mov.b32 	%f106, %r60;
	add.f32 	%f107, %f105, %f106;
	mov.b32 	%r61, %f107;
	shfl.sync.bfly.b32 	%r63|%p14, %r61, %r46, %r41, %r43;
	mov.b32 	%f108, %r63;
	add.f32 	%f109, %f107, %f108;
	mov.b32 	%r64, %f109;
	shfl.sync.bfly.b32 	%r66|%p15, %r64, %r49, %r41, %r43;
	mov.b32 	%f110, %r66;
	add.f32 	%f111, %f109, %f110;
	mov.b32 	%r67, %f111;
	shfl.sync.bfly.b32 	%r69|%p16, %r67, %r39, %r41, %r43;
	mov.b32 	%f112, %r69;
	add.f32 	%f113, %f111, %f112;
	mov.b32 	%r70, %f113;
	shfl.sync.bfly.b32 	%r72|%p17, %r70, %r54, %r41, %r43;
	mov.b32 	%f114, %r72;
	add.f32 	%f115, %f113, %f114;
	st.local.f32 	[%rd3], %f115;

$L__BB9_13:
	bar.sync 	0;
	mov.b32 	%r73, %f144;
	shfl.sync.bfly.b32 	%r77|%p19, %r73, %r42, %r41, %r43;
	mov.b32 	%f116, %r77;
	add.f32 	%f117, %f144, %f116;
	mov.b32 	%r78, %f117;
	shfl.sync.bfly.b32 	%r80|%p20, %r78, %r46, %r41, %r43;
	mov.b32 	%f118, %r80;
	add.f32 	%f119, %f117, %f118;
	mov.b32 	%r81, %f119;
	shfl.sync.bfly.b32 	%r83|%p21, %r81, %r49, %r41, %r43;
	mov.b32 	%f120, %r83;
	add.f32 	%f121, %f119, %f120;
	mov.b32 	%r84, %f121;
	shfl.sync.bfly.b32 	%r86|%p22, %r84, %r39, %r41, %r43;
	mov.b32 	%f122, %r86;
	add.f32 	%f123, %f121, %f122;
	mov.b32 	%r87, %f123;
	shfl.sync.bfly.b32 	%r89|%p23, %r87, %r54, %r41, %r43;
	mov.b32 	%f124, %r89;
	add.f32 	%f125, %f123, %f124;
	st.local.f32 	[%rd3+4], %f125;
	st.shared.f32 	[%r14], %f125;
	bar.sync 	0;
	@%p1 bra 	$L__BB9_15;

	ld.shared.f32 	%f126, [%r4];
	mov.b32 	%r90, %f126;
	mov.u32 	%r91, 31;
	mov.u32 	%r92, 16;
	mov.u32 	%r93, -1;
	shfl.sync.bfly.b32 	%r94|%p24, %r90, %r92, %r91, %r93;
	mov.b32 	%f127, %r94;
	add.f32 	%f128, %f126, %f127;
	mov.b32 	%r95, %f128;
	mov.u32 	%r96, 8;
	shfl.sync.bfly.b32 	%r97|%p25, %r95, %r96, %r91, %r93;
	mov.b32 	%f129, %r97;
	add.f32 	%f130, %f128, %f129;
	mov.b32 	%r98, %f130;
	mov.u32 	%r99, 4;
	shfl.sync.bfly.b32 	%r100|%p26, %r98, %r99, %r91, %r93;
	mov.b32 	%f131, %r100;
	add.f32 	%f132, %f130, %f131;
	mov.b32 	%r101, %f132;
	mov.u32 	%r102, 2;
	shfl.sync.bfly.b32 	%r103|%p27, %r101, %r102, %r91, %r93;
	mov.b32 	%f133, %r103;
	add.f32 	%f134, %f132, %f133;
	mov.b32 	%r104, %f134;
	mov.u32 	%r105, 1;
	shfl.sync.bfly.b32 	%r106|%p28, %r104, %r105, %r91, %r93;
	mov.b32 	%f135, %r106;
	add.f32 	%f136, %f134, %f135;
	st.local.f32 	[%rd3+4], %f136;

$L__BB9_15:
	bar.sync 	0;
	setp.gt.s32 	%p29, %r3, 1;
	@%p29 bra 	$L__BB9_17;

	mul.wide.s32 	%rd50, %r3, 4;
	add.s64 	%rd51, %rd3, %rd50;
	ld.local.f32 	%f137, [%rd51];
	mad.lo.s32 	%r107, %r3, %r17, %r2;
	cvt.s64.s32 	%rd52, %r107;
	mul.lo.s32 	%r108, %r1, %r18;
	cvt.s64.s32 	%rd53, %r108;
	add.s64 	%rd54, %rd53, %rd52;
	cvta.to.global.u64 	%rd55, %rd26;
	shl.b64 	%rd56, %rd54, 2;
	add.s64 	%rd57, %rd55, %rd56;
	st.global.f32 	[%rd57], %f137;

$L__BB9_17:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_3_bs_64
.visible .entry ggml_matvec_f32_ncols_3_bs_64(
	.param .u64 ggml_matvec_f32_ncols_3_bs_64_param_0,
	.param .u64 ggml_matvec_f32_ncols_3_bs_64_param_1,
	.param .u64 ggml_matvec_f32_ncols_3_bs_64_param_2,
	.param .u32 ggml_matvec_f32_ncols_3_bs_64_param_3,
	.param .u32 ggml_matvec_f32_ncols_3_bs_64_param_4,
	.param .u32 ggml_matvec_f32_ncols_3_bs_64_param_5,
	.param .u32 ggml_matvec_f32_ncols_3_bs_64_param_6,
	.param .u32 ggml_matvec_f32_ncols_3_bs_64_param_7,
	.param .u32 ggml_matvec_f32_ncols_3_bs_64_param_8,
	.param .u32 ggml_matvec_f32_ncols_3_bs_64_param_9,
	.param .u32 ggml_matvec_f32_ncols_3_bs_64_param_10,
	.param .u32 ggml_matvec_f32_ncols_3_bs_64_param_11
)
{
	.local .align 4 .b8 	__local_depot10[12];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<41>;
	.reg .f32 	%f<208>;
	.reg .b32 	%r<154>;
	.reg .b64 	%rd<72>;


	mov.u64 	%SPL, __local_depot10;
	ld.param.u64 	%rd29, [ggml_matvec_f32_ncols_3_bs_64_param_0];
	ld.param.u64 	%rd30, [ggml_matvec_f32_ncols_3_bs_64_param_1];
	ld.param.u64 	%rd28, [ggml_matvec_f32_ncols_3_bs_64_param_2];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_3_bs_64_param_3];
	ld.param.u32 	%r19, [ggml_matvec_f32_ncols_3_bs_64_param_5];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_3_bs_64_param_6];
	ld.param.u32 	%r17, [ggml_matvec_f32_ncols_3_bs_64_param_7];
	ld.param.u32 	%r20, [ggml_matvec_f32_ncols_3_bs_64_param_8];
	ld.param.u32 	%r21, [ggml_matvec_f32_ncols_3_bs_64_param_9];
	ld.param.u32 	%r22, [ggml_matvec_f32_ncols_3_bs_64_param_10];
	ld.param.u32 	%r18, [ggml_matvec_f32_ncols_3_bs_64_param_11];
	cvta.to.global.u64 	%rd71, %rd30;
	cvta.to.global.u64 	%rd2, %rd29;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r23, %r1, %r20;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r24, %r2, %r19;
	mad.lo.s32 	%r25, %r23, %r21, %r24;
	cvt.s64.s32 	%rd4, %r25;
	mul.lo.s32 	%r26, %r1, %r22;
	cvt.s64.s32 	%rd5, %r26;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r27, %r3, 2;
	mov.u32 	%r28, data_mmv;
	add.s32 	%r4, %r28, %r27;
	@%p1 bra 	$L__BB10_2;

	mov.u32 	%r29, 0;
	st.shared.u32 	[%r4], %r29;

$L__BB10_2:
	bar.sync 	0;
	mov.f32 	%f205, 0f00000000;
	mov.u32 	%r30, 0;
	st.local.u32 	[%rd3], %r30;
	st.local.u32 	[%rd3+4], %r30;
	st.local.u32 	[%rd3+8], %r30;
	setp.ge.s32 	%p2, %r3, %r15;
	mov.f32 	%f206, %f205;
	mov.f32 	%f207, %f205;
	@%p2 bra 	$L__BB10_11;

	not.b32 	%r31, %r3;
	add.s32 	%r5, %r31, %r15;
	shr.u32 	%r32, %r5, 6;
	add.s32 	%r33, %r32, 1;
	and.b32  	%r151, %r33, 3;
	setp.eq.s32 	%p3, %r151, 0;
	mov.f32 	%f205, 0f00000000;
	mov.u32 	%r152, %r3;
	@%p3 bra 	$L__BB10_7;

	shl.b32 	%r34, %r16, 1;
	add.s32 	%r35, %r3, %r34;
	mul.wide.s32 	%rd32, %r35, 2;
	add.s64 	%rd33, %rd32, %rd5;
	shl.b64 	%rd34, %rd33, 2;
	add.s64 	%rd69, %rd71, %rd34;
	mul.wide.s32 	%rd35, %r16, 2;
	mul.wide.s32 	%rd36, %r3, 2;
	add.s64 	%rd37, %rd35, %rd36;
	add.s64 	%rd38, %rd37, %rd5;
	shl.b64 	%rd39, %rd38, 2;
	add.s64 	%rd68, %rd71, %rd39;
	add.s64 	%rd40, %rd36, %rd5;
	shl.b64 	%rd41, %rd40, 2;
	add.s64 	%rd67, %rd71, %rd41;
	add.s64 	%rd42, %rd36, %rd4;
	shl.b64 	%rd43, %rd42, 2;
	add.s64 	%rd66, %rd2, %rd43;
	mov.f32 	%f205, 0f00000000;
	mov.f32 	%f206, %f205;
	mov.f32 	%f207, %f205;
	mov.u32 	%r152, %r3;

$L__BB10_5:
	.pragma "nounroll";
	ld.global.nc.v2.f32 	{%f28, %f29}, [%rd66];
	ld.global.nc.v2.f32 	{%f32, %f33}, [%rd67];
	fma.rn.f32 	%f36, %f28, %f32, %f207;
	fma.rn.f32 	%f207, %f29, %f33, %f36;
	ld.global.nc.v2.f32 	{%f37, %f38}, [%rd68];
	fma.rn.f32 	%f41, %f28, %f37, %f206;
	fma.rn.f32 	%f206, %f29, %f38, %f41;
	ld.global.nc.v2.f32 	{%f42, %f43}, [%rd69];
	fma.rn.f32 	%f46, %f28, %f42, %f205;
	fma.rn.f32 	%f205, %f29, %f43, %f46;
	add.s32 	%r152, %r152, 64;
	add.s64 	%rd69, %rd69, 512;
	add.s64 	%rd68, %rd68, 512;
	add.s64 	%rd67, %rd67, 512;
	add.s64 	%rd66, %rd66, 512;
	add.s32 	%r151, %r151, -1;
	setp.ne.s32 	%p4, %r151, 0;
	@%p4 bra 	$L__BB10_5;

	st.local.f32 	[%rd3], %f207;
	st.local.f32 	[%rd3+4], %f206;
	st.local.f32 	[%rd3+8], %f205;

$L__BB10_7:
	setp.lt.u32 	%p5, %r5, 192;
	@%p5 bra 	$L__BB10_11;

	add.s32 	%r36, %r152, %r16;
	shl.b32 	%r37, %r16, 1;
	add.s32 	%r38, %r152, %r37;
	add.s32 	%r39, %r36, 64;
	mul.wide.s32 	%rd44, %r39, 8;
	shl.b64 	%rd45, %rd5, 2;
	add.s64 	%rd19, %rd44, %rd45;
	mul.wide.s32 	%rd46, %r38, 8;
	add.s64 	%rd20, %rd46, %rd45;
	mul.wide.s32 	%rd47, %r152, 2;
	add.s64 	%rd48, %rd47, %rd4;
	shl.b64 	%rd49, %rd48, 2;
	add.s64 	%rd50, %rd2, %rd49;
	add.s64 	%rd70, %rd50, 1024;
	mul.wide.s32 	%rd51, %r152, 8;
	add.s64 	%rd22, %rd51, %rd45;
	mul.wide.s32 	%rd52, %r16, 8;
	add.s64 	%rd53, %rd51, %rd52;
	add.s64 	%rd23, %rd53, %rd45;

$L__BB10_9:
	ld.global.nc.v2.f32 	{%f47, %f48}, [%rd70+-1024];
	add.s64 	%rd54, %rd71, %rd22;
	ld.global.nc.v2.f32 	{%f51, %f52}, [%rd54];
	fma.rn.f32 	%f55, %f47, %f51, %f207;
	fma.rn.f32 	%f56, %f48, %f52, %f55;
	add.s64 	%rd55, %rd71, %rd23;
	ld.global.nc.v2.f32 	{%f57, %f58}, [%rd55];
	fma.rn.f32 	%f61, %f47, %f57, %f206;
	fma.rn.f32 	%f62, %f48, %f58, %f61;
	add.s64 	%rd56, %rd71, %rd20;
	ld.global.nc.v2.f32 	{%f63, %f64}, [%rd56];
	fma.rn.f32 	%f67, %f47, %f63, %f205;
	fma.rn.f32 	%f68, %f48, %f64, %f67;
	ld.global.nc.v2.f32 	{%f69, %f70}, [%rd70+-512];
	ld.global.nc.v2.f32 	{%f73, %f74}, [%rd54+512];
	fma.rn.f32 	%f77, %f69, %f73, %f56;
	fma.rn.f32 	%f78, %f70, %f74, %f77;
	add.s64 	%rd57, %rd71, %rd19;
	ld.global.nc.v2.f32 	{%f79, %f80}, [%rd57];
	fma.rn.f32 	%f83, %f69, %f79, %f62;
	fma.rn.f32 	%f84, %f70, %f80, %f83;
	ld.global.nc.v2.f32 	{%f85, %f86}, [%rd56+512];
	fma.rn.f32 	%f89, %f69, %f85, %f68;
	fma.rn.f32 	%f90, %f70, %f86, %f89;
	ld.global.nc.v2.f32 	{%f91, %f92}, [%rd70];
	ld.global.nc.v2.f32 	{%f95, %f96}, [%rd54+1024];
	fma.rn.f32 	%f99, %f91, %f95, %f78;
	fma.rn.f32 	%f100, %f92, %f96, %f99;
	ld.global.nc.v2.f32 	{%f101, %f102}, [%rd57+512];
	fma.rn.f32 	%f105, %f91, %f101, %f84;
	fma.rn.f32 	%f106, %f92, %f102, %f105;
	ld.global.nc.v2.f32 	{%f107, %f108}, [%rd56+1024];
	fma.rn.f32 	%f111, %f91, %f107, %f90;
	fma.rn.f32 	%f112, %f92, %f108, %f111;
	ld.global.nc.v2.f32 	{%f113, %f114}, [%rd70+512];
	ld.global.nc.v2.f32 	{%f117, %f118}, [%rd54+1536];
	fma.rn.f32 	%f121, %f113, %f117, %f100;
	fma.rn.f32 	%f207, %f114, %f118, %f121;
	ld.global.nc.v2.f32 	{%f122, %f123}, [%rd57+1024];
	fma.rn.f32 	%f126, %f113, %f122, %f106;
	fma.rn.f32 	%f206, %f114, %f123, %f126;
	ld.global.nc.v2.f32 	{%f127, %f128}, [%rd56+1536];
	fma.rn.f32 	%f131, %f113, %f127, %f112;
	fma.rn.f32 	%f205, %f114, %f128, %f131;
	add.s64 	%rd71, %rd71, 2048;
	add.s64 	%rd70, %rd70, 2048;
	add.s32 	%r152, %r152, 256;
	setp.lt.s32 	%p6, %r152, %r15;
	@%p6 bra 	$L__BB10_9;

	st.local.f32 	[%rd3], %f207;
	st.local.f32 	[%rd3+4], %f206;
	st.local.f32 	[%rd3+8], %f205;

$L__BB10_11:
	shr.s32 	%r40, %r3, 31;
	shr.u32 	%r41, %r40, 27;
	add.s32 	%r42, %r3, %r41;
	shr.s32 	%r43, %r42, 5;
	shl.b32 	%r44, %r43, 2;
	add.s32 	%r14, %r28, %r44;
	mov.u32 	%r46, 2;
	mov.b32 	%r47, %f207;
	mov.u32 	%r48, 31;
	mov.u32 	%r49, 16;
	mov.u32 	%r50, -1;
	shfl.sync.bfly.b32 	%r51|%p7, %r47, %r49, %r48, %r50;
	mov.b32 	%f132, %r51;
	add.f32 	%f133, %f207, %f132;
	mov.b32 	%r52, %f133;
	mov.u32 	%r53, 8;
	shfl.sync.bfly.b32 	%r54|%p8, %r52, %r53, %r48, %r50;
	mov.b32 	%f134, %r54;
	add.f32 	%f135, %f133, %f134;
	mov.b32 	%r55, %f135;
	mov.u32 	%r56, 4;
	shfl.sync.bfly.b32 	%r57|%p9, %r55, %r56, %r48, %r50;
	mov.b32 	%f136, %r57;
	add.f32 	%f137, %f135, %f136;
	mov.b32 	%r58, %f137;
	shfl.sync.bfly.b32 	%r59|%p10, %r58, %r46, %r48, %r50;
	mov.b32 	%f138, %r59;
	add.f32 	%f139, %f137, %f138;
	mov.b32 	%r60, %f139;
	mov.u32 	%r61, 1;
	shfl.sync.bfly.b32 	%r62|%p11, %r60, %r61, %r48, %r50;
	mov.b32 	%f140, %r62;
	add.f32 	%f141, %f139, %f140;
	st.local.f32 	[%rd3], %f141;
	st.shared.f32 	[%r14], %f141;
	bar.sync 	0;
	@%p1 bra 	$L__BB10_13;

	ld.shared.f32 	%f142, [%r4];
	mov.b32 	%r63, %f142;
	shfl.sync.bfly.b32 	%r67|%p13, %r63, %r49, %r48, %r50;
	mov.b32 	%f143, %r67;
	add.f32 	%f144, %f142, %f143;
	mov.b32 	%r68, %f144;
	shfl.sync.bfly.b32 	%r70|%p14, %r68, %r53, %r48, %r50;
	mov.b32 	%f145, %r70;
	add.f32 	%f146, %f144, %f145;
	mov.b32 	%r71, %f146;
	shfl.sync.bfly.b32 	%r73|%p15, %r71, %r56, %r48, %r50;
	mov.b32 	%f147, %r73;
	add.f32 	%f148, %f146, %f147;
	mov.b32 	%r74, %f148;
	shfl.sync.bfly.b32 	%r76|%p16, %r74, %r46, %r48, %r50;
	mov.b32 	%f149, %r76;
	add.f32 	%f150, %f148, %f149;
	mov.b32 	%r77, %f150;
	shfl.sync.bfly.b32 	%r79|%p17, %r77, %r61, %r48, %r50;
	mov.b32 	%f151, %r79;
	add.f32 	%f152, %f150, %f151;
	st.local.f32 	[%rd3], %f152;

$L__BB10_13:
	bar.sync 	0;
	mov.b32 	%r80, %f206;
	shfl.sync.bfly.b32 	%r84|%p19, %r80, %r49, %r48, %r50;
	mov.b32 	%f153, %r84;
	add.f32 	%f154, %f206, %f153;
	mov.b32 	%r85, %f154;
	shfl.sync.bfly.b32 	%r87|%p20, %r85, %r53, %r48, %r50;
	mov.b32 	%f155, %r87;
	add.f32 	%f156, %f154, %f155;
	mov.b32 	%r88, %f156;
	shfl.sync.bfly.b32 	%r90|%p21, %r88, %r56, %r48, %r50;
	mov.b32 	%f157, %r90;
	add.f32 	%f158, %f156, %f157;
	mov.b32 	%r91, %f158;
	shfl.sync.bfly.b32 	%r93|%p22, %r91, %r46, %r48, %r50;
	mov.b32 	%f159, %r93;
	add.f32 	%f160, %f158, %f159;
	mov.b32 	%r94, %f160;
	shfl.sync.bfly.b32 	%r96|%p23, %r94, %r61, %r48, %r50;
	mov.b32 	%f161, %r96;
	add.f32 	%f162, %f160, %f161;
	st.local.f32 	[%rd3+4], %f162;
	st.shared.f32 	[%r14], %f162;
	bar.sync 	0;
	@%p1 bra 	$L__BB10_15;

	ld.shared.f32 	%f163, [%r4];
	mov.b32 	%r97, %f163;
	mov.u32 	%r98, 31;
	mov.u32 	%r99, 16;
	mov.u32 	%r100, -1;
	shfl.sync.bfly.b32 	%r101|%p24, %r97, %r99, %r98, %r100;
	mov.b32 	%f164, %r101;
	add.f32 	%f165, %f163, %f164;
	mov.b32 	%r102, %f165;
	mov.u32 	%r103, 8;
	shfl.sync.bfly.b32 	%r104|%p25, %r102, %r103, %r98, %r100;
	mov.b32 	%f166, %r104;
	add.f32 	%f167, %f165, %f166;
	mov.b32 	%r105, %f167;
	mov.u32 	%r106, 4;
	shfl.sync.bfly.b32 	%r107|%p26, %r105, %r106, %r98, %r100;
	mov.b32 	%f168, %r107;
	add.f32 	%f169, %f167, %f168;
	mov.b32 	%r108, %f169;
	mov.u32 	%r109, 2;
	shfl.sync.bfly.b32 	%r110|%p27, %r108, %r109, %r98, %r100;
	mov.b32 	%f170, %r110;
	add.f32 	%f171, %f169, %f170;
	mov.b32 	%r111, %f171;
	mov.u32 	%r112, 1;
	shfl.sync.bfly.b32 	%r113|%p28, %r111, %r112, %r98, %r100;
	mov.b32 	%f172, %r113;
	add.f32 	%f173, %f171, %f172;
	st.local.f32 	[%rd3+4], %f173;

$L__BB10_15:
	bar.sync 	0;
	mov.b32 	%r114, %f205;
	mov.u32 	%r115, 31;
	mov.u32 	%r116, 16;
	mov.u32 	%r117, -1;
	shfl.sync.bfly.b32 	%r118|%p30, %r114, %r116, %r115, %r117;
	mov.b32 	%f174, %r118;
	add.f32 	%f175, %f205, %f174;
	mov.b32 	%r119, %f175;
	mov.u32 	%r120, 8;
	shfl.sync.bfly.b32 	%r121|%p31, %r119, %r120, %r115, %r117;
	mov.b32 	%f176, %r121;
	add.f32 	%f177, %f175, %f176;
	mov.b32 	%r122, %f177;
	mov.u32 	%r123, 4;
	shfl.sync.bfly.b32 	%r124|%p32, %r122, %r123, %r115, %r117;
	mov.b32 	%f178, %r124;
	add.f32 	%f179, %f177, %f178;
	mov.b32 	%r125, %f179;
	mov.u32 	%r126, 2;
	shfl.sync.bfly.b32 	%r127|%p33, %r125, %r126, %r115, %r117;
	mov.b32 	%f180, %r127;
	add.f32 	%f181, %f179, %f180;
	mov.b32 	%r128, %f181;
	mov.u32 	%r129, 1;
	shfl.sync.bfly.b32 	%r130|%p34, %r128, %r129, %r115, %r117;
	mov.b32 	%f182, %r130;
	add.f32 	%f183, %f181, %f182;
	st.local.f32 	[%rd3+8], %f183;
	st.shared.f32 	[%r14], %f183;
	bar.sync 	0;
	@%p1 bra 	$L__BB10_17;

	ld.shared.f32 	%f184, [%r4];
	mov.b32 	%r131, %f184;
	shfl.sync.bfly.b32 	%r135|%p35, %r131, %r116, %r115, %r117;
	mov.b32 	%f185, %r135;
	add.f32 	%f186, %f184, %f185;
	mov.b32 	%r136, %f186;
	shfl.sync.bfly.b32 	%r138|%p36, %r136, %r120, %r115, %r117;
	mov.b32 	%f187, %r138;
	add.f32 	%f188, %f186, %f187;
	mov.b32 	%r139, %f188;
	shfl.sync.bfly.b32 	%r141|%p37, %r139, %r123, %r115, %r117;
	mov.b32 	%f189, %r141;
	add.f32 	%f190, %f188, %f189;
	mov.b32 	%r142, %f190;
	shfl.sync.bfly.b32 	%r144|%p38, %r142, %r126, %r115, %r117;
	mov.b32 	%f191, %r144;
	add.f32 	%f192, %f190, %f191;
	mov.b32 	%r145, %f192;
	shfl.sync.bfly.b32 	%r147|%p39, %r145, %r129, %r115, %r117;
	mov.b32 	%f193, %r147;
	add.f32 	%f194, %f192, %f193;
	st.local.f32 	[%rd3+8], %f194;

$L__BB10_17:
	bar.sync 	0;
	setp.gt.s32 	%p40, %r3, 2;
	@%p40 bra 	$L__BB10_19;

	mul.wide.s32 	%rd58, %r3, 4;
	add.s64 	%rd59, %rd3, %rd58;
	ld.local.f32 	%f195, [%rd59];
	mad.lo.s32 	%r148, %r3, %r17, %r2;
	cvt.s64.s32 	%rd60, %r148;
	mul.lo.s32 	%r149, %r1, %r18;
	cvt.s64.s32 	%rd61, %r149;
	add.s64 	%rd62, %rd61, %rd60;
	cvta.to.global.u64 	%rd63, %rd28;
	shl.b64 	%rd64, %rd62, 2;
	add.s64 	%rd65, %rd63, %rd64;
	st.global.f32 	[%rd65], %f195;

$L__BB10_19:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_4_bs_64
.visible .entry ggml_matvec_f32_ncols_4_bs_64(
	.param .u64 ggml_matvec_f32_ncols_4_bs_64_param_0,
	.param .u64 ggml_matvec_f32_ncols_4_bs_64_param_1,
	.param .u64 ggml_matvec_f32_ncols_4_bs_64_param_2,
	.param .u32 ggml_matvec_f32_ncols_4_bs_64_param_3,
	.param .u32 ggml_matvec_f32_ncols_4_bs_64_param_4,
	.param .u32 ggml_matvec_f32_ncols_4_bs_64_param_5,
	.param .u32 ggml_matvec_f32_ncols_4_bs_64_param_6,
	.param .u32 ggml_matvec_f32_ncols_4_bs_64_param_7,
	.param .u32 ggml_matvec_f32_ncols_4_bs_64_param_8,
	.param .u32 ggml_matvec_f32_ncols_4_bs_64_param_9,
	.param .u32 ggml_matvec_f32_ncols_4_bs_64_param_10,
	.param .u32 ggml_matvec_f32_ncols_4_bs_64_param_11
)
{
	.local .align 16 .b8 	__local_depot11[16];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<52>;
	.reg .f32 	%f<270>;
	.reg .b32 	%r<189>;
	.reg .b64 	%rd<83>;


	mov.u64 	%SPL, __local_depot11;
	ld.param.u64 	%rd34, [ggml_matvec_f32_ncols_4_bs_64_param_0];
	ld.param.u64 	%rd35, [ggml_matvec_f32_ncols_4_bs_64_param_1];
	ld.param.u64 	%rd33, [ggml_matvec_f32_ncols_4_bs_64_param_2];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_4_bs_64_param_3];
	ld.param.u32 	%r19, [ggml_matvec_f32_ncols_4_bs_64_param_5];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_4_bs_64_param_6];
	ld.param.u32 	%r17, [ggml_matvec_f32_ncols_4_bs_64_param_7];
	ld.param.u32 	%r20, [ggml_matvec_f32_ncols_4_bs_64_param_8];
	ld.param.u32 	%r21, [ggml_matvec_f32_ncols_4_bs_64_param_9];
	ld.param.u32 	%r22, [ggml_matvec_f32_ncols_4_bs_64_param_10];
	ld.param.u32 	%r18, [ggml_matvec_f32_ncols_4_bs_64_param_11];
	cvta.to.global.u64 	%rd82, %rd35;
	cvta.to.global.u64 	%rd2, %rd34;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r23, %r1, %r20;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r24, %r2, %r19;
	mad.lo.s32 	%r25, %r23, %r21, %r24;
	cvt.s64.s32 	%rd4, %r25;
	mul.lo.s32 	%r26, %r1, %r22;
	cvt.s64.s32 	%rd5, %r26;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r27, %r3, 2;
	mov.u32 	%r28, data_mmv;
	add.s32 	%r4, %r28, %r27;
	@%p1 bra 	$L__BB11_2;

	mov.u32 	%r29, 0;
	st.shared.u32 	[%r4], %r29;

$L__BB11_2:
	bar.sync 	0;
	mov.f32 	%f266, 0f00000000;
	st.local.v4.f32 	[%rd3], {%f266, %f266, %f266, %f266};
	setp.ge.s32 	%p2, %r3, %r15;
	mov.f32 	%f267, %f266;
	mov.f32 	%f268, %f266;
	mov.f32 	%f269, %f266;
	@%p2 bra 	$L__BB11_11;

	not.b32 	%r30, %r3;
	add.s32 	%r5, %r30, %r15;
	shr.u32 	%r31, %r5, 6;
	add.s32 	%r32, %r31, 1;
	and.b32  	%r186, %r32, 3;
	setp.eq.s32 	%p3, %r186, 0;
	mov.f32 	%f266, 0f00000000;
	mov.u32 	%r187, %r3;
	@%p3 bra 	$L__BB11_7;

	shl.b32 	%r33, %r16, 1;
	add.s32 	%r34, %r3, %r33;
	mul.wide.s32 	%rd37, %r34, 2;
	add.s64 	%rd38, %rd37, %rd5;
	shl.b64 	%rd39, %rd38, 2;
	add.s64 	%rd80, %rd82, %rd39;
	mad.lo.s32 	%r35, %r16, 3, %r3;
	mul.wide.s32 	%rd40, %r35, 2;
	add.s64 	%rd41, %rd40, %rd5;
	shl.b64 	%rd42, %rd41, 2;
	add.s64 	%rd79, %rd82, %rd42;
	mul.wide.s32 	%rd43, %r16, 2;
	mul.wide.s32 	%rd44, %r3, 2;
	add.s64 	%rd45, %rd43, %rd44;
	add.s64 	%rd46, %rd45, %rd5;
	shl.b64 	%rd47, %rd46, 2;
	add.s64 	%rd78, %rd82, %rd47;
	add.s64 	%rd48, %rd44, %rd5;
	shl.b64 	%rd49, %rd48, 2;
	add.s64 	%rd77, %rd82, %rd49;
	add.s64 	%rd50, %rd44, %rd4;
	shl.b64 	%rd51, %rd50, 2;
	add.s64 	%rd76, %rd2, %rd51;
	mov.f32 	%f266, 0f00000000;
	mov.f32 	%f267, %f266;
	mov.f32 	%f268, %f266;
	mov.f32 	%f269, %f266;
	mov.u32 	%r187, %r3;

$L__BB11_5:
	.pragma "nounroll";
	ld.global.nc.v2.f32 	{%f37, %f38}, [%rd76];
	ld.global.nc.v2.f32 	{%f41, %f42}, [%rd77];
	fma.rn.f32 	%f45, %f37, %f41, %f269;
	fma.rn.f32 	%f269, %f38, %f42, %f45;
	ld.global.nc.v2.f32 	{%f46, %f47}, [%rd78];
	fma.rn.f32 	%f50, %f37, %f46, %f268;
	fma.rn.f32 	%f268, %f38, %f47, %f50;
	ld.global.nc.v2.f32 	{%f51, %f52}, [%rd80];
	fma.rn.f32 	%f55, %f37, %f51, %f267;
	fma.rn.f32 	%f267, %f38, %f52, %f55;
	ld.global.nc.v2.f32 	{%f56, %f57}, [%rd79];
	fma.rn.f32 	%f60, %f37, %f56, %f266;
	fma.rn.f32 	%f266, %f38, %f57, %f60;
	add.s32 	%r187, %r187, 64;
	add.s64 	%rd80, %rd80, 512;
	add.s64 	%rd79, %rd79, 512;
	add.s64 	%rd78, %rd78, 512;
	add.s64 	%rd77, %rd77, 512;
	add.s64 	%rd76, %rd76, 512;
	add.s32 	%r186, %r186, -1;
	setp.ne.s32 	%p4, %r186, 0;
	@%p4 bra 	$L__BB11_5;

	st.local.v4.f32 	[%rd3], {%f269, %f268, %f267, %f266};

$L__BB11_7:
	setp.lt.u32 	%p5, %r5, 192;
	@%p5 bra 	$L__BB11_11;

	add.s32 	%r36, %r187, %r16;
	shl.b32 	%r37, %r16, 1;
	add.s32 	%r38, %r187, %r37;
	mad.lo.s32 	%r39, %r16, 3, %r187;
	add.s32 	%r40, %r36, 64;
	mul.wide.s32 	%rd52, %r40, 8;
	shl.b64 	%rd53, %rd5, 2;
	add.s64 	%rd23, %rd52, %rd53;
	mul.wide.s32 	%rd54, %r38, 8;
	add.s64 	%rd24, %rd54, %rd53;
	mul.wide.s32 	%rd55, %r39, 8;
	add.s64 	%rd25, %rd55, %rd53;
	mul.wide.s32 	%rd56, %r187, 2;
	add.s64 	%rd57, %rd56, %rd4;
	shl.b64 	%rd58, %rd57, 2;
	add.s64 	%rd59, %rd2, %rd58;
	add.s64 	%rd81, %rd59, 1024;
	mul.wide.s32 	%rd60, %r187, 8;
	add.s64 	%rd27, %rd60, %rd53;
	mul.wide.s32 	%rd61, %r16, 8;
	add.s64 	%rd62, %rd60, %rd61;
	add.s64 	%rd28, %rd62, %rd53;

$L__BB11_9:
	ld.global.nc.v2.f32 	{%f61, %f62}, [%rd81+-1024];
	add.s64 	%rd63, %rd82, %rd27;
	ld.global.nc.v2.f32 	{%f65, %f66}, [%rd63];
	fma.rn.f32 	%f69, %f61, %f65, %f269;
	fma.rn.f32 	%f70, %f62, %f66, %f69;
	add.s64 	%rd64, %rd82, %rd28;
	ld.global.nc.v2.f32 	{%f71, %f72}, [%rd64];
	fma.rn.f32 	%f75, %f61, %f71, %f268;
	fma.rn.f32 	%f76, %f62, %f72, %f75;
	add.s64 	%rd65, %rd82, %rd24;
	ld.global.nc.v2.f32 	{%f77, %f78}, [%rd65];
	fma.rn.f32 	%f81, %f61, %f77, %f267;
	fma.rn.f32 	%f82, %f62, %f78, %f81;
	add.s64 	%rd66, %rd82, %rd25;
	ld.global.nc.v2.f32 	{%f83, %f84}, [%rd66];
	fma.rn.f32 	%f87, %f61, %f83, %f266;
	fma.rn.f32 	%f88, %f62, %f84, %f87;
	ld.global.nc.v2.f32 	{%f89, %f90}, [%rd81+-512];
	ld.global.nc.v2.f32 	{%f93, %f94}, [%rd63+512];
	fma.rn.f32 	%f97, %f89, %f93, %f70;
	fma.rn.f32 	%f98, %f90, %f94, %f97;
	add.s64 	%rd67, %rd82, %rd23;
	ld.global.nc.v2.f32 	{%f99, %f100}, [%rd67];
	fma.rn.f32 	%f103, %f89, %f99, %f76;
	fma.rn.f32 	%f104, %f90, %f100, %f103;
	ld.global.nc.v2.f32 	{%f105, %f106}, [%rd65+512];
	fma.rn.f32 	%f109, %f89, %f105, %f82;
	fma.rn.f32 	%f110, %f90, %f106, %f109;
	ld.global.nc.v2.f32 	{%f111, %f112}, [%rd66+512];
	fma.rn.f32 	%f115, %f89, %f111, %f88;
	fma.rn.f32 	%f116, %f90, %f112, %f115;
	ld.global.nc.v2.f32 	{%f117, %f118}, [%rd81];
	ld.global.nc.v2.f32 	{%f121, %f122}, [%rd63+1024];
	fma.rn.f32 	%f125, %f117, %f121, %f98;
	fma.rn.f32 	%f126, %f118, %f122, %f125;
	ld.global.nc.v2.f32 	{%f127, %f128}, [%rd67+512];
	fma.rn.f32 	%f131, %f117, %f127, %f104;
	fma.rn.f32 	%f132, %f118, %f128, %f131;
	ld.global.nc.v2.f32 	{%f133, %f134}, [%rd65+1024];
	fma.rn.f32 	%f137, %f117, %f133, %f110;
	fma.rn.f32 	%f138, %f118, %f134, %f137;
	ld.global.nc.v2.f32 	{%f139, %f140}, [%rd66+1024];
	fma.rn.f32 	%f143, %f117, %f139, %f116;
	fma.rn.f32 	%f144, %f118, %f140, %f143;
	ld.global.nc.v2.f32 	{%f145, %f146}, [%rd81+512];
	ld.global.nc.v2.f32 	{%f149, %f150}, [%rd63+1536];
	fma.rn.f32 	%f153, %f145, %f149, %f126;
	fma.rn.f32 	%f269, %f146, %f150, %f153;
	ld.global.nc.v2.f32 	{%f154, %f155}, [%rd67+1024];
	fma.rn.f32 	%f158, %f145, %f154, %f132;
	fma.rn.f32 	%f268, %f146, %f155, %f158;
	ld.global.nc.v2.f32 	{%f159, %f160}, [%rd65+1536];
	fma.rn.f32 	%f163, %f145, %f159, %f138;
	fma.rn.f32 	%f267, %f146, %f160, %f163;
	ld.global.nc.v2.f32 	{%f164, %f165}, [%rd66+1536];
	fma.rn.f32 	%f168, %f145, %f164, %f144;
	fma.rn.f32 	%f266, %f146, %f165, %f168;
	add.s64 	%rd82, %rd82, 2048;
	add.s64 	%rd81, %rd81, 2048;
	add.s32 	%r187, %r187, 256;
	setp.lt.s32 	%p6, %r187, %r15;
	@%p6 bra 	$L__BB11_9;

	st.local.v4.f32 	[%rd3], {%f269, %f268, %f267, %f266};

$L__BB11_11:
	shr.s32 	%r41, %r3, 31;
	shr.u32 	%r42, %r41, 27;
	add.s32 	%r43, %r3, %r42;
	shr.s32 	%r44, %r43, 5;
	shl.b32 	%r45, %r44, 2;
	add.s32 	%r14, %r28, %r45;
	mov.u32 	%r47, 2;
	mov.b32 	%r48, %f269;
	mov.u32 	%r49, 31;
	mov.u32 	%r50, 16;
	mov.u32 	%r51, -1;
	shfl.sync.bfly.b32 	%r52|%p7, %r48, %r50, %r49, %r51;
	mov.b32 	%f169, %r52;
	add.f32 	%f170, %f269, %f169;
	mov.b32 	%r53, %f170;
	mov.u32 	%r54, 8;
	shfl.sync.bfly.b32 	%r55|%p8, %r53, %r54, %r49, %r51;
	mov.b32 	%f171, %r55;
	add.f32 	%f172, %f170, %f171;
	mov.b32 	%r56, %f172;
	mov.u32 	%r57, 4;
	shfl.sync.bfly.b32 	%r58|%p9, %r56, %r57, %r49, %r51;
	mov.b32 	%f173, %r58;
	add.f32 	%f174, %f172, %f173;
	mov.b32 	%r59, %f174;
	shfl.sync.bfly.b32 	%r60|%p10, %r59, %r47, %r49, %r51;
	mov.b32 	%f175, %r60;
	add.f32 	%f176, %f174, %f175;
	mov.b32 	%r61, %f176;
	mov.u32 	%r62, 1;
	shfl.sync.bfly.b32 	%r63|%p11, %r61, %r62, %r49, %r51;
	mov.b32 	%f177, %r63;
	add.f32 	%f178, %f176, %f177;
	st.local.f32 	[%rd3], %f178;
	st.shared.f32 	[%r14], %f178;
	bar.sync 	0;
	@%p1 bra 	$L__BB11_13;

	ld.shared.f32 	%f179, [%r4];
	mov.b32 	%r64, %f179;
	shfl.sync.bfly.b32 	%r68|%p13, %r64, %r50, %r49, %r51;
	mov.b32 	%f180, %r68;
	add.f32 	%f181, %f179, %f180;
	mov.b32 	%r69, %f181;
	shfl.sync.bfly.b32 	%r71|%p14, %r69, %r54, %r49, %r51;
	mov.b32 	%f182, %r71;
	add.f32 	%f183, %f181, %f182;
	mov.b32 	%r72, %f183;
	shfl.sync.bfly.b32 	%r74|%p15, %r72, %r57, %r49, %r51;
	mov.b32 	%f184, %r74;
	add.f32 	%f185, %f183, %f184;
	mov.b32 	%r75, %f185;
	shfl.sync.bfly.b32 	%r77|%p16, %r75, %r47, %r49, %r51;
	mov.b32 	%f186, %r77;
	add.f32 	%f187, %f185, %f186;
	mov.b32 	%r78, %f187;
	shfl.sync.bfly.b32 	%r80|%p17, %r78, %r62, %r49, %r51;
	mov.b32 	%f188, %r80;
	add.f32 	%f189, %f187, %f188;
	st.local.f32 	[%rd3], %f189;

$L__BB11_13:
	bar.sync 	0;
	mov.b32 	%r81, %f268;
	shfl.sync.bfly.b32 	%r85|%p19, %r81, %r50, %r49, %r51;
	mov.b32 	%f190, %r85;
	add.f32 	%f191, %f268, %f190;
	mov.b32 	%r86, %f191;
	shfl.sync.bfly.b32 	%r88|%p20, %r86, %r54, %r49, %r51;
	mov.b32 	%f192, %r88;
	add.f32 	%f193, %f191, %f192;
	mov.b32 	%r89, %f193;
	shfl.sync.bfly.b32 	%r91|%p21, %r89, %r57, %r49, %r51;
	mov.b32 	%f194, %r91;
	add.f32 	%f195, %f193, %f194;
	mov.b32 	%r92, %f195;
	shfl.sync.bfly.b32 	%r94|%p22, %r92, %r47, %r49, %r51;
	mov.b32 	%f196, %r94;
	add.f32 	%f197, %f195, %f196;
	mov.b32 	%r95, %f197;
	shfl.sync.bfly.b32 	%r97|%p23, %r95, %r62, %r49, %r51;
	mov.b32 	%f198, %r97;
	add.f32 	%f199, %f197, %f198;
	st.local.f32 	[%rd3+4], %f199;
	st.shared.f32 	[%r14], %f199;
	bar.sync 	0;
	@%p1 bra 	$L__BB11_15;

	ld.shared.f32 	%f200, [%r4];
	mov.b32 	%r98, %f200;
	mov.u32 	%r99, 31;
	mov.u32 	%r100, 16;
	mov.u32 	%r101, -1;
	shfl.sync.bfly.b32 	%r102|%p24, %r98, %r100, %r99, %r101;
	mov.b32 	%f201, %r102;
	add.f32 	%f202, %f200, %f201;
	mov.b32 	%r103, %f202;
	mov.u32 	%r104, 8;
	shfl.sync.bfly.b32 	%r105|%p25, %r103, %r104, %r99, %r101;
	mov.b32 	%f203, %r105;
	add.f32 	%f204, %f202, %f203;
	mov.b32 	%r106, %f204;
	mov.u32 	%r107, 4;
	shfl.sync.bfly.b32 	%r108|%p26, %r106, %r107, %r99, %r101;
	mov.b32 	%f205, %r108;
	add.f32 	%f206, %f204, %f205;
	mov.b32 	%r109, %f206;
	mov.u32 	%r110, 2;
	shfl.sync.bfly.b32 	%r111|%p27, %r109, %r110, %r99, %r101;
	mov.b32 	%f207, %r111;
	add.f32 	%f208, %f206, %f207;
	mov.b32 	%r112, %f208;
	mov.u32 	%r113, 1;
	shfl.sync.bfly.b32 	%r114|%p28, %r112, %r113, %r99, %r101;
	mov.b32 	%f209, %r114;
	add.f32 	%f210, %f208, %f209;
	st.local.f32 	[%rd3+4], %f210;

$L__BB11_15:
	bar.sync 	0;
	mov.b32 	%r115, %f267;
	mov.u32 	%r116, 31;
	mov.u32 	%r117, 16;
	mov.u32 	%r118, -1;
	shfl.sync.bfly.b32 	%r119|%p30, %r115, %r117, %r116, %r118;
	mov.b32 	%f211, %r119;
	add.f32 	%f212, %f267, %f211;
	mov.b32 	%r120, %f212;
	mov.u32 	%r121, 8;
	shfl.sync.bfly.b32 	%r122|%p31, %r120, %r121, %r116, %r118;
	mov.b32 	%f213, %r122;
	add.f32 	%f214, %f212, %f213;
	mov.b32 	%r123, %f214;
	mov.u32 	%r124, 4;
	shfl.sync.bfly.b32 	%r125|%p32, %r123, %r124, %r116, %r118;
	mov.b32 	%f215, %r125;
	add.f32 	%f216, %f214, %f215;
	mov.b32 	%r126, %f216;
	mov.u32 	%r127, 2;
	shfl.sync.bfly.b32 	%r128|%p33, %r126, %r127, %r116, %r118;
	mov.b32 	%f217, %r128;
	add.f32 	%f218, %f216, %f217;
	mov.b32 	%r129, %f218;
	mov.u32 	%r130, 1;
	shfl.sync.bfly.b32 	%r131|%p34, %r129, %r130, %r116, %r118;
	mov.b32 	%f219, %r131;
	add.f32 	%f220, %f218, %f219;
	st.local.f32 	[%rd3+8], %f220;
	st.shared.f32 	[%r14], %f220;
	bar.sync 	0;
	@%p1 bra 	$L__BB11_17;

	ld.shared.f32 	%f221, [%r4];
	mov.b32 	%r132, %f221;
	shfl.sync.bfly.b32 	%r136|%p35, %r132, %r117, %r116, %r118;
	mov.b32 	%f222, %r136;
	add.f32 	%f223, %f221, %f222;
	mov.b32 	%r137, %f223;
	shfl.sync.bfly.b32 	%r139|%p36, %r137, %r121, %r116, %r118;
	mov.b32 	%f224, %r139;
	add.f32 	%f225, %f223, %f224;
	mov.b32 	%r140, %f225;
	shfl.sync.bfly.b32 	%r142|%p37, %r140, %r124, %r116, %r118;
	mov.b32 	%f226, %r142;
	add.f32 	%f227, %f225, %f226;
	mov.b32 	%r143, %f227;
	shfl.sync.bfly.b32 	%r145|%p38, %r143, %r127, %r116, %r118;
	mov.b32 	%f228, %r145;
	add.f32 	%f229, %f227, %f228;
	mov.b32 	%r146, %f229;
	shfl.sync.bfly.b32 	%r148|%p39, %r146, %r130, %r116, %r118;
	mov.b32 	%f230, %r148;
	add.f32 	%f231, %f229, %f230;
	st.local.f32 	[%rd3+8], %f231;

$L__BB11_17:
	bar.sync 	0;
	mov.b32 	%r149, %f266;
	shfl.sync.bfly.b32 	%r153|%p41, %r149, %r117, %r116, %r118;
	mov.b32 	%f232, %r153;
	add.f32 	%f233, %f266, %f232;
	mov.b32 	%r154, %f233;
	shfl.sync.bfly.b32 	%r156|%p42, %r154, %r121, %r116, %r118;
	mov.b32 	%f234, %r156;
	add.f32 	%f235, %f233, %f234;
	mov.b32 	%r157, %f235;
	shfl.sync.bfly.b32 	%r159|%p43, %r157, %r124, %r116, %r118;
	mov.b32 	%f236, %r159;
	add.f32 	%f237, %f235, %f236;
	mov.b32 	%r160, %f237;
	shfl.sync.bfly.b32 	%r162|%p44, %r160, %r127, %r116, %r118;
	mov.b32 	%f238, %r162;
	add.f32 	%f239, %f237, %f238;
	mov.b32 	%r163, %f239;
	shfl.sync.bfly.b32 	%r165|%p45, %r163, %r130, %r116, %r118;
	mov.b32 	%f240, %r165;
	add.f32 	%f241, %f239, %f240;
	st.local.f32 	[%rd3+12], %f241;
	st.shared.f32 	[%r14], %f241;
	bar.sync 	0;
	@%p1 bra 	$L__BB11_19;

	ld.shared.f32 	%f242, [%r4];
	mov.b32 	%r166, %f242;
	mov.u32 	%r167, 31;
	mov.u32 	%r168, 16;
	mov.u32 	%r169, -1;
	shfl.sync.bfly.b32 	%r170|%p46, %r166, %r168, %r167, %r169;
	mov.b32 	%f243, %r170;
	add.f32 	%f244, %f242, %f243;
	mov.b32 	%r171, %f244;
	mov.u32 	%r172, 8;
	shfl.sync.bfly.b32 	%r173|%p47, %r171, %r172, %r167, %r169;
	mov.b32 	%f245, %r173;
	add.f32 	%f246, %f244, %f245;
	mov.b32 	%r174, %f246;
	mov.u32 	%r175, 4;
	shfl.sync.bfly.b32 	%r176|%p48, %r174, %r175, %r167, %r169;
	mov.b32 	%f247, %r176;
	add.f32 	%f248, %f246, %f247;
	mov.b32 	%r177, %f248;
	mov.u32 	%r178, 2;
	shfl.sync.bfly.b32 	%r179|%p49, %r177, %r178, %r167, %r169;
	mov.b32 	%f249, %r179;
	add.f32 	%f250, %f248, %f249;
	mov.b32 	%r180, %f250;
	mov.u32 	%r181, 1;
	shfl.sync.bfly.b32 	%r182|%p50, %r180, %r181, %r167, %r169;
	mov.b32 	%f251, %r182;
	add.f32 	%f252, %f250, %f251;
	st.local.f32 	[%rd3+12], %f252;

$L__BB11_19:
	bar.sync 	0;
	setp.gt.s32 	%p51, %r3, 3;
	@%p51 bra 	$L__BB11_21;

	mul.wide.s32 	%rd68, %r3, 4;
	add.s64 	%rd69, %rd3, %rd68;
	ld.local.f32 	%f253, [%rd69];
	mad.lo.s32 	%r183, %r3, %r17, %r2;
	cvt.s64.s32 	%rd70, %r183;
	mul.lo.s32 	%r184, %r1, %r18;
	cvt.s64.s32 	%rd71, %r184;
	add.s64 	%rd72, %rd71, %rd70;
	cvta.to.global.u64 	%rd73, %rd33;
	shl.b64 	%rd74, %rd72, 2;
	add.s64 	%rd75, %rd73, %rd74;
	st.global.f32 	[%rd75], %f253;

$L__BB11_21:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_5_bs_64
.visible .entry ggml_matvec_f32_ncols_5_bs_64(
	.param .u64 ggml_matvec_f32_ncols_5_bs_64_param_0,
	.param .u64 ggml_matvec_f32_ncols_5_bs_64_param_1,
	.param .u64 ggml_matvec_f32_ncols_5_bs_64_param_2,
	.param .u32 ggml_matvec_f32_ncols_5_bs_64_param_3,
	.param .u32 ggml_matvec_f32_ncols_5_bs_64_param_4,
	.param .u32 ggml_matvec_f32_ncols_5_bs_64_param_5,
	.param .u32 ggml_matvec_f32_ncols_5_bs_64_param_6,
	.param .u32 ggml_matvec_f32_ncols_5_bs_64_param_7,
	.param .u32 ggml_matvec_f32_ncols_5_bs_64_param_8,
	.param .u32 ggml_matvec_f32_ncols_5_bs_64_param_9,
	.param .u32 ggml_matvec_f32_ncols_5_bs_64_param_10,
	.param .u32 ggml_matvec_f32_ncols_5_bs_64_param_11
)
{
	.local .align 4 .b8 	__local_depot12[20];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<63>;
	.reg .f32 	%f<332>;
	.reg .b32 	%r<225>;
	.reg .b64 	%rd<74>;


	mov.u64 	%SPL, __local_depot12;
	ld.param.u64 	%rd28, [ggml_matvec_f32_ncols_5_bs_64_param_0];
	ld.param.u64 	%rd29, [ggml_matvec_f32_ncols_5_bs_64_param_1];
	ld.param.u64 	%rd27, [ggml_matvec_f32_ncols_5_bs_64_param_2];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_5_bs_64_param_3];
	ld.param.u32 	%r19, [ggml_matvec_f32_ncols_5_bs_64_param_5];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_5_bs_64_param_6];
	ld.param.u32 	%r17, [ggml_matvec_f32_ncols_5_bs_64_param_7];
	ld.param.u32 	%r20, [ggml_matvec_f32_ncols_5_bs_64_param_8];
	ld.param.u32 	%r21, [ggml_matvec_f32_ncols_5_bs_64_param_9];
	ld.param.u32 	%r22, [ggml_matvec_f32_ncols_5_bs_64_param_10];
	ld.param.u32 	%r18, [ggml_matvec_f32_ncols_5_bs_64_param_11];
	cvta.to.global.u64 	%rd73, %rd29;
	cvta.to.global.u64 	%rd2, %rd28;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r23, %r1, %r20;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r24, %r2, %r19;
	mad.lo.s32 	%r25, %r23, %r21, %r24;
	cvt.s64.s32 	%rd4, %r25;
	mul.lo.s32 	%r26, %r1, %r22;
	cvt.s64.s32 	%rd5, %r26;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r27, %r3, 2;
	mov.u32 	%r28, data_mmv;
	add.s32 	%r4, %r28, %r27;
	@%p1 bra 	$L__BB12_2;

	mov.u32 	%r29, 0;
	st.shared.u32 	[%r4], %r29;

$L__BB12_2:
	bar.sync 	0;
	mov.f32 	%f327, 0f00000000;
	mov.u32 	%r30, 0;
	st.local.u32 	[%rd3], %r30;
	st.local.u32 	[%rd3+4], %r30;
	st.local.u32 	[%rd3+8], %r30;
	st.local.u32 	[%rd3+12], %r30;
	st.local.u32 	[%rd3+16], %r30;
	setp.ge.s32 	%p2, %r3, %r15;
	mov.f32 	%f328, %f327;
	mov.f32 	%f329, %f327;
	mov.f32 	%f330, %f327;
	mov.f32 	%f331, %f327;
	@%p2 bra 	$L__BB12_11;

	not.b32 	%r31, %r3;
	add.s32 	%r5, %r31, %r15;
	shr.u32 	%r32, %r5, 6;
	add.s32 	%r33, %r32, 1;
	and.b32  	%r222, %r33, 3;
	setp.eq.s32 	%p3, %r222, 0;
	mov.f32 	%f327, 0f00000000;
	mov.u32 	%r223, %r3;
	@%p3 bra 	$L__BB12_7;

	shl.b32 	%r34, %r16, 1;
	mad.lo.s32 	%r35, %r16, 3, %r3;
	mul.wide.s32 	%rd31, %r35, 8;
	shl.b64 	%rd32, %rd5, 2;
	add.s64 	%rd7, %rd31, %rd32;
	mul.wide.s32 	%rd33, %r3, 8;
	mul.wide.s32 	%rd34, %r16, 8;
	add.s64 	%rd35, %rd33, %rd34;
	add.s64 	%rd8, %rd35, %rd32;
	add.s64 	%rd9, %rd33, %rd32;
	mul.wide.s32 	%rd36, %r3, 2;
	add.s64 	%rd37, %rd36, %rd4;
	shl.b64 	%rd38, %rd37, 2;
	add.s64 	%rd70, %rd2, %rd38;
	mul.wide.s32 	%rd11, %r34, 8;
	mov.f32 	%f327, 0f00000000;
	mov.u64 	%rd71, %rd73;
	mov.f32 	%f328, %f327;
	mov.f32 	%f329, %f327;
	mov.f32 	%f330, %f327;
	mov.f32 	%f331, %f327;
	mov.u32 	%r223, %r3;

$L__BB12_5:
	.pragma "nounroll";
	ld.global.nc.v2.f32 	{%f46, %f47}, [%rd70];
	add.s64 	%rd39, %rd71, %rd9;
	ld.global.nc.v2.f32 	{%f50, %f51}, [%rd39];
	fma.rn.f32 	%f54, %f46, %f50, %f331;
	fma.rn.f32 	%f331, %f47, %f51, %f54;
	add.s64 	%rd40, %rd71, %rd8;
	ld.global.nc.v2.f32 	{%f55, %f56}, [%rd40];
	fma.rn.f32 	%f59, %f46, %f55, %f330;
	fma.rn.f32 	%f330, %f47, %f56, %f59;
	add.s64 	%rd41, %rd39, %rd11;
	ld.global.nc.v2.f32 	{%f60, %f61}, [%rd41];
	fma.rn.f32 	%f64, %f46, %f60, %f329;
	fma.rn.f32 	%f329, %f47, %f61, %f64;
	add.s64 	%rd42, %rd71, %rd7;
	ld.global.nc.v2.f32 	{%f65, %f66}, [%rd42];
	fma.rn.f32 	%f69, %f46, %f65, %f328;
	fma.rn.f32 	%f328, %f47, %f66, %f69;
	add.s64 	%rd43, %rd41, %rd11;
	ld.global.nc.v2.f32 	{%f70, %f71}, [%rd43];
	fma.rn.f32 	%f74, %f46, %f70, %f327;
	fma.rn.f32 	%f327, %f47, %f71, %f74;
	add.s32 	%r223, %r223, 64;
	add.s64 	%rd71, %rd71, 512;
	add.s64 	%rd70, %rd70, 512;
	add.s32 	%r222, %r222, -1;
	setp.ne.s32 	%p4, %r222, 0;
	@%p4 bra 	$L__BB12_5;

	st.local.f32 	[%rd3], %f331;
	st.local.f32 	[%rd3+4], %f330;
	st.local.f32 	[%rd3+8], %f329;
	st.local.f32 	[%rd3+12], %f328;
	st.local.f32 	[%rd3+16], %f327;

$L__BB12_7:
	setp.lt.u32 	%p5, %r5, 192;
	@%p5 bra 	$L__BB12_11;

	add.s32 	%r36, %r223, %r16;
	shl.b32 	%r37, %r16, 1;
	add.s32 	%r38, %r223, %r37;
	mad.lo.s32 	%r39, %r16, 3, %r223;
	shl.b32 	%r40, %r16, 2;
	add.s32 	%r41, %r223, %r40;
	add.s32 	%r42, %r36, 64;
	mul.wide.s32 	%rd44, %r42, 8;
	shl.b64 	%rd45, %rd5, 2;
	add.s64 	%rd16, %rd44, %rd45;
	mul.wide.s32 	%rd46, %r38, 8;
	add.s64 	%rd17, %rd46, %rd45;
	mul.wide.s32 	%rd47, %r39, 8;
	add.s64 	%rd18, %rd47, %rd45;
	mul.wide.s32 	%rd48, %r41, 8;
	add.s64 	%rd19, %rd48, %rd45;
	mul.wide.s32 	%rd49, %r223, 2;
	add.s64 	%rd50, %rd49, %rd4;
	shl.b64 	%rd51, %rd50, 2;
	add.s64 	%rd52, %rd2, %rd51;
	add.s64 	%rd72, %rd52, 1024;
	mul.wide.s32 	%rd53, %r223, 8;
	add.s64 	%rd21, %rd53, %rd45;
	mul.wide.s32 	%rd54, %r16, 8;
	add.s64 	%rd55, %rd53, %rd54;
	add.s64 	%rd22, %rd55, %rd45;

$L__BB12_9:
	ld.global.nc.v2.f32 	{%f75, %f76}, [%rd72+-1024];
	add.s64 	%rd56, %rd73, %rd21;
	ld.global.nc.v2.f32 	{%f79, %f80}, [%rd56];
	fma.rn.f32 	%f83, %f75, %f79, %f331;
	fma.rn.f32 	%f84, %f76, %f80, %f83;
	add.s64 	%rd57, %rd73, %rd22;
	ld.global.nc.v2.f32 	{%f85, %f86}, [%rd57];
	fma.rn.f32 	%f89, %f75, %f85, %f330;
	fma.rn.f32 	%f90, %f76, %f86, %f89;
	add.s64 	%rd58, %rd73, %rd17;
	ld.global.nc.v2.f32 	{%f91, %f92}, [%rd58];
	fma.rn.f32 	%f95, %f75, %f91, %f329;
	fma.rn.f32 	%f96, %f76, %f92, %f95;
	add.s64 	%rd59, %rd73, %rd18;
	ld.global.nc.v2.f32 	{%f97, %f98}, [%rd59];
	fma.rn.f32 	%f101, %f75, %f97, %f328;
	fma.rn.f32 	%f102, %f76, %f98, %f101;
	add.s64 	%rd60, %rd73, %rd19;
	ld.global.nc.v2.f32 	{%f103, %f104}, [%rd60];
	fma.rn.f32 	%f107, %f75, %f103, %f327;
	fma.rn.f32 	%f108, %f76, %f104, %f107;
	ld.global.nc.v2.f32 	{%f109, %f110}, [%rd72+-512];
	ld.global.nc.v2.f32 	{%f113, %f114}, [%rd56+512];
	fma.rn.f32 	%f117, %f109, %f113, %f84;
	fma.rn.f32 	%f118, %f110, %f114, %f117;
	add.s64 	%rd61, %rd73, %rd16;
	ld.global.nc.v2.f32 	{%f119, %f120}, [%rd61];
	fma.rn.f32 	%f123, %f109, %f119, %f90;
	fma.rn.f32 	%f124, %f110, %f120, %f123;
	ld.global.nc.v2.f32 	{%f125, %f126}, [%rd58+512];
	fma.rn.f32 	%f129, %f109, %f125, %f96;
	fma.rn.f32 	%f130, %f110, %f126, %f129;
	ld.global.nc.v2.f32 	{%f131, %f132}, [%rd59+512];
	fma.rn.f32 	%f135, %f109, %f131, %f102;
	fma.rn.f32 	%f136, %f110, %f132, %f135;
	ld.global.nc.v2.f32 	{%f137, %f138}, [%rd60+512];
	fma.rn.f32 	%f141, %f109, %f137, %f108;
	fma.rn.f32 	%f142, %f110, %f138, %f141;
	ld.global.nc.v2.f32 	{%f143, %f144}, [%rd72];
	ld.global.nc.v2.f32 	{%f147, %f148}, [%rd56+1024];
	fma.rn.f32 	%f151, %f143, %f147, %f118;
	fma.rn.f32 	%f152, %f144, %f148, %f151;
	ld.global.nc.v2.f32 	{%f153, %f154}, [%rd61+512];
	fma.rn.f32 	%f157, %f143, %f153, %f124;
	fma.rn.f32 	%f158, %f144, %f154, %f157;
	ld.global.nc.v2.f32 	{%f159, %f160}, [%rd58+1024];
	fma.rn.f32 	%f163, %f143, %f159, %f130;
	fma.rn.f32 	%f164, %f144, %f160, %f163;
	ld.global.nc.v2.f32 	{%f165, %f166}, [%rd59+1024];
	fma.rn.f32 	%f169, %f143, %f165, %f136;
	fma.rn.f32 	%f170, %f144, %f166, %f169;
	ld.global.nc.v2.f32 	{%f171, %f172}, [%rd60+1024];
	fma.rn.f32 	%f175, %f143, %f171, %f142;
	fma.rn.f32 	%f176, %f144, %f172, %f175;
	ld.global.nc.v2.f32 	{%f177, %f178}, [%rd72+512];
	ld.global.nc.v2.f32 	{%f181, %f182}, [%rd56+1536];
	fma.rn.f32 	%f185, %f177, %f181, %f152;
	fma.rn.f32 	%f331, %f178, %f182, %f185;
	ld.global.nc.v2.f32 	{%f186, %f187}, [%rd61+1024];
	fma.rn.f32 	%f190, %f177, %f186, %f158;
	fma.rn.f32 	%f330, %f178, %f187, %f190;
	ld.global.nc.v2.f32 	{%f191, %f192}, [%rd58+1536];
	fma.rn.f32 	%f195, %f177, %f191, %f164;
	fma.rn.f32 	%f329, %f178, %f192, %f195;
	ld.global.nc.v2.f32 	{%f196, %f197}, [%rd59+1536];
	fma.rn.f32 	%f200, %f177, %f196, %f170;
	fma.rn.f32 	%f328, %f178, %f197, %f200;
	ld.global.nc.v2.f32 	{%f201, %f202}, [%rd60+1536];
	fma.rn.f32 	%f205, %f177, %f201, %f176;
	fma.rn.f32 	%f327, %f178, %f202, %f205;
	add.s64 	%rd73, %rd73, 2048;
	add.s64 	%rd72, %rd72, 2048;
	add.s32 	%r223, %r223, 256;
	setp.lt.s32 	%p6, %r223, %r15;
	@%p6 bra 	$L__BB12_9;

	st.local.f32 	[%rd3], %f331;
	st.local.f32 	[%rd3+4], %f330;
	st.local.f32 	[%rd3+8], %f329;
	st.local.f32 	[%rd3+12], %f328;
	st.local.f32 	[%rd3+16], %f327;

$L__BB12_11:
	shr.s32 	%r43, %r3, 31;
	shr.u32 	%r44, %r43, 27;
	add.s32 	%r45, %r3, %r44;
	shr.s32 	%r46, %r45, 5;
	shl.b32 	%r47, %r46, 2;
	add.s32 	%r14, %r28, %r47;
	mov.u32 	%r49, 2;
	mov.b32 	%r50, %f331;
	mov.u32 	%r51, 31;
	mov.u32 	%r52, 16;
	mov.u32 	%r53, -1;
	shfl.sync.bfly.b32 	%r54|%p7, %r50, %r52, %r51, %r53;
	mov.b32 	%f206, %r54;
	add.f32 	%f207, %f331, %f206;
	mov.b32 	%r55, %f207;
	mov.u32 	%r56, 8;
	shfl.sync.bfly.b32 	%r57|%p8, %r55, %r56, %r51, %r53;
	mov.b32 	%f208, %r57;
	add.f32 	%f209, %f207, %f208;
	mov.b32 	%r58, %f209;
	mov.u32 	%r59, 4;
	shfl.sync.bfly.b32 	%r60|%p9, %r58, %r59, %r51, %r53;
	mov.b32 	%f210, %r60;
	add.f32 	%f211, %f209, %f210;
	mov.b32 	%r61, %f211;
	shfl.sync.bfly.b32 	%r62|%p10, %r61, %r49, %r51, %r53;
	mov.b32 	%f212, %r62;
	add.f32 	%f213, %f211, %f212;
	mov.b32 	%r63, %f213;
	mov.u32 	%r64, 1;
	shfl.sync.bfly.b32 	%r65|%p11, %r63, %r64, %r51, %r53;
	mov.b32 	%f214, %r65;
	add.f32 	%f215, %f213, %f214;
	st.local.f32 	[%rd3], %f215;
	st.shared.f32 	[%r14], %f215;
	bar.sync 	0;
	@%p1 bra 	$L__BB12_13;

	ld.shared.f32 	%f216, [%r4];
	mov.b32 	%r66, %f216;
	shfl.sync.bfly.b32 	%r70|%p13, %r66, %r52, %r51, %r53;
	mov.b32 	%f217, %r70;
	add.f32 	%f218, %f216, %f217;
	mov.b32 	%r71, %f218;
	shfl.sync.bfly.b32 	%r73|%p14, %r71, %r56, %r51, %r53;
	mov.b32 	%f219, %r73;
	add.f32 	%f220, %f218, %f219;
	mov.b32 	%r74, %f220;
	shfl.sync.bfly.b32 	%r76|%p15, %r74, %r59, %r51, %r53;
	mov.b32 	%f221, %r76;
	add.f32 	%f222, %f220, %f221;
	mov.b32 	%r77, %f222;
	shfl.sync.bfly.b32 	%r79|%p16, %r77, %r49, %r51, %r53;
	mov.b32 	%f223, %r79;
	add.f32 	%f224, %f222, %f223;
	mov.b32 	%r80, %f224;
	shfl.sync.bfly.b32 	%r82|%p17, %r80, %r64, %r51, %r53;
	mov.b32 	%f225, %r82;
	add.f32 	%f226, %f224, %f225;
	st.local.f32 	[%rd3], %f226;

$L__BB12_13:
	bar.sync 	0;
	mov.b32 	%r83, %f330;
	shfl.sync.bfly.b32 	%r87|%p19, %r83, %r52, %r51, %r53;
	mov.b32 	%f227, %r87;
	add.f32 	%f228, %f330, %f227;
	mov.b32 	%r88, %f228;
	shfl.sync.bfly.b32 	%r90|%p20, %r88, %r56, %r51, %r53;
	mov.b32 	%f229, %r90;
	add.f32 	%f230, %f228, %f229;
	mov.b32 	%r91, %f230;
	shfl.sync.bfly.b32 	%r93|%p21, %r91, %r59, %r51, %r53;
	mov.b32 	%f231, %r93;
	add.f32 	%f232, %f230, %f231;
	mov.b32 	%r94, %f232;
	shfl.sync.bfly.b32 	%r96|%p22, %r94, %r49, %r51, %r53;
	mov.b32 	%f233, %r96;
	add.f32 	%f234, %f232, %f233;
	mov.b32 	%r97, %f234;
	shfl.sync.bfly.b32 	%r99|%p23, %r97, %r64, %r51, %r53;
	mov.b32 	%f235, %r99;
	add.f32 	%f236, %f234, %f235;
	st.local.f32 	[%rd3+4], %f236;
	st.shared.f32 	[%r14], %f236;
	bar.sync 	0;
	@%p1 bra 	$L__BB12_15;

	ld.shared.f32 	%f237, [%r4];
	mov.b32 	%r100, %f237;
	mov.u32 	%r101, 31;
	mov.u32 	%r102, 16;
	mov.u32 	%r103, -1;
	shfl.sync.bfly.b32 	%r104|%p24, %r100, %r102, %r101, %r103;
	mov.b32 	%f238, %r104;
	add.f32 	%f239, %f237, %f238;
	mov.b32 	%r105, %f239;
	mov.u32 	%r106, 8;
	shfl.sync.bfly.b32 	%r107|%p25, %r105, %r106, %r101, %r103;
	mov.b32 	%f240, %r107;
	add.f32 	%f241, %f239, %f240;
	mov.b32 	%r108, %f241;
	mov.u32 	%r109, 4;
	shfl.sync.bfly.b32 	%r110|%p26, %r108, %r109, %r101, %r103;
	mov.b32 	%f242, %r110;
	add.f32 	%f243, %f241, %f242;
	mov.b32 	%r111, %f243;
	mov.u32 	%r112, 2;
	shfl.sync.bfly.b32 	%r113|%p27, %r111, %r112, %r101, %r103;
	mov.b32 	%f244, %r113;
	add.f32 	%f245, %f243, %f244;
	mov.b32 	%r114, %f245;
	mov.u32 	%r115, 1;
	shfl.sync.bfly.b32 	%r116|%p28, %r114, %r115, %r101, %r103;
	mov.b32 	%f246, %r116;
	add.f32 	%f247, %f245, %f246;
	st.local.f32 	[%rd3+4], %f247;

$L__BB12_15:
	bar.sync 	0;
	mov.b32 	%r117, %f329;
	mov.u32 	%r118, 31;
	mov.u32 	%r119, 16;
	mov.u32 	%r120, -1;
	shfl.sync.bfly.b32 	%r121|%p30, %r117, %r119, %r118, %r120;
	mov.b32 	%f248, %r121;
	add.f32 	%f249, %f329, %f248;
	mov.b32 	%r122, %f249;
	mov.u32 	%r123, 8;
	shfl.sync.bfly.b32 	%r124|%p31, %r122, %r123, %r118, %r120;
	mov.b32 	%f250, %r124;
	add.f32 	%f251, %f249, %f250;
	mov.b32 	%r125, %f251;
	mov.u32 	%r126, 4;
	shfl.sync.bfly.b32 	%r127|%p32, %r125, %r126, %r118, %r120;
	mov.b32 	%f252, %r127;
	add.f32 	%f253, %f251, %f252;
	mov.b32 	%r128, %f253;
	mov.u32 	%r129, 2;
	shfl.sync.bfly.b32 	%r130|%p33, %r128, %r129, %r118, %r120;
	mov.b32 	%f254, %r130;
	add.f32 	%f255, %f253, %f254;
	mov.b32 	%r131, %f255;
	mov.u32 	%r132, 1;
	shfl.sync.bfly.b32 	%r133|%p34, %r131, %r132, %r118, %r120;
	mov.b32 	%f256, %r133;
	add.f32 	%f257, %f255, %f256;
	st.local.f32 	[%rd3+8], %f257;
	st.shared.f32 	[%r14], %f257;
	bar.sync 	0;
	@%p1 bra 	$L__BB12_17;

	ld.shared.f32 	%f258, [%r4];
	mov.b32 	%r134, %f258;
	shfl.sync.bfly.b32 	%r138|%p35, %r134, %r119, %r118, %r120;
	mov.b32 	%f259, %r138;
	add.f32 	%f260, %f258, %f259;
	mov.b32 	%r139, %f260;
	shfl.sync.bfly.b32 	%r141|%p36, %r139, %r123, %r118, %r120;
	mov.b32 	%f261, %r141;
	add.f32 	%f262, %f260, %f261;
	mov.b32 	%r142, %f262;
	shfl.sync.bfly.b32 	%r144|%p37, %r142, %r126, %r118, %r120;
	mov.b32 	%f263, %r144;
	add.f32 	%f264, %f262, %f263;
	mov.b32 	%r145, %f264;
	shfl.sync.bfly.b32 	%r147|%p38, %r145, %r129, %r118, %r120;
	mov.b32 	%f265, %r147;
	add.f32 	%f266, %f264, %f265;
	mov.b32 	%r148, %f266;
	shfl.sync.bfly.b32 	%r150|%p39, %r148, %r132, %r118, %r120;
	mov.b32 	%f267, %r150;
	add.f32 	%f268, %f266, %f267;
	st.local.f32 	[%rd3+8], %f268;

$L__BB12_17:
	bar.sync 	0;
	mov.b32 	%r151, %f328;
	shfl.sync.bfly.b32 	%r155|%p41, %r151, %r119, %r118, %r120;
	mov.b32 	%f269, %r155;
	add.f32 	%f270, %f328, %f269;
	mov.b32 	%r156, %f270;
	shfl.sync.bfly.b32 	%r158|%p42, %r156, %r123, %r118, %r120;
	mov.b32 	%f271, %r158;
	add.f32 	%f272, %f270, %f271;
	mov.b32 	%r159, %f272;
	shfl.sync.bfly.b32 	%r161|%p43, %r159, %r126, %r118, %r120;
	mov.b32 	%f273, %r161;
	add.f32 	%f274, %f272, %f273;
	mov.b32 	%r162, %f274;
	shfl.sync.bfly.b32 	%r164|%p44, %r162, %r129, %r118, %r120;
	mov.b32 	%f275, %r164;
	add.f32 	%f276, %f274, %f275;
	mov.b32 	%r165, %f276;
	shfl.sync.bfly.b32 	%r167|%p45, %r165, %r132, %r118, %r120;
	mov.b32 	%f277, %r167;
	add.f32 	%f278, %f276, %f277;
	st.local.f32 	[%rd3+12], %f278;
	st.shared.f32 	[%r14], %f278;
	bar.sync 	0;
	@%p1 bra 	$L__BB12_19;

	ld.shared.f32 	%f279, [%r4];
	mov.b32 	%r168, %f279;
	mov.u32 	%r169, 31;
	mov.u32 	%r170, 16;
	mov.u32 	%r171, -1;
	shfl.sync.bfly.b32 	%r172|%p46, %r168, %r170, %r169, %r171;
	mov.b32 	%f280, %r172;
	add.f32 	%f281, %f279, %f280;
	mov.b32 	%r173, %f281;
	mov.u32 	%r174, 8;
	shfl.sync.bfly.b32 	%r175|%p47, %r173, %r174, %r169, %r171;
	mov.b32 	%f282, %r175;
	add.f32 	%f283, %f281, %f282;
	mov.b32 	%r176, %f283;
	mov.u32 	%r177, 4;
	shfl.sync.bfly.b32 	%r178|%p48, %r176, %r177, %r169, %r171;
	mov.b32 	%f284, %r178;
	add.f32 	%f285, %f283, %f284;
	mov.b32 	%r179, %f285;
	mov.u32 	%r180, 2;
	shfl.sync.bfly.b32 	%r181|%p49, %r179, %r180, %r169, %r171;
	mov.b32 	%f286, %r181;
	add.f32 	%f287, %f285, %f286;
	mov.b32 	%r182, %f287;
	mov.u32 	%r183, 1;
	shfl.sync.bfly.b32 	%r184|%p50, %r182, %r183, %r169, %r171;
	mov.b32 	%f288, %r184;
	add.f32 	%f289, %f287, %f288;
	st.local.f32 	[%rd3+12], %f289;

$L__BB12_19:
	bar.sync 	0;
	mov.b32 	%r185, %f327;
	mov.u32 	%r186, 31;
	mov.u32 	%r187, 16;
	mov.u32 	%r188, -1;
	shfl.sync.bfly.b32 	%r189|%p52, %r185, %r187, %r186, %r188;
	mov.b32 	%f290, %r189;
	add.f32 	%f291, %f327, %f290;
	mov.b32 	%r190, %f291;
	mov.u32 	%r191, 8;
	shfl.sync.bfly.b32 	%r192|%p53, %r190, %r191, %r186, %r188;
	mov.b32 	%f292, %r192;
	add.f32 	%f293, %f291, %f292;
	mov.b32 	%r193, %f293;
	mov.u32 	%r194, 4;
	shfl.sync.bfly.b32 	%r195|%p54, %r193, %r194, %r186, %r188;
	mov.b32 	%f294, %r195;
	add.f32 	%f295, %f293, %f294;
	mov.b32 	%r196, %f295;
	mov.u32 	%r197, 2;
	shfl.sync.bfly.b32 	%r198|%p55, %r196, %r197, %r186, %r188;
	mov.b32 	%f296, %r198;
	add.f32 	%f297, %f295, %f296;
	mov.b32 	%r199, %f297;
	mov.u32 	%r200, 1;
	shfl.sync.bfly.b32 	%r201|%p56, %r199, %r200, %r186, %r188;
	mov.b32 	%f298, %r201;
	add.f32 	%f299, %f297, %f298;
	st.local.f32 	[%rd3+16], %f299;
	st.shared.f32 	[%r14], %f299;
	bar.sync 	0;
	@%p1 bra 	$L__BB12_21;

	ld.shared.f32 	%f300, [%r4];
	mov.b32 	%r202, %f300;
	shfl.sync.bfly.b32 	%r206|%p57, %r202, %r187, %r186, %r188;
	mov.b32 	%f301, %r206;
	add.f32 	%f302, %f300, %f301;
	mov.b32 	%r207, %f302;
	shfl.sync.bfly.b32 	%r209|%p58, %r207, %r191, %r186, %r188;
	mov.b32 	%f303, %r209;
	add.f32 	%f304, %f302, %f303;
	mov.b32 	%r210, %f304;
	shfl.sync.bfly.b32 	%r212|%p59, %r210, %r194, %r186, %r188;
	mov.b32 	%f305, %r212;
	add.f32 	%f306, %f304, %f305;
	mov.b32 	%r213, %f306;
	shfl.sync.bfly.b32 	%r215|%p60, %r213, %r197, %r186, %r188;
	mov.b32 	%f307, %r215;
	add.f32 	%f308, %f306, %f307;
	mov.b32 	%r216, %f308;
	shfl.sync.bfly.b32 	%r218|%p61, %r216, %r200, %r186, %r188;
	mov.b32 	%f309, %r218;
	add.f32 	%f310, %f308, %f309;
	st.local.f32 	[%rd3+16], %f310;

$L__BB12_21:
	bar.sync 	0;
	setp.gt.s32 	%p62, %r3, 4;
	@%p62 bra 	$L__BB12_23;

	mul.wide.s32 	%rd62, %r3, 4;
	add.s64 	%rd63, %rd3, %rd62;
	ld.local.f32 	%f311, [%rd63];
	mad.lo.s32 	%r219, %r3, %r17, %r2;
	cvt.s64.s32 	%rd64, %r219;
	mul.lo.s32 	%r220, %r1, %r18;
	cvt.s64.s32 	%rd65, %r220;
	add.s64 	%rd66, %rd65, %rd64;
	cvta.to.global.u64 	%rd67, %rd27;
	shl.b64 	%rd68, %rd66, 2;
	add.s64 	%rd69, %rd67, %rd68;
	st.global.f32 	[%rd69], %f311;

$L__BB12_23:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_6_bs_64
.visible .entry ggml_matvec_f32_ncols_6_bs_64(
	.param .u64 ggml_matvec_f32_ncols_6_bs_64_param_0,
	.param .u64 ggml_matvec_f32_ncols_6_bs_64_param_1,
	.param .u64 ggml_matvec_f32_ncols_6_bs_64_param_2,
	.param .u32 ggml_matvec_f32_ncols_6_bs_64_param_3,
	.param .u32 ggml_matvec_f32_ncols_6_bs_64_param_4,
	.param .u32 ggml_matvec_f32_ncols_6_bs_64_param_5,
	.param .u32 ggml_matvec_f32_ncols_6_bs_64_param_6,
	.param .u32 ggml_matvec_f32_ncols_6_bs_64_param_7,
	.param .u32 ggml_matvec_f32_ncols_6_bs_64_param_8,
	.param .u32 ggml_matvec_f32_ncols_6_bs_64_param_9,
	.param .u32 ggml_matvec_f32_ncols_6_bs_64_param_10,
	.param .u32 ggml_matvec_f32_ncols_6_bs_64_param_11
)
{
	.local .align 8 .b8 	__local_depot13[24];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<73>;
	.reg .f32 	%f<296>;
	.reg .b32 	%r<253>;
	.reg .b64 	%rd<67>;


	mov.u64 	%SPL, __local_depot13;
	ld.param.u64 	%rd20, [ggml_matvec_f32_ncols_6_bs_64_param_0];
	ld.param.u64 	%rd21, [ggml_matvec_f32_ncols_6_bs_64_param_1];
	ld.param.u64 	%rd19, [ggml_matvec_f32_ncols_6_bs_64_param_2];
	ld.param.u32 	%r11, [ggml_matvec_f32_ncols_6_bs_64_param_3];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_6_bs_64_param_5];
	ld.param.u32 	%r12, [ggml_matvec_f32_ncols_6_bs_64_param_6];
	ld.param.u32 	%r13, [ggml_matvec_f32_ncols_6_bs_64_param_7];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_6_bs_64_param_8];
	ld.param.u32 	%r17, [ggml_matvec_f32_ncols_6_bs_64_param_9];
	ld.param.u32 	%r18, [ggml_matvec_f32_ncols_6_bs_64_param_10];
	ld.param.u32 	%r14, [ggml_matvec_f32_ncols_6_bs_64_param_11];
	cvta.to.global.u64 	%rd66, %rd21;
	add.u64 	%rd2, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r19, %r1, %r16;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r20, %r2, %r15;
	mad.lo.s32 	%r21, %r19, %r17, %r20;
	cvt.s64.s32 	%rd3, %r21;
	cvta.to.global.u64 	%rd4, %rd20;
	mul.lo.s32 	%r22, %r1, %r18;
	cvt.s64.s32 	%rd5, %r22;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r23, %r3, 2;
	mov.u32 	%r24, data_mmv;
	add.s32 	%r4, %r24, %r23;
	@%p1 bra 	$L__BB13_2;

	mov.u32 	%r25, 0;
	st.shared.u32 	[%r4], %r25;

$L__BB13_2:
	bar.sync 	0;
	mov.f32 	%f290, 0f00000000;
	st.local.v2.f32 	[%rd2], {%f290, %f290};
	st.local.v2.f32 	[%rd2+8], {%f290, %f290};
	st.local.v2.f32 	[%rd2+16], {%f290, %f290};
	setp.ge.s32 	%p2, %r3, %r11;
	mov.f32 	%f291, %f290;
	mov.f32 	%f292, %f290;
	mov.f32 	%f293, %f290;
	mov.f32 	%f294, %f290;
	mov.f32 	%f295, %f290;
	@%p2 bra 	$L__BB13_9;

	not.b32 	%r26, %r3;
	add.s32 	%r5, %r26, %r11;
	and.b32  	%r27, %r5, 64;
	setp.ne.s32 	%p3, %r27, 0;
	mov.f32 	%f290, 0f00000000;
	mov.u32 	%r252, %r3;
	@%p3 bra 	$L__BB13_5;

	shl.b64 	%rd23, %rd5, 2;
	add.s64 	%rd24, %rd66, %rd23;
	shl.b64 	%rd25, %rd3, 2;
	add.s64 	%rd26, %rd4, %rd25;
	mul.wide.s32 	%rd27, %r3, 8;
	add.s64 	%rd28, %rd26, %rd27;
	ld.global.nc.v2.f32 	{%f43, %f44}, [%rd28];
	add.s64 	%rd29, %rd24, %rd27;
	ld.global.nc.v2.f32 	{%f47, %f48}, [%rd29];
	fma.rn.f32 	%f51, %f43, %f47, 0f00000000;
	fma.rn.f32 	%f295, %f44, %f48, %f51;
	mul.wide.s32 	%rd30, %r12, 8;
	add.s64 	%rd31, %rd29, %rd30;
	ld.global.nc.v2.f32 	{%f52, %f53}, [%rd31];
	fma.rn.f32 	%f56, %f43, %f52, 0f00000000;
	fma.rn.f32 	%f294, %f44, %f53, %f56;
	st.local.v2.f32 	[%rd2], {%f295, %f294};
	add.s32 	%r28, %r3, %r12;
	add.s32 	%r29, %r28, %r12;
	mul.wide.s32 	%rd32, %r29, 8;
	add.s64 	%rd33, %rd24, %rd32;
	ld.global.nc.v2.f32 	{%f57, %f58}, [%rd33];
	fma.rn.f32 	%f61, %f43, %f57, 0f00000000;
	fma.rn.f32 	%f293, %f44, %f58, %f61;
	add.s64 	%rd34, %rd33, %rd30;
	ld.global.nc.v2.f32 	{%f62, %f63}, [%rd34];
	fma.rn.f32 	%f66, %f43, %f62, 0f00000000;
	fma.rn.f32 	%f292, %f44, %f63, %f66;
	st.local.v2.f32 	[%rd2+8], {%f293, %f292};
	add.s64 	%rd35, %rd34, %rd30;
	ld.global.nc.v2.f32 	{%f67, %f68}, [%rd35];
	fma.rn.f32 	%f71, %f43, %f67, 0f00000000;
	fma.rn.f32 	%f291, %f44, %f68, %f71;
	add.s64 	%rd36, %rd35, %rd30;
	ld.global.nc.v2.f32 	{%f72, %f73}, [%rd36];
	fma.rn.f32 	%f76, %f43, %f72, 0f00000000;
	fma.rn.f32 	%f290, %f44, %f73, %f76;
	st.local.v2.f32 	[%rd2+16], {%f291, %f290};
	add.s32 	%r252, %r3, 64;

$L__BB13_5:
	and.b32  	%r30, %r5, -64;
	setp.eq.s32 	%p4, %r30, 0;
	@%p4 bra 	$L__BB13_9;

	add.s32 	%r31, %r252, %r12;
	add.s32 	%r32, %r31, 64;
	mul.wide.s32 	%rd37, %r32, 8;
	shl.b64 	%rd38, %rd5, 2;
	add.s64 	%rd7, %rd37, %rd38;
	shl.b32 	%r33, %r12, 1;
	add.s32 	%r34, %r252, %r33;
	mad.lo.s32 	%r35, %r12, 3, %r252;
	shl.b32 	%r36, %r12, 2;
	add.s32 	%r37, %r252, %r36;
	mad.lo.s32 	%r38, %r12, 5, %r252;
	mul.wide.s32 	%rd39, %r34, 8;
	add.s64 	%rd8, %rd39, %rd38;
	mul.wide.s32 	%rd40, %r35, 8;
	add.s64 	%rd9, %rd40, %rd38;
	mul.wide.s32 	%rd41, %r37, 8;
	add.s64 	%rd10, %rd41, %rd38;
	mul.wide.s32 	%rd42, %r38, 8;
	add.s64 	%rd11, %rd42, %rd38;
	mul.wide.s32 	%rd43, %r252, 2;
	add.s64 	%rd44, %rd43, %rd3;
	shl.b64 	%rd45, %rd44, 2;
	add.s64 	%rd46, %rd4, %rd45;
	add.s64 	%rd65, %rd46, 512;
	mul.wide.s32 	%rd47, %r252, 8;
	mul.wide.s32 	%rd48, %r12, 8;
	add.s64 	%rd49, %rd47, %rd48;
	add.s64 	%rd13, %rd49, %rd38;
	add.s64 	%rd14, %rd47, %rd38;

$L__BB13_7:
	ld.global.nc.v2.f32 	{%f77, %f78}, [%rd65+-512];
	add.s64 	%rd50, %rd66, %rd14;
	ld.global.nc.v2.f32 	{%f81, %f82}, [%rd50];
	fma.rn.f32 	%f85, %f77, %f81, %f295;
	fma.rn.f32 	%f86, %f78, %f82, %f85;
	add.s64 	%rd51, %rd66, %rd13;
	ld.global.nc.v2.f32 	{%f87, %f88}, [%rd51];
	fma.rn.f32 	%f91, %f77, %f87, %f294;
	fma.rn.f32 	%f92, %f78, %f88, %f91;
	add.s64 	%rd52, %rd66, %rd8;
	ld.global.nc.v2.f32 	{%f93, %f94}, [%rd52];
	fma.rn.f32 	%f97, %f77, %f93, %f293;
	fma.rn.f32 	%f98, %f78, %f94, %f97;
	add.s64 	%rd53, %rd66, %rd9;
	ld.global.nc.v2.f32 	{%f99, %f100}, [%rd53];
	fma.rn.f32 	%f103, %f77, %f99, %f292;
	fma.rn.f32 	%f104, %f78, %f100, %f103;
	add.s64 	%rd54, %rd66, %rd10;
	ld.global.nc.v2.f32 	{%f105, %f106}, [%rd54];
	fma.rn.f32 	%f109, %f77, %f105, %f291;
	fma.rn.f32 	%f110, %f78, %f106, %f109;
	add.s64 	%rd55, %rd66, %rd11;
	ld.global.nc.v2.f32 	{%f111, %f112}, [%rd55];
	fma.rn.f32 	%f115, %f77, %f111, %f290;
	fma.rn.f32 	%f116, %f78, %f112, %f115;
	ld.global.nc.v2.f32 	{%f117, %f118}, [%rd65];
	ld.global.nc.v2.f32 	{%f121, %f122}, [%rd50+512];
	fma.rn.f32 	%f125, %f117, %f121, %f86;
	fma.rn.f32 	%f295, %f118, %f122, %f125;
	add.s64 	%rd56, %rd66, %rd7;
	ld.global.nc.v2.f32 	{%f126, %f127}, [%rd56];
	fma.rn.f32 	%f130, %f117, %f126, %f92;
	fma.rn.f32 	%f294, %f118, %f127, %f130;
	ld.global.nc.v2.f32 	{%f131, %f132}, [%rd52+512];
	fma.rn.f32 	%f135, %f117, %f131, %f98;
	fma.rn.f32 	%f293, %f118, %f132, %f135;
	ld.global.nc.v2.f32 	{%f136, %f137}, [%rd53+512];
	fma.rn.f32 	%f140, %f117, %f136, %f104;
	fma.rn.f32 	%f292, %f118, %f137, %f140;
	ld.global.nc.v2.f32 	{%f141, %f142}, [%rd54+512];
	fma.rn.f32 	%f145, %f117, %f141, %f110;
	fma.rn.f32 	%f291, %f118, %f142, %f145;
	ld.global.nc.v2.f32 	{%f146, %f147}, [%rd55+512];
	fma.rn.f32 	%f150, %f117, %f146, %f116;
	fma.rn.f32 	%f290, %f118, %f147, %f150;
	add.s64 	%rd66, %rd66, 1024;
	add.s64 	%rd65, %rd65, 1024;
	add.s32 	%r252, %r252, 128;
	setp.lt.s32 	%p5, %r252, %r11;
	@%p5 bra 	$L__BB13_7;

	st.local.v2.f32 	[%rd2], {%f295, %f294};
	st.local.v2.f32 	[%rd2+8], {%f293, %f292};
	st.local.v2.f32 	[%rd2+16], {%f291, %f290};

$L__BB13_9:
	shr.s32 	%r39, %r3, 31;
	shr.u32 	%r40, %r39, 27;
	add.s32 	%r41, %r3, %r40;
	shr.s32 	%r42, %r41, 5;
	shl.b32 	%r43, %r42, 2;
	add.s32 	%r10, %r24, %r43;
	mov.u32 	%r45, 2;
	mov.b32 	%r46, %f295;
	mov.u32 	%r47, 31;
	mov.u32 	%r48, 16;
	mov.u32 	%r49, -1;
	shfl.sync.bfly.b32 	%r50|%p6, %r46, %r48, %r47, %r49;
	mov.b32 	%f151, %r50;
	add.f32 	%f152, %f295, %f151;
	mov.b32 	%r51, %f152;
	mov.u32 	%r52, 8;
	shfl.sync.bfly.b32 	%r53|%p7, %r51, %r52, %r47, %r49;
	mov.b32 	%f153, %r53;
	add.f32 	%f154, %f152, %f153;
	mov.b32 	%r54, %f154;
	mov.u32 	%r55, 4;
	shfl.sync.bfly.b32 	%r56|%p8, %r54, %r55, %r47, %r49;
	mov.b32 	%f155, %r56;
	add.f32 	%f156, %f154, %f155;
	mov.b32 	%r57, %f156;
	shfl.sync.bfly.b32 	%r58|%p9, %r57, %r45, %r47, %r49;
	mov.b32 	%f157, %r58;
	add.f32 	%f158, %f156, %f157;
	mov.b32 	%r59, %f158;
	mov.u32 	%r60, 1;
	shfl.sync.bfly.b32 	%r61|%p10, %r59, %r60, %r47, %r49;
	mov.b32 	%f159, %r61;
	add.f32 	%f160, %f158, %f159;
	st.local.f32 	[%rd2], %f160;
	st.shared.f32 	[%r10], %f160;
	bar.sync 	0;
	@%p1 bra 	$L__BB13_11;

	ld.shared.f32 	%f161, [%r4];
	mov.b32 	%r62, %f161;
	shfl.sync.bfly.b32 	%r66|%p12, %r62, %r48, %r47, %r49;
	mov.b32 	%f162, %r66;
	add.f32 	%f163, %f161, %f162;
	mov.b32 	%r67, %f163;
	shfl.sync.bfly.b32 	%r69|%p13, %r67, %r52, %r47, %r49;
	mov.b32 	%f164, %r69;
	add.f32 	%f165, %f163, %f164;
	mov.b32 	%r70, %f165;
	shfl.sync.bfly.b32 	%r72|%p14, %r70, %r55, %r47, %r49;
	mov.b32 	%f166, %r72;
	add.f32 	%f167, %f165, %f166;
	mov.b32 	%r73, %f167;
	shfl.sync.bfly.b32 	%r75|%p15, %r73, %r45, %r47, %r49;
	mov.b32 	%f168, %r75;
	add.f32 	%f169, %f167, %f168;
	mov.b32 	%r76, %f169;
	shfl.sync.bfly.b32 	%r78|%p16, %r76, %r60, %r47, %r49;
	mov.b32 	%f170, %r78;
	add.f32 	%f171, %f169, %f170;
	st.local.f32 	[%rd2], %f171;

$L__BB13_11:
	bar.sync 	0;
	mov.b32 	%r79, %f294;
	shfl.sync.bfly.b32 	%r83|%p18, %r79, %r48, %r47, %r49;
	mov.b32 	%f172, %r83;
	add.f32 	%f173, %f294, %f172;
	mov.b32 	%r84, %f173;
	shfl.sync.bfly.b32 	%r86|%p19, %r84, %r52, %r47, %r49;
	mov.b32 	%f174, %r86;
	add.f32 	%f175, %f173, %f174;
	mov.b32 	%r87, %f175;
	shfl.sync.bfly.b32 	%r89|%p20, %r87, %r55, %r47, %r49;
	mov.b32 	%f176, %r89;
	add.f32 	%f177, %f175, %f176;
	mov.b32 	%r90, %f177;
	shfl.sync.bfly.b32 	%r92|%p21, %r90, %r45, %r47, %r49;
	mov.b32 	%f178, %r92;
	add.f32 	%f179, %f177, %f178;
	mov.b32 	%r93, %f179;
	shfl.sync.bfly.b32 	%r95|%p22, %r93, %r60, %r47, %r49;
	mov.b32 	%f180, %r95;
	add.f32 	%f181, %f179, %f180;
	st.local.f32 	[%rd2+4], %f181;
	st.shared.f32 	[%r10], %f181;
	bar.sync 	0;
	@%p1 bra 	$L__BB13_13;

	ld.shared.f32 	%f182, [%r4];
	mov.b32 	%r96, %f182;
	mov.u32 	%r97, 31;
	mov.u32 	%r98, 16;
	mov.u32 	%r99, -1;
	shfl.sync.bfly.b32 	%r100|%p23, %r96, %r98, %r97, %r99;
	mov.b32 	%f183, %r100;
	add.f32 	%f184, %f182, %f183;
	mov.b32 	%r101, %f184;
	mov.u32 	%r102, 8;
	shfl.sync.bfly.b32 	%r103|%p24, %r101, %r102, %r97, %r99;
	mov.b32 	%f185, %r103;
	add.f32 	%f186, %f184, %f185;
	mov.b32 	%r104, %f186;
	mov.u32 	%r105, 4;
	shfl.sync.bfly.b32 	%r106|%p25, %r104, %r105, %r97, %r99;
	mov.b32 	%f187, %r106;
	add.f32 	%f188, %f186, %f187;
	mov.b32 	%r107, %f188;
	mov.u32 	%r108, 2;
	shfl.sync.bfly.b32 	%r109|%p26, %r107, %r108, %r97, %r99;
	mov.b32 	%f189, %r109;
	add.f32 	%f190, %f188, %f189;
	mov.b32 	%r110, %f190;
	mov.u32 	%r111, 1;
	shfl.sync.bfly.b32 	%r112|%p27, %r110, %r111, %r97, %r99;
	mov.b32 	%f191, %r112;
	add.f32 	%f192, %f190, %f191;
	st.local.f32 	[%rd2+4], %f192;

$L__BB13_13:
	bar.sync 	0;
	mov.b32 	%r113, %f293;
	mov.u32 	%r114, 31;
	mov.u32 	%r115, 16;
	mov.u32 	%r116, -1;
	shfl.sync.bfly.b32 	%r117|%p29, %r113, %r115, %r114, %r116;
	mov.b32 	%f193, %r117;
	add.f32 	%f194, %f293, %f193;
	mov.b32 	%r118, %f194;
	mov.u32 	%r119, 8;
	shfl.sync.bfly.b32 	%r120|%p30, %r118, %r119, %r114, %r116;
	mov.b32 	%f195, %r120;
	add.f32 	%f196, %f194, %f195;
	mov.b32 	%r121, %f196;
	mov.u32 	%r122, 4;
	shfl.sync.bfly.b32 	%r123|%p31, %r121, %r122, %r114, %r116;
	mov.b32 	%f197, %r123;
	add.f32 	%f198, %f196, %f197;
	mov.b32 	%r124, %f198;
	mov.u32 	%r125, 2;
	shfl.sync.bfly.b32 	%r126|%p32, %r124, %r125, %r114, %r116;
	mov.b32 	%f199, %r126;
	add.f32 	%f200, %f198, %f199;
	mov.b32 	%r127, %f200;
	mov.u32 	%r128, 1;
	shfl.sync.bfly.b32 	%r129|%p33, %r127, %r128, %r114, %r116;
	mov.b32 	%f201, %r129;
	add.f32 	%f202, %f200, %f201;
	st.local.f32 	[%rd2+8], %f202;
	st.shared.f32 	[%r10], %f202;
	bar.sync 	0;
	@%p1 bra 	$L__BB13_15;

	ld.shared.f32 	%f203, [%r4];
	mov.b32 	%r130, %f203;
	shfl.sync.bfly.b32 	%r134|%p34, %r130, %r115, %r114, %r116;
	mov.b32 	%f204, %r134;
	add.f32 	%f205, %f203, %f204;
	mov.b32 	%r135, %f205;
	shfl.sync.bfly.b32 	%r137|%p35, %r135, %r119, %r114, %r116;
	mov.b32 	%f206, %r137;
	add.f32 	%f207, %f205, %f206;
	mov.b32 	%r138, %f207;
	shfl.sync.bfly.b32 	%r140|%p36, %r138, %r122, %r114, %r116;
	mov.b32 	%f208, %r140;
	add.f32 	%f209, %f207, %f208;
	mov.b32 	%r141, %f209;
	shfl.sync.bfly.b32 	%r143|%p37, %r141, %r125, %r114, %r116;
	mov.b32 	%f210, %r143;
	add.f32 	%f211, %f209, %f210;
	mov.b32 	%r144, %f211;
	shfl.sync.bfly.b32 	%r146|%p38, %r144, %r128, %r114, %r116;
	mov.b32 	%f212, %r146;
	add.f32 	%f213, %f211, %f212;
	st.local.f32 	[%rd2+8], %f213;

$L__BB13_15:
	bar.sync 	0;
	mov.b32 	%r147, %f292;
	shfl.sync.bfly.b32 	%r151|%p40, %r147, %r115, %r114, %r116;
	mov.b32 	%f214, %r151;
	add.f32 	%f215, %f292, %f214;
	mov.b32 	%r152, %f215;
	shfl.sync.bfly.b32 	%r154|%p41, %r152, %r119, %r114, %r116;
	mov.b32 	%f216, %r154;
	add.f32 	%f217, %f215, %f216;
	mov.b32 	%r155, %f217;
	shfl.sync.bfly.b32 	%r157|%p42, %r155, %r122, %r114, %r116;
	mov.b32 	%f218, %r157;
	add.f32 	%f219, %f217, %f218;
	mov.b32 	%r158, %f219;
	shfl.sync.bfly.b32 	%r160|%p43, %r158, %r125, %r114, %r116;
	mov.b32 	%f220, %r160;
	add.f32 	%f221, %f219, %f220;
	mov.b32 	%r161, %f221;
	shfl.sync.bfly.b32 	%r163|%p44, %r161, %r128, %r114, %r116;
	mov.b32 	%f222, %r163;
	add.f32 	%f223, %f221, %f222;
	st.local.f32 	[%rd2+12], %f223;
	st.shared.f32 	[%r10], %f223;
	bar.sync 	0;
	@%p1 bra 	$L__BB13_17;

	ld.shared.f32 	%f224, [%r4];
	mov.b32 	%r164, %f224;
	mov.u32 	%r165, 31;
	mov.u32 	%r166, 16;
	mov.u32 	%r167, -1;
	shfl.sync.bfly.b32 	%r168|%p45, %r164, %r166, %r165, %r167;
	mov.b32 	%f225, %r168;
	add.f32 	%f226, %f224, %f225;
	mov.b32 	%r169, %f226;
	mov.u32 	%r170, 8;
	shfl.sync.bfly.b32 	%r171|%p46, %r169, %r170, %r165, %r167;
	mov.b32 	%f227, %r171;
	add.f32 	%f228, %f226, %f227;
	mov.b32 	%r172, %f228;
	mov.u32 	%r173, 4;
	shfl.sync.bfly.b32 	%r174|%p47, %r172, %r173, %r165, %r167;
	mov.b32 	%f229, %r174;
	add.f32 	%f230, %f228, %f229;
	mov.b32 	%r175, %f230;
	mov.u32 	%r176, 2;
	shfl.sync.bfly.b32 	%r177|%p48, %r175, %r176, %r165, %r167;
	mov.b32 	%f231, %r177;
	add.f32 	%f232, %f230, %f231;
	mov.b32 	%r178, %f232;
	mov.u32 	%r179, 1;
	shfl.sync.bfly.b32 	%r180|%p49, %r178, %r179, %r165, %r167;
	mov.b32 	%f233, %r180;
	add.f32 	%f234, %f232, %f233;
	st.local.f32 	[%rd2+12], %f234;

$L__BB13_17:
	bar.sync 	0;
	mov.b32 	%r181, %f291;
	mov.u32 	%r182, 31;
	mov.u32 	%r183, 16;
	mov.u32 	%r184, -1;
	shfl.sync.bfly.b32 	%r185|%p51, %r181, %r183, %r182, %r184;
	mov.b32 	%f235, %r185;
	add.f32 	%f236, %f291, %f235;
	mov.b32 	%r186, %f236;
	mov.u32 	%r187, 8;
	shfl.sync.bfly.b32 	%r188|%p52, %r186, %r187, %r182, %r184;
	mov.b32 	%f237, %r188;
	add.f32 	%f238, %f236, %f237;
	mov.b32 	%r189, %f238;
	mov.u32 	%r190, 4;
	shfl.sync.bfly.b32 	%r191|%p53, %r189, %r190, %r182, %r184;
	mov.b32 	%f239, %r191;
	add.f32 	%f240, %f238, %f239;
	mov.b32 	%r192, %f240;
	mov.u32 	%r193, 2;
	shfl.sync.bfly.b32 	%r194|%p54, %r192, %r193, %r182, %r184;
	mov.b32 	%f241, %r194;
	add.f32 	%f242, %f240, %f241;
	mov.b32 	%r195, %f242;
	mov.u32 	%r196, 1;
	shfl.sync.bfly.b32 	%r197|%p55, %r195, %r196, %r182, %r184;
	mov.b32 	%f243, %r197;
	add.f32 	%f244, %f242, %f243;
	st.local.f32 	[%rd2+16], %f244;
	st.shared.f32 	[%r10], %f244;
	bar.sync 	0;
	@%p1 bra 	$L__BB13_19;

	ld.shared.f32 	%f245, [%r4];
	mov.b32 	%r198, %f245;
	shfl.sync.bfly.b32 	%r202|%p56, %r198, %r183, %r182, %r184;
	mov.b32 	%f246, %r202;
	add.f32 	%f247, %f245, %f246;
	mov.b32 	%r203, %f247;
	shfl.sync.bfly.b32 	%r205|%p57, %r203, %r187, %r182, %r184;
	mov.b32 	%f248, %r205;
	add.f32 	%f249, %f247, %f248;
	mov.b32 	%r206, %f249;
	shfl.sync.bfly.b32 	%r208|%p58, %r206, %r190, %r182, %r184;
	mov.b32 	%f250, %r208;
	add.f32 	%f251, %f249, %f250;
	mov.b32 	%r209, %f251;
	shfl.sync.bfly.b32 	%r211|%p59, %r209, %r193, %r182, %r184;
	mov.b32 	%f252, %r211;
	add.f32 	%f253, %f251, %f252;
	mov.b32 	%r212, %f253;
	shfl.sync.bfly.b32 	%r214|%p60, %r212, %r196, %r182, %r184;
	mov.b32 	%f254, %r214;
	add.f32 	%f255, %f253, %f254;
	st.local.f32 	[%rd2+16], %f255;

$L__BB13_19:
	bar.sync 	0;
	mov.b32 	%r215, %f290;
	shfl.sync.bfly.b32 	%r219|%p62, %r215, %r183, %r182, %r184;
	mov.b32 	%f256, %r219;
	add.f32 	%f257, %f290, %f256;
	mov.b32 	%r220, %f257;
	shfl.sync.bfly.b32 	%r222|%p63, %r220, %r187, %r182, %r184;
	mov.b32 	%f258, %r222;
	add.f32 	%f259, %f257, %f258;
	mov.b32 	%r223, %f259;
	shfl.sync.bfly.b32 	%r225|%p64, %r223, %r190, %r182, %r184;
	mov.b32 	%f260, %r225;
	add.f32 	%f261, %f259, %f260;
	mov.b32 	%r226, %f261;
	shfl.sync.bfly.b32 	%r228|%p65, %r226, %r193, %r182, %r184;
	mov.b32 	%f262, %r228;
	add.f32 	%f263, %f261, %f262;
	mov.b32 	%r229, %f263;
	shfl.sync.bfly.b32 	%r231|%p66, %r229, %r196, %r182, %r184;
	mov.b32 	%f264, %r231;
	add.f32 	%f265, %f263, %f264;
	st.local.f32 	[%rd2+20], %f265;
	st.shared.f32 	[%r10], %f265;
	bar.sync 	0;
	@%p1 bra 	$L__BB13_21;

	ld.shared.f32 	%f266, [%r4];
	mov.b32 	%r232, %f266;
	mov.u32 	%r233, 31;
	mov.u32 	%r234, 16;
	mov.u32 	%r235, -1;
	shfl.sync.bfly.b32 	%r236|%p67, %r232, %r234, %r233, %r235;
	mov.b32 	%f267, %r236;
	add.f32 	%f268, %f266, %f267;
	mov.b32 	%r237, %f268;
	mov.u32 	%r238, 8;
	shfl.sync.bfly.b32 	%r239|%p68, %r237, %r238, %r233, %r235;
	mov.b32 	%f269, %r239;
	add.f32 	%f270, %f268, %f269;
	mov.b32 	%r240, %f270;
	mov.u32 	%r241, 4;
	shfl.sync.bfly.b32 	%r242|%p69, %r240, %r241, %r233, %r235;
	mov.b32 	%f271, %r242;
	add.f32 	%f272, %f270, %f271;
	mov.b32 	%r243, %f272;
	mov.u32 	%r244, 2;
	shfl.sync.bfly.b32 	%r245|%p70, %r243, %r244, %r233, %r235;
	mov.b32 	%f273, %r245;
	add.f32 	%f274, %f272, %f273;
	mov.b32 	%r246, %f274;
	mov.u32 	%r247, 1;
	shfl.sync.bfly.b32 	%r248|%p71, %r246, %r247, %r233, %r235;
	mov.b32 	%f275, %r248;
	add.f32 	%f276, %f274, %f275;
	st.local.f32 	[%rd2+20], %f276;

$L__BB13_21:
	bar.sync 	0;
	setp.gt.s32 	%p72, %r3, 5;
	@%p72 bra 	$L__BB13_23;

	mul.wide.s32 	%rd57, %r3, 4;
	add.s64 	%rd58, %rd2, %rd57;
	ld.local.f32 	%f277, [%rd58];
	mad.lo.s32 	%r249, %r3, %r13, %r2;
	cvt.s64.s32 	%rd59, %r249;
	mul.lo.s32 	%r250, %r1, %r14;
	cvt.s64.s32 	%rd60, %r250;
	add.s64 	%rd61, %rd60, %rd59;
	cvta.to.global.u64 	%rd62, %rd19;
	shl.b64 	%rd63, %rd61, 2;
	add.s64 	%rd64, %rd62, %rd63;
	st.global.f32 	[%rd64], %f277;

$L__BB13_23:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_7_bs_64
.visible .entry ggml_matvec_f32_ncols_7_bs_64(
	.param .u64 ggml_matvec_f32_ncols_7_bs_64_param_0,
	.param .u64 ggml_matvec_f32_ncols_7_bs_64_param_1,
	.param .u64 ggml_matvec_f32_ncols_7_bs_64_param_2,
	.param .u32 ggml_matvec_f32_ncols_7_bs_64_param_3,
	.param .u32 ggml_matvec_f32_ncols_7_bs_64_param_4,
	.param .u32 ggml_matvec_f32_ncols_7_bs_64_param_5,
	.param .u32 ggml_matvec_f32_ncols_7_bs_64_param_6,
	.param .u32 ggml_matvec_f32_ncols_7_bs_64_param_7,
	.param .u32 ggml_matvec_f32_ncols_7_bs_64_param_8,
	.param .u32 ggml_matvec_f32_ncols_7_bs_64_param_9,
	.param .u32 ggml_matvec_f32_ncols_7_bs_64_param_10,
	.param .u32 ggml_matvec_f32_ncols_7_bs_64_param_11
)
{
	.local .align 4 .b8 	__local_depot14[28];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<84>;
	.reg .f32 	%f<343>;
	.reg .b32 	%r<289>;
	.reg .b64 	%rd<71>;


	mov.u64 	%SPL, __local_depot14;
	ld.param.u64 	%rd21, [ggml_matvec_f32_ncols_7_bs_64_param_0];
	ld.param.u64 	%rd22, [ggml_matvec_f32_ncols_7_bs_64_param_1];
	ld.param.u64 	%rd20, [ggml_matvec_f32_ncols_7_bs_64_param_2];
	ld.param.u32 	%r11, [ggml_matvec_f32_ncols_7_bs_64_param_3];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_7_bs_64_param_5];
	ld.param.u32 	%r12, [ggml_matvec_f32_ncols_7_bs_64_param_6];
	ld.param.u32 	%r13, [ggml_matvec_f32_ncols_7_bs_64_param_7];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_7_bs_64_param_8];
	ld.param.u32 	%r17, [ggml_matvec_f32_ncols_7_bs_64_param_9];
	ld.param.u32 	%r18, [ggml_matvec_f32_ncols_7_bs_64_param_10];
	ld.param.u32 	%r14, [ggml_matvec_f32_ncols_7_bs_64_param_11];
	cvta.to.global.u64 	%rd70, %rd22;
	add.u64 	%rd2, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r19, %r1, %r16;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r20, %r2, %r15;
	mad.lo.s32 	%r21, %r19, %r17, %r20;
	cvt.s64.s32 	%rd3, %r21;
	cvta.to.global.u64 	%rd4, %rd21;
	mul.lo.s32 	%r22, %r1, %r18;
	cvt.s64.s32 	%rd5, %r22;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r23, %r3, 2;
	mov.u32 	%r24, data_mmv;
	add.s32 	%r4, %r24, %r23;
	@%p1 bra 	$L__BB14_2;

	mov.u32 	%r25, 0;
	st.shared.u32 	[%r4], %r25;

$L__BB14_2:
	bar.sync 	0;
	mov.f32 	%f336, 0f00000000;
	mov.u32 	%r26, 0;
	st.local.u32 	[%rd2], %r26;
	st.local.u32 	[%rd2+4], %r26;
	st.local.u32 	[%rd2+8], %r26;
	st.local.u32 	[%rd2+12], %r26;
	st.local.u32 	[%rd2+16], %r26;
	st.local.u32 	[%rd2+20], %r26;
	st.local.u32 	[%rd2+24], %r26;
	setp.ge.s32 	%p2, %r3, %r11;
	mov.f32 	%f337, %f336;
	mov.f32 	%f338, %f336;
	mov.f32 	%f339, %f336;
	mov.f32 	%f340, %f336;
	mov.f32 	%f341, %f336;
	mov.f32 	%f342, %f336;
	@%p2 bra 	$L__BB14_9;

	not.b32 	%r27, %r3;
	add.s32 	%r5, %r27, %r11;
	and.b32  	%r28, %r5, 64;
	setp.ne.s32 	%p3, %r28, 0;
	mov.f32 	%f336, 0f00000000;
	mov.u32 	%r288, %r3;
	@%p3 bra 	$L__BB14_5;

	shl.b64 	%rd24, %rd5, 2;
	add.s64 	%rd25, %rd70, %rd24;
	shl.b64 	%rd26, %rd3, 2;
	add.s64 	%rd27, %rd4, %rd26;
	mul.wide.s32 	%rd28, %r3, 8;
	add.s64 	%rd29, %rd27, %rd28;
	ld.global.nc.v2.f32 	{%f50, %f51}, [%rd29];
	add.s64 	%rd30, %rd25, %rd28;
	ld.global.nc.v2.f32 	{%f54, %f55}, [%rd30];
	fma.rn.f32 	%f58, %f50, %f54, 0f00000000;
	fma.rn.f32 	%f342, %f51, %f55, %f58;
	st.local.f32 	[%rd2], %f342;
	mul.wide.s32 	%rd31, %r12, 8;
	add.s64 	%rd32, %rd30, %rd31;
	ld.global.nc.v2.f32 	{%f59, %f60}, [%rd32];
	fma.rn.f32 	%f63, %f50, %f59, 0f00000000;
	fma.rn.f32 	%f341, %f51, %f60, %f63;
	st.local.f32 	[%rd2+4], %f341;
	add.s32 	%r29, %r3, %r12;
	add.s32 	%r30, %r29, %r12;
	mul.wide.s32 	%rd33, %r30, 8;
	add.s64 	%rd34, %rd25, %rd33;
	ld.global.nc.v2.f32 	{%f64, %f65}, [%rd34];
	fma.rn.f32 	%f68, %f50, %f64, 0f00000000;
	fma.rn.f32 	%f340, %f51, %f65, %f68;
	st.local.f32 	[%rd2+8], %f340;
	add.s64 	%rd35, %rd34, %rd31;
	ld.global.nc.v2.f32 	{%f69, %f70}, [%rd35];
	fma.rn.f32 	%f73, %f50, %f69, 0f00000000;
	fma.rn.f32 	%f339, %f51, %f70, %f73;
	st.local.f32 	[%rd2+12], %f339;
	add.s64 	%rd36, %rd35, %rd31;
	ld.global.nc.v2.f32 	{%f74, %f75}, [%rd36];
	fma.rn.f32 	%f78, %f50, %f74, 0f00000000;
	fma.rn.f32 	%f338, %f51, %f75, %f78;
	st.local.f32 	[%rd2+16], %f338;
	add.s64 	%rd37, %rd36, %rd31;
	ld.global.nc.v2.f32 	{%f79, %f80}, [%rd37];
	fma.rn.f32 	%f83, %f50, %f79, 0f00000000;
	fma.rn.f32 	%f337, %f51, %f80, %f83;
	st.local.f32 	[%rd2+20], %f337;
	add.s64 	%rd38, %rd37, %rd31;
	ld.global.nc.v2.f32 	{%f84, %f85}, [%rd38];
	fma.rn.f32 	%f88, %f50, %f84, 0f00000000;
	fma.rn.f32 	%f336, %f51, %f85, %f88;
	st.local.f32 	[%rd2+24], %f336;
	add.s32 	%r288, %r3, 64;

$L__BB14_5:
	and.b32  	%r31, %r5, -64;
	setp.eq.s32 	%p4, %r31, 0;
	@%p4 bra 	$L__BB14_9;

	add.s32 	%r32, %r288, %r12;
	add.s32 	%r33, %r32, 64;
	mul.wide.s32 	%rd39, %r33, 8;
	shl.b64 	%rd40, %rd5, 2;
	add.s64 	%rd7, %rd39, %rd40;
	shl.b32 	%r34, %r12, 1;
	add.s32 	%r35, %r288, %r34;
	mad.lo.s32 	%r36, %r12, 3, %r288;
	shl.b32 	%r37, %r12, 2;
	add.s32 	%r38, %r288, %r37;
	mad.lo.s32 	%r39, %r12, 5, %r288;
	mad.lo.s32 	%r40, %r12, 6, %r288;
	mul.wide.s32 	%rd41, %r35, 8;
	add.s64 	%rd8, %rd41, %rd40;
	mul.wide.s32 	%rd42, %r36, 8;
	add.s64 	%rd9, %rd42, %rd40;
	mul.wide.s32 	%rd43, %r38, 8;
	add.s64 	%rd10, %rd43, %rd40;
	mul.wide.s32 	%rd44, %r39, 8;
	add.s64 	%rd11, %rd44, %rd40;
	mul.wide.s32 	%rd45, %r40, 8;
	add.s64 	%rd12, %rd45, %rd40;
	mul.wide.s32 	%rd46, %r288, 2;
	add.s64 	%rd47, %rd46, %rd3;
	shl.b64 	%rd48, %rd47, 2;
	add.s64 	%rd49, %rd4, %rd48;
	add.s64 	%rd69, %rd49, 512;
	mul.wide.s32 	%rd50, %r288, 8;
	mul.wide.s32 	%rd51, %r12, 8;
	add.s64 	%rd52, %rd50, %rd51;
	add.s64 	%rd14, %rd52, %rd40;
	add.s64 	%rd15, %rd50, %rd40;

$L__BB14_7:
	ld.global.nc.v2.f32 	{%f89, %f90}, [%rd69+-512];
	add.s64 	%rd53, %rd70, %rd15;
	ld.global.nc.v2.f32 	{%f93, %f94}, [%rd53];
	fma.rn.f32 	%f97, %f89, %f93, %f342;
	fma.rn.f32 	%f98, %f90, %f94, %f97;
	add.s64 	%rd54, %rd70, %rd14;
	ld.global.nc.v2.f32 	{%f99, %f100}, [%rd54];
	fma.rn.f32 	%f103, %f89, %f99, %f341;
	fma.rn.f32 	%f104, %f90, %f100, %f103;
	add.s64 	%rd55, %rd70, %rd8;
	ld.global.nc.v2.f32 	{%f105, %f106}, [%rd55];
	fma.rn.f32 	%f109, %f89, %f105, %f340;
	fma.rn.f32 	%f110, %f90, %f106, %f109;
	add.s64 	%rd56, %rd70, %rd9;
	ld.global.nc.v2.f32 	{%f111, %f112}, [%rd56];
	fma.rn.f32 	%f115, %f89, %f111, %f339;
	fma.rn.f32 	%f116, %f90, %f112, %f115;
	add.s64 	%rd57, %rd70, %rd10;
	ld.global.nc.v2.f32 	{%f117, %f118}, [%rd57];
	fma.rn.f32 	%f121, %f89, %f117, %f338;
	fma.rn.f32 	%f122, %f90, %f118, %f121;
	add.s64 	%rd58, %rd70, %rd11;
	ld.global.nc.v2.f32 	{%f123, %f124}, [%rd58];
	fma.rn.f32 	%f127, %f89, %f123, %f337;
	fma.rn.f32 	%f128, %f90, %f124, %f127;
	add.s64 	%rd59, %rd70, %rd12;
	ld.global.nc.v2.f32 	{%f129, %f130}, [%rd59];
	fma.rn.f32 	%f133, %f89, %f129, %f336;
	fma.rn.f32 	%f134, %f90, %f130, %f133;
	ld.global.nc.v2.f32 	{%f135, %f136}, [%rd69];
	ld.global.nc.v2.f32 	{%f139, %f140}, [%rd53+512];
	fma.rn.f32 	%f143, %f135, %f139, %f98;
	fma.rn.f32 	%f342, %f136, %f140, %f143;
	add.s64 	%rd60, %rd70, %rd7;
	ld.global.nc.v2.f32 	{%f144, %f145}, [%rd60];
	fma.rn.f32 	%f148, %f135, %f144, %f104;
	fma.rn.f32 	%f341, %f136, %f145, %f148;
	ld.global.nc.v2.f32 	{%f149, %f150}, [%rd55+512];
	fma.rn.f32 	%f153, %f135, %f149, %f110;
	fma.rn.f32 	%f340, %f136, %f150, %f153;
	ld.global.nc.v2.f32 	{%f154, %f155}, [%rd56+512];
	fma.rn.f32 	%f158, %f135, %f154, %f116;
	fma.rn.f32 	%f339, %f136, %f155, %f158;
	ld.global.nc.v2.f32 	{%f159, %f160}, [%rd57+512];
	fma.rn.f32 	%f163, %f135, %f159, %f122;
	fma.rn.f32 	%f338, %f136, %f160, %f163;
	ld.global.nc.v2.f32 	{%f164, %f165}, [%rd58+512];
	fma.rn.f32 	%f168, %f135, %f164, %f128;
	fma.rn.f32 	%f337, %f136, %f165, %f168;
	ld.global.nc.v2.f32 	{%f169, %f170}, [%rd59+512];
	fma.rn.f32 	%f173, %f135, %f169, %f134;
	fma.rn.f32 	%f336, %f136, %f170, %f173;
	add.s64 	%rd70, %rd70, 1024;
	add.s64 	%rd69, %rd69, 1024;
	add.s32 	%r288, %r288, 128;
	setp.lt.s32 	%p5, %r288, %r11;
	@%p5 bra 	$L__BB14_7;

	st.local.f32 	[%rd2], %f342;
	st.local.f32 	[%rd2+4], %f341;
	st.local.f32 	[%rd2+8], %f340;
	st.local.f32 	[%rd2+12], %f339;
	st.local.f32 	[%rd2+16], %f338;
	st.local.f32 	[%rd2+20], %f337;
	st.local.f32 	[%rd2+24], %f336;

$L__BB14_9:
	shr.s32 	%r41, %r3, 31;
	shr.u32 	%r42, %r41, 27;
	add.s32 	%r43, %r3, %r42;
	shr.s32 	%r44, %r43, 5;
	shl.b32 	%r45, %r44, 2;
	add.s32 	%r10, %r24, %r45;
	mov.u32 	%r47, 2;
	mov.b32 	%r48, %f342;
	mov.u32 	%r49, 31;
	mov.u32 	%r50, 16;
	mov.u32 	%r51, -1;
	shfl.sync.bfly.b32 	%r52|%p6, %r48, %r50, %r49, %r51;
	mov.b32 	%f174, %r52;
	add.f32 	%f175, %f342, %f174;
	mov.b32 	%r53, %f175;
	mov.u32 	%r54, 8;
	shfl.sync.bfly.b32 	%r55|%p7, %r53, %r54, %r49, %r51;
	mov.b32 	%f176, %r55;
	add.f32 	%f177, %f175, %f176;
	mov.b32 	%r56, %f177;
	mov.u32 	%r57, 4;
	shfl.sync.bfly.b32 	%r58|%p8, %r56, %r57, %r49, %r51;
	mov.b32 	%f178, %r58;
	add.f32 	%f179, %f177, %f178;
	mov.b32 	%r59, %f179;
	shfl.sync.bfly.b32 	%r60|%p9, %r59, %r47, %r49, %r51;
	mov.b32 	%f180, %r60;
	add.f32 	%f181, %f179, %f180;
	mov.b32 	%r61, %f181;
	mov.u32 	%r62, 1;
	shfl.sync.bfly.b32 	%r63|%p10, %r61, %r62, %r49, %r51;
	mov.b32 	%f182, %r63;
	add.f32 	%f183, %f181, %f182;
	st.local.f32 	[%rd2], %f183;
	st.shared.f32 	[%r10], %f183;
	bar.sync 	0;
	@%p1 bra 	$L__BB14_11;

	ld.shared.f32 	%f184, [%r4];
	mov.b32 	%r64, %f184;
	shfl.sync.bfly.b32 	%r68|%p12, %r64, %r50, %r49, %r51;
	mov.b32 	%f185, %r68;
	add.f32 	%f186, %f184, %f185;
	mov.b32 	%r69, %f186;
	shfl.sync.bfly.b32 	%r71|%p13, %r69, %r54, %r49, %r51;
	mov.b32 	%f187, %r71;
	add.f32 	%f188, %f186, %f187;
	mov.b32 	%r72, %f188;
	shfl.sync.bfly.b32 	%r74|%p14, %r72, %r57, %r49, %r51;
	mov.b32 	%f189, %r74;
	add.f32 	%f190, %f188, %f189;
	mov.b32 	%r75, %f190;
	shfl.sync.bfly.b32 	%r77|%p15, %r75, %r47, %r49, %r51;
	mov.b32 	%f191, %r77;
	add.f32 	%f192, %f190, %f191;
	mov.b32 	%r78, %f192;
	shfl.sync.bfly.b32 	%r80|%p16, %r78, %r62, %r49, %r51;
	mov.b32 	%f193, %r80;
	add.f32 	%f194, %f192, %f193;
	st.local.f32 	[%rd2], %f194;

$L__BB14_11:
	bar.sync 	0;
	mov.b32 	%r81, %f341;
	shfl.sync.bfly.b32 	%r85|%p18, %r81, %r50, %r49, %r51;
	mov.b32 	%f195, %r85;
	add.f32 	%f196, %f341, %f195;
	mov.b32 	%r86, %f196;
	shfl.sync.bfly.b32 	%r88|%p19, %r86, %r54, %r49, %r51;
	mov.b32 	%f197, %r88;
	add.f32 	%f198, %f196, %f197;
	mov.b32 	%r89, %f198;
	shfl.sync.bfly.b32 	%r91|%p20, %r89, %r57, %r49, %r51;
	mov.b32 	%f199, %r91;
	add.f32 	%f200, %f198, %f199;
	mov.b32 	%r92, %f200;
	shfl.sync.bfly.b32 	%r94|%p21, %r92, %r47, %r49, %r51;
	mov.b32 	%f201, %r94;
	add.f32 	%f202, %f200, %f201;
	mov.b32 	%r95, %f202;
	shfl.sync.bfly.b32 	%r97|%p22, %r95, %r62, %r49, %r51;
	mov.b32 	%f203, %r97;
	add.f32 	%f204, %f202, %f203;
	st.local.f32 	[%rd2+4], %f204;
	st.shared.f32 	[%r10], %f204;
	bar.sync 	0;
	@%p1 bra 	$L__BB14_13;

	ld.shared.f32 	%f205, [%r4];
	mov.b32 	%r98, %f205;
	mov.u32 	%r99, 31;
	mov.u32 	%r100, 16;
	mov.u32 	%r101, -1;
	shfl.sync.bfly.b32 	%r102|%p23, %r98, %r100, %r99, %r101;
	mov.b32 	%f206, %r102;
	add.f32 	%f207, %f205, %f206;
	mov.b32 	%r103, %f207;
	mov.u32 	%r104, 8;
	shfl.sync.bfly.b32 	%r105|%p24, %r103, %r104, %r99, %r101;
	mov.b32 	%f208, %r105;
	add.f32 	%f209, %f207, %f208;
	mov.b32 	%r106, %f209;
	mov.u32 	%r107, 4;
	shfl.sync.bfly.b32 	%r108|%p25, %r106, %r107, %r99, %r101;
	mov.b32 	%f210, %r108;
	add.f32 	%f211, %f209, %f210;
	mov.b32 	%r109, %f211;
	mov.u32 	%r110, 2;
	shfl.sync.bfly.b32 	%r111|%p26, %r109, %r110, %r99, %r101;
	mov.b32 	%f212, %r111;
	add.f32 	%f213, %f211, %f212;
	mov.b32 	%r112, %f213;
	mov.u32 	%r113, 1;
	shfl.sync.bfly.b32 	%r114|%p27, %r112, %r113, %r99, %r101;
	mov.b32 	%f214, %r114;
	add.f32 	%f215, %f213, %f214;
	st.local.f32 	[%rd2+4], %f215;

$L__BB14_13:
	bar.sync 	0;
	mov.b32 	%r115, %f340;
	mov.u32 	%r116, 31;
	mov.u32 	%r117, 16;
	mov.u32 	%r118, -1;
	shfl.sync.bfly.b32 	%r119|%p29, %r115, %r117, %r116, %r118;
	mov.b32 	%f216, %r119;
	add.f32 	%f217, %f340, %f216;
	mov.b32 	%r120, %f217;
	mov.u32 	%r121, 8;
	shfl.sync.bfly.b32 	%r122|%p30, %r120, %r121, %r116, %r118;
	mov.b32 	%f218, %r122;
	add.f32 	%f219, %f217, %f218;
	mov.b32 	%r123, %f219;
	mov.u32 	%r124, 4;
	shfl.sync.bfly.b32 	%r125|%p31, %r123, %r124, %r116, %r118;
	mov.b32 	%f220, %r125;
	add.f32 	%f221, %f219, %f220;
	mov.b32 	%r126, %f221;
	mov.u32 	%r127, 2;
	shfl.sync.bfly.b32 	%r128|%p32, %r126, %r127, %r116, %r118;
	mov.b32 	%f222, %r128;
	add.f32 	%f223, %f221, %f222;
	mov.b32 	%r129, %f223;
	mov.u32 	%r130, 1;
	shfl.sync.bfly.b32 	%r131|%p33, %r129, %r130, %r116, %r118;
	mov.b32 	%f224, %r131;
	add.f32 	%f225, %f223, %f224;
	st.local.f32 	[%rd2+8], %f225;
	st.shared.f32 	[%r10], %f225;
	bar.sync 	0;
	@%p1 bra 	$L__BB14_15;

	ld.shared.f32 	%f226, [%r4];
	mov.b32 	%r132, %f226;
	shfl.sync.bfly.b32 	%r136|%p34, %r132, %r117, %r116, %r118;
	mov.b32 	%f227, %r136;
	add.f32 	%f228, %f226, %f227;
	mov.b32 	%r137, %f228;
	shfl.sync.bfly.b32 	%r139|%p35, %r137, %r121, %r116, %r118;
	mov.b32 	%f229, %r139;
	add.f32 	%f230, %f228, %f229;
	mov.b32 	%r140, %f230;
	shfl.sync.bfly.b32 	%r142|%p36, %r140, %r124, %r116, %r118;
	mov.b32 	%f231, %r142;
	add.f32 	%f232, %f230, %f231;
	mov.b32 	%r143, %f232;
	shfl.sync.bfly.b32 	%r145|%p37, %r143, %r127, %r116, %r118;
	mov.b32 	%f233, %r145;
	add.f32 	%f234, %f232, %f233;
	mov.b32 	%r146, %f234;
	shfl.sync.bfly.b32 	%r148|%p38, %r146, %r130, %r116, %r118;
	mov.b32 	%f235, %r148;
	add.f32 	%f236, %f234, %f235;
	st.local.f32 	[%rd2+8], %f236;

$L__BB14_15:
	bar.sync 	0;
	mov.b32 	%r149, %f339;
	shfl.sync.bfly.b32 	%r153|%p40, %r149, %r117, %r116, %r118;
	mov.b32 	%f237, %r153;
	add.f32 	%f238, %f339, %f237;
	mov.b32 	%r154, %f238;
	shfl.sync.bfly.b32 	%r156|%p41, %r154, %r121, %r116, %r118;
	mov.b32 	%f239, %r156;
	add.f32 	%f240, %f238, %f239;
	mov.b32 	%r157, %f240;
	shfl.sync.bfly.b32 	%r159|%p42, %r157, %r124, %r116, %r118;
	mov.b32 	%f241, %r159;
	add.f32 	%f242, %f240, %f241;
	mov.b32 	%r160, %f242;
	shfl.sync.bfly.b32 	%r162|%p43, %r160, %r127, %r116, %r118;
	mov.b32 	%f243, %r162;
	add.f32 	%f244, %f242, %f243;
	mov.b32 	%r163, %f244;
	shfl.sync.bfly.b32 	%r165|%p44, %r163, %r130, %r116, %r118;
	mov.b32 	%f245, %r165;
	add.f32 	%f246, %f244, %f245;
	st.local.f32 	[%rd2+12], %f246;
	st.shared.f32 	[%r10], %f246;
	bar.sync 	0;
	@%p1 bra 	$L__BB14_17;

	ld.shared.f32 	%f247, [%r4];
	mov.b32 	%r166, %f247;
	mov.u32 	%r167, 31;
	mov.u32 	%r168, 16;
	mov.u32 	%r169, -1;
	shfl.sync.bfly.b32 	%r170|%p45, %r166, %r168, %r167, %r169;
	mov.b32 	%f248, %r170;
	add.f32 	%f249, %f247, %f248;
	mov.b32 	%r171, %f249;
	mov.u32 	%r172, 8;
	shfl.sync.bfly.b32 	%r173|%p46, %r171, %r172, %r167, %r169;
	mov.b32 	%f250, %r173;
	add.f32 	%f251, %f249, %f250;
	mov.b32 	%r174, %f251;
	mov.u32 	%r175, 4;
	shfl.sync.bfly.b32 	%r176|%p47, %r174, %r175, %r167, %r169;
	mov.b32 	%f252, %r176;
	add.f32 	%f253, %f251, %f252;
	mov.b32 	%r177, %f253;
	mov.u32 	%r178, 2;
	shfl.sync.bfly.b32 	%r179|%p48, %r177, %r178, %r167, %r169;
	mov.b32 	%f254, %r179;
	add.f32 	%f255, %f253, %f254;
	mov.b32 	%r180, %f255;
	mov.u32 	%r181, 1;
	shfl.sync.bfly.b32 	%r182|%p49, %r180, %r181, %r167, %r169;
	mov.b32 	%f256, %r182;
	add.f32 	%f257, %f255, %f256;
	st.local.f32 	[%rd2+12], %f257;

$L__BB14_17:
	bar.sync 	0;
	mov.b32 	%r183, %f338;
	mov.u32 	%r184, 31;
	mov.u32 	%r185, 16;
	mov.u32 	%r186, -1;
	shfl.sync.bfly.b32 	%r187|%p51, %r183, %r185, %r184, %r186;
	mov.b32 	%f258, %r187;
	add.f32 	%f259, %f338, %f258;
	mov.b32 	%r188, %f259;
	mov.u32 	%r189, 8;
	shfl.sync.bfly.b32 	%r190|%p52, %r188, %r189, %r184, %r186;
	mov.b32 	%f260, %r190;
	add.f32 	%f261, %f259, %f260;
	mov.b32 	%r191, %f261;
	mov.u32 	%r192, 4;
	shfl.sync.bfly.b32 	%r193|%p53, %r191, %r192, %r184, %r186;
	mov.b32 	%f262, %r193;
	add.f32 	%f263, %f261, %f262;
	mov.b32 	%r194, %f263;
	mov.u32 	%r195, 2;
	shfl.sync.bfly.b32 	%r196|%p54, %r194, %r195, %r184, %r186;
	mov.b32 	%f264, %r196;
	add.f32 	%f265, %f263, %f264;
	mov.b32 	%r197, %f265;
	mov.u32 	%r198, 1;
	shfl.sync.bfly.b32 	%r199|%p55, %r197, %r198, %r184, %r186;
	mov.b32 	%f266, %r199;
	add.f32 	%f267, %f265, %f266;
	st.local.f32 	[%rd2+16], %f267;
	st.shared.f32 	[%r10], %f267;
	bar.sync 	0;
	@%p1 bra 	$L__BB14_19;

	ld.shared.f32 	%f268, [%r4];
	mov.b32 	%r200, %f268;
	shfl.sync.bfly.b32 	%r204|%p56, %r200, %r185, %r184, %r186;
	mov.b32 	%f269, %r204;
	add.f32 	%f270, %f268, %f269;
	mov.b32 	%r205, %f270;
	shfl.sync.bfly.b32 	%r207|%p57, %r205, %r189, %r184, %r186;
	mov.b32 	%f271, %r207;
	add.f32 	%f272, %f270, %f271;
	mov.b32 	%r208, %f272;
	shfl.sync.bfly.b32 	%r210|%p58, %r208, %r192, %r184, %r186;
	mov.b32 	%f273, %r210;
	add.f32 	%f274, %f272, %f273;
	mov.b32 	%r211, %f274;
	shfl.sync.bfly.b32 	%r213|%p59, %r211, %r195, %r184, %r186;
	mov.b32 	%f275, %r213;
	add.f32 	%f276, %f274, %f275;
	mov.b32 	%r214, %f276;
	shfl.sync.bfly.b32 	%r216|%p60, %r214, %r198, %r184, %r186;
	mov.b32 	%f277, %r216;
	add.f32 	%f278, %f276, %f277;
	st.local.f32 	[%rd2+16], %f278;

$L__BB14_19:
	bar.sync 	0;
	mov.b32 	%r217, %f337;
	shfl.sync.bfly.b32 	%r221|%p62, %r217, %r185, %r184, %r186;
	mov.b32 	%f279, %r221;
	add.f32 	%f280, %f337, %f279;
	mov.b32 	%r222, %f280;
	shfl.sync.bfly.b32 	%r224|%p63, %r222, %r189, %r184, %r186;
	mov.b32 	%f281, %r224;
	add.f32 	%f282, %f280, %f281;
	mov.b32 	%r225, %f282;
	shfl.sync.bfly.b32 	%r227|%p64, %r225, %r192, %r184, %r186;
	mov.b32 	%f283, %r227;
	add.f32 	%f284, %f282, %f283;
	mov.b32 	%r228, %f284;
	shfl.sync.bfly.b32 	%r230|%p65, %r228, %r195, %r184, %r186;
	mov.b32 	%f285, %r230;
	add.f32 	%f286, %f284, %f285;
	mov.b32 	%r231, %f286;
	shfl.sync.bfly.b32 	%r233|%p66, %r231, %r198, %r184, %r186;
	mov.b32 	%f287, %r233;
	add.f32 	%f288, %f286, %f287;
	st.local.f32 	[%rd2+20], %f288;
	st.shared.f32 	[%r10], %f288;
	bar.sync 	0;
	@%p1 bra 	$L__BB14_21;

	ld.shared.f32 	%f289, [%r4];
	mov.b32 	%r234, %f289;
	mov.u32 	%r235, 31;
	mov.u32 	%r236, 16;
	mov.u32 	%r237, -1;
	shfl.sync.bfly.b32 	%r238|%p67, %r234, %r236, %r235, %r237;
	mov.b32 	%f290, %r238;
	add.f32 	%f291, %f289, %f290;
	mov.b32 	%r239, %f291;
	mov.u32 	%r240, 8;
	shfl.sync.bfly.b32 	%r241|%p68, %r239, %r240, %r235, %r237;
	mov.b32 	%f292, %r241;
	add.f32 	%f293, %f291, %f292;
	mov.b32 	%r242, %f293;
	mov.u32 	%r243, 4;
	shfl.sync.bfly.b32 	%r244|%p69, %r242, %r243, %r235, %r237;
	mov.b32 	%f294, %r244;
	add.f32 	%f295, %f293, %f294;
	mov.b32 	%r245, %f295;
	mov.u32 	%r246, 2;
	shfl.sync.bfly.b32 	%r247|%p70, %r245, %r246, %r235, %r237;
	mov.b32 	%f296, %r247;
	add.f32 	%f297, %f295, %f296;
	mov.b32 	%r248, %f297;
	mov.u32 	%r249, 1;
	shfl.sync.bfly.b32 	%r250|%p71, %r248, %r249, %r235, %r237;
	mov.b32 	%f298, %r250;
	add.f32 	%f299, %f297, %f298;
	st.local.f32 	[%rd2+20], %f299;

$L__BB14_21:
	bar.sync 	0;
	mov.b32 	%r251, %f336;
	mov.u32 	%r252, 31;
	mov.u32 	%r253, 16;
	mov.u32 	%r254, -1;
	shfl.sync.bfly.b32 	%r255|%p73, %r251, %r253, %r252, %r254;
	mov.b32 	%f300, %r255;
	add.f32 	%f301, %f336, %f300;
	mov.b32 	%r256, %f301;
	mov.u32 	%r257, 8;
	shfl.sync.bfly.b32 	%r258|%p74, %r256, %r257, %r252, %r254;
	mov.b32 	%f302, %r258;
	add.f32 	%f303, %f301, %f302;
	mov.b32 	%r259, %f303;
	mov.u32 	%r260, 4;
	shfl.sync.bfly.b32 	%r261|%p75, %r259, %r260, %r252, %r254;
	mov.b32 	%f304, %r261;
	add.f32 	%f305, %f303, %f304;
	mov.b32 	%r262, %f305;
	mov.u32 	%r263, 2;
	shfl.sync.bfly.b32 	%r264|%p76, %r262, %r263, %r252, %r254;
	mov.b32 	%f306, %r264;
	add.f32 	%f307, %f305, %f306;
	mov.b32 	%r265, %f307;
	mov.u32 	%r266, 1;
	shfl.sync.bfly.b32 	%r267|%p77, %r265, %r266, %r252, %r254;
	mov.b32 	%f308, %r267;
	add.f32 	%f309, %f307, %f308;
	st.local.f32 	[%rd2+24], %f309;
	st.shared.f32 	[%r10], %f309;
	bar.sync 	0;
	@%p1 bra 	$L__BB14_23;

	ld.shared.f32 	%f310, [%r4];
	mov.b32 	%r268, %f310;
	shfl.sync.bfly.b32 	%r272|%p78, %r268, %r253, %r252, %r254;
	mov.b32 	%f311, %r272;
	add.f32 	%f312, %f310, %f311;
	mov.b32 	%r273, %f312;
	shfl.sync.bfly.b32 	%r275|%p79, %r273, %r257, %r252, %r254;
	mov.b32 	%f313, %r275;
	add.f32 	%f314, %f312, %f313;
	mov.b32 	%r276, %f314;
	shfl.sync.bfly.b32 	%r278|%p80, %r276, %r260, %r252, %r254;
	mov.b32 	%f315, %r278;
	add.f32 	%f316, %f314, %f315;
	mov.b32 	%r279, %f316;
	shfl.sync.bfly.b32 	%r281|%p81, %r279, %r263, %r252, %r254;
	mov.b32 	%f317, %r281;
	add.f32 	%f318, %f316, %f317;
	mov.b32 	%r282, %f318;
	shfl.sync.bfly.b32 	%r284|%p82, %r282, %r266, %r252, %r254;
	mov.b32 	%f319, %r284;
	add.f32 	%f320, %f318, %f319;
	st.local.f32 	[%rd2+24], %f320;

$L__BB14_23:
	bar.sync 	0;
	setp.gt.s32 	%p83, %r3, 6;
	@%p83 bra 	$L__BB14_25;

	mul.wide.s32 	%rd61, %r3, 4;
	add.s64 	%rd62, %rd2, %rd61;
	ld.local.f32 	%f321, [%rd62];
	mad.lo.s32 	%r285, %r3, %r13, %r2;
	cvt.s64.s32 	%rd63, %r285;
	mul.lo.s32 	%r286, %r1, %r14;
	cvt.s64.s32 	%rd64, %r286;
	add.s64 	%rd65, %rd64, %rd63;
	cvta.to.global.u64 	%rd66, %rd20;
	shl.b64 	%rd67, %rd65, 2;
	add.s64 	%rd68, %rd66, %rd67;
	st.global.f32 	[%rd68], %f321;

$L__BB14_25:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_8_bs_64
.visible .entry ggml_matvec_f32_ncols_8_bs_64(
	.param .u64 ggml_matvec_f32_ncols_8_bs_64_param_0,
	.param .u64 ggml_matvec_f32_ncols_8_bs_64_param_1,
	.param .u64 ggml_matvec_f32_ncols_8_bs_64_param_2,
	.param .u32 ggml_matvec_f32_ncols_8_bs_64_param_3,
	.param .u32 ggml_matvec_f32_ncols_8_bs_64_param_4,
	.param .u32 ggml_matvec_f32_ncols_8_bs_64_param_5,
	.param .u32 ggml_matvec_f32_ncols_8_bs_64_param_6,
	.param .u32 ggml_matvec_f32_ncols_8_bs_64_param_7,
	.param .u32 ggml_matvec_f32_ncols_8_bs_64_param_8,
	.param .u32 ggml_matvec_f32_ncols_8_bs_64_param_9,
	.param .u32 ggml_matvec_f32_ncols_8_bs_64_param_10,
	.param .u32 ggml_matvec_f32_ncols_8_bs_64_param_11
)
{
	.local .align 16 .b8 	__local_depot15[32];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<95>;
	.reg .f32 	%f<390>;
	.reg .b32 	%r<323>;
	.reg .b64 	%rd<75>;


	mov.u64 	%SPL, __local_depot15;
	ld.param.u64 	%rd22, [ggml_matvec_f32_ncols_8_bs_64_param_0];
	ld.param.u64 	%rd23, [ggml_matvec_f32_ncols_8_bs_64_param_1];
	ld.param.u64 	%rd21, [ggml_matvec_f32_ncols_8_bs_64_param_2];
	ld.param.u32 	%r11, [ggml_matvec_f32_ncols_8_bs_64_param_3];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_8_bs_64_param_5];
	ld.param.u32 	%r12, [ggml_matvec_f32_ncols_8_bs_64_param_6];
	ld.param.u32 	%r13, [ggml_matvec_f32_ncols_8_bs_64_param_7];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_8_bs_64_param_8];
	ld.param.u32 	%r17, [ggml_matvec_f32_ncols_8_bs_64_param_9];
	ld.param.u32 	%r18, [ggml_matvec_f32_ncols_8_bs_64_param_10];
	ld.param.u32 	%r14, [ggml_matvec_f32_ncols_8_bs_64_param_11];
	cvta.to.global.u64 	%rd74, %rd23;
	add.u64 	%rd2, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r19, %r1, %r16;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r20, %r2, %r15;
	mad.lo.s32 	%r21, %r19, %r17, %r20;
	cvt.s64.s32 	%rd3, %r21;
	cvta.to.global.u64 	%rd4, %rd22;
	mul.lo.s32 	%r22, %r1, %r18;
	cvt.s64.s32 	%rd5, %r22;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r23, %r3, 2;
	mov.u32 	%r24, data_mmv;
	add.s32 	%r4, %r24, %r23;
	@%p1 bra 	$L__BB15_2;

	mov.u32 	%r25, 0;
	st.shared.u32 	[%r4], %r25;

$L__BB15_2:
	bar.sync 	0;
	mov.f32 	%f382, 0f00000000;
	st.local.v4.f32 	[%rd2], {%f382, %f382, %f382, %f382};
	st.local.v4.f32 	[%rd2+16], {%f382, %f382, %f382, %f382};
	setp.ge.s32 	%p2, %r3, %r11;
	mov.f32 	%f383, %f382;
	mov.f32 	%f384, %f382;
	mov.f32 	%f385, %f382;
	mov.f32 	%f386, %f382;
	mov.f32 	%f387, %f382;
	mov.f32 	%f388, %f382;
	mov.f32 	%f389, %f382;
	@%p2 bra 	$L__BB15_9;

	not.b32 	%r26, %r3;
	add.s32 	%r5, %r26, %r11;
	and.b32  	%r27, %r5, 64;
	setp.ne.s32 	%p3, %r27, 0;
	mov.f32 	%f382, 0f00000000;
	mov.u32 	%r322, %r3;
	@%p3 bra 	$L__BB15_5;

	shl.b64 	%rd25, %rd5, 2;
	add.s64 	%rd26, %rd74, %rd25;
	shl.b64 	%rd27, %rd3, 2;
	add.s64 	%rd28, %rd4, %rd27;
	mul.wide.s32 	%rd29, %r3, 8;
	add.s64 	%rd30, %rd28, %rd29;
	ld.global.nc.v2.f32 	{%f57, %f58}, [%rd30];
	add.s64 	%rd31, %rd26, %rd29;
	ld.global.nc.v2.f32 	{%f61, %f62}, [%rd31];
	fma.rn.f32 	%f65, %f57, %f61, 0f00000000;
	fma.rn.f32 	%f389, %f58, %f62, %f65;
	mul.wide.s32 	%rd32, %r12, 8;
	add.s64 	%rd33, %rd31, %rd32;
	ld.global.nc.v2.f32 	{%f66, %f67}, [%rd33];
	fma.rn.f32 	%f70, %f57, %f66, 0f00000000;
	fma.rn.f32 	%f388, %f58, %f67, %f70;
	add.s32 	%r28, %r3, %r12;
	add.s32 	%r29, %r28, %r12;
	mul.wide.s32 	%rd34, %r29, 8;
	add.s64 	%rd35, %rd26, %rd34;
	ld.global.nc.v2.f32 	{%f71, %f72}, [%rd35];
	fma.rn.f32 	%f75, %f57, %f71, 0f00000000;
	fma.rn.f32 	%f387, %f58, %f72, %f75;
	add.s64 	%rd36, %rd35, %rd32;
	ld.global.nc.v2.f32 	{%f76, %f77}, [%rd36];
	fma.rn.f32 	%f80, %f57, %f76, 0f00000000;
	fma.rn.f32 	%f386, %f58, %f77, %f80;
	st.local.v4.f32 	[%rd2], {%f389, %f388, %f387, %f386};
	add.s64 	%rd37, %rd36, %rd32;
	ld.global.nc.v2.f32 	{%f81, %f82}, [%rd37];
	fma.rn.f32 	%f85, %f57, %f81, 0f00000000;
	fma.rn.f32 	%f385, %f58, %f82, %f85;
	add.s64 	%rd38, %rd37, %rd32;
	ld.global.nc.v2.f32 	{%f86, %f87}, [%rd38];
	fma.rn.f32 	%f90, %f57, %f86, 0f00000000;
	fma.rn.f32 	%f384, %f58, %f87, %f90;
	add.s64 	%rd39, %rd38, %rd32;
	ld.global.nc.v2.f32 	{%f91, %f92}, [%rd39];
	fma.rn.f32 	%f95, %f57, %f91, 0f00000000;
	fma.rn.f32 	%f383, %f58, %f92, %f95;
	add.s64 	%rd40, %rd39, %rd32;
	ld.global.nc.v2.f32 	{%f96, %f97}, [%rd40];
	fma.rn.f32 	%f100, %f57, %f96, 0f00000000;
	fma.rn.f32 	%f382, %f58, %f97, %f100;
	st.local.v4.f32 	[%rd2+16], {%f385, %f384, %f383, %f382};
	add.s32 	%r322, %r3, 64;

$L__BB15_5:
	and.b32  	%r30, %r5, -64;
	setp.eq.s32 	%p4, %r30, 0;
	@%p4 bra 	$L__BB15_9;

	add.s32 	%r31, %r322, %r12;
	add.s32 	%r32, %r31, 64;
	mul.wide.s32 	%rd41, %r32, 8;
	shl.b64 	%rd42, %rd5, 2;
	add.s64 	%rd7, %rd41, %rd42;
	shl.b32 	%r33, %r12, 1;
	add.s32 	%r34, %r322, %r33;
	mad.lo.s32 	%r35, %r12, 3, %r322;
	shl.b32 	%r36, %r12, 2;
	add.s32 	%r37, %r322, %r36;
	mad.lo.s32 	%r38, %r12, 5, %r322;
	mad.lo.s32 	%r39, %r12, 6, %r322;
	mad.lo.s32 	%r40, %r12, 7, %r322;
	mul.wide.s32 	%rd43, %r34, 8;
	add.s64 	%rd8, %rd43, %rd42;
	mul.wide.s32 	%rd44, %r35, 8;
	add.s64 	%rd9, %rd44, %rd42;
	mul.wide.s32 	%rd45, %r37, 8;
	add.s64 	%rd10, %rd45, %rd42;
	mul.wide.s32 	%rd46, %r38, 8;
	add.s64 	%rd11, %rd46, %rd42;
	mul.wide.s32 	%rd47, %r39, 8;
	add.s64 	%rd12, %rd47, %rd42;
	mul.wide.s32 	%rd48, %r40, 8;
	add.s64 	%rd13, %rd48, %rd42;
	mul.wide.s32 	%rd49, %r322, 2;
	add.s64 	%rd50, %rd49, %rd3;
	shl.b64 	%rd51, %rd50, 2;
	add.s64 	%rd52, %rd4, %rd51;
	add.s64 	%rd73, %rd52, 512;
	mul.wide.s32 	%rd53, %r322, 8;
	mul.wide.s32 	%rd54, %r12, 8;
	add.s64 	%rd55, %rd53, %rd54;
	add.s64 	%rd15, %rd55, %rd42;
	add.s64 	%rd16, %rd53, %rd42;

$L__BB15_7:
	ld.global.nc.v2.f32 	{%f101, %f102}, [%rd73+-512];
	add.s64 	%rd56, %rd74, %rd16;
	ld.global.nc.v2.f32 	{%f105, %f106}, [%rd56];
	fma.rn.f32 	%f109, %f101, %f105, %f389;
	fma.rn.f32 	%f110, %f102, %f106, %f109;
	add.s64 	%rd57, %rd74, %rd15;
	ld.global.nc.v2.f32 	{%f111, %f112}, [%rd57];
	fma.rn.f32 	%f115, %f101, %f111, %f388;
	fma.rn.f32 	%f116, %f102, %f112, %f115;
	add.s64 	%rd58, %rd74, %rd8;
	ld.global.nc.v2.f32 	{%f117, %f118}, [%rd58];
	fma.rn.f32 	%f121, %f101, %f117, %f387;
	fma.rn.f32 	%f122, %f102, %f118, %f121;
	add.s64 	%rd59, %rd74, %rd9;
	ld.global.nc.v2.f32 	{%f123, %f124}, [%rd59];
	fma.rn.f32 	%f127, %f101, %f123, %f386;
	fma.rn.f32 	%f128, %f102, %f124, %f127;
	add.s64 	%rd60, %rd74, %rd10;
	ld.global.nc.v2.f32 	{%f129, %f130}, [%rd60];
	fma.rn.f32 	%f133, %f101, %f129, %f385;
	fma.rn.f32 	%f134, %f102, %f130, %f133;
	add.s64 	%rd61, %rd74, %rd11;
	ld.global.nc.v2.f32 	{%f135, %f136}, [%rd61];
	fma.rn.f32 	%f139, %f101, %f135, %f384;
	fma.rn.f32 	%f140, %f102, %f136, %f139;
	add.s64 	%rd62, %rd74, %rd12;
	ld.global.nc.v2.f32 	{%f141, %f142}, [%rd62];
	fma.rn.f32 	%f145, %f101, %f141, %f383;
	fma.rn.f32 	%f146, %f102, %f142, %f145;
	add.s64 	%rd63, %rd74, %rd13;
	ld.global.nc.v2.f32 	{%f147, %f148}, [%rd63];
	fma.rn.f32 	%f151, %f101, %f147, %f382;
	fma.rn.f32 	%f152, %f102, %f148, %f151;
	ld.global.nc.v2.f32 	{%f153, %f154}, [%rd73];
	ld.global.nc.v2.f32 	{%f157, %f158}, [%rd56+512];
	fma.rn.f32 	%f161, %f153, %f157, %f110;
	fma.rn.f32 	%f389, %f154, %f158, %f161;
	add.s64 	%rd64, %rd74, %rd7;
	ld.global.nc.v2.f32 	{%f162, %f163}, [%rd64];
	fma.rn.f32 	%f166, %f153, %f162, %f116;
	fma.rn.f32 	%f388, %f154, %f163, %f166;
	ld.global.nc.v2.f32 	{%f167, %f168}, [%rd58+512];
	fma.rn.f32 	%f171, %f153, %f167, %f122;
	fma.rn.f32 	%f387, %f154, %f168, %f171;
	ld.global.nc.v2.f32 	{%f172, %f173}, [%rd59+512];
	fma.rn.f32 	%f176, %f153, %f172, %f128;
	fma.rn.f32 	%f386, %f154, %f173, %f176;
	ld.global.nc.v2.f32 	{%f177, %f178}, [%rd60+512];
	fma.rn.f32 	%f181, %f153, %f177, %f134;
	fma.rn.f32 	%f385, %f154, %f178, %f181;
	ld.global.nc.v2.f32 	{%f182, %f183}, [%rd61+512];
	fma.rn.f32 	%f186, %f153, %f182, %f140;
	fma.rn.f32 	%f384, %f154, %f183, %f186;
	ld.global.nc.v2.f32 	{%f187, %f188}, [%rd62+512];
	fma.rn.f32 	%f191, %f153, %f187, %f146;
	fma.rn.f32 	%f383, %f154, %f188, %f191;
	ld.global.nc.v2.f32 	{%f192, %f193}, [%rd63+512];
	fma.rn.f32 	%f196, %f153, %f192, %f152;
	fma.rn.f32 	%f382, %f154, %f193, %f196;
	add.s64 	%rd74, %rd74, 1024;
	add.s64 	%rd73, %rd73, 1024;
	add.s32 	%r322, %r322, 128;
	setp.lt.s32 	%p5, %r322, %r11;
	@%p5 bra 	$L__BB15_7;

	st.local.v4.f32 	[%rd2], {%f389, %f388, %f387, %f386};
	st.local.v4.f32 	[%rd2+16], {%f385, %f384, %f383, %f382};

$L__BB15_9:
	shr.s32 	%r41, %r3, 31;
	shr.u32 	%r42, %r41, 27;
	add.s32 	%r43, %r3, %r42;
	shr.s32 	%r44, %r43, 5;
	shl.b32 	%r45, %r44, 2;
	add.s32 	%r10, %r24, %r45;
	mov.u32 	%r47, 2;
	mov.b32 	%r48, %f389;
	mov.u32 	%r49, 31;
	mov.u32 	%r50, 16;
	mov.u32 	%r51, -1;
	shfl.sync.bfly.b32 	%r52|%p6, %r48, %r50, %r49, %r51;
	mov.b32 	%f197, %r52;
	add.f32 	%f198, %f389, %f197;
	mov.b32 	%r53, %f198;
	mov.u32 	%r54, 8;
	shfl.sync.bfly.b32 	%r55|%p7, %r53, %r54, %r49, %r51;
	mov.b32 	%f199, %r55;
	add.f32 	%f200, %f198, %f199;
	mov.b32 	%r56, %f200;
	mov.u32 	%r57, 4;
	shfl.sync.bfly.b32 	%r58|%p8, %r56, %r57, %r49, %r51;
	mov.b32 	%f201, %r58;
	add.f32 	%f202, %f200, %f201;
	mov.b32 	%r59, %f202;
	shfl.sync.bfly.b32 	%r60|%p9, %r59, %r47, %r49, %r51;
	mov.b32 	%f203, %r60;
	add.f32 	%f204, %f202, %f203;
	mov.b32 	%r61, %f204;
	mov.u32 	%r62, 1;
	shfl.sync.bfly.b32 	%r63|%p10, %r61, %r62, %r49, %r51;
	mov.b32 	%f205, %r63;
	add.f32 	%f206, %f204, %f205;
	st.local.f32 	[%rd2], %f206;
	st.shared.f32 	[%r10], %f206;
	bar.sync 	0;
	@%p1 bra 	$L__BB15_11;

	ld.shared.f32 	%f207, [%r4];
	mov.b32 	%r64, %f207;
	shfl.sync.bfly.b32 	%r68|%p12, %r64, %r50, %r49, %r51;
	mov.b32 	%f208, %r68;
	add.f32 	%f209, %f207, %f208;
	mov.b32 	%r69, %f209;
	shfl.sync.bfly.b32 	%r71|%p13, %r69, %r54, %r49, %r51;
	mov.b32 	%f210, %r71;
	add.f32 	%f211, %f209, %f210;
	mov.b32 	%r72, %f211;
	shfl.sync.bfly.b32 	%r74|%p14, %r72, %r57, %r49, %r51;
	mov.b32 	%f212, %r74;
	add.f32 	%f213, %f211, %f212;
	mov.b32 	%r75, %f213;
	shfl.sync.bfly.b32 	%r77|%p15, %r75, %r47, %r49, %r51;
	mov.b32 	%f214, %r77;
	add.f32 	%f215, %f213, %f214;
	mov.b32 	%r78, %f215;
	shfl.sync.bfly.b32 	%r80|%p16, %r78, %r62, %r49, %r51;
	mov.b32 	%f216, %r80;
	add.f32 	%f217, %f215, %f216;
	st.local.f32 	[%rd2], %f217;

$L__BB15_11:
	bar.sync 	0;
	mov.b32 	%r81, %f388;
	shfl.sync.bfly.b32 	%r85|%p18, %r81, %r50, %r49, %r51;
	mov.b32 	%f218, %r85;
	add.f32 	%f219, %f388, %f218;
	mov.b32 	%r86, %f219;
	shfl.sync.bfly.b32 	%r88|%p19, %r86, %r54, %r49, %r51;
	mov.b32 	%f220, %r88;
	add.f32 	%f221, %f219, %f220;
	mov.b32 	%r89, %f221;
	shfl.sync.bfly.b32 	%r91|%p20, %r89, %r57, %r49, %r51;
	mov.b32 	%f222, %r91;
	add.f32 	%f223, %f221, %f222;
	mov.b32 	%r92, %f223;
	shfl.sync.bfly.b32 	%r94|%p21, %r92, %r47, %r49, %r51;
	mov.b32 	%f224, %r94;
	add.f32 	%f225, %f223, %f224;
	mov.b32 	%r95, %f225;
	shfl.sync.bfly.b32 	%r97|%p22, %r95, %r62, %r49, %r51;
	mov.b32 	%f226, %r97;
	add.f32 	%f227, %f225, %f226;
	st.local.f32 	[%rd2+4], %f227;
	st.shared.f32 	[%r10], %f227;
	bar.sync 	0;
	@%p1 bra 	$L__BB15_13;

	ld.shared.f32 	%f228, [%r4];
	mov.b32 	%r98, %f228;
	mov.u32 	%r99, 31;
	mov.u32 	%r100, 16;
	mov.u32 	%r101, -1;
	shfl.sync.bfly.b32 	%r102|%p23, %r98, %r100, %r99, %r101;
	mov.b32 	%f229, %r102;
	add.f32 	%f230, %f228, %f229;
	mov.b32 	%r103, %f230;
	mov.u32 	%r104, 8;
	shfl.sync.bfly.b32 	%r105|%p24, %r103, %r104, %r99, %r101;
	mov.b32 	%f231, %r105;
	add.f32 	%f232, %f230, %f231;
	mov.b32 	%r106, %f232;
	mov.u32 	%r107, 4;
	shfl.sync.bfly.b32 	%r108|%p25, %r106, %r107, %r99, %r101;
	mov.b32 	%f233, %r108;
	add.f32 	%f234, %f232, %f233;
	mov.b32 	%r109, %f234;
	mov.u32 	%r110, 2;
	shfl.sync.bfly.b32 	%r111|%p26, %r109, %r110, %r99, %r101;
	mov.b32 	%f235, %r111;
	add.f32 	%f236, %f234, %f235;
	mov.b32 	%r112, %f236;
	mov.u32 	%r113, 1;
	shfl.sync.bfly.b32 	%r114|%p27, %r112, %r113, %r99, %r101;
	mov.b32 	%f237, %r114;
	add.f32 	%f238, %f236, %f237;
	st.local.f32 	[%rd2+4], %f238;

$L__BB15_13:
	bar.sync 	0;
	mov.b32 	%r115, %f387;
	mov.u32 	%r116, 31;
	mov.u32 	%r117, 16;
	mov.u32 	%r118, -1;
	shfl.sync.bfly.b32 	%r119|%p29, %r115, %r117, %r116, %r118;
	mov.b32 	%f239, %r119;
	add.f32 	%f240, %f387, %f239;
	mov.b32 	%r120, %f240;
	mov.u32 	%r121, 8;
	shfl.sync.bfly.b32 	%r122|%p30, %r120, %r121, %r116, %r118;
	mov.b32 	%f241, %r122;
	add.f32 	%f242, %f240, %f241;
	mov.b32 	%r123, %f242;
	mov.u32 	%r124, 4;
	shfl.sync.bfly.b32 	%r125|%p31, %r123, %r124, %r116, %r118;
	mov.b32 	%f243, %r125;
	add.f32 	%f244, %f242, %f243;
	mov.b32 	%r126, %f244;
	mov.u32 	%r127, 2;
	shfl.sync.bfly.b32 	%r128|%p32, %r126, %r127, %r116, %r118;
	mov.b32 	%f245, %r128;
	add.f32 	%f246, %f244, %f245;
	mov.b32 	%r129, %f246;
	mov.u32 	%r130, 1;
	shfl.sync.bfly.b32 	%r131|%p33, %r129, %r130, %r116, %r118;
	mov.b32 	%f247, %r131;
	add.f32 	%f248, %f246, %f247;
	st.local.f32 	[%rd2+8], %f248;
	st.shared.f32 	[%r10], %f248;
	bar.sync 	0;
	@%p1 bra 	$L__BB15_15;

	ld.shared.f32 	%f249, [%r4];
	mov.b32 	%r132, %f249;
	shfl.sync.bfly.b32 	%r136|%p34, %r132, %r117, %r116, %r118;
	mov.b32 	%f250, %r136;
	add.f32 	%f251, %f249, %f250;
	mov.b32 	%r137, %f251;
	shfl.sync.bfly.b32 	%r139|%p35, %r137, %r121, %r116, %r118;
	mov.b32 	%f252, %r139;
	add.f32 	%f253, %f251, %f252;
	mov.b32 	%r140, %f253;
	shfl.sync.bfly.b32 	%r142|%p36, %r140, %r124, %r116, %r118;
	mov.b32 	%f254, %r142;
	add.f32 	%f255, %f253, %f254;
	mov.b32 	%r143, %f255;
	shfl.sync.bfly.b32 	%r145|%p37, %r143, %r127, %r116, %r118;
	mov.b32 	%f256, %r145;
	add.f32 	%f257, %f255, %f256;
	mov.b32 	%r146, %f257;
	shfl.sync.bfly.b32 	%r148|%p38, %r146, %r130, %r116, %r118;
	mov.b32 	%f258, %r148;
	add.f32 	%f259, %f257, %f258;
	st.local.f32 	[%rd2+8], %f259;

$L__BB15_15:
	bar.sync 	0;
	mov.b32 	%r149, %f386;
	shfl.sync.bfly.b32 	%r153|%p40, %r149, %r117, %r116, %r118;
	mov.b32 	%f260, %r153;
	add.f32 	%f261, %f386, %f260;
	mov.b32 	%r154, %f261;
	shfl.sync.bfly.b32 	%r156|%p41, %r154, %r121, %r116, %r118;
	mov.b32 	%f262, %r156;
	add.f32 	%f263, %f261, %f262;
	mov.b32 	%r157, %f263;
	shfl.sync.bfly.b32 	%r159|%p42, %r157, %r124, %r116, %r118;
	mov.b32 	%f264, %r159;
	add.f32 	%f265, %f263, %f264;
	mov.b32 	%r160, %f265;
	shfl.sync.bfly.b32 	%r162|%p43, %r160, %r127, %r116, %r118;
	mov.b32 	%f266, %r162;
	add.f32 	%f267, %f265, %f266;
	mov.b32 	%r163, %f267;
	shfl.sync.bfly.b32 	%r165|%p44, %r163, %r130, %r116, %r118;
	mov.b32 	%f268, %r165;
	add.f32 	%f269, %f267, %f268;
	st.local.f32 	[%rd2+12], %f269;
	st.shared.f32 	[%r10], %f269;
	bar.sync 	0;
	@%p1 bra 	$L__BB15_17;

	ld.shared.f32 	%f270, [%r4];
	mov.b32 	%r166, %f270;
	mov.u32 	%r167, 31;
	mov.u32 	%r168, 16;
	mov.u32 	%r169, -1;
	shfl.sync.bfly.b32 	%r170|%p45, %r166, %r168, %r167, %r169;
	mov.b32 	%f271, %r170;
	add.f32 	%f272, %f270, %f271;
	mov.b32 	%r171, %f272;
	mov.u32 	%r172, 8;
	shfl.sync.bfly.b32 	%r173|%p46, %r171, %r172, %r167, %r169;
	mov.b32 	%f273, %r173;
	add.f32 	%f274, %f272, %f273;
	mov.b32 	%r174, %f274;
	mov.u32 	%r175, 4;
	shfl.sync.bfly.b32 	%r176|%p47, %r174, %r175, %r167, %r169;
	mov.b32 	%f275, %r176;
	add.f32 	%f276, %f274, %f275;
	mov.b32 	%r177, %f276;
	mov.u32 	%r178, 2;
	shfl.sync.bfly.b32 	%r179|%p48, %r177, %r178, %r167, %r169;
	mov.b32 	%f277, %r179;
	add.f32 	%f278, %f276, %f277;
	mov.b32 	%r180, %f278;
	mov.u32 	%r181, 1;
	shfl.sync.bfly.b32 	%r182|%p49, %r180, %r181, %r167, %r169;
	mov.b32 	%f279, %r182;
	add.f32 	%f280, %f278, %f279;
	st.local.f32 	[%rd2+12], %f280;

$L__BB15_17:
	bar.sync 	0;
	mov.b32 	%r183, %f385;
	mov.u32 	%r184, 31;
	mov.u32 	%r185, 16;
	mov.u32 	%r186, -1;
	shfl.sync.bfly.b32 	%r187|%p51, %r183, %r185, %r184, %r186;
	mov.b32 	%f281, %r187;
	add.f32 	%f282, %f385, %f281;
	mov.b32 	%r188, %f282;
	mov.u32 	%r189, 8;
	shfl.sync.bfly.b32 	%r190|%p52, %r188, %r189, %r184, %r186;
	mov.b32 	%f283, %r190;
	add.f32 	%f284, %f282, %f283;
	mov.b32 	%r191, %f284;
	mov.u32 	%r192, 4;
	shfl.sync.bfly.b32 	%r193|%p53, %r191, %r192, %r184, %r186;
	mov.b32 	%f285, %r193;
	add.f32 	%f286, %f284, %f285;
	mov.b32 	%r194, %f286;
	mov.u32 	%r195, 2;
	shfl.sync.bfly.b32 	%r196|%p54, %r194, %r195, %r184, %r186;
	mov.b32 	%f287, %r196;
	add.f32 	%f288, %f286, %f287;
	mov.b32 	%r197, %f288;
	mov.u32 	%r198, 1;
	shfl.sync.bfly.b32 	%r199|%p55, %r197, %r198, %r184, %r186;
	mov.b32 	%f289, %r199;
	add.f32 	%f290, %f288, %f289;
	st.local.f32 	[%rd2+16], %f290;
	st.shared.f32 	[%r10], %f290;
	bar.sync 	0;
	@%p1 bra 	$L__BB15_19;

	ld.shared.f32 	%f291, [%r4];
	mov.b32 	%r200, %f291;
	shfl.sync.bfly.b32 	%r204|%p56, %r200, %r185, %r184, %r186;
	mov.b32 	%f292, %r204;
	add.f32 	%f293, %f291, %f292;
	mov.b32 	%r205, %f293;
	shfl.sync.bfly.b32 	%r207|%p57, %r205, %r189, %r184, %r186;
	mov.b32 	%f294, %r207;
	add.f32 	%f295, %f293, %f294;
	mov.b32 	%r208, %f295;
	shfl.sync.bfly.b32 	%r210|%p58, %r208, %r192, %r184, %r186;
	mov.b32 	%f296, %r210;
	add.f32 	%f297, %f295, %f296;
	mov.b32 	%r211, %f297;
	shfl.sync.bfly.b32 	%r213|%p59, %r211, %r195, %r184, %r186;
	mov.b32 	%f298, %r213;
	add.f32 	%f299, %f297, %f298;
	mov.b32 	%r214, %f299;
	shfl.sync.bfly.b32 	%r216|%p60, %r214, %r198, %r184, %r186;
	mov.b32 	%f300, %r216;
	add.f32 	%f301, %f299, %f300;
	st.local.f32 	[%rd2+16], %f301;

$L__BB15_19:
	bar.sync 	0;
	mov.b32 	%r217, %f384;
	shfl.sync.bfly.b32 	%r221|%p62, %r217, %r185, %r184, %r186;
	mov.b32 	%f302, %r221;
	add.f32 	%f303, %f384, %f302;
	mov.b32 	%r222, %f303;
	shfl.sync.bfly.b32 	%r224|%p63, %r222, %r189, %r184, %r186;
	mov.b32 	%f304, %r224;
	add.f32 	%f305, %f303, %f304;
	mov.b32 	%r225, %f305;
	shfl.sync.bfly.b32 	%r227|%p64, %r225, %r192, %r184, %r186;
	mov.b32 	%f306, %r227;
	add.f32 	%f307, %f305, %f306;
	mov.b32 	%r228, %f307;
	shfl.sync.bfly.b32 	%r230|%p65, %r228, %r195, %r184, %r186;
	mov.b32 	%f308, %r230;
	add.f32 	%f309, %f307, %f308;
	mov.b32 	%r231, %f309;
	shfl.sync.bfly.b32 	%r233|%p66, %r231, %r198, %r184, %r186;
	mov.b32 	%f310, %r233;
	add.f32 	%f311, %f309, %f310;
	st.local.f32 	[%rd2+20], %f311;
	st.shared.f32 	[%r10], %f311;
	bar.sync 	0;
	@%p1 bra 	$L__BB15_21;

	ld.shared.f32 	%f312, [%r4];
	mov.b32 	%r234, %f312;
	mov.u32 	%r235, 31;
	mov.u32 	%r236, 16;
	mov.u32 	%r237, -1;
	shfl.sync.bfly.b32 	%r238|%p67, %r234, %r236, %r235, %r237;
	mov.b32 	%f313, %r238;
	add.f32 	%f314, %f312, %f313;
	mov.b32 	%r239, %f314;
	mov.u32 	%r240, 8;
	shfl.sync.bfly.b32 	%r241|%p68, %r239, %r240, %r235, %r237;
	mov.b32 	%f315, %r241;
	add.f32 	%f316, %f314, %f315;
	mov.b32 	%r242, %f316;
	mov.u32 	%r243, 4;
	shfl.sync.bfly.b32 	%r244|%p69, %r242, %r243, %r235, %r237;
	mov.b32 	%f317, %r244;
	add.f32 	%f318, %f316, %f317;
	mov.b32 	%r245, %f318;
	mov.u32 	%r246, 2;
	shfl.sync.bfly.b32 	%r247|%p70, %r245, %r246, %r235, %r237;
	mov.b32 	%f319, %r247;
	add.f32 	%f320, %f318, %f319;
	mov.b32 	%r248, %f320;
	mov.u32 	%r249, 1;
	shfl.sync.bfly.b32 	%r250|%p71, %r248, %r249, %r235, %r237;
	mov.b32 	%f321, %r250;
	add.f32 	%f322, %f320, %f321;
	st.local.f32 	[%rd2+20], %f322;

$L__BB15_21:
	bar.sync 	0;
	mov.b32 	%r251, %f383;
	mov.u32 	%r252, 31;
	mov.u32 	%r253, 16;
	mov.u32 	%r254, -1;
	shfl.sync.bfly.b32 	%r255|%p73, %r251, %r253, %r252, %r254;
	mov.b32 	%f323, %r255;
	add.f32 	%f324, %f383, %f323;
	mov.b32 	%r256, %f324;
	mov.u32 	%r257, 8;
	shfl.sync.bfly.b32 	%r258|%p74, %r256, %r257, %r252, %r254;
	mov.b32 	%f325, %r258;
	add.f32 	%f326, %f324, %f325;
	mov.b32 	%r259, %f326;
	mov.u32 	%r260, 4;
	shfl.sync.bfly.b32 	%r261|%p75, %r259, %r260, %r252, %r254;
	mov.b32 	%f327, %r261;
	add.f32 	%f328, %f326, %f327;
	mov.b32 	%r262, %f328;
	mov.u32 	%r263, 2;
	shfl.sync.bfly.b32 	%r264|%p76, %r262, %r263, %r252, %r254;
	mov.b32 	%f329, %r264;
	add.f32 	%f330, %f328, %f329;
	mov.b32 	%r265, %f330;
	mov.u32 	%r266, 1;
	shfl.sync.bfly.b32 	%r267|%p77, %r265, %r266, %r252, %r254;
	mov.b32 	%f331, %r267;
	add.f32 	%f332, %f330, %f331;
	st.local.f32 	[%rd2+24], %f332;
	st.shared.f32 	[%r10], %f332;
	bar.sync 	0;
	@%p1 bra 	$L__BB15_23;

	ld.shared.f32 	%f333, [%r4];
	mov.b32 	%r268, %f333;
	shfl.sync.bfly.b32 	%r272|%p78, %r268, %r253, %r252, %r254;
	mov.b32 	%f334, %r272;
	add.f32 	%f335, %f333, %f334;
	mov.b32 	%r273, %f335;
	shfl.sync.bfly.b32 	%r275|%p79, %r273, %r257, %r252, %r254;
	mov.b32 	%f336, %r275;
	add.f32 	%f337, %f335, %f336;
	mov.b32 	%r276, %f337;
	shfl.sync.bfly.b32 	%r278|%p80, %r276, %r260, %r252, %r254;
	mov.b32 	%f338, %r278;
	add.f32 	%f339, %f337, %f338;
	mov.b32 	%r279, %f339;
	shfl.sync.bfly.b32 	%r281|%p81, %r279, %r263, %r252, %r254;
	mov.b32 	%f340, %r281;
	add.f32 	%f341, %f339, %f340;
	mov.b32 	%r282, %f341;
	shfl.sync.bfly.b32 	%r284|%p82, %r282, %r266, %r252, %r254;
	mov.b32 	%f342, %r284;
	add.f32 	%f343, %f341, %f342;
	st.local.f32 	[%rd2+24], %f343;

$L__BB15_23:
	bar.sync 	0;
	mov.b32 	%r285, %f382;
	shfl.sync.bfly.b32 	%r289|%p84, %r285, %r253, %r252, %r254;
	mov.b32 	%f344, %r289;
	add.f32 	%f345, %f382, %f344;
	mov.b32 	%r290, %f345;
	shfl.sync.bfly.b32 	%r292|%p85, %r290, %r257, %r252, %r254;
	mov.b32 	%f346, %r292;
	add.f32 	%f347, %f345, %f346;
	mov.b32 	%r293, %f347;
	shfl.sync.bfly.b32 	%r295|%p86, %r293, %r260, %r252, %r254;
	mov.b32 	%f348, %r295;
	add.f32 	%f349, %f347, %f348;
	mov.b32 	%r296, %f349;
	shfl.sync.bfly.b32 	%r298|%p87, %r296, %r263, %r252, %r254;
	mov.b32 	%f350, %r298;
	add.f32 	%f351, %f349, %f350;
	mov.b32 	%r299, %f351;
	shfl.sync.bfly.b32 	%r301|%p88, %r299, %r266, %r252, %r254;
	mov.b32 	%f352, %r301;
	add.f32 	%f353, %f351, %f352;
	st.local.f32 	[%rd2+28], %f353;
	st.shared.f32 	[%r10], %f353;
	bar.sync 	0;
	@%p1 bra 	$L__BB15_25;

	ld.shared.f32 	%f354, [%r4];
	mov.b32 	%r302, %f354;
	mov.u32 	%r303, 31;
	mov.u32 	%r304, 16;
	mov.u32 	%r305, -1;
	shfl.sync.bfly.b32 	%r306|%p89, %r302, %r304, %r303, %r305;
	mov.b32 	%f355, %r306;
	add.f32 	%f356, %f354, %f355;
	mov.b32 	%r307, %f356;
	mov.u32 	%r308, 8;
	shfl.sync.bfly.b32 	%r309|%p90, %r307, %r308, %r303, %r305;
	mov.b32 	%f357, %r309;
	add.f32 	%f358, %f356, %f357;
	mov.b32 	%r310, %f358;
	mov.u32 	%r311, 4;
	shfl.sync.bfly.b32 	%r312|%p91, %r310, %r311, %r303, %r305;
	mov.b32 	%f359, %r312;
	add.f32 	%f360, %f358, %f359;
	mov.b32 	%r313, %f360;
	mov.u32 	%r314, 2;
	shfl.sync.bfly.b32 	%r315|%p92, %r313, %r314, %r303, %r305;
	mov.b32 	%f361, %r315;
	add.f32 	%f362, %f360, %f361;
	mov.b32 	%r316, %f362;
	mov.u32 	%r317, 1;
	shfl.sync.bfly.b32 	%r318|%p93, %r316, %r317, %r303, %r305;
	mov.b32 	%f363, %r318;
	add.f32 	%f364, %f362, %f363;
	st.local.f32 	[%rd2+28], %f364;

$L__BB15_25:
	bar.sync 	0;
	setp.gt.s32 	%p94, %r3, 7;
	@%p94 bra 	$L__BB15_27;

	mul.wide.s32 	%rd65, %r3, 4;
	add.s64 	%rd66, %rd2, %rd65;
	ld.local.f32 	%f365, [%rd66];
	mad.lo.s32 	%r319, %r3, %r13, %r2;
	cvt.s64.s32 	%rd67, %r319;
	mul.lo.s32 	%r320, %r1, %r14;
	cvt.s64.s32 	%rd68, %r320;
	add.s64 	%rd69, %rd68, %rd67;
	cvta.to.global.u64 	%rd70, %rd21;
	shl.b64 	%rd71, %rd69, 2;
	add.s64 	%rd72, %rd70, %rd71;
	st.global.f32 	[%rd72], %f365;

$L__BB15_27:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_1_bs_96
.visible .entry ggml_matvec_f32_ncols_1_bs_96(
	.param .u64 ggml_matvec_f32_ncols_1_bs_96_param_0,
	.param .u64 ggml_matvec_f32_ncols_1_bs_96_param_1,
	.param .u64 ggml_matvec_f32_ncols_1_bs_96_param_2,
	.param .u32 ggml_matvec_f32_ncols_1_bs_96_param_3,
	.param .u32 ggml_matvec_f32_ncols_1_bs_96_param_4,
	.param .u32 ggml_matvec_f32_ncols_1_bs_96_param_5,
	.param .u32 ggml_matvec_f32_ncols_1_bs_96_param_6,
	.param .u32 ggml_matvec_f32_ncols_1_bs_96_param_7,
	.param .u32 ggml_matvec_f32_ncols_1_bs_96_param_8,
	.param .u32 ggml_matvec_f32_ncols_1_bs_96_param_9,
	.param .u32 ggml_matvec_f32_ncols_1_bs_96_param_10,
	.param .u32 ggml_matvec_f32_ncols_1_bs_96_param_11
)
{
	.reg .pred 	%p<19>;
	.reg .f32 	%f<88>;
	.reg .b32 	%r<79>;
	.reg .b64 	%rd<44>;


	ld.param.u64 	%rd18, [ggml_matvec_f32_ncols_1_bs_96_param_0];
	ld.param.u64 	%rd19, [ggml_matvec_f32_ncols_1_bs_96_param_1];
	ld.param.u64 	%rd17, [ggml_matvec_f32_ncols_1_bs_96_param_2];
	ld.param.u32 	%r13, [ggml_matvec_f32_ncols_1_bs_96_param_3];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_1_bs_96_param_5];
	ld.param.u32 	%r14, [ggml_matvec_f32_ncols_1_bs_96_param_7];
	ld.param.u32 	%r17, [ggml_matvec_f32_ncols_1_bs_96_param_8];
	ld.param.u32 	%r18, [ggml_matvec_f32_ncols_1_bs_96_param_9];
	ld.param.u32 	%r19, [ggml_matvec_f32_ncols_1_bs_96_param_10];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_1_bs_96_param_11];
	cvta.to.global.u64 	%rd1, %rd19;
	cvta.to.global.u64 	%rd2, %rd18;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r20, %r1, %r17;
	mov.u32 	%r21, %ctaid.x;
	mul.lo.s32 	%r22, %r21, %r16;
	mad.lo.s32 	%r23, %r20, %r18, %r22;
	cvt.s64.s32 	%rd3, %r23;
	mul.lo.s32 	%r24, %r1, %r19;
	cvt.s64.s32 	%rd4, %r24;
	mov.u32 	%r2, %tid.x;
	setp.gt.s32 	%p1, %r2, 31;
	shl.b32 	%r25, %r2, 2;
	mov.u32 	%r26, data_mmv;
	add.s32 	%r3, %r26, %r25;
	@%p1 bra 	$L__BB16_2;

	mov.u32 	%r27, 0;
	st.shared.u32 	[%r3], %r27;

$L__BB16_2:
	bar.sync 	0;
	setp.ge.s32 	%p2, %r2, %r13;
	mov.f32 	%f86, 0f00000000;
	@%p2 bra 	$L__BB16_9;

	not.b32 	%r28, %r2;
	add.s32 	%r4, %r28, %r13;
	mul.wide.u32 	%rd20, %r4, -1431655765;
	shr.u64 	%rd21, %rd20, 38;
	cvt.u32.u64 	%r29, %rd21;
	add.s32 	%r30, %r29, 1;
	and.b32  	%r76, %r30, 3;
	setp.eq.s32 	%p3, %r76, 0;
	mov.f32 	%f86, 0f00000000;
	mov.u32 	%r77, %r2;
	@%p3 bra 	$L__BB16_6;

	mul.wide.s32 	%rd22, %r2, 2;
	add.s64 	%rd23, %rd22, %rd4;
	shl.b64 	%rd24, %rd23, 2;
	add.s64 	%rd41, %rd1, %rd24;
	add.s64 	%rd25, %rd22, %rd3;
	shl.b64 	%rd26, %rd25, 2;
	add.s64 	%rd40, %rd2, %rd26;
	mov.f32 	%f86, 0f00000000;
	mov.u32 	%r77, %r2;

$L__BB16_5:
	.pragma "nounroll";
	ld.global.nc.v2.f32 	{%f15, %f16}, [%rd40];
	ld.global.nc.v2.f32 	{%f19, %f20}, [%rd41];
	fma.rn.f32 	%f23, %f15, %f19, %f86;
	fma.rn.f32 	%f86, %f16, %f20, %f23;
	add.s32 	%r77, %r77, 96;
	add.s64 	%rd41, %rd41, 768;
	add.s64 	%rd40, %rd40, 768;
	add.s32 	%r76, %r76, -1;
	setp.ne.s32 	%p4, %r76, 0;
	@%p4 bra 	$L__BB16_5;

$L__BB16_6:
	setp.lt.u32 	%p5, %r4, 288;
	@%p5 bra 	$L__BB16_9;

	mul.wide.s32 	%rd27, %r77, 2;
	add.s64 	%rd28, %rd27, %rd3;
	shl.b64 	%rd29, %rd28, 2;
	add.s64 	%rd30, %rd2, %rd29;
	add.s64 	%rd43, %rd30, 1536;
	add.s64 	%rd31, %rd27, %rd4;
	shl.b64 	%rd32, %rd31, 2;
	add.s64 	%rd33, %rd1, %rd32;
	add.s64 	%rd42, %rd33, 1536;

$L__BB16_8:
	ld.global.nc.v2.f32 	{%f24, %f25}, [%rd43+-1536];
	ld.global.nc.v2.f32 	{%f28, %f29}, [%rd42+-1536];
	fma.rn.f32 	%f32, %f24, %f28, %f86;
	fma.rn.f32 	%f33, %f25, %f29, %f32;
	ld.global.nc.v2.f32 	{%f34, %f35}, [%rd43+-768];
	ld.global.nc.v2.f32 	{%f38, %f39}, [%rd42+-768];
	fma.rn.f32 	%f42, %f34, %f38, %f33;
	fma.rn.f32 	%f43, %f35, %f39, %f42;
	ld.global.nc.v2.f32 	{%f44, %f45}, [%rd43];
	ld.global.nc.v2.f32 	{%f48, %f49}, [%rd42];
	fma.rn.f32 	%f52, %f44, %f48, %f43;
	fma.rn.f32 	%f53, %f45, %f49, %f52;
	ld.global.nc.v2.f32 	{%f54, %f55}, [%rd43+768];
	ld.global.nc.v2.f32 	{%f58, %f59}, [%rd42+768];
	fma.rn.f32 	%f62, %f54, %f58, %f53;
	fma.rn.f32 	%f86, %f55, %f59, %f62;
	add.s64 	%rd43, %rd43, 3072;
	add.s64 	%rd42, %rd42, 3072;
	add.s32 	%r77, %r77, 384;
	setp.lt.s32 	%p6, %r77, %r13;
	@%p6 bra 	$L__BB16_8;

$L__BB16_9:
	mov.b32 	%r31, %f86;
	mov.u32 	%r32, 31;
	mov.u32 	%r33, 16;
	mov.u32 	%r34, -1;
	shfl.sync.bfly.b32 	%r35|%p7, %r31, %r33, %r32, %r34;
	mov.b32 	%f63, %r35;
	add.f32 	%f64, %f86, %f63;
	mov.b32 	%r36, %f64;
	mov.u32 	%r37, 8;
	shfl.sync.bfly.b32 	%r38|%p8, %r36, %r37, %r32, %r34;
	mov.b32 	%f65, %r38;
	add.f32 	%f66, %f64, %f65;
	mov.b32 	%r39, %f66;
	mov.u32 	%r40, 4;
	shfl.sync.bfly.b32 	%r41|%p9, %r39, %r40, %r32, %r34;
	mov.b32 	%f67, %r41;
	add.f32 	%f68, %f66, %f67;
	mov.b32 	%r42, %f68;
	mov.u32 	%r43, 2;
	shfl.sync.bfly.b32 	%r44|%p10, %r42, %r43, %r32, %r34;
	mov.b32 	%f69, %r44;
	add.f32 	%f70, %f68, %f69;
	mov.b32 	%r45, %f70;
	mov.u32 	%r46, 1;
	shfl.sync.bfly.b32 	%r47|%p11, %r45, %r46, %r32, %r34;
	mov.b32 	%f71, %r47;
	add.f32 	%f87, %f70, %f71;
	shr.s32 	%r48, %r2, 31;
	shr.u32 	%r49, %r48, 27;
	add.s32 	%r50, %r2, %r49;
	shr.s32 	%r51, %r50, 5;
	shl.b32 	%r52, %r51, 2;
	add.s32 	%r54, %r26, %r52;
	st.shared.f32 	[%r54], %f87;
	bar.sync 	0;
	@%p1 bra 	$L__BB16_11;

	ld.shared.f32 	%f72, [%r3];
	mov.b32 	%r55, %f72;
	shfl.sync.bfly.b32 	%r59|%p13, %r55, %r33, %r32, %r34;
	mov.b32 	%f73, %r59;
	add.f32 	%f74, %f72, %f73;
	mov.b32 	%r60, %f74;
	shfl.sync.bfly.b32 	%r62|%p14, %r60, %r37, %r32, %r34;
	mov.b32 	%f75, %r62;
	add.f32 	%f76, %f74, %f75;
	mov.b32 	%r63, %f76;
	shfl.sync.bfly.b32 	%r65|%p15, %r63, %r40, %r32, %r34;
	mov.b32 	%f77, %r65;
	add.f32 	%f78, %f76, %f77;
	mov.b32 	%r66, %f78;
	shfl.sync.bfly.b32 	%r68|%p16, %r66, %r43, %r32, %r34;
	mov.b32 	%f79, %r68;
	add.f32 	%f80, %f78, %f79;
	mov.b32 	%r69, %f80;
	shfl.sync.bfly.b32 	%r71|%p17, %r69, %r46, %r32, %r34;
	mov.b32 	%f81, %r71;
	add.f32 	%f87, %f80, %f81;

$L__BB16_11:
	bar.sync 	0;
	setp.gt.s32 	%p18, %r2, 0;
	@%p18 bra 	$L__BB16_13;

	mad.lo.s32 	%r73, %r2, %r14, %r21;
	cvt.s64.s32 	%rd34, %r73;
	mul.lo.s32 	%r74, %r1, %r15;
	cvt.s64.s32 	%rd35, %r74;
	add.s64 	%rd36, %rd35, %rd34;
	cvta.to.global.u64 	%rd37, %rd17;
	shl.b64 	%rd38, %rd36, 2;
	add.s64 	%rd39, %rd37, %rd38;
	st.global.f32 	[%rd39], %f87;

$L__BB16_13:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_2_bs_96
.visible .entry ggml_matvec_f32_ncols_2_bs_96(
	.param .u64 ggml_matvec_f32_ncols_2_bs_96_param_0,
	.param .u64 ggml_matvec_f32_ncols_2_bs_96_param_1,
	.param .u64 ggml_matvec_f32_ncols_2_bs_96_param_2,
	.param .u32 ggml_matvec_f32_ncols_2_bs_96_param_3,
	.param .u32 ggml_matvec_f32_ncols_2_bs_96_param_4,
	.param .u32 ggml_matvec_f32_ncols_2_bs_96_param_5,
	.param .u32 ggml_matvec_f32_ncols_2_bs_96_param_6,
	.param .u32 ggml_matvec_f32_ncols_2_bs_96_param_7,
	.param .u32 ggml_matvec_f32_ncols_2_bs_96_param_8,
	.param .u32 ggml_matvec_f32_ncols_2_bs_96_param_9,
	.param .u32 ggml_matvec_f32_ncols_2_bs_96_param_10,
	.param .u32 ggml_matvec_f32_ncols_2_bs_96_param_11
)
{
	.local .align 8 .b8 	__local_depot17[8];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<30>;
	.reg .f32 	%f<146>;
	.reg .b32 	%r<113>;
	.reg .b64 	%rd<66>;


	mov.u64 	%SPL, __local_depot17;
	ld.param.u64 	%rd27, [ggml_matvec_f32_ncols_2_bs_96_param_0];
	ld.param.u64 	%rd28, [ggml_matvec_f32_ncols_2_bs_96_param_1];
	ld.param.u64 	%rd26, [ggml_matvec_f32_ncols_2_bs_96_param_2];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_2_bs_96_param_3];
	ld.param.u32 	%r19, [ggml_matvec_f32_ncols_2_bs_96_param_5];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_2_bs_96_param_6];
	ld.param.u32 	%r17, [ggml_matvec_f32_ncols_2_bs_96_param_7];
	ld.param.u32 	%r20, [ggml_matvec_f32_ncols_2_bs_96_param_8];
	ld.param.u32 	%r21, [ggml_matvec_f32_ncols_2_bs_96_param_9];
	ld.param.u32 	%r22, [ggml_matvec_f32_ncols_2_bs_96_param_10];
	ld.param.u32 	%r18, [ggml_matvec_f32_ncols_2_bs_96_param_11];
	cvta.to.global.u64 	%rd1, %rd28;
	cvta.to.global.u64 	%rd2, %rd27;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r23, %r1, %r20;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r24, %r2, %r19;
	mad.lo.s32 	%r25, %r23, %r21, %r24;
	cvt.s64.s32 	%rd4, %r25;
	mul.lo.s32 	%r26, %r1, %r22;
	cvt.s64.s32 	%rd5, %r26;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r27, %r3, 2;
	mov.u32 	%r28, data_mmv;
	add.s32 	%r4, %r28, %r27;
	@%p1 bra 	$L__BB17_2;

	mov.u32 	%r29, 0;
	st.shared.u32 	[%r4], %r29;

$L__BB17_2:
	bar.sync 	0;
	mov.f32 	%f144, 0f00000000;
	st.local.v2.f32 	[%rd3], {%f144, %f144};
	setp.ge.s32 	%p2, %r3, %r15;
	mov.f32 	%f145, %f144;
	@%p2 bra 	$L__BB17_11;

	not.b32 	%r30, %r3;
	add.s32 	%r5, %r30, %r15;
	mul.wide.u32 	%rd30, %r5, -1431655765;
	shr.u64 	%rd31, %rd30, 38;
	cvt.u32.u64 	%r31, %rd31;
	add.s32 	%r32, %r31, 1;
	and.b32  	%r110, %r32, 3;
	setp.eq.s32 	%p3, %r110, 0;
	mov.f32 	%f144, 0f00000000;
	mov.u32 	%r111, %r3;
	@%p3 bra 	$L__BB17_7;

	mul.wide.s32 	%rd32, %r16, 2;
	mul.wide.s32 	%rd33, %r3, 2;
	add.s64 	%rd34, %rd32, %rd33;
	add.s64 	%rd35, %rd34, %rd5;
	shl.b64 	%rd36, %rd35, 2;
	add.s64 	%rd62, %rd1, %rd36;
	add.s64 	%rd37, %rd33, %rd5;
	shl.b64 	%rd38, %rd37, 2;
	add.s64 	%rd61, %rd1, %rd38;
	add.s64 	%rd39, %rd33, %rd4;
	shl.b64 	%rd40, %rd39, 2;
	add.s64 	%rd60, %rd2, %rd40;
	mov.f32 	%f144, 0f00000000;
	mov.f32 	%f145, %f144;
	mov.u32 	%r111, %r3;

$L__BB17_5:
	.pragma "nounroll";
	ld.global.nc.v2.f32 	{%f19, %f20}, [%rd60];
	ld.global.nc.v2.f32 	{%f23, %f24}, [%rd61];
	fma.rn.f32 	%f27, %f19, %f23, %f145;
	fma.rn.f32 	%f145, %f20, %f24, %f27;
	ld.global.nc.v2.f32 	{%f28, %f29}, [%rd62];
	fma.rn.f32 	%f32, %f19, %f28, %f144;
	fma.rn.f32 	%f144, %f20, %f29, %f32;
	add.s32 	%r111, %r111, 96;
	add.s64 	%rd62, %rd62, 768;
	add.s64 	%rd61, %rd61, 768;
	add.s64 	%rd60, %rd60, 768;
	add.s32 	%r110, %r110, -1;
	setp.ne.s32 	%p4, %r110, 0;
	@%p4 bra 	$L__BB17_5;

	st.local.v2.f32 	[%rd3], {%f145, %f144};

$L__BB17_7:
	setp.lt.u32 	%p5, %r5, 288;
	@%p5 bra 	$L__BB17_11;

	mul.wide.s32 	%rd41, %r111, 2;
	add.s64 	%rd42, %rd41, %rd4;
	shl.b64 	%rd43, %rd42, 2;
	add.s64 	%rd44, %rd2, %rd43;
	add.s64 	%rd65, %rd44, 1536;
	add.s64 	%rd45, %rd41, %rd5;
	shl.b64 	%rd46, %rd45, 2;
	add.s64 	%rd47, %rd1, %rd46;
	add.s64 	%rd64, %rd47, 2304;
	mul.wide.s32 	%rd48, %r16, 2;
	add.s64 	%rd49, %rd45, %rd48;
	shl.b64 	%rd50, %rd49, 2;
	add.s64 	%rd51, %rd1, %rd50;
	add.s64 	%rd63, %rd51, 1536;

$L__BB17_9:
	ld.global.nc.v2.f32 	{%f33, %f34}, [%rd65+-1536];
	ld.global.nc.v2.f32 	{%f37, %f38}, [%rd64+-2304];
	fma.rn.f32 	%f41, %f33, %f37, %f145;
	fma.rn.f32 	%f42, %f34, %f38, %f41;
	ld.global.nc.v2.f32 	{%f43, %f44}, [%rd63+-1536];
	fma.rn.f32 	%f47, %f33, %f43, %f144;
	fma.rn.f32 	%f48, %f34, %f44, %f47;
	ld.global.nc.v2.f32 	{%f49, %f50}, [%rd65+-768];
	ld.global.nc.v2.f32 	{%f53, %f54}, [%rd64+-1536];
	fma.rn.f32 	%f57, %f49, %f53, %f42;
	fma.rn.f32 	%f58, %f50, %f54, %f57;
	ld.global.nc.v2.f32 	{%f59, %f60}, [%rd63+-768];
	fma.rn.f32 	%f63, %f49, %f59, %f48;
	fma.rn.f32 	%f64, %f50, %f60, %f63;
	ld.global.nc.v2.f32 	{%f65, %f66}, [%rd65];
	ld.global.nc.v2.f32 	{%f69, %f70}, [%rd64+-768];
	fma.rn.f32 	%f73, %f65, %f69, %f58;
	fma.rn.f32 	%f74, %f66, %f70, %f73;
	ld.global.nc.v2.f32 	{%f75, %f76}, [%rd63];
	fma.rn.f32 	%f79, %f65, %f75, %f64;
	fma.rn.f32 	%f80, %f66, %f76, %f79;
	ld.global.nc.v2.f32 	{%f81, %f82}, [%rd65+768];
	ld.global.nc.v2.f32 	{%f85, %f86}, [%rd64];
	fma.rn.f32 	%f89, %f81, %f85, %f74;
	fma.rn.f32 	%f145, %f82, %f86, %f89;
	ld.global.nc.v2.f32 	{%f90, %f91}, [%rd63+768];
	fma.rn.f32 	%f94, %f81, %f90, %f80;
	fma.rn.f32 	%f144, %f82, %f91, %f94;
	add.s64 	%rd65, %rd65, 3072;
	add.s64 	%rd64, %rd64, 3072;
	add.s64 	%rd63, %rd63, 3072;
	add.s32 	%r111, %r111, 384;
	setp.lt.s32 	%p6, %r111, %r15;
	@%p6 bra 	$L__BB17_9;

	st.local.v2.f32 	[%rd3], {%f145, %f144};

$L__BB17_11:
	shr.s32 	%r33, %r3, 31;
	shr.u32 	%r34, %r33, 27;
	add.s32 	%r35, %r3, %r34;
	shr.s32 	%r36, %r35, 5;
	shl.b32 	%r37, %r36, 2;
	add.s32 	%r14, %r28, %r37;
	mov.u32 	%r39, 2;
	mov.b32 	%r40, %f145;
	mov.u32 	%r41, 31;
	mov.u32 	%r42, 16;
	mov.u32 	%r43, -1;
	shfl.sync.bfly.b32 	%r44|%p7, %r40, %r42, %r41, %r43;
	mov.b32 	%f95, %r44;
	add.f32 	%f96, %f145, %f95;
	mov.b32 	%r45, %f96;
	mov.u32 	%r46, 8;
	shfl.sync.bfly.b32 	%r47|%p8, %r45, %r46, %r41, %r43;
	mov.b32 	%f97, %r47;
	add.f32 	%f98, %f96, %f97;
	mov.b32 	%r48, %f98;
	mov.u32 	%r49, 4;
	shfl.sync.bfly.b32 	%r50|%p9, %r48, %r49, %r41, %r43;
	mov.b32 	%f99, %r50;
	add.f32 	%f100, %f98, %f99;
	mov.b32 	%r51, %f100;
	shfl.sync.bfly.b32 	%r52|%p10, %r51, %r39, %r41, %r43;
	mov.b32 	%f101, %r52;
	add.f32 	%f102, %f100, %f101;
	mov.b32 	%r53, %f102;
	mov.u32 	%r54, 1;
	shfl.sync.bfly.b32 	%r55|%p11, %r53, %r54, %r41, %r43;
	mov.b32 	%f103, %r55;
	add.f32 	%f104, %f102, %f103;
	st.local.f32 	[%rd3], %f104;
	st.shared.f32 	[%r14], %f104;
	bar.sync 	0;
	@%p1 bra 	$L__BB17_13;

	ld.shared.f32 	%f105, [%r4];
	mov.b32 	%r56, %f105;
	shfl.sync.bfly.b32 	%r60|%p13, %r56, %r42, %r41, %r43;
	mov.b32 	%f106, %r60;
	add.f32 	%f107, %f105, %f106;
	mov.b32 	%r61, %f107;
	shfl.sync.bfly.b32 	%r63|%p14, %r61, %r46, %r41, %r43;
	mov.b32 	%f108, %r63;
	add.f32 	%f109, %f107, %f108;
	mov.b32 	%r64, %f109;
	shfl.sync.bfly.b32 	%r66|%p15, %r64, %r49, %r41, %r43;
	mov.b32 	%f110, %r66;
	add.f32 	%f111, %f109, %f110;
	mov.b32 	%r67, %f111;
	shfl.sync.bfly.b32 	%r69|%p16, %r67, %r39, %r41, %r43;
	mov.b32 	%f112, %r69;
	add.f32 	%f113, %f111, %f112;
	mov.b32 	%r70, %f113;
	shfl.sync.bfly.b32 	%r72|%p17, %r70, %r54, %r41, %r43;
	mov.b32 	%f114, %r72;
	add.f32 	%f115, %f113, %f114;
	st.local.f32 	[%rd3], %f115;

$L__BB17_13:
	bar.sync 	0;
	mov.b32 	%r73, %f144;
	shfl.sync.bfly.b32 	%r77|%p19, %r73, %r42, %r41, %r43;
	mov.b32 	%f116, %r77;
	add.f32 	%f117, %f144, %f116;
	mov.b32 	%r78, %f117;
	shfl.sync.bfly.b32 	%r80|%p20, %r78, %r46, %r41, %r43;
	mov.b32 	%f118, %r80;
	add.f32 	%f119, %f117, %f118;
	mov.b32 	%r81, %f119;
	shfl.sync.bfly.b32 	%r83|%p21, %r81, %r49, %r41, %r43;
	mov.b32 	%f120, %r83;
	add.f32 	%f121, %f119, %f120;
	mov.b32 	%r84, %f121;
	shfl.sync.bfly.b32 	%r86|%p22, %r84, %r39, %r41, %r43;
	mov.b32 	%f122, %r86;
	add.f32 	%f123, %f121, %f122;
	mov.b32 	%r87, %f123;
	shfl.sync.bfly.b32 	%r89|%p23, %r87, %r54, %r41, %r43;
	mov.b32 	%f124, %r89;
	add.f32 	%f125, %f123, %f124;
	st.local.f32 	[%rd3+4], %f125;
	st.shared.f32 	[%r14], %f125;
	bar.sync 	0;
	@%p1 bra 	$L__BB17_15;

	ld.shared.f32 	%f126, [%r4];
	mov.b32 	%r90, %f126;
	mov.u32 	%r91, 31;
	mov.u32 	%r92, 16;
	mov.u32 	%r93, -1;
	shfl.sync.bfly.b32 	%r94|%p24, %r90, %r92, %r91, %r93;
	mov.b32 	%f127, %r94;
	add.f32 	%f128, %f126, %f127;
	mov.b32 	%r95, %f128;
	mov.u32 	%r96, 8;
	shfl.sync.bfly.b32 	%r97|%p25, %r95, %r96, %r91, %r93;
	mov.b32 	%f129, %r97;
	add.f32 	%f130, %f128, %f129;
	mov.b32 	%r98, %f130;
	mov.u32 	%r99, 4;
	shfl.sync.bfly.b32 	%r100|%p26, %r98, %r99, %r91, %r93;
	mov.b32 	%f131, %r100;
	add.f32 	%f132, %f130, %f131;
	mov.b32 	%r101, %f132;
	mov.u32 	%r102, 2;
	shfl.sync.bfly.b32 	%r103|%p27, %r101, %r102, %r91, %r93;
	mov.b32 	%f133, %r103;
	add.f32 	%f134, %f132, %f133;
	mov.b32 	%r104, %f134;
	mov.u32 	%r105, 1;
	shfl.sync.bfly.b32 	%r106|%p28, %r104, %r105, %r91, %r93;
	mov.b32 	%f135, %r106;
	add.f32 	%f136, %f134, %f135;
	st.local.f32 	[%rd3+4], %f136;

$L__BB17_15:
	bar.sync 	0;
	setp.gt.s32 	%p29, %r3, 1;
	@%p29 bra 	$L__BB17_17;

	mul.wide.s32 	%rd52, %r3, 4;
	add.s64 	%rd53, %rd3, %rd52;
	ld.local.f32 	%f137, [%rd53];
	mad.lo.s32 	%r107, %r3, %r17, %r2;
	cvt.s64.s32 	%rd54, %r107;
	mul.lo.s32 	%r108, %r1, %r18;
	cvt.s64.s32 	%rd55, %r108;
	add.s64 	%rd56, %rd55, %rd54;
	cvta.to.global.u64 	%rd57, %rd26;
	shl.b64 	%rd58, %rd56, 2;
	add.s64 	%rd59, %rd57, %rd58;
	st.global.f32 	[%rd59], %f137;

$L__BB17_17:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_3_bs_96
.visible .entry ggml_matvec_f32_ncols_3_bs_96(
	.param .u64 ggml_matvec_f32_ncols_3_bs_96_param_0,
	.param .u64 ggml_matvec_f32_ncols_3_bs_96_param_1,
	.param .u64 ggml_matvec_f32_ncols_3_bs_96_param_2,
	.param .u32 ggml_matvec_f32_ncols_3_bs_96_param_3,
	.param .u32 ggml_matvec_f32_ncols_3_bs_96_param_4,
	.param .u32 ggml_matvec_f32_ncols_3_bs_96_param_5,
	.param .u32 ggml_matvec_f32_ncols_3_bs_96_param_6,
	.param .u32 ggml_matvec_f32_ncols_3_bs_96_param_7,
	.param .u32 ggml_matvec_f32_ncols_3_bs_96_param_8,
	.param .u32 ggml_matvec_f32_ncols_3_bs_96_param_9,
	.param .u32 ggml_matvec_f32_ncols_3_bs_96_param_10,
	.param .u32 ggml_matvec_f32_ncols_3_bs_96_param_11
)
{
	.local .align 4 .b8 	__local_depot18[12];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<41>;
	.reg .f32 	%f<208>;
	.reg .b32 	%r<154>;
	.reg .b64 	%rd<74>;


	mov.u64 	%SPL, __local_depot18;
	ld.param.u64 	%rd29, [ggml_matvec_f32_ncols_3_bs_96_param_0];
	ld.param.u64 	%rd30, [ggml_matvec_f32_ncols_3_bs_96_param_1];
	ld.param.u64 	%rd28, [ggml_matvec_f32_ncols_3_bs_96_param_2];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_3_bs_96_param_3];
	ld.param.u32 	%r19, [ggml_matvec_f32_ncols_3_bs_96_param_5];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_3_bs_96_param_6];
	ld.param.u32 	%r17, [ggml_matvec_f32_ncols_3_bs_96_param_7];
	ld.param.u32 	%r20, [ggml_matvec_f32_ncols_3_bs_96_param_8];
	ld.param.u32 	%r21, [ggml_matvec_f32_ncols_3_bs_96_param_9];
	ld.param.u32 	%r22, [ggml_matvec_f32_ncols_3_bs_96_param_10];
	ld.param.u32 	%r18, [ggml_matvec_f32_ncols_3_bs_96_param_11];
	cvta.to.global.u64 	%rd73, %rd30;
	cvta.to.global.u64 	%rd2, %rd29;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r23, %r1, %r20;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r24, %r2, %r19;
	mad.lo.s32 	%r25, %r23, %r21, %r24;
	cvt.s64.s32 	%rd4, %r25;
	mul.lo.s32 	%r26, %r1, %r22;
	cvt.s64.s32 	%rd5, %r26;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r27, %r3, 2;
	mov.u32 	%r28, data_mmv;
	add.s32 	%r4, %r28, %r27;
	@%p1 bra 	$L__BB18_2;

	mov.u32 	%r29, 0;
	st.shared.u32 	[%r4], %r29;

$L__BB18_2:
	bar.sync 	0;
	mov.f32 	%f205, 0f00000000;
	mov.u32 	%r30, 0;
	st.local.u32 	[%rd3], %r30;
	st.local.u32 	[%rd3+4], %r30;
	st.local.u32 	[%rd3+8], %r30;
	setp.ge.s32 	%p2, %r3, %r15;
	mov.f32 	%f206, %f205;
	mov.f32 	%f207, %f205;
	@%p2 bra 	$L__BB18_11;

	not.b32 	%r31, %r3;
	add.s32 	%r5, %r31, %r15;
	mul.wide.u32 	%rd32, %r5, -1431655765;
	shr.u64 	%rd33, %rd32, 38;
	cvt.u32.u64 	%r32, %rd33;
	add.s32 	%r33, %r32, 1;
	and.b32  	%r151, %r33, 3;
	setp.eq.s32 	%p3, %r151, 0;
	mov.f32 	%f205, 0f00000000;
	mov.u32 	%r152, %r3;
	@%p3 bra 	$L__BB18_7;

	shl.b32 	%r34, %r16, 1;
	add.s32 	%r35, %r3, %r34;
	mul.wide.s32 	%rd34, %r35, 2;
	add.s64 	%rd35, %rd34, %rd5;
	shl.b64 	%rd36, %rd35, 2;
	add.s64 	%rd71, %rd73, %rd36;
	mul.wide.s32 	%rd37, %r16, 2;
	mul.wide.s32 	%rd38, %r3, 2;
	add.s64 	%rd39, %rd37, %rd38;
	add.s64 	%rd40, %rd39, %rd5;
	shl.b64 	%rd41, %rd40, 2;
	add.s64 	%rd70, %rd73, %rd41;
	add.s64 	%rd42, %rd38, %rd5;
	shl.b64 	%rd43, %rd42, 2;
	add.s64 	%rd69, %rd73, %rd43;
	add.s64 	%rd44, %rd38, %rd4;
	shl.b64 	%rd45, %rd44, 2;
	add.s64 	%rd68, %rd2, %rd45;
	mov.f32 	%f205, 0f00000000;
	mov.f32 	%f206, %f205;
	mov.f32 	%f207, %f205;
	mov.u32 	%r152, %r3;

$L__BB18_5:
	.pragma "nounroll";
	ld.global.nc.v2.f32 	{%f28, %f29}, [%rd68];
	ld.global.nc.v2.f32 	{%f32, %f33}, [%rd69];
	fma.rn.f32 	%f36, %f28, %f32, %f207;
	fma.rn.f32 	%f207, %f29, %f33, %f36;
	ld.global.nc.v2.f32 	{%f37, %f38}, [%rd70];
	fma.rn.f32 	%f41, %f28, %f37, %f206;
	fma.rn.f32 	%f206, %f29, %f38, %f41;
	ld.global.nc.v2.f32 	{%f42, %f43}, [%rd71];
	fma.rn.f32 	%f46, %f28, %f42, %f205;
	fma.rn.f32 	%f205, %f29, %f43, %f46;
	add.s32 	%r152, %r152, 96;
	add.s64 	%rd71, %rd71, 768;
	add.s64 	%rd70, %rd70, 768;
	add.s64 	%rd69, %rd69, 768;
	add.s64 	%rd68, %rd68, 768;
	add.s32 	%r151, %r151, -1;
	setp.ne.s32 	%p4, %r151, 0;
	@%p4 bra 	$L__BB18_5;

	st.local.f32 	[%rd3], %f207;
	st.local.f32 	[%rd3+4], %f206;
	st.local.f32 	[%rd3+8], %f205;

$L__BB18_7:
	setp.lt.u32 	%p5, %r5, 288;
	@%p5 bra 	$L__BB18_11;

	add.s32 	%r36, %r152, %r16;
	shl.b32 	%r37, %r16, 1;
	add.s32 	%r38, %r152, %r37;
	add.s32 	%r39, %r36, 96;
	mul.wide.s32 	%rd46, %r39, 8;
	shl.b64 	%rd47, %rd5, 2;
	add.s64 	%rd19, %rd46, %rd47;
	mul.wide.s32 	%rd48, %r38, 8;
	add.s64 	%rd20, %rd48, %rd47;
	mul.wide.s32 	%rd49, %r152, 2;
	add.s64 	%rd50, %rd49, %rd4;
	shl.b64 	%rd51, %rd50, 2;
	add.s64 	%rd52, %rd2, %rd51;
	add.s64 	%rd72, %rd52, 1536;
	mul.wide.s32 	%rd53, %r152, 8;
	add.s64 	%rd22, %rd53, %rd47;
	mul.wide.s32 	%rd54, %r16, 8;
	add.s64 	%rd55, %rd53, %rd54;
	add.s64 	%rd23, %rd55, %rd47;

$L__BB18_9:
	ld.global.nc.v2.f32 	{%f47, %f48}, [%rd72+-1536];
	add.s64 	%rd56, %rd73, %rd22;
	ld.global.nc.v2.f32 	{%f51, %f52}, [%rd56];
	fma.rn.f32 	%f55, %f47, %f51, %f207;
	fma.rn.f32 	%f56, %f48, %f52, %f55;
	add.s64 	%rd57, %rd73, %rd23;
	ld.global.nc.v2.f32 	{%f57, %f58}, [%rd57];
	fma.rn.f32 	%f61, %f47, %f57, %f206;
	fma.rn.f32 	%f62, %f48, %f58, %f61;
	add.s64 	%rd58, %rd73, %rd20;
	ld.global.nc.v2.f32 	{%f63, %f64}, [%rd58];
	fma.rn.f32 	%f67, %f47, %f63, %f205;
	fma.rn.f32 	%f68, %f48, %f64, %f67;
	ld.global.nc.v2.f32 	{%f69, %f70}, [%rd72+-768];
	ld.global.nc.v2.f32 	{%f73, %f74}, [%rd56+768];
	fma.rn.f32 	%f77, %f69, %f73, %f56;
	fma.rn.f32 	%f78, %f70, %f74, %f77;
	add.s64 	%rd59, %rd73, %rd19;
	ld.global.nc.v2.f32 	{%f79, %f80}, [%rd59];
	fma.rn.f32 	%f83, %f69, %f79, %f62;
	fma.rn.f32 	%f84, %f70, %f80, %f83;
	ld.global.nc.v2.f32 	{%f85, %f86}, [%rd58+768];
	fma.rn.f32 	%f89, %f69, %f85, %f68;
	fma.rn.f32 	%f90, %f70, %f86, %f89;
	ld.global.nc.v2.f32 	{%f91, %f92}, [%rd72];
	ld.global.nc.v2.f32 	{%f95, %f96}, [%rd56+1536];
	fma.rn.f32 	%f99, %f91, %f95, %f78;
	fma.rn.f32 	%f100, %f92, %f96, %f99;
	ld.global.nc.v2.f32 	{%f101, %f102}, [%rd59+768];
	fma.rn.f32 	%f105, %f91, %f101, %f84;
	fma.rn.f32 	%f106, %f92, %f102, %f105;
	ld.global.nc.v2.f32 	{%f107, %f108}, [%rd58+1536];
	fma.rn.f32 	%f111, %f91, %f107, %f90;
	fma.rn.f32 	%f112, %f92, %f108, %f111;
	ld.global.nc.v2.f32 	{%f113, %f114}, [%rd72+768];
	ld.global.nc.v2.f32 	{%f117, %f118}, [%rd56+2304];
	fma.rn.f32 	%f121, %f113, %f117, %f100;
	fma.rn.f32 	%f207, %f114, %f118, %f121;
	ld.global.nc.v2.f32 	{%f122, %f123}, [%rd59+1536];
	fma.rn.f32 	%f126, %f113, %f122, %f106;
	fma.rn.f32 	%f206, %f114, %f123, %f126;
	ld.global.nc.v2.f32 	{%f127, %f128}, [%rd58+2304];
	fma.rn.f32 	%f131, %f113, %f127, %f112;
	fma.rn.f32 	%f205, %f114, %f128, %f131;
	add.s64 	%rd73, %rd73, 3072;
	add.s64 	%rd72, %rd72, 3072;
	add.s32 	%r152, %r152, 384;
	setp.lt.s32 	%p6, %r152, %r15;
	@%p6 bra 	$L__BB18_9;

	st.local.f32 	[%rd3], %f207;
	st.local.f32 	[%rd3+4], %f206;
	st.local.f32 	[%rd3+8], %f205;

$L__BB18_11:
	shr.s32 	%r40, %r3, 31;
	shr.u32 	%r41, %r40, 27;
	add.s32 	%r42, %r3, %r41;
	shr.s32 	%r43, %r42, 5;
	shl.b32 	%r44, %r43, 2;
	add.s32 	%r14, %r28, %r44;
	mov.u32 	%r46, 2;
	mov.b32 	%r47, %f207;
	mov.u32 	%r48, 31;
	mov.u32 	%r49, 16;
	mov.u32 	%r50, -1;
	shfl.sync.bfly.b32 	%r51|%p7, %r47, %r49, %r48, %r50;
	mov.b32 	%f132, %r51;
	add.f32 	%f133, %f207, %f132;
	mov.b32 	%r52, %f133;
	mov.u32 	%r53, 8;
	shfl.sync.bfly.b32 	%r54|%p8, %r52, %r53, %r48, %r50;
	mov.b32 	%f134, %r54;
	add.f32 	%f135, %f133, %f134;
	mov.b32 	%r55, %f135;
	mov.u32 	%r56, 4;
	shfl.sync.bfly.b32 	%r57|%p9, %r55, %r56, %r48, %r50;
	mov.b32 	%f136, %r57;
	add.f32 	%f137, %f135, %f136;
	mov.b32 	%r58, %f137;
	shfl.sync.bfly.b32 	%r59|%p10, %r58, %r46, %r48, %r50;
	mov.b32 	%f138, %r59;
	add.f32 	%f139, %f137, %f138;
	mov.b32 	%r60, %f139;
	mov.u32 	%r61, 1;
	shfl.sync.bfly.b32 	%r62|%p11, %r60, %r61, %r48, %r50;
	mov.b32 	%f140, %r62;
	add.f32 	%f141, %f139, %f140;
	st.local.f32 	[%rd3], %f141;
	st.shared.f32 	[%r14], %f141;
	bar.sync 	0;
	@%p1 bra 	$L__BB18_13;

	ld.shared.f32 	%f142, [%r4];
	mov.b32 	%r63, %f142;
	shfl.sync.bfly.b32 	%r67|%p13, %r63, %r49, %r48, %r50;
	mov.b32 	%f143, %r67;
	add.f32 	%f144, %f142, %f143;
	mov.b32 	%r68, %f144;
	shfl.sync.bfly.b32 	%r70|%p14, %r68, %r53, %r48, %r50;
	mov.b32 	%f145, %r70;
	add.f32 	%f146, %f144, %f145;
	mov.b32 	%r71, %f146;
	shfl.sync.bfly.b32 	%r73|%p15, %r71, %r56, %r48, %r50;
	mov.b32 	%f147, %r73;
	add.f32 	%f148, %f146, %f147;
	mov.b32 	%r74, %f148;
	shfl.sync.bfly.b32 	%r76|%p16, %r74, %r46, %r48, %r50;
	mov.b32 	%f149, %r76;
	add.f32 	%f150, %f148, %f149;
	mov.b32 	%r77, %f150;
	shfl.sync.bfly.b32 	%r79|%p17, %r77, %r61, %r48, %r50;
	mov.b32 	%f151, %r79;
	add.f32 	%f152, %f150, %f151;
	st.local.f32 	[%rd3], %f152;

$L__BB18_13:
	bar.sync 	0;
	mov.b32 	%r80, %f206;
	shfl.sync.bfly.b32 	%r84|%p19, %r80, %r49, %r48, %r50;
	mov.b32 	%f153, %r84;
	add.f32 	%f154, %f206, %f153;
	mov.b32 	%r85, %f154;
	shfl.sync.bfly.b32 	%r87|%p20, %r85, %r53, %r48, %r50;
	mov.b32 	%f155, %r87;
	add.f32 	%f156, %f154, %f155;
	mov.b32 	%r88, %f156;
	shfl.sync.bfly.b32 	%r90|%p21, %r88, %r56, %r48, %r50;
	mov.b32 	%f157, %r90;
	add.f32 	%f158, %f156, %f157;
	mov.b32 	%r91, %f158;
	shfl.sync.bfly.b32 	%r93|%p22, %r91, %r46, %r48, %r50;
	mov.b32 	%f159, %r93;
	add.f32 	%f160, %f158, %f159;
	mov.b32 	%r94, %f160;
	shfl.sync.bfly.b32 	%r96|%p23, %r94, %r61, %r48, %r50;
	mov.b32 	%f161, %r96;
	add.f32 	%f162, %f160, %f161;
	st.local.f32 	[%rd3+4], %f162;
	st.shared.f32 	[%r14], %f162;
	bar.sync 	0;
	@%p1 bra 	$L__BB18_15;

	ld.shared.f32 	%f163, [%r4];
	mov.b32 	%r97, %f163;
	mov.u32 	%r98, 31;
	mov.u32 	%r99, 16;
	mov.u32 	%r100, -1;
	shfl.sync.bfly.b32 	%r101|%p24, %r97, %r99, %r98, %r100;
	mov.b32 	%f164, %r101;
	add.f32 	%f165, %f163, %f164;
	mov.b32 	%r102, %f165;
	mov.u32 	%r103, 8;
	shfl.sync.bfly.b32 	%r104|%p25, %r102, %r103, %r98, %r100;
	mov.b32 	%f166, %r104;
	add.f32 	%f167, %f165, %f166;
	mov.b32 	%r105, %f167;
	mov.u32 	%r106, 4;
	shfl.sync.bfly.b32 	%r107|%p26, %r105, %r106, %r98, %r100;
	mov.b32 	%f168, %r107;
	add.f32 	%f169, %f167, %f168;
	mov.b32 	%r108, %f169;
	mov.u32 	%r109, 2;
	shfl.sync.bfly.b32 	%r110|%p27, %r108, %r109, %r98, %r100;
	mov.b32 	%f170, %r110;
	add.f32 	%f171, %f169, %f170;
	mov.b32 	%r111, %f171;
	mov.u32 	%r112, 1;
	shfl.sync.bfly.b32 	%r113|%p28, %r111, %r112, %r98, %r100;
	mov.b32 	%f172, %r113;
	add.f32 	%f173, %f171, %f172;
	st.local.f32 	[%rd3+4], %f173;

$L__BB18_15:
	bar.sync 	0;
	mov.b32 	%r114, %f205;
	mov.u32 	%r115, 31;
	mov.u32 	%r116, 16;
	mov.u32 	%r117, -1;
	shfl.sync.bfly.b32 	%r118|%p30, %r114, %r116, %r115, %r117;
	mov.b32 	%f174, %r118;
	add.f32 	%f175, %f205, %f174;
	mov.b32 	%r119, %f175;
	mov.u32 	%r120, 8;
	shfl.sync.bfly.b32 	%r121|%p31, %r119, %r120, %r115, %r117;
	mov.b32 	%f176, %r121;
	add.f32 	%f177, %f175, %f176;
	mov.b32 	%r122, %f177;
	mov.u32 	%r123, 4;
	shfl.sync.bfly.b32 	%r124|%p32, %r122, %r123, %r115, %r117;
	mov.b32 	%f178, %r124;
	add.f32 	%f179, %f177, %f178;
	mov.b32 	%r125, %f179;
	mov.u32 	%r126, 2;
	shfl.sync.bfly.b32 	%r127|%p33, %r125, %r126, %r115, %r117;
	mov.b32 	%f180, %r127;
	add.f32 	%f181, %f179, %f180;
	mov.b32 	%r128, %f181;
	mov.u32 	%r129, 1;
	shfl.sync.bfly.b32 	%r130|%p34, %r128, %r129, %r115, %r117;
	mov.b32 	%f182, %r130;
	add.f32 	%f183, %f181, %f182;
	st.local.f32 	[%rd3+8], %f183;
	st.shared.f32 	[%r14], %f183;
	bar.sync 	0;
	@%p1 bra 	$L__BB18_17;

	ld.shared.f32 	%f184, [%r4];
	mov.b32 	%r131, %f184;
	shfl.sync.bfly.b32 	%r135|%p35, %r131, %r116, %r115, %r117;
	mov.b32 	%f185, %r135;
	add.f32 	%f186, %f184, %f185;
	mov.b32 	%r136, %f186;
	shfl.sync.bfly.b32 	%r138|%p36, %r136, %r120, %r115, %r117;
	mov.b32 	%f187, %r138;
	add.f32 	%f188, %f186, %f187;
	mov.b32 	%r139, %f188;
	shfl.sync.bfly.b32 	%r141|%p37, %r139, %r123, %r115, %r117;
	mov.b32 	%f189, %r141;
	add.f32 	%f190, %f188, %f189;
	mov.b32 	%r142, %f190;
	shfl.sync.bfly.b32 	%r144|%p38, %r142, %r126, %r115, %r117;
	mov.b32 	%f191, %r144;
	add.f32 	%f192, %f190, %f191;
	mov.b32 	%r145, %f192;
	shfl.sync.bfly.b32 	%r147|%p39, %r145, %r129, %r115, %r117;
	mov.b32 	%f193, %r147;
	add.f32 	%f194, %f192, %f193;
	st.local.f32 	[%rd3+8], %f194;

$L__BB18_17:
	bar.sync 	0;
	setp.gt.s32 	%p40, %r3, 2;
	@%p40 bra 	$L__BB18_19;

	mul.wide.s32 	%rd60, %r3, 4;
	add.s64 	%rd61, %rd3, %rd60;
	ld.local.f32 	%f195, [%rd61];
	mad.lo.s32 	%r148, %r3, %r17, %r2;
	cvt.s64.s32 	%rd62, %r148;
	mul.lo.s32 	%r149, %r1, %r18;
	cvt.s64.s32 	%rd63, %r149;
	add.s64 	%rd64, %rd63, %rd62;
	cvta.to.global.u64 	%rd65, %rd28;
	shl.b64 	%rd66, %rd64, 2;
	add.s64 	%rd67, %rd65, %rd66;
	st.global.f32 	[%rd67], %f195;

$L__BB18_19:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_4_bs_96
.visible .entry ggml_matvec_f32_ncols_4_bs_96(
	.param .u64 ggml_matvec_f32_ncols_4_bs_96_param_0,
	.param .u64 ggml_matvec_f32_ncols_4_bs_96_param_1,
	.param .u64 ggml_matvec_f32_ncols_4_bs_96_param_2,
	.param .u32 ggml_matvec_f32_ncols_4_bs_96_param_3,
	.param .u32 ggml_matvec_f32_ncols_4_bs_96_param_4,
	.param .u32 ggml_matvec_f32_ncols_4_bs_96_param_5,
	.param .u32 ggml_matvec_f32_ncols_4_bs_96_param_6,
	.param .u32 ggml_matvec_f32_ncols_4_bs_96_param_7,
	.param .u32 ggml_matvec_f32_ncols_4_bs_96_param_8,
	.param .u32 ggml_matvec_f32_ncols_4_bs_96_param_9,
	.param .u32 ggml_matvec_f32_ncols_4_bs_96_param_10,
	.param .u32 ggml_matvec_f32_ncols_4_bs_96_param_11
)
{
	.local .align 16 .b8 	__local_depot19[16];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<52>;
	.reg .f32 	%f<270>;
	.reg .b32 	%r<189>;
	.reg .b64 	%rd<85>;


	mov.u64 	%SPL, __local_depot19;
	ld.param.u64 	%rd34, [ggml_matvec_f32_ncols_4_bs_96_param_0];
	ld.param.u64 	%rd35, [ggml_matvec_f32_ncols_4_bs_96_param_1];
	ld.param.u64 	%rd33, [ggml_matvec_f32_ncols_4_bs_96_param_2];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_4_bs_96_param_3];
	ld.param.u32 	%r19, [ggml_matvec_f32_ncols_4_bs_96_param_5];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_4_bs_96_param_6];
	ld.param.u32 	%r17, [ggml_matvec_f32_ncols_4_bs_96_param_7];
	ld.param.u32 	%r20, [ggml_matvec_f32_ncols_4_bs_96_param_8];
	ld.param.u32 	%r21, [ggml_matvec_f32_ncols_4_bs_96_param_9];
	ld.param.u32 	%r22, [ggml_matvec_f32_ncols_4_bs_96_param_10];
	ld.param.u32 	%r18, [ggml_matvec_f32_ncols_4_bs_96_param_11];
	cvta.to.global.u64 	%rd84, %rd35;
	cvta.to.global.u64 	%rd2, %rd34;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r23, %r1, %r20;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r24, %r2, %r19;
	mad.lo.s32 	%r25, %r23, %r21, %r24;
	cvt.s64.s32 	%rd4, %r25;
	mul.lo.s32 	%r26, %r1, %r22;
	cvt.s64.s32 	%rd5, %r26;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r27, %r3, 2;
	mov.u32 	%r28, data_mmv;
	add.s32 	%r4, %r28, %r27;
	@%p1 bra 	$L__BB19_2;

	mov.u32 	%r29, 0;
	st.shared.u32 	[%r4], %r29;

$L__BB19_2:
	bar.sync 	0;
	mov.f32 	%f266, 0f00000000;
	st.local.v4.f32 	[%rd3], {%f266, %f266, %f266, %f266};
	setp.ge.s32 	%p2, %r3, %r15;
	mov.f32 	%f267, %f266;
	mov.f32 	%f268, %f266;
	mov.f32 	%f269, %f266;
	@%p2 bra 	$L__BB19_11;

	not.b32 	%r30, %r3;
	add.s32 	%r5, %r30, %r15;
	mul.wide.u32 	%rd37, %r5, -1431655765;
	shr.u64 	%rd38, %rd37, 38;
	cvt.u32.u64 	%r31, %rd38;
	add.s32 	%r32, %r31, 1;
	and.b32  	%r186, %r32, 3;
	setp.eq.s32 	%p3, %r186, 0;
	mov.f32 	%f266, 0f00000000;
	mov.u32 	%r187, %r3;
	@%p3 bra 	$L__BB19_7;

	shl.b32 	%r33, %r16, 1;
	add.s32 	%r34, %r3, %r33;
	mul.wide.s32 	%rd39, %r34, 2;
	add.s64 	%rd40, %rd39, %rd5;
	shl.b64 	%rd41, %rd40, 2;
	add.s64 	%rd82, %rd84, %rd41;
	mad.lo.s32 	%r35, %r16, 3, %r3;
	mul.wide.s32 	%rd42, %r35, 2;
	add.s64 	%rd43, %rd42, %rd5;
	shl.b64 	%rd44, %rd43, 2;
	add.s64 	%rd81, %rd84, %rd44;
	mul.wide.s32 	%rd45, %r16, 2;
	mul.wide.s32 	%rd46, %r3, 2;
	add.s64 	%rd47, %rd45, %rd46;
	add.s64 	%rd48, %rd47, %rd5;
	shl.b64 	%rd49, %rd48, 2;
	add.s64 	%rd80, %rd84, %rd49;
	add.s64 	%rd50, %rd46, %rd5;
	shl.b64 	%rd51, %rd50, 2;
	add.s64 	%rd79, %rd84, %rd51;
	add.s64 	%rd52, %rd46, %rd4;
	shl.b64 	%rd53, %rd52, 2;
	add.s64 	%rd78, %rd2, %rd53;
	mov.f32 	%f266, 0f00000000;
	mov.f32 	%f267, %f266;
	mov.f32 	%f268, %f266;
	mov.f32 	%f269, %f266;
	mov.u32 	%r187, %r3;

$L__BB19_5:
	.pragma "nounroll";
	ld.global.nc.v2.f32 	{%f37, %f38}, [%rd78];
	ld.global.nc.v2.f32 	{%f41, %f42}, [%rd79];
	fma.rn.f32 	%f45, %f37, %f41, %f269;
	fma.rn.f32 	%f269, %f38, %f42, %f45;
	ld.global.nc.v2.f32 	{%f46, %f47}, [%rd80];
	fma.rn.f32 	%f50, %f37, %f46, %f268;
	fma.rn.f32 	%f268, %f38, %f47, %f50;
	ld.global.nc.v2.f32 	{%f51, %f52}, [%rd82];
	fma.rn.f32 	%f55, %f37, %f51, %f267;
	fma.rn.f32 	%f267, %f38, %f52, %f55;
	ld.global.nc.v2.f32 	{%f56, %f57}, [%rd81];
	fma.rn.f32 	%f60, %f37, %f56, %f266;
	fma.rn.f32 	%f266, %f38, %f57, %f60;
	add.s32 	%r187, %r187, 96;
	add.s64 	%rd82, %rd82, 768;
	add.s64 	%rd81, %rd81, 768;
	add.s64 	%rd80, %rd80, 768;
	add.s64 	%rd79, %rd79, 768;
	add.s64 	%rd78, %rd78, 768;
	add.s32 	%r186, %r186, -1;
	setp.ne.s32 	%p4, %r186, 0;
	@%p4 bra 	$L__BB19_5;

	st.local.v4.f32 	[%rd3], {%f269, %f268, %f267, %f266};

$L__BB19_7:
	setp.lt.u32 	%p5, %r5, 288;
	@%p5 bra 	$L__BB19_11;

	add.s32 	%r36, %r187, %r16;
	shl.b32 	%r37, %r16, 1;
	add.s32 	%r38, %r187, %r37;
	mad.lo.s32 	%r39, %r16, 3, %r187;
	add.s32 	%r40, %r36, 96;
	mul.wide.s32 	%rd54, %r40, 8;
	shl.b64 	%rd55, %rd5, 2;
	add.s64 	%rd23, %rd54, %rd55;
	mul.wide.s32 	%rd56, %r38, 8;
	add.s64 	%rd24, %rd56, %rd55;
	mul.wide.s32 	%rd57, %r39, 8;
	add.s64 	%rd25, %rd57, %rd55;
	mul.wide.s32 	%rd58, %r187, 2;
	add.s64 	%rd59, %rd58, %rd4;
	shl.b64 	%rd60, %rd59, 2;
	add.s64 	%rd61, %rd2, %rd60;
	add.s64 	%rd83, %rd61, 1536;
	mul.wide.s32 	%rd62, %r187, 8;
	add.s64 	%rd27, %rd62, %rd55;
	mul.wide.s32 	%rd63, %r16, 8;
	add.s64 	%rd64, %rd62, %rd63;
	add.s64 	%rd28, %rd64, %rd55;

$L__BB19_9:
	ld.global.nc.v2.f32 	{%f61, %f62}, [%rd83+-1536];
	add.s64 	%rd65, %rd84, %rd27;
	ld.global.nc.v2.f32 	{%f65, %f66}, [%rd65];
	fma.rn.f32 	%f69, %f61, %f65, %f269;
	fma.rn.f32 	%f70, %f62, %f66, %f69;
	add.s64 	%rd66, %rd84, %rd28;
	ld.global.nc.v2.f32 	{%f71, %f72}, [%rd66];
	fma.rn.f32 	%f75, %f61, %f71, %f268;
	fma.rn.f32 	%f76, %f62, %f72, %f75;
	add.s64 	%rd67, %rd84, %rd24;
	ld.global.nc.v2.f32 	{%f77, %f78}, [%rd67];
	fma.rn.f32 	%f81, %f61, %f77, %f267;
	fma.rn.f32 	%f82, %f62, %f78, %f81;
	add.s64 	%rd68, %rd84, %rd25;
	ld.global.nc.v2.f32 	{%f83, %f84}, [%rd68];
	fma.rn.f32 	%f87, %f61, %f83, %f266;
	fma.rn.f32 	%f88, %f62, %f84, %f87;
	ld.global.nc.v2.f32 	{%f89, %f90}, [%rd83+-768];
	ld.global.nc.v2.f32 	{%f93, %f94}, [%rd65+768];
	fma.rn.f32 	%f97, %f89, %f93, %f70;
	fma.rn.f32 	%f98, %f90, %f94, %f97;
	add.s64 	%rd69, %rd84, %rd23;
	ld.global.nc.v2.f32 	{%f99, %f100}, [%rd69];
	fma.rn.f32 	%f103, %f89, %f99, %f76;
	fma.rn.f32 	%f104, %f90, %f100, %f103;
	ld.global.nc.v2.f32 	{%f105, %f106}, [%rd67+768];
	fma.rn.f32 	%f109, %f89, %f105, %f82;
	fma.rn.f32 	%f110, %f90, %f106, %f109;
	ld.global.nc.v2.f32 	{%f111, %f112}, [%rd68+768];
	fma.rn.f32 	%f115, %f89, %f111, %f88;
	fma.rn.f32 	%f116, %f90, %f112, %f115;
	ld.global.nc.v2.f32 	{%f117, %f118}, [%rd83];
	ld.global.nc.v2.f32 	{%f121, %f122}, [%rd65+1536];
	fma.rn.f32 	%f125, %f117, %f121, %f98;
	fma.rn.f32 	%f126, %f118, %f122, %f125;
	ld.global.nc.v2.f32 	{%f127, %f128}, [%rd69+768];
	fma.rn.f32 	%f131, %f117, %f127, %f104;
	fma.rn.f32 	%f132, %f118, %f128, %f131;
	ld.global.nc.v2.f32 	{%f133, %f134}, [%rd67+1536];
	fma.rn.f32 	%f137, %f117, %f133, %f110;
	fma.rn.f32 	%f138, %f118, %f134, %f137;
	ld.global.nc.v2.f32 	{%f139, %f140}, [%rd68+1536];
	fma.rn.f32 	%f143, %f117, %f139, %f116;
	fma.rn.f32 	%f144, %f118, %f140, %f143;
	ld.global.nc.v2.f32 	{%f145, %f146}, [%rd83+768];
	ld.global.nc.v2.f32 	{%f149, %f150}, [%rd65+2304];
	fma.rn.f32 	%f153, %f145, %f149, %f126;
	fma.rn.f32 	%f269, %f146, %f150, %f153;
	ld.global.nc.v2.f32 	{%f154, %f155}, [%rd69+1536];
	fma.rn.f32 	%f158, %f145, %f154, %f132;
	fma.rn.f32 	%f268, %f146, %f155, %f158;
	ld.global.nc.v2.f32 	{%f159, %f160}, [%rd67+2304];
	fma.rn.f32 	%f163, %f145, %f159, %f138;
	fma.rn.f32 	%f267, %f146, %f160, %f163;
	ld.global.nc.v2.f32 	{%f164, %f165}, [%rd68+2304];
	fma.rn.f32 	%f168, %f145, %f164, %f144;
	fma.rn.f32 	%f266, %f146, %f165, %f168;
	add.s64 	%rd84, %rd84, 3072;
	add.s64 	%rd83, %rd83, 3072;
	add.s32 	%r187, %r187, 384;
	setp.lt.s32 	%p6, %r187, %r15;
	@%p6 bra 	$L__BB19_9;

	st.local.v4.f32 	[%rd3], {%f269, %f268, %f267, %f266};

$L__BB19_11:
	shr.s32 	%r41, %r3, 31;
	shr.u32 	%r42, %r41, 27;
	add.s32 	%r43, %r3, %r42;
	shr.s32 	%r44, %r43, 5;
	shl.b32 	%r45, %r44, 2;
	add.s32 	%r14, %r28, %r45;
	mov.u32 	%r47, 2;
	mov.b32 	%r48, %f269;
	mov.u32 	%r49, 31;
	mov.u32 	%r50, 16;
	mov.u32 	%r51, -1;
	shfl.sync.bfly.b32 	%r52|%p7, %r48, %r50, %r49, %r51;
	mov.b32 	%f169, %r52;
	add.f32 	%f170, %f269, %f169;
	mov.b32 	%r53, %f170;
	mov.u32 	%r54, 8;
	shfl.sync.bfly.b32 	%r55|%p8, %r53, %r54, %r49, %r51;
	mov.b32 	%f171, %r55;
	add.f32 	%f172, %f170, %f171;
	mov.b32 	%r56, %f172;
	mov.u32 	%r57, 4;
	shfl.sync.bfly.b32 	%r58|%p9, %r56, %r57, %r49, %r51;
	mov.b32 	%f173, %r58;
	add.f32 	%f174, %f172, %f173;
	mov.b32 	%r59, %f174;
	shfl.sync.bfly.b32 	%r60|%p10, %r59, %r47, %r49, %r51;
	mov.b32 	%f175, %r60;
	add.f32 	%f176, %f174, %f175;
	mov.b32 	%r61, %f176;
	mov.u32 	%r62, 1;
	shfl.sync.bfly.b32 	%r63|%p11, %r61, %r62, %r49, %r51;
	mov.b32 	%f177, %r63;
	add.f32 	%f178, %f176, %f177;
	st.local.f32 	[%rd3], %f178;
	st.shared.f32 	[%r14], %f178;
	bar.sync 	0;
	@%p1 bra 	$L__BB19_13;

	ld.shared.f32 	%f179, [%r4];
	mov.b32 	%r64, %f179;
	shfl.sync.bfly.b32 	%r68|%p13, %r64, %r50, %r49, %r51;
	mov.b32 	%f180, %r68;
	add.f32 	%f181, %f179, %f180;
	mov.b32 	%r69, %f181;
	shfl.sync.bfly.b32 	%r71|%p14, %r69, %r54, %r49, %r51;
	mov.b32 	%f182, %r71;
	add.f32 	%f183, %f181, %f182;
	mov.b32 	%r72, %f183;
	shfl.sync.bfly.b32 	%r74|%p15, %r72, %r57, %r49, %r51;
	mov.b32 	%f184, %r74;
	add.f32 	%f185, %f183, %f184;
	mov.b32 	%r75, %f185;
	shfl.sync.bfly.b32 	%r77|%p16, %r75, %r47, %r49, %r51;
	mov.b32 	%f186, %r77;
	add.f32 	%f187, %f185, %f186;
	mov.b32 	%r78, %f187;
	shfl.sync.bfly.b32 	%r80|%p17, %r78, %r62, %r49, %r51;
	mov.b32 	%f188, %r80;
	add.f32 	%f189, %f187, %f188;
	st.local.f32 	[%rd3], %f189;

$L__BB19_13:
	bar.sync 	0;
	mov.b32 	%r81, %f268;
	shfl.sync.bfly.b32 	%r85|%p19, %r81, %r50, %r49, %r51;
	mov.b32 	%f190, %r85;
	add.f32 	%f191, %f268, %f190;
	mov.b32 	%r86, %f191;
	shfl.sync.bfly.b32 	%r88|%p20, %r86, %r54, %r49, %r51;
	mov.b32 	%f192, %r88;
	add.f32 	%f193, %f191, %f192;
	mov.b32 	%r89, %f193;
	shfl.sync.bfly.b32 	%r91|%p21, %r89, %r57, %r49, %r51;
	mov.b32 	%f194, %r91;
	add.f32 	%f195, %f193, %f194;
	mov.b32 	%r92, %f195;
	shfl.sync.bfly.b32 	%r94|%p22, %r92, %r47, %r49, %r51;
	mov.b32 	%f196, %r94;
	add.f32 	%f197, %f195, %f196;
	mov.b32 	%r95, %f197;
	shfl.sync.bfly.b32 	%r97|%p23, %r95, %r62, %r49, %r51;
	mov.b32 	%f198, %r97;
	add.f32 	%f199, %f197, %f198;
	st.local.f32 	[%rd3+4], %f199;
	st.shared.f32 	[%r14], %f199;
	bar.sync 	0;
	@%p1 bra 	$L__BB19_15;

	ld.shared.f32 	%f200, [%r4];
	mov.b32 	%r98, %f200;
	mov.u32 	%r99, 31;
	mov.u32 	%r100, 16;
	mov.u32 	%r101, -1;
	shfl.sync.bfly.b32 	%r102|%p24, %r98, %r100, %r99, %r101;
	mov.b32 	%f201, %r102;
	add.f32 	%f202, %f200, %f201;
	mov.b32 	%r103, %f202;
	mov.u32 	%r104, 8;
	shfl.sync.bfly.b32 	%r105|%p25, %r103, %r104, %r99, %r101;
	mov.b32 	%f203, %r105;
	add.f32 	%f204, %f202, %f203;
	mov.b32 	%r106, %f204;
	mov.u32 	%r107, 4;
	shfl.sync.bfly.b32 	%r108|%p26, %r106, %r107, %r99, %r101;
	mov.b32 	%f205, %r108;
	add.f32 	%f206, %f204, %f205;
	mov.b32 	%r109, %f206;
	mov.u32 	%r110, 2;
	shfl.sync.bfly.b32 	%r111|%p27, %r109, %r110, %r99, %r101;
	mov.b32 	%f207, %r111;
	add.f32 	%f208, %f206, %f207;
	mov.b32 	%r112, %f208;
	mov.u32 	%r113, 1;
	shfl.sync.bfly.b32 	%r114|%p28, %r112, %r113, %r99, %r101;
	mov.b32 	%f209, %r114;
	add.f32 	%f210, %f208, %f209;
	st.local.f32 	[%rd3+4], %f210;

$L__BB19_15:
	bar.sync 	0;
	mov.b32 	%r115, %f267;
	mov.u32 	%r116, 31;
	mov.u32 	%r117, 16;
	mov.u32 	%r118, -1;
	shfl.sync.bfly.b32 	%r119|%p30, %r115, %r117, %r116, %r118;
	mov.b32 	%f211, %r119;
	add.f32 	%f212, %f267, %f211;
	mov.b32 	%r120, %f212;
	mov.u32 	%r121, 8;
	shfl.sync.bfly.b32 	%r122|%p31, %r120, %r121, %r116, %r118;
	mov.b32 	%f213, %r122;
	add.f32 	%f214, %f212, %f213;
	mov.b32 	%r123, %f214;
	mov.u32 	%r124, 4;
	shfl.sync.bfly.b32 	%r125|%p32, %r123, %r124, %r116, %r118;
	mov.b32 	%f215, %r125;
	add.f32 	%f216, %f214, %f215;
	mov.b32 	%r126, %f216;
	mov.u32 	%r127, 2;
	shfl.sync.bfly.b32 	%r128|%p33, %r126, %r127, %r116, %r118;
	mov.b32 	%f217, %r128;
	add.f32 	%f218, %f216, %f217;
	mov.b32 	%r129, %f218;
	mov.u32 	%r130, 1;
	shfl.sync.bfly.b32 	%r131|%p34, %r129, %r130, %r116, %r118;
	mov.b32 	%f219, %r131;
	add.f32 	%f220, %f218, %f219;
	st.local.f32 	[%rd3+8], %f220;
	st.shared.f32 	[%r14], %f220;
	bar.sync 	0;
	@%p1 bra 	$L__BB19_17;

	ld.shared.f32 	%f221, [%r4];
	mov.b32 	%r132, %f221;
	shfl.sync.bfly.b32 	%r136|%p35, %r132, %r117, %r116, %r118;
	mov.b32 	%f222, %r136;
	add.f32 	%f223, %f221, %f222;
	mov.b32 	%r137, %f223;
	shfl.sync.bfly.b32 	%r139|%p36, %r137, %r121, %r116, %r118;
	mov.b32 	%f224, %r139;
	add.f32 	%f225, %f223, %f224;
	mov.b32 	%r140, %f225;
	shfl.sync.bfly.b32 	%r142|%p37, %r140, %r124, %r116, %r118;
	mov.b32 	%f226, %r142;
	add.f32 	%f227, %f225, %f226;
	mov.b32 	%r143, %f227;
	shfl.sync.bfly.b32 	%r145|%p38, %r143, %r127, %r116, %r118;
	mov.b32 	%f228, %r145;
	add.f32 	%f229, %f227, %f228;
	mov.b32 	%r146, %f229;
	shfl.sync.bfly.b32 	%r148|%p39, %r146, %r130, %r116, %r118;
	mov.b32 	%f230, %r148;
	add.f32 	%f231, %f229, %f230;
	st.local.f32 	[%rd3+8], %f231;

$L__BB19_17:
	bar.sync 	0;
	mov.b32 	%r149, %f266;
	shfl.sync.bfly.b32 	%r153|%p41, %r149, %r117, %r116, %r118;
	mov.b32 	%f232, %r153;
	add.f32 	%f233, %f266, %f232;
	mov.b32 	%r154, %f233;
	shfl.sync.bfly.b32 	%r156|%p42, %r154, %r121, %r116, %r118;
	mov.b32 	%f234, %r156;
	add.f32 	%f235, %f233, %f234;
	mov.b32 	%r157, %f235;
	shfl.sync.bfly.b32 	%r159|%p43, %r157, %r124, %r116, %r118;
	mov.b32 	%f236, %r159;
	add.f32 	%f237, %f235, %f236;
	mov.b32 	%r160, %f237;
	shfl.sync.bfly.b32 	%r162|%p44, %r160, %r127, %r116, %r118;
	mov.b32 	%f238, %r162;
	add.f32 	%f239, %f237, %f238;
	mov.b32 	%r163, %f239;
	shfl.sync.bfly.b32 	%r165|%p45, %r163, %r130, %r116, %r118;
	mov.b32 	%f240, %r165;
	add.f32 	%f241, %f239, %f240;
	st.local.f32 	[%rd3+12], %f241;
	st.shared.f32 	[%r14], %f241;
	bar.sync 	0;
	@%p1 bra 	$L__BB19_19;

	ld.shared.f32 	%f242, [%r4];
	mov.b32 	%r166, %f242;
	mov.u32 	%r167, 31;
	mov.u32 	%r168, 16;
	mov.u32 	%r169, -1;
	shfl.sync.bfly.b32 	%r170|%p46, %r166, %r168, %r167, %r169;
	mov.b32 	%f243, %r170;
	add.f32 	%f244, %f242, %f243;
	mov.b32 	%r171, %f244;
	mov.u32 	%r172, 8;
	shfl.sync.bfly.b32 	%r173|%p47, %r171, %r172, %r167, %r169;
	mov.b32 	%f245, %r173;
	add.f32 	%f246, %f244, %f245;
	mov.b32 	%r174, %f246;
	mov.u32 	%r175, 4;
	shfl.sync.bfly.b32 	%r176|%p48, %r174, %r175, %r167, %r169;
	mov.b32 	%f247, %r176;
	add.f32 	%f248, %f246, %f247;
	mov.b32 	%r177, %f248;
	mov.u32 	%r178, 2;
	shfl.sync.bfly.b32 	%r179|%p49, %r177, %r178, %r167, %r169;
	mov.b32 	%f249, %r179;
	add.f32 	%f250, %f248, %f249;
	mov.b32 	%r180, %f250;
	mov.u32 	%r181, 1;
	shfl.sync.bfly.b32 	%r182|%p50, %r180, %r181, %r167, %r169;
	mov.b32 	%f251, %r182;
	add.f32 	%f252, %f250, %f251;
	st.local.f32 	[%rd3+12], %f252;

$L__BB19_19:
	bar.sync 	0;
	setp.gt.s32 	%p51, %r3, 3;
	@%p51 bra 	$L__BB19_21;

	mul.wide.s32 	%rd70, %r3, 4;
	add.s64 	%rd71, %rd3, %rd70;
	ld.local.f32 	%f253, [%rd71];
	mad.lo.s32 	%r183, %r3, %r17, %r2;
	cvt.s64.s32 	%rd72, %r183;
	mul.lo.s32 	%r184, %r1, %r18;
	cvt.s64.s32 	%rd73, %r184;
	add.s64 	%rd74, %rd73, %rd72;
	cvta.to.global.u64 	%rd75, %rd33;
	shl.b64 	%rd76, %rd74, 2;
	add.s64 	%rd77, %rd75, %rd76;
	st.global.f32 	[%rd77], %f253;

$L__BB19_21:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_5_bs_96
.visible .entry ggml_matvec_f32_ncols_5_bs_96(
	.param .u64 ggml_matvec_f32_ncols_5_bs_96_param_0,
	.param .u64 ggml_matvec_f32_ncols_5_bs_96_param_1,
	.param .u64 ggml_matvec_f32_ncols_5_bs_96_param_2,
	.param .u32 ggml_matvec_f32_ncols_5_bs_96_param_3,
	.param .u32 ggml_matvec_f32_ncols_5_bs_96_param_4,
	.param .u32 ggml_matvec_f32_ncols_5_bs_96_param_5,
	.param .u32 ggml_matvec_f32_ncols_5_bs_96_param_6,
	.param .u32 ggml_matvec_f32_ncols_5_bs_96_param_7,
	.param .u32 ggml_matvec_f32_ncols_5_bs_96_param_8,
	.param .u32 ggml_matvec_f32_ncols_5_bs_96_param_9,
	.param .u32 ggml_matvec_f32_ncols_5_bs_96_param_10,
	.param .u32 ggml_matvec_f32_ncols_5_bs_96_param_11
)
{
	.local .align 4 .b8 	__local_depot20[20];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<63>;
	.reg .f32 	%f<332>;
	.reg .b32 	%r<225>;
	.reg .b64 	%rd<76>;


	mov.u64 	%SPL, __local_depot20;
	ld.param.u64 	%rd28, [ggml_matvec_f32_ncols_5_bs_96_param_0];
	ld.param.u64 	%rd29, [ggml_matvec_f32_ncols_5_bs_96_param_1];
	ld.param.u64 	%rd27, [ggml_matvec_f32_ncols_5_bs_96_param_2];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_5_bs_96_param_3];
	ld.param.u32 	%r19, [ggml_matvec_f32_ncols_5_bs_96_param_5];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_5_bs_96_param_6];
	ld.param.u32 	%r17, [ggml_matvec_f32_ncols_5_bs_96_param_7];
	ld.param.u32 	%r20, [ggml_matvec_f32_ncols_5_bs_96_param_8];
	ld.param.u32 	%r21, [ggml_matvec_f32_ncols_5_bs_96_param_9];
	ld.param.u32 	%r22, [ggml_matvec_f32_ncols_5_bs_96_param_10];
	ld.param.u32 	%r18, [ggml_matvec_f32_ncols_5_bs_96_param_11];
	cvta.to.global.u64 	%rd75, %rd29;
	cvta.to.global.u64 	%rd2, %rd28;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r23, %r1, %r20;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r24, %r2, %r19;
	mad.lo.s32 	%r25, %r23, %r21, %r24;
	cvt.s64.s32 	%rd4, %r25;
	mul.lo.s32 	%r26, %r1, %r22;
	cvt.s64.s32 	%rd5, %r26;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r27, %r3, 2;
	mov.u32 	%r28, data_mmv;
	add.s32 	%r4, %r28, %r27;
	@%p1 bra 	$L__BB20_2;

	mov.u32 	%r29, 0;
	st.shared.u32 	[%r4], %r29;

$L__BB20_2:
	bar.sync 	0;
	mov.f32 	%f327, 0f00000000;
	mov.u32 	%r30, 0;
	st.local.u32 	[%rd3], %r30;
	st.local.u32 	[%rd3+4], %r30;
	st.local.u32 	[%rd3+8], %r30;
	st.local.u32 	[%rd3+12], %r30;
	st.local.u32 	[%rd3+16], %r30;
	setp.ge.s32 	%p2, %r3, %r15;
	mov.f32 	%f328, %f327;
	mov.f32 	%f329, %f327;
	mov.f32 	%f330, %f327;
	mov.f32 	%f331, %f327;
	@%p2 bra 	$L__BB20_11;

	not.b32 	%r31, %r3;
	add.s32 	%r5, %r31, %r15;
	mul.wide.u32 	%rd31, %r5, -1431655765;
	shr.u64 	%rd32, %rd31, 38;
	cvt.u32.u64 	%r32, %rd32;
	add.s32 	%r33, %r32, 1;
	and.b32  	%r222, %r33, 3;
	setp.eq.s32 	%p3, %r222, 0;
	mov.f32 	%f327, 0f00000000;
	mov.u32 	%r223, %r3;
	@%p3 bra 	$L__BB20_7;

	shl.b32 	%r34, %r16, 1;
	mad.lo.s32 	%r35, %r16, 3, %r3;
	mul.wide.s32 	%rd33, %r35, 8;
	shl.b64 	%rd34, %rd5, 2;
	add.s64 	%rd7, %rd33, %rd34;
	mul.wide.s32 	%rd35, %r3, 8;
	mul.wide.s32 	%rd36, %r16, 8;
	add.s64 	%rd37, %rd35, %rd36;
	add.s64 	%rd8, %rd37, %rd34;
	add.s64 	%rd9, %rd35, %rd34;
	mul.wide.s32 	%rd38, %r3, 2;
	add.s64 	%rd39, %rd38, %rd4;
	shl.b64 	%rd40, %rd39, 2;
	add.s64 	%rd72, %rd2, %rd40;
	mul.wide.s32 	%rd11, %r34, 8;
	mov.f32 	%f327, 0f00000000;
	mov.u64 	%rd73, %rd75;
	mov.f32 	%f328, %f327;
	mov.f32 	%f329, %f327;
	mov.f32 	%f330, %f327;
	mov.f32 	%f331, %f327;
	mov.u32 	%r223, %r3;

$L__BB20_5:
	.pragma "nounroll";
	ld.global.nc.v2.f32 	{%f46, %f47}, [%rd72];
	add.s64 	%rd41, %rd73, %rd9;
	ld.global.nc.v2.f32 	{%f50, %f51}, [%rd41];
	fma.rn.f32 	%f54, %f46, %f50, %f331;
	fma.rn.f32 	%f331, %f47, %f51, %f54;
	add.s64 	%rd42, %rd73, %rd8;
	ld.global.nc.v2.f32 	{%f55, %f56}, [%rd42];
	fma.rn.f32 	%f59, %f46, %f55, %f330;
	fma.rn.f32 	%f330, %f47, %f56, %f59;
	add.s64 	%rd43, %rd41, %rd11;
	ld.global.nc.v2.f32 	{%f60, %f61}, [%rd43];
	fma.rn.f32 	%f64, %f46, %f60, %f329;
	fma.rn.f32 	%f329, %f47, %f61, %f64;
	add.s64 	%rd44, %rd73, %rd7;
	ld.global.nc.v2.f32 	{%f65, %f66}, [%rd44];
	fma.rn.f32 	%f69, %f46, %f65, %f328;
	fma.rn.f32 	%f328, %f47, %f66, %f69;
	add.s64 	%rd45, %rd43, %rd11;
	ld.global.nc.v2.f32 	{%f70, %f71}, [%rd45];
	fma.rn.f32 	%f74, %f46, %f70, %f327;
	fma.rn.f32 	%f327, %f47, %f71, %f74;
	add.s32 	%r223, %r223, 96;
	add.s64 	%rd73, %rd73, 768;
	add.s64 	%rd72, %rd72, 768;
	add.s32 	%r222, %r222, -1;
	setp.ne.s32 	%p4, %r222, 0;
	@%p4 bra 	$L__BB20_5;

	st.local.f32 	[%rd3], %f331;
	st.local.f32 	[%rd3+4], %f330;
	st.local.f32 	[%rd3+8], %f329;
	st.local.f32 	[%rd3+12], %f328;
	st.local.f32 	[%rd3+16], %f327;

$L__BB20_7:
	setp.lt.u32 	%p5, %r5, 288;
	@%p5 bra 	$L__BB20_11;

	add.s32 	%r36, %r223, %r16;
	shl.b32 	%r37, %r16, 1;
	add.s32 	%r38, %r223, %r37;
	mad.lo.s32 	%r39, %r16, 3, %r223;
	shl.b32 	%r40, %r16, 2;
	add.s32 	%r41, %r223, %r40;
	add.s32 	%r42, %r36, 96;
	mul.wide.s32 	%rd46, %r42, 8;
	shl.b64 	%rd47, %rd5, 2;
	add.s64 	%rd16, %rd46, %rd47;
	mul.wide.s32 	%rd48, %r38, 8;
	add.s64 	%rd17, %rd48, %rd47;
	mul.wide.s32 	%rd49, %r39, 8;
	add.s64 	%rd18, %rd49, %rd47;
	mul.wide.s32 	%rd50, %r41, 8;
	add.s64 	%rd19, %rd50, %rd47;
	mul.wide.s32 	%rd51, %r223, 2;
	add.s64 	%rd52, %rd51, %rd4;
	shl.b64 	%rd53, %rd52, 2;
	add.s64 	%rd54, %rd2, %rd53;
	add.s64 	%rd74, %rd54, 1536;
	mul.wide.s32 	%rd55, %r223, 8;
	add.s64 	%rd21, %rd55, %rd47;
	mul.wide.s32 	%rd56, %r16, 8;
	add.s64 	%rd57, %rd55, %rd56;
	add.s64 	%rd22, %rd57, %rd47;

$L__BB20_9:
	ld.global.nc.v2.f32 	{%f75, %f76}, [%rd74+-1536];
	add.s64 	%rd58, %rd75, %rd21;
	ld.global.nc.v2.f32 	{%f79, %f80}, [%rd58];
	fma.rn.f32 	%f83, %f75, %f79, %f331;
	fma.rn.f32 	%f84, %f76, %f80, %f83;
	add.s64 	%rd59, %rd75, %rd22;
	ld.global.nc.v2.f32 	{%f85, %f86}, [%rd59];
	fma.rn.f32 	%f89, %f75, %f85, %f330;
	fma.rn.f32 	%f90, %f76, %f86, %f89;
	add.s64 	%rd60, %rd75, %rd17;
	ld.global.nc.v2.f32 	{%f91, %f92}, [%rd60];
	fma.rn.f32 	%f95, %f75, %f91, %f329;
	fma.rn.f32 	%f96, %f76, %f92, %f95;
	add.s64 	%rd61, %rd75, %rd18;
	ld.global.nc.v2.f32 	{%f97, %f98}, [%rd61];
	fma.rn.f32 	%f101, %f75, %f97, %f328;
	fma.rn.f32 	%f102, %f76, %f98, %f101;
	add.s64 	%rd62, %rd75, %rd19;
	ld.global.nc.v2.f32 	{%f103, %f104}, [%rd62];
	fma.rn.f32 	%f107, %f75, %f103, %f327;
	fma.rn.f32 	%f108, %f76, %f104, %f107;
	ld.global.nc.v2.f32 	{%f109, %f110}, [%rd74+-768];
	ld.global.nc.v2.f32 	{%f113, %f114}, [%rd58+768];
	fma.rn.f32 	%f117, %f109, %f113, %f84;
	fma.rn.f32 	%f118, %f110, %f114, %f117;
	add.s64 	%rd63, %rd75, %rd16;
	ld.global.nc.v2.f32 	{%f119, %f120}, [%rd63];
	fma.rn.f32 	%f123, %f109, %f119, %f90;
	fma.rn.f32 	%f124, %f110, %f120, %f123;
	ld.global.nc.v2.f32 	{%f125, %f126}, [%rd60+768];
	fma.rn.f32 	%f129, %f109, %f125, %f96;
	fma.rn.f32 	%f130, %f110, %f126, %f129;
	ld.global.nc.v2.f32 	{%f131, %f132}, [%rd61+768];
	fma.rn.f32 	%f135, %f109, %f131, %f102;
	fma.rn.f32 	%f136, %f110, %f132, %f135;
	ld.global.nc.v2.f32 	{%f137, %f138}, [%rd62+768];
	fma.rn.f32 	%f141, %f109, %f137, %f108;
	fma.rn.f32 	%f142, %f110, %f138, %f141;
	ld.global.nc.v2.f32 	{%f143, %f144}, [%rd74];
	ld.global.nc.v2.f32 	{%f147, %f148}, [%rd58+1536];
	fma.rn.f32 	%f151, %f143, %f147, %f118;
	fma.rn.f32 	%f152, %f144, %f148, %f151;
	ld.global.nc.v2.f32 	{%f153, %f154}, [%rd63+768];
	fma.rn.f32 	%f157, %f143, %f153, %f124;
	fma.rn.f32 	%f158, %f144, %f154, %f157;
	ld.global.nc.v2.f32 	{%f159, %f160}, [%rd60+1536];
	fma.rn.f32 	%f163, %f143, %f159, %f130;
	fma.rn.f32 	%f164, %f144, %f160, %f163;
	ld.global.nc.v2.f32 	{%f165, %f166}, [%rd61+1536];
	fma.rn.f32 	%f169, %f143, %f165, %f136;
	fma.rn.f32 	%f170, %f144, %f166, %f169;
	ld.global.nc.v2.f32 	{%f171, %f172}, [%rd62+1536];
	fma.rn.f32 	%f175, %f143, %f171, %f142;
	fma.rn.f32 	%f176, %f144, %f172, %f175;
	ld.global.nc.v2.f32 	{%f177, %f178}, [%rd74+768];
	ld.global.nc.v2.f32 	{%f181, %f182}, [%rd58+2304];
	fma.rn.f32 	%f185, %f177, %f181, %f152;
	fma.rn.f32 	%f331, %f178, %f182, %f185;
	ld.global.nc.v2.f32 	{%f186, %f187}, [%rd63+1536];
	fma.rn.f32 	%f190, %f177, %f186, %f158;
	fma.rn.f32 	%f330, %f178, %f187, %f190;
	ld.global.nc.v2.f32 	{%f191, %f192}, [%rd60+2304];
	fma.rn.f32 	%f195, %f177, %f191, %f164;
	fma.rn.f32 	%f329, %f178, %f192, %f195;
	ld.global.nc.v2.f32 	{%f196, %f197}, [%rd61+2304];
	fma.rn.f32 	%f200, %f177, %f196, %f170;
	fma.rn.f32 	%f328, %f178, %f197, %f200;
	ld.global.nc.v2.f32 	{%f201, %f202}, [%rd62+2304];
	fma.rn.f32 	%f205, %f177, %f201, %f176;
	fma.rn.f32 	%f327, %f178, %f202, %f205;
	add.s64 	%rd75, %rd75, 3072;
	add.s64 	%rd74, %rd74, 3072;
	add.s32 	%r223, %r223, 384;
	setp.lt.s32 	%p6, %r223, %r15;
	@%p6 bra 	$L__BB20_9;

	st.local.f32 	[%rd3], %f331;
	st.local.f32 	[%rd3+4], %f330;
	st.local.f32 	[%rd3+8], %f329;
	st.local.f32 	[%rd3+12], %f328;
	st.local.f32 	[%rd3+16], %f327;

$L__BB20_11:
	shr.s32 	%r43, %r3, 31;
	shr.u32 	%r44, %r43, 27;
	add.s32 	%r45, %r3, %r44;
	shr.s32 	%r46, %r45, 5;
	shl.b32 	%r47, %r46, 2;
	add.s32 	%r14, %r28, %r47;
	mov.u32 	%r49, 2;
	mov.b32 	%r50, %f331;
	mov.u32 	%r51, 31;
	mov.u32 	%r52, 16;
	mov.u32 	%r53, -1;
	shfl.sync.bfly.b32 	%r54|%p7, %r50, %r52, %r51, %r53;
	mov.b32 	%f206, %r54;
	add.f32 	%f207, %f331, %f206;
	mov.b32 	%r55, %f207;
	mov.u32 	%r56, 8;
	shfl.sync.bfly.b32 	%r57|%p8, %r55, %r56, %r51, %r53;
	mov.b32 	%f208, %r57;
	add.f32 	%f209, %f207, %f208;
	mov.b32 	%r58, %f209;
	mov.u32 	%r59, 4;
	shfl.sync.bfly.b32 	%r60|%p9, %r58, %r59, %r51, %r53;
	mov.b32 	%f210, %r60;
	add.f32 	%f211, %f209, %f210;
	mov.b32 	%r61, %f211;
	shfl.sync.bfly.b32 	%r62|%p10, %r61, %r49, %r51, %r53;
	mov.b32 	%f212, %r62;
	add.f32 	%f213, %f211, %f212;
	mov.b32 	%r63, %f213;
	mov.u32 	%r64, 1;
	shfl.sync.bfly.b32 	%r65|%p11, %r63, %r64, %r51, %r53;
	mov.b32 	%f214, %r65;
	add.f32 	%f215, %f213, %f214;
	st.local.f32 	[%rd3], %f215;
	st.shared.f32 	[%r14], %f215;
	bar.sync 	0;
	@%p1 bra 	$L__BB20_13;

	ld.shared.f32 	%f216, [%r4];
	mov.b32 	%r66, %f216;
	shfl.sync.bfly.b32 	%r70|%p13, %r66, %r52, %r51, %r53;
	mov.b32 	%f217, %r70;
	add.f32 	%f218, %f216, %f217;
	mov.b32 	%r71, %f218;
	shfl.sync.bfly.b32 	%r73|%p14, %r71, %r56, %r51, %r53;
	mov.b32 	%f219, %r73;
	add.f32 	%f220, %f218, %f219;
	mov.b32 	%r74, %f220;
	shfl.sync.bfly.b32 	%r76|%p15, %r74, %r59, %r51, %r53;
	mov.b32 	%f221, %r76;
	add.f32 	%f222, %f220, %f221;
	mov.b32 	%r77, %f222;
	shfl.sync.bfly.b32 	%r79|%p16, %r77, %r49, %r51, %r53;
	mov.b32 	%f223, %r79;
	add.f32 	%f224, %f222, %f223;
	mov.b32 	%r80, %f224;
	shfl.sync.bfly.b32 	%r82|%p17, %r80, %r64, %r51, %r53;
	mov.b32 	%f225, %r82;
	add.f32 	%f226, %f224, %f225;
	st.local.f32 	[%rd3], %f226;

$L__BB20_13:
	bar.sync 	0;
	mov.b32 	%r83, %f330;
	shfl.sync.bfly.b32 	%r87|%p19, %r83, %r52, %r51, %r53;
	mov.b32 	%f227, %r87;
	add.f32 	%f228, %f330, %f227;
	mov.b32 	%r88, %f228;
	shfl.sync.bfly.b32 	%r90|%p20, %r88, %r56, %r51, %r53;
	mov.b32 	%f229, %r90;
	add.f32 	%f230, %f228, %f229;
	mov.b32 	%r91, %f230;
	shfl.sync.bfly.b32 	%r93|%p21, %r91, %r59, %r51, %r53;
	mov.b32 	%f231, %r93;
	add.f32 	%f232, %f230, %f231;
	mov.b32 	%r94, %f232;
	shfl.sync.bfly.b32 	%r96|%p22, %r94, %r49, %r51, %r53;
	mov.b32 	%f233, %r96;
	add.f32 	%f234, %f232, %f233;
	mov.b32 	%r97, %f234;
	shfl.sync.bfly.b32 	%r99|%p23, %r97, %r64, %r51, %r53;
	mov.b32 	%f235, %r99;
	add.f32 	%f236, %f234, %f235;
	st.local.f32 	[%rd3+4], %f236;
	st.shared.f32 	[%r14], %f236;
	bar.sync 	0;
	@%p1 bra 	$L__BB20_15;

	ld.shared.f32 	%f237, [%r4];
	mov.b32 	%r100, %f237;
	mov.u32 	%r101, 31;
	mov.u32 	%r102, 16;
	mov.u32 	%r103, -1;
	shfl.sync.bfly.b32 	%r104|%p24, %r100, %r102, %r101, %r103;
	mov.b32 	%f238, %r104;
	add.f32 	%f239, %f237, %f238;
	mov.b32 	%r105, %f239;
	mov.u32 	%r106, 8;
	shfl.sync.bfly.b32 	%r107|%p25, %r105, %r106, %r101, %r103;
	mov.b32 	%f240, %r107;
	add.f32 	%f241, %f239, %f240;
	mov.b32 	%r108, %f241;
	mov.u32 	%r109, 4;
	shfl.sync.bfly.b32 	%r110|%p26, %r108, %r109, %r101, %r103;
	mov.b32 	%f242, %r110;
	add.f32 	%f243, %f241, %f242;
	mov.b32 	%r111, %f243;
	mov.u32 	%r112, 2;
	shfl.sync.bfly.b32 	%r113|%p27, %r111, %r112, %r101, %r103;
	mov.b32 	%f244, %r113;
	add.f32 	%f245, %f243, %f244;
	mov.b32 	%r114, %f245;
	mov.u32 	%r115, 1;
	shfl.sync.bfly.b32 	%r116|%p28, %r114, %r115, %r101, %r103;
	mov.b32 	%f246, %r116;
	add.f32 	%f247, %f245, %f246;
	st.local.f32 	[%rd3+4], %f247;

$L__BB20_15:
	bar.sync 	0;
	mov.b32 	%r117, %f329;
	mov.u32 	%r118, 31;
	mov.u32 	%r119, 16;
	mov.u32 	%r120, -1;
	shfl.sync.bfly.b32 	%r121|%p30, %r117, %r119, %r118, %r120;
	mov.b32 	%f248, %r121;
	add.f32 	%f249, %f329, %f248;
	mov.b32 	%r122, %f249;
	mov.u32 	%r123, 8;
	shfl.sync.bfly.b32 	%r124|%p31, %r122, %r123, %r118, %r120;
	mov.b32 	%f250, %r124;
	add.f32 	%f251, %f249, %f250;
	mov.b32 	%r125, %f251;
	mov.u32 	%r126, 4;
	shfl.sync.bfly.b32 	%r127|%p32, %r125, %r126, %r118, %r120;
	mov.b32 	%f252, %r127;
	add.f32 	%f253, %f251, %f252;
	mov.b32 	%r128, %f253;
	mov.u32 	%r129, 2;
	shfl.sync.bfly.b32 	%r130|%p33, %r128, %r129, %r118, %r120;
	mov.b32 	%f254, %r130;
	add.f32 	%f255, %f253, %f254;
	mov.b32 	%r131, %f255;
	mov.u32 	%r132, 1;
	shfl.sync.bfly.b32 	%r133|%p34, %r131, %r132, %r118, %r120;
	mov.b32 	%f256, %r133;
	add.f32 	%f257, %f255, %f256;
	st.local.f32 	[%rd3+8], %f257;
	st.shared.f32 	[%r14], %f257;
	bar.sync 	0;
	@%p1 bra 	$L__BB20_17;

	ld.shared.f32 	%f258, [%r4];
	mov.b32 	%r134, %f258;
	shfl.sync.bfly.b32 	%r138|%p35, %r134, %r119, %r118, %r120;
	mov.b32 	%f259, %r138;
	add.f32 	%f260, %f258, %f259;
	mov.b32 	%r139, %f260;
	shfl.sync.bfly.b32 	%r141|%p36, %r139, %r123, %r118, %r120;
	mov.b32 	%f261, %r141;
	add.f32 	%f262, %f260, %f261;
	mov.b32 	%r142, %f262;
	shfl.sync.bfly.b32 	%r144|%p37, %r142, %r126, %r118, %r120;
	mov.b32 	%f263, %r144;
	add.f32 	%f264, %f262, %f263;
	mov.b32 	%r145, %f264;
	shfl.sync.bfly.b32 	%r147|%p38, %r145, %r129, %r118, %r120;
	mov.b32 	%f265, %r147;
	add.f32 	%f266, %f264, %f265;
	mov.b32 	%r148, %f266;
	shfl.sync.bfly.b32 	%r150|%p39, %r148, %r132, %r118, %r120;
	mov.b32 	%f267, %r150;
	add.f32 	%f268, %f266, %f267;
	st.local.f32 	[%rd3+8], %f268;

$L__BB20_17:
	bar.sync 	0;
	mov.b32 	%r151, %f328;
	shfl.sync.bfly.b32 	%r155|%p41, %r151, %r119, %r118, %r120;
	mov.b32 	%f269, %r155;
	add.f32 	%f270, %f328, %f269;
	mov.b32 	%r156, %f270;
	shfl.sync.bfly.b32 	%r158|%p42, %r156, %r123, %r118, %r120;
	mov.b32 	%f271, %r158;
	add.f32 	%f272, %f270, %f271;
	mov.b32 	%r159, %f272;
	shfl.sync.bfly.b32 	%r161|%p43, %r159, %r126, %r118, %r120;
	mov.b32 	%f273, %r161;
	add.f32 	%f274, %f272, %f273;
	mov.b32 	%r162, %f274;
	shfl.sync.bfly.b32 	%r164|%p44, %r162, %r129, %r118, %r120;
	mov.b32 	%f275, %r164;
	add.f32 	%f276, %f274, %f275;
	mov.b32 	%r165, %f276;
	shfl.sync.bfly.b32 	%r167|%p45, %r165, %r132, %r118, %r120;
	mov.b32 	%f277, %r167;
	add.f32 	%f278, %f276, %f277;
	st.local.f32 	[%rd3+12], %f278;
	st.shared.f32 	[%r14], %f278;
	bar.sync 	0;
	@%p1 bra 	$L__BB20_19;

	ld.shared.f32 	%f279, [%r4];
	mov.b32 	%r168, %f279;
	mov.u32 	%r169, 31;
	mov.u32 	%r170, 16;
	mov.u32 	%r171, -1;
	shfl.sync.bfly.b32 	%r172|%p46, %r168, %r170, %r169, %r171;
	mov.b32 	%f280, %r172;
	add.f32 	%f281, %f279, %f280;
	mov.b32 	%r173, %f281;
	mov.u32 	%r174, 8;
	shfl.sync.bfly.b32 	%r175|%p47, %r173, %r174, %r169, %r171;
	mov.b32 	%f282, %r175;
	add.f32 	%f283, %f281, %f282;
	mov.b32 	%r176, %f283;
	mov.u32 	%r177, 4;
	shfl.sync.bfly.b32 	%r178|%p48, %r176, %r177, %r169, %r171;
	mov.b32 	%f284, %r178;
	add.f32 	%f285, %f283, %f284;
	mov.b32 	%r179, %f285;
	mov.u32 	%r180, 2;
	shfl.sync.bfly.b32 	%r181|%p49, %r179, %r180, %r169, %r171;
	mov.b32 	%f286, %r181;
	add.f32 	%f287, %f285, %f286;
	mov.b32 	%r182, %f287;
	mov.u32 	%r183, 1;
	shfl.sync.bfly.b32 	%r184|%p50, %r182, %r183, %r169, %r171;
	mov.b32 	%f288, %r184;
	add.f32 	%f289, %f287, %f288;
	st.local.f32 	[%rd3+12], %f289;

$L__BB20_19:
	bar.sync 	0;
	mov.b32 	%r185, %f327;
	mov.u32 	%r186, 31;
	mov.u32 	%r187, 16;
	mov.u32 	%r188, -1;
	shfl.sync.bfly.b32 	%r189|%p52, %r185, %r187, %r186, %r188;
	mov.b32 	%f290, %r189;
	add.f32 	%f291, %f327, %f290;
	mov.b32 	%r190, %f291;
	mov.u32 	%r191, 8;
	shfl.sync.bfly.b32 	%r192|%p53, %r190, %r191, %r186, %r188;
	mov.b32 	%f292, %r192;
	add.f32 	%f293, %f291, %f292;
	mov.b32 	%r193, %f293;
	mov.u32 	%r194, 4;
	shfl.sync.bfly.b32 	%r195|%p54, %r193, %r194, %r186, %r188;
	mov.b32 	%f294, %r195;
	add.f32 	%f295, %f293, %f294;
	mov.b32 	%r196, %f295;
	mov.u32 	%r197, 2;
	shfl.sync.bfly.b32 	%r198|%p55, %r196, %r197, %r186, %r188;
	mov.b32 	%f296, %r198;
	add.f32 	%f297, %f295, %f296;
	mov.b32 	%r199, %f297;
	mov.u32 	%r200, 1;
	shfl.sync.bfly.b32 	%r201|%p56, %r199, %r200, %r186, %r188;
	mov.b32 	%f298, %r201;
	add.f32 	%f299, %f297, %f298;
	st.local.f32 	[%rd3+16], %f299;
	st.shared.f32 	[%r14], %f299;
	bar.sync 	0;
	@%p1 bra 	$L__BB20_21;

	ld.shared.f32 	%f300, [%r4];
	mov.b32 	%r202, %f300;
	shfl.sync.bfly.b32 	%r206|%p57, %r202, %r187, %r186, %r188;
	mov.b32 	%f301, %r206;
	add.f32 	%f302, %f300, %f301;
	mov.b32 	%r207, %f302;
	shfl.sync.bfly.b32 	%r209|%p58, %r207, %r191, %r186, %r188;
	mov.b32 	%f303, %r209;
	add.f32 	%f304, %f302, %f303;
	mov.b32 	%r210, %f304;
	shfl.sync.bfly.b32 	%r212|%p59, %r210, %r194, %r186, %r188;
	mov.b32 	%f305, %r212;
	add.f32 	%f306, %f304, %f305;
	mov.b32 	%r213, %f306;
	shfl.sync.bfly.b32 	%r215|%p60, %r213, %r197, %r186, %r188;
	mov.b32 	%f307, %r215;
	add.f32 	%f308, %f306, %f307;
	mov.b32 	%r216, %f308;
	shfl.sync.bfly.b32 	%r218|%p61, %r216, %r200, %r186, %r188;
	mov.b32 	%f309, %r218;
	add.f32 	%f310, %f308, %f309;
	st.local.f32 	[%rd3+16], %f310;

$L__BB20_21:
	bar.sync 	0;
	setp.gt.s32 	%p62, %r3, 4;
	@%p62 bra 	$L__BB20_23;

	mul.wide.s32 	%rd64, %r3, 4;
	add.s64 	%rd65, %rd3, %rd64;
	ld.local.f32 	%f311, [%rd65];
	mad.lo.s32 	%r219, %r3, %r17, %r2;
	cvt.s64.s32 	%rd66, %r219;
	mul.lo.s32 	%r220, %r1, %r18;
	cvt.s64.s32 	%rd67, %r220;
	add.s64 	%rd68, %rd67, %rd66;
	cvta.to.global.u64 	%rd69, %rd27;
	shl.b64 	%rd70, %rd68, 2;
	add.s64 	%rd71, %rd69, %rd70;
	st.global.f32 	[%rd71], %f311;

$L__BB20_23:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_6_bs_96
.visible .entry ggml_matvec_f32_ncols_6_bs_96(
	.param .u64 ggml_matvec_f32_ncols_6_bs_96_param_0,
	.param .u64 ggml_matvec_f32_ncols_6_bs_96_param_1,
	.param .u64 ggml_matvec_f32_ncols_6_bs_96_param_2,
	.param .u32 ggml_matvec_f32_ncols_6_bs_96_param_3,
	.param .u32 ggml_matvec_f32_ncols_6_bs_96_param_4,
	.param .u32 ggml_matvec_f32_ncols_6_bs_96_param_5,
	.param .u32 ggml_matvec_f32_ncols_6_bs_96_param_6,
	.param .u32 ggml_matvec_f32_ncols_6_bs_96_param_7,
	.param .u32 ggml_matvec_f32_ncols_6_bs_96_param_8,
	.param .u32 ggml_matvec_f32_ncols_6_bs_96_param_9,
	.param .u32 ggml_matvec_f32_ncols_6_bs_96_param_10,
	.param .u32 ggml_matvec_f32_ncols_6_bs_96_param_11
)
{
	.local .align 8 .b8 	__local_depot21[24];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<75>;
	.reg .f32 	%f<296>;
	.reg .b32 	%r<251>;
	.reg .b64 	%rd<70>;


	mov.u64 	%SPL, __local_depot21;
	ld.param.u64 	%rd20, [ggml_matvec_f32_ncols_6_bs_96_param_0];
	ld.param.u64 	%rd21, [ggml_matvec_f32_ncols_6_bs_96_param_1];
	ld.param.u64 	%rd19, [ggml_matvec_f32_ncols_6_bs_96_param_2];
	ld.param.u32 	%r11, [ggml_matvec_f32_ncols_6_bs_96_param_3];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_6_bs_96_param_5];
	ld.param.u32 	%r12, [ggml_matvec_f32_ncols_6_bs_96_param_6];
	ld.param.u32 	%r13, [ggml_matvec_f32_ncols_6_bs_96_param_7];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_6_bs_96_param_8];
	ld.param.u32 	%r17, [ggml_matvec_f32_ncols_6_bs_96_param_9];
	ld.param.u32 	%r18, [ggml_matvec_f32_ncols_6_bs_96_param_10];
	ld.param.u32 	%r14, [ggml_matvec_f32_ncols_6_bs_96_param_11];
	cvta.to.global.u64 	%rd69, %rd21;
	add.u64 	%rd2, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r19, %r1, %r16;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r20, %r2, %r15;
	mad.lo.s32 	%r21, %r19, %r17, %r20;
	cvt.s64.s32 	%rd3, %r21;
	cvta.to.global.u64 	%rd4, %rd20;
	mul.lo.s32 	%r22, %r1, %r18;
	cvt.s64.s32 	%rd5, %r22;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r23, %r3, 2;
	mov.u32 	%r24, data_mmv;
	add.s32 	%r4, %r24, %r23;
	@%p1 bra 	$L__BB21_2;

	mov.u32 	%r25, 0;
	st.shared.u32 	[%r4], %r25;

$L__BB21_2:
	bar.sync 	0;
	mov.f32 	%f290, 0f00000000;
	st.local.v2.f32 	[%rd2], {%f290, %f290};
	st.local.v2.f32 	[%rd2+8], {%f290, %f290};
	st.local.v2.f32 	[%rd2+16], {%f290, %f290};
	setp.ge.s32 	%p2, %r3, %r11;
	mov.f32 	%f291, %f290;
	mov.f32 	%f292, %f290;
	mov.f32 	%f293, %f290;
	mov.f32 	%f294, %f290;
	mov.f32 	%f295, %f290;
	@%p2 bra 	$L__BB21_9;

	not.b32 	%r26, %r3;
	add.s32 	%r5, %r26, %r11;
	mul.wide.u32 	%rd23, %r5, -1431655765;
	shr.u64 	%rd24, %rd23, 38;
	and.b64  	%rd25, %rd24, 1;
	setp.eq.b64 	%p3, %rd25, 1;
	mov.pred 	%p4, 0;
	xor.pred  	%p5, %p3, %p4;
	mov.f32 	%f290, 0f00000000;
	mov.u32 	%r250, %r3;
	@%p5 bra 	$L__BB21_5;

	shl.b64 	%rd26, %rd5, 2;
	add.s64 	%rd27, %rd69, %rd26;
	shl.b64 	%rd28, %rd3, 2;
	add.s64 	%rd29, %rd4, %rd28;
	mul.wide.s32 	%rd30, %r3, 8;
	add.s64 	%rd31, %rd29, %rd30;
	ld.global.nc.v2.f32 	{%f43, %f44}, [%rd31];
	add.s64 	%rd32, %rd27, %rd30;
	ld.global.nc.v2.f32 	{%f47, %f48}, [%rd32];
	fma.rn.f32 	%f51, %f43, %f47, 0f00000000;
	fma.rn.f32 	%f295, %f44, %f48, %f51;
	mul.wide.s32 	%rd33, %r12, 8;
	add.s64 	%rd34, %rd32, %rd33;
	ld.global.nc.v2.f32 	{%f52, %f53}, [%rd34];
	fma.rn.f32 	%f56, %f43, %f52, 0f00000000;
	fma.rn.f32 	%f294, %f44, %f53, %f56;
	st.local.v2.f32 	[%rd2], {%f295, %f294};
	add.s32 	%r27, %r3, %r12;
	add.s32 	%r28, %r27, %r12;
	mul.wide.s32 	%rd35, %r28, 8;
	add.s64 	%rd36, %rd27, %rd35;
	ld.global.nc.v2.f32 	{%f57, %f58}, [%rd36];
	fma.rn.f32 	%f61, %f43, %f57, 0f00000000;
	fma.rn.f32 	%f293, %f44, %f58, %f61;
	add.s64 	%rd37, %rd36, %rd33;
	ld.global.nc.v2.f32 	{%f62, %f63}, [%rd37];
	fma.rn.f32 	%f66, %f43, %f62, 0f00000000;
	fma.rn.f32 	%f292, %f44, %f63, %f66;
	st.local.v2.f32 	[%rd2+8], {%f293, %f292};
	add.s64 	%rd38, %rd37, %rd33;
	ld.global.nc.v2.f32 	{%f67, %f68}, [%rd38];
	fma.rn.f32 	%f71, %f43, %f67, 0f00000000;
	fma.rn.f32 	%f291, %f44, %f68, %f71;
	add.s64 	%rd39, %rd38, %rd33;
	ld.global.nc.v2.f32 	{%f72, %f73}, [%rd39];
	fma.rn.f32 	%f76, %f43, %f72, 0f00000000;
	fma.rn.f32 	%f290, %f44, %f73, %f76;
	st.local.v2.f32 	[%rd2+16], {%f291, %f290};
	add.s32 	%r250, %r3, 96;

$L__BB21_5:
	setp.lt.u32 	%p6, %r5, 96;
	@%p6 bra 	$L__BB21_9;

	add.s32 	%r29, %r250, %r12;
	add.s32 	%r30, %r29, 96;
	mul.wide.s32 	%rd40, %r30, 8;
	shl.b64 	%rd41, %rd5, 2;
	add.s64 	%rd7, %rd40, %rd41;
	shl.b32 	%r31, %r12, 1;
	add.s32 	%r32, %r250, %r31;
	mad.lo.s32 	%r33, %r12, 3, %r250;
	shl.b32 	%r34, %r12, 2;
	add.s32 	%r35, %r250, %r34;
	mad.lo.s32 	%r36, %r12, 5, %r250;
	mul.wide.s32 	%rd42, %r32, 8;
	add.s64 	%rd8, %rd42, %rd41;
	mul.wide.s32 	%rd43, %r33, 8;
	add.s64 	%rd9, %rd43, %rd41;
	mul.wide.s32 	%rd44, %r35, 8;
	add.s64 	%rd10, %rd44, %rd41;
	mul.wide.s32 	%rd45, %r36, 8;
	add.s64 	%rd11, %rd45, %rd41;
	mul.wide.s32 	%rd46, %r250, 2;
	add.s64 	%rd47, %rd46, %rd3;
	shl.b64 	%rd48, %rd47, 2;
	add.s64 	%rd49, %rd4, %rd48;
	add.s64 	%rd68, %rd49, 768;
	mul.wide.s32 	%rd50, %r250, 8;
	mul.wide.s32 	%rd51, %r12, 8;
	add.s64 	%rd52, %rd50, %rd51;
	add.s64 	%rd13, %rd52, %rd41;
	add.s64 	%rd14, %rd50, %rd41;

$L__BB21_7:
	ld.global.nc.v2.f32 	{%f77, %f78}, [%rd68+-768];
	add.s64 	%rd53, %rd69, %rd14;
	ld.global.nc.v2.f32 	{%f81, %f82}, [%rd53];
	fma.rn.f32 	%f85, %f77, %f81, %f295;
	fma.rn.f32 	%f86, %f78, %f82, %f85;
	add.s64 	%rd54, %rd69, %rd13;
	ld.global.nc.v2.f32 	{%f87, %f88}, [%rd54];
	fma.rn.f32 	%f91, %f77, %f87, %f294;
	fma.rn.f32 	%f92, %f78, %f88, %f91;
	add.s64 	%rd55, %rd69, %rd8;
	ld.global.nc.v2.f32 	{%f93, %f94}, [%rd55];
	fma.rn.f32 	%f97, %f77, %f93, %f293;
	fma.rn.f32 	%f98, %f78, %f94, %f97;
	add.s64 	%rd56, %rd69, %rd9;
	ld.global.nc.v2.f32 	{%f99, %f100}, [%rd56];
	fma.rn.f32 	%f103, %f77, %f99, %f292;
	fma.rn.f32 	%f104, %f78, %f100, %f103;
	add.s64 	%rd57, %rd69, %rd10;
	ld.global.nc.v2.f32 	{%f105, %f106}, [%rd57];
	fma.rn.f32 	%f109, %f77, %f105, %f291;
	fma.rn.f32 	%f110, %f78, %f106, %f109;
	add.s64 	%rd58, %rd69, %rd11;
	ld.global.nc.v2.f32 	{%f111, %f112}, [%rd58];
	fma.rn.f32 	%f115, %f77, %f111, %f290;
	fma.rn.f32 	%f116, %f78, %f112, %f115;
	ld.global.nc.v2.f32 	{%f117, %f118}, [%rd68];
	ld.global.nc.v2.f32 	{%f121, %f122}, [%rd53+768];
	fma.rn.f32 	%f125, %f117, %f121, %f86;
	fma.rn.f32 	%f295, %f118, %f122, %f125;
	add.s64 	%rd59, %rd69, %rd7;
	ld.global.nc.v2.f32 	{%f126, %f127}, [%rd59];
	fma.rn.f32 	%f130, %f117, %f126, %f92;
	fma.rn.f32 	%f294, %f118, %f127, %f130;
	ld.global.nc.v2.f32 	{%f131, %f132}, [%rd55+768];
	fma.rn.f32 	%f135, %f117, %f131, %f98;
	fma.rn.f32 	%f293, %f118, %f132, %f135;
	ld.global.nc.v2.f32 	{%f136, %f137}, [%rd56+768];
	fma.rn.f32 	%f140, %f117, %f136, %f104;
	fma.rn.f32 	%f292, %f118, %f137, %f140;
	ld.global.nc.v2.f32 	{%f141, %f142}, [%rd57+768];
	fma.rn.f32 	%f145, %f117, %f141, %f110;
	fma.rn.f32 	%f291, %f118, %f142, %f145;
	ld.global.nc.v2.f32 	{%f146, %f147}, [%rd58+768];
	fma.rn.f32 	%f150, %f117, %f146, %f116;
	fma.rn.f32 	%f290, %f118, %f147, %f150;
	add.s64 	%rd69, %rd69, 1536;
	add.s64 	%rd68, %rd68, 1536;
	add.s32 	%r250, %r250, 192;
	setp.lt.s32 	%p7, %r250, %r11;
	@%p7 bra 	$L__BB21_7;

	st.local.v2.f32 	[%rd2], {%f295, %f294};
	st.local.v2.f32 	[%rd2+8], {%f293, %f292};
	st.local.v2.f32 	[%rd2+16], {%f291, %f290};

$L__BB21_9:
	shr.s32 	%r37, %r3, 31;
	shr.u32 	%r38, %r37, 27;
	add.s32 	%r39, %r3, %r38;
	shr.s32 	%r40, %r39, 5;
	shl.b32 	%r41, %r40, 2;
	add.s32 	%r10, %r24, %r41;
	mov.u32 	%r43, 2;
	mov.b32 	%r44, %f295;
	mov.u32 	%r45, 31;
	mov.u32 	%r46, 16;
	mov.u32 	%r47, -1;
	shfl.sync.bfly.b32 	%r48|%p8, %r44, %r46, %r45, %r47;
	mov.b32 	%f151, %r48;
	add.f32 	%f152, %f295, %f151;
	mov.b32 	%r49, %f152;
	mov.u32 	%r50, 8;
	shfl.sync.bfly.b32 	%r51|%p9, %r49, %r50, %r45, %r47;
	mov.b32 	%f153, %r51;
	add.f32 	%f154, %f152, %f153;
	mov.b32 	%r52, %f154;
	mov.u32 	%r53, 4;
	shfl.sync.bfly.b32 	%r54|%p10, %r52, %r53, %r45, %r47;
	mov.b32 	%f155, %r54;
	add.f32 	%f156, %f154, %f155;
	mov.b32 	%r55, %f156;
	shfl.sync.bfly.b32 	%r56|%p11, %r55, %r43, %r45, %r47;
	mov.b32 	%f157, %r56;
	add.f32 	%f158, %f156, %f157;
	mov.b32 	%r57, %f158;
	mov.u32 	%r58, 1;
	shfl.sync.bfly.b32 	%r59|%p12, %r57, %r58, %r45, %r47;
	mov.b32 	%f159, %r59;
	add.f32 	%f160, %f158, %f159;
	st.local.f32 	[%rd2], %f160;
	st.shared.f32 	[%r10], %f160;
	bar.sync 	0;
	@%p1 bra 	$L__BB21_11;

	ld.shared.f32 	%f161, [%r4];
	mov.b32 	%r60, %f161;
	shfl.sync.bfly.b32 	%r64|%p14, %r60, %r46, %r45, %r47;
	mov.b32 	%f162, %r64;
	add.f32 	%f163, %f161, %f162;
	mov.b32 	%r65, %f163;
	shfl.sync.bfly.b32 	%r67|%p15, %r65, %r50, %r45, %r47;
	mov.b32 	%f164, %r67;
	add.f32 	%f165, %f163, %f164;
	mov.b32 	%r68, %f165;
	shfl.sync.bfly.b32 	%r70|%p16, %r68, %r53, %r45, %r47;
	mov.b32 	%f166, %r70;
	add.f32 	%f167, %f165, %f166;
	mov.b32 	%r71, %f167;
	shfl.sync.bfly.b32 	%r73|%p17, %r71, %r43, %r45, %r47;
	mov.b32 	%f168, %r73;
	add.f32 	%f169, %f167, %f168;
	mov.b32 	%r74, %f169;
	shfl.sync.bfly.b32 	%r76|%p18, %r74, %r58, %r45, %r47;
	mov.b32 	%f170, %r76;
	add.f32 	%f171, %f169, %f170;
	st.local.f32 	[%rd2], %f171;

$L__BB21_11:
	bar.sync 	0;
	mov.b32 	%r77, %f294;
	shfl.sync.bfly.b32 	%r81|%p20, %r77, %r46, %r45, %r47;
	mov.b32 	%f172, %r81;
	add.f32 	%f173, %f294, %f172;
	mov.b32 	%r82, %f173;
	shfl.sync.bfly.b32 	%r84|%p21, %r82, %r50, %r45, %r47;
	mov.b32 	%f174, %r84;
	add.f32 	%f175, %f173, %f174;
	mov.b32 	%r85, %f175;
	shfl.sync.bfly.b32 	%r87|%p22, %r85, %r53, %r45, %r47;
	mov.b32 	%f176, %r87;
	add.f32 	%f177, %f175, %f176;
	mov.b32 	%r88, %f177;
	shfl.sync.bfly.b32 	%r90|%p23, %r88, %r43, %r45, %r47;
	mov.b32 	%f178, %r90;
	add.f32 	%f179, %f177, %f178;
	mov.b32 	%r91, %f179;
	shfl.sync.bfly.b32 	%r93|%p24, %r91, %r58, %r45, %r47;
	mov.b32 	%f180, %r93;
	add.f32 	%f181, %f179, %f180;
	st.local.f32 	[%rd2+4], %f181;
	st.shared.f32 	[%r10], %f181;
	bar.sync 	0;
	@%p1 bra 	$L__BB21_13;

	ld.shared.f32 	%f182, [%r4];
	mov.b32 	%r94, %f182;
	mov.u32 	%r95, 31;
	mov.u32 	%r96, 16;
	mov.u32 	%r97, -1;
	shfl.sync.bfly.b32 	%r98|%p25, %r94, %r96, %r95, %r97;
	mov.b32 	%f183, %r98;
	add.f32 	%f184, %f182, %f183;
	mov.b32 	%r99, %f184;
	mov.u32 	%r100, 8;
	shfl.sync.bfly.b32 	%r101|%p26, %r99, %r100, %r95, %r97;
	mov.b32 	%f185, %r101;
	add.f32 	%f186, %f184, %f185;
	mov.b32 	%r102, %f186;
	mov.u32 	%r103, 4;
	shfl.sync.bfly.b32 	%r104|%p27, %r102, %r103, %r95, %r97;
	mov.b32 	%f187, %r104;
	add.f32 	%f188, %f186, %f187;
	mov.b32 	%r105, %f188;
	mov.u32 	%r106, 2;
	shfl.sync.bfly.b32 	%r107|%p28, %r105, %r106, %r95, %r97;
	mov.b32 	%f189, %r107;
	add.f32 	%f190, %f188, %f189;
	mov.b32 	%r108, %f190;
	mov.u32 	%r109, 1;
	shfl.sync.bfly.b32 	%r110|%p29, %r108, %r109, %r95, %r97;
	mov.b32 	%f191, %r110;
	add.f32 	%f192, %f190, %f191;
	st.local.f32 	[%rd2+4], %f192;

$L__BB21_13:
	bar.sync 	0;
	mov.b32 	%r111, %f293;
	mov.u32 	%r112, 31;
	mov.u32 	%r113, 16;
	mov.u32 	%r114, -1;
	shfl.sync.bfly.b32 	%r115|%p31, %r111, %r113, %r112, %r114;
	mov.b32 	%f193, %r115;
	add.f32 	%f194, %f293, %f193;
	mov.b32 	%r116, %f194;
	mov.u32 	%r117, 8;
	shfl.sync.bfly.b32 	%r118|%p32, %r116, %r117, %r112, %r114;
	mov.b32 	%f195, %r118;
	add.f32 	%f196, %f194, %f195;
	mov.b32 	%r119, %f196;
	mov.u32 	%r120, 4;
	shfl.sync.bfly.b32 	%r121|%p33, %r119, %r120, %r112, %r114;
	mov.b32 	%f197, %r121;
	add.f32 	%f198, %f196, %f197;
	mov.b32 	%r122, %f198;
	mov.u32 	%r123, 2;
	shfl.sync.bfly.b32 	%r124|%p34, %r122, %r123, %r112, %r114;
	mov.b32 	%f199, %r124;
	add.f32 	%f200, %f198, %f199;
	mov.b32 	%r125, %f200;
	mov.u32 	%r126, 1;
	shfl.sync.bfly.b32 	%r127|%p35, %r125, %r126, %r112, %r114;
	mov.b32 	%f201, %r127;
	add.f32 	%f202, %f200, %f201;
	st.local.f32 	[%rd2+8], %f202;
	st.shared.f32 	[%r10], %f202;
	bar.sync 	0;
	@%p1 bra 	$L__BB21_15;

	ld.shared.f32 	%f203, [%r4];
	mov.b32 	%r128, %f203;
	shfl.sync.bfly.b32 	%r132|%p36, %r128, %r113, %r112, %r114;
	mov.b32 	%f204, %r132;
	add.f32 	%f205, %f203, %f204;
	mov.b32 	%r133, %f205;
	shfl.sync.bfly.b32 	%r135|%p37, %r133, %r117, %r112, %r114;
	mov.b32 	%f206, %r135;
	add.f32 	%f207, %f205, %f206;
	mov.b32 	%r136, %f207;
	shfl.sync.bfly.b32 	%r138|%p38, %r136, %r120, %r112, %r114;
	mov.b32 	%f208, %r138;
	add.f32 	%f209, %f207, %f208;
	mov.b32 	%r139, %f209;
	shfl.sync.bfly.b32 	%r141|%p39, %r139, %r123, %r112, %r114;
	mov.b32 	%f210, %r141;
	add.f32 	%f211, %f209, %f210;
	mov.b32 	%r142, %f211;
	shfl.sync.bfly.b32 	%r144|%p40, %r142, %r126, %r112, %r114;
	mov.b32 	%f212, %r144;
	add.f32 	%f213, %f211, %f212;
	st.local.f32 	[%rd2+8], %f213;

$L__BB21_15:
	bar.sync 	0;
	mov.b32 	%r145, %f292;
	shfl.sync.bfly.b32 	%r149|%p42, %r145, %r113, %r112, %r114;
	mov.b32 	%f214, %r149;
	add.f32 	%f215, %f292, %f214;
	mov.b32 	%r150, %f215;
	shfl.sync.bfly.b32 	%r152|%p43, %r150, %r117, %r112, %r114;
	mov.b32 	%f216, %r152;
	add.f32 	%f217, %f215, %f216;
	mov.b32 	%r153, %f217;
	shfl.sync.bfly.b32 	%r155|%p44, %r153, %r120, %r112, %r114;
	mov.b32 	%f218, %r155;
	add.f32 	%f219, %f217, %f218;
	mov.b32 	%r156, %f219;
	shfl.sync.bfly.b32 	%r158|%p45, %r156, %r123, %r112, %r114;
	mov.b32 	%f220, %r158;
	add.f32 	%f221, %f219, %f220;
	mov.b32 	%r159, %f221;
	shfl.sync.bfly.b32 	%r161|%p46, %r159, %r126, %r112, %r114;
	mov.b32 	%f222, %r161;
	add.f32 	%f223, %f221, %f222;
	st.local.f32 	[%rd2+12], %f223;
	st.shared.f32 	[%r10], %f223;
	bar.sync 	0;
	@%p1 bra 	$L__BB21_17;

	ld.shared.f32 	%f224, [%r4];
	mov.b32 	%r162, %f224;
	mov.u32 	%r163, 31;
	mov.u32 	%r164, 16;
	mov.u32 	%r165, -1;
	shfl.sync.bfly.b32 	%r166|%p47, %r162, %r164, %r163, %r165;
	mov.b32 	%f225, %r166;
	add.f32 	%f226, %f224, %f225;
	mov.b32 	%r167, %f226;
	mov.u32 	%r168, 8;
	shfl.sync.bfly.b32 	%r169|%p48, %r167, %r168, %r163, %r165;
	mov.b32 	%f227, %r169;
	add.f32 	%f228, %f226, %f227;
	mov.b32 	%r170, %f228;
	mov.u32 	%r171, 4;
	shfl.sync.bfly.b32 	%r172|%p49, %r170, %r171, %r163, %r165;
	mov.b32 	%f229, %r172;
	add.f32 	%f230, %f228, %f229;
	mov.b32 	%r173, %f230;
	mov.u32 	%r174, 2;
	shfl.sync.bfly.b32 	%r175|%p50, %r173, %r174, %r163, %r165;
	mov.b32 	%f231, %r175;
	add.f32 	%f232, %f230, %f231;
	mov.b32 	%r176, %f232;
	mov.u32 	%r177, 1;
	shfl.sync.bfly.b32 	%r178|%p51, %r176, %r177, %r163, %r165;
	mov.b32 	%f233, %r178;
	add.f32 	%f234, %f232, %f233;
	st.local.f32 	[%rd2+12], %f234;

$L__BB21_17:
	bar.sync 	0;
	mov.b32 	%r179, %f291;
	mov.u32 	%r180, 31;
	mov.u32 	%r181, 16;
	mov.u32 	%r182, -1;
	shfl.sync.bfly.b32 	%r183|%p53, %r179, %r181, %r180, %r182;
	mov.b32 	%f235, %r183;
	add.f32 	%f236, %f291, %f235;
	mov.b32 	%r184, %f236;
	mov.u32 	%r185, 8;
	shfl.sync.bfly.b32 	%r186|%p54, %r184, %r185, %r180, %r182;
	mov.b32 	%f237, %r186;
	add.f32 	%f238, %f236, %f237;
	mov.b32 	%r187, %f238;
	mov.u32 	%r188, 4;
	shfl.sync.bfly.b32 	%r189|%p55, %r187, %r188, %r180, %r182;
	mov.b32 	%f239, %r189;
	add.f32 	%f240, %f238, %f239;
	mov.b32 	%r190, %f240;
	mov.u32 	%r191, 2;
	shfl.sync.bfly.b32 	%r192|%p56, %r190, %r191, %r180, %r182;
	mov.b32 	%f241, %r192;
	add.f32 	%f242, %f240, %f241;
	mov.b32 	%r193, %f242;
	mov.u32 	%r194, 1;
	shfl.sync.bfly.b32 	%r195|%p57, %r193, %r194, %r180, %r182;
	mov.b32 	%f243, %r195;
	add.f32 	%f244, %f242, %f243;
	st.local.f32 	[%rd2+16], %f244;
	st.shared.f32 	[%r10], %f244;
	bar.sync 	0;
	@%p1 bra 	$L__BB21_19;

	ld.shared.f32 	%f245, [%r4];
	mov.b32 	%r196, %f245;
	shfl.sync.bfly.b32 	%r200|%p58, %r196, %r181, %r180, %r182;
	mov.b32 	%f246, %r200;
	add.f32 	%f247, %f245, %f246;
	mov.b32 	%r201, %f247;
	shfl.sync.bfly.b32 	%r203|%p59, %r201, %r185, %r180, %r182;
	mov.b32 	%f248, %r203;
	add.f32 	%f249, %f247, %f248;
	mov.b32 	%r204, %f249;
	shfl.sync.bfly.b32 	%r206|%p60, %r204, %r188, %r180, %r182;
	mov.b32 	%f250, %r206;
	add.f32 	%f251, %f249, %f250;
	mov.b32 	%r207, %f251;
	shfl.sync.bfly.b32 	%r209|%p61, %r207, %r191, %r180, %r182;
	mov.b32 	%f252, %r209;
	add.f32 	%f253, %f251, %f252;
	mov.b32 	%r210, %f253;
	shfl.sync.bfly.b32 	%r212|%p62, %r210, %r194, %r180, %r182;
	mov.b32 	%f254, %r212;
	add.f32 	%f255, %f253, %f254;
	st.local.f32 	[%rd2+16], %f255;

$L__BB21_19:
	bar.sync 	0;
	mov.b32 	%r213, %f290;
	shfl.sync.bfly.b32 	%r217|%p64, %r213, %r181, %r180, %r182;
	mov.b32 	%f256, %r217;
	add.f32 	%f257, %f290, %f256;
	mov.b32 	%r218, %f257;
	shfl.sync.bfly.b32 	%r220|%p65, %r218, %r185, %r180, %r182;
	mov.b32 	%f258, %r220;
	add.f32 	%f259, %f257, %f258;
	mov.b32 	%r221, %f259;
	shfl.sync.bfly.b32 	%r223|%p66, %r221, %r188, %r180, %r182;
	mov.b32 	%f260, %r223;
	add.f32 	%f261, %f259, %f260;
	mov.b32 	%r224, %f261;
	shfl.sync.bfly.b32 	%r226|%p67, %r224, %r191, %r180, %r182;
	mov.b32 	%f262, %r226;
	add.f32 	%f263, %f261, %f262;
	mov.b32 	%r227, %f263;
	shfl.sync.bfly.b32 	%r229|%p68, %r227, %r194, %r180, %r182;
	mov.b32 	%f264, %r229;
	add.f32 	%f265, %f263, %f264;
	st.local.f32 	[%rd2+20], %f265;
	st.shared.f32 	[%r10], %f265;
	bar.sync 	0;
	@%p1 bra 	$L__BB21_21;

	ld.shared.f32 	%f266, [%r4];
	mov.b32 	%r230, %f266;
	mov.u32 	%r231, 31;
	mov.u32 	%r232, 16;
	mov.u32 	%r233, -1;
	shfl.sync.bfly.b32 	%r234|%p69, %r230, %r232, %r231, %r233;
	mov.b32 	%f267, %r234;
	add.f32 	%f268, %f266, %f267;
	mov.b32 	%r235, %f268;
	mov.u32 	%r236, 8;
	shfl.sync.bfly.b32 	%r237|%p70, %r235, %r236, %r231, %r233;
	mov.b32 	%f269, %r237;
	add.f32 	%f270, %f268, %f269;
	mov.b32 	%r238, %f270;
	mov.u32 	%r239, 4;
	shfl.sync.bfly.b32 	%r240|%p71, %r238, %r239, %r231, %r233;
	mov.b32 	%f271, %r240;
	add.f32 	%f272, %f270, %f271;
	mov.b32 	%r241, %f272;
	mov.u32 	%r242, 2;
	shfl.sync.bfly.b32 	%r243|%p72, %r241, %r242, %r231, %r233;
	mov.b32 	%f273, %r243;
	add.f32 	%f274, %f272, %f273;
	mov.b32 	%r244, %f274;
	mov.u32 	%r245, 1;
	shfl.sync.bfly.b32 	%r246|%p73, %r244, %r245, %r231, %r233;
	mov.b32 	%f275, %r246;
	add.f32 	%f276, %f274, %f275;
	st.local.f32 	[%rd2+20], %f276;

$L__BB21_21:
	bar.sync 	0;
	setp.gt.s32 	%p74, %r3, 5;
	@%p74 bra 	$L__BB21_23;

	mul.wide.s32 	%rd60, %r3, 4;
	add.s64 	%rd61, %rd2, %rd60;
	ld.local.f32 	%f277, [%rd61];
	mad.lo.s32 	%r247, %r3, %r13, %r2;
	cvt.s64.s32 	%rd62, %r247;
	mul.lo.s32 	%r248, %r1, %r14;
	cvt.s64.s32 	%rd63, %r248;
	add.s64 	%rd64, %rd63, %rd62;
	cvta.to.global.u64 	%rd65, %rd19;
	shl.b64 	%rd66, %rd64, 2;
	add.s64 	%rd67, %rd65, %rd66;
	st.global.f32 	[%rd67], %f277;

$L__BB21_23:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_7_bs_96
.visible .entry ggml_matvec_f32_ncols_7_bs_96(
	.param .u64 ggml_matvec_f32_ncols_7_bs_96_param_0,
	.param .u64 ggml_matvec_f32_ncols_7_bs_96_param_1,
	.param .u64 ggml_matvec_f32_ncols_7_bs_96_param_2,
	.param .u32 ggml_matvec_f32_ncols_7_bs_96_param_3,
	.param .u32 ggml_matvec_f32_ncols_7_bs_96_param_4,
	.param .u32 ggml_matvec_f32_ncols_7_bs_96_param_5,
	.param .u32 ggml_matvec_f32_ncols_7_bs_96_param_6,
	.param .u32 ggml_matvec_f32_ncols_7_bs_96_param_7,
	.param .u32 ggml_matvec_f32_ncols_7_bs_96_param_8,
	.param .u32 ggml_matvec_f32_ncols_7_bs_96_param_9,
	.param .u32 ggml_matvec_f32_ncols_7_bs_96_param_10,
	.param .u32 ggml_matvec_f32_ncols_7_bs_96_param_11
)
{
	.local .align 4 .b8 	__local_depot22[28];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<86>;
	.reg .f32 	%f<343>;
	.reg .b32 	%r<287>;
	.reg .b64 	%rd<74>;


	mov.u64 	%SPL, __local_depot22;
	ld.param.u64 	%rd21, [ggml_matvec_f32_ncols_7_bs_96_param_0];
	ld.param.u64 	%rd22, [ggml_matvec_f32_ncols_7_bs_96_param_1];
	ld.param.u64 	%rd20, [ggml_matvec_f32_ncols_7_bs_96_param_2];
	ld.param.u32 	%r11, [ggml_matvec_f32_ncols_7_bs_96_param_3];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_7_bs_96_param_5];
	ld.param.u32 	%r12, [ggml_matvec_f32_ncols_7_bs_96_param_6];
	ld.param.u32 	%r13, [ggml_matvec_f32_ncols_7_bs_96_param_7];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_7_bs_96_param_8];
	ld.param.u32 	%r17, [ggml_matvec_f32_ncols_7_bs_96_param_9];
	ld.param.u32 	%r18, [ggml_matvec_f32_ncols_7_bs_96_param_10];
	ld.param.u32 	%r14, [ggml_matvec_f32_ncols_7_bs_96_param_11];
	cvta.to.global.u64 	%rd73, %rd22;
	add.u64 	%rd2, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r19, %r1, %r16;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r20, %r2, %r15;
	mad.lo.s32 	%r21, %r19, %r17, %r20;
	cvt.s64.s32 	%rd3, %r21;
	cvta.to.global.u64 	%rd4, %rd21;
	mul.lo.s32 	%r22, %r1, %r18;
	cvt.s64.s32 	%rd5, %r22;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r23, %r3, 2;
	mov.u32 	%r24, data_mmv;
	add.s32 	%r4, %r24, %r23;
	@%p1 bra 	$L__BB22_2;

	mov.u32 	%r25, 0;
	st.shared.u32 	[%r4], %r25;

$L__BB22_2:
	bar.sync 	0;
	mov.f32 	%f336, 0f00000000;
	mov.u32 	%r26, 0;
	st.local.u32 	[%rd2], %r26;
	st.local.u32 	[%rd2+4], %r26;
	st.local.u32 	[%rd2+8], %r26;
	st.local.u32 	[%rd2+12], %r26;
	st.local.u32 	[%rd2+16], %r26;
	st.local.u32 	[%rd2+20], %r26;
	st.local.u32 	[%rd2+24], %r26;
	setp.ge.s32 	%p2, %r3, %r11;
	mov.f32 	%f337, %f336;
	mov.f32 	%f338, %f336;
	mov.f32 	%f339, %f336;
	mov.f32 	%f340, %f336;
	mov.f32 	%f341, %f336;
	mov.f32 	%f342, %f336;
	@%p2 bra 	$L__BB22_9;

	not.b32 	%r27, %r3;
	add.s32 	%r5, %r27, %r11;
	mul.wide.u32 	%rd24, %r5, -1431655765;
	shr.u64 	%rd25, %rd24, 38;
	and.b64  	%rd26, %rd25, 1;
	setp.eq.b64 	%p3, %rd26, 1;
	mov.pred 	%p4, 0;
	xor.pred  	%p5, %p3, %p4;
	mov.f32 	%f336, 0f00000000;
	mov.u32 	%r286, %r3;
	@%p5 bra 	$L__BB22_5;

	shl.b64 	%rd27, %rd5, 2;
	add.s64 	%rd28, %rd73, %rd27;
	shl.b64 	%rd29, %rd3, 2;
	add.s64 	%rd30, %rd4, %rd29;
	mul.wide.s32 	%rd31, %r3, 8;
	add.s64 	%rd32, %rd30, %rd31;
	ld.global.nc.v2.f32 	{%f50, %f51}, [%rd32];
	add.s64 	%rd33, %rd28, %rd31;
	ld.global.nc.v2.f32 	{%f54, %f55}, [%rd33];
	fma.rn.f32 	%f58, %f50, %f54, 0f00000000;
	fma.rn.f32 	%f342, %f51, %f55, %f58;
	st.local.f32 	[%rd2], %f342;
	mul.wide.s32 	%rd34, %r12, 8;
	add.s64 	%rd35, %rd33, %rd34;
	ld.global.nc.v2.f32 	{%f59, %f60}, [%rd35];
	fma.rn.f32 	%f63, %f50, %f59, 0f00000000;
	fma.rn.f32 	%f341, %f51, %f60, %f63;
	st.local.f32 	[%rd2+4], %f341;
	add.s32 	%r28, %r3, %r12;
	add.s32 	%r29, %r28, %r12;
	mul.wide.s32 	%rd36, %r29, 8;
	add.s64 	%rd37, %rd28, %rd36;
	ld.global.nc.v2.f32 	{%f64, %f65}, [%rd37];
	fma.rn.f32 	%f68, %f50, %f64, 0f00000000;
	fma.rn.f32 	%f340, %f51, %f65, %f68;
	st.local.f32 	[%rd2+8], %f340;
	add.s64 	%rd38, %rd37, %rd34;
	ld.global.nc.v2.f32 	{%f69, %f70}, [%rd38];
	fma.rn.f32 	%f73, %f50, %f69, 0f00000000;
	fma.rn.f32 	%f339, %f51, %f70, %f73;
	st.local.f32 	[%rd2+12], %f339;
	add.s64 	%rd39, %rd38, %rd34;
	ld.global.nc.v2.f32 	{%f74, %f75}, [%rd39];
	fma.rn.f32 	%f78, %f50, %f74, 0f00000000;
	fma.rn.f32 	%f338, %f51, %f75, %f78;
	st.local.f32 	[%rd2+16], %f338;
	add.s64 	%rd40, %rd39, %rd34;
	ld.global.nc.v2.f32 	{%f79, %f80}, [%rd40];
	fma.rn.f32 	%f83, %f50, %f79, 0f00000000;
	fma.rn.f32 	%f337, %f51, %f80, %f83;
	st.local.f32 	[%rd2+20], %f337;
	add.s64 	%rd41, %rd40, %rd34;
	ld.global.nc.v2.f32 	{%f84, %f85}, [%rd41];
	fma.rn.f32 	%f88, %f50, %f84, 0f00000000;
	fma.rn.f32 	%f336, %f51, %f85, %f88;
	st.local.f32 	[%rd2+24], %f336;
	add.s32 	%r286, %r3, 96;

$L__BB22_5:
	setp.lt.u32 	%p6, %r5, 96;
	@%p6 bra 	$L__BB22_9;

	add.s32 	%r30, %r286, %r12;
	add.s32 	%r31, %r30, 96;
	mul.wide.s32 	%rd42, %r31, 8;
	shl.b64 	%rd43, %rd5, 2;
	add.s64 	%rd7, %rd42, %rd43;
	shl.b32 	%r32, %r12, 1;
	add.s32 	%r33, %r286, %r32;
	mad.lo.s32 	%r34, %r12, 3, %r286;
	shl.b32 	%r35, %r12, 2;
	add.s32 	%r36, %r286, %r35;
	mad.lo.s32 	%r37, %r12, 5, %r286;
	mad.lo.s32 	%r38, %r12, 6, %r286;
	mul.wide.s32 	%rd44, %r33, 8;
	add.s64 	%rd8, %rd44, %rd43;
	mul.wide.s32 	%rd45, %r34, 8;
	add.s64 	%rd9, %rd45, %rd43;
	mul.wide.s32 	%rd46, %r36, 8;
	add.s64 	%rd10, %rd46, %rd43;
	mul.wide.s32 	%rd47, %r37, 8;
	add.s64 	%rd11, %rd47, %rd43;
	mul.wide.s32 	%rd48, %r38, 8;
	add.s64 	%rd12, %rd48, %rd43;
	mul.wide.s32 	%rd49, %r286, 2;
	add.s64 	%rd50, %rd49, %rd3;
	shl.b64 	%rd51, %rd50, 2;
	add.s64 	%rd52, %rd4, %rd51;
	add.s64 	%rd72, %rd52, 768;
	mul.wide.s32 	%rd53, %r286, 8;
	mul.wide.s32 	%rd54, %r12, 8;
	add.s64 	%rd55, %rd53, %rd54;
	add.s64 	%rd14, %rd55, %rd43;
	add.s64 	%rd15, %rd53, %rd43;

$L__BB22_7:
	ld.global.nc.v2.f32 	{%f89, %f90}, [%rd72+-768];
	add.s64 	%rd56, %rd73, %rd15;
	ld.global.nc.v2.f32 	{%f93, %f94}, [%rd56];
	fma.rn.f32 	%f97, %f89, %f93, %f342;
	fma.rn.f32 	%f98, %f90, %f94, %f97;
	add.s64 	%rd57, %rd73, %rd14;
	ld.global.nc.v2.f32 	{%f99, %f100}, [%rd57];
	fma.rn.f32 	%f103, %f89, %f99, %f341;
	fma.rn.f32 	%f104, %f90, %f100, %f103;
	add.s64 	%rd58, %rd73, %rd8;
	ld.global.nc.v2.f32 	{%f105, %f106}, [%rd58];
	fma.rn.f32 	%f109, %f89, %f105, %f340;
	fma.rn.f32 	%f110, %f90, %f106, %f109;
	add.s64 	%rd59, %rd73, %rd9;
	ld.global.nc.v2.f32 	{%f111, %f112}, [%rd59];
	fma.rn.f32 	%f115, %f89, %f111, %f339;
	fma.rn.f32 	%f116, %f90, %f112, %f115;
	add.s64 	%rd60, %rd73, %rd10;
	ld.global.nc.v2.f32 	{%f117, %f118}, [%rd60];
	fma.rn.f32 	%f121, %f89, %f117, %f338;
	fma.rn.f32 	%f122, %f90, %f118, %f121;
	add.s64 	%rd61, %rd73, %rd11;
	ld.global.nc.v2.f32 	{%f123, %f124}, [%rd61];
	fma.rn.f32 	%f127, %f89, %f123, %f337;
	fma.rn.f32 	%f128, %f90, %f124, %f127;
	add.s64 	%rd62, %rd73, %rd12;
	ld.global.nc.v2.f32 	{%f129, %f130}, [%rd62];
	fma.rn.f32 	%f133, %f89, %f129, %f336;
	fma.rn.f32 	%f134, %f90, %f130, %f133;
	ld.global.nc.v2.f32 	{%f135, %f136}, [%rd72];
	ld.global.nc.v2.f32 	{%f139, %f140}, [%rd56+768];
	fma.rn.f32 	%f143, %f135, %f139, %f98;
	fma.rn.f32 	%f342, %f136, %f140, %f143;
	add.s64 	%rd63, %rd73, %rd7;
	ld.global.nc.v2.f32 	{%f144, %f145}, [%rd63];
	fma.rn.f32 	%f148, %f135, %f144, %f104;
	fma.rn.f32 	%f341, %f136, %f145, %f148;
	ld.global.nc.v2.f32 	{%f149, %f150}, [%rd58+768];
	fma.rn.f32 	%f153, %f135, %f149, %f110;
	fma.rn.f32 	%f340, %f136, %f150, %f153;
	ld.global.nc.v2.f32 	{%f154, %f155}, [%rd59+768];
	fma.rn.f32 	%f158, %f135, %f154, %f116;
	fma.rn.f32 	%f339, %f136, %f155, %f158;
	ld.global.nc.v2.f32 	{%f159, %f160}, [%rd60+768];
	fma.rn.f32 	%f163, %f135, %f159, %f122;
	fma.rn.f32 	%f338, %f136, %f160, %f163;
	ld.global.nc.v2.f32 	{%f164, %f165}, [%rd61+768];
	fma.rn.f32 	%f168, %f135, %f164, %f128;
	fma.rn.f32 	%f337, %f136, %f165, %f168;
	ld.global.nc.v2.f32 	{%f169, %f170}, [%rd62+768];
	fma.rn.f32 	%f173, %f135, %f169, %f134;
	fma.rn.f32 	%f336, %f136, %f170, %f173;
	add.s64 	%rd73, %rd73, 1536;
	add.s64 	%rd72, %rd72, 1536;
	add.s32 	%r286, %r286, 192;
	setp.lt.s32 	%p7, %r286, %r11;
	@%p7 bra 	$L__BB22_7;

	st.local.f32 	[%rd2], %f342;
	st.local.f32 	[%rd2+4], %f341;
	st.local.f32 	[%rd2+8], %f340;
	st.local.f32 	[%rd2+12], %f339;
	st.local.f32 	[%rd2+16], %f338;
	st.local.f32 	[%rd2+20], %f337;
	st.local.f32 	[%rd2+24], %f336;

$L__BB22_9:
	shr.s32 	%r39, %r3, 31;
	shr.u32 	%r40, %r39, 27;
	add.s32 	%r41, %r3, %r40;
	shr.s32 	%r42, %r41, 5;
	shl.b32 	%r43, %r42, 2;
	add.s32 	%r10, %r24, %r43;
	mov.u32 	%r45, 2;
	mov.b32 	%r46, %f342;
	mov.u32 	%r47, 31;
	mov.u32 	%r48, 16;
	mov.u32 	%r49, -1;
	shfl.sync.bfly.b32 	%r50|%p8, %r46, %r48, %r47, %r49;
	mov.b32 	%f174, %r50;
	add.f32 	%f175, %f342, %f174;
	mov.b32 	%r51, %f175;
	mov.u32 	%r52, 8;
	shfl.sync.bfly.b32 	%r53|%p9, %r51, %r52, %r47, %r49;
	mov.b32 	%f176, %r53;
	add.f32 	%f177, %f175, %f176;
	mov.b32 	%r54, %f177;
	mov.u32 	%r55, 4;
	shfl.sync.bfly.b32 	%r56|%p10, %r54, %r55, %r47, %r49;
	mov.b32 	%f178, %r56;
	add.f32 	%f179, %f177, %f178;
	mov.b32 	%r57, %f179;
	shfl.sync.bfly.b32 	%r58|%p11, %r57, %r45, %r47, %r49;
	mov.b32 	%f180, %r58;
	add.f32 	%f181, %f179, %f180;
	mov.b32 	%r59, %f181;
	mov.u32 	%r60, 1;
	shfl.sync.bfly.b32 	%r61|%p12, %r59, %r60, %r47, %r49;
	mov.b32 	%f182, %r61;
	add.f32 	%f183, %f181, %f182;
	st.local.f32 	[%rd2], %f183;
	st.shared.f32 	[%r10], %f183;
	bar.sync 	0;
	@%p1 bra 	$L__BB22_11;

	ld.shared.f32 	%f184, [%r4];
	mov.b32 	%r62, %f184;
	shfl.sync.bfly.b32 	%r66|%p14, %r62, %r48, %r47, %r49;
	mov.b32 	%f185, %r66;
	add.f32 	%f186, %f184, %f185;
	mov.b32 	%r67, %f186;
	shfl.sync.bfly.b32 	%r69|%p15, %r67, %r52, %r47, %r49;
	mov.b32 	%f187, %r69;
	add.f32 	%f188, %f186, %f187;
	mov.b32 	%r70, %f188;
	shfl.sync.bfly.b32 	%r72|%p16, %r70, %r55, %r47, %r49;
	mov.b32 	%f189, %r72;
	add.f32 	%f190, %f188, %f189;
	mov.b32 	%r73, %f190;
	shfl.sync.bfly.b32 	%r75|%p17, %r73, %r45, %r47, %r49;
	mov.b32 	%f191, %r75;
	add.f32 	%f192, %f190, %f191;
	mov.b32 	%r76, %f192;
	shfl.sync.bfly.b32 	%r78|%p18, %r76, %r60, %r47, %r49;
	mov.b32 	%f193, %r78;
	add.f32 	%f194, %f192, %f193;
	st.local.f32 	[%rd2], %f194;

$L__BB22_11:
	bar.sync 	0;
	mov.b32 	%r79, %f341;
	shfl.sync.bfly.b32 	%r83|%p20, %r79, %r48, %r47, %r49;
	mov.b32 	%f195, %r83;
	add.f32 	%f196, %f341, %f195;
	mov.b32 	%r84, %f196;
	shfl.sync.bfly.b32 	%r86|%p21, %r84, %r52, %r47, %r49;
	mov.b32 	%f197, %r86;
	add.f32 	%f198, %f196, %f197;
	mov.b32 	%r87, %f198;
	shfl.sync.bfly.b32 	%r89|%p22, %r87, %r55, %r47, %r49;
	mov.b32 	%f199, %r89;
	add.f32 	%f200, %f198, %f199;
	mov.b32 	%r90, %f200;
	shfl.sync.bfly.b32 	%r92|%p23, %r90, %r45, %r47, %r49;
	mov.b32 	%f201, %r92;
	add.f32 	%f202, %f200, %f201;
	mov.b32 	%r93, %f202;
	shfl.sync.bfly.b32 	%r95|%p24, %r93, %r60, %r47, %r49;
	mov.b32 	%f203, %r95;
	add.f32 	%f204, %f202, %f203;
	st.local.f32 	[%rd2+4], %f204;
	st.shared.f32 	[%r10], %f204;
	bar.sync 	0;
	@%p1 bra 	$L__BB22_13;

	ld.shared.f32 	%f205, [%r4];
	mov.b32 	%r96, %f205;
	mov.u32 	%r97, 31;
	mov.u32 	%r98, 16;
	mov.u32 	%r99, -1;
	shfl.sync.bfly.b32 	%r100|%p25, %r96, %r98, %r97, %r99;
	mov.b32 	%f206, %r100;
	add.f32 	%f207, %f205, %f206;
	mov.b32 	%r101, %f207;
	mov.u32 	%r102, 8;
	shfl.sync.bfly.b32 	%r103|%p26, %r101, %r102, %r97, %r99;
	mov.b32 	%f208, %r103;
	add.f32 	%f209, %f207, %f208;
	mov.b32 	%r104, %f209;
	mov.u32 	%r105, 4;
	shfl.sync.bfly.b32 	%r106|%p27, %r104, %r105, %r97, %r99;
	mov.b32 	%f210, %r106;
	add.f32 	%f211, %f209, %f210;
	mov.b32 	%r107, %f211;
	mov.u32 	%r108, 2;
	shfl.sync.bfly.b32 	%r109|%p28, %r107, %r108, %r97, %r99;
	mov.b32 	%f212, %r109;
	add.f32 	%f213, %f211, %f212;
	mov.b32 	%r110, %f213;
	mov.u32 	%r111, 1;
	shfl.sync.bfly.b32 	%r112|%p29, %r110, %r111, %r97, %r99;
	mov.b32 	%f214, %r112;
	add.f32 	%f215, %f213, %f214;
	st.local.f32 	[%rd2+4], %f215;

$L__BB22_13:
	bar.sync 	0;
	mov.b32 	%r113, %f340;
	mov.u32 	%r114, 31;
	mov.u32 	%r115, 16;
	mov.u32 	%r116, -1;
	shfl.sync.bfly.b32 	%r117|%p31, %r113, %r115, %r114, %r116;
	mov.b32 	%f216, %r117;
	add.f32 	%f217, %f340, %f216;
	mov.b32 	%r118, %f217;
	mov.u32 	%r119, 8;
	shfl.sync.bfly.b32 	%r120|%p32, %r118, %r119, %r114, %r116;
	mov.b32 	%f218, %r120;
	add.f32 	%f219, %f217, %f218;
	mov.b32 	%r121, %f219;
	mov.u32 	%r122, 4;
	shfl.sync.bfly.b32 	%r123|%p33, %r121, %r122, %r114, %r116;
	mov.b32 	%f220, %r123;
	add.f32 	%f221, %f219, %f220;
	mov.b32 	%r124, %f221;
	mov.u32 	%r125, 2;
	shfl.sync.bfly.b32 	%r126|%p34, %r124, %r125, %r114, %r116;
	mov.b32 	%f222, %r126;
	add.f32 	%f223, %f221, %f222;
	mov.b32 	%r127, %f223;
	mov.u32 	%r128, 1;
	shfl.sync.bfly.b32 	%r129|%p35, %r127, %r128, %r114, %r116;
	mov.b32 	%f224, %r129;
	add.f32 	%f225, %f223, %f224;
	st.local.f32 	[%rd2+8], %f225;
	st.shared.f32 	[%r10], %f225;
	bar.sync 	0;
	@%p1 bra 	$L__BB22_15;

	ld.shared.f32 	%f226, [%r4];
	mov.b32 	%r130, %f226;
	shfl.sync.bfly.b32 	%r134|%p36, %r130, %r115, %r114, %r116;
	mov.b32 	%f227, %r134;
	add.f32 	%f228, %f226, %f227;
	mov.b32 	%r135, %f228;
	shfl.sync.bfly.b32 	%r137|%p37, %r135, %r119, %r114, %r116;
	mov.b32 	%f229, %r137;
	add.f32 	%f230, %f228, %f229;
	mov.b32 	%r138, %f230;
	shfl.sync.bfly.b32 	%r140|%p38, %r138, %r122, %r114, %r116;
	mov.b32 	%f231, %r140;
	add.f32 	%f232, %f230, %f231;
	mov.b32 	%r141, %f232;
	shfl.sync.bfly.b32 	%r143|%p39, %r141, %r125, %r114, %r116;
	mov.b32 	%f233, %r143;
	add.f32 	%f234, %f232, %f233;
	mov.b32 	%r144, %f234;
	shfl.sync.bfly.b32 	%r146|%p40, %r144, %r128, %r114, %r116;
	mov.b32 	%f235, %r146;
	add.f32 	%f236, %f234, %f235;
	st.local.f32 	[%rd2+8], %f236;

$L__BB22_15:
	bar.sync 	0;
	mov.b32 	%r147, %f339;
	shfl.sync.bfly.b32 	%r151|%p42, %r147, %r115, %r114, %r116;
	mov.b32 	%f237, %r151;
	add.f32 	%f238, %f339, %f237;
	mov.b32 	%r152, %f238;
	shfl.sync.bfly.b32 	%r154|%p43, %r152, %r119, %r114, %r116;
	mov.b32 	%f239, %r154;
	add.f32 	%f240, %f238, %f239;
	mov.b32 	%r155, %f240;
	shfl.sync.bfly.b32 	%r157|%p44, %r155, %r122, %r114, %r116;
	mov.b32 	%f241, %r157;
	add.f32 	%f242, %f240, %f241;
	mov.b32 	%r158, %f242;
	shfl.sync.bfly.b32 	%r160|%p45, %r158, %r125, %r114, %r116;
	mov.b32 	%f243, %r160;
	add.f32 	%f244, %f242, %f243;
	mov.b32 	%r161, %f244;
	shfl.sync.bfly.b32 	%r163|%p46, %r161, %r128, %r114, %r116;
	mov.b32 	%f245, %r163;
	add.f32 	%f246, %f244, %f245;
	st.local.f32 	[%rd2+12], %f246;
	st.shared.f32 	[%r10], %f246;
	bar.sync 	0;
	@%p1 bra 	$L__BB22_17;

	ld.shared.f32 	%f247, [%r4];
	mov.b32 	%r164, %f247;
	mov.u32 	%r165, 31;
	mov.u32 	%r166, 16;
	mov.u32 	%r167, -1;
	shfl.sync.bfly.b32 	%r168|%p47, %r164, %r166, %r165, %r167;
	mov.b32 	%f248, %r168;
	add.f32 	%f249, %f247, %f248;
	mov.b32 	%r169, %f249;
	mov.u32 	%r170, 8;
	shfl.sync.bfly.b32 	%r171|%p48, %r169, %r170, %r165, %r167;
	mov.b32 	%f250, %r171;
	add.f32 	%f251, %f249, %f250;
	mov.b32 	%r172, %f251;
	mov.u32 	%r173, 4;
	shfl.sync.bfly.b32 	%r174|%p49, %r172, %r173, %r165, %r167;
	mov.b32 	%f252, %r174;
	add.f32 	%f253, %f251, %f252;
	mov.b32 	%r175, %f253;
	mov.u32 	%r176, 2;
	shfl.sync.bfly.b32 	%r177|%p50, %r175, %r176, %r165, %r167;
	mov.b32 	%f254, %r177;
	add.f32 	%f255, %f253, %f254;
	mov.b32 	%r178, %f255;
	mov.u32 	%r179, 1;
	shfl.sync.bfly.b32 	%r180|%p51, %r178, %r179, %r165, %r167;
	mov.b32 	%f256, %r180;
	add.f32 	%f257, %f255, %f256;
	st.local.f32 	[%rd2+12], %f257;

$L__BB22_17:
	bar.sync 	0;
	mov.b32 	%r181, %f338;
	mov.u32 	%r182, 31;
	mov.u32 	%r183, 16;
	mov.u32 	%r184, -1;
	shfl.sync.bfly.b32 	%r185|%p53, %r181, %r183, %r182, %r184;
	mov.b32 	%f258, %r185;
	add.f32 	%f259, %f338, %f258;
	mov.b32 	%r186, %f259;
	mov.u32 	%r187, 8;
	shfl.sync.bfly.b32 	%r188|%p54, %r186, %r187, %r182, %r184;
	mov.b32 	%f260, %r188;
	add.f32 	%f261, %f259, %f260;
	mov.b32 	%r189, %f261;
	mov.u32 	%r190, 4;
	shfl.sync.bfly.b32 	%r191|%p55, %r189, %r190, %r182, %r184;
	mov.b32 	%f262, %r191;
	add.f32 	%f263, %f261, %f262;
	mov.b32 	%r192, %f263;
	mov.u32 	%r193, 2;
	shfl.sync.bfly.b32 	%r194|%p56, %r192, %r193, %r182, %r184;
	mov.b32 	%f264, %r194;
	add.f32 	%f265, %f263, %f264;
	mov.b32 	%r195, %f265;
	mov.u32 	%r196, 1;
	shfl.sync.bfly.b32 	%r197|%p57, %r195, %r196, %r182, %r184;
	mov.b32 	%f266, %r197;
	add.f32 	%f267, %f265, %f266;
	st.local.f32 	[%rd2+16], %f267;
	st.shared.f32 	[%r10], %f267;
	bar.sync 	0;
	@%p1 bra 	$L__BB22_19;

	ld.shared.f32 	%f268, [%r4];
	mov.b32 	%r198, %f268;
	shfl.sync.bfly.b32 	%r202|%p58, %r198, %r183, %r182, %r184;
	mov.b32 	%f269, %r202;
	add.f32 	%f270, %f268, %f269;
	mov.b32 	%r203, %f270;
	shfl.sync.bfly.b32 	%r205|%p59, %r203, %r187, %r182, %r184;
	mov.b32 	%f271, %r205;
	add.f32 	%f272, %f270, %f271;
	mov.b32 	%r206, %f272;
	shfl.sync.bfly.b32 	%r208|%p60, %r206, %r190, %r182, %r184;
	mov.b32 	%f273, %r208;
	add.f32 	%f274, %f272, %f273;
	mov.b32 	%r209, %f274;
	shfl.sync.bfly.b32 	%r211|%p61, %r209, %r193, %r182, %r184;
	mov.b32 	%f275, %r211;
	add.f32 	%f276, %f274, %f275;
	mov.b32 	%r212, %f276;
	shfl.sync.bfly.b32 	%r214|%p62, %r212, %r196, %r182, %r184;
	mov.b32 	%f277, %r214;
	add.f32 	%f278, %f276, %f277;
	st.local.f32 	[%rd2+16], %f278;

$L__BB22_19:
	bar.sync 	0;
	mov.b32 	%r215, %f337;
	shfl.sync.bfly.b32 	%r219|%p64, %r215, %r183, %r182, %r184;
	mov.b32 	%f279, %r219;
	add.f32 	%f280, %f337, %f279;
	mov.b32 	%r220, %f280;
	shfl.sync.bfly.b32 	%r222|%p65, %r220, %r187, %r182, %r184;
	mov.b32 	%f281, %r222;
	add.f32 	%f282, %f280, %f281;
	mov.b32 	%r223, %f282;
	shfl.sync.bfly.b32 	%r225|%p66, %r223, %r190, %r182, %r184;
	mov.b32 	%f283, %r225;
	add.f32 	%f284, %f282, %f283;
	mov.b32 	%r226, %f284;
	shfl.sync.bfly.b32 	%r228|%p67, %r226, %r193, %r182, %r184;
	mov.b32 	%f285, %r228;
	add.f32 	%f286, %f284, %f285;
	mov.b32 	%r229, %f286;
	shfl.sync.bfly.b32 	%r231|%p68, %r229, %r196, %r182, %r184;
	mov.b32 	%f287, %r231;
	add.f32 	%f288, %f286, %f287;
	st.local.f32 	[%rd2+20], %f288;
	st.shared.f32 	[%r10], %f288;
	bar.sync 	0;
	@%p1 bra 	$L__BB22_21;

	ld.shared.f32 	%f289, [%r4];
	mov.b32 	%r232, %f289;
	mov.u32 	%r233, 31;
	mov.u32 	%r234, 16;
	mov.u32 	%r235, -1;
	shfl.sync.bfly.b32 	%r236|%p69, %r232, %r234, %r233, %r235;
	mov.b32 	%f290, %r236;
	add.f32 	%f291, %f289, %f290;
	mov.b32 	%r237, %f291;
	mov.u32 	%r238, 8;
	shfl.sync.bfly.b32 	%r239|%p70, %r237, %r238, %r233, %r235;
	mov.b32 	%f292, %r239;
	add.f32 	%f293, %f291, %f292;
	mov.b32 	%r240, %f293;
	mov.u32 	%r241, 4;
	shfl.sync.bfly.b32 	%r242|%p71, %r240, %r241, %r233, %r235;
	mov.b32 	%f294, %r242;
	add.f32 	%f295, %f293, %f294;
	mov.b32 	%r243, %f295;
	mov.u32 	%r244, 2;
	shfl.sync.bfly.b32 	%r245|%p72, %r243, %r244, %r233, %r235;
	mov.b32 	%f296, %r245;
	add.f32 	%f297, %f295, %f296;
	mov.b32 	%r246, %f297;
	mov.u32 	%r247, 1;
	shfl.sync.bfly.b32 	%r248|%p73, %r246, %r247, %r233, %r235;
	mov.b32 	%f298, %r248;
	add.f32 	%f299, %f297, %f298;
	st.local.f32 	[%rd2+20], %f299;

$L__BB22_21:
	bar.sync 	0;
	mov.b32 	%r249, %f336;
	mov.u32 	%r250, 31;
	mov.u32 	%r251, 16;
	mov.u32 	%r252, -1;
	shfl.sync.bfly.b32 	%r253|%p75, %r249, %r251, %r250, %r252;
	mov.b32 	%f300, %r253;
	add.f32 	%f301, %f336, %f300;
	mov.b32 	%r254, %f301;
	mov.u32 	%r255, 8;
	shfl.sync.bfly.b32 	%r256|%p76, %r254, %r255, %r250, %r252;
	mov.b32 	%f302, %r256;
	add.f32 	%f303, %f301, %f302;
	mov.b32 	%r257, %f303;
	mov.u32 	%r258, 4;
	shfl.sync.bfly.b32 	%r259|%p77, %r257, %r258, %r250, %r252;
	mov.b32 	%f304, %r259;
	add.f32 	%f305, %f303, %f304;
	mov.b32 	%r260, %f305;
	mov.u32 	%r261, 2;
	shfl.sync.bfly.b32 	%r262|%p78, %r260, %r261, %r250, %r252;
	mov.b32 	%f306, %r262;
	add.f32 	%f307, %f305, %f306;
	mov.b32 	%r263, %f307;
	mov.u32 	%r264, 1;
	shfl.sync.bfly.b32 	%r265|%p79, %r263, %r264, %r250, %r252;
	mov.b32 	%f308, %r265;
	add.f32 	%f309, %f307, %f308;
	st.local.f32 	[%rd2+24], %f309;
	st.shared.f32 	[%r10], %f309;
	bar.sync 	0;
	@%p1 bra 	$L__BB22_23;

	ld.shared.f32 	%f310, [%r4];
	mov.b32 	%r266, %f310;
	shfl.sync.bfly.b32 	%r270|%p80, %r266, %r251, %r250, %r252;
	mov.b32 	%f311, %r270;
	add.f32 	%f312, %f310, %f311;
	mov.b32 	%r271, %f312;
	shfl.sync.bfly.b32 	%r273|%p81, %r271, %r255, %r250, %r252;
	mov.b32 	%f313, %r273;
	add.f32 	%f314, %f312, %f313;
	mov.b32 	%r274, %f314;
	shfl.sync.bfly.b32 	%r276|%p82, %r274, %r258, %r250, %r252;
	mov.b32 	%f315, %r276;
	add.f32 	%f316, %f314, %f315;
	mov.b32 	%r277, %f316;
	shfl.sync.bfly.b32 	%r279|%p83, %r277, %r261, %r250, %r252;
	mov.b32 	%f317, %r279;
	add.f32 	%f318, %f316, %f317;
	mov.b32 	%r280, %f318;
	shfl.sync.bfly.b32 	%r282|%p84, %r280, %r264, %r250, %r252;
	mov.b32 	%f319, %r282;
	add.f32 	%f320, %f318, %f319;
	st.local.f32 	[%rd2+24], %f320;

$L__BB22_23:
	bar.sync 	0;
	setp.gt.s32 	%p85, %r3, 6;
	@%p85 bra 	$L__BB22_25;

	mul.wide.s32 	%rd64, %r3, 4;
	add.s64 	%rd65, %rd2, %rd64;
	ld.local.f32 	%f321, [%rd65];
	mad.lo.s32 	%r283, %r3, %r13, %r2;
	cvt.s64.s32 	%rd66, %r283;
	mul.lo.s32 	%r284, %r1, %r14;
	cvt.s64.s32 	%rd67, %r284;
	add.s64 	%rd68, %rd67, %rd66;
	cvta.to.global.u64 	%rd69, %rd20;
	shl.b64 	%rd70, %rd68, 2;
	add.s64 	%rd71, %rd69, %rd70;
	st.global.f32 	[%rd71], %f321;

$L__BB22_25:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_8_bs_96
.visible .entry ggml_matvec_f32_ncols_8_bs_96(
	.param .u64 ggml_matvec_f32_ncols_8_bs_96_param_0,
	.param .u64 ggml_matvec_f32_ncols_8_bs_96_param_1,
	.param .u64 ggml_matvec_f32_ncols_8_bs_96_param_2,
	.param .u32 ggml_matvec_f32_ncols_8_bs_96_param_3,
	.param .u32 ggml_matvec_f32_ncols_8_bs_96_param_4,
	.param .u32 ggml_matvec_f32_ncols_8_bs_96_param_5,
	.param .u32 ggml_matvec_f32_ncols_8_bs_96_param_6,
	.param .u32 ggml_matvec_f32_ncols_8_bs_96_param_7,
	.param .u32 ggml_matvec_f32_ncols_8_bs_96_param_8,
	.param .u32 ggml_matvec_f32_ncols_8_bs_96_param_9,
	.param .u32 ggml_matvec_f32_ncols_8_bs_96_param_10,
	.param .u32 ggml_matvec_f32_ncols_8_bs_96_param_11
)
{
	.local .align 16 .b8 	__local_depot23[32];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<97>;
	.reg .f32 	%f<390>;
	.reg .b32 	%r<321>;
	.reg .b64 	%rd<78>;


	mov.u64 	%SPL, __local_depot23;
	ld.param.u64 	%rd22, [ggml_matvec_f32_ncols_8_bs_96_param_0];
	ld.param.u64 	%rd23, [ggml_matvec_f32_ncols_8_bs_96_param_1];
	ld.param.u64 	%rd21, [ggml_matvec_f32_ncols_8_bs_96_param_2];
	ld.param.u32 	%r11, [ggml_matvec_f32_ncols_8_bs_96_param_3];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_8_bs_96_param_5];
	ld.param.u32 	%r12, [ggml_matvec_f32_ncols_8_bs_96_param_6];
	ld.param.u32 	%r13, [ggml_matvec_f32_ncols_8_bs_96_param_7];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_8_bs_96_param_8];
	ld.param.u32 	%r17, [ggml_matvec_f32_ncols_8_bs_96_param_9];
	ld.param.u32 	%r18, [ggml_matvec_f32_ncols_8_bs_96_param_10];
	ld.param.u32 	%r14, [ggml_matvec_f32_ncols_8_bs_96_param_11];
	cvta.to.global.u64 	%rd77, %rd23;
	add.u64 	%rd2, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r19, %r1, %r16;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r20, %r2, %r15;
	mad.lo.s32 	%r21, %r19, %r17, %r20;
	cvt.s64.s32 	%rd3, %r21;
	cvta.to.global.u64 	%rd4, %rd22;
	mul.lo.s32 	%r22, %r1, %r18;
	cvt.s64.s32 	%rd5, %r22;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r23, %r3, 2;
	mov.u32 	%r24, data_mmv;
	add.s32 	%r4, %r24, %r23;
	@%p1 bra 	$L__BB23_2;

	mov.u32 	%r25, 0;
	st.shared.u32 	[%r4], %r25;

$L__BB23_2:
	bar.sync 	0;
	mov.f32 	%f382, 0f00000000;
	st.local.v4.f32 	[%rd2], {%f382, %f382, %f382, %f382};
	st.local.v4.f32 	[%rd2+16], {%f382, %f382, %f382, %f382};
	setp.ge.s32 	%p2, %r3, %r11;
	mov.f32 	%f383, %f382;
	mov.f32 	%f384, %f382;
	mov.f32 	%f385, %f382;
	mov.f32 	%f386, %f382;
	mov.f32 	%f387, %f382;
	mov.f32 	%f388, %f382;
	mov.f32 	%f389, %f382;
	@%p2 bra 	$L__BB23_9;

	not.b32 	%r26, %r3;
	add.s32 	%r5, %r26, %r11;
	mul.wide.u32 	%rd25, %r5, -1431655765;
	shr.u64 	%rd26, %rd25, 38;
	and.b64  	%rd27, %rd26, 1;
	setp.eq.b64 	%p3, %rd27, 1;
	mov.pred 	%p4, 0;
	xor.pred  	%p5, %p3, %p4;
	mov.f32 	%f382, 0f00000000;
	mov.u32 	%r320, %r3;
	@%p5 bra 	$L__BB23_5;

	shl.b64 	%rd28, %rd5, 2;
	add.s64 	%rd29, %rd77, %rd28;
	shl.b64 	%rd30, %rd3, 2;
	add.s64 	%rd31, %rd4, %rd30;
	mul.wide.s32 	%rd32, %r3, 8;
	add.s64 	%rd33, %rd31, %rd32;
	ld.global.nc.v2.f32 	{%f57, %f58}, [%rd33];
	add.s64 	%rd34, %rd29, %rd32;
	ld.global.nc.v2.f32 	{%f61, %f62}, [%rd34];
	fma.rn.f32 	%f65, %f57, %f61, 0f00000000;
	fma.rn.f32 	%f389, %f58, %f62, %f65;
	mul.wide.s32 	%rd35, %r12, 8;
	add.s64 	%rd36, %rd34, %rd35;
	ld.global.nc.v2.f32 	{%f66, %f67}, [%rd36];
	fma.rn.f32 	%f70, %f57, %f66, 0f00000000;
	fma.rn.f32 	%f388, %f58, %f67, %f70;
	add.s32 	%r27, %r3, %r12;
	add.s32 	%r28, %r27, %r12;
	mul.wide.s32 	%rd37, %r28, 8;
	add.s64 	%rd38, %rd29, %rd37;
	ld.global.nc.v2.f32 	{%f71, %f72}, [%rd38];
	fma.rn.f32 	%f75, %f57, %f71, 0f00000000;
	fma.rn.f32 	%f387, %f58, %f72, %f75;
	add.s64 	%rd39, %rd38, %rd35;
	ld.global.nc.v2.f32 	{%f76, %f77}, [%rd39];
	fma.rn.f32 	%f80, %f57, %f76, 0f00000000;
	fma.rn.f32 	%f386, %f58, %f77, %f80;
	st.local.v4.f32 	[%rd2], {%f389, %f388, %f387, %f386};
	add.s64 	%rd40, %rd39, %rd35;
	ld.global.nc.v2.f32 	{%f81, %f82}, [%rd40];
	fma.rn.f32 	%f85, %f57, %f81, 0f00000000;
	fma.rn.f32 	%f385, %f58, %f82, %f85;
	add.s64 	%rd41, %rd40, %rd35;
	ld.global.nc.v2.f32 	{%f86, %f87}, [%rd41];
	fma.rn.f32 	%f90, %f57, %f86, 0f00000000;
	fma.rn.f32 	%f384, %f58, %f87, %f90;
	add.s64 	%rd42, %rd41, %rd35;
	ld.global.nc.v2.f32 	{%f91, %f92}, [%rd42];
	fma.rn.f32 	%f95, %f57, %f91, 0f00000000;
	fma.rn.f32 	%f383, %f58, %f92, %f95;
	add.s64 	%rd43, %rd42, %rd35;
	ld.global.nc.v2.f32 	{%f96, %f97}, [%rd43];
	fma.rn.f32 	%f100, %f57, %f96, 0f00000000;
	fma.rn.f32 	%f382, %f58, %f97, %f100;
	st.local.v4.f32 	[%rd2+16], {%f385, %f384, %f383, %f382};
	add.s32 	%r320, %r3, 96;

$L__BB23_5:
	setp.lt.u32 	%p6, %r5, 96;
	@%p6 bra 	$L__BB23_9;

	add.s32 	%r29, %r320, %r12;
	add.s32 	%r30, %r29, 96;
	mul.wide.s32 	%rd44, %r30, 8;
	shl.b64 	%rd45, %rd5, 2;
	add.s64 	%rd7, %rd44, %rd45;
	shl.b32 	%r31, %r12, 1;
	add.s32 	%r32, %r320, %r31;
	mad.lo.s32 	%r33, %r12, 3, %r320;
	shl.b32 	%r34, %r12, 2;
	add.s32 	%r35, %r320, %r34;
	mad.lo.s32 	%r36, %r12, 5, %r320;
	mad.lo.s32 	%r37, %r12, 6, %r320;
	mad.lo.s32 	%r38, %r12, 7, %r320;
	mul.wide.s32 	%rd46, %r32, 8;
	add.s64 	%rd8, %rd46, %rd45;
	mul.wide.s32 	%rd47, %r33, 8;
	add.s64 	%rd9, %rd47, %rd45;
	mul.wide.s32 	%rd48, %r35, 8;
	add.s64 	%rd10, %rd48, %rd45;
	mul.wide.s32 	%rd49, %r36, 8;
	add.s64 	%rd11, %rd49, %rd45;
	mul.wide.s32 	%rd50, %r37, 8;
	add.s64 	%rd12, %rd50, %rd45;
	mul.wide.s32 	%rd51, %r38, 8;
	add.s64 	%rd13, %rd51, %rd45;
	mul.wide.s32 	%rd52, %r320, 2;
	add.s64 	%rd53, %rd52, %rd3;
	shl.b64 	%rd54, %rd53, 2;
	add.s64 	%rd55, %rd4, %rd54;
	add.s64 	%rd76, %rd55, 768;
	mul.wide.s32 	%rd56, %r320, 8;
	mul.wide.s32 	%rd57, %r12, 8;
	add.s64 	%rd58, %rd56, %rd57;
	add.s64 	%rd15, %rd58, %rd45;
	add.s64 	%rd16, %rd56, %rd45;

$L__BB23_7:
	ld.global.nc.v2.f32 	{%f101, %f102}, [%rd76+-768];
	add.s64 	%rd59, %rd77, %rd16;
	ld.global.nc.v2.f32 	{%f105, %f106}, [%rd59];
	fma.rn.f32 	%f109, %f101, %f105, %f389;
	fma.rn.f32 	%f110, %f102, %f106, %f109;
	add.s64 	%rd60, %rd77, %rd15;
	ld.global.nc.v2.f32 	{%f111, %f112}, [%rd60];
	fma.rn.f32 	%f115, %f101, %f111, %f388;
	fma.rn.f32 	%f116, %f102, %f112, %f115;
	add.s64 	%rd61, %rd77, %rd8;
	ld.global.nc.v2.f32 	{%f117, %f118}, [%rd61];
	fma.rn.f32 	%f121, %f101, %f117, %f387;
	fma.rn.f32 	%f122, %f102, %f118, %f121;
	add.s64 	%rd62, %rd77, %rd9;
	ld.global.nc.v2.f32 	{%f123, %f124}, [%rd62];
	fma.rn.f32 	%f127, %f101, %f123, %f386;
	fma.rn.f32 	%f128, %f102, %f124, %f127;
	add.s64 	%rd63, %rd77, %rd10;
	ld.global.nc.v2.f32 	{%f129, %f130}, [%rd63];
	fma.rn.f32 	%f133, %f101, %f129, %f385;
	fma.rn.f32 	%f134, %f102, %f130, %f133;
	add.s64 	%rd64, %rd77, %rd11;
	ld.global.nc.v2.f32 	{%f135, %f136}, [%rd64];
	fma.rn.f32 	%f139, %f101, %f135, %f384;
	fma.rn.f32 	%f140, %f102, %f136, %f139;
	add.s64 	%rd65, %rd77, %rd12;
	ld.global.nc.v2.f32 	{%f141, %f142}, [%rd65];
	fma.rn.f32 	%f145, %f101, %f141, %f383;
	fma.rn.f32 	%f146, %f102, %f142, %f145;
	add.s64 	%rd66, %rd77, %rd13;
	ld.global.nc.v2.f32 	{%f147, %f148}, [%rd66];
	fma.rn.f32 	%f151, %f101, %f147, %f382;
	fma.rn.f32 	%f152, %f102, %f148, %f151;
	ld.global.nc.v2.f32 	{%f153, %f154}, [%rd76];
	ld.global.nc.v2.f32 	{%f157, %f158}, [%rd59+768];
	fma.rn.f32 	%f161, %f153, %f157, %f110;
	fma.rn.f32 	%f389, %f154, %f158, %f161;
	add.s64 	%rd67, %rd77, %rd7;
	ld.global.nc.v2.f32 	{%f162, %f163}, [%rd67];
	fma.rn.f32 	%f166, %f153, %f162, %f116;
	fma.rn.f32 	%f388, %f154, %f163, %f166;
	ld.global.nc.v2.f32 	{%f167, %f168}, [%rd61+768];
	fma.rn.f32 	%f171, %f153, %f167, %f122;
	fma.rn.f32 	%f387, %f154, %f168, %f171;
	ld.global.nc.v2.f32 	{%f172, %f173}, [%rd62+768];
	fma.rn.f32 	%f176, %f153, %f172, %f128;
	fma.rn.f32 	%f386, %f154, %f173, %f176;
	ld.global.nc.v2.f32 	{%f177, %f178}, [%rd63+768];
	fma.rn.f32 	%f181, %f153, %f177, %f134;
	fma.rn.f32 	%f385, %f154, %f178, %f181;
	ld.global.nc.v2.f32 	{%f182, %f183}, [%rd64+768];
	fma.rn.f32 	%f186, %f153, %f182, %f140;
	fma.rn.f32 	%f384, %f154, %f183, %f186;
	ld.global.nc.v2.f32 	{%f187, %f188}, [%rd65+768];
	fma.rn.f32 	%f191, %f153, %f187, %f146;
	fma.rn.f32 	%f383, %f154, %f188, %f191;
	ld.global.nc.v2.f32 	{%f192, %f193}, [%rd66+768];
	fma.rn.f32 	%f196, %f153, %f192, %f152;
	fma.rn.f32 	%f382, %f154, %f193, %f196;
	add.s64 	%rd77, %rd77, 1536;
	add.s64 	%rd76, %rd76, 1536;
	add.s32 	%r320, %r320, 192;
	setp.lt.s32 	%p7, %r320, %r11;
	@%p7 bra 	$L__BB23_7;

	st.local.v4.f32 	[%rd2], {%f389, %f388, %f387, %f386};
	st.local.v4.f32 	[%rd2+16], {%f385, %f384, %f383, %f382};

$L__BB23_9:
	shr.s32 	%r39, %r3, 31;
	shr.u32 	%r40, %r39, 27;
	add.s32 	%r41, %r3, %r40;
	shr.s32 	%r42, %r41, 5;
	shl.b32 	%r43, %r42, 2;
	add.s32 	%r10, %r24, %r43;
	mov.u32 	%r45, 2;
	mov.b32 	%r46, %f389;
	mov.u32 	%r47, 31;
	mov.u32 	%r48, 16;
	mov.u32 	%r49, -1;
	shfl.sync.bfly.b32 	%r50|%p8, %r46, %r48, %r47, %r49;
	mov.b32 	%f197, %r50;
	add.f32 	%f198, %f389, %f197;
	mov.b32 	%r51, %f198;
	mov.u32 	%r52, 8;
	shfl.sync.bfly.b32 	%r53|%p9, %r51, %r52, %r47, %r49;
	mov.b32 	%f199, %r53;
	add.f32 	%f200, %f198, %f199;
	mov.b32 	%r54, %f200;
	mov.u32 	%r55, 4;
	shfl.sync.bfly.b32 	%r56|%p10, %r54, %r55, %r47, %r49;
	mov.b32 	%f201, %r56;
	add.f32 	%f202, %f200, %f201;
	mov.b32 	%r57, %f202;
	shfl.sync.bfly.b32 	%r58|%p11, %r57, %r45, %r47, %r49;
	mov.b32 	%f203, %r58;
	add.f32 	%f204, %f202, %f203;
	mov.b32 	%r59, %f204;
	mov.u32 	%r60, 1;
	shfl.sync.bfly.b32 	%r61|%p12, %r59, %r60, %r47, %r49;
	mov.b32 	%f205, %r61;
	add.f32 	%f206, %f204, %f205;
	st.local.f32 	[%rd2], %f206;
	st.shared.f32 	[%r10], %f206;
	bar.sync 	0;
	@%p1 bra 	$L__BB23_11;

	ld.shared.f32 	%f207, [%r4];
	mov.b32 	%r62, %f207;
	shfl.sync.bfly.b32 	%r66|%p14, %r62, %r48, %r47, %r49;
	mov.b32 	%f208, %r66;
	add.f32 	%f209, %f207, %f208;
	mov.b32 	%r67, %f209;
	shfl.sync.bfly.b32 	%r69|%p15, %r67, %r52, %r47, %r49;
	mov.b32 	%f210, %r69;
	add.f32 	%f211, %f209, %f210;
	mov.b32 	%r70, %f211;
	shfl.sync.bfly.b32 	%r72|%p16, %r70, %r55, %r47, %r49;
	mov.b32 	%f212, %r72;
	add.f32 	%f213, %f211, %f212;
	mov.b32 	%r73, %f213;
	shfl.sync.bfly.b32 	%r75|%p17, %r73, %r45, %r47, %r49;
	mov.b32 	%f214, %r75;
	add.f32 	%f215, %f213, %f214;
	mov.b32 	%r76, %f215;
	shfl.sync.bfly.b32 	%r78|%p18, %r76, %r60, %r47, %r49;
	mov.b32 	%f216, %r78;
	add.f32 	%f217, %f215, %f216;
	st.local.f32 	[%rd2], %f217;

$L__BB23_11:
	bar.sync 	0;
	mov.b32 	%r79, %f388;
	shfl.sync.bfly.b32 	%r83|%p20, %r79, %r48, %r47, %r49;
	mov.b32 	%f218, %r83;
	add.f32 	%f219, %f388, %f218;
	mov.b32 	%r84, %f219;
	shfl.sync.bfly.b32 	%r86|%p21, %r84, %r52, %r47, %r49;
	mov.b32 	%f220, %r86;
	add.f32 	%f221, %f219, %f220;
	mov.b32 	%r87, %f221;
	shfl.sync.bfly.b32 	%r89|%p22, %r87, %r55, %r47, %r49;
	mov.b32 	%f222, %r89;
	add.f32 	%f223, %f221, %f222;
	mov.b32 	%r90, %f223;
	shfl.sync.bfly.b32 	%r92|%p23, %r90, %r45, %r47, %r49;
	mov.b32 	%f224, %r92;
	add.f32 	%f225, %f223, %f224;
	mov.b32 	%r93, %f225;
	shfl.sync.bfly.b32 	%r95|%p24, %r93, %r60, %r47, %r49;
	mov.b32 	%f226, %r95;
	add.f32 	%f227, %f225, %f226;
	st.local.f32 	[%rd2+4], %f227;
	st.shared.f32 	[%r10], %f227;
	bar.sync 	0;
	@%p1 bra 	$L__BB23_13;

	ld.shared.f32 	%f228, [%r4];
	mov.b32 	%r96, %f228;
	mov.u32 	%r97, 31;
	mov.u32 	%r98, 16;
	mov.u32 	%r99, -1;
	shfl.sync.bfly.b32 	%r100|%p25, %r96, %r98, %r97, %r99;
	mov.b32 	%f229, %r100;
	add.f32 	%f230, %f228, %f229;
	mov.b32 	%r101, %f230;
	mov.u32 	%r102, 8;
	shfl.sync.bfly.b32 	%r103|%p26, %r101, %r102, %r97, %r99;
	mov.b32 	%f231, %r103;
	add.f32 	%f232, %f230, %f231;
	mov.b32 	%r104, %f232;
	mov.u32 	%r105, 4;
	shfl.sync.bfly.b32 	%r106|%p27, %r104, %r105, %r97, %r99;
	mov.b32 	%f233, %r106;
	add.f32 	%f234, %f232, %f233;
	mov.b32 	%r107, %f234;
	mov.u32 	%r108, 2;
	shfl.sync.bfly.b32 	%r109|%p28, %r107, %r108, %r97, %r99;
	mov.b32 	%f235, %r109;
	add.f32 	%f236, %f234, %f235;
	mov.b32 	%r110, %f236;
	mov.u32 	%r111, 1;
	shfl.sync.bfly.b32 	%r112|%p29, %r110, %r111, %r97, %r99;
	mov.b32 	%f237, %r112;
	add.f32 	%f238, %f236, %f237;
	st.local.f32 	[%rd2+4], %f238;

$L__BB23_13:
	bar.sync 	0;
	mov.b32 	%r113, %f387;
	mov.u32 	%r114, 31;
	mov.u32 	%r115, 16;
	mov.u32 	%r116, -1;
	shfl.sync.bfly.b32 	%r117|%p31, %r113, %r115, %r114, %r116;
	mov.b32 	%f239, %r117;
	add.f32 	%f240, %f387, %f239;
	mov.b32 	%r118, %f240;
	mov.u32 	%r119, 8;
	shfl.sync.bfly.b32 	%r120|%p32, %r118, %r119, %r114, %r116;
	mov.b32 	%f241, %r120;
	add.f32 	%f242, %f240, %f241;
	mov.b32 	%r121, %f242;
	mov.u32 	%r122, 4;
	shfl.sync.bfly.b32 	%r123|%p33, %r121, %r122, %r114, %r116;
	mov.b32 	%f243, %r123;
	add.f32 	%f244, %f242, %f243;
	mov.b32 	%r124, %f244;
	mov.u32 	%r125, 2;
	shfl.sync.bfly.b32 	%r126|%p34, %r124, %r125, %r114, %r116;
	mov.b32 	%f245, %r126;
	add.f32 	%f246, %f244, %f245;
	mov.b32 	%r127, %f246;
	mov.u32 	%r128, 1;
	shfl.sync.bfly.b32 	%r129|%p35, %r127, %r128, %r114, %r116;
	mov.b32 	%f247, %r129;
	add.f32 	%f248, %f246, %f247;
	st.local.f32 	[%rd2+8], %f248;
	st.shared.f32 	[%r10], %f248;
	bar.sync 	0;
	@%p1 bra 	$L__BB23_15;

	ld.shared.f32 	%f249, [%r4];
	mov.b32 	%r130, %f249;
	shfl.sync.bfly.b32 	%r134|%p36, %r130, %r115, %r114, %r116;
	mov.b32 	%f250, %r134;
	add.f32 	%f251, %f249, %f250;
	mov.b32 	%r135, %f251;
	shfl.sync.bfly.b32 	%r137|%p37, %r135, %r119, %r114, %r116;
	mov.b32 	%f252, %r137;
	add.f32 	%f253, %f251, %f252;
	mov.b32 	%r138, %f253;
	shfl.sync.bfly.b32 	%r140|%p38, %r138, %r122, %r114, %r116;
	mov.b32 	%f254, %r140;
	add.f32 	%f255, %f253, %f254;
	mov.b32 	%r141, %f255;
	shfl.sync.bfly.b32 	%r143|%p39, %r141, %r125, %r114, %r116;
	mov.b32 	%f256, %r143;
	add.f32 	%f257, %f255, %f256;
	mov.b32 	%r144, %f257;
	shfl.sync.bfly.b32 	%r146|%p40, %r144, %r128, %r114, %r116;
	mov.b32 	%f258, %r146;
	add.f32 	%f259, %f257, %f258;
	st.local.f32 	[%rd2+8], %f259;

$L__BB23_15:
	bar.sync 	0;
	mov.b32 	%r147, %f386;
	shfl.sync.bfly.b32 	%r151|%p42, %r147, %r115, %r114, %r116;
	mov.b32 	%f260, %r151;
	add.f32 	%f261, %f386, %f260;
	mov.b32 	%r152, %f261;
	shfl.sync.bfly.b32 	%r154|%p43, %r152, %r119, %r114, %r116;
	mov.b32 	%f262, %r154;
	add.f32 	%f263, %f261, %f262;
	mov.b32 	%r155, %f263;
	shfl.sync.bfly.b32 	%r157|%p44, %r155, %r122, %r114, %r116;
	mov.b32 	%f264, %r157;
	add.f32 	%f265, %f263, %f264;
	mov.b32 	%r158, %f265;
	shfl.sync.bfly.b32 	%r160|%p45, %r158, %r125, %r114, %r116;
	mov.b32 	%f266, %r160;
	add.f32 	%f267, %f265, %f266;
	mov.b32 	%r161, %f267;
	shfl.sync.bfly.b32 	%r163|%p46, %r161, %r128, %r114, %r116;
	mov.b32 	%f268, %r163;
	add.f32 	%f269, %f267, %f268;
	st.local.f32 	[%rd2+12], %f269;
	st.shared.f32 	[%r10], %f269;
	bar.sync 	0;
	@%p1 bra 	$L__BB23_17;

	ld.shared.f32 	%f270, [%r4];
	mov.b32 	%r164, %f270;
	mov.u32 	%r165, 31;
	mov.u32 	%r166, 16;
	mov.u32 	%r167, -1;
	shfl.sync.bfly.b32 	%r168|%p47, %r164, %r166, %r165, %r167;
	mov.b32 	%f271, %r168;
	add.f32 	%f272, %f270, %f271;
	mov.b32 	%r169, %f272;
	mov.u32 	%r170, 8;
	shfl.sync.bfly.b32 	%r171|%p48, %r169, %r170, %r165, %r167;
	mov.b32 	%f273, %r171;
	add.f32 	%f274, %f272, %f273;
	mov.b32 	%r172, %f274;
	mov.u32 	%r173, 4;
	shfl.sync.bfly.b32 	%r174|%p49, %r172, %r173, %r165, %r167;
	mov.b32 	%f275, %r174;
	add.f32 	%f276, %f274, %f275;
	mov.b32 	%r175, %f276;
	mov.u32 	%r176, 2;
	shfl.sync.bfly.b32 	%r177|%p50, %r175, %r176, %r165, %r167;
	mov.b32 	%f277, %r177;
	add.f32 	%f278, %f276, %f277;
	mov.b32 	%r178, %f278;
	mov.u32 	%r179, 1;
	shfl.sync.bfly.b32 	%r180|%p51, %r178, %r179, %r165, %r167;
	mov.b32 	%f279, %r180;
	add.f32 	%f280, %f278, %f279;
	st.local.f32 	[%rd2+12], %f280;

$L__BB23_17:
	bar.sync 	0;
	mov.b32 	%r181, %f385;
	mov.u32 	%r182, 31;
	mov.u32 	%r183, 16;
	mov.u32 	%r184, -1;
	shfl.sync.bfly.b32 	%r185|%p53, %r181, %r183, %r182, %r184;
	mov.b32 	%f281, %r185;
	add.f32 	%f282, %f385, %f281;
	mov.b32 	%r186, %f282;
	mov.u32 	%r187, 8;
	shfl.sync.bfly.b32 	%r188|%p54, %r186, %r187, %r182, %r184;
	mov.b32 	%f283, %r188;
	add.f32 	%f284, %f282, %f283;
	mov.b32 	%r189, %f284;
	mov.u32 	%r190, 4;
	shfl.sync.bfly.b32 	%r191|%p55, %r189, %r190, %r182, %r184;
	mov.b32 	%f285, %r191;
	add.f32 	%f286, %f284, %f285;
	mov.b32 	%r192, %f286;
	mov.u32 	%r193, 2;
	shfl.sync.bfly.b32 	%r194|%p56, %r192, %r193, %r182, %r184;
	mov.b32 	%f287, %r194;
	add.f32 	%f288, %f286, %f287;
	mov.b32 	%r195, %f288;
	mov.u32 	%r196, 1;
	shfl.sync.bfly.b32 	%r197|%p57, %r195, %r196, %r182, %r184;
	mov.b32 	%f289, %r197;
	add.f32 	%f290, %f288, %f289;
	st.local.f32 	[%rd2+16], %f290;
	st.shared.f32 	[%r10], %f290;
	bar.sync 	0;
	@%p1 bra 	$L__BB23_19;

	ld.shared.f32 	%f291, [%r4];
	mov.b32 	%r198, %f291;
	shfl.sync.bfly.b32 	%r202|%p58, %r198, %r183, %r182, %r184;
	mov.b32 	%f292, %r202;
	add.f32 	%f293, %f291, %f292;
	mov.b32 	%r203, %f293;
	shfl.sync.bfly.b32 	%r205|%p59, %r203, %r187, %r182, %r184;
	mov.b32 	%f294, %r205;
	add.f32 	%f295, %f293, %f294;
	mov.b32 	%r206, %f295;
	shfl.sync.bfly.b32 	%r208|%p60, %r206, %r190, %r182, %r184;
	mov.b32 	%f296, %r208;
	add.f32 	%f297, %f295, %f296;
	mov.b32 	%r209, %f297;
	shfl.sync.bfly.b32 	%r211|%p61, %r209, %r193, %r182, %r184;
	mov.b32 	%f298, %r211;
	add.f32 	%f299, %f297, %f298;
	mov.b32 	%r212, %f299;
	shfl.sync.bfly.b32 	%r214|%p62, %r212, %r196, %r182, %r184;
	mov.b32 	%f300, %r214;
	add.f32 	%f301, %f299, %f300;
	st.local.f32 	[%rd2+16], %f301;

$L__BB23_19:
	bar.sync 	0;
	mov.b32 	%r215, %f384;
	shfl.sync.bfly.b32 	%r219|%p64, %r215, %r183, %r182, %r184;
	mov.b32 	%f302, %r219;
	add.f32 	%f303, %f384, %f302;
	mov.b32 	%r220, %f303;
	shfl.sync.bfly.b32 	%r222|%p65, %r220, %r187, %r182, %r184;
	mov.b32 	%f304, %r222;
	add.f32 	%f305, %f303, %f304;
	mov.b32 	%r223, %f305;
	shfl.sync.bfly.b32 	%r225|%p66, %r223, %r190, %r182, %r184;
	mov.b32 	%f306, %r225;
	add.f32 	%f307, %f305, %f306;
	mov.b32 	%r226, %f307;
	shfl.sync.bfly.b32 	%r228|%p67, %r226, %r193, %r182, %r184;
	mov.b32 	%f308, %r228;
	add.f32 	%f309, %f307, %f308;
	mov.b32 	%r229, %f309;
	shfl.sync.bfly.b32 	%r231|%p68, %r229, %r196, %r182, %r184;
	mov.b32 	%f310, %r231;
	add.f32 	%f311, %f309, %f310;
	st.local.f32 	[%rd2+20], %f311;
	st.shared.f32 	[%r10], %f311;
	bar.sync 	0;
	@%p1 bra 	$L__BB23_21;

	ld.shared.f32 	%f312, [%r4];
	mov.b32 	%r232, %f312;
	mov.u32 	%r233, 31;
	mov.u32 	%r234, 16;
	mov.u32 	%r235, -1;
	shfl.sync.bfly.b32 	%r236|%p69, %r232, %r234, %r233, %r235;
	mov.b32 	%f313, %r236;
	add.f32 	%f314, %f312, %f313;
	mov.b32 	%r237, %f314;
	mov.u32 	%r238, 8;
	shfl.sync.bfly.b32 	%r239|%p70, %r237, %r238, %r233, %r235;
	mov.b32 	%f315, %r239;
	add.f32 	%f316, %f314, %f315;
	mov.b32 	%r240, %f316;
	mov.u32 	%r241, 4;
	shfl.sync.bfly.b32 	%r242|%p71, %r240, %r241, %r233, %r235;
	mov.b32 	%f317, %r242;
	add.f32 	%f318, %f316, %f317;
	mov.b32 	%r243, %f318;
	mov.u32 	%r244, 2;
	shfl.sync.bfly.b32 	%r245|%p72, %r243, %r244, %r233, %r235;
	mov.b32 	%f319, %r245;
	add.f32 	%f320, %f318, %f319;
	mov.b32 	%r246, %f320;
	mov.u32 	%r247, 1;
	shfl.sync.bfly.b32 	%r248|%p73, %r246, %r247, %r233, %r235;
	mov.b32 	%f321, %r248;
	add.f32 	%f322, %f320, %f321;
	st.local.f32 	[%rd2+20], %f322;

$L__BB23_21:
	bar.sync 	0;
	mov.b32 	%r249, %f383;
	mov.u32 	%r250, 31;
	mov.u32 	%r251, 16;
	mov.u32 	%r252, -1;
	shfl.sync.bfly.b32 	%r253|%p75, %r249, %r251, %r250, %r252;
	mov.b32 	%f323, %r253;
	add.f32 	%f324, %f383, %f323;
	mov.b32 	%r254, %f324;
	mov.u32 	%r255, 8;
	shfl.sync.bfly.b32 	%r256|%p76, %r254, %r255, %r250, %r252;
	mov.b32 	%f325, %r256;
	add.f32 	%f326, %f324, %f325;
	mov.b32 	%r257, %f326;
	mov.u32 	%r258, 4;
	shfl.sync.bfly.b32 	%r259|%p77, %r257, %r258, %r250, %r252;
	mov.b32 	%f327, %r259;
	add.f32 	%f328, %f326, %f327;
	mov.b32 	%r260, %f328;
	mov.u32 	%r261, 2;
	shfl.sync.bfly.b32 	%r262|%p78, %r260, %r261, %r250, %r252;
	mov.b32 	%f329, %r262;
	add.f32 	%f330, %f328, %f329;
	mov.b32 	%r263, %f330;
	mov.u32 	%r264, 1;
	shfl.sync.bfly.b32 	%r265|%p79, %r263, %r264, %r250, %r252;
	mov.b32 	%f331, %r265;
	add.f32 	%f332, %f330, %f331;
	st.local.f32 	[%rd2+24], %f332;
	st.shared.f32 	[%r10], %f332;
	bar.sync 	0;
	@%p1 bra 	$L__BB23_23;

	ld.shared.f32 	%f333, [%r4];
	mov.b32 	%r266, %f333;
	shfl.sync.bfly.b32 	%r270|%p80, %r266, %r251, %r250, %r252;
	mov.b32 	%f334, %r270;
	add.f32 	%f335, %f333, %f334;
	mov.b32 	%r271, %f335;
	shfl.sync.bfly.b32 	%r273|%p81, %r271, %r255, %r250, %r252;
	mov.b32 	%f336, %r273;
	add.f32 	%f337, %f335, %f336;
	mov.b32 	%r274, %f337;
	shfl.sync.bfly.b32 	%r276|%p82, %r274, %r258, %r250, %r252;
	mov.b32 	%f338, %r276;
	add.f32 	%f339, %f337, %f338;
	mov.b32 	%r277, %f339;
	shfl.sync.bfly.b32 	%r279|%p83, %r277, %r261, %r250, %r252;
	mov.b32 	%f340, %r279;
	add.f32 	%f341, %f339, %f340;
	mov.b32 	%r280, %f341;
	shfl.sync.bfly.b32 	%r282|%p84, %r280, %r264, %r250, %r252;
	mov.b32 	%f342, %r282;
	add.f32 	%f343, %f341, %f342;
	st.local.f32 	[%rd2+24], %f343;

$L__BB23_23:
	bar.sync 	0;
	mov.b32 	%r283, %f382;
	shfl.sync.bfly.b32 	%r287|%p86, %r283, %r251, %r250, %r252;
	mov.b32 	%f344, %r287;
	add.f32 	%f345, %f382, %f344;
	mov.b32 	%r288, %f345;
	shfl.sync.bfly.b32 	%r290|%p87, %r288, %r255, %r250, %r252;
	mov.b32 	%f346, %r290;
	add.f32 	%f347, %f345, %f346;
	mov.b32 	%r291, %f347;
	shfl.sync.bfly.b32 	%r293|%p88, %r291, %r258, %r250, %r252;
	mov.b32 	%f348, %r293;
	add.f32 	%f349, %f347, %f348;
	mov.b32 	%r294, %f349;
	shfl.sync.bfly.b32 	%r296|%p89, %r294, %r261, %r250, %r252;
	mov.b32 	%f350, %r296;
	add.f32 	%f351, %f349, %f350;
	mov.b32 	%r297, %f351;
	shfl.sync.bfly.b32 	%r299|%p90, %r297, %r264, %r250, %r252;
	mov.b32 	%f352, %r299;
	add.f32 	%f353, %f351, %f352;
	st.local.f32 	[%rd2+28], %f353;
	st.shared.f32 	[%r10], %f353;
	bar.sync 	0;
	@%p1 bra 	$L__BB23_25;

	ld.shared.f32 	%f354, [%r4];
	mov.b32 	%r300, %f354;
	mov.u32 	%r301, 31;
	mov.u32 	%r302, 16;
	mov.u32 	%r303, -1;
	shfl.sync.bfly.b32 	%r304|%p91, %r300, %r302, %r301, %r303;
	mov.b32 	%f355, %r304;
	add.f32 	%f356, %f354, %f355;
	mov.b32 	%r305, %f356;
	mov.u32 	%r306, 8;
	shfl.sync.bfly.b32 	%r307|%p92, %r305, %r306, %r301, %r303;
	mov.b32 	%f357, %r307;
	add.f32 	%f358, %f356, %f357;
	mov.b32 	%r308, %f358;
	mov.u32 	%r309, 4;
	shfl.sync.bfly.b32 	%r310|%p93, %r308, %r309, %r301, %r303;
	mov.b32 	%f359, %r310;
	add.f32 	%f360, %f358, %f359;
	mov.b32 	%r311, %f360;
	mov.u32 	%r312, 2;
	shfl.sync.bfly.b32 	%r313|%p94, %r311, %r312, %r301, %r303;
	mov.b32 	%f361, %r313;
	add.f32 	%f362, %f360, %f361;
	mov.b32 	%r314, %f362;
	mov.u32 	%r315, 1;
	shfl.sync.bfly.b32 	%r316|%p95, %r314, %r315, %r301, %r303;
	mov.b32 	%f363, %r316;
	add.f32 	%f364, %f362, %f363;
	st.local.f32 	[%rd2+28], %f364;

$L__BB23_25:
	bar.sync 	0;
	setp.gt.s32 	%p96, %r3, 7;
	@%p96 bra 	$L__BB23_27;

	mul.wide.s32 	%rd68, %r3, 4;
	add.s64 	%rd69, %rd2, %rd68;
	ld.local.f32 	%f365, [%rd69];
	mad.lo.s32 	%r317, %r3, %r13, %r2;
	cvt.s64.s32 	%rd70, %r317;
	mul.lo.s32 	%r318, %r1, %r14;
	cvt.s64.s32 	%rd71, %r318;
	add.s64 	%rd72, %rd71, %rd70;
	cvta.to.global.u64 	%rd73, %rd21;
	shl.b64 	%rd74, %rd72, 2;
	add.s64 	%rd75, %rd73, %rd74;
	st.global.f32 	[%rd75], %f365;

$L__BB23_27:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_1_bs_128
.visible .entry ggml_matvec_f32_ncols_1_bs_128(
	.param .u64 ggml_matvec_f32_ncols_1_bs_128_param_0,
	.param .u64 ggml_matvec_f32_ncols_1_bs_128_param_1,
	.param .u64 ggml_matvec_f32_ncols_1_bs_128_param_2,
	.param .u32 ggml_matvec_f32_ncols_1_bs_128_param_3,
	.param .u32 ggml_matvec_f32_ncols_1_bs_128_param_4,
	.param .u32 ggml_matvec_f32_ncols_1_bs_128_param_5,
	.param .u32 ggml_matvec_f32_ncols_1_bs_128_param_6,
	.param .u32 ggml_matvec_f32_ncols_1_bs_128_param_7,
	.param .u32 ggml_matvec_f32_ncols_1_bs_128_param_8,
	.param .u32 ggml_matvec_f32_ncols_1_bs_128_param_9,
	.param .u32 ggml_matvec_f32_ncols_1_bs_128_param_10,
	.param .u32 ggml_matvec_f32_ncols_1_bs_128_param_11
)
{
	.reg .pred 	%p<19>;
	.reg .f32 	%f<88>;
	.reg .b32 	%r<79>;
	.reg .b64 	%rd<42>;


	ld.param.u64 	%rd18, [ggml_matvec_f32_ncols_1_bs_128_param_0];
	ld.param.u64 	%rd19, [ggml_matvec_f32_ncols_1_bs_128_param_1];
	ld.param.u64 	%rd17, [ggml_matvec_f32_ncols_1_bs_128_param_2];
	ld.param.u32 	%r13, [ggml_matvec_f32_ncols_1_bs_128_param_3];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_1_bs_128_param_5];
	ld.param.u32 	%r14, [ggml_matvec_f32_ncols_1_bs_128_param_7];
	ld.param.u32 	%r17, [ggml_matvec_f32_ncols_1_bs_128_param_8];
	ld.param.u32 	%r18, [ggml_matvec_f32_ncols_1_bs_128_param_9];
	ld.param.u32 	%r19, [ggml_matvec_f32_ncols_1_bs_128_param_10];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_1_bs_128_param_11];
	cvta.to.global.u64 	%rd1, %rd19;
	cvta.to.global.u64 	%rd2, %rd18;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r20, %r1, %r17;
	mov.u32 	%r21, %ctaid.x;
	mul.lo.s32 	%r22, %r21, %r16;
	mad.lo.s32 	%r23, %r20, %r18, %r22;
	cvt.s64.s32 	%rd3, %r23;
	mul.lo.s32 	%r24, %r1, %r19;
	cvt.s64.s32 	%rd4, %r24;
	mov.u32 	%r2, %tid.x;
	setp.gt.s32 	%p1, %r2, 31;
	shl.b32 	%r25, %r2, 2;
	mov.u32 	%r26, data_mmv;
	add.s32 	%r3, %r26, %r25;
	@%p1 bra 	$L__BB24_2;

	mov.u32 	%r27, 0;
	st.shared.u32 	[%r3], %r27;

$L__BB24_2:
	bar.sync 	0;
	setp.ge.s32 	%p2, %r2, %r13;
	mov.f32 	%f86, 0f00000000;
	@%p2 bra 	$L__BB24_9;

	not.b32 	%r28, %r2;
	add.s32 	%r4, %r28, %r13;
	shr.u32 	%r29, %r4, 7;
	add.s32 	%r30, %r29, 1;
	and.b32  	%r76, %r30, 3;
	setp.eq.s32 	%p3, %r76, 0;
	mov.f32 	%f86, 0f00000000;
	mov.u32 	%r77, %r2;
	@%p3 bra 	$L__BB24_6;

	mul.wide.s32 	%rd20, %r2, 2;
	add.s64 	%rd21, %rd20, %rd4;
	shl.b64 	%rd22, %rd21, 2;
	add.s64 	%rd39, %rd1, %rd22;
	add.s64 	%rd23, %rd20, %rd3;
	shl.b64 	%rd24, %rd23, 2;
	add.s64 	%rd38, %rd2, %rd24;
	mov.f32 	%f86, 0f00000000;
	mov.u32 	%r77, %r2;

$L__BB24_5:
	.pragma "nounroll";
	ld.global.nc.v2.f32 	{%f15, %f16}, [%rd38];
	ld.global.nc.v2.f32 	{%f19, %f20}, [%rd39];
	fma.rn.f32 	%f23, %f15, %f19, %f86;
	fma.rn.f32 	%f86, %f16, %f20, %f23;
	add.s32 	%r77, %r77, 128;
	add.s64 	%rd39, %rd39, 1024;
	add.s64 	%rd38, %rd38, 1024;
	add.s32 	%r76, %r76, -1;
	setp.ne.s32 	%p4, %r76, 0;
	@%p4 bra 	$L__BB24_5;

$L__BB24_6:
	setp.lt.u32 	%p5, %r4, 384;
	@%p5 bra 	$L__BB24_9;

	mul.wide.s32 	%rd25, %r77, 2;
	add.s64 	%rd26, %rd25, %rd3;
	shl.b64 	%rd27, %rd26, 2;
	add.s64 	%rd28, %rd2, %rd27;
	add.s64 	%rd41, %rd28, 2048;
	add.s64 	%rd29, %rd25, %rd4;
	shl.b64 	%rd30, %rd29, 2;
	add.s64 	%rd31, %rd1, %rd30;
	add.s64 	%rd40, %rd31, 2048;

$L__BB24_8:
	ld.global.nc.v2.f32 	{%f24, %f25}, [%rd41+-2048];
	ld.global.nc.v2.f32 	{%f28, %f29}, [%rd40+-2048];
	fma.rn.f32 	%f32, %f24, %f28, %f86;
	fma.rn.f32 	%f33, %f25, %f29, %f32;
	ld.global.nc.v2.f32 	{%f34, %f35}, [%rd41+-1024];
	ld.global.nc.v2.f32 	{%f38, %f39}, [%rd40+-1024];
	fma.rn.f32 	%f42, %f34, %f38, %f33;
	fma.rn.f32 	%f43, %f35, %f39, %f42;
	ld.global.nc.v2.f32 	{%f44, %f45}, [%rd41];
	ld.global.nc.v2.f32 	{%f48, %f49}, [%rd40];
	fma.rn.f32 	%f52, %f44, %f48, %f43;
	fma.rn.f32 	%f53, %f45, %f49, %f52;
	ld.global.nc.v2.f32 	{%f54, %f55}, [%rd41+1024];
	ld.global.nc.v2.f32 	{%f58, %f59}, [%rd40+1024];
	fma.rn.f32 	%f62, %f54, %f58, %f53;
	fma.rn.f32 	%f86, %f55, %f59, %f62;
	add.s64 	%rd41, %rd41, 4096;
	add.s64 	%rd40, %rd40, 4096;
	add.s32 	%r77, %r77, 512;
	setp.lt.s32 	%p6, %r77, %r13;
	@%p6 bra 	$L__BB24_8;

$L__BB24_9:
	mov.b32 	%r31, %f86;
	mov.u32 	%r32, 31;
	mov.u32 	%r33, 16;
	mov.u32 	%r34, -1;
	shfl.sync.bfly.b32 	%r35|%p7, %r31, %r33, %r32, %r34;
	mov.b32 	%f63, %r35;
	add.f32 	%f64, %f86, %f63;
	mov.b32 	%r36, %f64;
	mov.u32 	%r37, 8;
	shfl.sync.bfly.b32 	%r38|%p8, %r36, %r37, %r32, %r34;
	mov.b32 	%f65, %r38;
	add.f32 	%f66, %f64, %f65;
	mov.b32 	%r39, %f66;
	mov.u32 	%r40, 4;
	shfl.sync.bfly.b32 	%r41|%p9, %r39, %r40, %r32, %r34;
	mov.b32 	%f67, %r41;
	add.f32 	%f68, %f66, %f67;
	mov.b32 	%r42, %f68;
	mov.u32 	%r43, 2;
	shfl.sync.bfly.b32 	%r44|%p10, %r42, %r43, %r32, %r34;
	mov.b32 	%f69, %r44;
	add.f32 	%f70, %f68, %f69;
	mov.b32 	%r45, %f70;
	mov.u32 	%r46, 1;
	shfl.sync.bfly.b32 	%r47|%p11, %r45, %r46, %r32, %r34;
	mov.b32 	%f71, %r47;
	add.f32 	%f87, %f70, %f71;
	shr.s32 	%r48, %r2, 31;
	shr.u32 	%r49, %r48, 27;
	add.s32 	%r50, %r2, %r49;
	shr.s32 	%r51, %r50, 5;
	shl.b32 	%r52, %r51, 2;
	add.s32 	%r54, %r26, %r52;
	st.shared.f32 	[%r54], %f87;
	bar.sync 	0;
	@%p1 bra 	$L__BB24_11;

	ld.shared.f32 	%f72, [%r3];
	mov.b32 	%r55, %f72;
	shfl.sync.bfly.b32 	%r59|%p13, %r55, %r33, %r32, %r34;
	mov.b32 	%f73, %r59;
	add.f32 	%f74, %f72, %f73;
	mov.b32 	%r60, %f74;
	shfl.sync.bfly.b32 	%r62|%p14, %r60, %r37, %r32, %r34;
	mov.b32 	%f75, %r62;
	add.f32 	%f76, %f74, %f75;
	mov.b32 	%r63, %f76;
	shfl.sync.bfly.b32 	%r65|%p15, %r63, %r40, %r32, %r34;
	mov.b32 	%f77, %r65;
	add.f32 	%f78, %f76, %f77;
	mov.b32 	%r66, %f78;
	shfl.sync.bfly.b32 	%r68|%p16, %r66, %r43, %r32, %r34;
	mov.b32 	%f79, %r68;
	add.f32 	%f80, %f78, %f79;
	mov.b32 	%r69, %f80;
	shfl.sync.bfly.b32 	%r71|%p17, %r69, %r46, %r32, %r34;
	mov.b32 	%f81, %r71;
	add.f32 	%f87, %f80, %f81;

$L__BB24_11:
	bar.sync 	0;
	setp.gt.s32 	%p18, %r2, 0;
	@%p18 bra 	$L__BB24_13;

	mad.lo.s32 	%r73, %r2, %r14, %r21;
	cvt.s64.s32 	%rd32, %r73;
	mul.lo.s32 	%r74, %r1, %r15;
	cvt.s64.s32 	%rd33, %r74;
	add.s64 	%rd34, %rd33, %rd32;
	cvta.to.global.u64 	%rd35, %rd17;
	shl.b64 	%rd36, %rd34, 2;
	add.s64 	%rd37, %rd35, %rd36;
	st.global.f32 	[%rd37], %f87;

$L__BB24_13:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_2_bs_128
.visible .entry ggml_matvec_f32_ncols_2_bs_128(
	.param .u64 ggml_matvec_f32_ncols_2_bs_128_param_0,
	.param .u64 ggml_matvec_f32_ncols_2_bs_128_param_1,
	.param .u64 ggml_matvec_f32_ncols_2_bs_128_param_2,
	.param .u32 ggml_matvec_f32_ncols_2_bs_128_param_3,
	.param .u32 ggml_matvec_f32_ncols_2_bs_128_param_4,
	.param .u32 ggml_matvec_f32_ncols_2_bs_128_param_5,
	.param .u32 ggml_matvec_f32_ncols_2_bs_128_param_6,
	.param .u32 ggml_matvec_f32_ncols_2_bs_128_param_7,
	.param .u32 ggml_matvec_f32_ncols_2_bs_128_param_8,
	.param .u32 ggml_matvec_f32_ncols_2_bs_128_param_9,
	.param .u32 ggml_matvec_f32_ncols_2_bs_128_param_10,
	.param .u32 ggml_matvec_f32_ncols_2_bs_128_param_11
)
{
	.local .align 8 .b8 	__local_depot25[8];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<30>;
	.reg .f32 	%f<146>;
	.reg .b32 	%r<113>;
	.reg .b64 	%rd<64>;


	mov.u64 	%SPL, __local_depot25;
	ld.param.u64 	%rd27, [ggml_matvec_f32_ncols_2_bs_128_param_0];
	ld.param.u64 	%rd28, [ggml_matvec_f32_ncols_2_bs_128_param_1];
	ld.param.u64 	%rd26, [ggml_matvec_f32_ncols_2_bs_128_param_2];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_2_bs_128_param_3];
	ld.param.u32 	%r19, [ggml_matvec_f32_ncols_2_bs_128_param_5];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_2_bs_128_param_6];
	ld.param.u32 	%r17, [ggml_matvec_f32_ncols_2_bs_128_param_7];
	ld.param.u32 	%r20, [ggml_matvec_f32_ncols_2_bs_128_param_8];
	ld.param.u32 	%r21, [ggml_matvec_f32_ncols_2_bs_128_param_9];
	ld.param.u32 	%r22, [ggml_matvec_f32_ncols_2_bs_128_param_10];
	ld.param.u32 	%r18, [ggml_matvec_f32_ncols_2_bs_128_param_11];
	cvta.to.global.u64 	%rd1, %rd28;
	cvta.to.global.u64 	%rd2, %rd27;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r23, %r1, %r20;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r24, %r2, %r19;
	mad.lo.s32 	%r25, %r23, %r21, %r24;
	cvt.s64.s32 	%rd4, %r25;
	mul.lo.s32 	%r26, %r1, %r22;
	cvt.s64.s32 	%rd5, %r26;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r27, %r3, 2;
	mov.u32 	%r28, data_mmv;
	add.s32 	%r4, %r28, %r27;
	@%p1 bra 	$L__BB25_2;

	mov.u32 	%r29, 0;
	st.shared.u32 	[%r4], %r29;

$L__BB25_2:
	bar.sync 	0;
	mov.f32 	%f144, 0f00000000;
	st.local.v2.f32 	[%rd3], {%f144, %f144};
	setp.ge.s32 	%p2, %r3, %r15;
	mov.f32 	%f145, %f144;
	@%p2 bra 	$L__BB25_11;

	not.b32 	%r30, %r3;
	add.s32 	%r5, %r30, %r15;
	shr.u32 	%r31, %r5, 7;
	add.s32 	%r32, %r31, 1;
	and.b32  	%r110, %r32, 3;
	setp.eq.s32 	%p3, %r110, 0;
	mov.f32 	%f144, 0f00000000;
	mov.u32 	%r111, %r3;
	@%p3 bra 	$L__BB25_7;

	mul.wide.s32 	%rd30, %r16, 2;
	mul.wide.s32 	%rd31, %r3, 2;
	add.s64 	%rd32, %rd30, %rd31;
	add.s64 	%rd33, %rd32, %rd5;
	shl.b64 	%rd34, %rd33, 2;
	add.s64 	%rd60, %rd1, %rd34;
	add.s64 	%rd35, %rd31, %rd5;
	shl.b64 	%rd36, %rd35, 2;
	add.s64 	%rd59, %rd1, %rd36;
	add.s64 	%rd37, %rd31, %rd4;
	shl.b64 	%rd38, %rd37, 2;
	add.s64 	%rd58, %rd2, %rd38;
	mov.f32 	%f144, 0f00000000;
	mov.f32 	%f145, %f144;
	mov.u32 	%r111, %r3;

$L__BB25_5:
	.pragma "nounroll";
	ld.global.nc.v2.f32 	{%f19, %f20}, [%rd58];
	ld.global.nc.v2.f32 	{%f23, %f24}, [%rd59];
	fma.rn.f32 	%f27, %f19, %f23, %f145;
	fma.rn.f32 	%f145, %f20, %f24, %f27;
	ld.global.nc.v2.f32 	{%f28, %f29}, [%rd60];
	fma.rn.f32 	%f32, %f19, %f28, %f144;
	fma.rn.f32 	%f144, %f20, %f29, %f32;
	add.s32 	%r111, %r111, 128;
	add.s64 	%rd60, %rd60, 1024;
	add.s64 	%rd59, %rd59, 1024;
	add.s64 	%rd58, %rd58, 1024;
	add.s32 	%r110, %r110, -1;
	setp.ne.s32 	%p4, %r110, 0;
	@%p4 bra 	$L__BB25_5;

	st.local.v2.f32 	[%rd3], {%f145, %f144};

$L__BB25_7:
	setp.lt.u32 	%p5, %r5, 384;
	@%p5 bra 	$L__BB25_11;

	mul.wide.s32 	%rd39, %r111, 2;
	add.s64 	%rd40, %rd39, %rd4;
	shl.b64 	%rd41, %rd40, 2;
	add.s64 	%rd42, %rd2, %rd41;
	add.s64 	%rd63, %rd42, 2048;
	add.s64 	%rd43, %rd39, %rd5;
	shl.b64 	%rd44, %rd43, 2;
	add.s64 	%rd45, %rd1, %rd44;
	add.s64 	%rd62, %rd45, 3072;
	mul.wide.s32 	%rd46, %r16, 2;
	add.s64 	%rd47, %rd43, %rd46;
	shl.b64 	%rd48, %rd47, 2;
	add.s64 	%rd49, %rd1, %rd48;
	add.s64 	%rd61, %rd49, 2048;

$L__BB25_9:
	ld.global.nc.v2.f32 	{%f33, %f34}, [%rd63+-2048];
	ld.global.nc.v2.f32 	{%f37, %f38}, [%rd62+-3072];
	fma.rn.f32 	%f41, %f33, %f37, %f145;
	fma.rn.f32 	%f42, %f34, %f38, %f41;
	ld.global.nc.v2.f32 	{%f43, %f44}, [%rd61+-2048];
	fma.rn.f32 	%f47, %f33, %f43, %f144;
	fma.rn.f32 	%f48, %f34, %f44, %f47;
	ld.global.nc.v2.f32 	{%f49, %f50}, [%rd63+-1024];
	ld.global.nc.v2.f32 	{%f53, %f54}, [%rd62+-2048];
	fma.rn.f32 	%f57, %f49, %f53, %f42;
	fma.rn.f32 	%f58, %f50, %f54, %f57;
	ld.global.nc.v2.f32 	{%f59, %f60}, [%rd61+-1024];
	fma.rn.f32 	%f63, %f49, %f59, %f48;
	fma.rn.f32 	%f64, %f50, %f60, %f63;
	ld.global.nc.v2.f32 	{%f65, %f66}, [%rd63];
	ld.global.nc.v2.f32 	{%f69, %f70}, [%rd62+-1024];
	fma.rn.f32 	%f73, %f65, %f69, %f58;
	fma.rn.f32 	%f74, %f66, %f70, %f73;
	ld.global.nc.v2.f32 	{%f75, %f76}, [%rd61];
	fma.rn.f32 	%f79, %f65, %f75, %f64;
	fma.rn.f32 	%f80, %f66, %f76, %f79;
	ld.global.nc.v2.f32 	{%f81, %f82}, [%rd63+1024];
	ld.global.nc.v2.f32 	{%f85, %f86}, [%rd62];
	fma.rn.f32 	%f89, %f81, %f85, %f74;
	fma.rn.f32 	%f145, %f82, %f86, %f89;
	ld.global.nc.v2.f32 	{%f90, %f91}, [%rd61+1024];
	fma.rn.f32 	%f94, %f81, %f90, %f80;
	fma.rn.f32 	%f144, %f82, %f91, %f94;
	add.s64 	%rd63, %rd63, 4096;
	add.s64 	%rd62, %rd62, 4096;
	add.s64 	%rd61, %rd61, 4096;
	add.s32 	%r111, %r111, 512;
	setp.lt.s32 	%p6, %r111, %r15;
	@%p6 bra 	$L__BB25_9;

	st.local.v2.f32 	[%rd3], {%f145, %f144};

$L__BB25_11:
	shr.s32 	%r33, %r3, 31;
	shr.u32 	%r34, %r33, 27;
	add.s32 	%r35, %r3, %r34;
	shr.s32 	%r36, %r35, 5;
	shl.b32 	%r37, %r36, 2;
	add.s32 	%r14, %r28, %r37;
	mov.u32 	%r39, 2;
	mov.b32 	%r40, %f145;
	mov.u32 	%r41, 31;
	mov.u32 	%r42, 16;
	mov.u32 	%r43, -1;
	shfl.sync.bfly.b32 	%r44|%p7, %r40, %r42, %r41, %r43;
	mov.b32 	%f95, %r44;
	add.f32 	%f96, %f145, %f95;
	mov.b32 	%r45, %f96;
	mov.u32 	%r46, 8;
	shfl.sync.bfly.b32 	%r47|%p8, %r45, %r46, %r41, %r43;
	mov.b32 	%f97, %r47;
	add.f32 	%f98, %f96, %f97;
	mov.b32 	%r48, %f98;
	mov.u32 	%r49, 4;
	shfl.sync.bfly.b32 	%r50|%p9, %r48, %r49, %r41, %r43;
	mov.b32 	%f99, %r50;
	add.f32 	%f100, %f98, %f99;
	mov.b32 	%r51, %f100;
	shfl.sync.bfly.b32 	%r52|%p10, %r51, %r39, %r41, %r43;
	mov.b32 	%f101, %r52;
	add.f32 	%f102, %f100, %f101;
	mov.b32 	%r53, %f102;
	mov.u32 	%r54, 1;
	shfl.sync.bfly.b32 	%r55|%p11, %r53, %r54, %r41, %r43;
	mov.b32 	%f103, %r55;
	add.f32 	%f104, %f102, %f103;
	st.local.f32 	[%rd3], %f104;
	st.shared.f32 	[%r14], %f104;
	bar.sync 	0;
	@%p1 bra 	$L__BB25_13;

	ld.shared.f32 	%f105, [%r4];
	mov.b32 	%r56, %f105;
	shfl.sync.bfly.b32 	%r60|%p13, %r56, %r42, %r41, %r43;
	mov.b32 	%f106, %r60;
	add.f32 	%f107, %f105, %f106;
	mov.b32 	%r61, %f107;
	shfl.sync.bfly.b32 	%r63|%p14, %r61, %r46, %r41, %r43;
	mov.b32 	%f108, %r63;
	add.f32 	%f109, %f107, %f108;
	mov.b32 	%r64, %f109;
	shfl.sync.bfly.b32 	%r66|%p15, %r64, %r49, %r41, %r43;
	mov.b32 	%f110, %r66;
	add.f32 	%f111, %f109, %f110;
	mov.b32 	%r67, %f111;
	shfl.sync.bfly.b32 	%r69|%p16, %r67, %r39, %r41, %r43;
	mov.b32 	%f112, %r69;
	add.f32 	%f113, %f111, %f112;
	mov.b32 	%r70, %f113;
	shfl.sync.bfly.b32 	%r72|%p17, %r70, %r54, %r41, %r43;
	mov.b32 	%f114, %r72;
	add.f32 	%f115, %f113, %f114;
	st.local.f32 	[%rd3], %f115;

$L__BB25_13:
	bar.sync 	0;
	mov.b32 	%r73, %f144;
	shfl.sync.bfly.b32 	%r77|%p19, %r73, %r42, %r41, %r43;
	mov.b32 	%f116, %r77;
	add.f32 	%f117, %f144, %f116;
	mov.b32 	%r78, %f117;
	shfl.sync.bfly.b32 	%r80|%p20, %r78, %r46, %r41, %r43;
	mov.b32 	%f118, %r80;
	add.f32 	%f119, %f117, %f118;
	mov.b32 	%r81, %f119;
	shfl.sync.bfly.b32 	%r83|%p21, %r81, %r49, %r41, %r43;
	mov.b32 	%f120, %r83;
	add.f32 	%f121, %f119, %f120;
	mov.b32 	%r84, %f121;
	shfl.sync.bfly.b32 	%r86|%p22, %r84, %r39, %r41, %r43;
	mov.b32 	%f122, %r86;
	add.f32 	%f123, %f121, %f122;
	mov.b32 	%r87, %f123;
	shfl.sync.bfly.b32 	%r89|%p23, %r87, %r54, %r41, %r43;
	mov.b32 	%f124, %r89;
	add.f32 	%f125, %f123, %f124;
	st.local.f32 	[%rd3+4], %f125;
	st.shared.f32 	[%r14], %f125;
	bar.sync 	0;
	@%p1 bra 	$L__BB25_15;

	ld.shared.f32 	%f126, [%r4];
	mov.b32 	%r90, %f126;
	mov.u32 	%r91, 31;
	mov.u32 	%r92, 16;
	mov.u32 	%r93, -1;
	shfl.sync.bfly.b32 	%r94|%p24, %r90, %r92, %r91, %r93;
	mov.b32 	%f127, %r94;
	add.f32 	%f128, %f126, %f127;
	mov.b32 	%r95, %f128;
	mov.u32 	%r96, 8;
	shfl.sync.bfly.b32 	%r97|%p25, %r95, %r96, %r91, %r93;
	mov.b32 	%f129, %r97;
	add.f32 	%f130, %f128, %f129;
	mov.b32 	%r98, %f130;
	mov.u32 	%r99, 4;
	shfl.sync.bfly.b32 	%r100|%p26, %r98, %r99, %r91, %r93;
	mov.b32 	%f131, %r100;
	add.f32 	%f132, %f130, %f131;
	mov.b32 	%r101, %f132;
	mov.u32 	%r102, 2;
	shfl.sync.bfly.b32 	%r103|%p27, %r101, %r102, %r91, %r93;
	mov.b32 	%f133, %r103;
	add.f32 	%f134, %f132, %f133;
	mov.b32 	%r104, %f134;
	mov.u32 	%r105, 1;
	shfl.sync.bfly.b32 	%r106|%p28, %r104, %r105, %r91, %r93;
	mov.b32 	%f135, %r106;
	add.f32 	%f136, %f134, %f135;
	st.local.f32 	[%rd3+4], %f136;

$L__BB25_15:
	bar.sync 	0;
	setp.gt.s32 	%p29, %r3, 1;
	@%p29 bra 	$L__BB25_17;

	mul.wide.s32 	%rd50, %r3, 4;
	add.s64 	%rd51, %rd3, %rd50;
	ld.local.f32 	%f137, [%rd51];
	mad.lo.s32 	%r107, %r3, %r17, %r2;
	cvt.s64.s32 	%rd52, %r107;
	mul.lo.s32 	%r108, %r1, %r18;
	cvt.s64.s32 	%rd53, %r108;
	add.s64 	%rd54, %rd53, %rd52;
	cvta.to.global.u64 	%rd55, %rd26;
	shl.b64 	%rd56, %rd54, 2;
	add.s64 	%rd57, %rd55, %rd56;
	st.global.f32 	[%rd57], %f137;

$L__BB25_17:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_3_bs_128
.visible .entry ggml_matvec_f32_ncols_3_bs_128(
	.param .u64 ggml_matvec_f32_ncols_3_bs_128_param_0,
	.param .u64 ggml_matvec_f32_ncols_3_bs_128_param_1,
	.param .u64 ggml_matvec_f32_ncols_3_bs_128_param_2,
	.param .u32 ggml_matvec_f32_ncols_3_bs_128_param_3,
	.param .u32 ggml_matvec_f32_ncols_3_bs_128_param_4,
	.param .u32 ggml_matvec_f32_ncols_3_bs_128_param_5,
	.param .u32 ggml_matvec_f32_ncols_3_bs_128_param_6,
	.param .u32 ggml_matvec_f32_ncols_3_bs_128_param_7,
	.param .u32 ggml_matvec_f32_ncols_3_bs_128_param_8,
	.param .u32 ggml_matvec_f32_ncols_3_bs_128_param_9,
	.param .u32 ggml_matvec_f32_ncols_3_bs_128_param_10,
	.param .u32 ggml_matvec_f32_ncols_3_bs_128_param_11
)
{
	.local .align 4 .b8 	__local_depot26[12];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<41>;
	.reg .f32 	%f<208>;
	.reg .b32 	%r<154>;
	.reg .b64 	%rd<72>;


	mov.u64 	%SPL, __local_depot26;
	ld.param.u64 	%rd29, [ggml_matvec_f32_ncols_3_bs_128_param_0];
	ld.param.u64 	%rd30, [ggml_matvec_f32_ncols_3_bs_128_param_1];
	ld.param.u64 	%rd28, [ggml_matvec_f32_ncols_3_bs_128_param_2];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_3_bs_128_param_3];
	ld.param.u32 	%r19, [ggml_matvec_f32_ncols_3_bs_128_param_5];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_3_bs_128_param_6];
	ld.param.u32 	%r17, [ggml_matvec_f32_ncols_3_bs_128_param_7];
	ld.param.u32 	%r20, [ggml_matvec_f32_ncols_3_bs_128_param_8];
	ld.param.u32 	%r21, [ggml_matvec_f32_ncols_3_bs_128_param_9];
	ld.param.u32 	%r22, [ggml_matvec_f32_ncols_3_bs_128_param_10];
	ld.param.u32 	%r18, [ggml_matvec_f32_ncols_3_bs_128_param_11];
	cvta.to.global.u64 	%rd71, %rd30;
	cvta.to.global.u64 	%rd2, %rd29;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r23, %r1, %r20;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r24, %r2, %r19;
	mad.lo.s32 	%r25, %r23, %r21, %r24;
	cvt.s64.s32 	%rd4, %r25;
	mul.lo.s32 	%r26, %r1, %r22;
	cvt.s64.s32 	%rd5, %r26;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r27, %r3, 2;
	mov.u32 	%r28, data_mmv;
	add.s32 	%r4, %r28, %r27;
	@%p1 bra 	$L__BB26_2;

	mov.u32 	%r29, 0;
	st.shared.u32 	[%r4], %r29;

$L__BB26_2:
	bar.sync 	0;
	mov.f32 	%f205, 0f00000000;
	mov.u32 	%r30, 0;
	st.local.u32 	[%rd3], %r30;
	st.local.u32 	[%rd3+4], %r30;
	st.local.u32 	[%rd3+8], %r30;
	setp.ge.s32 	%p2, %r3, %r15;
	mov.f32 	%f206, %f205;
	mov.f32 	%f207, %f205;
	@%p2 bra 	$L__BB26_11;

	not.b32 	%r31, %r3;
	add.s32 	%r5, %r31, %r15;
	shr.u32 	%r32, %r5, 7;
	add.s32 	%r33, %r32, 1;
	and.b32  	%r151, %r33, 3;
	setp.eq.s32 	%p3, %r151, 0;
	mov.f32 	%f205, 0f00000000;
	mov.u32 	%r152, %r3;
	@%p3 bra 	$L__BB26_7;

	shl.b32 	%r34, %r16, 1;
	add.s32 	%r35, %r3, %r34;
	mul.wide.s32 	%rd32, %r35, 2;
	add.s64 	%rd33, %rd32, %rd5;
	shl.b64 	%rd34, %rd33, 2;
	add.s64 	%rd69, %rd71, %rd34;
	mul.wide.s32 	%rd35, %r16, 2;
	mul.wide.s32 	%rd36, %r3, 2;
	add.s64 	%rd37, %rd35, %rd36;
	add.s64 	%rd38, %rd37, %rd5;
	shl.b64 	%rd39, %rd38, 2;
	add.s64 	%rd68, %rd71, %rd39;
	add.s64 	%rd40, %rd36, %rd5;
	shl.b64 	%rd41, %rd40, 2;
	add.s64 	%rd67, %rd71, %rd41;
	add.s64 	%rd42, %rd36, %rd4;
	shl.b64 	%rd43, %rd42, 2;
	add.s64 	%rd66, %rd2, %rd43;
	mov.f32 	%f205, 0f00000000;
	mov.f32 	%f206, %f205;
	mov.f32 	%f207, %f205;
	mov.u32 	%r152, %r3;

$L__BB26_5:
	.pragma "nounroll";
	ld.global.nc.v2.f32 	{%f28, %f29}, [%rd66];
	ld.global.nc.v2.f32 	{%f32, %f33}, [%rd67];
	fma.rn.f32 	%f36, %f28, %f32, %f207;
	fma.rn.f32 	%f207, %f29, %f33, %f36;
	ld.global.nc.v2.f32 	{%f37, %f38}, [%rd68];
	fma.rn.f32 	%f41, %f28, %f37, %f206;
	fma.rn.f32 	%f206, %f29, %f38, %f41;
	ld.global.nc.v2.f32 	{%f42, %f43}, [%rd69];
	fma.rn.f32 	%f46, %f28, %f42, %f205;
	fma.rn.f32 	%f205, %f29, %f43, %f46;
	add.s32 	%r152, %r152, 128;
	add.s64 	%rd69, %rd69, 1024;
	add.s64 	%rd68, %rd68, 1024;
	add.s64 	%rd67, %rd67, 1024;
	add.s64 	%rd66, %rd66, 1024;
	add.s32 	%r151, %r151, -1;
	setp.ne.s32 	%p4, %r151, 0;
	@%p4 bra 	$L__BB26_5;

	st.local.f32 	[%rd3], %f207;
	st.local.f32 	[%rd3+4], %f206;
	st.local.f32 	[%rd3+8], %f205;

$L__BB26_7:
	setp.lt.u32 	%p5, %r5, 384;
	@%p5 bra 	$L__BB26_11;

	add.s32 	%r36, %r152, %r16;
	shl.b32 	%r37, %r16, 1;
	add.s32 	%r38, %r152, %r37;
	add.s32 	%r39, %r36, 128;
	mul.wide.s32 	%rd44, %r39, 8;
	shl.b64 	%rd45, %rd5, 2;
	add.s64 	%rd19, %rd44, %rd45;
	mul.wide.s32 	%rd46, %r38, 8;
	add.s64 	%rd20, %rd46, %rd45;
	mul.wide.s32 	%rd47, %r152, 2;
	add.s64 	%rd48, %rd47, %rd4;
	shl.b64 	%rd49, %rd48, 2;
	add.s64 	%rd50, %rd2, %rd49;
	add.s64 	%rd70, %rd50, 2048;
	mul.wide.s32 	%rd51, %r152, 8;
	add.s64 	%rd22, %rd51, %rd45;
	mul.wide.s32 	%rd52, %r16, 8;
	add.s64 	%rd53, %rd51, %rd52;
	add.s64 	%rd23, %rd53, %rd45;

$L__BB26_9:
	ld.global.nc.v2.f32 	{%f47, %f48}, [%rd70+-2048];
	add.s64 	%rd54, %rd71, %rd22;
	ld.global.nc.v2.f32 	{%f51, %f52}, [%rd54];
	fma.rn.f32 	%f55, %f47, %f51, %f207;
	fma.rn.f32 	%f56, %f48, %f52, %f55;
	add.s64 	%rd55, %rd71, %rd23;
	ld.global.nc.v2.f32 	{%f57, %f58}, [%rd55];
	fma.rn.f32 	%f61, %f47, %f57, %f206;
	fma.rn.f32 	%f62, %f48, %f58, %f61;
	add.s64 	%rd56, %rd71, %rd20;
	ld.global.nc.v2.f32 	{%f63, %f64}, [%rd56];
	fma.rn.f32 	%f67, %f47, %f63, %f205;
	fma.rn.f32 	%f68, %f48, %f64, %f67;
	ld.global.nc.v2.f32 	{%f69, %f70}, [%rd70+-1024];
	ld.global.nc.v2.f32 	{%f73, %f74}, [%rd54+1024];
	fma.rn.f32 	%f77, %f69, %f73, %f56;
	fma.rn.f32 	%f78, %f70, %f74, %f77;
	add.s64 	%rd57, %rd71, %rd19;
	ld.global.nc.v2.f32 	{%f79, %f80}, [%rd57];
	fma.rn.f32 	%f83, %f69, %f79, %f62;
	fma.rn.f32 	%f84, %f70, %f80, %f83;
	ld.global.nc.v2.f32 	{%f85, %f86}, [%rd56+1024];
	fma.rn.f32 	%f89, %f69, %f85, %f68;
	fma.rn.f32 	%f90, %f70, %f86, %f89;
	ld.global.nc.v2.f32 	{%f91, %f92}, [%rd70];
	ld.global.nc.v2.f32 	{%f95, %f96}, [%rd54+2048];
	fma.rn.f32 	%f99, %f91, %f95, %f78;
	fma.rn.f32 	%f100, %f92, %f96, %f99;
	ld.global.nc.v2.f32 	{%f101, %f102}, [%rd57+1024];
	fma.rn.f32 	%f105, %f91, %f101, %f84;
	fma.rn.f32 	%f106, %f92, %f102, %f105;
	ld.global.nc.v2.f32 	{%f107, %f108}, [%rd56+2048];
	fma.rn.f32 	%f111, %f91, %f107, %f90;
	fma.rn.f32 	%f112, %f92, %f108, %f111;
	ld.global.nc.v2.f32 	{%f113, %f114}, [%rd70+1024];
	ld.global.nc.v2.f32 	{%f117, %f118}, [%rd54+3072];
	fma.rn.f32 	%f121, %f113, %f117, %f100;
	fma.rn.f32 	%f207, %f114, %f118, %f121;
	ld.global.nc.v2.f32 	{%f122, %f123}, [%rd57+2048];
	fma.rn.f32 	%f126, %f113, %f122, %f106;
	fma.rn.f32 	%f206, %f114, %f123, %f126;
	ld.global.nc.v2.f32 	{%f127, %f128}, [%rd56+3072];
	fma.rn.f32 	%f131, %f113, %f127, %f112;
	fma.rn.f32 	%f205, %f114, %f128, %f131;
	add.s64 	%rd71, %rd71, 4096;
	add.s64 	%rd70, %rd70, 4096;
	add.s32 	%r152, %r152, 512;
	setp.lt.s32 	%p6, %r152, %r15;
	@%p6 bra 	$L__BB26_9;

	st.local.f32 	[%rd3], %f207;
	st.local.f32 	[%rd3+4], %f206;
	st.local.f32 	[%rd3+8], %f205;

$L__BB26_11:
	shr.s32 	%r40, %r3, 31;
	shr.u32 	%r41, %r40, 27;
	add.s32 	%r42, %r3, %r41;
	shr.s32 	%r43, %r42, 5;
	shl.b32 	%r44, %r43, 2;
	add.s32 	%r14, %r28, %r44;
	mov.u32 	%r46, 2;
	mov.b32 	%r47, %f207;
	mov.u32 	%r48, 31;
	mov.u32 	%r49, 16;
	mov.u32 	%r50, -1;
	shfl.sync.bfly.b32 	%r51|%p7, %r47, %r49, %r48, %r50;
	mov.b32 	%f132, %r51;
	add.f32 	%f133, %f207, %f132;
	mov.b32 	%r52, %f133;
	mov.u32 	%r53, 8;
	shfl.sync.bfly.b32 	%r54|%p8, %r52, %r53, %r48, %r50;
	mov.b32 	%f134, %r54;
	add.f32 	%f135, %f133, %f134;
	mov.b32 	%r55, %f135;
	mov.u32 	%r56, 4;
	shfl.sync.bfly.b32 	%r57|%p9, %r55, %r56, %r48, %r50;
	mov.b32 	%f136, %r57;
	add.f32 	%f137, %f135, %f136;
	mov.b32 	%r58, %f137;
	shfl.sync.bfly.b32 	%r59|%p10, %r58, %r46, %r48, %r50;
	mov.b32 	%f138, %r59;
	add.f32 	%f139, %f137, %f138;
	mov.b32 	%r60, %f139;
	mov.u32 	%r61, 1;
	shfl.sync.bfly.b32 	%r62|%p11, %r60, %r61, %r48, %r50;
	mov.b32 	%f140, %r62;
	add.f32 	%f141, %f139, %f140;
	st.local.f32 	[%rd3], %f141;
	st.shared.f32 	[%r14], %f141;
	bar.sync 	0;
	@%p1 bra 	$L__BB26_13;

	ld.shared.f32 	%f142, [%r4];
	mov.b32 	%r63, %f142;
	shfl.sync.bfly.b32 	%r67|%p13, %r63, %r49, %r48, %r50;
	mov.b32 	%f143, %r67;
	add.f32 	%f144, %f142, %f143;
	mov.b32 	%r68, %f144;
	shfl.sync.bfly.b32 	%r70|%p14, %r68, %r53, %r48, %r50;
	mov.b32 	%f145, %r70;
	add.f32 	%f146, %f144, %f145;
	mov.b32 	%r71, %f146;
	shfl.sync.bfly.b32 	%r73|%p15, %r71, %r56, %r48, %r50;
	mov.b32 	%f147, %r73;
	add.f32 	%f148, %f146, %f147;
	mov.b32 	%r74, %f148;
	shfl.sync.bfly.b32 	%r76|%p16, %r74, %r46, %r48, %r50;
	mov.b32 	%f149, %r76;
	add.f32 	%f150, %f148, %f149;
	mov.b32 	%r77, %f150;
	shfl.sync.bfly.b32 	%r79|%p17, %r77, %r61, %r48, %r50;
	mov.b32 	%f151, %r79;
	add.f32 	%f152, %f150, %f151;
	st.local.f32 	[%rd3], %f152;

$L__BB26_13:
	bar.sync 	0;
	mov.b32 	%r80, %f206;
	shfl.sync.bfly.b32 	%r84|%p19, %r80, %r49, %r48, %r50;
	mov.b32 	%f153, %r84;
	add.f32 	%f154, %f206, %f153;
	mov.b32 	%r85, %f154;
	shfl.sync.bfly.b32 	%r87|%p20, %r85, %r53, %r48, %r50;
	mov.b32 	%f155, %r87;
	add.f32 	%f156, %f154, %f155;
	mov.b32 	%r88, %f156;
	shfl.sync.bfly.b32 	%r90|%p21, %r88, %r56, %r48, %r50;
	mov.b32 	%f157, %r90;
	add.f32 	%f158, %f156, %f157;
	mov.b32 	%r91, %f158;
	shfl.sync.bfly.b32 	%r93|%p22, %r91, %r46, %r48, %r50;
	mov.b32 	%f159, %r93;
	add.f32 	%f160, %f158, %f159;
	mov.b32 	%r94, %f160;
	shfl.sync.bfly.b32 	%r96|%p23, %r94, %r61, %r48, %r50;
	mov.b32 	%f161, %r96;
	add.f32 	%f162, %f160, %f161;
	st.local.f32 	[%rd3+4], %f162;
	st.shared.f32 	[%r14], %f162;
	bar.sync 	0;
	@%p1 bra 	$L__BB26_15;

	ld.shared.f32 	%f163, [%r4];
	mov.b32 	%r97, %f163;
	mov.u32 	%r98, 31;
	mov.u32 	%r99, 16;
	mov.u32 	%r100, -1;
	shfl.sync.bfly.b32 	%r101|%p24, %r97, %r99, %r98, %r100;
	mov.b32 	%f164, %r101;
	add.f32 	%f165, %f163, %f164;
	mov.b32 	%r102, %f165;
	mov.u32 	%r103, 8;
	shfl.sync.bfly.b32 	%r104|%p25, %r102, %r103, %r98, %r100;
	mov.b32 	%f166, %r104;
	add.f32 	%f167, %f165, %f166;
	mov.b32 	%r105, %f167;
	mov.u32 	%r106, 4;
	shfl.sync.bfly.b32 	%r107|%p26, %r105, %r106, %r98, %r100;
	mov.b32 	%f168, %r107;
	add.f32 	%f169, %f167, %f168;
	mov.b32 	%r108, %f169;
	mov.u32 	%r109, 2;
	shfl.sync.bfly.b32 	%r110|%p27, %r108, %r109, %r98, %r100;
	mov.b32 	%f170, %r110;
	add.f32 	%f171, %f169, %f170;
	mov.b32 	%r111, %f171;
	mov.u32 	%r112, 1;
	shfl.sync.bfly.b32 	%r113|%p28, %r111, %r112, %r98, %r100;
	mov.b32 	%f172, %r113;
	add.f32 	%f173, %f171, %f172;
	st.local.f32 	[%rd3+4], %f173;

$L__BB26_15:
	bar.sync 	0;
	mov.b32 	%r114, %f205;
	mov.u32 	%r115, 31;
	mov.u32 	%r116, 16;
	mov.u32 	%r117, -1;
	shfl.sync.bfly.b32 	%r118|%p30, %r114, %r116, %r115, %r117;
	mov.b32 	%f174, %r118;
	add.f32 	%f175, %f205, %f174;
	mov.b32 	%r119, %f175;
	mov.u32 	%r120, 8;
	shfl.sync.bfly.b32 	%r121|%p31, %r119, %r120, %r115, %r117;
	mov.b32 	%f176, %r121;
	add.f32 	%f177, %f175, %f176;
	mov.b32 	%r122, %f177;
	mov.u32 	%r123, 4;
	shfl.sync.bfly.b32 	%r124|%p32, %r122, %r123, %r115, %r117;
	mov.b32 	%f178, %r124;
	add.f32 	%f179, %f177, %f178;
	mov.b32 	%r125, %f179;
	mov.u32 	%r126, 2;
	shfl.sync.bfly.b32 	%r127|%p33, %r125, %r126, %r115, %r117;
	mov.b32 	%f180, %r127;
	add.f32 	%f181, %f179, %f180;
	mov.b32 	%r128, %f181;
	mov.u32 	%r129, 1;
	shfl.sync.bfly.b32 	%r130|%p34, %r128, %r129, %r115, %r117;
	mov.b32 	%f182, %r130;
	add.f32 	%f183, %f181, %f182;
	st.local.f32 	[%rd3+8], %f183;
	st.shared.f32 	[%r14], %f183;
	bar.sync 	0;
	@%p1 bra 	$L__BB26_17;

	ld.shared.f32 	%f184, [%r4];
	mov.b32 	%r131, %f184;
	shfl.sync.bfly.b32 	%r135|%p35, %r131, %r116, %r115, %r117;
	mov.b32 	%f185, %r135;
	add.f32 	%f186, %f184, %f185;
	mov.b32 	%r136, %f186;
	shfl.sync.bfly.b32 	%r138|%p36, %r136, %r120, %r115, %r117;
	mov.b32 	%f187, %r138;
	add.f32 	%f188, %f186, %f187;
	mov.b32 	%r139, %f188;
	shfl.sync.bfly.b32 	%r141|%p37, %r139, %r123, %r115, %r117;
	mov.b32 	%f189, %r141;
	add.f32 	%f190, %f188, %f189;
	mov.b32 	%r142, %f190;
	shfl.sync.bfly.b32 	%r144|%p38, %r142, %r126, %r115, %r117;
	mov.b32 	%f191, %r144;
	add.f32 	%f192, %f190, %f191;
	mov.b32 	%r145, %f192;
	shfl.sync.bfly.b32 	%r147|%p39, %r145, %r129, %r115, %r117;
	mov.b32 	%f193, %r147;
	add.f32 	%f194, %f192, %f193;
	st.local.f32 	[%rd3+8], %f194;

$L__BB26_17:
	bar.sync 	0;
	setp.gt.s32 	%p40, %r3, 2;
	@%p40 bra 	$L__BB26_19;

	mul.wide.s32 	%rd58, %r3, 4;
	add.s64 	%rd59, %rd3, %rd58;
	ld.local.f32 	%f195, [%rd59];
	mad.lo.s32 	%r148, %r3, %r17, %r2;
	cvt.s64.s32 	%rd60, %r148;
	mul.lo.s32 	%r149, %r1, %r18;
	cvt.s64.s32 	%rd61, %r149;
	add.s64 	%rd62, %rd61, %rd60;
	cvta.to.global.u64 	%rd63, %rd28;
	shl.b64 	%rd64, %rd62, 2;
	add.s64 	%rd65, %rd63, %rd64;
	st.global.f32 	[%rd65], %f195;

$L__BB26_19:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_4_bs_128
.visible .entry ggml_matvec_f32_ncols_4_bs_128(
	.param .u64 ggml_matvec_f32_ncols_4_bs_128_param_0,
	.param .u64 ggml_matvec_f32_ncols_4_bs_128_param_1,
	.param .u64 ggml_matvec_f32_ncols_4_bs_128_param_2,
	.param .u32 ggml_matvec_f32_ncols_4_bs_128_param_3,
	.param .u32 ggml_matvec_f32_ncols_4_bs_128_param_4,
	.param .u32 ggml_matvec_f32_ncols_4_bs_128_param_5,
	.param .u32 ggml_matvec_f32_ncols_4_bs_128_param_6,
	.param .u32 ggml_matvec_f32_ncols_4_bs_128_param_7,
	.param .u32 ggml_matvec_f32_ncols_4_bs_128_param_8,
	.param .u32 ggml_matvec_f32_ncols_4_bs_128_param_9,
	.param .u32 ggml_matvec_f32_ncols_4_bs_128_param_10,
	.param .u32 ggml_matvec_f32_ncols_4_bs_128_param_11
)
{
	.local .align 16 .b8 	__local_depot27[16];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<52>;
	.reg .f32 	%f<270>;
	.reg .b32 	%r<189>;
	.reg .b64 	%rd<83>;


	mov.u64 	%SPL, __local_depot27;
	ld.param.u64 	%rd34, [ggml_matvec_f32_ncols_4_bs_128_param_0];
	ld.param.u64 	%rd35, [ggml_matvec_f32_ncols_4_bs_128_param_1];
	ld.param.u64 	%rd33, [ggml_matvec_f32_ncols_4_bs_128_param_2];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_4_bs_128_param_3];
	ld.param.u32 	%r19, [ggml_matvec_f32_ncols_4_bs_128_param_5];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_4_bs_128_param_6];
	ld.param.u32 	%r17, [ggml_matvec_f32_ncols_4_bs_128_param_7];
	ld.param.u32 	%r20, [ggml_matvec_f32_ncols_4_bs_128_param_8];
	ld.param.u32 	%r21, [ggml_matvec_f32_ncols_4_bs_128_param_9];
	ld.param.u32 	%r22, [ggml_matvec_f32_ncols_4_bs_128_param_10];
	ld.param.u32 	%r18, [ggml_matvec_f32_ncols_4_bs_128_param_11];
	cvta.to.global.u64 	%rd82, %rd35;
	cvta.to.global.u64 	%rd2, %rd34;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r23, %r1, %r20;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r24, %r2, %r19;
	mad.lo.s32 	%r25, %r23, %r21, %r24;
	cvt.s64.s32 	%rd4, %r25;
	mul.lo.s32 	%r26, %r1, %r22;
	cvt.s64.s32 	%rd5, %r26;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r27, %r3, 2;
	mov.u32 	%r28, data_mmv;
	add.s32 	%r4, %r28, %r27;
	@%p1 bra 	$L__BB27_2;

	mov.u32 	%r29, 0;
	st.shared.u32 	[%r4], %r29;

$L__BB27_2:
	bar.sync 	0;
	mov.f32 	%f266, 0f00000000;
	st.local.v4.f32 	[%rd3], {%f266, %f266, %f266, %f266};
	setp.ge.s32 	%p2, %r3, %r15;
	mov.f32 	%f267, %f266;
	mov.f32 	%f268, %f266;
	mov.f32 	%f269, %f266;
	@%p2 bra 	$L__BB27_11;

	not.b32 	%r30, %r3;
	add.s32 	%r5, %r30, %r15;
	shr.u32 	%r31, %r5, 7;
	add.s32 	%r32, %r31, 1;
	and.b32  	%r186, %r32, 3;
	setp.eq.s32 	%p3, %r186, 0;
	mov.f32 	%f266, 0f00000000;
	mov.u32 	%r187, %r3;
	@%p3 bra 	$L__BB27_7;

	shl.b32 	%r33, %r16, 1;
	add.s32 	%r34, %r3, %r33;
	mul.wide.s32 	%rd37, %r34, 2;
	add.s64 	%rd38, %rd37, %rd5;
	shl.b64 	%rd39, %rd38, 2;
	add.s64 	%rd80, %rd82, %rd39;
	mad.lo.s32 	%r35, %r16, 3, %r3;
	mul.wide.s32 	%rd40, %r35, 2;
	add.s64 	%rd41, %rd40, %rd5;
	shl.b64 	%rd42, %rd41, 2;
	add.s64 	%rd79, %rd82, %rd42;
	mul.wide.s32 	%rd43, %r16, 2;
	mul.wide.s32 	%rd44, %r3, 2;
	add.s64 	%rd45, %rd43, %rd44;
	add.s64 	%rd46, %rd45, %rd5;
	shl.b64 	%rd47, %rd46, 2;
	add.s64 	%rd78, %rd82, %rd47;
	add.s64 	%rd48, %rd44, %rd5;
	shl.b64 	%rd49, %rd48, 2;
	add.s64 	%rd77, %rd82, %rd49;
	add.s64 	%rd50, %rd44, %rd4;
	shl.b64 	%rd51, %rd50, 2;
	add.s64 	%rd76, %rd2, %rd51;
	mov.f32 	%f266, 0f00000000;
	mov.f32 	%f267, %f266;
	mov.f32 	%f268, %f266;
	mov.f32 	%f269, %f266;
	mov.u32 	%r187, %r3;

$L__BB27_5:
	.pragma "nounroll";
	ld.global.nc.v2.f32 	{%f37, %f38}, [%rd76];
	ld.global.nc.v2.f32 	{%f41, %f42}, [%rd77];
	fma.rn.f32 	%f45, %f37, %f41, %f269;
	fma.rn.f32 	%f269, %f38, %f42, %f45;
	ld.global.nc.v2.f32 	{%f46, %f47}, [%rd78];
	fma.rn.f32 	%f50, %f37, %f46, %f268;
	fma.rn.f32 	%f268, %f38, %f47, %f50;
	ld.global.nc.v2.f32 	{%f51, %f52}, [%rd80];
	fma.rn.f32 	%f55, %f37, %f51, %f267;
	fma.rn.f32 	%f267, %f38, %f52, %f55;
	ld.global.nc.v2.f32 	{%f56, %f57}, [%rd79];
	fma.rn.f32 	%f60, %f37, %f56, %f266;
	fma.rn.f32 	%f266, %f38, %f57, %f60;
	add.s32 	%r187, %r187, 128;
	add.s64 	%rd80, %rd80, 1024;
	add.s64 	%rd79, %rd79, 1024;
	add.s64 	%rd78, %rd78, 1024;
	add.s64 	%rd77, %rd77, 1024;
	add.s64 	%rd76, %rd76, 1024;
	add.s32 	%r186, %r186, -1;
	setp.ne.s32 	%p4, %r186, 0;
	@%p4 bra 	$L__BB27_5;

	st.local.v4.f32 	[%rd3], {%f269, %f268, %f267, %f266};

$L__BB27_7:
	setp.lt.u32 	%p5, %r5, 384;
	@%p5 bra 	$L__BB27_11;

	add.s32 	%r36, %r187, %r16;
	shl.b32 	%r37, %r16, 1;
	add.s32 	%r38, %r187, %r37;
	mad.lo.s32 	%r39, %r16, 3, %r187;
	add.s32 	%r40, %r36, 128;
	mul.wide.s32 	%rd52, %r40, 8;
	shl.b64 	%rd53, %rd5, 2;
	add.s64 	%rd23, %rd52, %rd53;
	mul.wide.s32 	%rd54, %r38, 8;
	add.s64 	%rd24, %rd54, %rd53;
	mul.wide.s32 	%rd55, %r39, 8;
	add.s64 	%rd25, %rd55, %rd53;
	mul.wide.s32 	%rd56, %r187, 2;
	add.s64 	%rd57, %rd56, %rd4;
	shl.b64 	%rd58, %rd57, 2;
	add.s64 	%rd59, %rd2, %rd58;
	add.s64 	%rd81, %rd59, 2048;
	mul.wide.s32 	%rd60, %r187, 8;
	add.s64 	%rd27, %rd60, %rd53;
	mul.wide.s32 	%rd61, %r16, 8;
	add.s64 	%rd62, %rd60, %rd61;
	add.s64 	%rd28, %rd62, %rd53;

$L__BB27_9:
	ld.global.nc.v2.f32 	{%f61, %f62}, [%rd81+-2048];
	add.s64 	%rd63, %rd82, %rd27;
	ld.global.nc.v2.f32 	{%f65, %f66}, [%rd63];
	fma.rn.f32 	%f69, %f61, %f65, %f269;
	fma.rn.f32 	%f70, %f62, %f66, %f69;
	add.s64 	%rd64, %rd82, %rd28;
	ld.global.nc.v2.f32 	{%f71, %f72}, [%rd64];
	fma.rn.f32 	%f75, %f61, %f71, %f268;
	fma.rn.f32 	%f76, %f62, %f72, %f75;
	add.s64 	%rd65, %rd82, %rd24;
	ld.global.nc.v2.f32 	{%f77, %f78}, [%rd65];
	fma.rn.f32 	%f81, %f61, %f77, %f267;
	fma.rn.f32 	%f82, %f62, %f78, %f81;
	add.s64 	%rd66, %rd82, %rd25;
	ld.global.nc.v2.f32 	{%f83, %f84}, [%rd66];
	fma.rn.f32 	%f87, %f61, %f83, %f266;
	fma.rn.f32 	%f88, %f62, %f84, %f87;
	ld.global.nc.v2.f32 	{%f89, %f90}, [%rd81+-1024];
	ld.global.nc.v2.f32 	{%f93, %f94}, [%rd63+1024];
	fma.rn.f32 	%f97, %f89, %f93, %f70;
	fma.rn.f32 	%f98, %f90, %f94, %f97;
	add.s64 	%rd67, %rd82, %rd23;
	ld.global.nc.v2.f32 	{%f99, %f100}, [%rd67];
	fma.rn.f32 	%f103, %f89, %f99, %f76;
	fma.rn.f32 	%f104, %f90, %f100, %f103;
	ld.global.nc.v2.f32 	{%f105, %f106}, [%rd65+1024];
	fma.rn.f32 	%f109, %f89, %f105, %f82;
	fma.rn.f32 	%f110, %f90, %f106, %f109;
	ld.global.nc.v2.f32 	{%f111, %f112}, [%rd66+1024];
	fma.rn.f32 	%f115, %f89, %f111, %f88;
	fma.rn.f32 	%f116, %f90, %f112, %f115;
	ld.global.nc.v2.f32 	{%f117, %f118}, [%rd81];
	ld.global.nc.v2.f32 	{%f121, %f122}, [%rd63+2048];
	fma.rn.f32 	%f125, %f117, %f121, %f98;
	fma.rn.f32 	%f126, %f118, %f122, %f125;
	ld.global.nc.v2.f32 	{%f127, %f128}, [%rd67+1024];
	fma.rn.f32 	%f131, %f117, %f127, %f104;
	fma.rn.f32 	%f132, %f118, %f128, %f131;
	ld.global.nc.v2.f32 	{%f133, %f134}, [%rd65+2048];
	fma.rn.f32 	%f137, %f117, %f133, %f110;
	fma.rn.f32 	%f138, %f118, %f134, %f137;
	ld.global.nc.v2.f32 	{%f139, %f140}, [%rd66+2048];
	fma.rn.f32 	%f143, %f117, %f139, %f116;
	fma.rn.f32 	%f144, %f118, %f140, %f143;
	ld.global.nc.v2.f32 	{%f145, %f146}, [%rd81+1024];
	ld.global.nc.v2.f32 	{%f149, %f150}, [%rd63+3072];
	fma.rn.f32 	%f153, %f145, %f149, %f126;
	fma.rn.f32 	%f269, %f146, %f150, %f153;
	ld.global.nc.v2.f32 	{%f154, %f155}, [%rd67+2048];
	fma.rn.f32 	%f158, %f145, %f154, %f132;
	fma.rn.f32 	%f268, %f146, %f155, %f158;
	ld.global.nc.v2.f32 	{%f159, %f160}, [%rd65+3072];
	fma.rn.f32 	%f163, %f145, %f159, %f138;
	fma.rn.f32 	%f267, %f146, %f160, %f163;
	ld.global.nc.v2.f32 	{%f164, %f165}, [%rd66+3072];
	fma.rn.f32 	%f168, %f145, %f164, %f144;
	fma.rn.f32 	%f266, %f146, %f165, %f168;
	add.s64 	%rd82, %rd82, 4096;
	add.s64 	%rd81, %rd81, 4096;
	add.s32 	%r187, %r187, 512;
	setp.lt.s32 	%p6, %r187, %r15;
	@%p6 bra 	$L__BB27_9;

	st.local.v4.f32 	[%rd3], {%f269, %f268, %f267, %f266};

$L__BB27_11:
	shr.s32 	%r41, %r3, 31;
	shr.u32 	%r42, %r41, 27;
	add.s32 	%r43, %r3, %r42;
	shr.s32 	%r44, %r43, 5;
	shl.b32 	%r45, %r44, 2;
	add.s32 	%r14, %r28, %r45;
	mov.u32 	%r47, 2;
	mov.b32 	%r48, %f269;
	mov.u32 	%r49, 31;
	mov.u32 	%r50, 16;
	mov.u32 	%r51, -1;
	shfl.sync.bfly.b32 	%r52|%p7, %r48, %r50, %r49, %r51;
	mov.b32 	%f169, %r52;
	add.f32 	%f170, %f269, %f169;
	mov.b32 	%r53, %f170;
	mov.u32 	%r54, 8;
	shfl.sync.bfly.b32 	%r55|%p8, %r53, %r54, %r49, %r51;
	mov.b32 	%f171, %r55;
	add.f32 	%f172, %f170, %f171;
	mov.b32 	%r56, %f172;
	mov.u32 	%r57, 4;
	shfl.sync.bfly.b32 	%r58|%p9, %r56, %r57, %r49, %r51;
	mov.b32 	%f173, %r58;
	add.f32 	%f174, %f172, %f173;
	mov.b32 	%r59, %f174;
	shfl.sync.bfly.b32 	%r60|%p10, %r59, %r47, %r49, %r51;
	mov.b32 	%f175, %r60;
	add.f32 	%f176, %f174, %f175;
	mov.b32 	%r61, %f176;
	mov.u32 	%r62, 1;
	shfl.sync.bfly.b32 	%r63|%p11, %r61, %r62, %r49, %r51;
	mov.b32 	%f177, %r63;
	add.f32 	%f178, %f176, %f177;
	st.local.f32 	[%rd3], %f178;
	st.shared.f32 	[%r14], %f178;
	bar.sync 	0;
	@%p1 bra 	$L__BB27_13;

	ld.shared.f32 	%f179, [%r4];
	mov.b32 	%r64, %f179;
	shfl.sync.bfly.b32 	%r68|%p13, %r64, %r50, %r49, %r51;
	mov.b32 	%f180, %r68;
	add.f32 	%f181, %f179, %f180;
	mov.b32 	%r69, %f181;
	shfl.sync.bfly.b32 	%r71|%p14, %r69, %r54, %r49, %r51;
	mov.b32 	%f182, %r71;
	add.f32 	%f183, %f181, %f182;
	mov.b32 	%r72, %f183;
	shfl.sync.bfly.b32 	%r74|%p15, %r72, %r57, %r49, %r51;
	mov.b32 	%f184, %r74;
	add.f32 	%f185, %f183, %f184;
	mov.b32 	%r75, %f185;
	shfl.sync.bfly.b32 	%r77|%p16, %r75, %r47, %r49, %r51;
	mov.b32 	%f186, %r77;
	add.f32 	%f187, %f185, %f186;
	mov.b32 	%r78, %f187;
	shfl.sync.bfly.b32 	%r80|%p17, %r78, %r62, %r49, %r51;
	mov.b32 	%f188, %r80;
	add.f32 	%f189, %f187, %f188;
	st.local.f32 	[%rd3], %f189;

$L__BB27_13:
	bar.sync 	0;
	mov.b32 	%r81, %f268;
	shfl.sync.bfly.b32 	%r85|%p19, %r81, %r50, %r49, %r51;
	mov.b32 	%f190, %r85;
	add.f32 	%f191, %f268, %f190;
	mov.b32 	%r86, %f191;
	shfl.sync.bfly.b32 	%r88|%p20, %r86, %r54, %r49, %r51;
	mov.b32 	%f192, %r88;
	add.f32 	%f193, %f191, %f192;
	mov.b32 	%r89, %f193;
	shfl.sync.bfly.b32 	%r91|%p21, %r89, %r57, %r49, %r51;
	mov.b32 	%f194, %r91;
	add.f32 	%f195, %f193, %f194;
	mov.b32 	%r92, %f195;
	shfl.sync.bfly.b32 	%r94|%p22, %r92, %r47, %r49, %r51;
	mov.b32 	%f196, %r94;
	add.f32 	%f197, %f195, %f196;
	mov.b32 	%r95, %f197;
	shfl.sync.bfly.b32 	%r97|%p23, %r95, %r62, %r49, %r51;
	mov.b32 	%f198, %r97;
	add.f32 	%f199, %f197, %f198;
	st.local.f32 	[%rd3+4], %f199;
	st.shared.f32 	[%r14], %f199;
	bar.sync 	0;
	@%p1 bra 	$L__BB27_15;

	ld.shared.f32 	%f200, [%r4];
	mov.b32 	%r98, %f200;
	mov.u32 	%r99, 31;
	mov.u32 	%r100, 16;
	mov.u32 	%r101, -1;
	shfl.sync.bfly.b32 	%r102|%p24, %r98, %r100, %r99, %r101;
	mov.b32 	%f201, %r102;
	add.f32 	%f202, %f200, %f201;
	mov.b32 	%r103, %f202;
	mov.u32 	%r104, 8;
	shfl.sync.bfly.b32 	%r105|%p25, %r103, %r104, %r99, %r101;
	mov.b32 	%f203, %r105;
	add.f32 	%f204, %f202, %f203;
	mov.b32 	%r106, %f204;
	mov.u32 	%r107, 4;
	shfl.sync.bfly.b32 	%r108|%p26, %r106, %r107, %r99, %r101;
	mov.b32 	%f205, %r108;
	add.f32 	%f206, %f204, %f205;
	mov.b32 	%r109, %f206;
	mov.u32 	%r110, 2;
	shfl.sync.bfly.b32 	%r111|%p27, %r109, %r110, %r99, %r101;
	mov.b32 	%f207, %r111;
	add.f32 	%f208, %f206, %f207;
	mov.b32 	%r112, %f208;
	mov.u32 	%r113, 1;
	shfl.sync.bfly.b32 	%r114|%p28, %r112, %r113, %r99, %r101;
	mov.b32 	%f209, %r114;
	add.f32 	%f210, %f208, %f209;
	st.local.f32 	[%rd3+4], %f210;

$L__BB27_15:
	bar.sync 	0;
	mov.b32 	%r115, %f267;
	mov.u32 	%r116, 31;
	mov.u32 	%r117, 16;
	mov.u32 	%r118, -1;
	shfl.sync.bfly.b32 	%r119|%p30, %r115, %r117, %r116, %r118;
	mov.b32 	%f211, %r119;
	add.f32 	%f212, %f267, %f211;
	mov.b32 	%r120, %f212;
	mov.u32 	%r121, 8;
	shfl.sync.bfly.b32 	%r122|%p31, %r120, %r121, %r116, %r118;
	mov.b32 	%f213, %r122;
	add.f32 	%f214, %f212, %f213;
	mov.b32 	%r123, %f214;
	mov.u32 	%r124, 4;
	shfl.sync.bfly.b32 	%r125|%p32, %r123, %r124, %r116, %r118;
	mov.b32 	%f215, %r125;
	add.f32 	%f216, %f214, %f215;
	mov.b32 	%r126, %f216;
	mov.u32 	%r127, 2;
	shfl.sync.bfly.b32 	%r128|%p33, %r126, %r127, %r116, %r118;
	mov.b32 	%f217, %r128;
	add.f32 	%f218, %f216, %f217;
	mov.b32 	%r129, %f218;
	mov.u32 	%r130, 1;
	shfl.sync.bfly.b32 	%r131|%p34, %r129, %r130, %r116, %r118;
	mov.b32 	%f219, %r131;
	add.f32 	%f220, %f218, %f219;
	st.local.f32 	[%rd3+8], %f220;
	st.shared.f32 	[%r14], %f220;
	bar.sync 	0;
	@%p1 bra 	$L__BB27_17;

	ld.shared.f32 	%f221, [%r4];
	mov.b32 	%r132, %f221;
	shfl.sync.bfly.b32 	%r136|%p35, %r132, %r117, %r116, %r118;
	mov.b32 	%f222, %r136;
	add.f32 	%f223, %f221, %f222;
	mov.b32 	%r137, %f223;
	shfl.sync.bfly.b32 	%r139|%p36, %r137, %r121, %r116, %r118;
	mov.b32 	%f224, %r139;
	add.f32 	%f225, %f223, %f224;
	mov.b32 	%r140, %f225;
	shfl.sync.bfly.b32 	%r142|%p37, %r140, %r124, %r116, %r118;
	mov.b32 	%f226, %r142;
	add.f32 	%f227, %f225, %f226;
	mov.b32 	%r143, %f227;
	shfl.sync.bfly.b32 	%r145|%p38, %r143, %r127, %r116, %r118;
	mov.b32 	%f228, %r145;
	add.f32 	%f229, %f227, %f228;
	mov.b32 	%r146, %f229;
	shfl.sync.bfly.b32 	%r148|%p39, %r146, %r130, %r116, %r118;
	mov.b32 	%f230, %r148;
	add.f32 	%f231, %f229, %f230;
	st.local.f32 	[%rd3+8], %f231;

$L__BB27_17:
	bar.sync 	0;
	mov.b32 	%r149, %f266;
	shfl.sync.bfly.b32 	%r153|%p41, %r149, %r117, %r116, %r118;
	mov.b32 	%f232, %r153;
	add.f32 	%f233, %f266, %f232;
	mov.b32 	%r154, %f233;
	shfl.sync.bfly.b32 	%r156|%p42, %r154, %r121, %r116, %r118;
	mov.b32 	%f234, %r156;
	add.f32 	%f235, %f233, %f234;
	mov.b32 	%r157, %f235;
	shfl.sync.bfly.b32 	%r159|%p43, %r157, %r124, %r116, %r118;
	mov.b32 	%f236, %r159;
	add.f32 	%f237, %f235, %f236;
	mov.b32 	%r160, %f237;
	shfl.sync.bfly.b32 	%r162|%p44, %r160, %r127, %r116, %r118;
	mov.b32 	%f238, %r162;
	add.f32 	%f239, %f237, %f238;
	mov.b32 	%r163, %f239;
	shfl.sync.bfly.b32 	%r165|%p45, %r163, %r130, %r116, %r118;
	mov.b32 	%f240, %r165;
	add.f32 	%f241, %f239, %f240;
	st.local.f32 	[%rd3+12], %f241;
	st.shared.f32 	[%r14], %f241;
	bar.sync 	0;
	@%p1 bra 	$L__BB27_19;

	ld.shared.f32 	%f242, [%r4];
	mov.b32 	%r166, %f242;
	mov.u32 	%r167, 31;
	mov.u32 	%r168, 16;
	mov.u32 	%r169, -1;
	shfl.sync.bfly.b32 	%r170|%p46, %r166, %r168, %r167, %r169;
	mov.b32 	%f243, %r170;
	add.f32 	%f244, %f242, %f243;
	mov.b32 	%r171, %f244;
	mov.u32 	%r172, 8;
	shfl.sync.bfly.b32 	%r173|%p47, %r171, %r172, %r167, %r169;
	mov.b32 	%f245, %r173;
	add.f32 	%f246, %f244, %f245;
	mov.b32 	%r174, %f246;
	mov.u32 	%r175, 4;
	shfl.sync.bfly.b32 	%r176|%p48, %r174, %r175, %r167, %r169;
	mov.b32 	%f247, %r176;
	add.f32 	%f248, %f246, %f247;
	mov.b32 	%r177, %f248;
	mov.u32 	%r178, 2;
	shfl.sync.bfly.b32 	%r179|%p49, %r177, %r178, %r167, %r169;
	mov.b32 	%f249, %r179;
	add.f32 	%f250, %f248, %f249;
	mov.b32 	%r180, %f250;
	mov.u32 	%r181, 1;
	shfl.sync.bfly.b32 	%r182|%p50, %r180, %r181, %r167, %r169;
	mov.b32 	%f251, %r182;
	add.f32 	%f252, %f250, %f251;
	st.local.f32 	[%rd3+12], %f252;

$L__BB27_19:
	bar.sync 	0;
	setp.gt.s32 	%p51, %r3, 3;
	@%p51 bra 	$L__BB27_21;

	mul.wide.s32 	%rd68, %r3, 4;
	add.s64 	%rd69, %rd3, %rd68;
	ld.local.f32 	%f253, [%rd69];
	mad.lo.s32 	%r183, %r3, %r17, %r2;
	cvt.s64.s32 	%rd70, %r183;
	mul.lo.s32 	%r184, %r1, %r18;
	cvt.s64.s32 	%rd71, %r184;
	add.s64 	%rd72, %rd71, %rd70;
	cvta.to.global.u64 	%rd73, %rd33;
	shl.b64 	%rd74, %rd72, 2;
	add.s64 	%rd75, %rd73, %rd74;
	st.global.f32 	[%rd75], %f253;

$L__BB27_21:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_5_bs_128
.visible .entry ggml_matvec_f32_ncols_5_bs_128(
	.param .u64 ggml_matvec_f32_ncols_5_bs_128_param_0,
	.param .u64 ggml_matvec_f32_ncols_5_bs_128_param_1,
	.param .u64 ggml_matvec_f32_ncols_5_bs_128_param_2,
	.param .u32 ggml_matvec_f32_ncols_5_bs_128_param_3,
	.param .u32 ggml_matvec_f32_ncols_5_bs_128_param_4,
	.param .u32 ggml_matvec_f32_ncols_5_bs_128_param_5,
	.param .u32 ggml_matvec_f32_ncols_5_bs_128_param_6,
	.param .u32 ggml_matvec_f32_ncols_5_bs_128_param_7,
	.param .u32 ggml_matvec_f32_ncols_5_bs_128_param_8,
	.param .u32 ggml_matvec_f32_ncols_5_bs_128_param_9,
	.param .u32 ggml_matvec_f32_ncols_5_bs_128_param_10,
	.param .u32 ggml_matvec_f32_ncols_5_bs_128_param_11
)
{
	.local .align 4 .b8 	__local_depot28[20];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<63>;
	.reg .f32 	%f<332>;
	.reg .b32 	%r<225>;
	.reg .b64 	%rd<74>;


	mov.u64 	%SPL, __local_depot28;
	ld.param.u64 	%rd28, [ggml_matvec_f32_ncols_5_bs_128_param_0];
	ld.param.u64 	%rd29, [ggml_matvec_f32_ncols_5_bs_128_param_1];
	ld.param.u64 	%rd27, [ggml_matvec_f32_ncols_5_bs_128_param_2];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_5_bs_128_param_3];
	ld.param.u32 	%r19, [ggml_matvec_f32_ncols_5_bs_128_param_5];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_5_bs_128_param_6];
	ld.param.u32 	%r17, [ggml_matvec_f32_ncols_5_bs_128_param_7];
	ld.param.u32 	%r20, [ggml_matvec_f32_ncols_5_bs_128_param_8];
	ld.param.u32 	%r21, [ggml_matvec_f32_ncols_5_bs_128_param_9];
	ld.param.u32 	%r22, [ggml_matvec_f32_ncols_5_bs_128_param_10];
	ld.param.u32 	%r18, [ggml_matvec_f32_ncols_5_bs_128_param_11];
	cvta.to.global.u64 	%rd73, %rd29;
	cvta.to.global.u64 	%rd2, %rd28;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r23, %r1, %r20;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r24, %r2, %r19;
	mad.lo.s32 	%r25, %r23, %r21, %r24;
	cvt.s64.s32 	%rd4, %r25;
	mul.lo.s32 	%r26, %r1, %r22;
	cvt.s64.s32 	%rd5, %r26;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r27, %r3, 2;
	mov.u32 	%r28, data_mmv;
	add.s32 	%r4, %r28, %r27;
	@%p1 bra 	$L__BB28_2;

	mov.u32 	%r29, 0;
	st.shared.u32 	[%r4], %r29;

$L__BB28_2:
	bar.sync 	0;
	mov.f32 	%f327, 0f00000000;
	mov.u32 	%r30, 0;
	st.local.u32 	[%rd3], %r30;
	st.local.u32 	[%rd3+4], %r30;
	st.local.u32 	[%rd3+8], %r30;
	st.local.u32 	[%rd3+12], %r30;
	st.local.u32 	[%rd3+16], %r30;
	setp.ge.s32 	%p2, %r3, %r15;
	mov.f32 	%f328, %f327;
	mov.f32 	%f329, %f327;
	mov.f32 	%f330, %f327;
	mov.f32 	%f331, %f327;
	@%p2 bra 	$L__BB28_11;

	not.b32 	%r31, %r3;
	add.s32 	%r5, %r31, %r15;
	shr.u32 	%r32, %r5, 7;
	add.s32 	%r33, %r32, 1;
	and.b32  	%r222, %r33, 3;
	setp.eq.s32 	%p3, %r222, 0;
	mov.f32 	%f327, 0f00000000;
	mov.u32 	%r223, %r3;
	@%p3 bra 	$L__BB28_7;

	shl.b32 	%r34, %r16, 1;
	mad.lo.s32 	%r35, %r16, 3, %r3;
	mul.wide.s32 	%rd31, %r35, 8;
	shl.b64 	%rd32, %rd5, 2;
	add.s64 	%rd7, %rd31, %rd32;
	mul.wide.s32 	%rd33, %r3, 8;
	mul.wide.s32 	%rd34, %r16, 8;
	add.s64 	%rd35, %rd33, %rd34;
	add.s64 	%rd8, %rd35, %rd32;
	add.s64 	%rd9, %rd33, %rd32;
	mul.wide.s32 	%rd36, %r3, 2;
	add.s64 	%rd37, %rd36, %rd4;
	shl.b64 	%rd38, %rd37, 2;
	add.s64 	%rd70, %rd2, %rd38;
	mul.wide.s32 	%rd11, %r34, 8;
	mov.f32 	%f327, 0f00000000;
	mov.u64 	%rd71, %rd73;
	mov.f32 	%f328, %f327;
	mov.f32 	%f329, %f327;
	mov.f32 	%f330, %f327;
	mov.f32 	%f331, %f327;
	mov.u32 	%r223, %r3;

$L__BB28_5:
	.pragma "nounroll";
	ld.global.nc.v2.f32 	{%f46, %f47}, [%rd70];
	add.s64 	%rd39, %rd71, %rd9;
	ld.global.nc.v2.f32 	{%f50, %f51}, [%rd39];
	fma.rn.f32 	%f54, %f46, %f50, %f331;
	fma.rn.f32 	%f331, %f47, %f51, %f54;
	add.s64 	%rd40, %rd71, %rd8;
	ld.global.nc.v2.f32 	{%f55, %f56}, [%rd40];
	fma.rn.f32 	%f59, %f46, %f55, %f330;
	fma.rn.f32 	%f330, %f47, %f56, %f59;
	add.s64 	%rd41, %rd39, %rd11;
	ld.global.nc.v2.f32 	{%f60, %f61}, [%rd41];
	fma.rn.f32 	%f64, %f46, %f60, %f329;
	fma.rn.f32 	%f329, %f47, %f61, %f64;
	add.s64 	%rd42, %rd71, %rd7;
	ld.global.nc.v2.f32 	{%f65, %f66}, [%rd42];
	fma.rn.f32 	%f69, %f46, %f65, %f328;
	fma.rn.f32 	%f328, %f47, %f66, %f69;
	add.s64 	%rd43, %rd41, %rd11;
	ld.global.nc.v2.f32 	{%f70, %f71}, [%rd43];
	fma.rn.f32 	%f74, %f46, %f70, %f327;
	fma.rn.f32 	%f327, %f47, %f71, %f74;
	add.s32 	%r223, %r223, 128;
	add.s64 	%rd71, %rd71, 1024;
	add.s64 	%rd70, %rd70, 1024;
	add.s32 	%r222, %r222, -1;
	setp.ne.s32 	%p4, %r222, 0;
	@%p4 bra 	$L__BB28_5;

	st.local.f32 	[%rd3], %f331;
	st.local.f32 	[%rd3+4], %f330;
	st.local.f32 	[%rd3+8], %f329;
	st.local.f32 	[%rd3+12], %f328;
	st.local.f32 	[%rd3+16], %f327;

$L__BB28_7:
	setp.lt.u32 	%p5, %r5, 384;
	@%p5 bra 	$L__BB28_11;

	add.s32 	%r36, %r223, %r16;
	shl.b32 	%r37, %r16, 1;
	add.s32 	%r38, %r223, %r37;
	mad.lo.s32 	%r39, %r16, 3, %r223;
	shl.b32 	%r40, %r16, 2;
	add.s32 	%r41, %r223, %r40;
	add.s32 	%r42, %r36, 128;
	mul.wide.s32 	%rd44, %r42, 8;
	shl.b64 	%rd45, %rd5, 2;
	add.s64 	%rd16, %rd44, %rd45;
	mul.wide.s32 	%rd46, %r38, 8;
	add.s64 	%rd17, %rd46, %rd45;
	mul.wide.s32 	%rd47, %r39, 8;
	add.s64 	%rd18, %rd47, %rd45;
	mul.wide.s32 	%rd48, %r41, 8;
	add.s64 	%rd19, %rd48, %rd45;
	mul.wide.s32 	%rd49, %r223, 2;
	add.s64 	%rd50, %rd49, %rd4;
	shl.b64 	%rd51, %rd50, 2;
	add.s64 	%rd52, %rd2, %rd51;
	add.s64 	%rd72, %rd52, 2048;
	mul.wide.s32 	%rd53, %r223, 8;
	add.s64 	%rd21, %rd53, %rd45;
	mul.wide.s32 	%rd54, %r16, 8;
	add.s64 	%rd55, %rd53, %rd54;
	add.s64 	%rd22, %rd55, %rd45;

$L__BB28_9:
	ld.global.nc.v2.f32 	{%f75, %f76}, [%rd72+-2048];
	add.s64 	%rd56, %rd73, %rd21;
	ld.global.nc.v2.f32 	{%f79, %f80}, [%rd56];
	fma.rn.f32 	%f83, %f75, %f79, %f331;
	fma.rn.f32 	%f84, %f76, %f80, %f83;
	add.s64 	%rd57, %rd73, %rd22;
	ld.global.nc.v2.f32 	{%f85, %f86}, [%rd57];
	fma.rn.f32 	%f89, %f75, %f85, %f330;
	fma.rn.f32 	%f90, %f76, %f86, %f89;
	add.s64 	%rd58, %rd73, %rd17;
	ld.global.nc.v2.f32 	{%f91, %f92}, [%rd58];
	fma.rn.f32 	%f95, %f75, %f91, %f329;
	fma.rn.f32 	%f96, %f76, %f92, %f95;
	add.s64 	%rd59, %rd73, %rd18;
	ld.global.nc.v2.f32 	{%f97, %f98}, [%rd59];
	fma.rn.f32 	%f101, %f75, %f97, %f328;
	fma.rn.f32 	%f102, %f76, %f98, %f101;
	add.s64 	%rd60, %rd73, %rd19;
	ld.global.nc.v2.f32 	{%f103, %f104}, [%rd60];
	fma.rn.f32 	%f107, %f75, %f103, %f327;
	fma.rn.f32 	%f108, %f76, %f104, %f107;
	ld.global.nc.v2.f32 	{%f109, %f110}, [%rd72+-1024];
	ld.global.nc.v2.f32 	{%f113, %f114}, [%rd56+1024];
	fma.rn.f32 	%f117, %f109, %f113, %f84;
	fma.rn.f32 	%f118, %f110, %f114, %f117;
	add.s64 	%rd61, %rd73, %rd16;
	ld.global.nc.v2.f32 	{%f119, %f120}, [%rd61];
	fma.rn.f32 	%f123, %f109, %f119, %f90;
	fma.rn.f32 	%f124, %f110, %f120, %f123;
	ld.global.nc.v2.f32 	{%f125, %f126}, [%rd58+1024];
	fma.rn.f32 	%f129, %f109, %f125, %f96;
	fma.rn.f32 	%f130, %f110, %f126, %f129;
	ld.global.nc.v2.f32 	{%f131, %f132}, [%rd59+1024];
	fma.rn.f32 	%f135, %f109, %f131, %f102;
	fma.rn.f32 	%f136, %f110, %f132, %f135;
	ld.global.nc.v2.f32 	{%f137, %f138}, [%rd60+1024];
	fma.rn.f32 	%f141, %f109, %f137, %f108;
	fma.rn.f32 	%f142, %f110, %f138, %f141;
	ld.global.nc.v2.f32 	{%f143, %f144}, [%rd72];
	ld.global.nc.v2.f32 	{%f147, %f148}, [%rd56+2048];
	fma.rn.f32 	%f151, %f143, %f147, %f118;
	fma.rn.f32 	%f152, %f144, %f148, %f151;
	ld.global.nc.v2.f32 	{%f153, %f154}, [%rd61+1024];
	fma.rn.f32 	%f157, %f143, %f153, %f124;
	fma.rn.f32 	%f158, %f144, %f154, %f157;
	ld.global.nc.v2.f32 	{%f159, %f160}, [%rd58+2048];
	fma.rn.f32 	%f163, %f143, %f159, %f130;
	fma.rn.f32 	%f164, %f144, %f160, %f163;
	ld.global.nc.v2.f32 	{%f165, %f166}, [%rd59+2048];
	fma.rn.f32 	%f169, %f143, %f165, %f136;
	fma.rn.f32 	%f170, %f144, %f166, %f169;
	ld.global.nc.v2.f32 	{%f171, %f172}, [%rd60+2048];
	fma.rn.f32 	%f175, %f143, %f171, %f142;
	fma.rn.f32 	%f176, %f144, %f172, %f175;
	ld.global.nc.v2.f32 	{%f177, %f178}, [%rd72+1024];
	ld.global.nc.v2.f32 	{%f181, %f182}, [%rd56+3072];
	fma.rn.f32 	%f185, %f177, %f181, %f152;
	fma.rn.f32 	%f331, %f178, %f182, %f185;
	ld.global.nc.v2.f32 	{%f186, %f187}, [%rd61+2048];
	fma.rn.f32 	%f190, %f177, %f186, %f158;
	fma.rn.f32 	%f330, %f178, %f187, %f190;
	ld.global.nc.v2.f32 	{%f191, %f192}, [%rd58+3072];
	fma.rn.f32 	%f195, %f177, %f191, %f164;
	fma.rn.f32 	%f329, %f178, %f192, %f195;
	ld.global.nc.v2.f32 	{%f196, %f197}, [%rd59+3072];
	fma.rn.f32 	%f200, %f177, %f196, %f170;
	fma.rn.f32 	%f328, %f178, %f197, %f200;
	ld.global.nc.v2.f32 	{%f201, %f202}, [%rd60+3072];
	fma.rn.f32 	%f205, %f177, %f201, %f176;
	fma.rn.f32 	%f327, %f178, %f202, %f205;
	add.s64 	%rd73, %rd73, 4096;
	add.s64 	%rd72, %rd72, 4096;
	add.s32 	%r223, %r223, 512;
	setp.lt.s32 	%p6, %r223, %r15;
	@%p6 bra 	$L__BB28_9;

	st.local.f32 	[%rd3], %f331;
	st.local.f32 	[%rd3+4], %f330;
	st.local.f32 	[%rd3+8], %f329;
	st.local.f32 	[%rd3+12], %f328;
	st.local.f32 	[%rd3+16], %f327;

$L__BB28_11:
	shr.s32 	%r43, %r3, 31;
	shr.u32 	%r44, %r43, 27;
	add.s32 	%r45, %r3, %r44;
	shr.s32 	%r46, %r45, 5;
	shl.b32 	%r47, %r46, 2;
	add.s32 	%r14, %r28, %r47;
	mov.u32 	%r49, 2;
	mov.b32 	%r50, %f331;
	mov.u32 	%r51, 31;
	mov.u32 	%r52, 16;
	mov.u32 	%r53, -1;
	shfl.sync.bfly.b32 	%r54|%p7, %r50, %r52, %r51, %r53;
	mov.b32 	%f206, %r54;
	add.f32 	%f207, %f331, %f206;
	mov.b32 	%r55, %f207;
	mov.u32 	%r56, 8;
	shfl.sync.bfly.b32 	%r57|%p8, %r55, %r56, %r51, %r53;
	mov.b32 	%f208, %r57;
	add.f32 	%f209, %f207, %f208;
	mov.b32 	%r58, %f209;
	mov.u32 	%r59, 4;
	shfl.sync.bfly.b32 	%r60|%p9, %r58, %r59, %r51, %r53;
	mov.b32 	%f210, %r60;
	add.f32 	%f211, %f209, %f210;
	mov.b32 	%r61, %f211;
	shfl.sync.bfly.b32 	%r62|%p10, %r61, %r49, %r51, %r53;
	mov.b32 	%f212, %r62;
	add.f32 	%f213, %f211, %f212;
	mov.b32 	%r63, %f213;
	mov.u32 	%r64, 1;
	shfl.sync.bfly.b32 	%r65|%p11, %r63, %r64, %r51, %r53;
	mov.b32 	%f214, %r65;
	add.f32 	%f215, %f213, %f214;
	st.local.f32 	[%rd3], %f215;
	st.shared.f32 	[%r14], %f215;
	bar.sync 	0;
	@%p1 bra 	$L__BB28_13;

	ld.shared.f32 	%f216, [%r4];
	mov.b32 	%r66, %f216;
	shfl.sync.bfly.b32 	%r70|%p13, %r66, %r52, %r51, %r53;
	mov.b32 	%f217, %r70;
	add.f32 	%f218, %f216, %f217;
	mov.b32 	%r71, %f218;
	shfl.sync.bfly.b32 	%r73|%p14, %r71, %r56, %r51, %r53;
	mov.b32 	%f219, %r73;
	add.f32 	%f220, %f218, %f219;
	mov.b32 	%r74, %f220;
	shfl.sync.bfly.b32 	%r76|%p15, %r74, %r59, %r51, %r53;
	mov.b32 	%f221, %r76;
	add.f32 	%f222, %f220, %f221;
	mov.b32 	%r77, %f222;
	shfl.sync.bfly.b32 	%r79|%p16, %r77, %r49, %r51, %r53;
	mov.b32 	%f223, %r79;
	add.f32 	%f224, %f222, %f223;
	mov.b32 	%r80, %f224;
	shfl.sync.bfly.b32 	%r82|%p17, %r80, %r64, %r51, %r53;
	mov.b32 	%f225, %r82;
	add.f32 	%f226, %f224, %f225;
	st.local.f32 	[%rd3], %f226;

$L__BB28_13:
	bar.sync 	0;
	mov.b32 	%r83, %f330;
	shfl.sync.bfly.b32 	%r87|%p19, %r83, %r52, %r51, %r53;
	mov.b32 	%f227, %r87;
	add.f32 	%f228, %f330, %f227;
	mov.b32 	%r88, %f228;
	shfl.sync.bfly.b32 	%r90|%p20, %r88, %r56, %r51, %r53;
	mov.b32 	%f229, %r90;
	add.f32 	%f230, %f228, %f229;
	mov.b32 	%r91, %f230;
	shfl.sync.bfly.b32 	%r93|%p21, %r91, %r59, %r51, %r53;
	mov.b32 	%f231, %r93;
	add.f32 	%f232, %f230, %f231;
	mov.b32 	%r94, %f232;
	shfl.sync.bfly.b32 	%r96|%p22, %r94, %r49, %r51, %r53;
	mov.b32 	%f233, %r96;
	add.f32 	%f234, %f232, %f233;
	mov.b32 	%r97, %f234;
	shfl.sync.bfly.b32 	%r99|%p23, %r97, %r64, %r51, %r53;
	mov.b32 	%f235, %r99;
	add.f32 	%f236, %f234, %f235;
	st.local.f32 	[%rd3+4], %f236;
	st.shared.f32 	[%r14], %f236;
	bar.sync 	0;
	@%p1 bra 	$L__BB28_15;

	ld.shared.f32 	%f237, [%r4];
	mov.b32 	%r100, %f237;
	mov.u32 	%r101, 31;
	mov.u32 	%r102, 16;
	mov.u32 	%r103, -1;
	shfl.sync.bfly.b32 	%r104|%p24, %r100, %r102, %r101, %r103;
	mov.b32 	%f238, %r104;
	add.f32 	%f239, %f237, %f238;
	mov.b32 	%r105, %f239;
	mov.u32 	%r106, 8;
	shfl.sync.bfly.b32 	%r107|%p25, %r105, %r106, %r101, %r103;
	mov.b32 	%f240, %r107;
	add.f32 	%f241, %f239, %f240;
	mov.b32 	%r108, %f241;
	mov.u32 	%r109, 4;
	shfl.sync.bfly.b32 	%r110|%p26, %r108, %r109, %r101, %r103;
	mov.b32 	%f242, %r110;
	add.f32 	%f243, %f241, %f242;
	mov.b32 	%r111, %f243;
	mov.u32 	%r112, 2;
	shfl.sync.bfly.b32 	%r113|%p27, %r111, %r112, %r101, %r103;
	mov.b32 	%f244, %r113;
	add.f32 	%f245, %f243, %f244;
	mov.b32 	%r114, %f245;
	mov.u32 	%r115, 1;
	shfl.sync.bfly.b32 	%r116|%p28, %r114, %r115, %r101, %r103;
	mov.b32 	%f246, %r116;
	add.f32 	%f247, %f245, %f246;
	st.local.f32 	[%rd3+4], %f247;

$L__BB28_15:
	bar.sync 	0;
	mov.b32 	%r117, %f329;
	mov.u32 	%r118, 31;
	mov.u32 	%r119, 16;
	mov.u32 	%r120, -1;
	shfl.sync.bfly.b32 	%r121|%p30, %r117, %r119, %r118, %r120;
	mov.b32 	%f248, %r121;
	add.f32 	%f249, %f329, %f248;
	mov.b32 	%r122, %f249;
	mov.u32 	%r123, 8;
	shfl.sync.bfly.b32 	%r124|%p31, %r122, %r123, %r118, %r120;
	mov.b32 	%f250, %r124;
	add.f32 	%f251, %f249, %f250;
	mov.b32 	%r125, %f251;
	mov.u32 	%r126, 4;
	shfl.sync.bfly.b32 	%r127|%p32, %r125, %r126, %r118, %r120;
	mov.b32 	%f252, %r127;
	add.f32 	%f253, %f251, %f252;
	mov.b32 	%r128, %f253;
	mov.u32 	%r129, 2;
	shfl.sync.bfly.b32 	%r130|%p33, %r128, %r129, %r118, %r120;
	mov.b32 	%f254, %r130;
	add.f32 	%f255, %f253, %f254;
	mov.b32 	%r131, %f255;
	mov.u32 	%r132, 1;
	shfl.sync.bfly.b32 	%r133|%p34, %r131, %r132, %r118, %r120;
	mov.b32 	%f256, %r133;
	add.f32 	%f257, %f255, %f256;
	st.local.f32 	[%rd3+8], %f257;
	st.shared.f32 	[%r14], %f257;
	bar.sync 	0;
	@%p1 bra 	$L__BB28_17;

	ld.shared.f32 	%f258, [%r4];
	mov.b32 	%r134, %f258;
	shfl.sync.bfly.b32 	%r138|%p35, %r134, %r119, %r118, %r120;
	mov.b32 	%f259, %r138;
	add.f32 	%f260, %f258, %f259;
	mov.b32 	%r139, %f260;
	shfl.sync.bfly.b32 	%r141|%p36, %r139, %r123, %r118, %r120;
	mov.b32 	%f261, %r141;
	add.f32 	%f262, %f260, %f261;
	mov.b32 	%r142, %f262;
	shfl.sync.bfly.b32 	%r144|%p37, %r142, %r126, %r118, %r120;
	mov.b32 	%f263, %r144;
	add.f32 	%f264, %f262, %f263;
	mov.b32 	%r145, %f264;
	shfl.sync.bfly.b32 	%r147|%p38, %r145, %r129, %r118, %r120;
	mov.b32 	%f265, %r147;
	add.f32 	%f266, %f264, %f265;
	mov.b32 	%r148, %f266;
	shfl.sync.bfly.b32 	%r150|%p39, %r148, %r132, %r118, %r120;
	mov.b32 	%f267, %r150;
	add.f32 	%f268, %f266, %f267;
	st.local.f32 	[%rd3+8], %f268;

$L__BB28_17:
	bar.sync 	0;
	mov.b32 	%r151, %f328;
	shfl.sync.bfly.b32 	%r155|%p41, %r151, %r119, %r118, %r120;
	mov.b32 	%f269, %r155;
	add.f32 	%f270, %f328, %f269;
	mov.b32 	%r156, %f270;
	shfl.sync.bfly.b32 	%r158|%p42, %r156, %r123, %r118, %r120;
	mov.b32 	%f271, %r158;
	add.f32 	%f272, %f270, %f271;
	mov.b32 	%r159, %f272;
	shfl.sync.bfly.b32 	%r161|%p43, %r159, %r126, %r118, %r120;
	mov.b32 	%f273, %r161;
	add.f32 	%f274, %f272, %f273;
	mov.b32 	%r162, %f274;
	shfl.sync.bfly.b32 	%r164|%p44, %r162, %r129, %r118, %r120;
	mov.b32 	%f275, %r164;
	add.f32 	%f276, %f274, %f275;
	mov.b32 	%r165, %f276;
	shfl.sync.bfly.b32 	%r167|%p45, %r165, %r132, %r118, %r120;
	mov.b32 	%f277, %r167;
	add.f32 	%f278, %f276, %f277;
	st.local.f32 	[%rd3+12], %f278;
	st.shared.f32 	[%r14], %f278;
	bar.sync 	0;
	@%p1 bra 	$L__BB28_19;

	ld.shared.f32 	%f279, [%r4];
	mov.b32 	%r168, %f279;
	mov.u32 	%r169, 31;
	mov.u32 	%r170, 16;
	mov.u32 	%r171, -1;
	shfl.sync.bfly.b32 	%r172|%p46, %r168, %r170, %r169, %r171;
	mov.b32 	%f280, %r172;
	add.f32 	%f281, %f279, %f280;
	mov.b32 	%r173, %f281;
	mov.u32 	%r174, 8;
	shfl.sync.bfly.b32 	%r175|%p47, %r173, %r174, %r169, %r171;
	mov.b32 	%f282, %r175;
	add.f32 	%f283, %f281, %f282;
	mov.b32 	%r176, %f283;
	mov.u32 	%r177, 4;
	shfl.sync.bfly.b32 	%r178|%p48, %r176, %r177, %r169, %r171;
	mov.b32 	%f284, %r178;
	add.f32 	%f285, %f283, %f284;
	mov.b32 	%r179, %f285;
	mov.u32 	%r180, 2;
	shfl.sync.bfly.b32 	%r181|%p49, %r179, %r180, %r169, %r171;
	mov.b32 	%f286, %r181;
	add.f32 	%f287, %f285, %f286;
	mov.b32 	%r182, %f287;
	mov.u32 	%r183, 1;
	shfl.sync.bfly.b32 	%r184|%p50, %r182, %r183, %r169, %r171;
	mov.b32 	%f288, %r184;
	add.f32 	%f289, %f287, %f288;
	st.local.f32 	[%rd3+12], %f289;

$L__BB28_19:
	bar.sync 	0;
	mov.b32 	%r185, %f327;
	mov.u32 	%r186, 31;
	mov.u32 	%r187, 16;
	mov.u32 	%r188, -1;
	shfl.sync.bfly.b32 	%r189|%p52, %r185, %r187, %r186, %r188;
	mov.b32 	%f290, %r189;
	add.f32 	%f291, %f327, %f290;
	mov.b32 	%r190, %f291;
	mov.u32 	%r191, 8;
	shfl.sync.bfly.b32 	%r192|%p53, %r190, %r191, %r186, %r188;
	mov.b32 	%f292, %r192;
	add.f32 	%f293, %f291, %f292;
	mov.b32 	%r193, %f293;
	mov.u32 	%r194, 4;
	shfl.sync.bfly.b32 	%r195|%p54, %r193, %r194, %r186, %r188;
	mov.b32 	%f294, %r195;
	add.f32 	%f295, %f293, %f294;
	mov.b32 	%r196, %f295;
	mov.u32 	%r197, 2;
	shfl.sync.bfly.b32 	%r198|%p55, %r196, %r197, %r186, %r188;
	mov.b32 	%f296, %r198;
	add.f32 	%f297, %f295, %f296;
	mov.b32 	%r199, %f297;
	mov.u32 	%r200, 1;
	shfl.sync.bfly.b32 	%r201|%p56, %r199, %r200, %r186, %r188;
	mov.b32 	%f298, %r201;
	add.f32 	%f299, %f297, %f298;
	st.local.f32 	[%rd3+16], %f299;
	st.shared.f32 	[%r14], %f299;
	bar.sync 	0;
	@%p1 bra 	$L__BB28_21;

	ld.shared.f32 	%f300, [%r4];
	mov.b32 	%r202, %f300;
	shfl.sync.bfly.b32 	%r206|%p57, %r202, %r187, %r186, %r188;
	mov.b32 	%f301, %r206;
	add.f32 	%f302, %f300, %f301;
	mov.b32 	%r207, %f302;
	shfl.sync.bfly.b32 	%r209|%p58, %r207, %r191, %r186, %r188;
	mov.b32 	%f303, %r209;
	add.f32 	%f304, %f302, %f303;
	mov.b32 	%r210, %f304;
	shfl.sync.bfly.b32 	%r212|%p59, %r210, %r194, %r186, %r188;
	mov.b32 	%f305, %r212;
	add.f32 	%f306, %f304, %f305;
	mov.b32 	%r213, %f306;
	shfl.sync.bfly.b32 	%r215|%p60, %r213, %r197, %r186, %r188;
	mov.b32 	%f307, %r215;
	add.f32 	%f308, %f306, %f307;
	mov.b32 	%r216, %f308;
	shfl.sync.bfly.b32 	%r218|%p61, %r216, %r200, %r186, %r188;
	mov.b32 	%f309, %r218;
	add.f32 	%f310, %f308, %f309;
	st.local.f32 	[%rd3+16], %f310;

$L__BB28_21:
	bar.sync 	0;
	setp.gt.s32 	%p62, %r3, 4;
	@%p62 bra 	$L__BB28_23;

	mul.wide.s32 	%rd62, %r3, 4;
	add.s64 	%rd63, %rd3, %rd62;
	ld.local.f32 	%f311, [%rd63];
	mad.lo.s32 	%r219, %r3, %r17, %r2;
	cvt.s64.s32 	%rd64, %r219;
	mul.lo.s32 	%r220, %r1, %r18;
	cvt.s64.s32 	%rd65, %r220;
	add.s64 	%rd66, %rd65, %rd64;
	cvta.to.global.u64 	%rd67, %rd27;
	shl.b64 	%rd68, %rd66, 2;
	add.s64 	%rd69, %rd67, %rd68;
	st.global.f32 	[%rd69], %f311;

$L__BB28_23:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_6_bs_128
.visible .entry ggml_matvec_f32_ncols_6_bs_128(
	.param .u64 ggml_matvec_f32_ncols_6_bs_128_param_0,
	.param .u64 ggml_matvec_f32_ncols_6_bs_128_param_1,
	.param .u64 ggml_matvec_f32_ncols_6_bs_128_param_2,
	.param .u32 ggml_matvec_f32_ncols_6_bs_128_param_3,
	.param .u32 ggml_matvec_f32_ncols_6_bs_128_param_4,
	.param .u32 ggml_matvec_f32_ncols_6_bs_128_param_5,
	.param .u32 ggml_matvec_f32_ncols_6_bs_128_param_6,
	.param .u32 ggml_matvec_f32_ncols_6_bs_128_param_7,
	.param .u32 ggml_matvec_f32_ncols_6_bs_128_param_8,
	.param .u32 ggml_matvec_f32_ncols_6_bs_128_param_9,
	.param .u32 ggml_matvec_f32_ncols_6_bs_128_param_10,
	.param .u32 ggml_matvec_f32_ncols_6_bs_128_param_11
)
{
	.local .align 8 .b8 	__local_depot29[24];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<73>;
	.reg .f32 	%f<296>;
	.reg .b32 	%r<253>;
	.reg .b64 	%rd<67>;


	mov.u64 	%SPL, __local_depot29;
	ld.param.u64 	%rd20, [ggml_matvec_f32_ncols_6_bs_128_param_0];
	ld.param.u64 	%rd21, [ggml_matvec_f32_ncols_6_bs_128_param_1];
	ld.param.u64 	%rd19, [ggml_matvec_f32_ncols_6_bs_128_param_2];
	ld.param.u32 	%r11, [ggml_matvec_f32_ncols_6_bs_128_param_3];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_6_bs_128_param_5];
	ld.param.u32 	%r12, [ggml_matvec_f32_ncols_6_bs_128_param_6];
	ld.param.u32 	%r13, [ggml_matvec_f32_ncols_6_bs_128_param_7];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_6_bs_128_param_8];
	ld.param.u32 	%r17, [ggml_matvec_f32_ncols_6_bs_128_param_9];
	ld.param.u32 	%r18, [ggml_matvec_f32_ncols_6_bs_128_param_10];
	ld.param.u32 	%r14, [ggml_matvec_f32_ncols_6_bs_128_param_11];
	cvta.to.global.u64 	%rd66, %rd21;
	add.u64 	%rd2, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r19, %r1, %r16;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r20, %r2, %r15;
	mad.lo.s32 	%r21, %r19, %r17, %r20;
	cvt.s64.s32 	%rd3, %r21;
	cvta.to.global.u64 	%rd4, %rd20;
	mul.lo.s32 	%r22, %r1, %r18;
	cvt.s64.s32 	%rd5, %r22;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r23, %r3, 2;
	mov.u32 	%r24, data_mmv;
	add.s32 	%r4, %r24, %r23;
	@%p1 bra 	$L__BB29_2;

	mov.u32 	%r25, 0;
	st.shared.u32 	[%r4], %r25;

$L__BB29_2:
	bar.sync 	0;
	mov.f32 	%f290, 0f00000000;
	st.local.v2.f32 	[%rd2], {%f290, %f290};
	st.local.v2.f32 	[%rd2+8], {%f290, %f290};
	st.local.v2.f32 	[%rd2+16], {%f290, %f290};
	setp.ge.s32 	%p2, %r3, %r11;
	mov.f32 	%f291, %f290;
	mov.f32 	%f292, %f290;
	mov.f32 	%f293, %f290;
	mov.f32 	%f294, %f290;
	mov.f32 	%f295, %f290;
	@%p2 bra 	$L__BB29_9;

	not.b32 	%r26, %r3;
	add.s32 	%r5, %r26, %r11;
	and.b32  	%r27, %r5, 128;
	setp.ne.s32 	%p3, %r27, 0;
	mov.f32 	%f290, 0f00000000;
	mov.u32 	%r252, %r3;
	@%p3 bra 	$L__BB29_5;

	shl.b64 	%rd23, %rd5, 2;
	add.s64 	%rd24, %rd66, %rd23;
	shl.b64 	%rd25, %rd3, 2;
	add.s64 	%rd26, %rd4, %rd25;
	mul.wide.s32 	%rd27, %r3, 8;
	add.s64 	%rd28, %rd26, %rd27;
	ld.global.nc.v2.f32 	{%f43, %f44}, [%rd28];
	add.s64 	%rd29, %rd24, %rd27;
	ld.global.nc.v2.f32 	{%f47, %f48}, [%rd29];
	fma.rn.f32 	%f51, %f43, %f47, 0f00000000;
	fma.rn.f32 	%f295, %f44, %f48, %f51;
	mul.wide.s32 	%rd30, %r12, 8;
	add.s64 	%rd31, %rd29, %rd30;
	ld.global.nc.v2.f32 	{%f52, %f53}, [%rd31];
	fma.rn.f32 	%f56, %f43, %f52, 0f00000000;
	fma.rn.f32 	%f294, %f44, %f53, %f56;
	st.local.v2.f32 	[%rd2], {%f295, %f294};
	add.s32 	%r28, %r3, %r12;
	add.s32 	%r29, %r28, %r12;
	mul.wide.s32 	%rd32, %r29, 8;
	add.s64 	%rd33, %rd24, %rd32;
	ld.global.nc.v2.f32 	{%f57, %f58}, [%rd33];
	fma.rn.f32 	%f61, %f43, %f57, 0f00000000;
	fma.rn.f32 	%f293, %f44, %f58, %f61;
	add.s64 	%rd34, %rd33, %rd30;
	ld.global.nc.v2.f32 	{%f62, %f63}, [%rd34];
	fma.rn.f32 	%f66, %f43, %f62, 0f00000000;
	fma.rn.f32 	%f292, %f44, %f63, %f66;
	st.local.v2.f32 	[%rd2+8], {%f293, %f292};
	add.s64 	%rd35, %rd34, %rd30;
	ld.global.nc.v2.f32 	{%f67, %f68}, [%rd35];
	fma.rn.f32 	%f71, %f43, %f67, 0f00000000;
	fma.rn.f32 	%f291, %f44, %f68, %f71;
	add.s64 	%rd36, %rd35, %rd30;
	ld.global.nc.v2.f32 	{%f72, %f73}, [%rd36];
	fma.rn.f32 	%f76, %f43, %f72, 0f00000000;
	fma.rn.f32 	%f290, %f44, %f73, %f76;
	st.local.v2.f32 	[%rd2+16], {%f291, %f290};
	add.s32 	%r252, %r3, 128;

$L__BB29_5:
	and.b32  	%r30, %r5, -128;
	setp.eq.s32 	%p4, %r30, 0;
	@%p4 bra 	$L__BB29_9;

	add.s32 	%r31, %r252, %r12;
	add.s32 	%r32, %r31, 128;
	mul.wide.s32 	%rd37, %r32, 8;
	shl.b64 	%rd38, %rd5, 2;
	add.s64 	%rd7, %rd37, %rd38;
	shl.b32 	%r33, %r12, 1;
	add.s32 	%r34, %r252, %r33;
	mad.lo.s32 	%r35, %r12, 3, %r252;
	shl.b32 	%r36, %r12, 2;
	add.s32 	%r37, %r252, %r36;
	mad.lo.s32 	%r38, %r12, 5, %r252;
	mul.wide.s32 	%rd39, %r34, 8;
	add.s64 	%rd8, %rd39, %rd38;
	mul.wide.s32 	%rd40, %r35, 8;
	add.s64 	%rd9, %rd40, %rd38;
	mul.wide.s32 	%rd41, %r37, 8;
	add.s64 	%rd10, %rd41, %rd38;
	mul.wide.s32 	%rd42, %r38, 8;
	add.s64 	%rd11, %rd42, %rd38;
	mul.wide.s32 	%rd43, %r252, 2;
	add.s64 	%rd44, %rd43, %rd3;
	shl.b64 	%rd45, %rd44, 2;
	add.s64 	%rd46, %rd4, %rd45;
	add.s64 	%rd65, %rd46, 1024;
	mul.wide.s32 	%rd47, %r252, 8;
	mul.wide.s32 	%rd48, %r12, 8;
	add.s64 	%rd49, %rd47, %rd48;
	add.s64 	%rd13, %rd49, %rd38;
	add.s64 	%rd14, %rd47, %rd38;

$L__BB29_7:
	ld.global.nc.v2.f32 	{%f77, %f78}, [%rd65+-1024];
	add.s64 	%rd50, %rd66, %rd14;
	ld.global.nc.v2.f32 	{%f81, %f82}, [%rd50];
	fma.rn.f32 	%f85, %f77, %f81, %f295;
	fma.rn.f32 	%f86, %f78, %f82, %f85;
	add.s64 	%rd51, %rd66, %rd13;
	ld.global.nc.v2.f32 	{%f87, %f88}, [%rd51];
	fma.rn.f32 	%f91, %f77, %f87, %f294;
	fma.rn.f32 	%f92, %f78, %f88, %f91;
	add.s64 	%rd52, %rd66, %rd8;
	ld.global.nc.v2.f32 	{%f93, %f94}, [%rd52];
	fma.rn.f32 	%f97, %f77, %f93, %f293;
	fma.rn.f32 	%f98, %f78, %f94, %f97;
	add.s64 	%rd53, %rd66, %rd9;
	ld.global.nc.v2.f32 	{%f99, %f100}, [%rd53];
	fma.rn.f32 	%f103, %f77, %f99, %f292;
	fma.rn.f32 	%f104, %f78, %f100, %f103;
	add.s64 	%rd54, %rd66, %rd10;
	ld.global.nc.v2.f32 	{%f105, %f106}, [%rd54];
	fma.rn.f32 	%f109, %f77, %f105, %f291;
	fma.rn.f32 	%f110, %f78, %f106, %f109;
	add.s64 	%rd55, %rd66, %rd11;
	ld.global.nc.v2.f32 	{%f111, %f112}, [%rd55];
	fma.rn.f32 	%f115, %f77, %f111, %f290;
	fma.rn.f32 	%f116, %f78, %f112, %f115;
	ld.global.nc.v2.f32 	{%f117, %f118}, [%rd65];
	ld.global.nc.v2.f32 	{%f121, %f122}, [%rd50+1024];
	fma.rn.f32 	%f125, %f117, %f121, %f86;
	fma.rn.f32 	%f295, %f118, %f122, %f125;
	add.s64 	%rd56, %rd66, %rd7;
	ld.global.nc.v2.f32 	{%f126, %f127}, [%rd56];
	fma.rn.f32 	%f130, %f117, %f126, %f92;
	fma.rn.f32 	%f294, %f118, %f127, %f130;
	ld.global.nc.v2.f32 	{%f131, %f132}, [%rd52+1024];
	fma.rn.f32 	%f135, %f117, %f131, %f98;
	fma.rn.f32 	%f293, %f118, %f132, %f135;
	ld.global.nc.v2.f32 	{%f136, %f137}, [%rd53+1024];
	fma.rn.f32 	%f140, %f117, %f136, %f104;
	fma.rn.f32 	%f292, %f118, %f137, %f140;
	ld.global.nc.v2.f32 	{%f141, %f142}, [%rd54+1024];
	fma.rn.f32 	%f145, %f117, %f141, %f110;
	fma.rn.f32 	%f291, %f118, %f142, %f145;
	ld.global.nc.v2.f32 	{%f146, %f147}, [%rd55+1024];
	fma.rn.f32 	%f150, %f117, %f146, %f116;
	fma.rn.f32 	%f290, %f118, %f147, %f150;
	add.s64 	%rd66, %rd66, 2048;
	add.s64 	%rd65, %rd65, 2048;
	add.s32 	%r252, %r252, 256;
	setp.lt.s32 	%p5, %r252, %r11;
	@%p5 bra 	$L__BB29_7;

	st.local.v2.f32 	[%rd2], {%f295, %f294};
	st.local.v2.f32 	[%rd2+8], {%f293, %f292};
	st.local.v2.f32 	[%rd2+16], {%f291, %f290};

$L__BB29_9:
	shr.s32 	%r39, %r3, 31;
	shr.u32 	%r40, %r39, 27;
	add.s32 	%r41, %r3, %r40;
	shr.s32 	%r42, %r41, 5;
	shl.b32 	%r43, %r42, 2;
	add.s32 	%r10, %r24, %r43;
	mov.u32 	%r45, 2;
	mov.b32 	%r46, %f295;
	mov.u32 	%r47, 31;
	mov.u32 	%r48, 16;
	mov.u32 	%r49, -1;
	shfl.sync.bfly.b32 	%r50|%p6, %r46, %r48, %r47, %r49;
	mov.b32 	%f151, %r50;
	add.f32 	%f152, %f295, %f151;
	mov.b32 	%r51, %f152;
	mov.u32 	%r52, 8;
	shfl.sync.bfly.b32 	%r53|%p7, %r51, %r52, %r47, %r49;
	mov.b32 	%f153, %r53;
	add.f32 	%f154, %f152, %f153;
	mov.b32 	%r54, %f154;
	mov.u32 	%r55, 4;
	shfl.sync.bfly.b32 	%r56|%p8, %r54, %r55, %r47, %r49;
	mov.b32 	%f155, %r56;
	add.f32 	%f156, %f154, %f155;
	mov.b32 	%r57, %f156;
	shfl.sync.bfly.b32 	%r58|%p9, %r57, %r45, %r47, %r49;
	mov.b32 	%f157, %r58;
	add.f32 	%f158, %f156, %f157;
	mov.b32 	%r59, %f158;
	mov.u32 	%r60, 1;
	shfl.sync.bfly.b32 	%r61|%p10, %r59, %r60, %r47, %r49;
	mov.b32 	%f159, %r61;
	add.f32 	%f160, %f158, %f159;
	st.local.f32 	[%rd2], %f160;
	st.shared.f32 	[%r10], %f160;
	bar.sync 	0;
	@%p1 bra 	$L__BB29_11;

	ld.shared.f32 	%f161, [%r4];
	mov.b32 	%r62, %f161;
	shfl.sync.bfly.b32 	%r66|%p12, %r62, %r48, %r47, %r49;
	mov.b32 	%f162, %r66;
	add.f32 	%f163, %f161, %f162;
	mov.b32 	%r67, %f163;
	shfl.sync.bfly.b32 	%r69|%p13, %r67, %r52, %r47, %r49;
	mov.b32 	%f164, %r69;
	add.f32 	%f165, %f163, %f164;
	mov.b32 	%r70, %f165;
	shfl.sync.bfly.b32 	%r72|%p14, %r70, %r55, %r47, %r49;
	mov.b32 	%f166, %r72;
	add.f32 	%f167, %f165, %f166;
	mov.b32 	%r73, %f167;
	shfl.sync.bfly.b32 	%r75|%p15, %r73, %r45, %r47, %r49;
	mov.b32 	%f168, %r75;
	add.f32 	%f169, %f167, %f168;
	mov.b32 	%r76, %f169;
	shfl.sync.bfly.b32 	%r78|%p16, %r76, %r60, %r47, %r49;
	mov.b32 	%f170, %r78;
	add.f32 	%f171, %f169, %f170;
	st.local.f32 	[%rd2], %f171;

$L__BB29_11:
	bar.sync 	0;
	mov.b32 	%r79, %f294;
	shfl.sync.bfly.b32 	%r83|%p18, %r79, %r48, %r47, %r49;
	mov.b32 	%f172, %r83;
	add.f32 	%f173, %f294, %f172;
	mov.b32 	%r84, %f173;
	shfl.sync.bfly.b32 	%r86|%p19, %r84, %r52, %r47, %r49;
	mov.b32 	%f174, %r86;
	add.f32 	%f175, %f173, %f174;
	mov.b32 	%r87, %f175;
	shfl.sync.bfly.b32 	%r89|%p20, %r87, %r55, %r47, %r49;
	mov.b32 	%f176, %r89;
	add.f32 	%f177, %f175, %f176;
	mov.b32 	%r90, %f177;
	shfl.sync.bfly.b32 	%r92|%p21, %r90, %r45, %r47, %r49;
	mov.b32 	%f178, %r92;
	add.f32 	%f179, %f177, %f178;
	mov.b32 	%r93, %f179;
	shfl.sync.bfly.b32 	%r95|%p22, %r93, %r60, %r47, %r49;
	mov.b32 	%f180, %r95;
	add.f32 	%f181, %f179, %f180;
	st.local.f32 	[%rd2+4], %f181;
	st.shared.f32 	[%r10], %f181;
	bar.sync 	0;
	@%p1 bra 	$L__BB29_13;

	ld.shared.f32 	%f182, [%r4];
	mov.b32 	%r96, %f182;
	mov.u32 	%r97, 31;
	mov.u32 	%r98, 16;
	mov.u32 	%r99, -1;
	shfl.sync.bfly.b32 	%r100|%p23, %r96, %r98, %r97, %r99;
	mov.b32 	%f183, %r100;
	add.f32 	%f184, %f182, %f183;
	mov.b32 	%r101, %f184;
	mov.u32 	%r102, 8;
	shfl.sync.bfly.b32 	%r103|%p24, %r101, %r102, %r97, %r99;
	mov.b32 	%f185, %r103;
	add.f32 	%f186, %f184, %f185;
	mov.b32 	%r104, %f186;
	mov.u32 	%r105, 4;
	shfl.sync.bfly.b32 	%r106|%p25, %r104, %r105, %r97, %r99;
	mov.b32 	%f187, %r106;
	add.f32 	%f188, %f186, %f187;
	mov.b32 	%r107, %f188;
	mov.u32 	%r108, 2;
	shfl.sync.bfly.b32 	%r109|%p26, %r107, %r108, %r97, %r99;
	mov.b32 	%f189, %r109;
	add.f32 	%f190, %f188, %f189;
	mov.b32 	%r110, %f190;
	mov.u32 	%r111, 1;
	shfl.sync.bfly.b32 	%r112|%p27, %r110, %r111, %r97, %r99;
	mov.b32 	%f191, %r112;
	add.f32 	%f192, %f190, %f191;
	st.local.f32 	[%rd2+4], %f192;

$L__BB29_13:
	bar.sync 	0;
	mov.b32 	%r113, %f293;
	mov.u32 	%r114, 31;
	mov.u32 	%r115, 16;
	mov.u32 	%r116, -1;
	shfl.sync.bfly.b32 	%r117|%p29, %r113, %r115, %r114, %r116;
	mov.b32 	%f193, %r117;
	add.f32 	%f194, %f293, %f193;
	mov.b32 	%r118, %f194;
	mov.u32 	%r119, 8;
	shfl.sync.bfly.b32 	%r120|%p30, %r118, %r119, %r114, %r116;
	mov.b32 	%f195, %r120;
	add.f32 	%f196, %f194, %f195;
	mov.b32 	%r121, %f196;
	mov.u32 	%r122, 4;
	shfl.sync.bfly.b32 	%r123|%p31, %r121, %r122, %r114, %r116;
	mov.b32 	%f197, %r123;
	add.f32 	%f198, %f196, %f197;
	mov.b32 	%r124, %f198;
	mov.u32 	%r125, 2;
	shfl.sync.bfly.b32 	%r126|%p32, %r124, %r125, %r114, %r116;
	mov.b32 	%f199, %r126;
	add.f32 	%f200, %f198, %f199;
	mov.b32 	%r127, %f200;
	mov.u32 	%r128, 1;
	shfl.sync.bfly.b32 	%r129|%p33, %r127, %r128, %r114, %r116;
	mov.b32 	%f201, %r129;
	add.f32 	%f202, %f200, %f201;
	st.local.f32 	[%rd2+8], %f202;
	st.shared.f32 	[%r10], %f202;
	bar.sync 	0;
	@%p1 bra 	$L__BB29_15;

	ld.shared.f32 	%f203, [%r4];
	mov.b32 	%r130, %f203;
	shfl.sync.bfly.b32 	%r134|%p34, %r130, %r115, %r114, %r116;
	mov.b32 	%f204, %r134;
	add.f32 	%f205, %f203, %f204;
	mov.b32 	%r135, %f205;
	shfl.sync.bfly.b32 	%r137|%p35, %r135, %r119, %r114, %r116;
	mov.b32 	%f206, %r137;
	add.f32 	%f207, %f205, %f206;
	mov.b32 	%r138, %f207;
	shfl.sync.bfly.b32 	%r140|%p36, %r138, %r122, %r114, %r116;
	mov.b32 	%f208, %r140;
	add.f32 	%f209, %f207, %f208;
	mov.b32 	%r141, %f209;
	shfl.sync.bfly.b32 	%r143|%p37, %r141, %r125, %r114, %r116;
	mov.b32 	%f210, %r143;
	add.f32 	%f211, %f209, %f210;
	mov.b32 	%r144, %f211;
	shfl.sync.bfly.b32 	%r146|%p38, %r144, %r128, %r114, %r116;
	mov.b32 	%f212, %r146;
	add.f32 	%f213, %f211, %f212;
	st.local.f32 	[%rd2+8], %f213;

$L__BB29_15:
	bar.sync 	0;
	mov.b32 	%r147, %f292;
	shfl.sync.bfly.b32 	%r151|%p40, %r147, %r115, %r114, %r116;
	mov.b32 	%f214, %r151;
	add.f32 	%f215, %f292, %f214;
	mov.b32 	%r152, %f215;
	shfl.sync.bfly.b32 	%r154|%p41, %r152, %r119, %r114, %r116;
	mov.b32 	%f216, %r154;
	add.f32 	%f217, %f215, %f216;
	mov.b32 	%r155, %f217;
	shfl.sync.bfly.b32 	%r157|%p42, %r155, %r122, %r114, %r116;
	mov.b32 	%f218, %r157;
	add.f32 	%f219, %f217, %f218;
	mov.b32 	%r158, %f219;
	shfl.sync.bfly.b32 	%r160|%p43, %r158, %r125, %r114, %r116;
	mov.b32 	%f220, %r160;
	add.f32 	%f221, %f219, %f220;
	mov.b32 	%r161, %f221;
	shfl.sync.bfly.b32 	%r163|%p44, %r161, %r128, %r114, %r116;
	mov.b32 	%f222, %r163;
	add.f32 	%f223, %f221, %f222;
	st.local.f32 	[%rd2+12], %f223;
	st.shared.f32 	[%r10], %f223;
	bar.sync 	0;
	@%p1 bra 	$L__BB29_17;

	ld.shared.f32 	%f224, [%r4];
	mov.b32 	%r164, %f224;
	mov.u32 	%r165, 31;
	mov.u32 	%r166, 16;
	mov.u32 	%r167, -1;
	shfl.sync.bfly.b32 	%r168|%p45, %r164, %r166, %r165, %r167;
	mov.b32 	%f225, %r168;
	add.f32 	%f226, %f224, %f225;
	mov.b32 	%r169, %f226;
	mov.u32 	%r170, 8;
	shfl.sync.bfly.b32 	%r171|%p46, %r169, %r170, %r165, %r167;
	mov.b32 	%f227, %r171;
	add.f32 	%f228, %f226, %f227;
	mov.b32 	%r172, %f228;
	mov.u32 	%r173, 4;
	shfl.sync.bfly.b32 	%r174|%p47, %r172, %r173, %r165, %r167;
	mov.b32 	%f229, %r174;
	add.f32 	%f230, %f228, %f229;
	mov.b32 	%r175, %f230;
	mov.u32 	%r176, 2;
	shfl.sync.bfly.b32 	%r177|%p48, %r175, %r176, %r165, %r167;
	mov.b32 	%f231, %r177;
	add.f32 	%f232, %f230, %f231;
	mov.b32 	%r178, %f232;
	mov.u32 	%r179, 1;
	shfl.sync.bfly.b32 	%r180|%p49, %r178, %r179, %r165, %r167;
	mov.b32 	%f233, %r180;
	add.f32 	%f234, %f232, %f233;
	st.local.f32 	[%rd2+12], %f234;

$L__BB29_17:
	bar.sync 	0;
	mov.b32 	%r181, %f291;
	mov.u32 	%r182, 31;
	mov.u32 	%r183, 16;
	mov.u32 	%r184, -1;
	shfl.sync.bfly.b32 	%r185|%p51, %r181, %r183, %r182, %r184;
	mov.b32 	%f235, %r185;
	add.f32 	%f236, %f291, %f235;
	mov.b32 	%r186, %f236;
	mov.u32 	%r187, 8;
	shfl.sync.bfly.b32 	%r188|%p52, %r186, %r187, %r182, %r184;
	mov.b32 	%f237, %r188;
	add.f32 	%f238, %f236, %f237;
	mov.b32 	%r189, %f238;
	mov.u32 	%r190, 4;
	shfl.sync.bfly.b32 	%r191|%p53, %r189, %r190, %r182, %r184;
	mov.b32 	%f239, %r191;
	add.f32 	%f240, %f238, %f239;
	mov.b32 	%r192, %f240;
	mov.u32 	%r193, 2;
	shfl.sync.bfly.b32 	%r194|%p54, %r192, %r193, %r182, %r184;
	mov.b32 	%f241, %r194;
	add.f32 	%f242, %f240, %f241;
	mov.b32 	%r195, %f242;
	mov.u32 	%r196, 1;
	shfl.sync.bfly.b32 	%r197|%p55, %r195, %r196, %r182, %r184;
	mov.b32 	%f243, %r197;
	add.f32 	%f244, %f242, %f243;
	st.local.f32 	[%rd2+16], %f244;
	st.shared.f32 	[%r10], %f244;
	bar.sync 	0;
	@%p1 bra 	$L__BB29_19;

	ld.shared.f32 	%f245, [%r4];
	mov.b32 	%r198, %f245;
	shfl.sync.bfly.b32 	%r202|%p56, %r198, %r183, %r182, %r184;
	mov.b32 	%f246, %r202;
	add.f32 	%f247, %f245, %f246;
	mov.b32 	%r203, %f247;
	shfl.sync.bfly.b32 	%r205|%p57, %r203, %r187, %r182, %r184;
	mov.b32 	%f248, %r205;
	add.f32 	%f249, %f247, %f248;
	mov.b32 	%r206, %f249;
	shfl.sync.bfly.b32 	%r208|%p58, %r206, %r190, %r182, %r184;
	mov.b32 	%f250, %r208;
	add.f32 	%f251, %f249, %f250;
	mov.b32 	%r209, %f251;
	shfl.sync.bfly.b32 	%r211|%p59, %r209, %r193, %r182, %r184;
	mov.b32 	%f252, %r211;
	add.f32 	%f253, %f251, %f252;
	mov.b32 	%r212, %f253;
	shfl.sync.bfly.b32 	%r214|%p60, %r212, %r196, %r182, %r184;
	mov.b32 	%f254, %r214;
	add.f32 	%f255, %f253, %f254;
	st.local.f32 	[%rd2+16], %f255;

$L__BB29_19:
	bar.sync 	0;
	mov.b32 	%r215, %f290;
	shfl.sync.bfly.b32 	%r219|%p62, %r215, %r183, %r182, %r184;
	mov.b32 	%f256, %r219;
	add.f32 	%f257, %f290, %f256;
	mov.b32 	%r220, %f257;
	shfl.sync.bfly.b32 	%r222|%p63, %r220, %r187, %r182, %r184;
	mov.b32 	%f258, %r222;
	add.f32 	%f259, %f257, %f258;
	mov.b32 	%r223, %f259;
	shfl.sync.bfly.b32 	%r225|%p64, %r223, %r190, %r182, %r184;
	mov.b32 	%f260, %r225;
	add.f32 	%f261, %f259, %f260;
	mov.b32 	%r226, %f261;
	shfl.sync.bfly.b32 	%r228|%p65, %r226, %r193, %r182, %r184;
	mov.b32 	%f262, %r228;
	add.f32 	%f263, %f261, %f262;
	mov.b32 	%r229, %f263;
	shfl.sync.bfly.b32 	%r231|%p66, %r229, %r196, %r182, %r184;
	mov.b32 	%f264, %r231;
	add.f32 	%f265, %f263, %f264;
	st.local.f32 	[%rd2+20], %f265;
	st.shared.f32 	[%r10], %f265;
	bar.sync 	0;
	@%p1 bra 	$L__BB29_21;

	ld.shared.f32 	%f266, [%r4];
	mov.b32 	%r232, %f266;
	mov.u32 	%r233, 31;
	mov.u32 	%r234, 16;
	mov.u32 	%r235, -1;
	shfl.sync.bfly.b32 	%r236|%p67, %r232, %r234, %r233, %r235;
	mov.b32 	%f267, %r236;
	add.f32 	%f268, %f266, %f267;
	mov.b32 	%r237, %f268;
	mov.u32 	%r238, 8;
	shfl.sync.bfly.b32 	%r239|%p68, %r237, %r238, %r233, %r235;
	mov.b32 	%f269, %r239;
	add.f32 	%f270, %f268, %f269;
	mov.b32 	%r240, %f270;
	mov.u32 	%r241, 4;
	shfl.sync.bfly.b32 	%r242|%p69, %r240, %r241, %r233, %r235;
	mov.b32 	%f271, %r242;
	add.f32 	%f272, %f270, %f271;
	mov.b32 	%r243, %f272;
	mov.u32 	%r244, 2;
	shfl.sync.bfly.b32 	%r245|%p70, %r243, %r244, %r233, %r235;
	mov.b32 	%f273, %r245;
	add.f32 	%f274, %f272, %f273;
	mov.b32 	%r246, %f274;
	mov.u32 	%r247, 1;
	shfl.sync.bfly.b32 	%r248|%p71, %r246, %r247, %r233, %r235;
	mov.b32 	%f275, %r248;
	add.f32 	%f276, %f274, %f275;
	st.local.f32 	[%rd2+20], %f276;

$L__BB29_21:
	bar.sync 	0;
	setp.gt.s32 	%p72, %r3, 5;
	@%p72 bra 	$L__BB29_23;

	mul.wide.s32 	%rd57, %r3, 4;
	add.s64 	%rd58, %rd2, %rd57;
	ld.local.f32 	%f277, [%rd58];
	mad.lo.s32 	%r249, %r3, %r13, %r2;
	cvt.s64.s32 	%rd59, %r249;
	mul.lo.s32 	%r250, %r1, %r14;
	cvt.s64.s32 	%rd60, %r250;
	add.s64 	%rd61, %rd60, %rd59;
	cvta.to.global.u64 	%rd62, %rd19;
	shl.b64 	%rd63, %rd61, 2;
	add.s64 	%rd64, %rd62, %rd63;
	st.global.f32 	[%rd64], %f277;

$L__BB29_23:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_7_bs_128
.visible .entry ggml_matvec_f32_ncols_7_bs_128(
	.param .u64 ggml_matvec_f32_ncols_7_bs_128_param_0,
	.param .u64 ggml_matvec_f32_ncols_7_bs_128_param_1,
	.param .u64 ggml_matvec_f32_ncols_7_bs_128_param_2,
	.param .u32 ggml_matvec_f32_ncols_7_bs_128_param_3,
	.param .u32 ggml_matvec_f32_ncols_7_bs_128_param_4,
	.param .u32 ggml_matvec_f32_ncols_7_bs_128_param_5,
	.param .u32 ggml_matvec_f32_ncols_7_bs_128_param_6,
	.param .u32 ggml_matvec_f32_ncols_7_bs_128_param_7,
	.param .u32 ggml_matvec_f32_ncols_7_bs_128_param_8,
	.param .u32 ggml_matvec_f32_ncols_7_bs_128_param_9,
	.param .u32 ggml_matvec_f32_ncols_7_bs_128_param_10,
	.param .u32 ggml_matvec_f32_ncols_7_bs_128_param_11
)
{
	.local .align 4 .b8 	__local_depot30[28];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<84>;
	.reg .f32 	%f<343>;
	.reg .b32 	%r<289>;
	.reg .b64 	%rd<71>;


	mov.u64 	%SPL, __local_depot30;
	ld.param.u64 	%rd21, [ggml_matvec_f32_ncols_7_bs_128_param_0];
	ld.param.u64 	%rd22, [ggml_matvec_f32_ncols_7_bs_128_param_1];
	ld.param.u64 	%rd20, [ggml_matvec_f32_ncols_7_bs_128_param_2];
	ld.param.u32 	%r11, [ggml_matvec_f32_ncols_7_bs_128_param_3];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_7_bs_128_param_5];
	ld.param.u32 	%r12, [ggml_matvec_f32_ncols_7_bs_128_param_6];
	ld.param.u32 	%r13, [ggml_matvec_f32_ncols_7_bs_128_param_7];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_7_bs_128_param_8];
	ld.param.u32 	%r17, [ggml_matvec_f32_ncols_7_bs_128_param_9];
	ld.param.u32 	%r18, [ggml_matvec_f32_ncols_7_bs_128_param_10];
	ld.param.u32 	%r14, [ggml_matvec_f32_ncols_7_bs_128_param_11];
	cvta.to.global.u64 	%rd70, %rd22;
	add.u64 	%rd2, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r19, %r1, %r16;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r20, %r2, %r15;
	mad.lo.s32 	%r21, %r19, %r17, %r20;
	cvt.s64.s32 	%rd3, %r21;
	cvta.to.global.u64 	%rd4, %rd21;
	mul.lo.s32 	%r22, %r1, %r18;
	cvt.s64.s32 	%rd5, %r22;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r23, %r3, 2;
	mov.u32 	%r24, data_mmv;
	add.s32 	%r4, %r24, %r23;
	@%p1 bra 	$L__BB30_2;

	mov.u32 	%r25, 0;
	st.shared.u32 	[%r4], %r25;

$L__BB30_2:
	bar.sync 	0;
	mov.f32 	%f336, 0f00000000;
	mov.u32 	%r26, 0;
	st.local.u32 	[%rd2], %r26;
	st.local.u32 	[%rd2+4], %r26;
	st.local.u32 	[%rd2+8], %r26;
	st.local.u32 	[%rd2+12], %r26;
	st.local.u32 	[%rd2+16], %r26;
	st.local.u32 	[%rd2+20], %r26;
	st.local.u32 	[%rd2+24], %r26;
	setp.ge.s32 	%p2, %r3, %r11;
	mov.f32 	%f337, %f336;
	mov.f32 	%f338, %f336;
	mov.f32 	%f339, %f336;
	mov.f32 	%f340, %f336;
	mov.f32 	%f341, %f336;
	mov.f32 	%f342, %f336;
	@%p2 bra 	$L__BB30_9;

	not.b32 	%r27, %r3;
	add.s32 	%r5, %r27, %r11;
	and.b32  	%r28, %r5, 128;
	setp.ne.s32 	%p3, %r28, 0;
	mov.f32 	%f336, 0f00000000;
	mov.u32 	%r288, %r3;
	@%p3 bra 	$L__BB30_5;

	shl.b64 	%rd24, %rd5, 2;
	add.s64 	%rd25, %rd70, %rd24;
	shl.b64 	%rd26, %rd3, 2;
	add.s64 	%rd27, %rd4, %rd26;
	mul.wide.s32 	%rd28, %r3, 8;
	add.s64 	%rd29, %rd27, %rd28;
	ld.global.nc.v2.f32 	{%f50, %f51}, [%rd29];
	add.s64 	%rd30, %rd25, %rd28;
	ld.global.nc.v2.f32 	{%f54, %f55}, [%rd30];
	fma.rn.f32 	%f58, %f50, %f54, 0f00000000;
	fma.rn.f32 	%f342, %f51, %f55, %f58;
	st.local.f32 	[%rd2], %f342;
	mul.wide.s32 	%rd31, %r12, 8;
	add.s64 	%rd32, %rd30, %rd31;
	ld.global.nc.v2.f32 	{%f59, %f60}, [%rd32];
	fma.rn.f32 	%f63, %f50, %f59, 0f00000000;
	fma.rn.f32 	%f341, %f51, %f60, %f63;
	st.local.f32 	[%rd2+4], %f341;
	add.s32 	%r29, %r3, %r12;
	add.s32 	%r30, %r29, %r12;
	mul.wide.s32 	%rd33, %r30, 8;
	add.s64 	%rd34, %rd25, %rd33;
	ld.global.nc.v2.f32 	{%f64, %f65}, [%rd34];
	fma.rn.f32 	%f68, %f50, %f64, 0f00000000;
	fma.rn.f32 	%f340, %f51, %f65, %f68;
	st.local.f32 	[%rd2+8], %f340;
	add.s64 	%rd35, %rd34, %rd31;
	ld.global.nc.v2.f32 	{%f69, %f70}, [%rd35];
	fma.rn.f32 	%f73, %f50, %f69, 0f00000000;
	fma.rn.f32 	%f339, %f51, %f70, %f73;
	st.local.f32 	[%rd2+12], %f339;
	add.s64 	%rd36, %rd35, %rd31;
	ld.global.nc.v2.f32 	{%f74, %f75}, [%rd36];
	fma.rn.f32 	%f78, %f50, %f74, 0f00000000;
	fma.rn.f32 	%f338, %f51, %f75, %f78;
	st.local.f32 	[%rd2+16], %f338;
	add.s64 	%rd37, %rd36, %rd31;
	ld.global.nc.v2.f32 	{%f79, %f80}, [%rd37];
	fma.rn.f32 	%f83, %f50, %f79, 0f00000000;
	fma.rn.f32 	%f337, %f51, %f80, %f83;
	st.local.f32 	[%rd2+20], %f337;
	add.s64 	%rd38, %rd37, %rd31;
	ld.global.nc.v2.f32 	{%f84, %f85}, [%rd38];
	fma.rn.f32 	%f88, %f50, %f84, 0f00000000;
	fma.rn.f32 	%f336, %f51, %f85, %f88;
	st.local.f32 	[%rd2+24], %f336;
	add.s32 	%r288, %r3, 128;

$L__BB30_5:
	and.b32  	%r31, %r5, -128;
	setp.eq.s32 	%p4, %r31, 0;
	@%p4 bra 	$L__BB30_9;

	add.s32 	%r32, %r288, %r12;
	add.s32 	%r33, %r32, 128;
	mul.wide.s32 	%rd39, %r33, 8;
	shl.b64 	%rd40, %rd5, 2;
	add.s64 	%rd7, %rd39, %rd40;
	shl.b32 	%r34, %r12, 1;
	add.s32 	%r35, %r288, %r34;
	mad.lo.s32 	%r36, %r12, 3, %r288;
	shl.b32 	%r37, %r12, 2;
	add.s32 	%r38, %r288, %r37;
	mad.lo.s32 	%r39, %r12, 5, %r288;
	mad.lo.s32 	%r40, %r12, 6, %r288;
	mul.wide.s32 	%rd41, %r35, 8;
	add.s64 	%rd8, %rd41, %rd40;
	mul.wide.s32 	%rd42, %r36, 8;
	add.s64 	%rd9, %rd42, %rd40;
	mul.wide.s32 	%rd43, %r38, 8;
	add.s64 	%rd10, %rd43, %rd40;
	mul.wide.s32 	%rd44, %r39, 8;
	add.s64 	%rd11, %rd44, %rd40;
	mul.wide.s32 	%rd45, %r40, 8;
	add.s64 	%rd12, %rd45, %rd40;
	mul.wide.s32 	%rd46, %r288, 2;
	add.s64 	%rd47, %rd46, %rd3;
	shl.b64 	%rd48, %rd47, 2;
	add.s64 	%rd49, %rd4, %rd48;
	add.s64 	%rd69, %rd49, 1024;
	mul.wide.s32 	%rd50, %r288, 8;
	mul.wide.s32 	%rd51, %r12, 8;
	add.s64 	%rd52, %rd50, %rd51;
	add.s64 	%rd14, %rd52, %rd40;
	add.s64 	%rd15, %rd50, %rd40;

$L__BB30_7:
	ld.global.nc.v2.f32 	{%f89, %f90}, [%rd69+-1024];
	add.s64 	%rd53, %rd70, %rd15;
	ld.global.nc.v2.f32 	{%f93, %f94}, [%rd53];
	fma.rn.f32 	%f97, %f89, %f93, %f342;
	fma.rn.f32 	%f98, %f90, %f94, %f97;
	add.s64 	%rd54, %rd70, %rd14;
	ld.global.nc.v2.f32 	{%f99, %f100}, [%rd54];
	fma.rn.f32 	%f103, %f89, %f99, %f341;
	fma.rn.f32 	%f104, %f90, %f100, %f103;
	add.s64 	%rd55, %rd70, %rd8;
	ld.global.nc.v2.f32 	{%f105, %f106}, [%rd55];
	fma.rn.f32 	%f109, %f89, %f105, %f340;
	fma.rn.f32 	%f110, %f90, %f106, %f109;
	add.s64 	%rd56, %rd70, %rd9;
	ld.global.nc.v2.f32 	{%f111, %f112}, [%rd56];
	fma.rn.f32 	%f115, %f89, %f111, %f339;
	fma.rn.f32 	%f116, %f90, %f112, %f115;
	add.s64 	%rd57, %rd70, %rd10;
	ld.global.nc.v2.f32 	{%f117, %f118}, [%rd57];
	fma.rn.f32 	%f121, %f89, %f117, %f338;
	fma.rn.f32 	%f122, %f90, %f118, %f121;
	add.s64 	%rd58, %rd70, %rd11;
	ld.global.nc.v2.f32 	{%f123, %f124}, [%rd58];
	fma.rn.f32 	%f127, %f89, %f123, %f337;
	fma.rn.f32 	%f128, %f90, %f124, %f127;
	add.s64 	%rd59, %rd70, %rd12;
	ld.global.nc.v2.f32 	{%f129, %f130}, [%rd59];
	fma.rn.f32 	%f133, %f89, %f129, %f336;
	fma.rn.f32 	%f134, %f90, %f130, %f133;
	ld.global.nc.v2.f32 	{%f135, %f136}, [%rd69];
	ld.global.nc.v2.f32 	{%f139, %f140}, [%rd53+1024];
	fma.rn.f32 	%f143, %f135, %f139, %f98;
	fma.rn.f32 	%f342, %f136, %f140, %f143;
	add.s64 	%rd60, %rd70, %rd7;
	ld.global.nc.v2.f32 	{%f144, %f145}, [%rd60];
	fma.rn.f32 	%f148, %f135, %f144, %f104;
	fma.rn.f32 	%f341, %f136, %f145, %f148;
	ld.global.nc.v2.f32 	{%f149, %f150}, [%rd55+1024];
	fma.rn.f32 	%f153, %f135, %f149, %f110;
	fma.rn.f32 	%f340, %f136, %f150, %f153;
	ld.global.nc.v2.f32 	{%f154, %f155}, [%rd56+1024];
	fma.rn.f32 	%f158, %f135, %f154, %f116;
	fma.rn.f32 	%f339, %f136, %f155, %f158;
	ld.global.nc.v2.f32 	{%f159, %f160}, [%rd57+1024];
	fma.rn.f32 	%f163, %f135, %f159, %f122;
	fma.rn.f32 	%f338, %f136, %f160, %f163;
	ld.global.nc.v2.f32 	{%f164, %f165}, [%rd58+1024];
	fma.rn.f32 	%f168, %f135, %f164, %f128;
	fma.rn.f32 	%f337, %f136, %f165, %f168;
	ld.global.nc.v2.f32 	{%f169, %f170}, [%rd59+1024];
	fma.rn.f32 	%f173, %f135, %f169, %f134;
	fma.rn.f32 	%f336, %f136, %f170, %f173;
	add.s64 	%rd70, %rd70, 2048;
	add.s64 	%rd69, %rd69, 2048;
	add.s32 	%r288, %r288, 256;
	setp.lt.s32 	%p5, %r288, %r11;
	@%p5 bra 	$L__BB30_7;

	st.local.f32 	[%rd2], %f342;
	st.local.f32 	[%rd2+4], %f341;
	st.local.f32 	[%rd2+8], %f340;
	st.local.f32 	[%rd2+12], %f339;
	st.local.f32 	[%rd2+16], %f338;
	st.local.f32 	[%rd2+20], %f337;
	st.local.f32 	[%rd2+24], %f336;

$L__BB30_9:
	shr.s32 	%r41, %r3, 31;
	shr.u32 	%r42, %r41, 27;
	add.s32 	%r43, %r3, %r42;
	shr.s32 	%r44, %r43, 5;
	shl.b32 	%r45, %r44, 2;
	add.s32 	%r10, %r24, %r45;
	mov.u32 	%r47, 2;
	mov.b32 	%r48, %f342;
	mov.u32 	%r49, 31;
	mov.u32 	%r50, 16;
	mov.u32 	%r51, -1;
	shfl.sync.bfly.b32 	%r52|%p6, %r48, %r50, %r49, %r51;
	mov.b32 	%f174, %r52;
	add.f32 	%f175, %f342, %f174;
	mov.b32 	%r53, %f175;
	mov.u32 	%r54, 8;
	shfl.sync.bfly.b32 	%r55|%p7, %r53, %r54, %r49, %r51;
	mov.b32 	%f176, %r55;
	add.f32 	%f177, %f175, %f176;
	mov.b32 	%r56, %f177;
	mov.u32 	%r57, 4;
	shfl.sync.bfly.b32 	%r58|%p8, %r56, %r57, %r49, %r51;
	mov.b32 	%f178, %r58;
	add.f32 	%f179, %f177, %f178;
	mov.b32 	%r59, %f179;
	shfl.sync.bfly.b32 	%r60|%p9, %r59, %r47, %r49, %r51;
	mov.b32 	%f180, %r60;
	add.f32 	%f181, %f179, %f180;
	mov.b32 	%r61, %f181;
	mov.u32 	%r62, 1;
	shfl.sync.bfly.b32 	%r63|%p10, %r61, %r62, %r49, %r51;
	mov.b32 	%f182, %r63;
	add.f32 	%f183, %f181, %f182;
	st.local.f32 	[%rd2], %f183;
	st.shared.f32 	[%r10], %f183;
	bar.sync 	0;
	@%p1 bra 	$L__BB30_11;

	ld.shared.f32 	%f184, [%r4];
	mov.b32 	%r64, %f184;
	shfl.sync.bfly.b32 	%r68|%p12, %r64, %r50, %r49, %r51;
	mov.b32 	%f185, %r68;
	add.f32 	%f186, %f184, %f185;
	mov.b32 	%r69, %f186;
	shfl.sync.bfly.b32 	%r71|%p13, %r69, %r54, %r49, %r51;
	mov.b32 	%f187, %r71;
	add.f32 	%f188, %f186, %f187;
	mov.b32 	%r72, %f188;
	shfl.sync.bfly.b32 	%r74|%p14, %r72, %r57, %r49, %r51;
	mov.b32 	%f189, %r74;
	add.f32 	%f190, %f188, %f189;
	mov.b32 	%r75, %f190;
	shfl.sync.bfly.b32 	%r77|%p15, %r75, %r47, %r49, %r51;
	mov.b32 	%f191, %r77;
	add.f32 	%f192, %f190, %f191;
	mov.b32 	%r78, %f192;
	shfl.sync.bfly.b32 	%r80|%p16, %r78, %r62, %r49, %r51;
	mov.b32 	%f193, %r80;
	add.f32 	%f194, %f192, %f193;
	st.local.f32 	[%rd2], %f194;

$L__BB30_11:
	bar.sync 	0;
	mov.b32 	%r81, %f341;
	shfl.sync.bfly.b32 	%r85|%p18, %r81, %r50, %r49, %r51;
	mov.b32 	%f195, %r85;
	add.f32 	%f196, %f341, %f195;
	mov.b32 	%r86, %f196;
	shfl.sync.bfly.b32 	%r88|%p19, %r86, %r54, %r49, %r51;
	mov.b32 	%f197, %r88;
	add.f32 	%f198, %f196, %f197;
	mov.b32 	%r89, %f198;
	shfl.sync.bfly.b32 	%r91|%p20, %r89, %r57, %r49, %r51;
	mov.b32 	%f199, %r91;
	add.f32 	%f200, %f198, %f199;
	mov.b32 	%r92, %f200;
	shfl.sync.bfly.b32 	%r94|%p21, %r92, %r47, %r49, %r51;
	mov.b32 	%f201, %r94;
	add.f32 	%f202, %f200, %f201;
	mov.b32 	%r95, %f202;
	shfl.sync.bfly.b32 	%r97|%p22, %r95, %r62, %r49, %r51;
	mov.b32 	%f203, %r97;
	add.f32 	%f204, %f202, %f203;
	st.local.f32 	[%rd2+4], %f204;
	st.shared.f32 	[%r10], %f204;
	bar.sync 	0;
	@%p1 bra 	$L__BB30_13;

	ld.shared.f32 	%f205, [%r4];
	mov.b32 	%r98, %f205;
	mov.u32 	%r99, 31;
	mov.u32 	%r100, 16;
	mov.u32 	%r101, -1;
	shfl.sync.bfly.b32 	%r102|%p23, %r98, %r100, %r99, %r101;
	mov.b32 	%f206, %r102;
	add.f32 	%f207, %f205, %f206;
	mov.b32 	%r103, %f207;
	mov.u32 	%r104, 8;
	shfl.sync.bfly.b32 	%r105|%p24, %r103, %r104, %r99, %r101;
	mov.b32 	%f208, %r105;
	add.f32 	%f209, %f207, %f208;
	mov.b32 	%r106, %f209;
	mov.u32 	%r107, 4;
	shfl.sync.bfly.b32 	%r108|%p25, %r106, %r107, %r99, %r101;
	mov.b32 	%f210, %r108;
	add.f32 	%f211, %f209, %f210;
	mov.b32 	%r109, %f211;
	mov.u32 	%r110, 2;
	shfl.sync.bfly.b32 	%r111|%p26, %r109, %r110, %r99, %r101;
	mov.b32 	%f212, %r111;
	add.f32 	%f213, %f211, %f212;
	mov.b32 	%r112, %f213;
	mov.u32 	%r113, 1;
	shfl.sync.bfly.b32 	%r114|%p27, %r112, %r113, %r99, %r101;
	mov.b32 	%f214, %r114;
	add.f32 	%f215, %f213, %f214;
	st.local.f32 	[%rd2+4], %f215;

$L__BB30_13:
	bar.sync 	0;
	mov.b32 	%r115, %f340;
	mov.u32 	%r116, 31;
	mov.u32 	%r117, 16;
	mov.u32 	%r118, -1;
	shfl.sync.bfly.b32 	%r119|%p29, %r115, %r117, %r116, %r118;
	mov.b32 	%f216, %r119;
	add.f32 	%f217, %f340, %f216;
	mov.b32 	%r120, %f217;
	mov.u32 	%r121, 8;
	shfl.sync.bfly.b32 	%r122|%p30, %r120, %r121, %r116, %r118;
	mov.b32 	%f218, %r122;
	add.f32 	%f219, %f217, %f218;
	mov.b32 	%r123, %f219;
	mov.u32 	%r124, 4;
	shfl.sync.bfly.b32 	%r125|%p31, %r123, %r124, %r116, %r118;
	mov.b32 	%f220, %r125;
	add.f32 	%f221, %f219, %f220;
	mov.b32 	%r126, %f221;
	mov.u32 	%r127, 2;
	shfl.sync.bfly.b32 	%r128|%p32, %r126, %r127, %r116, %r118;
	mov.b32 	%f222, %r128;
	add.f32 	%f223, %f221, %f222;
	mov.b32 	%r129, %f223;
	mov.u32 	%r130, 1;
	shfl.sync.bfly.b32 	%r131|%p33, %r129, %r130, %r116, %r118;
	mov.b32 	%f224, %r131;
	add.f32 	%f225, %f223, %f224;
	st.local.f32 	[%rd2+8], %f225;
	st.shared.f32 	[%r10], %f225;
	bar.sync 	0;
	@%p1 bra 	$L__BB30_15;

	ld.shared.f32 	%f226, [%r4];
	mov.b32 	%r132, %f226;
	shfl.sync.bfly.b32 	%r136|%p34, %r132, %r117, %r116, %r118;
	mov.b32 	%f227, %r136;
	add.f32 	%f228, %f226, %f227;
	mov.b32 	%r137, %f228;
	shfl.sync.bfly.b32 	%r139|%p35, %r137, %r121, %r116, %r118;
	mov.b32 	%f229, %r139;
	add.f32 	%f230, %f228, %f229;
	mov.b32 	%r140, %f230;
	shfl.sync.bfly.b32 	%r142|%p36, %r140, %r124, %r116, %r118;
	mov.b32 	%f231, %r142;
	add.f32 	%f232, %f230, %f231;
	mov.b32 	%r143, %f232;
	shfl.sync.bfly.b32 	%r145|%p37, %r143, %r127, %r116, %r118;
	mov.b32 	%f233, %r145;
	add.f32 	%f234, %f232, %f233;
	mov.b32 	%r146, %f234;
	shfl.sync.bfly.b32 	%r148|%p38, %r146, %r130, %r116, %r118;
	mov.b32 	%f235, %r148;
	add.f32 	%f236, %f234, %f235;
	st.local.f32 	[%rd2+8], %f236;

$L__BB30_15:
	bar.sync 	0;
	mov.b32 	%r149, %f339;
	shfl.sync.bfly.b32 	%r153|%p40, %r149, %r117, %r116, %r118;
	mov.b32 	%f237, %r153;
	add.f32 	%f238, %f339, %f237;
	mov.b32 	%r154, %f238;
	shfl.sync.bfly.b32 	%r156|%p41, %r154, %r121, %r116, %r118;
	mov.b32 	%f239, %r156;
	add.f32 	%f240, %f238, %f239;
	mov.b32 	%r157, %f240;
	shfl.sync.bfly.b32 	%r159|%p42, %r157, %r124, %r116, %r118;
	mov.b32 	%f241, %r159;
	add.f32 	%f242, %f240, %f241;
	mov.b32 	%r160, %f242;
	shfl.sync.bfly.b32 	%r162|%p43, %r160, %r127, %r116, %r118;
	mov.b32 	%f243, %r162;
	add.f32 	%f244, %f242, %f243;
	mov.b32 	%r163, %f244;
	shfl.sync.bfly.b32 	%r165|%p44, %r163, %r130, %r116, %r118;
	mov.b32 	%f245, %r165;
	add.f32 	%f246, %f244, %f245;
	st.local.f32 	[%rd2+12], %f246;
	st.shared.f32 	[%r10], %f246;
	bar.sync 	0;
	@%p1 bra 	$L__BB30_17;

	ld.shared.f32 	%f247, [%r4];
	mov.b32 	%r166, %f247;
	mov.u32 	%r167, 31;
	mov.u32 	%r168, 16;
	mov.u32 	%r169, -1;
	shfl.sync.bfly.b32 	%r170|%p45, %r166, %r168, %r167, %r169;
	mov.b32 	%f248, %r170;
	add.f32 	%f249, %f247, %f248;
	mov.b32 	%r171, %f249;
	mov.u32 	%r172, 8;
	shfl.sync.bfly.b32 	%r173|%p46, %r171, %r172, %r167, %r169;
	mov.b32 	%f250, %r173;
	add.f32 	%f251, %f249, %f250;
	mov.b32 	%r174, %f251;
	mov.u32 	%r175, 4;
	shfl.sync.bfly.b32 	%r176|%p47, %r174, %r175, %r167, %r169;
	mov.b32 	%f252, %r176;
	add.f32 	%f253, %f251, %f252;
	mov.b32 	%r177, %f253;
	mov.u32 	%r178, 2;
	shfl.sync.bfly.b32 	%r179|%p48, %r177, %r178, %r167, %r169;
	mov.b32 	%f254, %r179;
	add.f32 	%f255, %f253, %f254;
	mov.b32 	%r180, %f255;
	mov.u32 	%r181, 1;
	shfl.sync.bfly.b32 	%r182|%p49, %r180, %r181, %r167, %r169;
	mov.b32 	%f256, %r182;
	add.f32 	%f257, %f255, %f256;
	st.local.f32 	[%rd2+12], %f257;

$L__BB30_17:
	bar.sync 	0;
	mov.b32 	%r183, %f338;
	mov.u32 	%r184, 31;
	mov.u32 	%r185, 16;
	mov.u32 	%r186, -1;
	shfl.sync.bfly.b32 	%r187|%p51, %r183, %r185, %r184, %r186;
	mov.b32 	%f258, %r187;
	add.f32 	%f259, %f338, %f258;
	mov.b32 	%r188, %f259;
	mov.u32 	%r189, 8;
	shfl.sync.bfly.b32 	%r190|%p52, %r188, %r189, %r184, %r186;
	mov.b32 	%f260, %r190;
	add.f32 	%f261, %f259, %f260;
	mov.b32 	%r191, %f261;
	mov.u32 	%r192, 4;
	shfl.sync.bfly.b32 	%r193|%p53, %r191, %r192, %r184, %r186;
	mov.b32 	%f262, %r193;
	add.f32 	%f263, %f261, %f262;
	mov.b32 	%r194, %f263;
	mov.u32 	%r195, 2;
	shfl.sync.bfly.b32 	%r196|%p54, %r194, %r195, %r184, %r186;
	mov.b32 	%f264, %r196;
	add.f32 	%f265, %f263, %f264;
	mov.b32 	%r197, %f265;
	mov.u32 	%r198, 1;
	shfl.sync.bfly.b32 	%r199|%p55, %r197, %r198, %r184, %r186;
	mov.b32 	%f266, %r199;
	add.f32 	%f267, %f265, %f266;
	st.local.f32 	[%rd2+16], %f267;
	st.shared.f32 	[%r10], %f267;
	bar.sync 	0;
	@%p1 bra 	$L__BB30_19;

	ld.shared.f32 	%f268, [%r4];
	mov.b32 	%r200, %f268;
	shfl.sync.bfly.b32 	%r204|%p56, %r200, %r185, %r184, %r186;
	mov.b32 	%f269, %r204;
	add.f32 	%f270, %f268, %f269;
	mov.b32 	%r205, %f270;
	shfl.sync.bfly.b32 	%r207|%p57, %r205, %r189, %r184, %r186;
	mov.b32 	%f271, %r207;
	add.f32 	%f272, %f270, %f271;
	mov.b32 	%r208, %f272;
	shfl.sync.bfly.b32 	%r210|%p58, %r208, %r192, %r184, %r186;
	mov.b32 	%f273, %r210;
	add.f32 	%f274, %f272, %f273;
	mov.b32 	%r211, %f274;
	shfl.sync.bfly.b32 	%r213|%p59, %r211, %r195, %r184, %r186;
	mov.b32 	%f275, %r213;
	add.f32 	%f276, %f274, %f275;
	mov.b32 	%r214, %f276;
	shfl.sync.bfly.b32 	%r216|%p60, %r214, %r198, %r184, %r186;
	mov.b32 	%f277, %r216;
	add.f32 	%f278, %f276, %f277;
	st.local.f32 	[%rd2+16], %f278;

$L__BB30_19:
	bar.sync 	0;
	mov.b32 	%r217, %f337;
	shfl.sync.bfly.b32 	%r221|%p62, %r217, %r185, %r184, %r186;
	mov.b32 	%f279, %r221;
	add.f32 	%f280, %f337, %f279;
	mov.b32 	%r222, %f280;
	shfl.sync.bfly.b32 	%r224|%p63, %r222, %r189, %r184, %r186;
	mov.b32 	%f281, %r224;
	add.f32 	%f282, %f280, %f281;
	mov.b32 	%r225, %f282;
	shfl.sync.bfly.b32 	%r227|%p64, %r225, %r192, %r184, %r186;
	mov.b32 	%f283, %r227;
	add.f32 	%f284, %f282, %f283;
	mov.b32 	%r228, %f284;
	shfl.sync.bfly.b32 	%r230|%p65, %r228, %r195, %r184, %r186;
	mov.b32 	%f285, %r230;
	add.f32 	%f286, %f284, %f285;
	mov.b32 	%r231, %f286;
	shfl.sync.bfly.b32 	%r233|%p66, %r231, %r198, %r184, %r186;
	mov.b32 	%f287, %r233;
	add.f32 	%f288, %f286, %f287;
	st.local.f32 	[%rd2+20], %f288;
	st.shared.f32 	[%r10], %f288;
	bar.sync 	0;
	@%p1 bra 	$L__BB30_21;

	ld.shared.f32 	%f289, [%r4];
	mov.b32 	%r234, %f289;
	mov.u32 	%r235, 31;
	mov.u32 	%r236, 16;
	mov.u32 	%r237, -1;
	shfl.sync.bfly.b32 	%r238|%p67, %r234, %r236, %r235, %r237;
	mov.b32 	%f290, %r238;
	add.f32 	%f291, %f289, %f290;
	mov.b32 	%r239, %f291;
	mov.u32 	%r240, 8;
	shfl.sync.bfly.b32 	%r241|%p68, %r239, %r240, %r235, %r237;
	mov.b32 	%f292, %r241;
	add.f32 	%f293, %f291, %f292;
	mov.b32 	%r242, %f293;
	mov.u32 	%r243, 4;
	shfl.sync.bfly.b32 	%r244|%p69, %r242, %r243, %r235, %r237;
	mov.b32 	%f294, %r244;
	add.f32 	%f295, %f293, %f294;
	mov.b32 	%r245, %f295;
	mov.u32 	%r246, 2;
	shfl.sync.bfly.b32 	%r247|%p70, %r245, %r246, %r235, %r237;
	mov.b32 	%f296, %r247;
	add.f32 	%f297, %f295, %f296;
	mov.b32 	%r248, %f297;
	mov.u32 	%r249, 1;
	shfl.sync.bfly.b32 	%r250|%p71, %r248, %r249, %r235, %r237;
	mov.b32 	%f298, %r250;
	add.f32 	%f299, %f297, %f298;
	st.local.f32 	[%rd2+20], %f299;

$L__BB30_21:
	bar.sync 	0;
	mov.b32 	%r251, %f336;
	mov.u32 	%r252, 31;
	mov.u32 	%r253, 16;
	mov.u32 	%r254, -1;
	shfl.sync.bfly.b32 	%r255|%p73, %r251, %r253, %r252, %r254;
	mov.b32 	%f300, %r255;
	add.f32 	%f301, %f336, %f300;
	mov.b32 	%r256, %f301;
	mov.u32 	%r257, 8;
	shfl.sync.bfly.b32 	%r258|%p74, %r256, %r257, %r252, %r254;
	mov.b32 	%f302, %r258;
	add.f32 	%f303, %f301, %f302;
	mov.b32 	%r259, %f303;
	mov.u32 	%r260, 4;
	shfl.sync.bfly.b32 	%r261|%p75, %r259, %r260, %r252, %r254;
	mov.b32 	%f304, %r261;
	add.f32 	%f305, %f303, %f304;
	mov.b32 	%r262, %f305;
	mov.u32 	%r263, 2;
	shfl.sync.bfly.b32 	%r264|%p76, %r262, %r263, %r252, %r254;
	mov.b32 	%f306, %r264;
	add.f32 	%f307, %f305, %f306;
	mov.b32 	%r265, %f307;
	mov.u32 	%r266, 1;
	shfl.sync.bfly.b32 	%r267|%p77, %r265, %r266, %r252, %r254;
	mov.b32 	%f308, %r267;
	add.f32 	%f309, %f307, %f308;
	st.local.f32 	[%rd2+24], %f309;
	st.shared.f32 	[%r10], %f309;
	bar.sync 	0;
	@%p1 bra 	$L__BB30_23;

	ld.shared.f32 	%f310, [%r4];
	mov.b32 	%r268, %f310;
	shfl.sync.bfly.b32 	%r272|%p78, %r268, %r253, %r252, %r254;
	mov.b32 	%f311, %r272;
	add.f32 	%f312, %f310, %f311;
	mov.b32 	%r273, %f312;
	shfl.sync.bfly.b32 	%r275|%p79, %r273, %r257, %r252, %r254;
	mov.b32 	%f313, %r275;
	add.f32 	%f314, %f312, %f313;
	mov.b32 	%r276, %f314;
	shfl.sync.bfly.b32 	%r278|%p80, %r276, %r260, %r252, %r254;
	mov.b32 	%f315, %r278;
	add.f32 	%f316, %f314, %f315;
	mov.b32 	%r279, %f316;
	shfl.sync.bfly.b32 	%r281|%p81, %r279, %r263, %r252, %r254;
	mov.b32 	%f317, %r281;
	add.f32 	%f318, %f316, %f317;
	mov.b32 	%r282, %f318;
	shfl.sync.bfly.b32 	%r284|%p82, %r282, %r266, %r252, %r254;
	mov.b32 	%f319, %r284;
	add.f32 	%f320, %f318, %f319;
	st.local.f32 	[%rd2+24], %f320;

$L__BB30_23:
	bar.sync 	0;
	setp.gt.s32 	%p83, %r3, 6;
	@%p83 bra 	$L__BB30_25;

	mul.wide.s32 	%rd61, %r3, 4;
	add.s64 	%rd62, %rd2, %rd61;
	ld.local.f32 	%f321, [%rd62];
	mad.lo.s32 	%r285, %r3, %r13, %r2;
	cvt.s64.s32 	%rd63, %r285;
	mul.lo.s32 	%r286, %r1, %r14;
	cvt.s64.s32 	%rd64, %r286;
	add.s64 	%rd65, %rd64, %rd63;
	cvta.to.global.u64 	%rd66, %rd20;
	shl.b64 	%rd67, %rd65, 2;
	add.s64 	%rd68, %rd66, %rd67;
	st.global.f32 	[%rd68], %f321;

$L__BB30_25:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_8_bs_128
.visible .entry ggml_matvec_f32_ncols_8_bs_128(
	.param .u64 ggml_matvec_f32_ncols_8_bs_128_param_0,
	.param .u64 ggml_matvec_f32_ncols_8_bs_128_param_1,
	.param .u64 ggml_matvec_f32_ncols_8_bs_128_param_2,
	.param .u32 ggml_matvec_f32_ncols_8_bs_128_param_3,
	.param .u32 ggml_matvec_f32_ncols_8_bs_128_param_4,
	.param .u32 ggml_matvec_f32_ncols_8_bs_128_param_5,
	.param .u32 ggml_matvec_f32_ncols_8_bs_128_param_6,
	.param .u32 ggml_matvec_f32_ncols_8_bs_128_param_7,
	.param .u32 ggml_matvec_f32_ncols_8_bs_128_param_8,
	.param .u32 ggml_matvec_f32_ncols_8_bs_128_param_9,
	.param .u32 ggml_matvec_f32_ncols_8_bs_128_param_10,
	.param .u32 ggml_matvec_f32_ncols_8_bs_128_param_11
)
{
	.local .align 16 .b8 	__local_depot31[32];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<95>;
	.reg .f32 	%f<390>;
	.reg .b32 	%r<323>;
	.reg .b64 	%rd<75>;


	mov.u64 	%SPL, __local_depot31;
	ld.param.u64 	%rd22, [ggml_matvec_f32_ncols_8_bs_128_param_0];
	ld.param.u64 	%rd23, [ggml_matvec_f32_ncols_8_bs_128_param_1];
	ld.param.u64 	%rd21, [ggml_matvec_f32_ncols_8_bs_128_param_2];
	ld.param.u32 	%r11, [ggml_matvec_f32_ncols_8_bs_128_param_3];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_8_bs_128_param_5];
	ld.param.u32 	%r12, [ggml_matvec_f32_ncols_8_bs_128_param_6];
	ld.param.u32 	%r13, [ggml_matvec_f32_ncols_8_bs_128_param_7];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_8_bs_128_param_8];
	ld.param.u32 	%r17, [ggml_matvec_f32_ncols_8_bs_128_param_9];
	ld.param.u32 	%r18, [ggml_matvec_f32_ncols_8_bs_128_param_10];
	ld.param.u32 	%r14, [ggml_matvec_f32_ncols_8_bs_128_param_11];
	cvta.to.global.u64 	%rd74, %rd23;
	add.u64 	%rd2, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r19, %r1, %r16;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r20, %r2, %r15;
	mad.lo.s32 	%r21, %r19, %r17, %r20;
	cvt.s64.s32 	%rd3, %r21;
	cvta.to.global.u64 	%rd4, %rd22;
	mul.lo.s32 	%r22, %r1, %r18;
	cvt.s64.s32 	%rd5, %r22;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r23, %r3, 2;
	mov.u32 	%r24, data_mmv;
	add.s32 	%r4, %r24, %r23;
	@%p1 bra 	$L__BB31_2;

	mov.u32 	%r25, 0;
	st.shared.u32 	[%r4], %r25;

$L__BB31_2:
	bar.sync 	0;
	mov.f32 	%f382, 0f00000000;
	st.local.v4.f32 	[%rd2], {%f382, %f382, %f382, %f382};
	st.local.v4.f32 	[%rd2+16], {%f382, %f382, %f382, %f382};
	setp.ge.s32 	%p2, %r3, %r11;
	mov.f32 	%f383, %f382;
	mov.f32 	%f384, %f382;
	mov.f32 	%f385, %f382;
	mov.f32 	%f386, %f382;
	mov.f32 	%f387, %f382;
	mov.f32 	%f388, %f382;
	mov.f32 	%f389, %f382;
	@%p2 bra 	$L__BB31_9;

	not.b32 	%r26, %r3;
	add.s32 	%r5, %r26, %r11;
	and.b32  	%r27, %r5, 128;
	setp.ne.s32 	%p3, %r27, 0;
	mov.f32 	%f382, 0f00000000;
	mov.u32 	%r322, %r3;
	@%p3 bra 	$L__BB31_5;

	shl.b64 	%rd25, %rd5, 2;
	add.s64 	%rd26, %rd74, %rd25;
	shl.b64 	%rd27, %rd3, 2;
	add.s64 	%rd28, %rd4, %rd27;
	mul.wide.s32 	%rd29, %r3, 8;
	add.s64 	%rd30, %rd28, %rd29;
	ld.global.nc.v2.f32 	{%f57, %f58}, [%rd30];
	add.s64 	%rd31, %rd26, %rd29;
	ld.global.nc.v2.f32 	{%f61, %f62}, [%rd31];
	fma.rn.f32 	%f65, %f57, %f61, 0f00000000;
	fma.rn.f32 	%f389, %f58, %f62, %f65;
	mul.wide.s32 	%rd32, %r12, 8;
	add.s64 	%rd33, %rd31, %rd32;
	ld.global.nc.v2.f32 	{%f66, %f67}, [%rd33];
	fma.rn.f32 	%f70, %f57, %f66, 0f00000000;
	fma.rn.f32 	%f388, %f58, %f67, %f70;
	add.s32 	%r28, %r3, %r12;
	add.s32 	%r29, %r28, %r12;
	mul.wide.s32 	%rd34, %r29, 8;
	add.s64 	%rd35, %rd26, %rd34;
	ld.global.nc.v2.f32 	{%f71, %f72}, [%rd35];
	fma.rn.f32 	%f75, %f57, %f71, 0f00000000;
	fma.rn.f32 	%f387, %f58, %f72, %f75;
	add.s64 	%rd36, %rd35, %rd32;
	ld.global.nc.v2.f32 	{%f76, %f77}, [%rd36];
	fma.rn.f32 	%f80, %f57, %f76, 0f00000000;
	fma.rn.f32 	%f386, %f58, %f77, %f80;
	st.local.v4.f32 	[%rd2], {%f389, %f388, %f387, %f386};
	add.s64 	%rd37, %rd36, %rd32;
	ld.global.nc.v2.f32 	{%f81, %f82}, [%rd37];
	fma.rn.f32 	%f85, %f57, %f81, 0f00000000;
	fma.rn.f32 	%f385, %f58, %f82, %f85;
	add.s64 	%rd38, %rd37, %rd32;
	ld.global.nc.v2.f32 	{%f86, %f87}, [%rd38];
	fma.rn.f32 	%f90, %f57, %f86, 0f00000000;
	fma.rn.f32 	%f384, %f58, %f87, %f90;
	add.s64 	%rd39, %rd38, %rd32;
	ld.global.nc.v2.f32 	{%f91, %f92}, [%rd39];
	fma.rn.f32 	%f95, %f57, %f91, 0f00000000;
	fma.rn.f32 	%f383, %f58, %f92, %f95;
	add.s64 	%rd40, %rd39, %rd32;
	ld.global.nc.v2.f32 	{%f96, %f97}, [%rd40];
	fma.rn.f32 	%f100, %f57, %f96, 0f00000000;
	fma.rn.f32 	%f382, %f58, %f97, %f100;
	st.local.v4.f32 	[%rd2+16], {%f385, %f384, %f383, %f382};
	add.s32 	%r322, %r3, 128;

$L__BB31_5:
	and.b32  	%r30, %r5, -128;
	setp.eq.s32 	%p4, %r30, 0;
	@%p4 bra 	$L__BB31_9;

	add.s32 	%r31, %r322, %r12;
	add.s32 	%r32, %r31, 128;
	mul.wide.s32 	%rd41, %r32, 8;
	shl.b64 	%rd42, %rd5, 2;
	add.s64 	%rd7, %rd41, %rd42;
	shl.b32 	%r33, %r12, 1;
	add.s32 	%r34, %r322, %r33;
	mad.lo.s32 	%r35, %r12, 3, %r322;
	shl.b32 	%r36, %r12, 2;
	add.s32 	%r37, %r322, %r36;
	mad.lo.s32 	%r38, %r12, 5, %r322;
	mad.lo.s32 	%r39, %r12, 6, %r322;
	mad.lo.s32 	%r40, %r12, 7, %r322;
	mul.wide.s32 	%rd43, %r34, 8;
	add.s64 	%rd8, %rd43, %rd42;
	mul.wide.s32 	%rd44, %r35, 8;
	add.s64 	%rd9, %rd44, %rd42;
	mul.wide.s32 	%rd45, %r37, 8;
	add.s64 	%rd10, %rd45, %rd42;
	mul.wide.s32 	%rd46, %r38, 8;
	add.s64 	%rd11, %rd46, %rd42;
	mul.wide.s32 	%rd47, %r39, 8;
	add.s64 	%rd12, %rd47, %rd42;
	mul.wide.s32 	%rd48, %r40, 8;
	add.s64 	%rd13, %rd48, %rd42;
	mul.wide.s32 	%rd49, %r322, 2;
	add.s64 	%rd50, %rd49, %rd3;
	shl.b64 	%rd51, %rd50, 2;
	add.s64 	%rd52, %rd4, %rd51;
	add.s64 	%rd73, %rd52, 1024;
	mul.wide.s32 	%rd53, %r322, 8;
	mul.wide.s32 	%rd54, %r12, 8;
	add.s64 	%rd55, %rd53, %rd54;
	add.s64 	%rd15, %rd55, %rd42;
	add.s64 	%rd16, %rd53, %rd42;

$L__BB31_7:
	ld.global.nc.v2.f32 	{%f101, %f102}, [%rd73+-1024];
	add.s64 	%rd56, %rd74, %rd16;
	ld.global.nc.v2.f32 	{%f105, %f106}, [%rd56];
	fma.rn.f32 	%f109, %f101, %f105, %f389;
	fma.rn.f32 	%f110, %f102, %f106, %f109;
	add.s64 	%rd57, %rd74, %rd15;
	ld.global.nc.v2.f32 	{%f111, %f112}, [%rd57];
	fma.rn.f32 	%f115, %f101, %f111, %f388;
	fma.rn.f32 	%f116, %f102, %f112, %f115;
	add.s64 	%rd58, %rd74, %rd8;
	ld.global.nc.v2.f32 	{%f117, %f118}, [%rd58];
	fma.rn.f32 	%f121, %f101, %f117, %f387;
	fma.rn.f32 	%f122, %f102, %f118, %f121;
	add.s64 	%rd59, %rd74, %rd9;
	ld.global.nc.v2.f32 	{%f123, %f124}, [%rd59];
	fma.rn.f32 	%f127, %f101, %f123, %f386;
	fma.rn.f32 	%f128, %f102, %f124, %f127;
	add.s64 	%rd60, %rd74, %rd10;
	ld.global.nc.v2.f32 	{%f129, %f130}, [%rd60];
	fma.rn.f32 	%f133, %f101, %f129, %f385;
	fma.rn.f32 	%f134, %f102, %f130, %f133;
	add.s64 	%rd61, %rd74, %rd11;
	ld.global.nc.v2.f32 	{%f135, %f136}, [%rd61];
	fma.rn.f32 	%f139, %f101, %f135, %f384;
	fma.rn.f32 	%f140, %f102, %f136, %f139;
	add.s64 	%rd62, %rd74, %rd12;
	ld.global.nc.v2.f32 	{%f141, %f142}, [%rd62];
	fma.rn.f32 	%f145, %f101, %f141, %f383;
	fma.rn.f32 	%f146, %f102, %f142, %f145;
	add.s64 	%rd63, %rd74, %rd13;
	ld.global.nc.v2.f32 	{%f147, %f148}, [%rd63];
	fma.rn.f32 	%f151, %f101, %f147, %f382;
	fma.rn.f32 	%f152, %f102, %f148, %f151;
	ld.global.nc.v2.f32 	{%f153, %f154}, [%rd73];
	ld.global.nc.v2.f32 	{%f157, %f158}, [%rd56+1024];
	fma.rn.f32 	%f161, %f153, %f157, %f110;
	fma.rn.f32 	%f389, %f154, %f158, %f161;
	add.s64 	%rd64, %rd74, %rd7;
	ld.global.nc.v2.f32 	{%f162, %f163}, [%rd64];
	fma.rn.f32 	%f166, %f153, %f162, %f116;
	fma.rn.f32 	%f388, %f154, %f163, %f166;
	ld.global.nc.v2.f32 	{%f167, %f168}, [%rd58+1024];
	fma.rn.f32 	%f171, %f153, %f167, %f122;
	fma.rn.f32 	%f387, %f154, %f168, %f171;
	ld.global.nc.v2.f32 	{%f172, %f173}, [%rd59+1024];
	fma.rn.f32 	%f176, %f153, %f172, %f128;
	fma.rn.f32 	%f386, %f154, %f173, %f176;
	ld.global.nc.v2.f32 	{%f177, %f178}, [%rd60+1024];
	fma.rn.f32 	%f181, %f153, %f177, %f134;
	fma.rn.f32 	%f385, %f154, %f178, %f181;
	ld.global.nc.v2.f32 	{%f182, %f183}, [%rd61+1024];
	fma.rn.f32 	%f186, %f153, %f182, %f140;
	fma.rn.f32 	%f384, %f154, %f183, %f186;
	ld.global.nc.v2.f32 	{%f187, %f188}, [%rd62+1024];
	fma.rn.f32 	%f191, %f153, %f187, %f146;
	fma.rn.f32 	%f383, %f154, %f188, %f191;
	ld.global.nc.v2.f32 	{%f192, %f193}, [%rd63+1024];
	fma.rn.f32 	%f196, %f153, %f192, %f152;
	fma.rn.f32 	%f382, %f154, %f193, %f196;
	add.s64 	%rd74, %rd74, 2048;
	add.s64 	%rd73, %rd73, 2048;
	add.s32 	%r322, %r322, 256;
	setp.lt.s32 	%p5, %r322, %r11;
	@%p5 bra 	$L__BB31_7;

	st.local.v4.f32 	[%rd2], {%f389, %f388, %f387, %f386};
	st.local.v4.f32 	[%rd2+16], {%f385, %f384, %f383, %f382};

$L__BB31_9:
	shr.s32 	%r41, %r3, 31;
	shr.u32 	%r42, %r41, 27;
	add.s32 	%r43, %r3, %r42;
	shr.s32 	%r44, %r43, 5;
	shl.b32 	%r45, %r44, 2;
	add.s32 	%r10, %r24, %r45;
	mov.u32 	%r47, 2;
	mov.b32 	%r48, %f389;
	mov.u32 	%r49, 31;
	mov.u32 	%r50, 16;
	mov.u32 	%r51, -1;
	shfl.sync.bfly.b32 	%r52|%p6, %r48, %r50, %r49, %r51;
	mov.b32 	%f197, %r52;
	add.f32 	%f198, %f389, %f197;
	mov.b32 	%r53, %f198;
	mov.u32 	%r54, 8;
	shfl.sync.bfly.b32 	%r55|%p7, %r53, %r54, %r49, %r51;
	mov.b32 	%f199, %r55;
	add.f32 	%f200, %f198, %f199;
	mov.b32 	%r56, %f200;
	mov.u32 	%r57, 4;
	shfl.sync.bfly.b32 	%r58|%p8, %r56, %r57, %r49, %r51;
	mov.b32 	%f201, %r58;
	add.f32 	%f202, %f200, %f201;
	mov.b32 	%r59, %f202;
	shfl.sync.bfly.b32 	%r60|%p9, %r59, %r47, %r49, %r51;
	mov.b32 	%f203, %r60;
	add.f32 	%f204, %f202, %f203;
	mov.b32 	%r61, %f204;
	mov.u32 	%r62, 1;
	shfl.sync.bfly.b32 	%r63|%p10, %r61, %r62, %r49, %r51;
	mov.b32 	%f205, %r63;
	add.f32 	%f206, %f204, %f205;
	st.local.f32 	[%rd2], %f206;
	st.shared.f32 	[%r10], %f206;
	bar.sync 	0;
	@%p1 bra 	$L__BB31_11;

	ld.shared.f32 	%f207, [%r4];
	mov.b32 	%r64, %f207;
	shfl.sync.bfly.b32 	%r68|%p12, %r64, %r50, %r49, %r51;
	mov.b32 	%f208, %r68;
	add.f32 	%f209, %f207, %f208;
	mov.b32 	%r69, %f209;
	shfl.sync.bfly.b32 	%r71|%p13, %r69, %r54, %r49, %r51;
	mov.b32 	%f210, %r71;
	add.f32 	%f211, %f209, %f210;
	mov.b32 	%r72, %f211;
	shfl.sync.bfly.b32 	%r74|%p14, %r72, %r57, %r49, %r51;
	mov.b32 	%f212, %r74;
	add.f32 	%f213, %f211, %f212;
	mov.b32 	%r75, %f213;
	shfl.sync.bfly.b32 	%r77|%p15, %r75, %r47, %r49, %r51;
	mov.b32 	%f214, %r77;
	add.f32 	%f215, %f213, %f214;
	mov.b32 	%r78, %f215;
	shfl.sync.bfly.b32 	%r80|%p16, %r78, %r62, %r49, %r51;
	mov.b32 	%f216, %r80;
	add.f32 	%f217, %f215, %f216;
	st.local.f32 	[%rd2], %f217;

$L__BB31_11:
	bar.sync 	0;
	mov.b32 	%r81, %f388;
	shfl.sync.bfly.b32 	%r85|%p18, %r81, %r50, %r49, %r51;
	mov.b32 	%f218, %r85;
	add.f32 	%f219, %f388, %f218;
	mov.b32 	%r86, %f219;
	shfl.sync.bfly.b32 	%r88|%p19, %r86, %r54, %r49, %r51;
	mov.b32 	%f220, %r88;
	add.f32 	%f221, %f219, %f220;
	mov.b32 	%r89, %f221;
	shfl.sync.bfly.b32 	%r91|%p20, %r89, %r57, %r49, %r51;
	mov.b32 	%f222, %r91;
	add.f32 	%f223, %f221, %f222;
	mov.b32 	%r92, %f223;
	shfl.sync.bfly.b32 	%r94|%p21, %r92, %r47, %r49, %r51;
	mov.b32 	%f224, %r94;
	add.f32 	%f225, %f223, %f224;
	mov.b32 	%r95, %f225;
	shfl.sync.bfly.b32 	%r97|%p22, %r95, %r62, %r49, %r51;
	mov.b32 	%f226, %r97;
	add.f32 	%f227, %f225, %f226;
	st.local.f32 	[%rd2+4], %f227;
	st.shared.f32 	[%r10], %f227;
	bar.sync 	0;
	@%p1 bra 	$L__BB31_13;

	ld.shared.f32 	%f228, [%r4];
	mov.b32 	%r98, %f228;
	mov.u32 	%r99, 31;
	mov.u32 	%r100, 16;
	mov.u32 	%r101, -1;
	shfl.sync.bfly.b32 	%r102|%p23, %r98, %r100, %r99, %r101;
	mov.b32 	%f229, %r102;
	add.f32 	%f230, %f228, %f229;
	mov.b32 	%r103, %f230;
	mov.u32 	%r104, 8;
	shfl.sync.bfly.b32 	%r105|%p24, %r103, %r104, %r99, %r101;
	mov.b32 	%f231, %r105;
	add.f32 	%f232, %f230, %f231;
	mov.b32 	%r106, %f232;
	mov.u32 	%r107, 4;
	shfl.sync.bfly.b32 	%r108|%p25, %r106, %r107, %r99, %r101;
	mov.b32 	%f233, %r108;
	add.f32 	%f234, %f232, %f233;
	mov.b32 	%r109, %f234;
	mov.u32 	%r110, 2;
	shfl.sync.bfly.b32 	%r111|%p26, %r109, %r110, %r99, %r101;
	mov.b32 	%f235, %r111;
	add.f32 	%f236, %f234, %f235;
	mov.b32 	%r112, %f236;
	mov.u32 	%r113, 1;
	shfl.sync.bfly.b32 	%r114|%p27, %r112, %r113, %r99, %r101;
	mov.b32 	%f237, %r114;
	add.f32 	%f238, %f236, %f237;
	st.local.f32 	[%rd2+4], %f238;

$L__BB31_13:
	bar.sync 	0;
	mov.b32 	%r115, %f387;
	mov.u32 	%r116, 31;
	mov.u32 	%r117, 16;
	mov.u32 	%r118, -1;
	shfl.sync.bfly.b32 	%r119|%p29, %r115, %r117, %r116, %r118;
	mov.b32 	%f239, %r119;
	add.f32 	%f240, %f387, %f239;
	mov.b32 	%r120, %f240;
	mov.u32 	%r121, 8;
	shfl.sync.bfly.b32 	%r122|%p30, %r120, %r121, %r116, %r118;
	mov.b32 	%f241, %r122;
	add.f32 	%f242, %f240, %f241;
	mov.b32 	%r123, %f242;
	mov.u32 	%r124, 4;
	shfl.sync.bfly.b32 	%r125|%p31, %r123, %r124, %r116, %r118;
	mov.b32 	%f243, %r125;
	add.f32 	%f244, %f242, %f243;
	mov.b32 	%r126, %f244;
	mov.u32 	%r127, 2;
	shfl.sync.bfly.b32 	%r128|%p32, %r126, %r127, %r116, %r118;
	mov.b32 	%f245, %r128;
	add.f32 	%f246, %f244, %f245;
	mov.b32 	%r129, %f246;
	mov.u32 	%r130, 1;
	shfl.sync.bfly.b32 	%r131|%p33, %r129, %r130, %r116, %r118;
	mov.b32 	%f247, %r131;
	add.f32 	%f248, %f246, %f247;
	st.local.f32 	[%rd2+8], %f248;
	st.shared.f32 	[%r10], %f248;
	bar.sync 	0;
	@%p1 bra 	$L__BB31_15;

	ld.shared.f32 	%f249, [%r4];
	mov.b32 	%r132, %f249;
	shfl.sync.bfly.b32 	%r136|%p34, %r132, %r117, %r116, %r118;
	mov.b32 	%f250, %r136;
	add.f32 	%f251, %f249, %f250;
	mov.b32 	%r137, %f251;
	shfl.sync.bfly.b32 	%r139|%p35, %r137, %r121, %r116, %r118;
	mov.b32 	%f252, %r139;
	add.f32 	%f253, %f251, %f252;
	mov.b32 	%r140, %f253;
	shfl.sync.bfly.b32 	%r142|%p36, %r140, %r124, %r116, %r118;
	mov.b32 	%f254, %r142;
	add.f32 	%f255, %f253, %f254;
	mov.b32 	%r143, %f255;
	shfl.sync.bfly.b32 	%r145|%p37, %r143, %r127, %r116, %r118;
	mov.b32 	%f256, %r145;
	add.f32 	%f257, %f255, %f256;
	mov.b32 	%r146, %f257;
	shfl.sync.bfly.b32 	%r148|%p38, %r146, %r130, %r116, %r118;
	mov.b32 	%f258, %r148;
	add.f32 	%f259, %f257, %f258;
	st.local.f32 	[%rd2+8], %f259;

$L__BB31_15:
	bar.sync 	0;
	mov.b32 	%r149, %f386;
	shfl.sync.bfly.b32 	%r153|%p40, %r149, %r117, %r116, %r118;
	mov.b32 	%f260, %r153;
	add.f32 	%f261, %f386, %f260;
	mov.b32 	%r154, %f261;
	shfl.sync.bfly.b32 	%r156|%p41, %r154, %r121, %r116, %r118;
	mov.b32 	%f262, %r156;
	add.f32 	%f263, %f261, %f262;
	mov.b32 	%r157, %f263;
	shfl.sync.bfly.b32 	%r159|%p42, %r157, %r124, %r116, %r118;
	mov.b32 	%f264, %r159;
	add.f32 	%f265, %f263, %f264;
	mov.b32 	%r160, %f265;
	shfl.sync.bfly.b32 	%r162|%p43, %r160, %r127, %r116, %r118;
	mov.b32 	%f266, %r162;
	add.f32 	%f267, %f265, %f266;
	mov.b32 	%r163, %f267;
	shfl.sync.bfly.b32 	%r165|%p44, %r163, %r130, %r116, %r118;
	mov.b32 	%f268, %r165;
	add.f32 	%f269, %f267, %f268;
	st.local.f32 	[%rd2+12], %f269;
	st.shared.f32 	[%r10], %f269;
	bar.sync 	0;
	@%p1 bra 	$L__BB31_17;

	ld.shared.f32 	%f270, [%r4];
	mov.b32 	%r166, %f270;
	mov.u32 	%r167, 31;
	mov.u32 	%r168, 16;
	mov.u32 	%r169, -1;
	shfl.sync.bfly.b32 	%r170|%p45, %r166, %r168, %r167, %r169;
	mov.b32 	%f271, %r170;
	add.f32 	%f272, %f270, %f271;
	mov.b32 	%r171, %f272;
	mov.u32 	%r172, 8;
	shfl.sync.bfly.b32 	%r173|%p46, %r171, %r172, %r167, %r169;
	mov.b32 	%f273, %r173;
	add.f32 	%f274, %f272, %f273;
	mov.b32 	%r174, %f274;
	mov.u32 	%r175, 4;
	shfl.sync.bfly.b32 	%r176|%p47, %r174, %r175, %r167, %r169;
	mov.b32 	%f275, %r176;
	add.f32 	%f276, %f274, %f275;
	mov.b32 	%r177, %f276;
	mov.u32 	%r178, 2;
	shfl.sync.bfly.b32 	%r179|%p48, %r177, %r178, %r167, %r169;
	mov.b32 	%f277, %r179;
	add.f32 	%f278, %f276, %f277;
	mov.b32 	%r180, %f278;
	mov.u32 	%r181, 1;
	shfl.sync.bfly.b32 	%r182|%p49, %r180, %r181, %r167, %r169;
	mov.b32 	%f279, %r182;
	add.f32 	%f280, %f278, %f279;
	st.local.f32 	[%rd2+12], %f280;

$L__BB31_17:
	bar.sync 	0;
	mov.b32 	%r183, %f385;
	mov.u32 	%r184, 31;
	mov.u32 	%r185, 16;
	mov.u32 	%r186, -1;
	shfl.sync.bfly.b32 	%r187|%p51, %r183, %r185, %r184, %r186;
	mov.b32 	%f281, %r187;
	add.f32 	%f282, %f385, %f281;
	mov.b32 	%r188, %f282;
	mov.u32 	%r189, 8;
	shfl.sync.bfly.b32 	%r190|%p52, %r188, %r189, %r184, %r186;
	mov.b32 	%f283, %r190;
	add.f32 	%f284, %f282, %f283;
	mov.b32 	%r191, %f284;
	mov.u32 	%r192, 4;
	shfl.sync.bfly.b32 	%r193|%p53, %r191, %r192, %r184, %r186;
	mov.b32 	%f285, %r193;
	add.f32 	%f286, %f284, %f285;
	mov.b32 	%r194, %f286;
	mov.u32 	%r195, 2;
	shfl.sync.bfly.b32 	%r196|%p54, %r194, %r195, %r184, %r186;
	mov.b32 	%f287, %r196;
	add.f32 	%f288, %f286, %f287;
	mov.b32 	%r197, %f288;
	mov.u32 	%r198, 1;
	shfl.sync.bfly.b32 	%r199|%p55, %r197, %r198, %r184, %r186;
	mov.b32 	%f289, %r199;
	add.f32 	%f290, %f288, %f289;
	st.local.f32 	[%rd2+16], %f290;
	st.shared.f32 	[%r10], %f290;
	bar.sync 	0;
	@%p1 bra 	$L__BB31_19;

	ld.shared.f32 	%f291, [%r4];
	mov.b32 	%r200, %f291;
	shfl.sync.bfly.b32 	%r204|%p56, %r200, %r185, %r184, %r186;
	mov.b32 	%f292, %r204;
	add.f32 	%f293, %f291, %f292;
	mov.b32 	%r205, %f293;
	shfl.sync.bfly.b32 	%r207|%p57, %r205, %r189, %r184, %r186;
	mov.b32 	%f294, %r207;
	add.f32 	%f295, %f293, %f294;
	mov.b32 	%r208, %f295;
	shfl.sync.bfly.b32 	%r210|%p58, %r208, %r192, %r184, %r186;
	mov.b32 	%f296, %r210;
	add.f32 	%f297, %f295, %f296;
	mov.b32 	%r211, %f297;
	shfl.sync.bfly.b32 	%r213|%p59, %r211, %r195, %r184, %r186;
	mov.b32 	%f298, %r213;
	add.f32 	%f299, %f297, %f298;
	mov.b32 	%r214, %f299;
	shfl.sync.bfly.b32 	%r216|%p60, %r214, %r198, %r184, %r186;
	mov.b32 	%f300, %r216;
	add.f32 	%f301, %f299, %f300;
	st.local.f32 	[%rd2+16], %f301;

$L__BB31_19:
	bar.sync 	0;
	mov.b32 	%r217, %f384;
	shfl.sync.bfly.b32 	%r221|%p62, %r217, %r185, %r184, %r186;
	mov.b32 	%f302, %r221;
	add.f32 	%f303, %f384, %f302;
	mov.b32 	%r222, %f303;
	shfl.sync.bfly.b32 	%r224|%p63, %r222, %r189, %r184, %r186;
	mov.b32 	%f304, %r224;
	add.f32 	%f305, %f303, %f304;
	mov.b32 	%r225, %f305;
	shfl.sync.bfly.b32 	%r227|%p64, %r225, %r192, %r184, %r186;
	mov.b32 	%f306, %r227;
	add.f32 	%f307, %f305, %f306;
	mov.b32 	%r228, %f307;
	shfl.sync.bfly.b32 	%r230|%p65, %r228, %r195, %r184, %r186;
	mov.b32 	%f308, %r230;
	add.f32 	%f309, %f307, %f308;
	mov.b32 	%r231, %f309;
	shfl.sync.bfly.b32 	%r233|%p66, %r231, %r198, %r184, %r186;
	mov.b32 	%f310, %r233;
	add.f32 	%f311, %f309, %f310;
	st.local.f32 	[%rd2+20], %f311;
	st.shared.f32 	[%r10], %f311;
	bar.sync 	0;
	@%p1 bra 	$L__BB31_21;

	ld.shared.f32 	%f312, [%r4];
	mov.b32 	%r234, %f312;
	mov.u32 	%r235, 31;
	mov.u32 	%r236, 16;
	mov.u32 	%r237, -1;
	shfl.sync.bfly.b32 	%r238|%p67, %r234, %r236, %r235, %r237;
	mov.b32 	%f313, %r238;
	add.f32 	%f314, %f312, %f313;
	mov.b32 	%r239, %f314;
	mov.u32 	%r240, 8;
	shfl.sync.bfly.b32 	%r241|%p68, %r239, %r240, %r235, %r237;
	mov.b32 	%f315, %r241;
	add.f32 	%f316, %f314, %f315;
	mov.b32 	%r242, %f316;
	mov.u32 	%r243, 4;
	shfl.sync.bfly.b32 	%r244|%p69, %r242, %r243, %r235, %r237;
	mov.b32 	%f317, %r244;
	add.f32 	%f318, %f316, %f317;
	mov.b32 	%r245, %f318;
	mov.u32 	%r246, 2;
	shfl.sync.bfly.b32 	%r247|%p70, %r245, %r246, %r235, %r237;
	mov.b32 	%f319, %r247;
	add.f32 	%f320, %f318, %f319;
	mov.b32 	%r248, %f320;
	mov.u32 	%r249, 1;
	shfl.sync.bfly.b32 	%r250|%p71, %r248, %r249, %r235, %r237;
	mov.b32 	%f321, %r250;
	add.f32 	%f322, %f320, %f321;
	st.local.f32 	[%rd2+20], %f322;

$L__BB31_21:
	bar.sync 	0;
	mov.b32 	%r251, %f383;
	mov.u32 	%r252, 31;
	mov.u32 	%r253, 16;
	mov.u32 	%r254, -1;
	shfl.sync.bfly.b32 	%r255|%p73, %r251, %r253, %r252, %r254;
	mov.b32 	%f323, %r255;
	add.f32 	%f324, %f383, %f323;
	mov.b32 	%r256, %f324;
	mov.u32 	%r257, 8;
	shfl.sync.bfly.b32 	%r258|%p74, %r256, %r257, %r252, %r254;
	mov.b32 	%f325, %r258;
	add.f32 	%f326, %f324, %f325;
	mov.b32 	%r259, %f326;
	mov.u32 	%r260, 4;
	shfl.sync.bfly.b32 	%r261|%p75, %r259, %r260, %r252, %r254;
	mov.b32 	%f327, %r261;
	add.f32 	%f328, %f326, %f327;
	mov.b32 	%r262, %f328;
	mov.u32 	%r263, 2;
	shfl.sync.bfly.b32 	%r264|%p76, %r262, %r263, %r252, %r254;
	mov.b32 	%f329, %r264;
	add.f32 	%f330, %f328, %f329;
	mov.b32 	%r265, %f330;
	mov.u32 	%r266, 1;
	shfl.sync.bfly.b32 	%r267|%p77, %r265, %r266, %r252, %r254;
	mov.b32 	%f331, %r267;
	add.f32 	%f332, %f330, %f331;
	st.local.f32 	[%rd2+24], %f332;
	st.shared.f32 	[%r10], %f332;
	bar.sync 	0;
	@%p1 bra 	$L__BB31_23;

	ld.shared.f32 	%f333, [%r4];
	mov.b32 	%r268, %f333;
	shfl.sync.bfly.b32 	%r272|%p78, %r268, %r253, %r252, %r254;
	mov.b32 	%f334, %r272;
	add.f32 	%f335, %f333, %f334;
	mov.b32 	%r273, %f335;
	shfl.sync.bfly.b32 	%r275|%p79, %r273, %r257, %r252, %r254;
	mov.b32 	%f336, %r275;
	add.f32 	%f337, %f335, %f336;
	mov.b32 	%r276, %f337;
	shfl.sync.bfly.b32 	%r278|%p80, %r276, %r260, %r252, %r254;
	mov.b32 	%f338, %r278;
	add.f32 	%f339, %f337, %f338;
	mov.b32 	%r279, %f339;
	shfl.sync.bfly.b32 	%r281|%p81, %r279, %r263, %r252, %r254;
	mov.b32 	%f340, %r281;
	add.f32 	%f341, %f339, %f340;
	mov.b32 	%r282, %f341;
	shfl.sync.bfly.b32 	%r284|%p82, %r282, %r266, %r252, %r254;
	mov.b32 	%f342, %r284;
	add.f32 	%f343, %f341, %f342;
	st.local.f32 	[%rd2+24], %f343;

$L__BB31_23:
	bar.sync 	0;
	mov.b32 	%r285, %f382;
	shfl.sync.bfly.b32 	%r289|%p84, %r285, %r253, %r252, %r254;
	mov.b32 	%f344, %r289;
	add.f32 	%f345, %f382, %f344;
	mov.b32 	%r290, %f345;
	shfl.sync.bfly.b32 	%r292|%p85, %r290, %r257, %r252, %r254;
	mov.b32 	%f346, %r292;
	add.f32 	%f347, %f345, %f346;
	mov.b32 	%r293, %f347;
	shfl.sync.bfly.b32 	%r295|%p86, %r293, %r260, %r252, %r254;
	mov.b32 	%f348, %r295;
	add.f32 	%f349, %f347, %f348;
	mov.b32 	%r296, %f349;
	shfl.sync.bfly.b32 	%r298|%p87, %r296, %r263, %r252, %r254;
	mov.b32 	%f350, %r298;
	add.f32 	%f351, %f349, %f350;
	mov.b32 	%r299, %f351;
	shfl.sync.bfly.b32 	%r301|%p88, %r299, %r266, %r252, %r254;
	mov.b32 	%f352, %r301;
	add.f32 	%f353, %f351, %f352;
	st.local.f32 	[%rd2+28], %f353;
	st.shared.f32 	[%r10], %f353;
	bar.sync 	0;
	@%p1 bra 	$L__BB31_25;

	ld.shared.f32 	%f354, [%r4];
	mov.b32 	%r302, %f354;
	mov.u32 	%r303, 31;
	mov.u32 	%r304, 16;
	mov.u32 	%r305, -1;
	shfl.sync.bfly.b32 	%r306|%p89, %r302, %r304, %r303, %r305;
	mov.b32 	%f355, %r306;
	add.f32 	%f356, %f354, %f355;
	mov.b32 	%r307, %f356;
	mov.u32 	%r308, 8;
	shfl.sync.bfly.b32 	%r309|%p90, %r307, %r308, %r303, %r305;
	mov.b32 	%f357, %r309;
	add.f32 	%f358, %f356, %f357;
	mov.b32 	%r310, %f358;
	mov.u32 	%r311, 4;
	shfl.sync.bfly.b32 	%r312|%p91, %r310, %r311, %r303, %r305;
	mov.b32 	%f359, %r312;
	add.f32 	%f360, %f358, %f359;
	mov.b32 	%r313, %f360;
	mov.u32 	%r314, 2;
	shfl.sync.bfly.b32 	%r315|%p92, %r313, %r314, %r303, %r305;
	mov.b32 	%f361, %r315;
	add.f32 	%f362, %f360, %f361;
	mov.b32 	%r316, %f362;
	mov.u32 	%r317, 1;
	shfl.sync.bfly.b32 	%r318|%p93, %r316, %r317, %r303, %r305;
	mov.b32 	%f363, %r318;
	add.f32 	%f364, %f362, %f363;
	st.local.f32 	[%rd2+28], %f364;

$L__BB31_25:
	bar.sync 	0;
	setp.gt.s32 	%p94, %r3, 7;
	@%p94 bra 	$L__BB31_27;

	mul.wide.s32 	%rd65, %r3, 4;
	add.s64 	%rd66, %rd2, %rd65;
	ld.local.f32 	%f365, [%rd66];
	mad.lo.s32 	%r319, %r3, %r13, %r2;
	cvt.s64.s32 	%rd67, %r319;
	mul.lo.s32 	%r320, %r1, %r14;
	cvt.s64.s32 	%rd68, %r320;
	add.s64 	%rd69, %rd68, %rd67;
	cvta.to.global.u64 	%rd70, %rd21;
	shl.b64 	%rd71, %rd69, 2;
	add.s64 	%rd72, %rd70, %rd71;
	st.global.f32 	[%rd72], %f365;

$L__BB31_27:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_1_bs_160
.visible .entry ggml_matvec_f32_ncols_1_bs_160(
	.param .u64 ggml_matvec_f32_ncols_1_bs_160_param_0,
	.param .u64 ggml_matvec_f32_ncols_1_bs_160_param_1,
	.param .u64 ggml_matvec_f32_ncols_1_bs_160_param_2,
	.param .u32 ggml_matvec_f32_ncols_1_bs_160_param_3,
	.param .u32 ggml_matvec_f32_ncols_1_bs_160_param_4,
	.param .u32 ggml_matvec_f32_ncols_1_bs_160_param_5,
	.param .u32 ggml_matvec_f32_ncols_1_bs_160_param_6,
	.param .u32 ggml_matvec_f32_ncols_1_bs_160_param_7,
	.param .u32 ggml_matvec_f32_ncols_1_bs_160_param_8,
	.param .u32 ggml_matvec_f32_ncols_1_bs_160_param_9,
	.param .u32 ggml_matvec_f32_ncols_1_bs_160_param_10,
	.param .u32 ggml_matvec_f32_ncols_1_bs_160_param_11
)
{
	.reg .pred 	%p<19>;
	.reg .f32 	%f<88>;
	.reg .b32 	%r<79>;
	.reg .b64 	%rd<44>;


	ld.param.u64 	%rd18, [ggml_matvec_f32_ncols_1_bs_160_param_0];
	ld.param.u64 	%rd19, [ggml_matvec_f32_ncols_1_bs_160_param_1];
	ld.param.u64 	%rd17, [ggml_matvec_f32_ncols_1_bs_160_param_2];
	ld.param.u32 	%r13, [ggml_matvec_f32_ncols_1_bs_160_param_3];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_1_bs_160_param_5];
	ld.param.u32 	%r14, [ggml_matvec_f32_ncols_1_bs_160_param_7];
	ld.param.u32 	%r17, [ggml_matvec_f32_ncols_1_bs_160_param_8];
	ld.param.u32 	%r18, [ggml_matvec_f32_ncols_1_bs_160_param_9];
	ld.param.u32 	%r19, [ggml_matvec_f32_ncols_1_bs_160_param_10];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_1_bs_160_param_11];
	cvta.to.global.u64 	%rd1, %rd19;
	cvta.to.global.u64 	%rd2, %rd18;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r20, %r1, %r17;
	mov.u32 	%r21, %ctaid.x;
	mul.lo.s32 	%r22, %r21, %r16;
	mad.lo.s32 	%r23, %r20, %r18, %r22;
	cvt.s64.s32 	%rd3, %r23;
	mul.lo.s32 	%r24, %r1, %r19;
	cvt.s64.s32 	%rd4, %r24;
	mov.u32 	%r2, %tid.x;
	setp.gt.s32 	%p1, %r2, 31;
	shl.b32 	%r25, %r2, 2;
	mov.u32 	%r26, data_mmv;
	add.s32 	%r3, %r26, %r25;
	@%p1 bra 	$L__BB32_2;

	mov.u32 	%r27, 0;
	st.shared.u32 	[%r3], %r27;

$L__BB32_2:
	bar.sync 	0;
	setp.ge.s32 	%p2, %r2, %r13;
	mov.f32 	%f86, 0f00000000;
	@%p2 bra 	$L__BB32_9;

	not.b32 	%r28, %r2;
	add.s32 	%r4, %r28, %r13;
	mul.wide.u32 	%rd20, %r4, -858993459;
	shr.u64 	%rd21, %rd20, 39;
	cvt.u32.u64 	%r29, %rd21;
	add.s32 	%r30, %r29, 1;
	and.b32  	%r76, %r30, 3;
	setp.eq.s32 	%p3, %r76, 0;
	mov.f32 	%f86, 0f00000000;
	mov.u32 	%r77, %r2;
	@%p3 bra 	$L__BB32_6;

	mul.wide.s32 	%rd22, %r2, 2;
	add.s64 	%rd23, %rd22, %rd4;
	shl.b64 	%rd24, %rd23, 2;
	add.s64 	%rd41, %rd1, %rd24;
	add.s64 	%rd25, %rd22, %rd3;
	shl.b64 	%rd26, %rd25, 2;
	add.s64 	%rd40, %rd2, %rd26;
	mov.f32 	%f86, 0f00000000;
	mov.u32 	%r77, %r2;

$L__BB32_5:
	.pragma "nounroll";
	ld.global.nc.v2.f32 	{%f15, %f16}, [%rd40];
	ld.global.nc.v2.f32 	{%f19, %f20}, [%rd41];
	fma.rn.f32 	%f23, %f15, %f19, %f86;
	fma.rn.f32 	%f86, %f16, %f20, %f23;
	add.s32 	%r77, %r77, 160;
	add.s64 	%rd41, %rd41, 1280;
	add.s64 	%rd40, %rd40, 1280;
	add.s32 	%r76, %r76, -1;
	setp.ne.s32 	%p4, %r76, 0;
	@%p4 bra 	$L__BB32_5;

$L__BB32_6:
	setp.lt.u32 	%p5, %r4, 480;
	@%p5 bra 	$L__BB32_9;

	mul.wide.s32 	%rd27, %r77, 2;
	add.s64 	%rd28, %rd27, %rd3;
	shl.b64 	%rd29, %rd28, 2;
	add.s64 	%rd30, %rd2, %rd29;
	add.s64 	%rd43, %rd30, 2560;
	add.s64 	%rd31, %rd27, %rd4;
	shl.b64 	%rd32, %rd31, 2;
	add.s64 	%rd33, %rd1, %rd32;
	add.s64 	%rd42, %rd33, 2560;

$L__BB32_8:
	ld.global.nc.v2.f32 	{%f24, %f25}, [%rd43+-2560];
	ld.global.nc.v2.f32 	{%f28, %f29}, [%rd42+-2560];
	fma.rn.f32 	%f32, %f24, %f28, %f86;
	fma.rn.f32 	%f33, %f25, %f29, %f32;
	ld.global.nc.v2.f32 	{%f34, %f35}, [%rd43+-1280];
	ld.global.nc.v2.f32 	{%f38, %f39}, [%rd42+-1280];
	fma.rn.f32 	%f42, %f34, %f38, %f33;
	fma.rn.f32 	%f43, %f35, %f39, %f42;
	ld.global.nc.v2.f32 	{%f44, %f45}, [%rd43];
	ld.global.nc.v2.f32 	{%f48, %f49}, [%rd42];
	fma.rn.f32 	%f52, %f44, %f48, %f43;
	fma.rn.f32 	%f53, %f45, %f49, %f52;
	ld.global.nc.v2.f32 	{%f54, %f55}, [%rd43+1280];
	ld.global.nc.v2.f32 	{%f58, %f59}, [%rd42+1280];
	fma.rn.f32 	%f62, %f54, %f58, %f53;
	fma.rn.f32 	%f86, %f55, %f59, %f62;
	add.s64 	%rd43, %rd43, 5120;
	add.s64 	%rd42, %rd42, 5120;
	add.s32 	%r77, %r77, 640;
	setp.lt.s32 	%p6, %r77, %r13;
	@%p6 bra 	$L__BB32_8;

$L__BB32_9:
	mov.b32 	%r31, %f86;
	mov.u32 	%r32, 31;
	mov.u32 	%r33, 16;
	mov.u32 	%r34, -1;
	shfl.sync.bfly.b32 	%r35|%p7, %r31, %r33, %r32, %r34;
	mov.b32 	%f63, %r35;
	add.f32 	%f64, %f86, %f63;
	mov.b32 	%r36, %f64;
	mov.u32 	%r37, 8;
	shfl.sync.bfly.b32 	%r38|%p8, %r36, %r37, %r32, %r34;
	mov.b32 	%f65, %r38;
	add.f32 	%f66, %f64, %f65;
	mov.b32 	%r39, %f66;
	mov.u32 	%r40, 4;
	shfl.sync.bfly.b32 	%r41|%p9, %r39, %r40, %r32, %r34;
	mov.b32 	%f67, %r41;
	add.f32 	%f68, %f66, %f67;
	mov.b32 	%r42, %f68;
	mov.u32 	%r43, 2;
	shfl.sync.bfly.b32 	%r44|%p10, %r42, %r43, %r32, %r34;
	mov.b32 	%f69, %r44;
	add.f32 	%f70, %f68, %f69;
	mov.b32 	%r45, %f70;
	mov.u32 	%r46, 1;
	shfl.sync.bfly.b32 	%r47|%p11, %r45, %r46, %r32, %r34;
	mov.b32 	%f71, %r47;
	add.f32 	%f87, %f70, %f71;
	shr.s32 	%r48, %r2, 31;
	shr.u32 	%r49, %r48, 27;
	add.s32 	%r50, %r2, %r49;
	shr.s32 	%r51, %r50, 5;
	shl.b32 	%r52, %r51, 2;
	add.s32 	%r54, %r26, %r52;
	st.shared.f32 	[%r54], %f87;
	bar.sync 	0;
	@%p1 bra 	$L__BB32_11;

	ld.shared.f32 	%f72, [%r3];
	mov.b32 	%r55, %f72;
	shfl.sync.bfly.b32 	%r59|%p13, %r55, %r33, %r32, %r34;
	mov.b32 	%f73, %r59;
	add.f32 	%f74, %f72, %f73;
	mov.b32 	%r60, %f74;
	shfl.sync.bfly.b32 	%r62|%p14, %r60, %r37, %r32, %r34;
	mov.b32 	%f75, %r62;
	add.f32 	%f76, %f74, %f75;
	mov.b32 	%r63, %f76;
	shfl.sync.bfly.b32 	%r65|%p15, %r63, %r40, %r32, %r34;
	mov.b32 	%f77, %r65;
	add.f32 	%f78, %f76, %f77;
	mov.b32 	%r66, %f78;
	shfl.sync.bfly.b32 	%r68|%p16, %r66, %r43, %r32, %r34;
	mov.b32 	%f79, %r68;
	add.f32 	%f80, %f78, %f79;
	mov.b32 	%r69, %f80;
	shfl.sync.bfly.b32 	%r71|%p17, %r69, %r46, %r32, %r34;
	mov.b32 	%f81, %r71;
	add.f32 	%f87, %f80, %f81;

$L__BB32_11:
	bar.sync 	0;
	setp.gt.s32 	%p18, %r2, 0;
	@%p18 bra 	$L__BB32_13;

	mad.lo.s32 	%r73, %r2, %r14, %r21;
	cvt.s64.s32 	%rd34, %r73;
	mul.lo.s32 	%r74, %r1, %r15;
	cvt.s64.s32 	%rd35, %r74;
	add.s64 	%rd36, %rd35, %rd34;
	cvta.to.global.u64 	%rd37, %rd17;
	shl.b64 	%rd38, %rd36, 2;
	add.s64 	%rd39, %rd37, %rd38;
	st.global.f32 	[%rd39], %f87;

$L__BB32_13:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_2_bs_160
.visible .entry ggml_matvec_f32_ncols_2_bs_160(
	.param .u64 ggml_matvec_f32_ncols_2_bs_160_param_0,
	.param .u64 ggml_matvec_f32_ncols_2_bs_160_param_1,
	.param .u64 ggml_matvec_f32_ncols_2_bs_160_param_2,
	.param .u32 ggml_matvec_f32_ncols_2_bs_160_param_3,
	.param .u32 ggml_matvec_f32_ncols_2_bs_160_param_4,
	.param .u32 ggml_matvec_f32_ncols_2_bs_160_param_5,
	.param .u32 ggml_matvec_f32_ncols_2_bs_160_param_6,
	.param .u32 ggml_matvec_f32_ncols_2_bs_160_param_7,
	.param .u32 ggml_matvec_f32_ncols_2_bs_160_param_8,
	.param .u32 ggml_matvec_f32_ncols_2_bs_160_param_9,
	.param .u32 ggml_matvec_f32_ncols_2_bs_160_param_10,
	.param .u32 ggml_matvec_f32_ncols_2_bs_160_param_11
)
{
	.local .align 8 .b8 	__local_depot33[8];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<30>;
	.reg .f32 	%f<146>;
	.reg .b32 	%r<113>;
	.reg .b64 	%rd<66>;


	mov.u64 	%SPL, __local_depot33;
	ld.param.u64 	%rd27, [ggml_matvec_f32_ncols_2_bs_160_param_0];
	ld.param.u64 	%rd28, [ggml_matvec_f32_ncols_2_bs_160_param_1];
	ld.param.u64 	%rd26, [ggml_matvec_f32_ncols_2_bs_160_param_2];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_2_bs_160_param_3];
	ld.param.u32 	%r19, [ggml_matvec_f32_ncols_2_bs_160_param_5];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_2_bs_160_param_6];
	ld.param.u32 	%r17, [ggml_matvec_f32_ncols_2_bs_160_param_7];
	ld.param.u32 	%r20, [ggml_matvec_f32_ncols_2_bs_160_param_8];
	ld.param.u32 	%r21, [ggml_matvec_f32_ncols_2_bs_160_param_9];
	ld.param.u32 	%r22, [ggml_matvec_f32_ncols_2_bs_160_param_10];
	ld.param.u32 	%r18, [ggml_matvec_f32_ncols_2_bs_160_param_11];
	cvta.to.global.u64 	%rd1, %rd28;
	cvta.to.global.u64 	%rd2, %rd27;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r23, %r1, %r20;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r24, %r2, %r19;
	mad.lo.s32 	%r25, %r23, %r21, %r24;
	cvt.s64.s32 	%rd4, %r25;
	mul.lo.s32 	%r26, %r1, %r22;
	cvt.s64.s32 	%rd5, %r26;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r27, %r3, 2;
	mov.u32 	%r28, data_mmv;
	add.s32 	%r4, %r28, %r27;
	@%p1 bra 	$L__BB33_2;

	mov.u32 	%r29, 0;
	st.shared.u32 	[%r4], %r29;

$L__BB33_2:
	bar.sync 	0;
	mov.f32 	%f144, 0f00000000;
	st.local.v2.f32 	[%rd3], {%f144, %f144};
	setp.ge.s32 	%p2, %r3, %r15;
	mov.f32 	%f145, %f144;
	@%p2 bra 	$L__BB33_11;

	not.b32 	%r30, %r3;
	add.s32 	%r5, %r30, %r15;
	mul.wide.u32 	%rd30, %r5, -858993459;
	shr.u64 	%rd31, %rd30, 39;
	cvt.u32.u64 	%r31, %rd31;
	add.s32 	%r32, %r31, 1;
	and.b32  	%r110, %r32, 3;
	setp.eq.s32 	%p3, %r110, 0;
	mov.f32 	%f144, 0f00000000;
	mov.u32 	%r111, %r3;
	@%p3 bra 	$L__BB33_7;

	mul.wide.s32 	%rd32, %r16, 2;
	mul.wide.s32 	%rd33, %r3, 2;
	add.s64 	%rd34, %rd32, %rd33;
	add.s64 	%rd35, %rd34, %rd5;
	shl.b64 	%rd36, %rd35, 2;
	add.s64 	%rd62, %rd1, %rd36;
	add.s64 	%rd37, %rd33, %rd5;
	shl.b64 	%rd38, %rd37, 2;
	add.s64 	%rd61, %rd1, %rd38;
	add.s64 	%rd39, %rd33, %rd4;
	shl.b64 	%rd40, %rd39, 2;
	add.s64 	%rd60, %rd2, %rd40;
	mov.f32 	%f144, 0f00000000;
	mov.f32 	%f145, %f144;
	mov.u32 	%r111, %r3;

$L__BB33_5:
	.pragma "nounroll";
	ld.global.nc.v2.f32 	{%f19, %f20}, [%rd60];
	ld.global.nc.v2.f32 	{%f23, %f24}, [%rd61];
	fma.rn.f32 	%f27, %f19, %f23, %f145;
	fma.rn.f32 	%f145, %f20, %f24, %f27;
	ld.global.nc.v2.f32 	{%f28, %f29}, [%rd62];
	fma.rn.f32 	%f32, %f19, %f28, %f144;
	fma.rn.f32 	%f144, %f20, %f29, %f32;
	add.s32 	%r111, %r111, 160;
	add.s64 	%rd62, %rd62, 1280;
	add.s64 	%rd61, %rd61, 1280;
	add.s64 	%rd60, %rd60, 1280;
	add.s32 	%r110, %r110, -1;
	setp.ne.s32 	%p4, %r110, 0;
	@%p4 bra 	$L__BB33_5;

	st.local.v2.f32 	[%rd3], {%f145, %f144};

$L__BB33_7:
	setp.lt.u32 	%p5, %r5, 480;
	@%p5 bra 	$L__BB33_11;

	mul.wide.s32 	%rd41, %r111, 2;
	add.s64 	%rd42, %rd41, %rd4;
	shl.b64 	%rd43, %rd42, 2;
	add.s64 	%rd44, %rd2, %rd43;
	add.s64 	%rd65, %rd44, 2560;
	add.s64 	%rd45, %rd41, %rd5;
	shl.b64 	%rd46, %rd45, 2;
	add.s64 	%rd47, %rd1, %rd46;
	add.s64 	%rd64, %rd47, 3840;
	mul.wide.s32 	%rd48, %r16, 2;
	add.s64 	%rd49, %rd45, %rd48;
	shl.b64 	%rd50, %rd49, 2;
	add.s64 	%rd51, %rd1, %rd50;
	add.s64 	%rd63, %rd51, 2560;

$L__BB33_9:
	ld.global.nc.v2.f32 	{%f33, %f34}, [%rd65+-2560];
	ld.global.nc.v2.f32 	{%f37, %f38}, [%rd64+-3840];
	fma.rn.f32 	%f41, %f33, %f37, %f145;
	fma.rn.f32 	%f42, %f34, %f38, %f41;
	ld.global.nc.v2.f32 	{%f43, %f44}, [%rd63+-2560];
	fma.rn.f32 	%f47, %f33, %f43, %f144;
	fma.rn.f32 	%f48, %f34, %f44, %f47;
	ld.global.nc.v2.f32 	{%f49, %f50}, [%rd65+-1280];
	ld.global.nc.v2.f32 	{%f53, %f54}, [%rd64+-2560];
	fma.rn.f32 	%f57, %f49, %f53, %f42;
	fma.rn.f32 	%f58, %f50, %f54, %f57;
	ld.global.nc.v2.f32 	{%f59, %f60}, [%rd63+-1280];
	fma.rn.f32 	%f63, %f49, %f59, %f48;
	fma.rn.f32 	%f64, %f50, %f60, %f63;
	ld.global.nc.v2.f32 	{%f65, %f66}, [%rd65];
	ld.global.nc.v2.f32 	{%f69, %f70}, [%rd64+-1280];
	fma.rn.f32 	%f73, %f65, %f69, %f58;
	fma.rn.f32 	%f74, %f66, %f70, %f73;
	ld.global.nc.v2.f32 	{%f75, %f76}, [%rd63];
	fma.rn.f32 	%f79, %f65, %f75, %f64;
	fma.rn.f32 	%f80, %f66, %f76, %f79;
	ld.global.nc.v2.f32 	{%f81, %f82}, [%rd65+1280];
	ld.global.nc.v2.f32 	{%f85, %f86}, [%rd64];
	fma.rn.f32 	%f89, %f81, %f85, %f74;
	fma.rn.f32 	%f145, %f82, %f86, %f89;
	ld.global.nc.v2.f32 	{%f90, %f91}, [%rd63+1280];
	fma.rn.f32 	%f94, %f81, %f90, %f80;
	fma.rn.f32 	%f144, %f82, %f91, %f94;
	add.s64 	%rd65, %rd65, 5120;
	add.s64 	%rd64, %rd64, 5120;
	add.s64 	%rd63, %rd63, 5120;
	add.s32 	%r111, %r111, 640;
	setp.lt.s32 	%p6, %r111, %r15;
	@%p6 bra 	$L__BB33_9;

	st.local.v2.f32 	[%rd3], {%f145, %f144};

$L__BB33_11:
	shr.s32 	%r33, %r3, 31;
	shr.u32 	%r34, %r33, 27;
	add.s32 	%r35, %r3, %r34;
	shr.s32 	%r36, %r35, 5;
	shl.b32 	%r37, %r36, 2;
	add.s32 	%r14, %r28, %r37;
	mov.u32 	%r39, 2;
	mov.b32 	%r40, %f145;
	mov.u32 	%r41, 31;
	mov.u32 	%r42, 16;
	mov.u32 	%r43, -1;
	shfl.sync.bfly.b32 	%r44|%p7, %r40, %r42, %r41, %r43;
	mov.b32 	%f95, %r44;
	add.f32 	%f96, %f145, %f95;
	mov.b32 	%r45, %f96;
	mov.u32 	%r46, 8;
	shfl.sync.bfly.b32 	%r47|%p8, %r45, %r46, %r41, %r43;
	mov.b32 	%f97, %r47;
	add.f32 	%f98, %f96, %f97;
	mov.b32 	%r48, %f98;
	mov.u32 	%r49, 4;
	shfl.sync.bfly.b32 	%r50|%p9, %r48, %r49, %r41, %r43;
	mov.b32 	%f99, %r50;
	add.f32 	%f100, %f98, %f99;
	mov.b32 	%r51, %f100;
	shfl.sync.bfly.b32 	%r52|%p10, %r51, %r39, %r41, %r43;
	mov.b32 	%f101, %r52;
	add.f32 	%f102, %f100, %f101;
	mov.b32 	%r53, %f102;
	mov.u32 	%r54, 1;
	shfl.sync.bfly.b32 	%r55|%p11, %r53, %r54, %r41, %r43;
	mov.b32 	%f103, %r55;
	add.f32 	%f104, %f102, %f103;
	st.local.f32 	[%rd3], %f104;
	st.shared.f32 	[%r14], %f104;
	bar.sync 	0;
	@%p1 bra 	$L__BB33_13;

	ld.shared.f32 	%f105, [%r4];
	mov.b32 	%r56, %f105;
	shfl.sync.bfly.b32 	%r60|%p13, %r56, %r42, %r41, %r43;
	mov.b32 	%f106, %r60;
	add.f32 	%f107, %f105, %f106;
	mov.b32 	%r61, %f107;
	shfl.sync.bfly.b32 	%r63|%p14, %r61, %r46, %r41, %r43;
	mov.b32 	%f108, %r63;
	add.f32 	%f109, %f107, %f108;
	mov.b32 	%r64, %f109;
	shfl.sync.bfly.b32 	%r66|%p15, %r64, %r49, %r41, %r43;
	mov.b32 	%f110, %r66;
	add.f32 	%f111, %f109, %f110;
	mov.b32 	%r67, %f111;
	shfl.sync.bfly.b32 	%r69|%p16, %r67, %r39, %r41, %r43;
	mov.b32 	%f112, %r69;
	add.f32 	%f113, %f111, %f112;
	mov.b32 	%r70, %f113;
	shfl.sync.bfly.b32 	%r72|%p17, %r70, %r54, %r41, %r43;
	mov.b32 	%f114, %r72;
	add.f32 	%f115, %f113, %f114;
	st.local.f32 	[%rd3], %f115;

$L__BB33_13:
	bar.sync 	0;
	mov.b32 	%r73, %f144;
	shfl.sync.bfly.b32 	%r77|%p19, %r73, %r42, %r41, %r43;
	mov.b32 	%f116, %r77;
	add.f32 	%f117, %f144, %f116;
	mov.b32 	%r78, %f117;
	shfl.sync.bfly.b32 	%r80|%p20, %r78, %r46, %r41, %r43;
	mov.b32 	%f118, %r80;
	add.f32 	%f119, %f117, %f118;
	mov.b32 	%r81, %f119;
	shfl.sync.bfly.b32 	%r83|%p21, %r81, %r49, %r41, %r43;
	mov.b32 	%f120, %r83;
	add.f32 	%f121, %f119, %f120;
	mov.b32 	%r84, %f121;
	shfl.sync.bfly.b32 	%r86|%p22, %r84, %r39, %r41, %r43;
	mov.b32 	%f122, %r86;
	add.f32 	%f123, %f121, %f122;
	mov.b32 	%r87, %f123;
	shfl.sync.bfly.b32 	%r89|%p23, %r87, %r54, %r41, %r43;
	mov.b32 	%f124, %r89;
	add.f32 	%f125, %f123, %f124;
	st.local.f32 	[%rd3+4], %f125;
	st.shared.f32 	[%r14], %f125;
	bar.sync 	0;
	@%p1 bra 	$L__BB33_15;

	ld.shared.f32 	%f126, [%r4];
	mov.b32 	%r90, %f126;
	mov.u32 	%r91, 31;
	mov.u32 	%r92, 16;
	mov.u32 	%r93, -1;
	shfl.sync.bfly.b32 	%r94|%p24, %r90, %r92, %r91, %r93;
	mov.b32 	%f127, %r94;
	add.f32 	%f128, %f126, %f127;
	mov.b32 	%r95, %f128;
	mov.u32 	%r96, 8;
	shfl.sync.bfly.b32 	%r97|%p25, %r95, %r96, %r91, %r93;
	mov.b32 	%f129, %r97;
	add.f32 	%f130, %f128, %f129;
	mov.b32 	%r98, %f130;
	mov.u32 	%r99, 4;
	shfl.sync.bfly.b32 	%r100|%p26, %r98, %r99, %r91, %r93;
	mov.b32 	%f131, %r100;
	add.f32 	%f132, %f130, %f131;
	mov.b32 	%r101, %f132;
	mov.u32 	%r102, 2;
	shfl.sync.bfly.b32 	%r103|%p27, %r101, %r102, %r91, %r93;
	mov.b32 	%f133, %r103;
	add.f32 	%f134, %f132, %f133;
	mov.b32 	%r104, %f134;
	mov.u32 	%r105, 1;
	shfl.sync.bfly.b32 	%r106|%p28, %r104, %r105, %r91, %r93;
	mov.b32 	%f135, %r106;
	add.f32 	%f136, %f134, %f135;
	st.local.f32 	[%rd3+4], %f136;

$L__BB33_15:
	bar.sync 	0;
	setp.gt.s32 	%p29, %r3, 1;
	@%p29 bra 	$L__BB33_17;

	mul.wide.s32 	%rd52, %r3, 4;
	add.s64 	%rd53, %rd3, %rd52;
	ld.local.f32 	%f137, [%rd53];
	mad.lo.s32 	%r107, %r3, %r17, %r2;
	cvt.s64.s32 	%rd54, %r107;
	mul.lo.s32 	%r108, %r1, %r18;
	cvt.s64.s32 	%rd55, %r108;
	add.s64 	%rd56, %rd55, %rd54;
	cvta.to.global.u64 	%rd57, %rd26;
	shl.b64 	%rd58, %rd56, 2;
	add.s64 	%rd59, %rd57, %rd58;
	st.global.f32 	[%rd59], %f137;

$L__BB33_17:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_3_bs_160
.visible .entry ggml_matvec_f32_ncols_3_bs_160(
	.param .u64 ggml_matvec_f32_ncols_3_bs_160_param_0,
	.param .u64 ggml_matvec_f32_ncols_3_bs_160_param_1,
	.param .u64 ggml_matvec_f32_ncols_3_bs_160_param_2,
	.param .u32 ggml_matvec_f32_ncols_3_bs_160_param_3,
	.param .u32 ggml_matvec_f32_ncols_3_bs_160_param_4,
	.param .u32 ggml_matvec_f32_ncols_3_bs_160_param_5,
	.param .u32 ggml_matvec_f32_ncols_3_bs_160_param_6,
	.param .u32 ggml_matvec_f32_ncols_3_bs_160_param_7,
	.param .u32 ggml_matvec_f32_ncols_3_bs_160_param_8,
	.param .u32 ggml_matvec_f32_ncols_3_bs_160_param_9,
	.param .u32 ggml_matvec_f32_ncols_3_bs_160_param_10,
	.param .u32 ggml_matvec_f32_ncols_3_bs_160_param_11
)
{
	.local .align 4 .b8 	__local_depot34[12];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<41>;
	.reg .f32 	%f<208>;
	.reg .b32 	%r<154>;
	.reg .b64 	%rd<74>;


	mov.u64 	%SPL, __local_depot34;
	ld.param.u64 	%rd29, [ggml_matvec_f32_ncols_3_bs_160_param_0];
	ld.param.u64 	%rd30, [ggml_matvec_f32_ncols_3_bs_160_param_1];
	ld.param.u64 	%rd28, [ggml_matvec_f32_ncols_3_bs_160_param_2];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_3_bs_160_param_3];
	ld.param.u32 	%r19, [ggml_matvec_f32_ncols_3_bs_160_param_5];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_3_bs_160_param_6];
	ld.param.u32 	%r17, [ggml_matvec_f32_ncols_3_bs_160_param_7];
	ld.param.u32 	%r20, [ggml_matvec_f32_ncols_3_bs_160_param_8];
	ld.param.u32 	%r21, [ggml_matvec_f32_ncols_3_bs_160_param_9];
	ld.param.u32 	%r22, [ggml_matvec_f32_ncols_3_bs_160_param_10];
	ld.param.u32 	%r18, [ggml_matvec_f32_ncols_3_bs_160_param_11];
	cvta.to.global.u64 	%rd73, %rd30;
	cvta.to.global.u64 	%rd2, %rd29;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r23, %r1, %r20;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r24, %r2, %r19;
	mad.lo.s32 	%r25, %r23, %r21, %r24;
	cvt.s64.s32 	%rd4, %r25;
	mul.lo.s32 	%r26, %r1, %r22;
	cvt.s64.s32 	%rd5, %r26;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r27, %r3, 2;
	mov.u32 	%r28, data_mmv;
	add.s32 	%r4, %r28, %r27;
	@%p1 bra 	$L__BB34_2;

	mov.u32 	%r29, 0;
	st.shared.u32 	[%r4], %r29;

$L__BB34_2:
	bar.sync 	0;
	mov.f32 	%f205, 0f00000000;
	mov.u32 	%r30, 0;
	st.local.u32 	[%rd3], %r30;
	st.local.u32 	[%rd3+4], %r30;
	st.local.u32 	[%rd3+8], %r30;
	setp.ge.s32 	%p2, %r3, %r15;
	mov.f32 	%f206, %f205;
	mov.f32 	%f207, %f205;
	@%p2 bra 	$L__BB34_11;

	not.b32 	%r31, %r3;
	add.s32 	%r5, %r31, %r15;
	mul.wide.u32 	%rd32, %r5, -858993459;
	shr.u64 	%rd33, %rd32, 39;
	cvt.u32.u64 	%r32, %rd33;
	add.s32 	%r33, %r32, 1;
	and.b32  	%r151, %r33, 3;
	setp.eq.s32 	%p3, %r151, 0;
	mov.f32 	%f205, 0f00000000;
	mov.u32 	%r152, %r3;
	@%p3 bra 	$L__BB34_7;

	shl.b32 	%r34, %r16, 1;
	add.s32 	%r35, %r3, %r34;
	mul.wide.s32 	%rd34, %r35, 2;
	add.s64 	%rd35, %rd34, %rd5;
	shl.b64 	%rd36, %rd35, 2;
	add.s64 	%rd71, %rd73, %rd36;
	mul.wide.s32 	%rd37, %r16, 2;
	mul.wide.s32 	%rd38, %r3, 2;
	add.s64 	%rd39, %rd37, %rd38;
	add.s64 	%rd40, %rd39, %rd5;
	shl.b64 	%rd41, %rd40, 2;
	add.s64 	%rd70, %rd73, %rd41;
	add.s64 	%rd42, %rd38, %rd5;
	shl.b64 	%rd43, %rd42, 2;
	add.s64 	%rd69, %rd73, %rd43;
	add.s64 	%rd44, %rd38, %rd4;
	shl.b64 	%rd45, %rd44, 2;
	add.s64 	%rd68, %rd2, %rd45;
	mov.f32 	%f205, 0f00000000;
	mov.f32 	%f206, %f205;
	mov.f32 	%f207, %f205;
	mov.u32 	%r152, %r3;

$L__BB34_5:
	.pragma "nounroll";
	ld.global.nc.v2.f32 	{%f28, %f29}, [%rd68];
	ld.global.nc.v2.f32 	{%f32, %f33}, [%rd69];
	fma.rn.f32 	%f36, %f28, %f32, %f207;
	fma.rn.f32 	%f207, %f29, %f33, %f36;
	ld.global.nc.v2.f32 	{%f37, %f38}, [%rd70];
	fma.rn.f32 	%f41, %f28, %f37, %f206;
	fma.rn.f32 	%f206, %f29, %f38, %f41;
	ld.global.nc.v2.f32 	{%f42, %f43}, [%rd71];
	fma.rn.f32 	%f46, %f28, %f42, %f205;
	fma.rn.f32 	%f205, %f29, %f43, %f46;
	add.s32 	%r152, %r152, 160;
	add.s64 	%rd71, %rd71, 1280;
	add.s64 	%rd70, %rd70, 1280;
	add.s64 	%rd69, %rd69, 1280;
	add.s64 	%rd68, %rd68, 1280;
	add.s32 	%r151, %r151, -1;
	setp.ne.s32 	%p4, %r151, 0;
	@%p4 bra 	$L__BB34_5;

	st.local.f32 	[%rd3], %f207;
	st.local.f32 	[%rd3+4], %f206;
	st.local.f32 	[%rd3+8], %f205;

$L__BB34_7:
	setp.lt.u32 	%p5, %r5, 480;
	@%p5 bra 	$L__BB34_11;

	add.s32 	%r36, %r152, %r16;
	shl.b32 	%r37, %r16, 1;
	add.s32 	%r38, %r152, %r37;
	add.s32 	%r39, %r36, 160;
	mul.wide.s32 	%rd46, %r39, 8;
	shl.b64 	%rd47, %rd5, 2;
	add.s64 	%rd19, %rd46, %rd47;
	mul.wide.s32 	%rd48, %r38, 8;
	add.s64 	%rd20, %rd48, %rd47;
	mul.wide.s32 	%rd49, %r152, 2;
	add.s64 	%rd50, %rd49, %rd4;
	shl.b64 	%rd51, %rd50, 2;
	add.s64 	%rd52, %rd2, %rd51;
	add.s64 	%rd72, %rd52, 2560;
	mul.wide.s32 	%rd53, %r152, 8;
	add.s64 	%rd22, %rd53, %rd47;
	mul.wide.s32 	%rd54, %r16, 8;
	add.s64 	%rd55, %rd53, %rd54;
	add.s64 	%rd23, %rd55, %rd47;

$L__BB34_9:
	ld.global.nc.v2.f32 	{%f47, %f48}, [%rd72+-2560];
	add.s64 	%rd56, %rd73, %rd22;
	ld.global.nc.v2.f32 	{%f51, %f52}, [%rd56];
	fma.rn.f32 	%f55, %f47, %f51, %f207;
	fma.rn.f32 	%f56, %f48, %f52, %f55;
	add.s64 	%rd57, %rd73, %rd23;
	ld.global.nc.v2.f32 	{%f57, %f58}, [%rd57];
	fma.rn.f32 	%f61, %f47, %f57, %f206;
	fma.rn.f32 	%f62, %f48, %f58, %f61;
	add.s64 	%rd58, %rd73, %rd20;
	ld.global.nc.v2.f32 	{%f63, %f64}, [%rd58];
	fma.rn.f32 	%f67, %f47, %f63, %f205;
	fma.rn.f32 	%f68, %f48, %f64, %f67;
	ld.global.nc.v2.f32 	{%f69, %f70}, [%rd72+-1280];
	ld.global.nc.v2.f32 	{%f73, %f74}, [%rd56+1280];
	fma.rn.f32 	%f77, %f69, %f73, %f56;
	fma.rn.f32 	%f78, %f70, %f74, %f77;
	add.s64 	%rd59, %rd73, %rd19;
	ld.global.nc.v2.f32 	{%f79, %f80}, [%rd59];
	fma.rn.f32 	%f83, %f69, %f79, %f62;
	fma.rn.f32 	%f84, %f70, %f80, %f83;
	ld.global.nc.v2.f32 	{%f85, %f86}, [%rd58+1280];
	fma.rn.f32 	%f89, %f69, %f85, %f68;
	fma.rn.f32 	%f90, %f70, %f86, %f89;
	ld.global.nc.v2.f32 	{%f91, %f92}, [%rd72];
	ld.global.nc.v2.f32 	{%f95, %f96}, [%rd56+2560];
	fma.rn.f32 	%f99, %f91, %f95, %f78;
	fma.rn.f32 	%f100, %f92, %f96, %f99;
	ld.global.nc.v2.f32 	{%f101, %f102}, [%rd59+1280];
	fma.rn.f32 	%f105, %f91, %f101, %f84;
	fma.rn.f32 	%f106, %f92, %f102, %f105;
	ld.global.nc.v2.f32 	{%f107, %f108}, [%rd58+2560];
	fma.rn.f32 	%f111, %f91, %f107, %f90;
	fma.rn.f32 	%f112, %f92, %f108, %f111;
	ld.global.nc.v2.f32 	{%f113, %f114}, [%rd72+1280];
	ld.global.nc.v2.f32 	{%f117, %f118}, [%rd56+3840];
	fma.rn.f32 	%f121, %f113, %f117, %f100;
	fma.rn.f32 	%f207, %f114, %f118, %f121;
	ld.global.nc.v2.f32 	{%f122, %f123}, [%rd59+2560];
	fma.rn.f32 	%f126, %f113, %f122, %f106;
	fma.rn.f32 	%f206, %f114, %f123, %f126;
	ld.global.nc.v2.f32 	{%f127, %f128}, [%rd58+3840];
	fma.rn.f32 	%f131, %f113, %f127, %f112;
	fma.rn.f32 	%f205, %f114, %f128, %f131;
	add.s64 	%rd73, %rd73, 5120;
	add.s64 	%rd72, %rd72, 5120;
	add.s32 	%r152, %r152, 640;
	setp.lt.s32 	%p6, %r152, %r15;
	@%p6 bra 	$L__BB34_9;

	st.local.f32 	[%rd3], %f207;
	st.local.f32 	[%rd3+4], %f206;
	st.local.f32 	[%rd3+8], %f205;

$L__BB34_11:
	shr.s32 	%r40, %r3, 31;
	shr.u32 	%r41, %r40, 27;
	add.s32 	%r42, %r3, %r41;
	shr.s32 	%r43, %r42, 5;
	shl.b32 	%r44, %r43, 2;
	add.s32 	%r14, %r28, %r44;
	mov.u32 	%r46, 2;
	mov.b32 	%r47, %f207;
	mov.u32 	%r48, 31;
	mov.u32 	%r49, 16;
	mov.u32 	%r50, -1;
	shfl.sync.bfly.b32 	%r51|%p7, %r47, %r49, %r48, %r50;
	mov.b32 	%f132, %r51;
	add.f32 	%f133, %f207, %f132;
	mov.b32 	%r52, %f133;
	mov.u32 	%r53, 8;
	shfl.sync.bfly.b32 	%r54|%p8, %r52, %r53, %r48, %r50;
	mov.b32 	%f134, %r54;
	add.f32 	%f135, %f133, %f134;
	mov.b32 	%r55, %f135;
	mov.u32 	%r56, 4;
	shfl.sync.bfly.b32 	%r57|%p9, %r55, %r56, %r48, %r50;
	mov.b32 	%f136, %r57;
	add.f32 	%f137, %f135, %f136;
	mov.b32 	%r58, %f137;
	shfl.sync.bfly.b32 	%r59|%p10, %r58, %r46, %r48, %r50;
	mov.b32 	%f138, %r59;
	add.f32 	%f139, %f137, %f138;
	mov.b32 	%r60, %f139;
	mov.u32 	%r61, 1;
	shfl.sync.bfly.b32 	%r62|%p11, %r60, %r61, %r48, %r50;
	mov.b32 	%f140, %r62;
	add.f32 	%f141, %f139, %f140;
	st.local.f32 	[%rd3], %f141;
	st.shared.f32 	[%r14], %f141;
	bar.sync 	0;
	@%p1 bra 	$L__BB34_13;

	ld.shared.f32 	%f142, [%r4];
	mov.b32 	%r63, %f142;
	shfl.sync.bfly.b32 	%r67|%p13, %r63, %r49, %r48, %r50;
	mov.b32 	%f143, %r67;
	add.f32 	%f144, %f142, %f143;
	mov.b32 	%r68, %f144;
	shfl.sync.bfly.b32 	%r70|%p14, %r68, %r53, %r48, %r50;
	mov.b32 	%f145, %r70;
	add.f32 	%f146, %f144, %f145;
	mov.b32 	%r71, %f146;
	shfl.sync.bfly.b32 	%r73|%p15, %r71, %r56, %r48, %r50;
	mov.b32 	%f147, %r73;
	add.f32 	%f148, %f146, %f147;
	mov.b32 	%r74, %f148;
	shfl.sync.bfly.b32 	%r76|%p16, %r74, %r46, %r48, %r50;
	mov.b32 	%f149, %r76;
	add.f32 	%f150, %f148, %f149;
	mov.b32 	%r77, %f150;
	shfl.sync.bfly.b32 	%r79|%p17, %r77, %r61, %r48, %r50;
	mov.b32 	%f151, %r79;
	add.f32 	%f152, %f150, %f151;
	st.local.f32 	[%rd3], %f152;

$L__BB34_13:
	bar.sync 	0;
	mov.b32 	%r80, %f206;
	shfl.sync.bfly.b32 	%r84|%p19, %r80, %r49, %r48, %r50;
	mov.b32 	%f153, %r84;
	add.f32 	%f154, %f206, %f153;
	mov.b32 	%r85, %f154;
	shfl.sync.bfly.b32 	%r87|%p20, %r85, %r53, %r48, %r50;
	mov.b32 	%f155, %r87;
	add.f32 	%f156, %f154, %f155;
	mov.b32 	%r88, %f156;
	shfl.sync.bfly.b32 	%r90|%p21, %r88, %r56, %r48, %r50;
	mov.b32 	%f157, %r90;
	add.f32 	%f158, %f156, %f157;
	mov.b32 	%r91, %f158;
	shfl.sync.bfly.b32 	%r93|%p22, %r91, %r46, %r48, %r50;
	mov.b32 	%f159, %r93;
	add.f32 	%f160, %f158, %f159;
	mov.b32 	%r94, %f160;
	shfl.sync.bfly.b32 	%r96|%p23, %r94, %r61, %r48, %r50;
	mov.b32 	%f161, %r96;
	add.f32 	%f162, %f160, %f161;
	st.local.f32 	[%rd3+4], %f162;
	st.shared.f32 	[%r14], %f162;
	bar.sync 	0;
	@%p1 bra 	$L__BB34_15;

	ld.shared.f32 	%f163, [%r4];
	mov.b32 	%r97, %f163;
	mov.u32 	%r98, 31;
	mov.u32 	%r99, 16;
	mov.u32 	%r100, -1;
	shfl.sync.bfly.b32 	%r101|%p24, %r97, %r99, %r98, %r100;
	mov.b32 	%f164, %r101;
	add.f32 	%f165, %f163, %f164;
	mov.b32 	%r102, %f165;
	mov.u32 	%r103, 8;
	shfl.sync.bfly.b32 	%r104|%p25, %r102, %r103, %r98, %r100;
	mov.b32 	%f166, %r104;
	add.f32 	%f167, %f165, %f166;
	mov.b32 	%r105, %f167;
	mov.u32 	%r106, 4;
	shfl.sync.bfly.b32 	%r107|%p26, %r105, %r106, %r98, %r100;
	mov.b32 	%f168, %r107;
	add.f32 	%f169, %f167, %f168;
	mov.b32 	%r108, %f169;
	mov.u32 	%r109, 2;
	shfl.sync.bfly.b32 	%r110|%p27, %r108, %r109, %r98, %r100;
	mov.b32 	%f170, %r110;
	add.f32 	%f171, %f169, %f170;
	mov.b32 	%r111, %f171;
	mov.u32 	%r112, 1;
	shfl.sync.bfly.b32 	%r113|%p28, %r111, %r112, %r98, %r100;
	mov.b32 	%f172, %r113;
	add.f32 	%f173, %f171, %f172;
	st.local.f32 	[%rd3+4], %f173;

$L__BB34_15:
	bar.sync 	0;
	mov.b32 	%r114, %f205;
	mov.u32 	%r115, 31;
	mov.u32 	%r116, 16;
	mov.u32 	%r117, -1;
	shfl.sync.bfly.b32 	%r118|%p30, %r114, %r116, %r115, %r117;
	mov.b32 	%f174, %r118;
	add.f32 	%f175, %f205, %f174;
	mov.b32 	%r119, %f175;
	mov.u32 	%r120, 8;
	shfl.sync.bfly.b32 	%r121|%p31, %r119, %r120, %r115, %r117;
	mov.b32 	%f176, %r121;
	add.f32 	%f177, %f175, %f176;
	mov.b32 	%r122, %f177;
	mov.u32 	%r123, 4;
	shfl.sync.bfly.b32 	%r124|%p32, %r122, %r123, %r115, %r117;
	mov.b32 	%f178, %r124;
	add.f32 	%f179, %f177, %f178;
	mov.b32 	%r125, %f179;
	mov.u32 	%r126, 2;
	shfl.sync.bfly.b32 	%r127|%p33, %r125, %r126, %r115, %r117;
	mov.b32 	%f180, %r127;
	add.f32 	%f181, %f179, %f180;
	mov.b32 	%r128, %f181;
	mov.u32 	%r129, 1;
	shfl.sync.bfly.b32 	%r130|%p34, %r128, %r129, %r115, %r117;
	mov.b32 	%f182, %r130;
	add.f32 	%f183, %f181, %f182;
	st.local.f32 	[%rd3+8], %f183;
	st.shared.f32 	[%r14], %f183;
	bar.sync 	0;
	@%p1 bra 	$L__BB34_17;

	ld.shared.f32 	%f184, [%r4];
	mov.b32 	%r131, %f184;
	shfl.sync.bfly.b32 	%r135|%p35, %r131, %r116, %r115, %r117;
	mov.b32 	%f185, %r135;
	add.f32 	%f186, %f184, %f185;
	mov.b32 	%r136, %f186;
	shfl.sync.bfly.b32 	%r138|%p36, %r136, %r120, %r115, %r117;
	mov.b32 	%f187, %r138;
	add.f32 	%f188, %f186, %f187;
	mov.b32 	%r139, %f188;
	shfl.sync.bfly.b32 	%r141|%p37, %r139, %r123, %r115, %r117;
	mov.b32 	%f189, %r141;
	add.f32 	%f190, %f188, %f189;
	mov.b32 	%r142, %f190;
	shfl.sync.bfly.b32 	%r144|%p38, %r142, %r126, %r115, %r117;
	mov.b32 	%f191, %r144;
	add.f32 	%f192, %f190, %f191;
	mov.b32 	%r145, %f192;
	shfl.sync.bfly.b32 	%r147|%p39, %r145, %r129, %r115, %r117;
	mov.b32 	%f193, %r147;
	add.f32 	%f194, %f192, %f193;
	st.local.f32 	[%rd3+8], %f194;

$L__BB34_17:
	bar.sync 	0;
	setp.gt.s32 	%p40, %r3, 2;
	@%p40 bra 	$L__BB34_19;

	mul.wide.s32 	%rd60, %r3, 4;
	add.s64 	%rd61, %rd3, %rd60;
	ld.local.f32 	%f195, [%rd61];
	mad.lo.s32 	%r148, %r3, %r17, %r2;
	cvt.s64.s32 	%rd62, %r148;
	mul.lo.s32 	%r149, %r1, %r18;
	cvt.s64.s32 	%rd63, %r149;
	add.s64 	%rd64, %rd63, %rd62;
	cvta.to.global.u64 	%rd65, %rd28;
	shl.b64 	%rd66, %rd64, 2;
	add.s64 	%rd67, %rd65, %rd66;
	st.global.f32 	[%rd67], %f195;

$L__BB34_19:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_4_bs_160
.visible .entry ggml_matvec_f32_ncols_4_bs_160(
	.param .u64 ggml_matvec_f32_ncols_4_bs_160_param_0,
	.param .u64 ggml_matvec_f32_ncols_4_bs_160_param_1,
	.param .u64 ggml_matvec_f32_ncols_4_bs_160_param_2,
	.param .u32 ggml_matvec_f32_ncols_4_bs_160_param_3,
	.param .u32 ggml_matvec_f32_ncols_4_bs_160_param_4,
	.param .u32 ggml_matvec_f32_ncols_4_bs_160_param_5,
	.param .u32 ggml_matvec_f32_ncols_4_bs_160_param_6,
	.param .u32 ggml_matvec_f32_ncols_4_bs_160_param_7,
	.param .u32 ggml_matvec_f32_ncols_4_bs_160_param_8,
	.param .u32 ggml_matvec_f32_ncols_4_bs_160_param_9,
	.param .u32 ggml_matvec_f32_ncols_4_bs_160_param_10,
	.param .u32 ggml_matvec_f32_ncols_4_bs_160_param_11
)
{
	.local .align 16 .b8 	__local_depot35[16];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<52>;
	.reg .f32 	%f<270>;
	.reg .b32 	%r<189>;
	.reg .b64 	%rd<85>;


	mov.u64 	%SPL, __local_depot35;
	ld.param.u64 	%rd34, [ggml_matvec_f32_ncols_4_bs_160_param_0];
	ld.param.u64 	%rd35, [ggml_matvec_f32_ncols_4_bs_160_param_1];
	ld.param.u64 	%rd33, [ggml_matvec_f32_ncols_4_bs_160_param_2];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_4_bs_160_param_3];
	ld.param.u32 	%r19, [ggml_matvec_f32_ncols_4_bs_160_param_5];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_4_bs_160_param_6];
	ld.param.u32 	%r17, [ggml_matvec_f32_ncols_4_bs_160_param_7];
	ld.param.u32 	%r20, [ggml_matvec_f32_ncols_4_bs_160_param_8];
	ld.param.u32 	%r21, [ggml_matvec_f32_ncols_4_bs_160_param_9];
	ld.param.u32 	%r22, [ggml_matvec_f32_ncols_4_bs_160_param_10];
	ld.param.u32 	%r18, [ggml_matvec_f32_ncols_4_bs_160_param_11];
	cvta.to.global.u64 	%rd84, %rd35;
	cvta.to.global.u64 	%rd2, %rd34;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r23, %r1, %r20;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r24, %r2, %r19;
	mad.lo.s32 	%r25, %r23, %r21, %r24;
	cvt.s64.s32 	%rd4, %r25;
	mul.lo.s32 	%r26, %r1, %r22;
	cvt.s64.s32 	%rd5, %r26;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r27, %r3, 2;
	mov.u32 	%r28, data_mmv;
	add.s32 	%r4, %r28, %r27;
	@%p1 bra 	$L__BB35_2;

	mov.u32 	%r29, 0;
	st.shared.u32 	[%r4], %r29;

$L__BB35_2:
	bar.sync 	0;
	mov.f32 	%f266, 0f00000000;
	st.local.v4.f32 	[%rd3], {%f266, %f266, %f266, %f266};
	setp.ge.s32 	%p2, %r3, %r15;
	mov.f32 	%f267, %f266;
	mov.f32 	%f268, %f266;
	mov.f32 	%f269, %f266;
	@%p2 bra 	$L__BB35_11;

	not.b32 	%r30, %r3;
	add.s32 	%r5, %r30, %r15;
	mul.wide.u32 	%rd37, %r5, -858993459;
	shr.u64 	%rd38, %rd37, 39;
	cvt.u32.u64 	%r31, %rd38;
	add.s32 	%r32, %r31, 1;
	and.b32  	%r186, %r32, 3;
	setp.eq.s32 	%p3, %r186, 0;
	mov.f32 	%f266, 0f00000000;
	mov.u32 	%r187, %r3;
	@%p3 bra 	$L__BB35_7;

	shl.b32 	%r33, %r16, 1;
	add.s32 	%r34, %r3, %r33;
	mul.wide.s32 	%rd39, %r34, 2;
	add.s64 	%rd40, %rd39, %rd5;
	shl.b64 	%rd41, %rd40, 2;
	add.s64 	%rd82, %rd84, %rd41;
	mad.lo.s32 	%r35, %r16, 3, %r3;
	mul.wide.s32 	%rd42, %r35, 2;
	add.s64 	%rd43, %rd42, %rd5;
	shl.b64 	%rd44, %rd43, 2;
	add.s64 	%rd81, %rd84, %rd44;
	mul.wide.s32 	%rd45, %r16, 2;
	mul.wide.s32 	%rd46, %r3, 2;
	add.s64 	%rd47, %rd45, %rd46;
	add.s64 	%rd48, %rd47, %rd5;
	shl.b64 	%rd49, %rd48, 2;
	add.s64 	%rd80, %rd84, %rd49;
	add.s64 	%rd50, %rd46, %rd5;
	shl.b64 	%rd51, %rd50, 2;
	add.s64 	%rd79, %rd84, %rd51;
	add.s64 	%rd52, %rd46, %rd4;
	shl.b64 	%rd53, %rd52, 2;
	add.s64 	%rd78, %rd2, %rd53;
	mov.f32 	%f266, 0f00000000;
	mov.f32 	%f267, %f266;
	mov.f32 	%f268, %f266;
	mov.f32 	%f269, %f266;
	mov.u32 	%r187, %r3;

$L__BB35_5:
	.pragma "nounroll";
	ld.global.nc.v2.f32 	{%f37, %f38}, [%rd78];
	ld.global.nc.v2.f32 	{%f41, %f42}, [%rd79];
	fma.rn.f32 	%f45, %f37, %f41, %f269;
	fma.rn.f32 	%f269, %f38, %f42, %f45;
	ld.global.nc.v2.f32 	{%f46, %f47}, [%rd80];
	fma.rn.f32 	%f50, %f37, %f46, %f268;
	fma.rn.f32 	%f268, %f38, %f47, %f50;
	ld.global.nc.v2.f32 	{%f51, %f52}, [%rd82];
	fma.rn.f32 	%f55, %f37, %f51, %f267;
	fma.rn.f32 	%f267, %f38, %f52, %f55;
	ld.global.nc.v2.f32 	{%f56, %f57}, [%rd81];
	fma.rn.f32 	%f60, %f37, %f56, %f266;
	fma.rn.f32 	%f266, %f38, %f57, %f60;
	add.s32 	%r187, %r187, 160;
	add.s64 	%rd82, %rd82, 1280;
	add.s64 	%rd81, %rd81, 1280;
	add.s64 	%rd80, %rd80, 1280;
	add.s64 	%rd79, %rd79, 1280;
	add.s64 	%rd78, %rd78, 1280;
	add.s32 	%r186, %r186, -1;
	setp.ne.s32 	%p4, %r186, 0;
	@%p4 bra 	$L__BB35_5;

	st.local.v4.f32 	[%rd3], {%f269, %f268, %f267, %f266};

$L__BB35_7:
	setp.lt.u32 	%p5, %r5, 480;
	@%p5 bra 	$L__BB35_11;

	add.s32 	%r36, %r187, %r16;
	shl.b32 	%r37, %r16, 1;
	add.s32 	%r38, %r187, %r37;
	mad.lo.s32 	%r39, %r16, 3, %r187;
	add.s32 	%r40, %r36, 160;
	mul.wide.s32 	%rd54, %r40, 8;
	shl.b64 	%rd55, %rd5, 2;
	add.s64 	%rd23, %rd54, %rd55;
	mul.wide.s32 	%rd56, %r38, 8;
	add.s64 	%rd24, %rd56, %rd55;
	mul.wide.s32 	%rd57, %r39, 8;
	add.s64 	%rd25, %rd57, %rd55;
	mul.wide.s32 	%rd58, %r187, 2;
	add.s64 	%rd59, %rd58, %rd4;
	shl.b64 	%rd60, %rd59, 2;
	add.s64 	%rd61, %rd2, %rd60;
	add.s64 	%rd83, %rd61, 2560;
	mul.wide.s32 	%rd62, %r187, 8;
	add.s64 	%rd27, %rd62, %rd55;
	mul.wide.s32 	%rd63, %r16, 8;
	add.s64 	%rd64, %rd62, %rd63;
	add.s64 	%rd28, %rd64, %rd55;

$L__BB35_9:
	ld.global.nc.v2.f32 	{%f61, %f62}, [%rd83+-2560];
	add.s64 	%rd65, %rd84, %rd27;
	ld.global.nc.v2.f32 	{%f65, %f66}, [%rd65];
	fma.rn.f32 	%f69, %f61, %f65, %f269;
	fma.rn.f32 	%f70, %f62, %f66, %f69;
	add.s64 	%rd66, %rd84, %rd28;
	ld.global.nc.v2.f32 	{%f71, %f72}, [%rd66];
	fma.rn.f32 	%f75, %f61, %f71, %f268;
	fma.rn.f32 	%f76, %f62, %f72, %f75;
	add.s64 	%rd67, %rd84, %rd24;
	ld.global.nc.v2.f32 	{%f77, %f78}, [%rd67];
	fma.rn.f32 	%f81, %f61, %f77, %f267;
	fma.rn.f32 	%f82, %f62, %f78, %f81;
	add.s64 	%rd68, %rd84, %rd25;
	ld.global.nc.v2.f32 	{%f83, %f84}, [%rd68];
	fma.rn.f32 	%f87, %f61, %f83, %f266;
	fma.rn.f32 	%f88, %f62, %f84, %f87;
	ld.global.nc.v2.f32 	{%f89, %f90}, [%rd83+-1280];
	ld.global.nc.v2.f32 	{%f93, %f94}, [%rd65+1280];
	fma.rn.f32 	%f97, %f89, %f93, %f70;
	fma.rn.f32 	%f98, %f90, %f94, %f97;
	add.s64 	%rd69, %rd84, %rd23;
	ld.global.nc.v2.f32 	{%f99, %f100}, [%rd69];
	fma.rn.f32 	%f103, %f89, %f99, %f76;
	fma.rn.f32 	%f104, %f90, %f100, %f103;
	ld.global.nc.v2.f32 	{%f105, %f106}, [%rd67+1280];
	fma.rn.f32 	%f109, %f89, %f105, %f82;
	fma.rn.f32 	%f110, %f90, %f106, %f109;
	ld.global.nc.v2.f32 	{%f111, %f112}, [%rd68+1280];
	fma.rn.f32 	%f115, %f89, %f111, %f88;
	fma.rn.f32 	%f116, %f90, %f112, %f115;
	ld.global.nc.v2.f32 	{%f117, %f118}, [%rd83];
	ld.global.nc.v2.f32 	{%f121, %f122}, [%rd65+2560];
	fma.rn.f32 	%f125, %f117, %f121, %f98;
	fma.rn.f32 	%f126, %f118, %f122, %f125;
	ld.global.nc.v2.f32 	{%f127, %f128}, [%rd69+1280];
	fma.rn.f32 	%f131, %f117, %f127, %f104;
	fma.rn.f32 	%f132, %f118, %f128, %f131;
	ld.global.nc.v2.f32 	{%f133, %f134}, [%rd67+2560];
	fma.rn.f32 	%f137, %f117, %f133, %f110;
	fma.rn.f32 	%f138, %f118, %f134, %f137;
	ld.global.nc.v2.f32 	{%f139, %f140}, [%rd68+2560];
	fma.rn.f32 	%f143, %f117, %f139, %f116;
	fma.rn.f32 	%f144, %f118, %f140, %f143;
	ld.global.nc.v2.f32 	{%f145, %f146}, [%rd83+1280];
	ld.global.nc.v2.f32 	{%f149, %f150}, [%rd65+3840];
	fma.rn.f32 	%f153, %f145, %f149, %f126;
	fma.rn.f32 	%f269, %f146, %f150, %f153;
	ld.global.nc.v2.f32 	{%f154, %f155}, [%rd69+2560];
	fma.rn.f32 	%f158, %f145, %f154, %f132;
	fma.rn.f32 	%f268, %f146, %f155, %f158;
	ld.global.nc.v2.f32 	{%f159, %f160}, [%rd67+3840];
	fma.rn.f32 	%f163, %f145, %f159, %f138;
	fma.rn.f32 	%f267, %f146, %f160, %f163;
	ld.global.nc.v2.f32 	{%f164, %f165}, [%rd68+3840];
	fma.rn.f32 	%f168, %f145, %f164, %f144;
	fma.rn.f32 	%f266, %f146, %f165, %f168;
	add.s64 	%rd84, %rd84, 5120;
	add.s64 	%rd83, %rd83, 5120;
	add.s32 	%r187, %r187, 640;
	setp.lt.s32 	%p6, %r187, %r15;
	@%p6 bra 	$L__BB35_9;

	st.local.v4.f32 	[%rd3], {%f269, %f268, %f267, %f266};

$L__BB35_11:
	shr.s32 	%r41, %r3, 31;
	shr.u32 	%r42, %r41, 27;
	add.s32 	%r43, %r3, %r42;
	shr.s32 	%r44, %r43, 5;
	shl.b32 	%r45, %r44, 2;
	add.s32 	%r14, %r28, %r45;
	mov.u32 	%r47, 2;
	mov.b32 	%r48, %f269;
	mov.u32 	%r49, 31;
	mov.u32 	%r50, 16;
	mov.u32 	%r51, -1;
	shfl.sync.bfly.b32 	%r52|%p7, %r48, %r50, %r49, %r51;
	mov.b32 	%f169, %r52;
	add.f32 	%f170, %f269, %f169;
	mov.b32 	%r53, %f170;
	mov.u32 	%r54, 8;
	shfl.sync.bfly.b32 	%r55|%p8, %r53, %r54, %r49, %r51;
	mov.b32 	%f171, %r55;
	add.f32 	%f172, %f170, %f171;
	mov.b32 	%r56, %f172;
	mov.u32 	%r57, 4;
	shfl.sync.bfly.b32 	%r58|%p9, %r56, %r57, %r49, %r51;
	mov.b32 	%f173, %r58;
	add.f32 	%f174, %f172, %f173;
	mov.b32 	%r59, %f174;
	shfl.sync.bfly.b32 	%r60|%p10, %r59, %r47, %r49, %r51;
	mov.b32 	%f175, %r60;
	add.f32 	%f176, %f174, %f175;
	mov.b32 	%r61, %f176;
	mov.u32 	%r62, 1;
	shfl.sync.bfly.b32 	%r63|%p11, %r61, %r62, %r49, %r51;
	mov.b32 	%f177, %r63;
	add.f32 	%f178, %f176, %f177;
	st.local.f32 	[%rd3], %f178;
	st.shared.f32 	[%r14], %f178;
	bar.sync 	0;
	@%p1 bra 	$L__BB35_13;

	ld.shared.f32 	%f179, [%r4];
	mov.b32 	%r64, %f179;
	shfl.sync.bfly.b32 	%r68|%p13, %r64, %r50, %r49, %r51;
	mov.b32 	%f180, %r68;
	add.f32 	%f181, %f179, %f180;
	mov.b32 	%r69, %f181;
	shfl.sync.bfly.b32 	%r71|%p14, %r69, %r54, %r49, %r51;
	mov.b32 	%f182, %r71;
	add.f32 	%f183, %f181, %f182;
	mov.b32 	%r72, %f183;
	shfl.sync.bfly.b32 	%r74|%p15, %r72, %r57, %r49, %r51;
	mov.b32 	%f184, %r74;
	add.f32 	%f185, %f183, %f184;
	mov.b32 	%r75, %f185;
	shfl.sync.bfly.b32 	%r77|%p16, %r75, %r47, %r49, %r51;
	mov.b32 	%f186, %r77;
	add.f32 	%f187, %f185, %f186;
	mov.b32 	%r78, %f187;
	shfl.sync.bfly.b32 	%r80|%p17, %r78, %r62, %r49, %r51;
	mov.b32 	%f188, %r80;
	add.f32 	%f189, %f187, %f188;
	st.local.f32 	[%rd3], %f189;

$L__BB35_13:
	bar.sync 	0;
	mov.b32 	%r81, %f268;
	shfl.sync.bfly.b32 	%r85|%p19, %r81, %r50, %r49, %r51;
	mov.b32 	%f190, %r85;
	add.f32 	%f191, %f268, %f190;
	mov.b32 	%r86, %f191;
	shfl.sync.bfly.b32 	%r88|%p20, %r86, %r54, %r49, %r51;
	mov.b32 	%f192, %r88;
	add.f32 	%f193, %f191, %f192;
	mov.b32 	%r89, %f193;
	shfl.sync.bfly.b32 	%r91|%p21, %r89, %r57, %r49, %r51;
	mov.b32 	%f194, %r91;
	add.f32 	%f195, %f193, %f194;
	mov.b32 	%r92, %f195;
	shfl.sync.bfly.b32 	%r94|%p22, %r92, %r47, %r49, %r51;
	mov.b32 	%f196, %r94;
	add.f32 	%f197, %f195, %f196;
	mov.b32 	%r95, %f197;
	shfl.sync.bfly.b32 	%r97|%p23, %r95, %r62, %r49, %r51;
	mov.b32 	%f198, %r97;
	add.f32 	%f199, %f197, %f198;
	st.local.f32 	[%rd3+4], %f199;
	st.shared.f32 	[%r14], %f199;
	bar.sync 	0;
	@%p1 bra 	$L__BB35_15;

	ld.shared.f32 	%f200, [%r4];
	mov.b32 	%r98, %f200;
	mov.u32 	%r99, 31;
	mov.u32 	%r100, 16;
	mov.u32 	%r101, -1;
	shfl.sync.bfly.b32 	%r102|%p24, %r98, %r100, %r99, %r101;
	mov.b32 	%f201, %r102;
	add.f32 	%f202, %f200, %f201;
	mov.b32 	%r103, %f202;
	mov.u32 	%r104, 8;
	shfl.sync.bfly.b32 	%r105|%p25, %r103, %r104, %r99, %r101;
	mov.b32 	%f203, %r105;
	add.f32 	%f204, %f202, %f203;
	mov.b32 	%r106, %f204;
	mov.u32 	%r107, 4;
	shfl.sync.bfly.b32 	%r108|%p26, %r106, %r107, %r99, %r101;
	mov.b32 	%f205, %r108;
	add.f32 	%f206, %f204, %f205;
	mov.b32 	%r109, %f206;
	mov.u32 	%r110, 2;
	shfl.sync.bfly.b32 	%r111|%p27, %r109, %r110, %r99, %r101;
	mov.b32 	%f207, %r111;
	add.f32 	%f208, %f206, %f207;
	mov.b32 	%r112, %f208;
	mov.u32 	%r113, 1;
	shfl.sync.bfly.b32 	%r114|%p28, %r112, %r113, %r99, %r101;
	mov.b32 	%f209, %r114;
	add.f32 	%f210, %f208, %f209;
	st.local.f32 	[%rd3+4], %f210;

$L__BB35_15:
	bar.sync 	0;
	mov.b32 	%r115, %f267;
	mov.u32 	%r116, 31;
	mov.u32 	%r117, 16;
	mov.u32 	%r118, -1;
	shfl.sync.bfly.b32 	%r119|%p30, %r115, %r117, %r116, %r118;
	mov.b32 	%f211, %r119;
	add.f32 	%f212, %f267, %f211;
	mov.b32 	%r120, %f212;
	mov.u32 	%r121, 8;
	shfl.sync.bfly.b32 	%r122|%p31, %r120, %r121, %r116, %r118;
	mov.b32 	%f213, %r122;
	add.f32 	%f214, %f212, %f213;
	mov.b32 	%r123, %f214;
	mov.u32 	%r124, 4;
	shfl.sync.bfly.b32 	%r125|%p32, %r123, %r124, %r116, %r118;
	mov.b32 	%f215, %r125;
	add.f32 	%f216, %f214, %f215;
	mov.b32 	%r126, %f216;
	mov.u32 	%r127, 2;
	shfl.sync.bfly.b32 	%r128|%p33, %r126, %r127, %r116, %r118;
	mov.b32 	%f217, %r128;
	add.f32 	%f218, %f216, %f217;
	mov.b32 	%r129, %f218;
	mov.u32 	%r130, 1;
	shfl.sync.bfly.b32 	%r131|%p34, %r129, %r130, %r116, %r118;
	mov.b32 	%f219, %r131;
	add.f32 	%f220, %f218, %f219;
	st.local.f32 	[%rd3+8], %f220;
	st.shared.f32 	[%r14], %f220;
	bar.sync 	0;
	@%p1 bra 	$L__BB35_17;

	ld.shared.f32 	%f221, [%r4];
	mov.b32 	%r132, %f221;
	shfl.sync.bfly.b32 	%r136|%p35, %r132, %r117, %r116, %r118;
	mov.b32 	%f222, %r136;
	add.f32 	%f223, %f221, %f222;
	mov.b32 	%r137, %f223;
	shfl.sync.bfly.b32 	%r139|%p36, %r137, %r121, %r116, %r118;
	mov.b32 	%f224, %r139;
	add.f32 	%f225, %f223, %f224;
	mov.b32 	%r140, %f225;
	shfl.sync.bfly.b32 	%r142|%p37, %r140, %r124, %r116, %r118;
	mov.b32 	%f226, %r142;
	add.f32 	%f227, %f225, %f226;
	mov.b32 	%r143, %f227;
	shfl.sync.bfly.b32 	%r145|%p38, %r143, %r127, %r116, %r118;
	mov.b32 	%f228, %r145;
	add.f32 	%f229, %f227, %f228;
	mov.b32 	%r146, %f229;
	shfl.sync.bfly.b32 	%r148|%p39, %r146, %r130, %r116, %r118;
	mov.b32 	%f230, %r148;
	add.f32 	%f231, %f229, %f230;
	st.local.f32 	[%rd3+8], %f231;

$L__BB35_17:
	bar.sync 	0;
	mov.b32 	%r149, %f266;
	shfl.sync.bfly.b32 	%r153|%p41, %r149, %r117, %r116, %r118;
	mov.b32 	%f232, %r153;
	add.f32 	%f233, %f266, %f232;
	mov.b32 	%r154, %f233;
	shfl.sync.bfly.b32 	%r156|%p42, %r154, %r121, %r116, %r118;
	mov.b32 	%f234, %r156;
	add.f32 	%f235, %f233, %f234;
	mov.b32 	%r157, %f235;
	shfl.sync.bfly.b32 	%r159|%p43, %r157, %r124, %r116, %r118;
	mov.b32 	%f236, %r159;
	add.f32 	%f237, %f235, %f236;
	mov.b32 	%r160, %f237;
	shfl.sync.bfly.b32 	%r162|%p44, %r160, %r127, %r116, %r118;
	mov.b32 	%f238, %r162;
	add.f32 	%f239, %f237, %f238;
	mov.b32 	%r163, %f239;
	shfl.sync.bfly.b32 	%r165|%p45, %r163, %r130, %r116, %r118;
	mov.b32 	%f240, %r165;
	add.f32 	%f241, %f239, %f240;
	st.local.f32 	[%rd3+12], %f241;
	st.shared.f32 	[%r14], %f241;
	bar.sync 	0;
	@%p1 bra 	$L__BB35_19;

	ld.shared.f32 	%f242, [%r4];
	mov.b32 	%r166, %f242;
	mov.u32 	%r167, 31;
	mov.u32 	%r168, 16;
	mov.u32 	%r169, -1;
	shfl.sync.bfly.b32 	%r170|%p46, %r166, %r168, %r167, %r169;
	mov.b32 	%f243, %r170;
	add.f32 	%f244, %f242, %f243;
	mov.b32 	%r171, %f244;
	mov.u32 	%r172, 8;
	shfl.sync.bfly.b32 	%r173|%p47, %r171, %r172, %r167, %r169;
	mov.b32 	%f245, %r173;
	add.f32 	%f246, %f244, %f245;
	mov.b32 	%r174, %f246;
	mov.u32 	%r175, 4;
	shfl.sync.bfly.b32 	%r176|%p48, %r174, %r175, %r167, %r169;
	mov.b32 	%f247, %r176;
	add.f32 	%f248, %f246, %f247;
	mov.b32 	%r177, %f248;
	mov.u32 	%r178, 2;
	shfl.sync.bfly.b32 	%r179|%p49, %r177, %r178, %r167, %r169;
	mov.b32 	%f249, %r179;
	add.f32 	%f250, %f248, %f249;
	mov.b32 	%r180, %f250;
	mov.u32 	%r181, 1;
	shfl.sync.bfly.b32 	%r182|%p50, %r180, %r181, %r167, %r169;
	mov.b32 	%f251, %r182;
	add.f32 	%f252, %f250, %f251;
	st.local.f32 	[%rd3+12], %f252;

$L__BB35_19:
	bar.sync 	0;
	setp.gt.s32 	%p51, %r3, 3;
	@%p51 bra 	$L__BB35_21;

	mul.wide.s32 	%rd70, %r3, 4;
	add.s64 	%rd71, %rd3, %rd70;
	ld.local.f32 	%f253, [%rd71];
	mad.lo.s32 	%r183, %r3, %r17, %r2;
	cvt.s64.s32 	%rd72, %r183;
	mul.lo.s32 	%r184, %r1, %r18;
	cvt.s64.s32 	%rd73, %r184;
	add.s64 	%rd74, %rd73, %rd72;
	cvta.to.global.u64 	%rd75, %rd33;
	shl.b64 	%rd76, %rd74, 2;
	add.s64 	%rd77, %rd75, %rd76;
	st.global.f32 	[%rd77], %f253;

$L__BB35_21:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_5_bs_160
.visible .entry ggml_matvec_f32_ncols_5_bs_160(
	.param .u64 ggml_matvec_f32_ncols_5_bs_160_param_0,
	.param .u64 ggml_matvec_f32_ncols_5_bs_160_param_1,
	.param .u64 ggml_matvec_f32_ncols_5_bs_160_param_2,
	.param .u32 ggml_matvec_f32_ncols_5_bs_160_param_3,
	.param .u32 ggml_matvec_f32_ncols_5_bs_160_param_4,
	.param .u32 ggml_matvec_f32_ncols_5_bs_160_param_5,
	.param .u32 ggml_matvec_f32_ncols_5_bs_160_param_6,
	.param .u32 ggml_matvec_f32_ncols_5_bs_160_param_7,
	.param .u32 ggml_matvec_f32_ncols_5_bs_160_param_8,
	.param .u32 ggml_matvec_f32_ncols_5_bs_160_param_9,
	.param .u32 ggml_matvec_f32_ncols_5_bs_160_param_10,
	.param .u32 ggml_matvec_f32_ncols_5_bs_160_param_11
)
{
	.local .align 4 .b8 	__local_depot36[20];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<63>;
	.reg .f32 	%f<332>;
	.reg .b32 	%r<225>;
	.reg .b64 	%rd<76>;


	mov.u64 	%SPL, __local_depot36;
	ld.param.u64 	%rd28, [ggml_matvec_f32_ncols_5_bs_160_param_0];
	ld.param.u64 	%rd29, [ggml_matvec_f32_ncols_5_bs_160_param_1];
	ld.param.u64 	%rd27, [ggml_matvec_f32_ncols_5_bs_160_param_2];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_5_bs_160_param_3];
	ld.param.u32 	%r19, [ggml_matvec_f32_ncols_5_bs_160_param_5];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_5_bs_160_param_6];
	ld.param.u32 	%r17, [ggml_matvec_f32_ncols_5_bs_160_param_7];
	ld.param.u32 	%r20, [ggml_matvec_f32_ncols_5_bs_160_param_8];
	ld.param.u32 	%r21, [ggml_matvec_f32_ncols_5_bs_160_param_9];
	ld.param.u32 	%r22, [ggml_matvec_f32_ncols_5_bs_160_param_10];
	ld.param.u32 	%r18, [ggml_matvec_f32_ncols_5_bs_160_param_11];
	cvta.to.global.u64 	%rd75, %rd29;
	cvta.to.global.u64 	%rd2, %rd28;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r23, %r1, %r20;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r24, %r2, %r19;
	mad.lo.s32 	%r25, %r23, %r21, %r24;
	cvt.s64.s32 	%rd4, %r25;
	mul.lo.s32 	%r26, %r1, %r22;
	cvt.s64.s32 	%rd5, %r26;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r27, %r3, 2;
	mov.u32 	%r28, data_mmv;
	add.s32 	%r4, %r28, %r27;
	@%p1 bra 	$L__BB36_2;

	mov.u32 	%r29, 0;
	st.shared.u32 	[%r4], %r29;

$L__BB36_2:
	bar.sync 	0;
	mov.f32 	%f327, 0f00000000;
	mov.u32 	%r30, 0;
	st.local.u32 	[%rd3], %r30;
	st.local.u32 	[%rd3+4], %r30;
	st.local.u32 	[%rd3+8], %r30;
	st.local.u32 	[%rd3+12], %r30;
	st.local.u32 	[%rd3+16], %r30;
	setp.ge.s32 	%p2, %r3, %r15;
	mov.f32 	%f328, %f327;
	mov.f32 	%f329, %f327;
	mov.f32 	%f330, %f327;
	mov.f32 	%f331, %f327;
	@%p2 bra 	$L__BB36_11;

	not.b32 	%r31, %r3;
	add.s32 	%r5, %r31, %r15;
	mul.wide.u32 	%rd31, %r5, -858993459;
	shr.u64 	%rd32, %rd31, 39;
	cvt.u32.u64 	%r32, %rd32;
	add.s32 	%r33, %r32, 1;
	and.b32  	%r222, %r33, 3;
	setp.eq.s32 	%p3, %r222, 0;
	mov.f32 	%f327, 0f00000000;
	mov.u32 	%r223, %r3;
	@%p3 bra 	$L__BB36_7;

	shl.b32 	%r34, %r16, 1;
	mad.lo.s32 	%r35, %r16, 3, %r3;
	mul.wide.s32 	%rd33, %r35, 8;
	shl.b64 	%rd34, %rd5, 2;
	add.s64 	%rd7, %rd33, %rd34;
	mul.wide.s32 	%rd35, %r3, 8;
	mul.wide.s32 	%rd36, %r16, 8;
	add.s64 	%rd37, %rd35, %rd36;
	add.s64 	%rd8, %rd37, %rd34;
	add.s64 	%rd9, %rd35, %rd34;
	mul.wide.s32 	%rd38, %r3, 2;
	add.s64 	%rd39, %rd38, %rd4;
	shl.b64 	%rd40, %rd39, 2;
	add.s64 	%rd72, %rd2, %rd40;
	mul.wide.s32 	%rd11, %r34, 8;
	mov.f32 	%f327, 0f00000000;
	mov.u64 	%rd73, %rd75;
	mov.f32 	%f328, %f327;
	mov.f32 	%f329, %f327;
	mov.f32 	%f330, %f327;
	mov.f32 	%f331, %f327;
	mov.u32 	%r223, %r3;

$L__BB36_5:
	.pragma "nounroll";
	ld.global.nc.v2.f32 	{%f46, %f47}, [%rd72];
	add.s64 	%rd41, %rd73, %rd9;
	ld.global.nc.v2.f32 	{%f50, %f51}, [%rd41];
	fma.rn.f32 	%f54, %f46, %f50, %f331;
	fma.rn.f32 	%f331, %f47, %f51, %f54;
	add.s64 	%rd42, %rd73, %rd8;
	ld.global.nc.v2.f32 	{%f55, %f56}, [%rd42];
	fma.rn.f32 	%f59, %f46, %f55, %f330;
	fma.rn.f32 	%f330, %f47, %f56, %f59;
	add.s64 	%rd43, %rd41, %rd11;
	ld.global.nc.v2.f32 	{%f60, %f61}, [%rd43];
	fma.rn.f32 	%f64, %f46, %f60, %f329;
	fma.rn.f32 	%f329, %f47, %f61, %f64;
	add.s64 	%rd44, %rd73, %rd7;
	ld.global.nc.v2.f32 	{%f65, %f66}, [%rd44];
	fma.rn.f32 	%f69, %f46, %f65, %f328;
	fma.rn.f32 	%f328, %f47, %f66, %f69;
	add.s64 	%rd45, %rd43, %rd11;
	ld.global.nc.v2.f32 	{%f70, %f71}, [%rd45];
	fma.rn.f32 	%f74, %f46, %f70, %f327;
	fma.rn.f32 	%f327, %f47, %f71, %f74;
	add.s32 	%r223, %r223, 160;
	add.s64 	%rd73, %rd73, 1280;
	add.s64 	%rd72, %rd72, 1280;
	add.s32 	%r222, %r222, -1;
	setp.ne.s32 	%p4, %r222, 0;
	@%p4 bra 	$L__BB36_5;

	st.local.f32 	[%rd3], %f331;
	st.local.f32 	[%rd3+4], %f330;
	st.local.f32 	[%rd3+8], %f329;
	st.local.f32 	[%rd3+12], %f328;
	st.local.f32 	[%rd3+16], %f327;

$L__BB36_7:
	setp.lt.u32 	%p5, %r5, 480;
	@%p5 bra 	$L__BB36_11;

	add.s32 	%r36, %r223, %r16;
	shl.b32 	%r37, %r16, 1;
	add.s32 	%r38, %r223, %r37;
	mad.lo.s32 	%r39, %r16, 3, %r223;
	shl.b32 	%r40, %r16, 2;
	add.s32 	%r41, %r223, %r40;
	add.s32 	%r42, %r36, 160;
	mul.wide.s32 	%rd46, %r42, 8;
	shl.b64 	%rd47, %rd5, 2;
	add.s64 	%rd16, %rd46, %rd47;
	mul.wide.s32 	%rd48, %r38, 8;
	add.s64 	%rd17, %rd48, %rd47;
	mul.wide.s32 	%rd49, %r39, 8;
	add.s64 	%rd18, %rd49, %rd47;
	mul.wide.s32 	%rd50, %r41, 8;
	add.s64 	%rd19, %rd50, %rd47;
	mul.wide.s32 	%rd51, %r223, 2;
	add.s64 	%rd52, %rd51, %rd4;
	shl.b64 	%rd53, %rd52, 2;
	add.s64 	%rd54, %rd2, %rd53;
	add.s64 	%rd74, %rd54, 2560;
	mul.wide.s32 	%rd55, %r223, 8;
	add.s64 	%rd21, %rd55, %rd47;
	mul.wide.s32 	%rd56, %r16, 8;
	add.s64 	%rd57, %rd55, %rd56;
	add.s64 	%rd22, %rd57, %rd47;

$L__BB36_9:
	ld.global.nc.v2.f32 	{%f75, %f76}, [%rd74+-2560];
	add.s64 	%rd58, %rd75, %rd21;
	ld.global.nc.v2.f32 	{%f79, %f80}, [%rd58];
	fma.rn.f32 	%f83, %f75, %f79, %f331;
	fma.rn.f32 	%f84, %f76, %f80, %f83;
	add.s64 	%rd59, %rd75, %rd22;
	ld.global.nc.v2.f32 	{%f85, %f86}, [%rd59];
	fma.rn.f32 	%f89, %f75, %f85, %f330;
	fma.rn.f32 	%f90, %f76, %f86, %f89;
	add.s64 	%rd60, %rd75, %rd17;
	ld.global.nc.v2.f32 	{%f91, %f92}, [%rd60];
	fma.rn.f32 	%f95, %f75, %f91, %f329;
	fma.rn.f32 	%f96, %f76, %f92, %f95;
	add.s64 	%rd61, %rd75, %rd18;
	ld.global.nc.v2.f32 	{%f97, %f98}, [%rd61];
	fma.rn.f32 	%f101, %f75, %f97, %f328;
	fma.rn.f32 	%f102, %f76, %f98, %f101;
	add.s64 	%rd62, %rd75, %rd19;
	ld.global.nc.v2.f32 	{%f103, %f104}, [%rd62];
	fma.rn.f32 	%f107, %f75, %f103, %f327;
	fma.rn.f32 	%f108, %f76, %f104, %f107;
	ld.global.nc.v2.f32 	{%f109, %f110}, [%rd74+-1280];
	ld.global.nc.v2.f32 	{%f113, %f114}, [%rd58+1280];
	fma.rn.f32 	%f117, %f109, %f113, %f84;
	fma.rn.f32 	%f118, %f110, %f114, %f117;
	add.s64 	%rd63, %rd75, %rd16;
	ld.global.nc.v2.f32 	{%f119, %f120}, [%rd63];
	fma.rn.f32 	%f123, %f109, %f119, %f90;
	fma.rn.f32 	%f124, %f110, %f120, %f123;
	ld.global.nc.v2.f32 	{%f125, %f126}, [%rd60+1280];
	fma.rn.f32 	%f129, %f109, %f125, %f96;
	fma.rn.f32 	%f130, %f110, %f126, %f129;
	ld.global.nc.v2.f32 	{%f131, %f132}, [%rd61+1280];
	fma.rn.f32 	%f135, %f109, %f131, %f102;
	fma.rn.f32 	%f136, %f110, %f132, %f135;
	ld.global.nc.v2.f32 	{%f137, %f138}, [%rd62+1280];
	fma.rn.f32 	%f141, %f109, %f137, %f108;
	fma.rn.f32 	%f142, %f110, %f138, %f141;
	ld.global.nc.v2.f32 	{%f143, %f144}, [%rd74];
	ld.global.nc.v2.f32 	{%f147, %f148}, [%rd58+2560];
	fma.rn.f32 	%f151, %f143, %f147, %f118;
	fma.rn.f32 	%f152, %f144, %f148, %f151;
	ld.global.nc.v2.f32 	{%f153, %f154}, [%rd63+1280];
	fma.rn.f32 	%f157, %f143, %f153, %f124;
	fma.rn.f32 	%f158, %f144, %f154, %f157;
	ld.global.nc.v2.f32 	{%f159, %f160}, [%rd60+2560];
	fma.rn.f32 	%f163, %f143, %f159, %f130;
	fma.rn.f32 	%f164, %f144, %f160, %f163;
	ld.global.nc.v2.f32 	{%f165, %f166}, [%rd61+2560];
	fma.rn.f32 	%f169, %f143, %f165, %f136;
	fma.rn.f32 	%f170, %f144, %f166, %f169;
	ld.global.nc.v2.f32 	{%f171, %f172}, [%rd62+2560];
	fma.rn.f32 	%f175, %f143, %f171, %f142;
	fma.rn.f32 	%f176, %f144, %f172, %f175;
	ld.global.nc.v2.f32 	{%f177, %f178}, [%rd74+1280];
	ld.global.nc.v2.f32 	{%f181, %f182}, [%rd58+3840];
	fma.rn.f32 	%f185, %f177, %f181, %f152;
	fma.rn.f32 	%f331, %f178, %f182, %f185;
	ld.global.nc.v2.f32 	{%f186, %f187}, [%rd63+2560];
	fma.rn.f32 	%f190, %f177, %f186, %f158;
	fma.rn.f32 	%f330, %f178, %f187, %f190;
	ld.global.nc.v2.f32 	{%f191, %f192}, [%rd60+3840];
	fma.rn.f32 	%f195, %f177, %f191, %f164;
	fma.rn.f32 	%f329, %f178, %f192, %f195;
	ld.global.nc.v2.f32 	{%f196, %f197}, [%rd61+3840];
	fma.rn.f32 	%f200, %f177, %f196, %f170;
	fma.rn.f32 	%f328, %f178, %f197, %f200;
	ld.global.nc.v2.f32 	{%f201, %f202}, [%rd62+3840];
	fma.rn.f32 	%f205, %f177, %f201, %f176;
	fma.rn.f32 	%f327, %f178, %f202, %f205;
	add.s64 	%rd75, %rd75, 5120;
	add.s64 	%rd74, %rd74, 5120;
	add.s32 	%r223, %r223, 640;
	setp.lt.s32 	%p6, %r223, %r15;
	@%p6 bra 	$L__BB36_9;

	st.local.f32 	[%rd3], %f331;
	st.local.f32 	[%rd3+4], %f330;
	st.local.f32 	[%rd3+8], %f329;
	st.local.f32 	[%rd3+12], %f328;
	st.local.f32 	[%rd3+16], %f327;

$L__BB36_11:
	shr.s32 	%r43, %r3, 31;
	shr.u32 	%r44, %r43, 27;
	add.s32 	%r45, %r3, %r44;
	shr.s32 	%r46, %r45, 5;
	shl.b32 	%r47, %r46, 2;
	add.s32 	%r14, %r28, %r47;
	mov.u32 	%r49, 2;
	mov.b32 	%r50, %f331;
	mov.u32 	%r51, 31;
	mov.u32 	%r52, 16;
	mov.u32 	%r53, -1;
	shfl.sync.bfly.b32 	%r54|%p7, %r50, %r52, %r51, %r53;
	mov.b32 	%f206, %r54;
	add.f32 	%f207, %f331, %f206;
	mov.b32 	%r55, %f207;
	mov.u32 	%r56, 8;
	shfl.sync.bfly.b32 	%r57|%p8, %r55, %r56, %r51, %r53;
	mov.b32 	%f208, %r57;
	add.f32 	%f209, %f207, %f208;
	mov.b32 	%r58, %f209;
	mov.u32 	%r59, 4;
	shfl.sync.bfly.b32 	%r60|%p9, %r58, %r59, %r51, %r53;
	mov.b32 	%f210, %r60;
	add.f32 	%f211, %f209, %f210;
	mov.b32 	%r61, %f211;
	shfl.sync.bfly.b32 	%r62|%p10, %r61, %r49, %r51, %r53;
	mov.b32 	%f212, %r62;
	add.f32 	%f213, %f211, %f212;
	mov.b32 	%r63, %f213;
	mov.u32 	%r64, 1;
	shfl.sync.bfly.b32 	%r65|%p11, %r63, %r64, %r51, %r53;
	mov.b32 	%f214, %r65;
	add.f32 	%f215, %f213, %f214;
	st.local.f32 	[%rd3], %f215;
	st.shared.f32 	[%r14], %f215;
	bar.sync 	0;
	@%p1 bra 	$L__BB36_13;

	ld.shared.f32 	%f216, [%r4];
	mov.b32 	%r66, %f216;
	shfl.sync.bfly.b32 	%r70|%p13, %r66, %r52, %r51, %r53;
	mov.b32 	%f217, %r70;
	add.f32 	%f218, %f216, %f217;
	mov.b32 	%r71, %f218;
	shfl.sync.bfly.b32 	%r73|%p14, %r71, %r56, %r51, %r53;
	mov.b32 	%f219, %r73;
	add.f32 	%f220, %f218, %f219;
	mov.b32 	%r74, %f220;
	shfl.sync.bfly.b32 	%r76|%p15, %r74, %r59, %r51, %r53;
	mov.b32 	%f221, %r76;
	add.f32 	%f222, %f220, %f221;
	mov.b32 	%r77, %f222;
	shfl.sync.bfly.b32 	%r79|%p16, %r77, %r49, %r51, %r53;
	mov.b32 	%f223, %r79;
	add.f32 	%f224, %f222, %f223;
	mov.b32 	%r80, %f224;
	shfl.sync.bfly.b32 	%r82|%p17, %r80, %r64, %r51, %r53;
	mov.b32 	%f225, %r82;
	add.f32 	%f226, %f224, %f225;
	st.local.f32 	[%rd3], %f226;

$L__BB36_13:
	bar.sync 	0;
	mov.b32 	%r83, %f330;
	shfl.sync.bfly.b32 	%r87|%p19, %r83, %r52, %r51, %r53;
	mov.b32 	%f227, %r87;
	add.f32 	%f228, %f330, %f227;
	mov.b32 	%r88, %f228;
	shfl.sync.bfly.b32 	%r90|%p20, %r88, %r56, %r51, %r53;
	mov.b32 	%f229, %r90;
	add.f32 	%f230, %f228, %f229;
	mov.b32 	%r91, %f230;
	shfl.sync.bfly.b32 	%r93|%p21, %r91, %r59, %r51, %r53;
	mov.b32 	%f231, %r93;
	add.f32 	%f232, %f230, %f231;
	mov.b32 	%r94, %f232;
	shfl.sync.bfly.b32 	%r96|%p22, %r94, %r49, %r51, %r53;
	mov.b32 	%f233, %r96;
	add.f32 	%f234, %f232, %f233;
	mov.b32 	%r97, %f234;
	shfl.sync.bfly.b32 	%r99|%p23, %r97, %r64, %r51, %r53;
	mov.b32 	%f235, %r99;
	add.f32 	%f236, %f234, %f235;
	st.local.f32 	[%rd3+4], %f236;
	st.shared.f32 	[%r14], %f236;
	bar.sync 	0;
	@%p1 bra 	$L__BB36_15;

	ld.shared.f32 	%f237, [%r4];
	mov.b32 	%r100, %f237;
	mov.u32 	%r101, 31;
	mov.u32 	%r102, 16;
	mov.u32 	%r103, -1;
	shfl.sync.bfly.b32 	%r104|%p24, %r100, %r102, %r101, %r103;
	mov.b32 	%f238, %r104;
	add.f32 	%f239, %f237, %f238;
	mov.b32 	%r105, %f239;
	mov.u32 	%r106, 8;
	shfl.sync.bfly.b32 	%r107|%p25, %r105, %r106, %r101, %r103;
	mov.b32 	%f240, %r107;
	add.f32 	%f241, %f239, %f240;
	mov.b32 	%r108, %f241;
	mov.u32 	%r109, 4;
	shfl.sync.bfly.b32 	%r110|%p26, %r108, %r109, %r101, %r103;
	mov.b32 	%f242, %r110;
	add.f32 	%f243, %f241, %f242;
	mov.b32 	%r111, %f243;
	mov.u32 	%r112, 2;
	shfl.sync.bfly.b32 	%r113|%p27, %r111, %r112, %r101, %r103;
	mov.b32 	%f244, %r113;
	add.f32 	%f245, %f243, %f244;
	mov.b32 	%r114, %f245;
	mov.u32 	%r115, 1;
	shfl.sync.bfly.b32 	%r116|%p28, %r114, %r115, %r101, %r103;
	mov.b32 	%f246, %r116;
	add.f32 	%f247, %f245, %f246;
	st.local.f32 	[%rd3+4], %f247;

$L__BB36_15:
	bar.sync 	0;
	mov.b32 	%r117, %f329;
	mov.u32 	%r118, 31;
	mov.u32 	%r119, 16;
	mov.u32 	%r120, -1;
	shfl.sync.bfly.b32 	%r121|%p30, %r117, %r119, %r118, %r120;
	mov.b32 	%f248, %r121;
	add.f32 	%f249, %f329, %f248;
	mov.b32 	%r122, %f249;
	mov.u32 	%r123, 8;
	shfl.sync.bfly.b32 	%r124|%p31, %r122, %r123, %r118, %r120;
	mov.b32 	%f250, %r124;
	add.f32 	%f251, %f249, %f250;
	mov.b32 	%r125, %f251;
	mov.u32 	%r126, 4;
	shfl.sync.bfly.b32 	%r127|%p32, %r125, %r126, %r118, %r120;
	mov.b32 	%f252, %r127;
	add.f32 	%f253, %f251, %f252;
	mov.b32 	%r128, %f253;
	mov.u32 	%r129, 2;
	shfl.sync.bfly.b32 	%r130|%p33, %r128, %r129, %r118, %r120;
	mov.b32 	%f254, %r130;
	add.f32 	%f255, %f253, %f254;
	mov.b32 	%r131, %f255;
	mov.u32 	%r132, 1;
	shfl.sync.bfly.b32 	%r133|%p34, %r131, %r132, %r118, %r120;
	mov.b32 	%f256, %r133;
	add.f32 	%f257, %f255, %f256;
	st.local.f32 	[%rd3+8], %f257;
	st.shared.f32 	[%r14], %f257;
	bar.sync 	0;
	@%p1 bra 	$L__BB36_17;

	ld.shared.f32 	%f258, [%r4];
	mov.b32 	%r134, %f258;
	shfl.sync.bfly.b32 	%r138|%p35, %r134, %r119, %r118, %r120;
	mov.b32 	%f259, %r138;
	add.f32 	%f260, %f258, %f259;
	mov.b32 	%r139, %f260;
	shfl.sync.bfly.b32 	%r141|%p36, %r139, %r123, %r118, %r120;
	mov.b32 	%f261, %r141;
	add.f32 	%f262, %f260, %f261;
	mov.b32 	%r142, %f262;
	shfl.sync.bfly.b32 	%r144|%p37, %r142, %r126, %r118, %r120;
	mov.b32 	%f263, %r144;
	add.f32 	%f264, %f262, %f263;
	mov.b32 	%r145, %f264;
	shfl.sync.bfly.b32 	%r147|%p38, %r145, %r129, %r118, %r120;
	mov.b32 	%f265, %r147;
	add.f32 	%f266, %f264, %f265;
	mov.b32 	%r148, %f266;
	shfl.sync.bfly.b32 	%r150|%p39, %r148, %r132, %r118, %r120;
	mov.b32 	%f267, %r150;
	add.f32 	%f268, %f266, %f267;
	st.local.f32 	[%rd3+8], %f268;

$L__BB36_17:
	bar.sync 	0;
	mov.b32 	%r151, %f328;
	shfl.sync.bfly.b32 	%r155|%p41, %r151, %r119, %r118, %r120;
	mov.b32 	%f269, %r155;
	add.f32 	%f270, %f328, %f269;
	mov.b32 	%r156, %f270;
	shfl.sync.bfly.b32 	%r158|%p42, %r156, %r123, %r118, %r120;
	mov.b32 	%f271, %r158;
	add.f32 	%f272, %f270, %f271;
	mov.b32 	%r159, %f272;
	shfl.sync.bfly.b32 	%r161|%p43, %r159, %r126, %r118, %r120;
	mov.b32 	%f273, %r161;
	add.f32 	%f274, %f272, %f273;
	mov.b32 	%r162, %f274;
	shfl.sync.bfly.b32 	%r164|%p44, %r162, %r129, %r118, %r120;
	mov.b32 	%f275, %r164;
	add.f32 	%f276, %f274, %f275;
	mov.b32 	%r165, %f276;
	shfl.sync.bfly.b32 	%r167|%p45, %r165, %r132, %r118, %r120;
	mov.b32 	%f277, %r167;
	add.f32 	%f278, %f276, %f277;
	st.local.f32 	[%rd3+12], %f278;
	st.shared.f32 	[%r14], %f278;
	bar.sync 	0;
	@%p1 bra 	$L__BB36_19;

	ld.shared.f32 	%f279, [%r4];
	mov.b32 	%r168, %f279;
	mov.u32 	%r169, 31;
	mov.u32 	%r170, 16;
	mov.u32 	%r171, -1;
	shfl.sync.bfly.b32 	%r172|%p46, %r168, %r170, %r169, %r171;
	mov.b32 	%f280, %r172;
	add.f32 	%f281, %f279, %f280;
	mov.b32 	%r173, %f281;
	mov.u32 	%r174, 8;
	shfl.sync.bfly.b32 	%r175|%p47, %r173, %r174, %r169, %r171;
	mov.b32 	%f282, %r175;
	add.f32 	%f283, %f281, %f282;
	mov.b32 	%r176, %f283;
	mov.u32 	%r177, 4;
	shfl.sync.bfly.b32 	%r178|%p48, %r176, %r177, %r169, %r171;
	mov.b32 	%f284, %r178;
	add.f32 	%f285, %f283, %f284;
	mov.b32 	%r179, %f285;
	mov.u32 	%r180, 2;
	shfl.sync.bfly.b32 	%r181|%p49, %r179, %r180, %r169, %r171;
	mov.b32 	%f286, %r181;
	add.f32 	%f287, %f285, %f286;
	mov.b32 	%r182, %f287;
	mov.u32 	%r183, 1;
	shfl.sync.bfly.b32 	%r184|%p50, %r182, %r183, %r169, %r171;
	mov.b32 	%f288, %r184;
	add.f32 	%f289, %f287, %f288;
	st.local.f32 	[%rd3+12], %f289;

$L__BB36_19:
	bar.sync 	0;
	mov.b32 	%r185, %f327;
	mov.u32 	%r186, 31;
	mov.u32 	%r187, 16;
	mov.u32 	%r188, -1;
	shfl.sync.bfly.b32 	%r189|%p52, %r185, %r187, %r186, %r188;
	mov.b32 	%f290, %r189;
	add.f32 	%f291, %f327, %f290;
	mov.b32 	%r190, %f291;
	mov.u32 	%r191, 8;
	shfl.sync.bfly.b32 	%r192|%p53, %r190, %r191, %r186, %r188;
	mov.b32 	%f292, %r192;
	add.f32 	%f293, %f291, %f292;
	mov.b32 	%r193, %f293;
	mov.u32 	%r194, 4;
	shfl.sync.bfly.b32 	%r195|%p54, %r193, %r194, %r186, %r188;
	mov.b32 	%f294, %r195;
	add.f32 	%f295, %f293, %f294;
	mov.b32 	%r196, %f295;
	mov.u32 	%r197, 2;
	shfl.sync.bfly.b32 	%r198|%p55, %r196, %r197, %r186, %r188;
	mov.b32 	%f296, %r198;
	add.f32 	%f297, %f295, %f296;
	mov.b32 	%r199, %f297;
	mov.u32 	%r200, 1;
	shfl.sync.bfly.b32 	%r201|%p56, %r199, %r200, %r186, %r188;
	mov.b32 	%f298, %r201;
	add.f32 	%f299, %f297, %f298;
	st.local.f32 	[%rd3+16], %f299;
	st.shared.f32 	[%r14], %f299;
	bar.sync 	0;
	@%p1 bra 	$L__BB36_21;

	ld.shared.f32 	%f300, [%r4];
	mov.b32 	%r202, %f300;
	shfl.sync.bfly.b32 	%r206|%p57, %r202, %r187, %r186, %r188;
	mov.b32 	%f301, %r206;
	add.f32 	%f302, %f300, %f301;
	mov.b32 	%r207, %f302;
	shfl.sync.bfly.b32 	%r209|%p58, %r207, %r191, %r186, %r188;
	mov.b32 	%f303, %r209;
	add.f32 	%f304, %f302, %f303;
	mov.b32 	%r210, %f304;
	shfl.sync.bfly.b32 	%r212|%p59, %r210, %r194, %r186, %r188;
	mov.b32 	%f305, %r212;
	add.f32 	%f306, %f304, %f305;
	mov.b32 	%r213, %f306;
	shfl.sync.bfly.b32 	%r215|%p60, %r213, %r197, %r186, %r188;
	mov.b32 	%f307, %r215;
	add.f32 	%f308, %f306, %f307;
	mov.b32 	%r216, %f308;
	shfl.sync.bfly.b32 	%r218|%p61, %r216, %r200, %r186, %r188;
	mov.b32 	%f309, %r218;
	add.f32 	%f310, %f308, %f309;
	st.local.f32 	[%rd3+16], %f310;

$L__BB36_21:
	bar.sync 	0;
	setp.gt.s32 	%p62, %r3, 4;
	@%p62 bra 	$L__BB36_23;

	mul.wide.s32 	%rd64, %r3, 4;
	add.s64 	%rd65, %rd3, %rd64;
	ld.local.f32 	%f311, [%rd65];
	mad.lo.s32 	%r219, %r3, %r17, %r2;
	cvt.s64.s32 	%rd66, %r219;
	mul.lo.s32 	%r220, %r1, %r18;
	cvt.s64.s32 	%rd67, %r220;
	add.s64 	%rd68, %rd67, %rd66;
	cvta.to.global.u64 	%rd69, %rd27;
	shl.b64 	%rd70, %rd68, 2;
	add.s64 	%rd71, %rd69, %rd70;
	st.global.f32 	[%rd71], %f311;

$L__BB36_23:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_6_bs_160
.visible .entry ggml_matvec_f32_ncols_6_bs_160(
	.param .u64 ggml_matvec_f32_ncols_6_bs_160_param_0,
	.param .u64 ggml_matvec_f32_ncols_6_bs_160_param_1,
	.param .u64 ggml_matvec_f32_ncols_6_bs_160_param_2,
	.param .u32 ggml_matvec_f32_ncols_6_bs_160_param_3,
	.param .u32 ggml_matvec_f32_ncols_6_bs_160_param_4,
	.param .u32 ggml_matvec_f32_ncols_6_bs_160_param_5,
	.param .u32 ggml_matvec_f32_ncols_6_bs_160_param_6,
	.param .u32 ggml_matvec_f32_ncols_6_bs_160_param_7,
	.param .u32 ggml_matvec_f32_ncols_6_bs_160_param_8,
	.param .u32 ggml_matvec_f32_ncols_6_bs_160_param_9,
	.param .u32 ggml_matvec_f32_ncols_6_bs_160_param_10,
	.param .u32 ggml_matvec_f32_ncols_6_bs_160_param_11
)
{
	.local .align 8 .b8 	__local_depot37[24];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<75>;
	.reg .f32 	%f<296>;
	.reg .b32 	%r<251>;
	.reg .b64 	%rd<70>;


	mov.u64 	%SPL, __local_depot37;
	ld.param.u64 	%rd20, [ggml_matvec_f32_ncols_6_bs_160_param_0];
	ld.param.u64 	%rd21, [ggml_matvec_f32_ncols_6_bs_160_param_1];
	ld.param.u64 	%rd19, [ggml_matvec_f32_ncols_6_bs_160_param_2];
	ld.param.u32 	%r11, [ggml_matvec_f32_ncols_6_bs_160_param_3];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_6_bs_160_param_5];
	ld.param.u32 	%r12, [ggml_matvec_f32_ncols_6_bs_160_param_6];
	ld.param.u32 	%r13, [ggml_matvec_f32_ncols_6_bs_160_param_7];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_6_bs_160_param_8];
	ld.param.u32 	%r17, [ggml_matvec_f32_ncols_6_bs_160_param_9];
	ld.param.u32 	%r18, [ggml_matvec_f32_ncols_6_bs_160_param_10];
	ld.param.u32 	%r14, [ggml_matvec_f32_ncols_6_bs_160_param_11];
	cvta.to.global.u64 	%rd69, %rd21;
	add.u64 	%rd2, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r19, %r1, %r16;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r20, %r2, %r15;
	mad.lo.s32 	%r21, %r19, %r17, %r20;
	cvt.s64.s32 	%rd3, %r21;
	cvta.to.global.u64 	%rd4, %rd20;
	mul.lo.s32 	%r22, %r1, %r18;
	cvt.s64.s32 	%rd5, %r22;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r23, %r3, 2;
	mov.u32 	%r24, data_mmv;
	add.s32 	%r4, %r24, %r23;
	@%p1 bra 	$L__BB37_2;

	mov.u32 	%r25, 0;
	st.shared.u32 	[%r4], %r25;

$L__BB37_2:
	bar.sync 	0;
	mov.f32 	%f290, 0f00000000;
	st.local.v2.f32 	[%rd2], {%f290, %f290};
	st.local.v2.f32 	[%rd2+8], {%f290, %f290};
	st.local.v2.f32 	[%rd2+16], {%f290, %f290};
	setp.ge.s32 	%p2, %r3, %r11;
	mov.f32 	%f291, %f290;
	mov.f32 	%f292, %f290;
	mov.f32 	%f293, %f290;
	mov.f32 	%f294, %f290;
	mov.f32 	%f295, %f290;
	@%p2 bra 	$L__BB37_9;

	not.b32 	%r26, %r3;
	add.s32 	%r5, %r26, %r11;
	mul.wide.u32 	%rd23, %r5, -858993459;
	shr.u64 	%rd24, %rd23, 39;
	and.b64  	%rd25, %rd24, 1;
	setp.eq.b64 	%p3, %rd25, 1;
	mov.pred 	%p4, 0;
	xor.pred  	%p5, %p3, %p4;
	mov.f32 	%f290, 0f00000000;
	mov.u32 	%r250, %r3;
	@%p5 bra 	$L__BB37_5;

	shl.b64 	%rd26, %rd5, 2;
	add.s64 	%rd27, %rd69, %rd26;
	shl.b64 	%rd28, %rd3, 2;
	add.s64 	%rd29, %rd4, %rd28;
	mul.wide.s32 	%rd30, %r3, 8;
	add.s64 	%rd31, %rd29, %rd30;
	ld.global.nc.v2.f32 	{%f43, %f44}, [%rd31];
	add.s64 	%rd32, %rd27, %rd30;
	ld.global.nc.v2.f32 	{%f47, %f48}, [%rd32];
	fma.rn.f32 	%f51, %f43, %f47, 0f00000000;
	fma.rn.f32 	%f295, %f44, %f48, %f51;
	mul.wide.s32 	%rd33, %r12, 8;
	add.s64 	%rd34, %rd32, %rd33;
	ld.global.nc.v2.f32 	{%f52, %f53}, [%rd34];
	fma.rn.f32 	%f56, %f43, %f52, 0f00000000;
	fma.rn.f32 	%f294, %f44, %f53, %f56;
	st.local.v2.f32 	[%rd2], {%f295, %f294};
	add.s32 	%r27, %r3, %r12;
	add.s32 	%r28, %r27, %r12;
	mul.wide.s32 	%rd35, %r28, 8;
	add.s64 	%rd36, %rd27, %rd35;
	ld.global.nc.v2.f32 	{%f57, %f58}, [%rd36];
	fma.rn.f32 	%f61, %f43, %f57, 0f00000000;
	fma.rn.f32 	%f293, %f44, %f58, %f61;
	add.s64 	%rd37, %rd36, %rd33;
	ld.global.nc.v2.f32 	{%f62, %f63}, [%rd37];
	fma.rn.f32 	%f66, %f43, %f62, 0f00000000;
	fma.rn.f32 	%f292, %f44, %f63, %f66;
	st.local.v2.f32 	[%rd2+8], {%f293, %f292};
	add.s64 	%rd38, %rd37, %rd33;
	ld.global.nc.v2.f32 	{%f67, %f68}, [%rd38];
	fma.rn.f32 	%f71, %f43, %f67, 0f00000000;
	fma.rn.f32 	%f291, %f44, %f68, %f71;
	add.s64 	%rd39, %rd38, %rd33;
	ld.global.nc.v2.f32 	{%f72, %f73}, [%rd39];
	fma.rn.f32 	%f76, %f43, %f72, 0f00000000;
	fma.rn.f32 	%f290, %f44, %f73, %f76;
	st.local.v2.f32 	[%rd2+16], {%f291, %f290};
	add.s32 	%r250, %r3, 160;

$L__BB37_5:
	setp.lt.u32 	%p6, %r5, 160;
	@%p6 bra 	$L__BB37_9;

	add.s32 	%r29, %r250, %r12;
	add.s32 	%r30, %r29, 160;
	mul.wide.s32 	%rd40, %r30, 8;
	shl.b64 	%rd41, %rd5, 2;
	add.s64 	%rd7, %rd40, %rd41;
	shl.b32 	%r31, %r12, 1;
	add.s32 	%r32, %r250, %r31;
	mad.lo.s32 	%r33, %r12, 3, %r250;
	shl.b32 	%r34, %r12, 2;
	add.s32 	%r35, %r250, %r34;
	mad.lo.s32 	%r36, %r12, 5, %r250;
	mul.wide.s32 	%rd42, %r32, 8;
	add.s64 	%rd8, %rd42, %rd41;
	mul.wide.s32 	%rd43, %r33, 8;
	add.s64 	%rd9, %rd43, %rd41;
	mul.wide.s32 	%rd44, %r35, 8;
	add.s64 	%rd10, %rd44, %rd41;
	mul.wide.s32 	%rd45, %r36, 8;
	add.s64 	%rd11, %rd45, %rd41;
	mul.wide.s32 	%rd46, %r250, 2;
	add.s64 	%rd47, %rd46, %rd3;
	shl.b64 	%rd48, %rd47, 2;
	add.s64 	%rd49, %rd4, %rd48;
	add.s64 	%rd68, %rd49, 1280;
	mul.wide.s32 	%rd50, %r250, 8;
	mul.wide.s32 	%rd51, %r12, 8;
	add.s64 	%rd52, %rd50, %rd51;
	add.s64 	%rd13, %rd52, %rd41;
	add.s64 	%rd14, %rd50, %rd41;

$L__BB37_7:
	ld.global.nc.v2.f32 	{%f77, %f78}, [%rd68+-1280];
	add.s64 	%rd53, %rd69, %rd14;
	ld.global.nc.v2.f32 	{%f81, %f82}, [%rd53];
	fma.rn.f32 	%f85, %f77, %f81, %f295;
	fma.rn.f32 	%f86, %f78, %f82, %f85;
	add.s64 	%rd54, %rd69, %rd13;
	ld.global.nc.v2.f32 	{%f87, %f88}, [%rd54];
	fma.rn.f32 	%f91, %f77, %f87, %f294;
	fma.rn.f32 	%f92, %f78, %f88, %f91;
	add.s64 	%rd55, %rd69, %rd8;
	ld.global.nc.v2.f32 	{%f93, %f94}, [%rd55];
	fma.rn.f32 	%f97, %f77, %f93, %f293;
	fma.rn.f32 	%f98, %f78, %f94, %f97;
	add.s64 	%rd56, %rd69, %rd9;
	ld.global.nc.v2.f32 	{%f99, %f100}, [%rd56];
	fma.rn.f32 	%f103, %f77, %f99, %f292;
	fma.rn.f32 	%f104, %f78, %f100, %f103;
	add.s64 	%rd57, %rd69, %rd10;
	ld.global.nc.v2.f32 	{%f105, %f106}, [%rd57];
	fma.rn.f32 	%f109, %f77, %f105, %f291;
	fma.rn.f32 	%f110, %f78, %f106, %f109;
	add.s64 	%rd58, %rd69, %rd11;
	ld.global.nc.v2.f32 	{%f111, %f112}, [%rd58];
	fma.rn.f32 	%f115, %f77, %f111, %f290;
	fma.rn.f32 	%f116, %f78, %f112, %f115;
	ld.global.nc.v2.f32 	{%f117, %f118}, [%rd68];
	ld.global.nc.v2.f32 	{%f121, %f122}, [%rd53+1280];
	fma.rn.f32 	%f125, %f117, %f121, %f86;
	fma.rn.f32 	%f295, %f118, %f122, %f125;
	add.s64 	%rd59, %rd69, %rd7;
	ld.global.nc.v2.f32 	{%f126, %f127}, [%rd59];
	fma.rn.f32 	%f130, %f117, %f126, %f92;
	fma.rn.f32 	%f294, %f118, %f127, %f130;
	ld.global.nc.v2.f32 	{%f131, %f132}, [%rd55+1280];
	fma.rn.f32 	%f135, %f117, %f131, %f98;
	fma.rn.f32 	%f293, %f118, %f132, %f135;
	ld.global.nc.v2.f32 	{%f136, %f137}, [%rd56+1280];
	fma.rn.f32 	%f140, %f117, %f136, %f104;
	fma.rn.f32 	%f292, %f118, %f137, %f140;
	ld.global.nc.v2.f32 	{%f141, %f142}, [%rd57+1280];
	fma.rn.f32 	%f145, %f117, %f141, %f110;
	fma.rn.f32 	%f291, %f118, %f142, %f145;
	ld.global.nc.v2.f32 	{%f146, %f147}, [%rd58+1280];
	fma.rn.f32 	%f150, %f117, %f146, %f116;
	fma.rn.f32 	%f290, %f118, %f147, %f150;
	add.s64 	%rd69, %rd69, 2560;
	add.s64 	%rd68, %rd68, 2560;
	add.s32 	%r250, %r250, 320;
	setp.lt.s32 	%p7, %r250, %r11;
	@%p7 bra 	$L__BB37_7;

	st.local.v2.f32 	[%rd2], {%f295, %f294};
	st.local.v2.f32 	[%rd2+8], {%f293, %f292};
	st.local.v2.f32 	[%rd2+16], {%f291, %f290};

$L__BB37_9:
	shr.s32 	%r37, %r3, 31;
	shr.u32 	%r38, %r37, 27;
	add.s32 	%r39, %r3, %r38;
	shr.s32 	%r40, %r39, 5;
	shl.b32 	%r41, %r40, 2;
	add.s32 	%r10, %r24, %r41;
	mov.u32 	%r43, 2;
	mov.b32 	%r44, %f295;
	mov.u32 	%r45, 31;
	mov.u32 	%r46, 16;
	mov.u32 	%r47, -1;
	shfl.sync.bfly.b32 	%r48|%p8, %r44, %r46, %r45, %r47;
	mov.b32 	%f151, %r48;
	add.f32 	%f152, %f295, %f151;
	mov.b32 	%r49, %f152;
	mov.u32 	%r50, 8;
	shfl.sync.bfly.b32 	%r51|%p9, %r49, %r50, %r45, %r47;
	mov.b32 	%f153, %r51;
	add.f32 	%f154, %f152, %f153;
	mov.b32 	%r52, %f154;
	mov.u32 	%r53, 4;
	shfl.sync.bfly.b32 	%r54|%p10, %r52, %r53, %r45, %r47;
	mov.b32 	%f155, %r54;
	add.f32 	%f156, %f154, %f155;
	mov.b32 	%r55, %f156;
	shfl.sync.bfly.b32 	%r56|%p11, %r55, %r43, %r45, %r47;
	mov.b32 	%f157, %r56;
	add.f32 	%f158, %f156, %f157;
	mov.b32 	%r57, %f158;
	mov.u32 	%r58, 1;
	shfl.sync.bfly.b32 	%r59|%p12, %r57, %r58, %r45, %r47;
	mov.b32 	%f159, %r59;
	add.f32 	%f160, %f158, %f159;
	st.local.f32 	[%rd2], %f160;
	st.shared.f32 	[%r10], %f160;
	bar.sync 	0;
	@%p1 bra 	$L__BB37_11;

	ld.shared.f32 	%f161, [%r4];
	mov.b32 	%r60, %f161;
	shfl.sync.bfly.b32 	%r64|%p14, %r60, %r46, %r45, %r47;
	mov.b32 	%f162, %r64;
	add.f32 	%f163, %f161, %f162;
	mov.b32 	%r65, %f163;
	shfl.sync.bfly.b32 	%r67|%p15, %r65, %r50, %r45, %r47;
	mov.b32 	%f164, %r67;
	add.f32 	%f165, %f163, %f164;
	mov.b32 	%r68, %f165;
	shfl.sync.bfly.b32 	%r70|%p16, %r68, %r53, %r45, %r47;
	mov.b32 	%f166, %r70;
	add.f32 	%f167, %f165, %f166;
	mov.b32 	%r71, %f167;
	shfl.sync.bfly.b32 	%r73|%p17, %r71, %r43, %r45, %r47;
	mov.b32 	%f168, %r73;
	add.f32 	%f169, %f167, %f168;
	mov.b32 	%r74, %f169;
	shfl.sync.bfly.b32 	%r76|%p18, %r74, %r58, %r45, %r47;
	mov.b32 	%f170, %r76;
	add.f32 	%f171, %f169, %f170;
	st.local.f32 	[%rd2], %f171;

$L__BB37_11:
	bar.sync 	0;
	mov.b32 	%r77, %f294;
	shfl.sync.bfly.b32 	%r81|%p20, %r77, %r46, %r45, %r47;
	mov.b32 	%f172, %r81;
	add.f32 	%f173, %f294, %f172;
	mov.b32 	%r82, %f173;
	shfl.sync.bfly.b32 	%r84|%p21, %r82, %r50, %r45, %r47;
	mov.b32 	%f174, %r84;
	add.f32 	%f175, %f173, %f174;
	mov.b32 	%r85, %f175;
	shfl.sync.bfly.b32 	%r87|%p22, %r85, %r53, %r45, %r47;
	mov.b32 	%f176, %r87;
	add.f32 	%f177, %f175, %f176;
	mov.b32 	%r88, %f177;
	shfl.sync.bfly.b32 	%r90|%p23, %r88, %r43, %r45, %r47;
	mov.b32 	%f178, %r90;
	add.f32 	%f179, %f177, %f178;
	mov.b32 	%r91, %f179;
	shfl.sync.bfly.b32 	%r93|%p24, %r91, %r58, %r45, %r47;
	mov.b32 	%f180, %r93;
	add.f32 	%f181, %f179, %f180;
	st.local.f32 	[%rd2+4], %f181;
	st.shared.f32 	[%r10], %f181;
	bar.sync 	0;
	@%p1 bra 	$L__BB37_13;

	ld.shared.f32 	%f182, [%r4];
	mov.b32 	%r94, %f182;
	mov.u32 	%r95, 31;
	mov.u32 	%r96, 16;
	mov.u32 	%r97, -1;
	shfl.sync.bfly.b32 	%r98|%p25, %r94, %r96, %r95, %r97;
	mov.b32 	%f183, %r98;
	add.f32 	%f184, %f182, %f183;
	mov.b32 	%r99, %f184;
	mov.u32 	%r100, 8;
	shfl.sync.bfly.b32 	%r101|%p26, %r99, %r100, %r95, %r97;
	mov.b32 	%f185, %r101;
	add.f32 	%f186, %f184, %f185;
	mov.b32 	%r102, %f186;
	mov.u32 	%r103, 4;
	shfl.sync.bfly.b32 	%r104|%p27, %r102, %r103, %r95, %r97;
	mov.b32 	%f187, %r104;
	add.f32 	%f188, %f186, %f187;
	mov.b32 	%r105, %f188;
	mov.u32 	%r106, 2;
	shfl.sync.bfly.b32 	%r107|%p28, %r105, %r106, %r95, %r97;
	mov.b32 	%f189, %r107;
	add.f32 	%f190, %f188, %f189;
	mov.b32 	%r108, %f190;
	mov.u32 	%r109, 1;
	shfl.sync.bfly.b32 	%r110|%p29, %r108, %r109, %r95, %r97;
	mov.b32 	%f191, %r110;
	add.f32 	%f192, %f190, %f191;
	st.local.f32 	[%rd2+4], %f192;

$L__BB37_13:
	bar.sync 	0;
	mov.b32 	%r111, %f293;
	mov.u32 	%r112, 31;
	mov.u32 	%r113, 16;
	mov.u32 	%r114, -1;
	shfl.sync.bfly.b32 	%r115|%p31, %r111, %r113, %r112, %r114;
	mov.b32 	%f193, %r115;
	add.f32 	%f194, %f293, %f193;
	mov.b32 	%r116, %f194;
	mov.u32 	%r117, 8;
	shfl.sync.bfly.b32 	%r118|%p32, %r116, %r117, %r112, %r114;
	mov.b32 	%f195, %r118;
	add.f32 	%f196, %f194, %f195;
	mov.b32 	%r119, %f196;
	mov.u32 	%r120, 4;
	shfl.sync.bfly.b32 	%r121|%p33, %r119, %r120, %r112, %r114;
	mov.b32 	%f197, %r121;
	add.f32 	%f198, %f196, %f197;
	mov.b32 	%r122, %f198;
	mov.u32 	%r123, 2;
	shfl.sync.bfly.b32 	%r124|%p34, %r122, %r123, %r112, %r114;
	mov.b32 	%f199, %r124;
	add.f32 	%f200, %f198, %f199;
	mov.b32 	%r125, %f200;
	mov.u32 	%r126, 1;
	shfl.sync.bfly.b32 	%r127|%p35, %r125, %r126, %r112, %r114;
	mov.b32 	%f201, %r127;
	add.f32 	%f202, %f200, %f201;
	st.local.f32 	[%rd2+8], %f202;
	st.shared.f32 	[%r10], %f202;
	bar.sync 	0;
	@%p1 bra 	$L__BB37_15;

	ld.shared.f32 	%f203, [%r4];
	mov.b32 	%r128, %f203;
	shfl.sync.bfly.b32 	%r132|%p36, %r128, %r113, %r112, %r114;
	mov.b32 	%f204, %r132;
	add.f32 	%f205, %f203, %f204;
	mov.b32 	%r133, %f205;
	shfl.sync.bfly.b32 	%r135|%p37, %r133, %r117, %r112, %r114;
	mov.b32 	%f206, %r135;
	add.f32 	%f207, %f205, %f206;
	mov.b32 	%r136, %f207;
	shfl.sync.bfly.b32 	%r138|%p38, %r136, %r120, %r112, %r114;
	mov.b32 	%f208, %r138;
	add.f32 	%f209, %f207, %f208;
	mov.b32 	%r139, %f209;
	shfl.sync.bfly.b32 	%r141|%p39, %r139, %r123, %r112, %r114;
	mov.b32 	%f210, %r141;
	add.f32 	%f211, %f209, %f210;
	mov.b32 	%r142, %f211;
	shfl.sync.bfly.b32 	%r144|%p40, %r142, %r126, %r112, %r114;
	mov.b32 	%f212, %r144;
	add.f32 	%f213, %f211, %f212;
	st.local.f32 	[%rd2+8], %f213;

$L__BB37_15:
	bar.sync 	0;
	mov.b32 	%r145, %f292;
	shfl.sync.bfly.b32 	%r149|%p42, %r145, %r113, %r112, %r114;
	mov.b32 	%f214, %r149;
	add.f32 	%f215, %f292, %f214;
	mov.b32 	%r150, %f215;
	shfl.sync.bfly.b32 	%r152|%p43, %r150, %r117, %r112, %r114;
	mov.b32 	%f216, %r152;
	add.f32 	%f217, %f215, %f216;
	mov.b32 	%r153, %f217;
	shfl.sync.bfly.b32 	%r155|%p44, %r153, %r120, %r112, %r114;
	mov.b32 	%f218, %r155;
	add.f32 	%f219, %f217, %f218;
	mov.b32 	%r156, %f219;
	shfl.sync.bfly.b32 	%r158|%p45, %r156, %r123, %r112, %r114;
	mov.b32 	%f220, %r158;
	add.f32 	%f221, %f219, %f220;
	mov.b32 	%r159, %f221;
	shfl.sync.bfly.b32 	%r161|%p46, %r159, %r126, %r112, %r114;
	mov.b32 	%f222, %r161;
	add.f32 	%f223, %f221, %f222;
	st.local.f32 	[%rd2+12], %f223;
	st.shared.f32 	[%r10], %f223;
	bar.sync 	0;
	@%p1 bra 	$L__BB37_17;

	ld.shared.f32 	%f224, [%r4];
	mov.b32 	%r162, %f224;
	mov.u32 	%r163, 31;
	mov.u32 	%r164, 16;
	mov.u32 	%r165, -1;
	shfl.sync.bfly.b32 	%r166|%p47, %r162, %r164, %r163, %r165;
	mov.b32 	%f225, %r166;
	add.f32 	%f226, %f224, %f225;
	mov.b32 	%r167, %f226;
	mov.u32 	%r168, 8;
	shfl.sync.bfly.b32 	%r169|%p48, %r167, %r168, %r163, %r165;
	mov.b32 	%f227, %r169;
	add.f32 	%f228, %f226, %f227;
	mov.b32 	%r170, %f228;
	mov.u32 	%r171, 4;
	shfl.sync.bfly.b32 	%r172|%p49, %r170, %r171, %r163, %r165;
	mov.b32 	%f229, %r172;
	add.f32 	%f230, %f228, %f229;
	mov.b32 	%r173, %f230;
	mov.u32 	%r174, 2;
	shfl.sync.bfly.b32 	%r175|%p50, %r173, %r174, %r163, %r165;
	mov.b32 	%f231, %r175;
	add.f32 	%f232, %f230, %f231;
	mov.b32 	%r176, %f232;
	mov.u32 	%r177, 1;
	shfl.sync.bfly.b32 	%r178|%p51, %r176, %r177, %r163, %r165;
	mov.b32 	%f233, %r178;
	add.f32 	%f234, %f232, %f233;
	st.local.f32 	[%rd2+12], %f234;

$L__BB37_17:
	bar.sync 	0;
	mov.b32 	%r179, %f291;
	mov.u32 	%r180, 31;
	mov.u32 	%r181, 16;
	mov.u32 	%r182, -1;
	shfl.sync.bfly.b32 	%r183|%p53, %r179, %r181, %r180, %r182;
	mov.b32 	%f235, %r183;
	add.f32 	%f236, %f291, %f235;
	mov.b32 	%r184, %f236;
	mov.u32 	%r185, 8;
	shfl.sync.bfly.b32 	%r186|%p54, %r184, %r185, %r180, %r182;
	mov.b32 	%f237, %r186;
	add.f32 	%f238, %f236, %f237;
	mov.b32 	%r187, %f238;
	mov.u32 	%r188, 4;
	shfl.sync.bfly.b32 	%r189|%p55, %r187, %r188, %r180, %r182;
	mov.b32 	%f239, %r189;
	add.f32 	%f240, %f238, %f239;
	mov.b32 	%r190, %f240;
	mov.u32 	%r191, 2;
	shfl.sync.bfly.b32 	%r192|%p56, %r190, %r191, %r180, %r182;
	mov.b32 	%f241, %r192;
	add.f32 	%f242, %f240, %f241;
	mov.b32 	%r193, %f242;
	mov.u32 	%r194, 1;
	shfl.sync.bfly.b32 	%r195|%p57, %r193, %r194, %r180, %r182;
	mov.b32 	%f243, %r195;
	add.f32 	%f244, %f242, %f243;
	st.local.f32 	[%rd2+16], %f244;
	st.shared.f32 	[%r10], %f244;
	bar.sync 	0;
	@%p1 bra 	$L__BB37_19;

	ld.shared.f32 	%f245, [%r4];
	mov.b32 	%r196, %f245;
	shfl.sync.bfly.b32 	%r200|%p58, %r196, %r181, %r180, %r182;
	mov.b32 	%f246, %r200;
	add.f32 	%f247, %f245, %f246;
	mov.b32 	%r201, %f247;
	shfl.sync.bfly.b32 	%r203|%p59, %r201, %r185, %r180, %r182;
	mov.b32 	%f248, %r203;
	add.f32 	%f249, %f247, %f248;
	mov.b32 	%r204, %f249;
	shfl.sync.bfly.b32 	%r206|%p60, %r204, %r188, %r180, %r182;
	mov.b32 	%f250, %r206;
	add.f32 	%f251, %f249, %f250;
	mov.b32 	%r207, %f251;
	shfl.sync.bfly.b32 	%r209|%p61, %r207, %r191, %r180, %r182;
	mov.b32 	%f252, %r209;
	add.f32 	%f253, %f251, %f252;
	mov.b32 	%r210, %f253;
	shfl.sync.bfly.b32 	%r212|%p62, %r210, %r194, %r180, %r182;
	mov.b32 	%f254, %r212;
	add.f32 	%f255, %f253, %f254;
	st.local.f32 	[%rd2+16], %f255;

$L__BB37_19:
	bar.sync 	0;
	mov.b32 	%r213, %f290;
	shfl.sync.bfly.b32 	%r217|%p64, %r213, %r181, %r180, %r182;
	mov.b32 	%f256, %r217;
	add.f32 	%f257, %f290, %f256;
	mov.b32 	%r218, %f257;
	shfl.sync.bfly.b32 	%r220|%p65, %r218, %r185, %r180, %r182;
	mov.b32 	%f258, %r220;
	add.f32 	%f259, %f257, %f258;
	mov.b32 	%r221, %f259;
	shfl.sync.bfly.b32 	%r223|%p66, %r221, %r188, %r180, %r182;
	mov.b32 	%f260, %r223;
	add.f32 	%f261, %f259, %f260;
	mov.b32 	%r224, %f261;
	shfl.sync.bfly.b32 	%r226|%p67, %r224, %r191, %r180, %r182;
	mov.b32 	%f262, %r226;
	add.f32 	%f263, %f261, %f262;
	mov.b32 	%r227, %f263;
	shfl.sync.bfly.b32 	%r229|%p68, %r227, %r194, %r180, %r182;
	mov.b32 	%f264, %r229;
	add.f32 	%f265, %f263, %f264;
	st.local.f32 	[%rd2+20], %f265;
	st.shared.f32 	[%r10], %f265;
	bar.sync 	0;
	@%p1 bra 	$L__BB37_21;

	ld.shared.f32 	%f266, [%r4];
	mov.b32 	%r230, %f266;
	mov.u32 	%r231, 31;
	mov.u32 	%r232, 16;
	mov.u32 	%r233, -1;
	shfl.sync.bfly.b32 	%r234|%p69, %r230, %r232, %r231, %r233;
	mov.b32 	%f267, %r234;
	add.f32 	%f268, %f266, %f267;
	mov.b32 	%r235, %f268;
	mov.u32 	%r236, 8;
	shfl.sync.bfly.b32 	%r237|%p70, %r235, %r236, %r231, %r233;
	mov.b32 	%f269, %r237;
	add.f32 	%f270, %f268, %f269;
	mov.b32 	%r238, %f270;
	mov.u32 	%r239, 4;
	shfl.sync.bfly.b32 	%r240|%p71, %r238, %r239, %r231, %r233;
	mov.b32 	%f271, %r240;
	add.f32 	%f272, %f270, %f271;
	mov.b32 	%r241, %f272;
	mov.u32 	%r242, 2;
	shfl.sync.bfly.b32 	%r243|%p72, %r241, %r242, %r231, %r233;
	mov.b32 	%f273, %r243;
	add.f32 	%f274, %f272, %f273;
	mov.b32 	%r244, %f274;
	mov.u32 	%r245, 1;
	shfl.sync.bfly.b32 	%r246|%p73, %r244, %r245, %r231, %r233;
	mov.b32 	%f275, %r246;
	add.f32 	%f276, %f274, %f275;
	st.local.f32 	[%rd2+20], %f276;

$L__BB37_21:
	bar.sync 	0;
	setp.gt.s32 	%p74, %r3, 5;
	@%p74 bra 	$L__BB37_23;

	mul.wide.s32 	%rd60, %r3, 4;
	add.s64 	%rd61, %rd2, %rd60;
	ld.local.f32 	%f277, [%rd61];
	mad.lo.s32 	%r247, %r3, %r13, %r2;
	cvt.s64.s32 	%rd62, %r247;
	mul.lo.s32 	%r248, %r1, %r14;
	cvt.s64.s32 	%rd63, %r248;
	add.s64 	%rd64, %rd63, %rd62;
	cvta.to.global.u64 	%rd65, %rd19;
	shl.b64 	%rd66, %rd64, 2;
	add.s64 	%rd67, %rd65, %rd66;
	st.global.f32 	[%rd67], %f277;

$L__BB37_23:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_7_bs_160
.visible .entry ggml_matvec_f32_ncols_7_bs_160(
	.param .u64 ggml_matvec_f32_ncols_7_bs_160_param_0,
	.param .u64 ggml_matvec_f32_ncols_7_bs_160_param_1,
	.param .u64 ggml_matvec_f32_ncols_7_bs_160_param_2,
	.param .u32 ggml_matvec_f32_ncols_7_bs_160_param_3,
	.param .u32 ggml_matvec_f32_ncols_7_bs_160_param_4,
	.param .u32 ggml_matvec_f32_ncols_7_bs_160_param_5,
	.param .u32 ggml_matvec_f32_ncols_7_bs_160_param_6,
	.param .u32 ggml_matvec_f32_ncols_7_bs_160_param_7,
	.param .u32 ggml_matvec_f32_ncols_7_bs_160_param_8,
	.param .u32 ggml_matvec_f32_ncols_7_bs_160_param_9,
	.param .u32 ggml_matvec_f32_ncols_7_bs_160_param_10,
	.param .u32 ggml_matvec_f32_ncols_7_bs_160_param_11
)
{
	.local .align 4 .b8 	__local_depot38[28];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<86>;
	.reg .f32 	%f<343>;
	.reg .b32 	%r<287>;
	.reg .b64 	%rd<74>;


	mov.u64 	%SPL, __local_depot38;
	ld.param.u64 	%rd21, [ggml_matvec_f32_ncols_7_bs_160_param_0];
	ld.param.u64 	%rd22, [ggml_matvec_f32_ncols_7_bs_160_param_1];
	ld.param.u64 	%rd20, [ggml_matvec_f32_ncols_7_bs_160_param_2];
	ld.param.u32 	%r11, [ggml_matvec_f32_ncols_7_bs_160_param_3];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_7_bs_160_param_5];
	ld.param.u32 	%r12, [ggml_matvec_f32_ncols_7_bs_160_param_6];
	ld.param.u32 	%r13, [ggml_matvec_f32_ncols_7_bs_160_param_7];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_7_bs_160_param_8];
	ld.param.u32 	%r17, [ggml_matvec_f32_ncols_7_bs_160_param_9];
	ld.param.u32 	%r18, [ggml_matvec_f32_ncols_7_bs_160_param_10];
	ld.param.u32 	%r14, [ggml_matvec_f32_ncols_7_bs_160_param_11];
	cvta.to.global.u64 	%rd73, %rd22;
	add.u64 	%rd2, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r19, %r1, %r16;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r20, %r2, %r15;
	mad.lo.s32 	%r21, %r19, %r17, %r20;
	cvt.s64.s32 	%rd3, %r21;
	cvta.to.global.u64 	%rd4, %rd21;
	mul.lo.s32 	%r22, %r1, %r18;
	cvt.s64.s32 	%rd5, %r22;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r23, %r3, 2;
	mov.u32 	%r24, data_mmv;
	add.s32 	%r4, %r24, %r23;
	@%p1 bra 	$L__BB38_2;

	mov.u32 	%r25, 0;
	st.shared.u32 	[%r4], %r25;

$L__BB38_2:
	bar.sync 	0;
	mov.f32 	%f336, 0f00000000;
	mov.u32 	%r26, 0;
	st.local.u32 	[%rd2], %r26;
	st.local.u32 	[%rd2+4], %r26;
	st.local.u32 	[%rd2+8], %r26;
	st.local.u32 	[%rd2+12], %r26;
	st.local.u32 	[%rd2+16], %r26;
	st.local.u32 	[%rd2+20], %r26;
	st.local.u32 	[%rd2+24], %r26;
	setp.ge.s32 	%p2, %r3, %r11;
	mov.f32 	%f337, %f336;
	mov.f32 	%f338, %f336;
	mov.f32 	%f339, %f336;
	mov.f32 	%f340, %f336;
	mov.f32 	%f341, %f336;
	mov.f32 	%f342, %f336;
	@%p2 bra 	$L__BB38_9;

	not.b32 	%r27, %r3;
	add.s32 	%r5, %r27, %r11;
	mul.wide.u32 	%rd24, %r5, -858993459;
	shr.u64 	%rd25, %rd24, 39;
	and.b64  	%rd26, %rd25, 1;
	setp.eq.b64 	%p3, %rd26, 1;
	mov.pred 	%p4, 0;
	xor.pred  	%p5, %p3, %p4;
	mov.f32 	%f336, 0f00000000;
	mov.u32 	%r286, %r3;
	@%p5 bra 	$L__BB38_5;

	shl.b64 	%rd27, %rd5, 2;
	add.s64 	%rd28, %rd73, %rd27;
	shl.b64 	%rd29, %rd3, 2;
	add.s64 	%rd30, %rd4, %rd29;
	mul.wide.s32 	%rd31, %r3, 8;
	add.s64 	%rd32, %rd30, %rd31;
	ld.global.nc.v2.f32 	{%f50, %f51}, [%rd32];
	add.s64 	%rd33, %rd28, %rd31;
	ld.global.nc.v2.f32 	{%f54, %f55}, [%rd33];
	fma.rn.f32 	%f58, %f50, %f54, 0f00000000;
	fma.rn.f32 	%f342, %f51, %f55, %f58;
	st.local.f32 	[%rd2], %f342;
	mul.wide.s32 	%rd34, %r12, 8;
	add.s64 	%rd35, %rd33, %rd34;
	ld.global.nc.v2.f32 	{%f59, %f60}, [%rd35];
	fma.rn.f32 	%f63, %f50, %f59, 0f00000000;
	fma.rn.f32 	%f341, %f51, %f60, %f63;
	st.local.f32 	[%rd2+4], %f341;
	add.s32 	%r28, %r3, %r12;
	add.s32 	%r29, %r28, %r12;
	mul.wide.s32 	%rd36, %r29, 8;
	add.s64 	%rd37, %rd28, %rd36;
	ld.global.nc.v2.f32 	{%f64, %f65}, [%rd37];
	fma.rn.f32 	%f68, %f50, %f64, 0f00000000;
	fma.rn.f32 	%f340, %f51, %f65, %f68;
	st.local.f32 	[%rd2+8], %f340;
	add.s64 	%rd38, %rd37, %rd34;
	ld.global.nc.v2.f32 	{%f69, %f70}, [%rd38];
	fma.rn.f32 	%f73, %f50, %f69, 0f00000000;
	fma.rn.f32 	%f339, %f51, %f70, %f73;
	st.local.f32 	[%rd2+12], %f339;
	add.s64 	%rd39, %rd38, %rd34;
	ld.global.nc.v2.f32 	{%f74, %f75}, [%rd39];
	fma.rn.f32 	%f78, %f50, %f74, 0f00000000;
	fma.rn.f32 	%f338, %f51, %f75, %f78;
	st.local.f32 	[%rd2+16], %f338;
	add.s64 	%rd40, %rd39, %rd34;
	ld.global.nc.v2.f32 	{%f79, %f80}, [%rd40];
	fma.rn.f32 	%f83, %f50, %f79, 0f00000000;
	fma.rn.f32 	%f337, %f51, %f80, %f83;
	st.local.f32 	[%rd2+20], %f337;
	add.s64 	%rd41, %rd40, %rd34;
	ld.global.nc.v2.f32 	{%f84, %f85}, [%rd41];
	fma.rn.f32 	%f88, %f50, %f84, 0f00000000;
	fma.rn.f32 	%f336, %f51, %f85, %f88;
	st.local.f32 	[%rd2+24], %f336;
	add.s32 	%r286, %r3, 160;

$L__BB38_5:
	setp.lt.u32 	%p6, %r5, 160;
	@%p6 bra 	$L__BB38_9;

	add.s32 	%r30, %r286, %r12;
	add.s32 	%r31, %r30, 160;
	mul.wide.s32 	%rd42, %r31, 8;
	shl.b64 	%rd43, %rd5, 2;
	add.s64 	%rd7, %rd42, %rd43;
	shl.b32 	%r32, %r12, 1;
	add.s32 	%r33, %r286, %r32;
	mad.lo.s32 	%r34, %r12, 3, %r286;
	shl.b32 	%r35, %r12, 2;
	add.s32 	%r36, %r286, %r35;
	mad.lo.s32 	%r37, %r12, 5, %r286;
	mad.lo.s32 	%r38, %r12, 6, %r286;
	mul.wide.s32 	%rd44, %r33, 8;
	add.s64 	%rd8, %rd44, %rd43;
	mul.wide.s32 	%rd45, %r34, 8;
	add.s64 	%rd9, %rd45, %rd43;
	mul.wide.s32 	%rd46, %r36, 8;
	add.s64 	%rd10, %rd46, %rd43;
	mul.wide.s32 	%rd47, %r37, 8;
	add.s64 	%rd11, %rd47, %rd43;
	mul.wide.s32 	%rd48, %r38, 8;
	add.s64 	%rd12, %rd48, %rd43;
	mul.wide.s32 	%rd49, %r286, 2;
	add.s64 	%rd50, %rd49, %rd3;
	shl.b64 	%rd51, %rd50, 2;
	add.s64 	%rd52, %rd4, %rd51;
	add.s64 	%rd72, %rd52, 1280;
	mul.wide.s32 	%rd53, %r286, 8;
	mul.wide.s32 	%rd54, %r12, 8;
	add.s64 	%rd55, %rd53, %rd54;
	add.s64 	%rd14, %rd55, %rd43;
	add.s64 	%rd15, %rd53, %rd43;

$L__BB38_7:
	ld.global.nc.v2.f32 	{%f89, %f90}, [%rd72+-1280];
	add.s64 	%rd56, %rd73, %rd15;
	ld.global.nc.v2.f32 	{%f93, %f94}, [%rd56];
	fma.rn.f32 	%f97, %f89, %f93, %f342;
	fma.rn.f32 	%f98, %f90, %f94, %f97;
	add.s64 	%rd57, %rd73, %rd14;
	ld.global.nc.v2.f32 	{%f99, %f100}, [%rd57];
	fma.rn.f32 	%f103, %f89, %f99, %f341;
	fma.rn.f32 	%f104, %f90, %f100, %f103;
	add.s64 	%rd58, %rd73, %rd8;
	ld.global.nc.v2.f32 	{%f105, %f106}, [%rd58];
	fma.rn.f32 	%f109, %f89, %f105, %f340;
	fma.rn.f32 	%f110, %f90, %f106, %f109;
	add.s64 	%rd59, %rd73, %rd9;
	ld.global.nc.v2.f32 	{%f111, %f112}, [%rd59];
	fma.rn.f32 	%f115, %f89, %f111, %f339;
	fma.rn.f32 	%f116, %f90, %f112, %f115;
	add.s64 	%rd60, %rd73, %rd10;
	ld.global.nc.v2.f32 	{%f117, %f118}, [%rd60];
	fma.rn.f32 	%f121, %f89, %f117, %f338;
	fma.rn.f32 	%f122, %f90, %f118, %f121;
	add.s64 	%rd61, %rd73, %rd11;
	ld.global.nc.v2.f32 	{%f123, %f124}, [%rd61];
	fma.rn.f32 	%f127, %f89, %f123, %f337;
	fma.rn.f32 	%f128, %f90, %f124, %f127;
	add.s64 	%rd62, %rd73, %rd12;
	ld.global.nc.v2.f32 	{%f129, %f130}, [%rd62];
	fma.rn.f32 	%f133, %f89, %f129, %f336;
	fma.rn.f32 	%f134, %f90, %f130, %f133;
	ld.global.nc.v2.f32 	{%f135, %f136}, [%rd72];
	ld.global.nc.v2.f32 	{%f139, %f140}, [%rd56+1280];
	fma.rn.f32 	%f143, %f135, %f139, %f98;
	fma.rn.f32 	%f342, %f136, %f140, %f143;
	add.s64 	%rd63, %rd73, %rd7;
	ld.global.nc.v2.f32 	{%f144, %f145}, [%rd63];
	fma.rn.f32 	%f148, %f135, %f144, %f104;
	fma.rn.f32 	%f341, %f136, %f145, %f148;
	ld.global.nc.v2.f32 	{%f149, %f150}, [%rd58+1280];
	fma.rn.f32 	%f153, %f135, %f149, %f110;
	fma.rn.f32 	%f340, %f136, %f150, %f153;
	ld.global.nc.v2.f32 	{%f154, %f155}, [%rd59+1280];
	fma.rn.f32 	%f158, %f135, %f154, %f116;
	fma.rn.f32 	%f339, %f136, %f155, %f158;
	ld.global.nc.v2.f32 	{%f159, %f160}, [%rd60+1280];
	fma.rn.f32 	%f163, %f135, %f159, %f122;
	fma.rn.f32 	%f338, %f136, %f160, %f163;
	ld.global.nc.v2.f32 	{%f164, %f165}, [%rd61+1280];
	fma.rn.f32 	%f168, %f135, %f164, %f128;
	fma.rn.f32 	%f337, %f136, %f165, %f168;
	ld.global.nc.v2.f32 	{%f169, %f170}, [%rd62+1280];
	fma.rn.f32 	%f173, %f135, %f169, %f134;
	fma.rn.f32 	%f336, %f136, %f170, %f173;
	add.s64 	%rd73, %rd73, 2560;
	add.s64 	%rd72, %rd72, 2560;
	add.s32 	%r286, %r286, 320;
	setp.lt.s32 	%p7, %r286, %r11;
	@%p7 bra 	$L__BB38_7;

	st.local.f32 	[%rd2], %f342;
	st.local.f32 	[%rd2+4], %f341;
	st.local.f32 	[%rd2+8], %f340;
	st.local.f32 	[%rd2+12], %f339;
	st.local.f32 	[%rd2+16], %f338;
	st.local.f32 	[%rd2+20], %f337;
	st.local.f32 	[%rd2+24], %f336;

$L__BB38_9:
	shr.s32 	%r39, %r3, 31;
	shr.u32 	%r40, %r39, 27;
	add.s32 	%r41, %r3, %r40;
	shr.s32 	%r42, %r41, 5;
	shl.b32 	%r43, %r42, 2;
	add.s32 	%r10, %r24, %r43;
	mov.u32 	%r45, 2;
	mov.b32 	%r46, %f342;
	mov.u32 	%r47, 31;
	mov.u32 	%r48, 16;
	mov.u32 	%r49, -1;
	shfl.sync.bfly.b32 	%r50|%p8, %r46, %r48, %r47, %r49;
	mov.b32 	%f174, %r50;
	add.f32 	%f175, %f342, %f174;
	mov.b32 	%r51, %f175;
	mov.u32 	%r52, 8;
	shfl.sync.bfly.b32 	%r53|%p9, %r51, %r52, %r47, %r49;
	mov.b32 	%f176, %r53;
	add.f32 	%f177, %f175, %f176;
	mov.b32 	%r54, %f177;
	mov.u32 	%r55, 4;
	shfl.sync.bfly.b32 	%r56|%p10, %r54, %r55, %r47, %r49;
	mov.b32 	%f178, %r56;
	add.f32 	%f179, %f177, %f178;
	mov.b32 	%r57, %f179;
	shfl.sync.bfly.b32 	%r58|%p11, %r57, %r45, %r47, %r49;
	mov.b32 	%f180, %r58;
	add.f32 	%f181, %f179, %f180;
	mov.b32 	%r59, %f181;
	mov.u32 	%r60, 1;
	shfl.sync.bfly.b32 	%r61|%p12, %r59, %r60, %r47, %r49;
	mov.b32 	%f182, %r61;
	add.f32 	%f183, %f181, %f182;
	st.local.f32 	[%rd2], %f183;
	st.shared.f32 	[%r10], %f183;
	bar.sync 	0;
	@%p1 bra 	$L__BB38_11;

	ld.shared.f32 	%f184, [%r4];
	mov.b32 	%r62, %f184;
	shfl.sync.bfly.b32 	%r66|%p14, %r62, %r48, %r47, %r49;
	mov.b32 	%f185, %r66;
	add.f32 	%f186, %f184, %f185;
	mov.b32 	%r67, %f186;
	shfl.sync.bfly.b32 	%r69|%p15, %r67, %r52, %r47, %r49;
	mov.b32 	%f187, %r69;
	add.f32 	%f188, %f186, %f187;
	mov.b32 	%r70, %f188;
	shfl.sync.bfly.b32 	%r72|%p16, %r70, %r55, %r47, %r49;
	mov.b32 	%f189, %r72;
	add.f32 	%f190, %f188, %f189;
	mov.b32 	%r73, %f190;
	shfl.sync.bfly.b32 	%r75|%p17, %r73, %r45, %r47, %r49;
	mov.b32 	%f191, %r75;
	add.f32 	%f192, %f190, %f191;
	mov.b32 	%r76, %f192;
	shfl.sync.bfly.b32 	%r78|%p18, %r76, %r60, %r47, %r49;
	mov.b32 	%f193, %r78;
	add.f32 	%f194, %f192, %f193;
	st.local.f32 	[%rd2], %f194;

$L__BB38_11:
	bar.sync 	0;
	mov.b32 	%r79, %f341;
	shfl.sync.bfly.b32 	%r83|%p20, %r79, %r48, %r47, %r49;
	mov.b32 	%f195, %r83;
	add.f32 	%f196, %f341, %f195;
	mov.b32 	%r84, %f196;
	shfl.sync.bfly.b32 	%r86|%p21, %r84, %r52, %r47, %r49;
	mov.b32 	%f197, %r86;
	add.f32 	%f198, %f196, %f197;
	mov.b32 	%r87, %f198;
	shfl.sync.bfly.b32 	%r89|%p22, %r87, %r55, %r47, %r49;
	mov.b32 	%f199, %r89;
	add.f32 	%f200, %f198, %f199;
	mov.b32 	%r90, %f200;
	shfl.sync.bfly.b32 	%r92|%p23, %r90, %r45, %r47, %r49;
	mov.b32 	%f201, %r92;
	add.f32 	%f202, %f200, %f201;
	mov.b32 	%r93, %f202;
	shfl.sync.bfly.b32 	%r95|%p24, %r93, %r60, %r47, %r49;
	mov.b32 	%f203, %r95;
	add.f32 	%f204, %f202, %f203;
	st.local.f32 	[%rd2+4], %f204;
	st.shared.f32 	[%r10], %f204;
	bar.sync 	0;
	@%p1 bra 	$L__BB38_13;

	ld.shared.f32 	%f205, [%r4];
	mov.b32 	%r96, %f205;
	mov.u32 	%r97, 31;
	mov.u32 	%r98, 16;
	mov.u32 	%r99, -1;
	shfl.sync.bfly.b32 	%r100|%p25, %r96, %r98, %r97, %r99;
	mov.b32 	%f206, %r100;
	add.f32 	%f207, %f205, %f206;
	mov.b32 	%r101, %f207;
	mov.u32 	%r102, 8;
	shfl.sync.bfly.b32 	%r103|%p26, %r101, %r102, %r97, %r99;
	mov.b32 	%f208, %r103;
	add.f32 	%f209, %f207, %f208;
	mov.b32 	%r104, %f209;
	mov.u32 	%r105, 4;
	shfl.sync.bfly.b32 	%r106|%p27, %r104, %r105, %r97, %r99;
	mov.b32 	%f210, %r106;
	add.f32 	%f211, %f209, %f210;
	mov.b32 	%r107, %f211;
	mov.u32 	%r108, 2;
	shfl.sync.bfly.b32 	%r109|%p28, %r107, %r108, %r97, %r99;
	mov.b32 	%f212, %r109;
	add.f32 	%f213, %f211, %f212;
	mov.b32 	%r110, %f213;
	mov.u32 	%r111, 1;
	shfl.sync.bfly.b32 	%r112|%p29, %r110, %r111, %r97, %r99;
	mov.b32 	%f214, %r112;
	add.f32 	%f215, %f213, %f214;
	st.local.f32 	[%rd2+4], %f215;

$L__BB38_13:
	bar.sync 	0;
	mov.b32 	%r113, %f340;
	mov.u32 	%r114, 31;
	mov.u32 	%r115, 16;
	mov.u32 	%r116, -1;
	shfl.sync.bfly.b32 	%r117|%p31, %r113, %r115, %r114, %r116;
	mov.b32 	%f216, %r117;
	add.f32 	%f217, %f340, %f216;
	mov.b32 	%r118, %f217;
	mov.u32 	%r119, 8;
	shfl.sync.bfly.b32 	%r120|%p32, %r118, %r119, %r114, %r116;
	mov.b32 	%f218, %r120;
	add.f32 	%f219, %f217, %f218;
	mov.b32 	%r121, %f219;
	mov.u32 	%r122, 4;
	shfl.sync.bfly.b32 	%r123|%p33, %r121, %r122, %r114, %r116;
	mov.b32 	%f220, %r123;
	add.f32 	%f221, %f219, %f220;
	mov.b32 	%r124, %f221;
	mov.u32 	%r125, 2;
	shfl.sync.bfly.b32 	%r126|%p34, %r124, %r125, %r114, %r116;
	mov.b32 	%f222, %r126;
	add.f32 	%f223, %f221, %f222;
	mov.b32 	%r127, %f223;
	mov.u32 	%r128, 1;
	shfl.sync.bfly.b32 	%r129|%p35, %r127, %r128, %r114, %r116;
	mov.b32 	%f224, %r129;
	add.f32 	%f225, %f223, %f224;
	st.local.f32 	[%rd2+8], %f225;
	st.shared.f32 	[%r10], %f225;
	bar.sync 	0;
	@%p1 bra 	$L__BB38_15;

	ld.shared.f32 	%f226, [%r4];
	mov.b32 	%r130, %f226;
	shfl.sync.bfly.b32 	%r134|%p36, %r130, %r115, %r114, %r116;
	mov.b32 	%f227, %r134;
	add.f32 	%f228, %f226, %f227;
	mov.b32 	%r135, %f228;
	shfl.sync.bfly.b32 	%r137|%p37, %r135, %r119, %r114, %r116;
	mov.b32 	%f229, %r137;
	add.f32 	%f230, %f228, %f229;
	mov.b32 	%r138, %f230;
	shfl.sync.bfly.b32 	%r140|%p38, %r138, %r122, %r114, %r116;
	mov.b32 	%f231, %r140;
	add.f32 	%f232, %f230, %f231;
	mov.b32 	%r141, %f232;
	shfl.sync.bfly.b32 	%r143|%p39, %r141, %r125, %r114, %r116;
	mov.b32 	%f233, %r143;
	add.f32 	%f234, %f232, %f233;
	mov.b32 	%r144, %f234;
	shfl.sync.bfly.b32 	%r146|%p40, %r144, %r128, %r114, %r116;
	mov.b32 	%f235, %r146;
	add.f32 	%f236, %f234, %f235;
	st.local.f32 	[%rd2+8], %f236;

$L__BB38_15:
	bar.sync 	0;
	mov.b32 	%r147, %f339;
	shfl.sync.bfly.b32 	%r151|%p42, %r147, %r115, %r114, %r116;
	mov.b32 	%f237, %r151;
	add.f32 	%f238, %f339, %f237;
	mov.b32 	%r152, %f238;
	shfl.sync.bfly.b32 	%r154|%p43, %r152, %r119, %r114, %r116;
	mov.b32 	%f239, %r154;
	add.f32 	%f240, %f238, %f239;
	mov.b32 	%r155, %f240;
	shfl.sync.bfly.b32 	%r157|%p44, %r155, %r122, %r114, %r116;
	mov.b32 	%f241, %r157;
	add.f32 	%f242, %f240, %f241;
	mov.b32 	%r158, %f242;
	shfl.sync.bfly.b32 	%r160|%p45, %r158, %r125, %r114, %r116;
	mov.b32 	%f243, %r160;
	add.f32 	%f244, %f242, %f243;
	mov.b32 	%r161, %f244;
	shfl.sync.bfly.b32 	%r163|%p46, %r161, %r128, %r114, %r116;
	mov.b32 	%f245, %r163;
	add.f32 	%f246, %f244, %f245;
	st.local.f32 	[%rd2+12], %f246;
	st.shared.f32 	[%r10], %f246;
	bar.sync 	0;
	@%p1 bra 	$L__BB38_17;

	ld.shared.f32 	%f247, [%r4];
	mov.b32 	%r164, %f247;
	mov.u32 	%r165, 31;
	mov.u32 	%r166, 16;
	mov.u32 	%r167, -1;
	shfl.sync.bfly.b32 	%r168|%p47, %r164, %r166, %r165, %r167;
	mov.b32 	%f248, %r168;
	add.f32 	%f249, %f247, %f248;
	mov.b32 	%r169, %f249;
	mov.u32 	%r170, 8;
	shfl.sync.bfly.b32 	%r171|%p48, %r169, %r170, %r165, %r167;
	mov.b32 	%f250, %r171;
	add.f32 	%f251, %f249, %f250;
	mov.b32 	%r172, %f251;
	mov.u32 	%r173, 4;
	shfl.sync.bfly.b32 	%r174|%p49, %r172, %r173, %r165, %r167;
	mov.b32 	%f252, %r174;
	add.f32 	%f253, %f251, %f252;
	mov.b32 	%r175, %f253;
	mov.u32 	%r176, 2;
	shfl.sync.bfly.b32 	%r177|%p50, %r175, %r176, %r165, %r167;
	mov.b32 	%f254, %r177;
	add.f32 	%f255, %f253, %f254;
	mov.b32 	%r178, %f255;
	mov.u32 	%r179, 1;
	shfl.sync.bfly.b32 	%r180|%p51, %r178, %r179, %r165, %r167;
	mov.b32 	%f256, %r180;
	add.f32 	%f257, %f255, %f256;
	st.local.f32 	[%rd2+12], %f257;

$L__BB38_17:
	bar.sync 	0;
	mov.b32 	%r181, %f338;
	mov.u32 	%r182, 31;
	mov.u32 	%r183, 16;
	mov.u32 	%r184, -1;
	shfl.sync.bfly.b32 	%r185|%p53, %r181, %r183, %r182, %r184;
	mov.b32 	%f258, %r185;
	add.f32 	%f259, %f338, %f258;
	mov.b32 	%r186, %f259;
	mov.u32 	%r187, 8;
	shfl.sync.bfly.b32 	%r188|%p54, %r186, %r187, %r182, %r184;
	mov.b32 	%f260, %r188;
	add.f32 	%f261, %f259, %f260;
	mov.b32 	%r189, %f261;
	mov.u32 	%r190, 4;
	shfl.sync.bfly.b32 	%r191|%p55, %r189, %r190, %r182, %r184;
	mov.b32 	%f262, %r191;
	add.f32 	%f263, %f261, %f262;
	mov.b32 	%r192, %f263;
	mov.u32 	%r193, 2;
	shfl.sync.bfly.b32 	%r194|%p56, %r192, %r193, %r182, %r184;
	mov.b32 	%f264, %r194;
	add.f32 	%f265, %f263, %f264;
	mov.b32 	%r195, %f265;
	mov.u32 	%r196, 1;
	shfl.sync.bfly.b32 	%r197|%p57, %r195, %r196, %r182, %r184;
	mov.b32 	%f266, %r197;
	add.f32 	%f267, %f265, %f266;
	st.local.f32 	[%rd2+16], %f267;
	st.shared.f32 	[%r10], %f267;
	bar.sync 	0;
	@%p1 bra 	$L__BB38_19;

	ld.shared.f32 	%f268, [%r4];
	mov.b32 	%r198, %f268;
	shfl.sync.bfly.b32 	%r202|%p58, %r198, %r183, %r182, %r184;
	mov.b32 	%f269, %r202;
	add.f32 	%f270, %f268, %f269;
	mov.b32 	%r203, %f270;
	shfl.sync.bfly.b32 	%r205|%p59, %r203, %r187, %r182, %r184;
	mov.b32 	%f271, %r205;
	add.f32 	%f272, %f270, %f271;
	mov.b32 	%r206, %f272;
	shfl.sync.bfly.b32 	%r208|%p60, %r206, %r190, %r182, %r184;
	mov.b32 	%f273, %r208;
	add.f32 	%f274, %f272, %f273;
	mov.b32 	%r209, %f274;
	shfl.sync.bfly.b32 	%r211|%p61, %r209, %r193, %r182, %r184;
	mov.b32 	%f275, %r211;
	add.f32 	%f276, %f274, %f275;
	mov.b32 	%r212, %f276;
	shfl.sync.bfly.b32 	%r214|%p62, %r212, %r196, %r182, %r184;
	mov.b32 	%f277, %r214;
	add.f32 	%f278, %f276, %f277;
	st.local.f32 	[%rd2+16], %f278;

$L__BB38_19:
	bar.sync 	0;
	mov.b32 	%r215, %f337;
	shfl.sync.bfly.b32 	%r219|%p64, %r215, %r183, %r182, %r184;
	mov.b32 	%f279, %r219;
	add.f32 	%f280, %f337, %f279;
	mov.b32 	%r220, %f280;
	shfl.sync.bfly.b32 	%r222|%p65, %r220, %r187, %r182, %r184;
	mov.b32 	%f281, %r222;
	add.f32 	%f282, %f280, %f281;
	mov.b32 	%r223, %f282;
	shfl.sync.bfly.b32 	%r225|%p66, %r223, %r190, %r182, %r184;
	mov.b32 	%f283, %r225;
	add.f32 	%f284, %f282, %f283;
	mov.b32 	%r226, %f284;
	shfl.sync.bfly.b32 	%r228|%p67, %r226, %r193, %r182, %r184;
	mov.b32 	%f285, %r228;
	add.f32 	%f286, %f284, %f285;
	mov.b32 	%r229, %f286;
	shfl.sync.bfly.b32 	%r231|%p68, %r229, %r196, %r182, %r184;
	mov.b32 	%f287, %r231;
	add.f32 	%f288, %f286, %f287;
	st.local.f32 	[%rd2+20], %f288;
	st.shared.f32 	[%r10], %f288;
	bar.sync 	0;
	@%p1 bra 	$L__BB38_21;

	ld.shared.f32 	%f289, [%r4];
	mov.b32 	%r232, %f289;
	mov.u32 	%r233, 31;
	mov.u32 	%r234, 16;
	mov.u32 	%r235, -1;
	shfl.sync.bfly.b32 	%r236|%p69, %r232, %r234, %r233, %r235;
	mov.b32 	%f290, %r236;
	add.f32 	%f291, %f289, %f290;
	mov.b32 	%r237, %f291;
	mov.u32 	%r238, 8;
	shfl.sync.bfly.b32 	%r239|%p70, %r237, %r238, %r233, %r235;
	mov.b32 	%f292, %r239;
	add.f32 	%f293, %f291, %f292;
	mov.b32 	%r240, %f293;
	mov.u32 	%r241, 4;
	shfl.sync.bfly.b32 	%r242|%p71, %r240, %r241, %r233, %r235;
	mov.b32 	%f294, %r242;
	add.f32 	%f295, %f293, %f294;
	mov.b32 	%r243, %f295;
	mov.u32 	%r244, 2;
	shfl.sync.bfly.b32 	%r245|%p72, %r243, %r244, %r233, %r235;
	mov.b32 	%f296, %r245;
	add.f32 	%f297, %f295, %f296;
	mov.b32 	%r246, %f297;
	mov.u32 	%r247, 1;
	shfl.sync.bfly.b32 	%r248|%p73, %r246, %r247, %r233, %r235;
	mov.b32 	%f298, %r248;
	add.f32 	%f299, %f297, %f298;
	st.local.f32 	[%rd2+20], %f299;

$L__BB38_21:
	bar.sync 	0;
	mov.b32 	%r249, %f336;
	mov.u32 	%r250, 31;
	mov.u32 	%r251, 16;
	mov.u32 	%r252, -1;
	shfl.sync.bfly.b32 	%r253|%p75, %r249, %r251, %r250, %r252;
	mov.b32 	%f300, %r253;
	add.f32 	%f301, %f336, %f300;
	mov.b32 	%r254, %f301;
	mov.u32 	%r255, 8;
	shfl.sync.bfly.b32 	%r256|%p76, %r254, %r255, %r250, %r252;
	mov.b32 	%f302, %r256;
	add.f32 	%f303, %f301, %f302;
	mov.b32 	%r257, %f303;
	mov.u32 	%r258, 4;
	shfl.sync.bfly.b32 	%r259|%p77, %r257, %r258, %r250, %r252;
	mov.b32 	%f304, %r259;
	add.f32 	%f305, %f303, %f304;
	mov.b32 	%r260, %f305;
	mov.u32 	%r261, 2;
	shfl.sync.bfly.b32 	%r262|%p78, %r260, %r261, %r250, %r252;
	mov.b32 	%f306, %r262;
	add.f32 	%f307, %f305, %f306;
	mov.b32 	%r263, %f307;
	mov.u32 	%r264, 1;
	shfl.sync.bfly.b32 	%r265|%p79, %r263, %r264, %r250, %r252;
	mov.b32 	%f308, %r265;
	add.f32 	%f309, %f307, %f308;
	st.local.f32 	[%rd2+24], %f309;
	st.shared.f32 	[%r10], %f309;
	bar.sync 	0;
	@%p1 bra 	$L__BB38_23;

	ld.shared.f32 	%f310, [%r4];
	mov.b32 	%r266, %f310;
	shfl.sync.bfly.b32 	%r270|%p80, %r266, %r251, %r250, %r252;
	mov.b32 	%f311, %r270;
	add.f32 	%f312, %f310, %f311;
	mov.b32 	%r271, %f312;
	shfl.sync.bfly.b32 	%r273|%p81, %r271, %r255, %r250, %r252;
	mov.b32 	%f313, %r273;
	add.f32 	%f314, %f312, %f313;
	mov.b32 	%r274, %f314;
	shfl.sync.bfly.b32 	%r276|%p82, %r274, %r258, %r250, %r252;
	mov.b32 	%f315, %r276;
	add.f32 	%f316, %f314, %f315;
	mov.b32 	%r277, %f316;
	shfl.sync.bfly.b32 	%r279|%p83, %r277, %r261, %r250, %r252;
	mov.b32 	%f317, %r279;
	add.f32 	%f318, %f316, %f317;
	mov.b32 	%r280, %f318;
	shfl.sync.bfly.b32 	%r282|%p84, %r280, %r264, %r250, %r252;
	mov.b32 	%f319, %r282;
	add.f32 	%f320, %f318, %f319;
	st.local.f32 	[%rd2+24], %f320;

$L__BB38_23:
	bar.sync 	0;
	setp.gt.s32 	%p85, %r3, 6;
	@%p85 bra 	$L__BB38_25;

	mul.wide.s32 	%rd64, %r3, 4;
	add.s64 	%rd65, %rd2, %rd64;
	ld.local.f32 	%f321, [%rd65];
	mad.lo.s32 	%r283, %r3, %r13, %r2;
	cvt.s64.s32 	%rd66, %r283;
	mul.lo.s32 	%r284, %r1, %r14;
	cvt.s64.s32 	%rd67, %r284;
	add.s64 	%rd68, %rd67, %rd66;
	cvta.to.global.u64 	%rd69, %rd20;
	shl.b64 	%rd70, %rd68, 2;
	add.s64 	%rd71, %rd69, %rd70;
	st.global.f32 	[%rd71], %f321;

$L__BB38_25:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_8_bs_160
.visible .entry ggml_matvec_f32_ncols_8_bs_160(
	.param .u64 ggml_matvec_f32_ncols_8_bs_160_param_0,
	.param .u64 ggml_matvec_f32_ncols_8_bs_160_param_1,
	.param .u64 ggml_matvec_f32_ncols_8_bs_160_param_2,
	.param .u32 ggml_matvec_f32_ncols_8_bs_160_param_3,
	.param .u32 ggml_matvec_f32_ncols_8_bs_160_param_4,
	.param .u32 ggml_matvec_f32_ncols_8_bs_160_param_5,
	.param .u32 ggml_matvec_f32_ncols_8_bs_160_param_6,
	.param .u32 ggml_matvec_f32_ncols_8_bs_160_param_7,
	.param .u32 ggml_matvec_f32_ncols_8_bs_160_param_8,
	.param .u32 ggml_matvec_f32_ncols_8_bs_160_param_9,
	.param .u32 ggml_matvec_f32_ncols_8_bs_160_param_10,
	.param .u32 ggml_matvec_f32_ncols_8_bs_160_param_11
)
{
	.local .align 16 .b8 	__local_depot39[32];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<97>;
	.reg .f32 	%f<390>;
	.reg .b32 	%r<321>;
	.reg .b64 	%rd<78>;


	mov.u64 	%SPL, __local_depot39;
	ld.param.u64 	%rd22, [ggml_matvec_f32_ncols_8_bs_160_param_0];
	ld.param.u64 	%rd23, [ggml_matvec_f32_ncols_8_bs_160_param_1];
	ld.param.u64 	%rd21, [ggml_matvec_f32_ncols_8_bs_160_param_2];
	ld.param.u32 	%r11, [ggml_matvec_f32_ncols_8_bs_160_param_3];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_8_bs_160_param_5];
	ld.param.u32 	%r12, [ggml_matvec_f32_ncols_8_bs_160_param_6];
	ld.param.u32 	%r13, [ggml_matvec_f32_ncols_8_bs_160_param_7];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_8_bs_160_param_8];
	ld.param.u32 	%r17, [ggml_matvec_f32_ncols_8_bs_160_param_9];
	ld.param.u32 	%r18, [ggml_matvec_f32_ncols_8_bs_160_param_10];
	ld.param.u32 	%r14, [ggml_matvec_f32_ncols_8_bs_160_param_11];
	cvta.to.global.u64 	%rd77, %rd23;
	add.u64 	%rd2, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r19, %r1, %r16;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r20, %r2, %r15;
	mad.lo.s32 	%r21, %r19, %r17, %r20;
	cvt.s64.s32 	%rd3, %r21;
	cvta.to.global.u64 	%rd4, %rd22;
	mul.lo.s32 	%r22, %r1, %r18;
	cvt.s64.s32 	%rd5, %r22;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r23, %r3, 2;
	mov.u32 	%r24, data_mmv;
	add.s32 	%r4, %r24, %r23;
	@%p1 bra 	$L__BB39_2;

	mov.u32 	%r25, 0;
	st.shared.u32 	[%r4], %r25;

$L__BB39_2:
	bar.sync 	0;
	mov.f32 	%f382, 0f00000000;
	st.local.v4.f32 	[%rd2], {%f382, %f382, %f382, %f382};
	st.local.v4.f32 	[%rd2+16], {%f382, %f382, %f382, %f382};
	setp.ge.s32 	%p2, %r3, %r11;
	mov.f32 	%f383, %f382;
	mov.f32 	%f384, %f382;
	mov.f32 	%f385, %f382;
	mov.f32 	%f386, %f382;
	mov.f32 	%f387, %f382;
	mov.f32 	%f388, %f382;
	mov.f32 	%f389, %f382;
	@%p2 bra 	$L__BB39_9;

	not.b32 	%r26, %r3;
	add.s32 	%r5, %r26, %r11;
	mul.wide.u32 	%rd25, %r5, -858993459;
	shr.u64 	%rd26, %rd25, 39;
	and.b64  	%rd27, %rd26, 1;
	setp.eq.b64 	%p3, %rd27, 1;
	mov.pred 	%p4, 0;
	xor.pred  	%p5, %p3, %p4;
	mov.f32 	%f382, 0f00000000;
	mov.u32 	%r320, %r3;
	@%p5 bra 	$L__BB39_5;

	shl.b64 	%rd28, %rd5, 2;
	add.s64 	%rd29, %rd77, %rd28;
	shl.b64 	%rd30, %rd3, 2;
	add.s64 	%rd31, %rd4, %rd30;
	mul.wide.s32 	%rd32, %r3, 8;
	add.s64 	%rd33, %rd31, %rd32;
	ld.global.nc.v2.f32 	{%f57, %f58}, [%rd33];
	add.s64 	%rd34, %rd29, %rd32;
	ld.global.nc.v2.f32 	{%f61, %f62}, [%rd34];
	fma.rn.f32 	%f65, %f57, %f61, 0f00000000;
	fma.rn.f32 	%f389, %f58, %f62, %f65;
	mul.wide.s32 	%rd35, %r12, 8;
	add.s64 	%rd36, %rd34, %rd35;
	ld.global.nc.v2.f32 	{%f66, %f67}, [%rd36];
	fma.rn.f32 	%f70, %f57, %f66, 0f00000000;
	fma.rn.f32 	%f388, %f58, %f67, %f70;
	add.s32 	%r27, %r3, %r12;
	add.s32 	%r28, %r27, %r12;
	mul.wide.s32 	%rd37, %r28, 8;
	add.s64 	%rd38, %rd29, %rd37;
	ld.global.nc.v2.f32 	{%f71, %f72}, [%rd38];
	fma.rn.f32 	%f75, %f57, %f71, 0f00000000;
	fma.rn.f32 	%f387, %f58, %f72, %f75;
	add.s64 	%rd39, %rd38, %rd35;
	ld.global.nc.v2.f32 	{%f76, %f77}, [%rd39];
	fma.rn.f32 	%f80, %f57, %f76, 0f00000000;
	fma.rn.f32 	%f386, %f58, %f77, %f80;
	st.local.v4.f32 	[%rd2], {%f389, %f388, %f387, %f386};
	add.s64 	%rd40, %rd39, %rd35;
	ld.global.nc.v2.f32 	{%f81, %f82}, [%rd40];
	fma.rn.f32 	%f85, %f57, %f81, 0f00000000;
	fma.rn.f32 	%f385, %f58, %f82, %f85;
	add.s64 	%rd41, %rd40, %rd35;
	ld.global.nc.v2.f32 	{%f86, %f87}, [%rd41];
	fma.rn.f32 	%f90, %f57, %f86, 0f00000000;
	fma.rn.f32 	%f384, %f58, %f87, %f90;
	add.s64 	%rd42, %rd41, %rd35;
	ld.global.nc.v2.f32 	{%f91, %f92}, [%rd42];
	fma.rn.f32 	%f95, %f57, %f91, 0f00000000;
	fma.rn.f32 	%f383, %f58, %f92, %f95;
	add.s64 	%rd43, %rd42, %rd35;
	ld.global.nc.v2.f32 	{%f96, %f97}, [%rd43];
	fma.rn.f32 	%f100, %f57, %f96, 0f00000000;
	fma.rn.f32 	%f382, %f58, %f97, %f100;
	st.local.v4.f32 	[%rd2+16], {%f385, %f384, %f383, %f382};
	add.s32 	%r320, %r3, 160;

$L__BB39_5:
	setp.lt.u32 	%p6, %r5, 160;
	@%p6 bra 	$L__BB39_9;

	add.s32 	%r29, %r320, %r12;
	add.s32 	%r30, %r29, 160;
	mul.wide.s32 	%rd44, %r30, 8;
	shl.b64 	%rd45, %rd5, 2;
	add.s64 	%rd7, %rd44, %rd45;
	shl.b32 	%r31, %r12, 1;
	add.s32 	%r32, %r320, %r31;
	mad.lo.s32 	%r33, %r12, 3, %r320;
	shl.b32 	%r34, %r12, 2;
	add.s32 	%r35, %r320, %r34;
	mad.lo.s32 	%r36, %r12, 5, %r320;
	mad.lo.s32 	%r37, %r12, 6, %r320;
	mad.lo.s32 	%r38, %r12, 7, %r320;
	mul.wide.s32 	%rd46, %r32, 8;
	add.s64 	%rd8, %rd46, %rd45;
	mul.wide.s32 	%rd47, %r33, 8;
	add.s64 	%rd9, %rd47, %rd45;
	mul.wide.s32 	%rd48, %r35, 8;
	add.s64 	%rd10, %rd48, %rd45;
	mul.wide.s32 	%rd49, %r36, 8;
	add.s64 	%rd11, %rd49, %rd45;
	mul.wide.s32 	%rd50, %r37, 8;
	add.s64 	%rd12, %rd50, %rd45;
	mul.wide.s32 	%rd51, %r38, 8;
	add.s64 	%rd13, %rd51, %rd45;
	mul.wide.s32 	%rd52, %r320, 2;
	add.s64 	%rd53, %rd52, %rd3;
	shl.b64 	%rd54, %rd53, 2;
	add.s64 	%rd55, %rd4, %rd54;
	add.s64 	%rd76, %rd55, 1280;
	mul.wide.s32 	%rd56, %r320, 8;
	mul.wide.s32 	%rd57, %r12, 8;
	add.s64 	%rd58, %rd56, %rd57;
	add.s64 	%rd15, %rd58, %rd45;
	add.s64 	%rd16, %rd56, %rd45;

$L__BB39_7:
	ld.global.nc.v2.f32 	{%f101, %f102}, [%rd76+-1280];
	add.s64 	%rd59, %rd77, %rd16;
	ld.global.nc.v2.f32 	{%f105, %f106}, [%rd59];
	fma.rn.f32 	%f109, %f101, %f105, %f389;
	fma.rn.f32 	%f110, %f102, %f106, %f109;
	add.s64 	%rd60, %rd77, %rd15;
	ld.global.nc.v2.f32 	{%f111, %f112}, [%rd60];
	fma.rn.f32 	%f115, %f101, %f111, %f388;
	fma.rn.f32 	%f116, %f102, %f112, %f115;
	add.s64 	%rd61, %rd77, %rd8;
	ld.global.nc.v2.f32 	{%f117, %f118}, [%rd61];
	fma.rn.f32 	%f121, %f101, %f117, %f387;
	fma.rn.f32 	%f122, %f102, %f118, %f121;
	add.s64 	%rd62, %rd77, %rd9;
	ld.global.nc.v2.f32 	{%f123, %f124}, [%rd62];
	fma.rn.f32 	%f127, %f101, %f123, %f386;
	fma.rn.f32 	%f128, %f102, %f124, %f127;
	add.s64 	%rd63, %rd77, %rd10;
	ld.global.nc.v2.f32 	{%f129, %f130}, [%rd63];
	fma.rn.f32 	%f133, %f101, %f129, %f385;
	fma.rn.f32 	%f134, %f102, %f130, %f133;
	add.s64 	%rd64, %rd77, %rd11;
	ld.global.nc.v2.f32 	{%f135, %f136}, [%rd64];
	fma.rn.f32 	%f139, %f101, %f135, %f384;
	fma.rn.f32 	%f140, %f102, %f136, %f139;
	add.s64 	%rd65, %rd77, %rd12;
	ld.global.nc.v2.f32 	{%f141, %f142}, [%rd65];
	fma.rn.f32 	%f145, %f101, %f141, %f383;
	fma.rn.f32 	%f146, %f102, %f142, %f145;
	add.s64 	%rd66, %rd77, %rd13;
	ld.global.nc.v2.f32 	{%f147, %f148}, [%rd66];
	fma.rn.f32 	%f151, %f101, %f147, %f382;
	fma.rn.f32 	%f152, %f102, %f148, %f151;
	ld.global.nc.v2.f32 	{%f153, %f154}, [%rd76];
	ld.global.nc.v2.f32 	{%f157, %f158}, [%rd59+1280];
	fma.rn.f32 	%f161, %f153, %f157, %f110;
	fma.rn.f32 	%f389, %f154, %f158, %f161;
	add.s64 	%rd67, %rd77, %rd7;
	ld.global.nc.v2.f32 	{%f162, %f163}, [%rd67];
	fma.rn.f32 	%f166, %f153, %f162, %f116;
	fma.rn.f32 	%f388, %f154, %f163, %f166;
	ld.global.nc.v2.f32 	{%f167, %f168}, [%rd61+1280];
	fma.rn.f32 	%f171, %f153, %f167, %f122;
	fma.rn.f32 	%f387, %f154, %f168, %f171;
	ld.global.nc.v2.f32 	{%f172, %f173}, [%rd62+1280];
	fma.rn.f32 	%f176, %f153, %f172, %f128;
	fma.rn.f32 	%f386, %f154, %f173, %f176;
	ld.global.nc.v2.f32 	{%f177, %f178}, [%rd63+1280];
	fma.rn.f32 	%f181, %f153, %f177, %f134;
	fma.rn.f32 	%f385, %f154, %f178, %f181;
	ld.global.nc.v2.f32 	{%f182, %f183}, [%rd64+1280];
	fma.rn.f32 	%f186, %f153, %f182, %f140;
	fma.rn.f32 	%f384, %f154, %f183, %f186;
	ld.global.nc.v2.f32 	{%f187, %f188}, [%rd65+1280];
	fma.rn.f32 	%f191, %f153, %f187, %f146;
	fma.rn.f32 	%f383, %f154, %f188, %f191;
	ld.global.nc.v2.f32 	{%f192, %f193}, [%rd66+1280];
	fma.rn.f32 	%f196, %f153, %f192, %f152;
	fma.rn.f32 	%f382, %f154, %f193, %f196;
	add.s64 	%rd77, %rd77, 2560;
	add.s64 	%rd76, %rd76, 2560;
	add.s32 	%r320, %r320, 320;
	setp.lt.s32 	%p7, %r320, %r11;
	@%p7 bra 	$L__BB39_7;

	st.local.v4.f32 	[%rd2], {%f389, %f388, %f387, %f386};
	st.local.v4.f32 	[%rd2+16], {%f385, %f384, %f383, %f382};

$L__BB39_9:
	shr.s32 	%r39, %r3, 31;
	shr.u32 	%r40, %r39, 27;
	add.s32 	%r41, %r3, %r40;
	shr.s32 	%r42, %r41, 5;
	shl.b32 	%r43, %r42, 2;
	add.s32 	%r10, %r24, %r43;
	mov.u32 	%r45, 2;
	mov.b32 	%r46, %f389;
	mov.u32 	%r47, 31;
	mov.u32 	%r48, 16;
	mov.u32 	%r49, -1;
	shfl.sync.bfly.b32 	%r50|%p8, %r46, %r48, %r47, %r49;
	mov.b32 	%f197, %r50;
	add.f32 	%f198, %f389, %f197;
	mov.b32 	%r51, %f198;
	mov.u32 	%r52, 8;
	shfl.sync.bfly.b32 	%r53|%p9, %r51, %r52, %r47, %r49;
	mov.b32 	%f199, %r53;
	add.f32 	%f200, %f198, %f199;
	mov.b32 	%r54, %f200;
	mov.u32 	%r55, 4;
	shfl.sync.bfly.b32 	%r56|%p10, %r54, %r55, %r47, %r49;
	mov.b32 	%f201, %r56;
	add.f32 	%f202, %f200, %f201;
	mov.b32 	%r57, %f202;
	shfl.sync.bfly.b32 	%r58|%p11, %r57, %r45, %r47, %r49;
	mov.b32 	%f203, %r58;
	add.f32 	%f204, %f202, %f203;
	mov.b32 	%r59, %f204;
	mov.u32 	%r60, 1;
	shfl.sync.bfly.b32 	%r61|%p12, %r59, %r60, %r47, %r49;
	mov.b32 	%f205, %r61;
	add.f32 	%f206, %f204, %f205;
	st.local.f32 	[%rd2], %f206;
	st.shared.f32 	[%r10], %f206;
	bar.sync 	0;
	@%p1 bra 	$L__BB39_11;

	ld.shared.f32 	%f207, [%r4];
	mov.b32 	%r62, %f207;
	shfl.sync.bfly.b32 	%r66|%p14, %r62, %r48, %r47, %r49;
	mov.b32 	%f208, %r66;
	add.f32 	%f209, %f207, %f208;
	mov.b32 	%r67, %f209;
	shfl.sync.bfly.b32 	%r69|%p15, %r67, %r52, %r47, %r49;
	mov.b32 	%f210, %r69;
	add.f32 	%f211, %f209, %f210;
	mov.b32 	%r70, %f211;
	shfl.sync.bfly.b32 	%r72|%p16, %r70, %r55, %r47, %r49;
	mov.b32 	%f212, %r72;
	add.f32 	%f213, %f211, %f212;
	mov.b32 	%r73, %f213;
	shfl.sync.bfly.b32 	%r75|%p17, %r73, %r45, %r47, %r49;
	mov.b32 	%f214, %r75;
	add.f32 	%f215, %f213, %f214;
	mov.b32 	%r76, %f215;
	shfl.sync.bfly.b32 	%r78|%p18, %r76, %r60, %r47, %r49;
	mov.b32 	%f216, %r78;
	add.f32 	%f217, %f215, %f216;
	st.local.f32 	[%rd2], %f217;

$L__BB39_11:
	bar.sync 	0;
	mov.b32 	%r79, %f388;
	shfl.sync.bfly.b32 	%r83|%p20, %r79, %r48, %r47, %r49;
	mov.b32 	%f218, %r83;
	add.f32 	%f219, %f388, %f218;
	mov.b32 	%r84, %f219;
	shfl.sync.bfly.b32 	%r86|%p21, %r84, %r52, %r47, %r49;
	mov.b32 	%f220, %r86;
	add.f32 	%f221, %f219, %f220;
	mov.b32 	%r87, %f221;
	shfl.sync.bfly.b32 	%r89|%p22, %r87, %r55, %r47, %r49;
	mov.b32 	%f222, %r89;
	add.f32 	%f223, %f221, %f222;
	mov.b32 	%r90, %f223;
	shfl.sync.bfly.b32 	%r92|%p23, %r90, %r45, %r47, %r49;
	mov.b32 	%f224, %r92;
	add.f32 	%f225, %f223, %f224;
	mov.b32 	%r93, %f225;
	shfl.sync.bfly.b32 	%r95|%p24, %r93, %r60, %r47, %r49;
	mov.b32 	%f226, %r95;
	add.f32 	%f227, %f225, %f226;
	st.local.f32 	[%rd2+4], %f227;
	st.shared.f32 	[%r10], %f227;
	bar.sync 	0;
	@%p1 bra 	$L__BB39_13;

	ld.shared.f32 	%f228, [%r4];
	mov.b32 	%r96, %f228;
	mov.u32 	%r97, 31;
	mov.u32 	%r98, 16;
	mov.u32 	%r99, -1;
	shfl.sync.bfly.b32 	%r100|%p25, %r96, %r98, %r97, %r99;
	mov.b32 	%f229, %r100;
	add.f32 	%f230, %f228, %f229;
	mov.b32 	%r101, %f230;
	mov.u32 	%r102, 8;
	shfl.sync.bfly.b32 	%r103|%p26, %r101, %r102, %r97, %r99;
	mov.b32 	%f231, %r103;
	add.f32 	%f232, %f230, %f231;
	mov.b32 	%r104, %f232;
	mov.u32 	%r105, 4;
	shfl.sync.bfly.b32 	%r106|%p27, %r104, %r105, %r97, %r99;
	mov.b32 	%f233, %r106;
	add.f32 	%f234, %f232, %f233;
	mov.b32 	%r107, %f234;
	mov.u32 	%r108, 2;
	shfl.sync.bfly.b32 	%r109|%p28, %r107, %r108, %r97, %r99;
	mov.b32 	%f235, %r109;
	add.f32 	%f236, %f234, %f235;
	mov.b32 	%r110, %f236;
	mov.u32 	%r111, 1;
	shfl.sync.bfly.b32 	%r112|%p29, %r110, %r111, %r97, %r99;
	mov.b32 	%f237, %r112;
	add.f32 	%f238, %f236, %f237;
	st.local.f32 	[%rd2+4], %f238;

$L__BB39_13:
	bar.sync 	0;
	mov.b32 	%r113, %f387;
	mov.u32 	%r114, 31;
	mov.u32 	%r115, 16;
	mov.u32 	%r116, -1;
	shfl.sync.bfly.b32 	%r117|%p31, %r113, %r115, %r114, %r116;
	mov.b32 	%f239, %r117;
	add.f32 	%f240, %f387, %f239;
	mov.b32 	%r118, %f240;
	mov.u32 	%r119, 8;
	shfl.sync.bfly.b32 	%r120|%p32, %r118, %r119, %r114, %r116;
	mov.b32 	%f241, %r120;
	add.f32 	%f242, %f240, %f241;
	mov.b32 	%r121, %f242;
	mov.u32 	%r122, 4;
	shfl.sync.bfly.b32 	%r123|%p33, %r121, %r122, %r114, %r116;
	mov.b32 	%f243, %r123;
	add.f32 	%f244, %f242, %f243;
	mov.b32 	%r124, %f244;
	mov.u32 	%r125, 2;
	shfl.sync.bfly.b32 	%r126|%p34, %r124, %r125, %r114, %r116;
	mov.b32 	%f245, %r126;
	add.f32 	%f246, %f244, %f245;
	mov.b32 	%r127, %f246;
	mov.u32 	%r128, 1;
	shfl.sync.bfly.b32 	%r129|%p35, %r127, %r128, %r114, %r116;
	mov.b32 	%f247, %r129;
	add.f32 	%f248, %f246, %f247;
	st.local.f32 	[%rd2+8], %f248;
	st.shared.f32 	[%r10], %f248;
	bar.sync 	0;
	@%p1 bra 	$L__BB39_15;

	ld.shared.f32 	%f249, [%r4];
	mov.b32 	%r130, %f249;
	shfl.sync.bfly.b32 	%r134|%p36, %r130, %r115, %r114, %r116;
	mov.b32 	%f250, %r134;
	add.f32 	%f251, %f249, %f250;
	mov.b32 	%r135, %f251;
	shfl.sync.bfly.b32 	%r137|%p37, %r135, %r119, %r114, %r116;
	mov.b32 	%f252, %r137;
	add.f32 	%f253, %f251, %f252;
	mov.b32 	%r138, %f253;
	shfl.sync.bfly.b32 	%r140|%p38, %r138, %r122, %r114, %r116;
	mov.b32 	%f254, %r140;
	add.f32 	%f255, %f253, %f254;
	mov.b32 	%r141, %f255;
	shfl.sync.bfly.b32 	%r143|%p39, %r141, %r125, %r114, %r116;
	mov.b32 	%f256, %r143;
	add.f32 	%f257, %f255, %f256;
	mov.b32 	%r144, %f257;
	shfl.sync.bfly.b32 	%r146|%p40, %r144, %r128, %r114, %r116;
	mov.b32 	%f258, %r146;
	add.f32 	%f259, %f257, %f258;
	st.local.f32 	[%rd2+8], %f259;

$L__BB39_15:
	bar.sync 	0;
	mov.b32 	%r147, %f386;
	shfl.sync.bfly.b32 	%r151|%p42, %r147, %r115, %r114, %r116;
	mov.b32 	%f260, %r151;
	add.f32 	%f261, %f386, %f260;
	mov.b32 	%r152, %f261;
	shfl.sync.bfly.b32 	%r154|%p43, %r152, %r119, %r114, %r116;
	mov.b32 	%f262, %r154;
	add.f32 	%f263, %f261, %f262;
	mov.b32 	%r155, %f263;
	shfl.sync.bfly.b32 	%r157|%p44, %r155, %r122, %r114, %r116;
	mov.b32 	%f264, %r157;
	add.f32 	%f265, %f263, %f264;
	mov.b32 	%r158, %f265;
	shfl.sync.bfly.b32 	%r160|%p45, %r158, %r125, %r114, %r116;
	mov.b32 	%f266, %r160;
	add.f32 	%f267, %f265, %f266;
	mov.b32 	%r161, %f267;
	shfl.sync.bfly.b32 	%r163|%p46, %r161, %r128, %r114, %r116;
	mov.b32 	%f268, %r163;
	add.f32 	%f269, %f267, %f268;
	st.local.f32 	[%rd2+12], %f269;
	st.shared.f32 	[%r10], %f269;
	bar.sync 	0;
	@%p1 bra 	$L__BB39_17;

	ld.shared.f32 	%f270, [%r4];
	mov.b32 	%r164, %f270;
	mov.u32 	%r165, 31;
	mov.u32 	%r166, 16;
	mov.u32 	%r167, -1;
	shfl.sync.bfly.b32 	%r168|%p47, %r164, %r166, %r165, %r167;
	mov.b32 	%f271, %r168;
	add.f32 	%f272, %f270, %f271;
	mov.b32 	%r169, %f272;
	mov.u32 	%r170, 8;
	shfl.sync.bfly.b32 	%r171|%p48, %r169, %r170, %r165, %r167;
	mov.b32 	%f273, %r171;
	add.f32 	%f274, %f272, %f273;
	mov.b32 	%r172, %f274;
	mov.u32 	%r173, 4;
	shfl.sync.bfly.b32 	%r174|%p49, %r172, %r173, %r165, %r167;
	mov.b32 	%f275, %r174;
	add.f32 	%f276, %f274, %f275;
	mov.b32 	%r175, %f276;
	mov.u32 	%r176, 2;
	shfl.sync.bfly.b32 	%r177|%p50, %r175, %r176, %r165, %r167;
	mov.b32 	%f277, %r177;
	add.f32 	%f278, %f276, %f277;
	mov.b32 	%r178, %f278;
	mov.u32 	%r179, 1;
	shfl.sync.bfly.b32 	%r180|%p51, %r178, %r179, %r165, %r167;
	mov.b32 	%f279, %r180;
	add.f32 	%f280, %f278, %f279;
	st.local.f32 	[%rd2+12], %f280;

$L__BB39_17:
	bar.sync 	0;
	mov.b32 	%r181, %f385;
	mov.u32 	%r182, 31;
	mov.u32 	%r183, 16;
	mov.u32 	%r184, -1;
	shfl.sync.bfly.b32 	%r185|%p53, %r181, %r183, %r182, %r184;
	mov.b32 	%f281, %r185;
	add.f32 	%f282, %f385, %f281;
	mov.b32 	%r186, %f282;
	mov.u32 	%r187, 8;
	shfl.sync.bfly.b32 	%r188|%p54, %r186, %r187, %r182, %r184;
	mov.b32 	%f283, %r188;
	add.f32 	%f284, %f282, %f283;
	mov.b32 	%r189, %f284;
	mov.u32 	%r190, 4;
	shfl.sync.bfly.b32 	%r191|%p55, %r189, %r190, %r182, %r184;
	mov.b32 	%f285, %r191;
	add.f32 	%f286, %f284, %f285;
	mov.b32 	%r192, %f286;
	mov.u32 	%r193, 2;
	shfl.sync.bfly.b32 	%r194|%p56, %r192, %r193, %r182, %r184;
	mov.b32 	%f287, %r194;
	add.f32 	%f288, %f286, %f287;
	mov.b32 	%r195, %f288;
	mov.u32 	%r196, 1;
	shfl.sync.bfly.b32 	%r197|%p57, %r195, %r196, %r182, %r184;
	mov.b32 	%f289, %r197;
	add.f32 	%f290, %f288, %f289;
	st.local.f32 	[%rd2+16], %f290;
	st.shared.f32 	[%r10], %f290;
	bar.sync 	0;
	@%p1 bra 	$L__BB39_19;

	ld.shared.f32 	%f291, [%r4];
	mov.b32 	%r198, %f291;
	shfl.sync.bfly.b32 	%r202|%p58, %r198, %r183, %r182, %r184;
	mov.b32 	%f292, %r202;
	add.f32 	%f293, %f291, %f292;
	mov.b32 	%r203, %f293;
	shfl.sync.bfly.b32 	%r205|%p59, %r203, %r187, %r182, %r184;
	mov.b32 	%f294, %r205;
	add.f32 	%f295, %f293, %f294;
	mov.b32 	%r206, %f295;
	shfl.sync.bfly.b32 	%r208|%p60, %r206, %r190, %r182, %r184;
	mov.b32 	%f296, %r208;
	add.f32 	%f297, %f295, %f296;
	mov.b32 	%r209, %f297;
	shfl.sync.bfly.b32 	%r211|%p61, %r209, %r193, %r182, %r184;
	mov.b32 	%f298, %r211;
	add.f32 	%f299, %f297, %f298;
	mov.b32 	%r212, %f299;
	shfl.sync.bfly.b32 	%r214|%p62, %r212, %r196, %r182, %r184;
	mov.b32 	%f300, %r214;
	add.f32 	%f301, %f299, %f300;
	st.local.f32 	[%rd2+16], %f301;

$L__BB39_19:
	bar.sync 	0;
	mov.b32 	%r215, %f384;
	shfl.sync.bfly.b32 	%r219|%p64, %r215, %r183, %r182, %r184;
	mov.b32 	%f302, %r219;
	add.f32 	%f303, %f384, %f302;
	mov.b32 	%r220, %f303;
	shfl.sync.bfly.b32 	%r222|%p65, %r220, %r187, %r182, %r184;
	mov.b32 	%f304, %r222;
	add.f32 	%f305, %f303, %f304;
	mov.b32 	%r223, %f305;
	shfl.sync.bfly.b32 	%r225|%p66, %r223, %r190, %r182, %r184;
	mov.b32 	%f306, %r225;
	add.f32 	%f307, %f305, %f306;
	mov.b32 	%r226, %f307;
	shfl.sync.bfly.b32 	%r228|%p67, %r226, %r193, %r182, %r184;
	mov.b32 	%f308, %r228;
	add.f32 	%f309, %f307, %f308;
	mov.b32 	%r229, %f309;
	shfl.sync.bfly.b32 	%r231|%p68, %r229, %r196, %r182, %r184;
	mov.b32 	%f310, %r231;
	add.f32 	%f311, %f309, %f310;
	st.local.f32 	[%rd2+20], %f311;
	st.shared.f32 	[%r10], %f311;
	bar.sync 	0;
	@%p1 bra 	$L__BB39_21;

	ld.shared.f32 	%f312, [%r4];
	mov.b32 	%r232, %f312;
	mov.u32 	%r233, 31;
	mov.u32 	%r234, 16;
	mov.u32 	%r235, -1;
	shfl.sync.bfly.b32 	%r236|%p69, %r232, %r234, %r233, %r235;
	mov.b32 	%f313, %r236;
	add.f32 	%f314, %f312, %f313;
	mov.b32 	%r237, %f314;
	mov.u32 	%r238, 8;
	shfl.sync.bfly.b32 	%r239|%p70, %r237, %r238, %r233, %r235;
	mov.b32 	%f315, %r239;
	add.f32 	%f316, %f314, %f315;
	mov.b32 	%r240, %f316;
	mov.u32 	%r241, 4;
	shfl.sync.bfly.b32 	%r242|%p71, %r240, %r241, %r233, %r235;
	mov.b32 	%f317, %r242;
	add.f32 	%f318, %f316, %f317;
	mov.b32 	%r243, %f318;
	mov.u32 	%r244, 2;
	shfl.sync.bfly.b32 	%r245|%p72, %r243, %r244, %r233, %r235;
	mov.b32 	%f319, %r245;
	add.f32 	%f320, %f318, %f319;
	mov.b32 	%r246, %f320;
	mov.u32 	%r247, 1;
	shfl.sync.bfly.b32 	%r248|%p73, %r246, %r247, %r233, %r235;
	mov.b32 	%f321, %r248;
	add.f32 	%f322, %f320, %f321;
	st.local.f32 	[%rd2+20], %f322;

$L__BB39_21:
	bar.sync 	0;
	mov.b32 	%r249, %f383;
	mov.u32 	%r250, 31;
	mov.u32 	%r251, 16;
	mov.u32 	%r252, -1;
	shfl.sync.bfly.b32 	%r253|%p75, %r249, %r251, %r250, %r252;
	mov.b32 	%f323, %r253;
	add.f32 	%f324, %f383, %f323;
	mov.b32 	%r254, %f324;
	mov.u32 	%r255, 8;
	shfl.sync.bfly.b32 	%r256|%p76, %r254, %r255, %r250, %r252;
	mov.b32 	%f325, %r256;
	add.f32 	%f326, %f324, %f325;
	mov.b32 	%r257, %f326;
	mov.u32 	%r258, 4;
	shfl.sync.bfly.b32 	%r259|%p77, %r257, %r258, %r250, %r252;
	mov.b32 	%f327, %r259;
	add.f32 	%f328, %f326, %f327;
	mov.b32 	%r260, %f328;
	mov.u32 	%r261, 2;
	shfl.sync.bfly.b32 	%r262|%p78, %r260, %r261, %r250, %r252;
	mov.b32 	%f329, %r262;
	add.f32 	%f330, %f328, %f329;
	mov.b32 	%r263, %f330;
	mov.u32 	%r264, 1;
	shfl.sync.bfly.b32 	%r265|%p79, %r263, %r264, %r250, %r252;
	mov.b32 	%f331, %r265;
	add.f32 	%f332, %f330, %f331;
	st.local.f32 	[%rd2+24], %f332;
	st.shared.f32 	[%r10], %f332;
	bar.sync 	0;
	@%p1 bra 	$L__BB39_23;

	ld.shared.f32 	%f333, [%r4];
	mov.b32 	%r266, %f333;
	shfl.sync.bfly.b32 	%r270|%p80, %r266, %r251, %r250, %r252;
	mov.b32 	%f334, %r270;
	add.f32 	%f335, %f333, %f334;
	mov.b32 	%r271, %f335;
	shfl.sync.bfly.b32 	%r273|%p81, %r271, %r255, %r250, %r252;
	mov.b32 	%f336, %r273;
	add.f32 	%f337, %f335, %f336;
	mov.b32 	%r274, %f337;
	shfl.sync.bfly.b32 	%r276|%p82, %r274, %r258, %r250, %r252;
	mov.b32 	%f338, %r276;
	add.f32 	%f339, %f337, %f338;
	mov.b32 	%r277, %f339;
	shfl.sync.bfly.b32 	%r279|%p83, %r277, %r261, %r250, %r252;
	mov.b32 	%f340, %r279;
	add.f32 	%f341, %f339, %f340;
	mov.b32 	%r280, %f341;
	shfl.sync.bfly.b32 	%r282|%p84, %r280, %r264, %r250, %r252;
	mov.b32 	%f342, %r282;
	add.f32 	%f343, %f341, %f342;
	st.local.f32 	[%rd2+24], %f343;

$L__BB39_23:
	bar.sync 	0;
	mov.b32 	%r283, %f382;
	shfl.sync.bfly.b32 	%r287|%p86, %r283, %r251, %r250, %r252;
	mov.b32 	%f344, %r287;
	add.f32 	%f345, %f382, %f344;
	mov.b32 	%r288, %f345;
	shfl.sync.bfly.b32 	%r290|%p87, %r288, %r255, %r250, %r252;
	mov.b32 	%f346, %r290;
	add.f32 	%f347, %f345, %f346;
	mov.b32 	%r291, %f347;
	shfl.sync.bfly.b32 	%r293|%p88, %r291, %r258, %r250, %r252;
	mov.b32 	%f348, %r293;
	add.f32 	%f349, %f347, %f348;
	mov.b32 	%r294, %f349;
	shfl.sync.bfly.b32 	%r296|%p89, %r294, %r261, %r250, %r252;
	mov.b32 	%f350, %r296;
	add.f32 	%f351, %f349, %f350;
	mov.b32 	%r297, %f351;
	shfl.sync.bfly.b32 	%r299|%p90, %r297, %r264, %r250, %r252;
	mov.b32 	%f352, %r299;
	add.f32 	%f353, %f351, %f352;
	st.local.f32 	[%rd2+28], %f353;
	st.shared.f32 	[%r10], %f353;
	bar.sync 	0;
	@%p1 bra 	$L__BB39_25;

	ld.shared.f32 	%f354, [%r4];
	mov.b32 	%r300, %f354;
	mov.u32 	%r301, 31;
	mov.u32 	%r302, 16;
	mov.u32 	%r303, -1;
	shfl.sync.bfly.b32 	%r304|%p91, %r300, %r302, %r301, %r303;
	mov.b32 	%f355, %r304;
	add.f32 	%f356, %f354, %f355;
	mov.b32 	%r305, %f356;
	mov.u32 	%r306, 8;
	shfl.sync.bfly.b32 	%r307|%p92, %r305, %r306, %r301, %r303;
	mov.b32 	%f357, %r307;
	add.f32 	%f358, %f356, %f357;
	mov.b32 	%r308, %f358;
	mov.u32 	%r309, 4;
	shfl.sync.bfly.b32 	%r310|%p93, %r308, %r309, %r301, %r303;
	mov.b32 	%f359, %r310;
	add.f32 	%f360, %f358, %f359;
	mov.b32 	%r311, %f360;
	mov.u32 	%r312, 2;
	shfl.sync.bfly.b32 	%r313|%p94, %r311, %r312, %r301, %r303;
	mov.b32 	%f361, %r313;
	add.f32 	%f362, %f360, %f361;
	mov.b32 	%r314, %f362;
	mov.u32 	%r315, 1;
	shfl.sync.bfly.b32 	%r316|%p95, %r314, %r315, %r301, %r303;
	mov.b32 	%f363, %r316;
	add.f32 	%f364, %f362, %f363;
	st.local.f32 	[%rd2+28], %f364;

$L__BB39_25:
	bar.sync 	0;
	setp.gt.s32 	%p96, %r3, 7;
	@%p96 bra 	$L__BB39_27;

	mul.wide.s32 	%rd68, %r3, 4;
	add.s64 	%rd69, %rd2, %rd68;
	ld.local.f32 	%f365, [%rd69];
	mad.lo.s32 	%r317, %r3, %r13, %r2;
	cvt.s64.s32 	%rd70, %r317;
	mul.lo.s32 	%r318, %r1, %r14;
	cvt.s64.s32 	%rd71, %r318;
	add.s64 	%rd72, %rd71, %rd70;
	cvta.to.global.u64 	%rd73, %rd21;
	shl.b64 	%rd74, %rd72, 2;
	add.s64 	%rd75, %rd73, %rd74;
	st.global.f32 	[%rd75], %f365;

$L__BB39_27:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_1_bs_192
.visible .entry ggml_matvec_f32_ncols_1_bs_192(
	.param .u64 ggml_matvec_f32_ncols_1_bs_192_param_0,
	.param .u64 ggml_matvec_f32_ncols_1_bs_192_param_1,
	.param .u64 ggml_matvec_f32_ncols_1_bs_192_param_2,
	.param .u32 ggml_matvec_f32_ncols_1_bs_192_param_3,
	.param .u32 ggml_matvec_f32_ncols_1_bs_192_param_4,
	.param .u32 ggml_matvec_f32_ncols_1_bs_192_param_5,
	.param .u32 ggml_matvec_f32_ncols_1_bs_192_param_6,
	.param .u32 ggml_matvec_f32_ncols_1_bs_192_param_7,
	.param .u32 ggml_matvec_f32_ncols_1_bs_192_param_8,
	.param .u32 ggml_matvec_f32_ncols_1_bs_192_param_9,
	.param .u32 ggml_matvec_f32_ncols_1_bs_192_param_10,
	.param .u32 ggml_matvec_f32_ncols_1_bs_192_param_11
)
{
	.reg .pred 	%p<19>;
	.reg .f32 	%f<88>;
	.reg .b32 	%r<79>;
	.reg .b64 	%rd<44>;


	ld.param.u64 	%rd18, [ggml_matvec_f32_ncols_1_bs_192_param_0];
	ld.param.u64 	%rd19, [ggml_matvec_f32_ncols_1_bs_192_param_1];
	ld.param.u64 	%rd17, [ggml_matvec_f32_ncols_1_bs_192_param_2];
	ld.param.u32 	%r13, [ggml_matvec_f32_ncols_1_bs_192_param_3];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_1_bs_192_param_5];
	ld.param.u32 	%r14, [ggml_matvec_f32_ncols_1_bs_192_param_7];
	ld.param.u32 	%r17, [ggml_matvec_f32_ncols_1_bs_192_param_8];
	ld.param.u32 	%r18, [ggml_matvec_f32_ncols_1_bs_192_param_9];
	ld.param.u32 	%r19, [ggml_matvec_f32_ncols_1_bs_192_param_10];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_1_bs_192_param_11];
	cvta.to.global.u64 	%rd1, %rd19;
	cvta.to.global.u64 	%rd2, %rd18;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r20, %r1, %r17;
	mov.u32 	%r21, %ctaid.x;
	mul.lo.s32 	%r22, %r21, %r16;
	mad.lo.s32 	%r23, %r20, %r18, %r22;
	cvt.s64.s32 	%rd3, %r23;
	mul.lo.s32 	%r24, %r1, %r19;
	cvt.s64.s32 	%rd4, %r24;
	mov.u32 	%r2, %tid.x;
	setp.gt.s32 	%p1, %r2, 31;
	shl.b32 	%r25, %r2, 2;
	mov.u32 	%r26, data_mmv;
	add.s32 	%r3, %r26, %r25;
	@%p1 bra 	$L__BB40_2;

	mov.u32 	%r27, 0;
	st.shared.u32 	[%r3], %r27;

$L__BB40_2:
	bar.sync 	0;
	setp.ge.s32 	%p2, %r2, %r13;
	mov.f32 	%f86, 0f00000000;
	@%p2 bra 	$L__BB40_9;

	not.b32 	%r28, %r2;
	add.s32 	%r4, %r28, %r13;
	mul.wide.u32 	%rd20, %r4, -1431655765;
	shr.u64 	%rd21, %rd20, 39;
	cvt.u32.u64 	%r29, %rd21;
	add.s32 	%r30, %r29, 1;
	and.b32  	%r76, %r30, 3;
	setp.eq.s32 	%p3, %r76, 0;
	mov.f32 	%f86, 0f00000000;
	mov.u32 	%r77, %r2;
	@%p3 bra 	$L__BB40_6;

	mul.wide.s32 	%rd22, %r2, 2;
	add.s64 	%rd23, %rd22, %rd4;
	shl.b64 	%rd24, %rd23, 2;
	add.s64 	%rd41, %rd1, %rd24;
	add.s64 	%rd25, %rd22, %rd3;
	shl.b64 	%rd26, %rd25, 2;
	add.s64 	%rd40, %rd2, %rd26;
	mov.f32 	%f86, 0f00000000;
	mov.u32 	%r77, %r2;

$L__BB40_5:
	.pragma "nounroll";
	ld.global.nc.v2.f32 	{%f15, %f16}, [%rd40];
	ld.global.nc.v2.f32 	{%f19, %f20}, [%rd41];
	fma.rn.f32 	%f23, %f15, %f19, %f86;
	fma.rn.f32 	%f86, %f16, %f20, %f23;
	add.s32 	%r77, %r77, 192;
	add.s64 	%rd41, %rd41, 1536;
	add.s64 	%rd40, %rd40, 1536;
	add.s32 	%r76, %r76, -1;
	setp.ne.s32 	%p4, %r76, 0;
	@%p4 bra 	$L__BB40_5;

$L__BB40_6:
	setp.lt.u32 	%p5, %r4, 576;
	@%p5 bra 	$L__BB40_9;

	mul.wide.s32 	%rd27, %r77, 2;
	add.s64 	%rd28, %rd27, %rd3;
	shl.b64 	%rd29, %rd28, 2;
	add.s64 	%rd30, %rd2, %rd29;
	add.s64 	%rd43, %rd30, 3072;
	add.s64 	%rd31, %rd27, %rd4;
	shl.b64 	%rd32, %rd31, 2;
	add.s64 	%rd33, %rd1, %rd32;
	add.s64 	%rd42, %rd33, 3072;

$L__BB40_8:
	ld.global.nc.v2.f32 	{%f24, %f25}, [%rd43+-3072];
	ld.global.nc.v2.f32 	{%f28, %f29}, [%rd42+-3072];
	fma.rn.f32 	%f32, %f24, %f28, %f86;
	fma.rn.f32 	%f33, %f25, %f29, %f32;
	ld.global.nc.v2.f32 	{%f34, %f35}, [%rd43+-1536];
	ld.global.nc.v2.f32 	{%f38, %f39}, [%rd42+-1536];
	fma.rn.f32 	%f42, %f34, %f38, %f33;
	fma.rn.f32 	%f43, %f35, %f39, %f42;
	ld.global.nc.v2.f32 	{%f44, %f45}, [%rd43];
	ld.global.nc.v2.f32 	{%f48, %f49}, [%rd42];
	fma.rn.f32 	%f52, %f44, %f48, %f43;
	fma.rn.f32 	%f53, %f45, %f49, %f52;
	ld.global.nc.v2.f32 	{%f54, %f55}, [%rd43+1536];
	ld.global.nc.v2.f32 	{%f58, %f59}, [%rd42+1536];
	fma.rn.f32 	%f62, %f54, %f58, %f53;
	fma.rn.f32 	%f86, %f55, %f59, %f62;
	add.s64 	%rd43, %rd43, 6144;
	add.s64 	%rd42, %rd42, 6144;
	add.s32 	%r77, %r77, 768;
	setp.lt.s32 	%p6, %r77, %r13;
	@%p6 bra 	$L__BB40_8;

$L__BB40_9:
	mov.b32 	%r31, %f86;
	mov.u32 	%r32, 31;
	mov.u32 	%r33, 16;
	mov.u32 	%r34, -1;
	shfl.sync.bfly.b32 	%r35|%p7, %r31, %r33, %r32, %r34;
	mov.b32 	%f63, %r35;
	add.f32 	%f64, %f86, %f63;
	mov.b32 	%r36, %f64;
	mov.u32 	%r37, 8;
	shfl.sync.bfly.b32 	%r38|%p8, %r36, %r37, %r32, %r34;
	mov.b32 	%f65, %r38;
	add.f32 	%f66, %f64, %f65;
	mov.b32 	%r39, %f66;
	mov.u32 	%r40, 4;
	shfl.sync.bfly.b32 	%r41|%p9, %r39, %r40, %r32, %r34;
	mov.b32 	%f67, %r41;
	add.f32 	%f68, %f66, %f67;
	mov.b32 	%r42, %f68;
	mov.u32 	%r43, 2;
	shfl.sync.bfly.b32 	%r44|%p10, %r42, %r43, %r32, %r34;
	mov.b32 	%f69, %r44;
	add.f32 	%f70, %f68, %f69;
	mov.b32 	%r45, %f70;
	mov.u32 	%r46, 1;
	shfl.sync.bfly.b32 	%r47|%p11, %r45, %r46, %r32, %r34;
	mov.b32 	%f71, %r47;
	add.f32 	%f87, %f70, %f71;
	shr.s32 	%r48, %r2, 31;
	shr.u32 	%r49, %r48, 27;
	add.s32 	%r50, %r2, %r49;
	shr.s32 	%r51, %r50, 5;
	shl.b32 	%r52, %r51, 2;
	add.s32 	%r54, %r26, %r52;
	st.shared.f32 	[%r54], %f87;
	bar.sync 	0;
	@%p1 bra 	$L__BB40_11;

	ld.shared.f32 	%f72, [%r3];
	mov.b32 	%r55, %f72;
	shfl.sync.bfly.b32 	%r59|%p13, %r55, %r33, %r32, %r34;
	mov.b32 	%f73, %r59;
	add.f32 	%f74, %f72, %f73;
	mov.b32 	%r60, %f74;
	shfl.sync.bfly.b32 	%r62|%p14, %r60, %r37, %r32, %r34;
	mov.b32 	%f75, %r62;
	add.f32 	%f76, %f74, %f75;
	mov.b32 	%r63, %f76;
	shfl.sync.bfly.b32 	%r65|%p15, %r63, %r40, %r32, %r34;
	mov.b32 	%f77, %r65;
	add.f32 	%f78, %f76, %f77;
	mov.b32 	%r66, %f78;
	shfl.sync.bfly.b32 	%r68|%p16, %r66, %r43, %r32, %r34;
	mov.b32 	%f79, %r68;
	add.f32 	%f80, %f78, %f79;
	mov.b32 	%r69, %f80;
	shfl.sync.bfly.b32 	%r71|%p17, %r69, %r46, %r32, %r34;
	mov.b32 	%f81, %r71;
	add.f32 	%f87, %f80, %f81;

$L__BB40_11:
	bar.sync 	0;
	setp.gt.s32 	%p18, %r2, 0;
	@%p18 bra 	$L__BB40_13;

	mad.lo.s32 	%r73, %r2, %r14, %r21;
	cvt.s64.s32 	%rd34, %r73;
	mul.lo.s32 	%r74, %r1, %r15;
	cvt.s64.s32 	%rd35, %r74;
	add.s64 	%rd36, %rd35, %rd34;
	cvta.to.global.u64 	%rd37, %rd17;
	shl.b64 	%rd38, %rd36, 2;
	add.s64 	%rd39, %rd37, %rd38;
	st.global.f32 	[%rd39], %f87;

$L__BB40_13:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_2_bs_192
.visible .entry ggml_matvec_f32_ncols_2_bs_192(
	.param .u64 ggml_matvec_f32_ncols_2_bs_192_param_0,
	.param .u64 ggml_matvec_f32_ncols_2_bs_192_param_1,
	.param .u64 ggml_matvec_f32_ncols_2_bs_192_param_2,
	.param .u32 ggml_matvec_f32_ncols_2_bs_192_param_3,
	.param .u32 ggml_matvec_f32_ncols_2_bs_192_param_4,
	.param .u32 ggml_matvec_f32_ncols_2_bs_192_param_5,
	.param .u32 ggml_matvec_f32_ncols_2_bs_192_param_6,
	.param .u32 ggml_matvec_f32_ncols_2_bs_192_param_7,
	.param .u32 ggml_matvec_f32_ncols_2_bs_192_param_8,
	.param .u32 ggml_matvec_f32_ncols_2_bs_192_param_9,
	.param .u32 ggml_matvec_f32_ncols_2_bs_192_param_10,
	.param .u32 ggml_matvec_f32_ncols_2_bs_192_param_11
)
{
	.local .align 8 .b8 	__local_depot41[8];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<30>;
	.reg .f32 	%f<146>;
	.reg .b32 	%r<113>;
	.reg .b64 	%rd<66>;


	mov.u64 	%SPL, __local_depot41;
	ld.param.u64 	%rd27, [ggml_matvec_f32_ncols_2_bs_192_param_0];
	ld.param.u64 	%rd28, [ggml_matvec_f32_ncols_2_bs_192_param_1];
	ld.param.u64 	%rd26, [ggml_matvec_f32_ncols_2_bs_192_param_2];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_2_bs_192_param_3];
	ld.param.u32 	%r19, [ggml_matvec_f32_ncols_2_bs_192_param_5];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_2_bs_192_param_6];
	ld.param.u32 	%r17, [ggml_matvec_f32_ncols_2_bs_192_param_7];
	ld.param.u32 	%r20, [ggml_matvec_f32_ncols_2_bs_192_param_8];
	ld.param.u32 	%r21, [ggml_matvec_f32_ncols_2_bs_192_param_9];
	ld.param.u32 	%r22, [ggml_matvec_f32_ncols_2_bs_192_param_10];
	ld.param.u32 	%r18, [ggml_matvec_f32_ncols_2_bs_192_param_11];
	cvta.to.global.u64 	%rd1, %rd28;
	cvta.to.global.u64 	%rd2, %rd27;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r23, %r1, %r20;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r24, %r2, %r19;
	mad.lo.s32 	%r25, %r23, %r21, %r24;
	cvt.s64.s32 	%rd4, %r25;
	mul.lo.s32 	%r26, %r1, %r22;
	cvt.s64.s32 	%rd5, %r26;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r27, %r3, 2;
	mov.u32 	%r28, data_mmv;
	add.s32 	%r4, %r28, %r27;
	@%p1 bra 	$L__BB41_2;

	mov.u32 	%r29, 0;
	st.shared.u32 	[%r4], %r29;

$L__BB41_2:
	bar.sync 	0;
	mov.f32 	%f144, 0f00000000;
	st.local.v2.f32 	[%rd3], {%f144, %f144};
	setp.ge.s32 	%p2, %r3, %r15;
	mov.f32 	%f145, %f144;
	@%p2 bra 	$L__BB41_11;

	not.b32 	%r30, %r3;
	add.s32 	%r5, %r30, %r15;
	mul.wide.u32 	%rd30, %r5, -1431655765;
	shr.u64 	%rd31, %rd30, 39;
	cvt.u32.u64 	%r31, %rd31;
	add.s32 	%r32, %r31, 1;
	and.b32  	%r110, %r32, 3;
	setp.eq.s32 	%p3, %r110, 0;
	mov.f32 	%f144, 0f00000000;
	mov.u32 	%r111, %r3;
	@%p3 bra 	$L__BB41_7;

	mul.wide.s32 	%rd32, %r16, 2;
	mul.wide.s32 	%rd33, %r3, 2;
	add.s64 	%rd34, %rd32, %rd33;
	add.s64 	%rd35, %rd34, %rd5;
	shl.b64 	%rd36, %rd35, 2;
	add.s64 	%rd62, %rd1, %rd36;
	add.s64 	%rd37, %rd33, %rd5;
	shl.b64 	%rd38, %rd37, 2;
	add.s64 	%rd61, %rd1, %rd38;
	add.s64 	%rd39, %rd33, %rd4;
	shl.b64 	%rd40, %rd39, 2;
	add.s64 	%rd60, %rd2, %rd40;
	mov.f32 	%f144, 0f00000000;
	mov.f32 	%f145, %f144;
	mov.u32 	%r111, %r3;

$L__BB41_5:
	.pragma "nounroll";
	ld.global.nc.v2.f32 	{%f19, %f20}, [%rd60];
	ld.global.nc.v2.f32 	{%f23, %f24}, [%rd61];
	fma.rn.f32 	%f27, %f19, %f23, %f145;
	fma.rn.f32 	%f145, %f20, %f24, %f27;
	ld.global.nc.v2.f32 	{%f28, %f29}, [%rd62];
	fma.rn.f32 	%f32, %f19, %f28, %f144;
	fma.rn.f32 	%f144, %f20, %f29, %f32;
	add.s32 	%r111, %r111, 192;
	add.s64 	%rd62, %rd62, 1536;
	add.s64 	%rd61, %rd61, 1536;
	add.s64 	%rd60, %rd60, 1536;
	add.s32 	%r110, %r110, -1;
	setp.ne.s32 	%p4, %r110, 0;
	@%p4 bra 	$L__BB41_5;

	st.local.v2.f32 	[%rd3], {%f145, %f144};

$L__BB41_7:
	setp.lt.u32 	%p5, %r5, 576;
	@%p5 bra 	$L__BB41_11;

	mul.wide.s32 	%rd41, %r111, 2;
	add.s64 	%rd42, %rd41, %rd4;
	shl.b64 	%rd43, %rd42, 2;
	add.s64 	%rd44, %rd2, %rd43;
	add.s64 	%rd65, %rd44, 3072;
	add.s64 	%rd45, %rd41, %rd5;
	shl.b64 	%rd46, %rd45, 2;
	add.s64 	%rd47, %rd1, %rd46;
	add.s64 	%rd64, %rd47, 4608;
	mul.wide.s32 	%rd48, %r16, 2;
	add.s64 	%rd49, %rd45, %rd48;
	shl.b64 	%rd50, %rd49, 2;
	add.s64 	%rd51, %rd1, %rd50;
	add.s64 	%rd63, %rd51, 3072;

$L__BB41_9:
	ld.global.nc.v2.f32 	{%f33, %f34}, [%rd65+-3072];
	ld.global.nc.v2.f32 	{%f37, %f38}, [%rd64+-4608];
	fma.rn.f32 	%f41, %f33, %f37, %f145;
	fma.rn.f32 	%f42, %f34, %f38, %f41;
	ld.global.nc.v2.f32 	{%f43, %f44}, [%rd63+-3072];
	fma.rn.f32 	%f47, %f33, %f43, %f144;
	fma.rn.f32 	%f48, %f34, %f44, %f47;
	ld.global.nc.v2.f32 	{%f49, %f50}, [%rd65+-1536];
	ld.global.nc.v2.f32 	{%f53, %f54}, [%rd64+-3072];
	fma.rn.f32 	%f57, %f49, %f53, %f42;
	fma.rn.f32 	%f58, %f50, %f54, %f57;
	ld.global.nc.v2.f32 	{%f59, %f60}, [%rd63+-1536];
	fma.rn.f32 	%f63, %f49, %f59, %f48;
	fma.rn.f32 	%f64, %f50, %f60, %f63;
	ld.global.nc.v2.f32 	{%f65, %f66}, [%rd65];
	ld.global.nc.v2.f32 	{%f69, %f70}, [%rd64+-1536];
	fma.rn.f32 	%f73, %f65, %f69, %f58;
	fma.rn.f32 	%f74, %f66, %f70, %f73;
	ld.global.nc.v2.f32 	{%f75, %f76}, [%rd63];
	fma.rn.f32 	%f79, %f65, %f75, %f64;
	fma.rn.f32 	%f80, %f66, %f76, %f79;
	ld.global.nc.v2.f32 	{%f81, %f82}, [%rd65+1536];
	ld.global.nc.v2.f32 	{%f85, %f86}, [%rd64];
	fma.rn.f32 	%f89, %f81, %f85, %f74;
	fma.rn.f32 	%f145, %f82, %f86, %f89;
	ld.global.nc.v2.f32 	{%f90, %f91}, [%rd63+1536];
	fma.rn.f32 	%f94, %f81, %f90, %f80;
	fma.rn.f32 	%f144, %f82, %f91, %f94;
	add.s64 	%rd65, %rd65, 6144;
	add.s64 	%rd64, %rd64, 6144;
	add.s64 	%rd63, %rd63, 6144;
	add.s32 	%r111, %r111, 768;
	setp.lt.s32 	%p6, %r111, %r15;
	@%p6 bra 	$L__BB41_9;

	st.local.v2.f32 	[%rd3], {%f145, %f144};

$L__BB41_11:
	shr.s32 	%r33, %r3, 31;
	shr.u32 	%r34, %r33, 27;
	add.s32 	%r35, %r3, %r34;
	shr.s32 	%r36, %r35, 5;
	shl.b32 	%r37, %r36, 2;
	add.s32 	%r14, %r28, %r37;
	mov.u32 	%r39, 2;
	mov.b32 	%r40, %f145;
	mov.u32 	%r41, 31;
	mov.u32 	%r42, 16;
	mov.u32 	%r43, -1;
	shfl.sync.bfly.b32 	%r44|%p7, %r40, %r42, %r41, %r43;
	mov.b32 	%f95, %r44;
	add.f32 	%f96, %f145, %f95;
	mov.b32 	%r45, %f96;
	mov.u32 	%r46, 8;
	shfl.sync.bfly.b32 	%r47|%p8, %r45, %r46, %r41, %r43;
	mov.b32 	%f97, %r47;
	add.f32 	%f98, %f96, %f97;
	mov.b32 	%r48, %f98;
	mov.u32 	%r49, 4;
	shfl.sync.bfly.b32 	%r50|%p9, %r48, %r49, %r41, %r43;
	mov.b32 	%f99, %r50;
	add.f32 	%f100, %f98, %f99;
	mov.b32 	%r51, %f100;
	shfl.sync.bfly.b32 	%r52|%p10, %r51, %r39, %r41, %r43;
	mov.b32 	%f101, %r52;
	add.f32 	%f102, %f100, %f101;
	mov.b32 	%r53, %f102;
	mov.u32 	%r54, 1;
	shfl.sync.bfly.b32 	%r55|%p11, %r53, %r54, %r41, %r43;
	mov.b32 	%f103, %r55;
	add.f32 	%f104, %f102, %f103;
	st.local.f32 	[%rd3], %f104;
	st.shared.f32 	[%r14], %f104;
	bar.sync 	0;
	@%p1 bra 	$L__BB41_13;

	ld.shared.f32 	%f105, [%r4];
	mov.b32 	%r56, %f105;
	shfl.sync.bfly.b32 	%r60|%p13, %r56, %r42, %r41, %r43;
	mov.b32 	%f106, %r60;
	add.f32 	%f107, %f105, %f106;
	mov.b32 	%r61, %f107;
	shfl.sync.bfly.b32 	%r63|%p14, %r61, %r46, %r41, %r43;
	mov.b32 	%f108, %r63;
	add.f32 	%f109, %f107, %f108;
	mov.b32 	%r64, %f109;
	shfl.sync.bfly.b32 	%r66|%p15, %r64, %r49, %r41, %r43;
	mov.b32 	%f110, %r66;
	add.f32 	%f111, %f109, %f110;
	mov.b32 	%r67, %f111;
	shfl.sync.bfly.b32 	%r69|%p16, %r67, %r39, %r41, %r43;
	mov.b32 	%f112, %r69;
	add.f32 	%f113, %f111, %f112;
	mov.b32 	%r70, %f113;
	shfl.sync.bfly.b32 	%r72|%p17, %r70, %r54, %r41, %r43;
	mov.b32 	%f114, %r72;
	add.f32 	%f115, %f113, %f114;
	st.local.f32 	[%rd3], %f115;

$L__BB41_13:
	bar.sync 	0;
	mov.b32 	%r73, %f144;
	shfl.sync.bfly.b32 	%r77|%p19, %r73, %r42, %r41, %r43;
	mov.b32 	%f116, %r77;
	add.f32 	%f117, %f144, %f116;
	mov.b32 	%r78, %f117;
	shfl.sync.bfly.b32 	%r80|%p20, %r78, %r46, %r41, %r43;
	mov.b32 	%f118, %r80;
	add.f32 	%f119, %f117, %f118;
	mov.b32 	%r81, %f119;
	shfl.sync.bfly.b32 	%r83|%p21, %r81, %r49, %r41, %r43;
	mov.b32 	%f120, %r83;
	add.f32 	%f121, %f119, %f120;
	mov.b32 	%r84, %f121;
	shfl.sync.bfly.b32 	%r86|%p22, %r84, %r39, %r41, %r43;
	mov.b32 	%f122, %r86;
	add.f32 	%f123, %f121, %f122;
	mov.b32 	%r87, %f123;
	shfl.sync.bfly.b32 	%r89|%p23, %r87, %r54, %r41, %r43;
	mov.b32 	%f124, %r89;
	add.f32 	%f125, %f123, %f124;
	st.local.f32 	[%rd3+4], %f125;
	st.shared.f32 	[%r14], %f125;
	bar.sync 	0;
	@%p1 bra 	$L__BB41_15;

	ld.shared.f32 	%f126, [%r4];
	mov.b32 	%r90, %f126;
	mov.u32 	%r91, 31;
	mov.u32 	%r92, 16;
	mov.u32 	%r93, -1;
	shfl.sync.bfly.b32 	%r94|%p24, %r90, %r92, %r91, %r93;
	mov.b32 	%f127, %r94;
	add.f32 	%f128, %f126, %f127;
	mov.b32 	%r95, %f128;
	mov.u32 	%r96, 8;
	shfl.sync.bfly.b32 	%r97|%p25, %r95, %r96, %r91, %r93;
	mov.b32 	%f129, %r97;
	add.f32 	%f130, %f128, %f129;
	mov.b32 	%r98, %f130;
	mov.u32 	%r99, 4;
	shfl.sync.bfly.b32 	%r100|%p26, %r98, %r99, %r91, %r93;
	mov.b32 	%f131, %r100;
	add.f32 	%f132, %f130, %f131;
	mov.b32 	%r101, %f132;
	mov.u32 	%r102, 2;
	shfl.sync.bfly.b32 	%r103|%p27, %r101, %r102, %r91, %r93;
	mov.b32 	%f133, %r103;
	add.f32 	%f134, %f132, %f133;
	mov.b32 	%r104, %f134;
	mov.u32 	%r105, 1;
	shfl.sync.bfly.b32 	%r106|%p28, %r104, %r105, %r91, %r93;
	mov.b32 	%f135, %r106;
	add.f32 	%f136, %f134, %f135;
	st.local.f32 	[%rd3+4], %f136;

$L__BB41_15:
	bar.sync 	0;
	setp.gt.s32 	%p29, %r3, 1;
	@%p29 bra 	$L__BB41_17;

	mul.wide.s32 	%rd52, %r3, 4;
	add.s64 	%rd53, %rd3, %rd52;
	ld.local.f32 	%f137, [%rd53];
	mad.lo.s32 	%r107, %r3, %r17, %r2;
	cvt.s64.s32 	%rd54, %r107;
	mul.lo.s32 	%r108, %r1, %r18;
	cvt.s64.s32 	%rd55, %r108;
	add.s64 	%rd56, %rd55, %rd54;
	cvta.to.global.u64 	%rd57, %rd26;
	shl.b64 	%rd58, %rd56, 2;
	add.s64 	%rd59, %rd57, %rd58;
	st.global.f32 	[%rd59], %f137;

$L__BB41_17:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_3_bs_192
.visible .entry ggml_matvec_f32_ncols_3_bs_192(
	.param .u64 ggml_matvec_f32_ncols_3_bs_192_param_0,
	.param .u64 ggml_matvec_f32_ncols_3_bs_192_param_1,
	.param .u64 ggml_matvec_f32_ncols_3_bs_192_param_2,
	.param .u32 ggml_matvec_f32_ncols_3_bs_192_param_3,
	.param .u32 ggml_matvec_f32_ncols_3_bs_192_param_4,
	.param .u32 ggml_matvec_f32_ncols_3_bs_192_param_5,
	.param .u32 ggml_matvec_f32_ncols_3_bs_192_param_6,
	.param .u32 ggml_matvec_f32_ncols_3_bs_192_param_7,
	.param .u32 ggml_matvec_f32_ncols_3_bs_192_param_8,
	.param .u32 ggml_matvec_f32_ncols_3_bs_192_param_9,
	.param .u32 ggml_matvec_f32_ncols_3_bs_192_param_10,
	.param .u32 ggml_matvec_f32_ncols_3_bs_192_param_11
)
{
	.local .align 4 .b8 	__local_depot42[12];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<41>;
	.reg .f32 	%f<208>;
	.reg .b32 	%r<154>;
	.reg .b64 	%rd<74>;


	mov.u64 	%SPL, __local_depot42;
	ld.param.u64 	%rd29, [ggml_matvec_f32_ncols_3_bs_192_param_0];
	ld.param.u64 	%rd30, [ggml_matvec_f32_ncols_3_bs_192_param_1];
	ld.param.u64 	%rd28, [ggml_matvec_f32_ncols_3_bs_192_param_2];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_3_bs_192_param_3];
	ld.param.u32 	%r19, [ggml_matvec_f32_ncols_3_bs_192_param_5];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_3_bs_192_param_6];
	ld.param.u32 	%r17, [ggml_matvec_f32_ncols_3_bs_192_param_7];
	ld.param.u32 	%r20, [ggml_matvec_f32_ncols_3_bs_192_param_8];
	ld.param.u32 	%r21, [ggml_matvec_f32_ncols_3_bs_192_param_9];
	ld.param.u32 	%r22, [ggml_matvec_f32_ncols_3_bs_192_param_10];
	ld.param.u32 	%r18, [ggml_matvec_f32_ncols_3_bs_192_param_11];
	cvta.to.global.u64 	%rd73, %rd30;
	cvta.to.global.u64 	%rd2, %rd29;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r23, %r1, %r20;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r24, %r2, %r19;
	mad.lo.s32 	%r25, %r23, %r21, %r24;
	cvt.s64.s32 	%rd4, %r25;
	mul.lo.s32 	%r26, %r1, %r22;
	cvt.s64.s32 	%rd5, %r26;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r27, %r3, 2;
	mov.u32 	%r28, data_mmv;
	add.s32 	%r4, %r28, %r27;
	@%p1 bra 	$L__BB42_2;

	mov.u32 	%r29, 0;
	st.shared.u32 	[%r4], %r29;

$L__BB42_2:
	bar.sync 	0;
	mov.f32 	%f205, 0f00000000;
	mov.u32 	%r30, 0;
	st.local.u32 	[%rd3], %r30;
	st.local.u32 	[%rd3+4], %r30;
	st.local.u32 	[%rd3+8], %r30;
	setp.ge.s32 	%p2, %r3, %r15;
	mov.f32 	%f206, %f205;
	mov.f32 	%f207, %f205;
	@%p2 bra 	$L__BB42_11;

	not.b32 	%r31, %r3;
	add.s32 	%r5, %r31, %r15;
	mul.wide.u32 	%rd32, %r5, -1431655765;
	shr.u64 	%rd33, %rd32, 39;
	cvt.u32.u64 	%r32, %rd33;
	add.s32 	%r33, %r32, 1;
	and.b32  	%r151, %r33, 3;
	setp.eq.s32 	%p3, %r151, 0;
	mov.f32 	%f205, 0f00000000;
	mov.u32 	%r152, %r3;
	@%p3 bra 	$L__BB42_7;

	shl.b32 	%r34, %r16, 1;
	add.s32 	%r35, %r3, %r34;
	mul.wide.s32 	%rd34, %r35, 2;
	add.s64 	%rd35, %rd34, %rd5;
	shl.b64 	%rd36, %rd35, 2;
	add.s64 	%rd71, %rd73, %rd36;
	mul.wide.s32 	%rd37, %r16, 2;
	mul.wide.s32 	%rd38, %r3, 2;
	add.s64 	%rd39, %rd37, %rd38;
	add.s64 	%rd40, %rd39, %rd5;
	shl.b64 	%rd41, %rd40, 2;
	add.s64 	%rd70, %rd73, %rd41;
	add.s64 	%rd42, %rd38, %rd5;
	shl.b64 	%rd43, %rd42, 2;
	add.s64 	%rd69, %rd73, %rd43;
	add.s64 	%rd44, %rd38, %rd4;
	shl.b64 	%rd45, %rd44, 2;
	add.s64 	%rd68, %rd2, %rd45;
	mov.f32 	%f205, 0f00000000;
	mov.f32 	%f206, %f205;
	mov.f32 	%f207, %f205;
	mov.u32 	%r152, %r3;

$L__BB42_5:
	.pragma "nounroll";
	ld.global.nc.v2.f32 	{%f28, %f29}, [%rd68];
	ld.global.nc.v2.f32 	{%f32, %f33}, [%rd69];
	fma.rn.f32 	%f36, %f28, %f32, %f207;
	fma.rn.f32 	%f207, %f29, %f33, %f36;
	ld.global.nc.v2.f32 	{%f37, %f38}, [%rd70];
	fma.rn.f32 	%f41, %f28, %f37, %f206;
	fma.rn.f32 	%f206, %f29, %f38, %f41;
	ld.global.nc.v2.f32 	{%f42, %f43}, [%rd71];
	fma.rn.f32 	%f46, %f28, %f42, %f205;
	fma.rn.f32 	%f205, %f29, %f43, %f46;
	add.s32 	%r152, %r152, 192;
	add.s64 	%rd71, %rd71, 1536;
	add.s64 	%rd70, %rd70, 1536;
	add.s64 	%rd69, %rd69, 1536;
	add.s64 	%rd68, %rd68, 1536;
	add.s32 	%r151, %r151, -1;
	setp.ne.s32 	%p4, %r151, 0;
	@%p4 bra 	$L__BB42_5;

	st.local.f32 	[%rd3], %f207;
	st.local.f32 	[%rd3+4], %f206;
	st.local.f32 	[%rd3+8], %f205;

$L__BB42_7:
	setp.lt.u32 	%p5, %r5, 576;
	@%p5 bra 	$L__BB42_11;

	add.s32 	%r36, %r152, %r16;
	shl.b32 	%r37, %r16, 1;
	add.s32 	%r38, %r152, %r37;
	add.s32 	%r39, %r36, 192;
	mul.wide.s32 	%rd46, %r39, 8;
	shl.b64 	%rd47, %rd5, 2;
	add.s64 	%rd19, %rd46, %rd47;
	mul.wide.s32 	%rd48, %r38, 8;
	add.s64 	%rd20, %rd48, %rd47;
	mul.wide.s32 	%rd49, %r152, 2;
	add.s64 	%rd50, %rd49, %rd4;
	shl.b64 	%rd51, %rd50, 2;
	add.s64 	%rd52, %rd2, %rd51;
	add.s64 	%rd72, %rd52, 3072;
	mul.wide.s32 	%rd53, %r152, 8;
	add.s64 	%rd22, %rd53, %rd47;
	mul.wide.s32 	%rd54, %r16, 8;
	add.s64 	%rd55, %rd53, %rd54;
	add.s64 	%rd23, %rd55, %rd47;

$L__BB42_9:
	ld.global.nc.v2.f32 	{%f47, %f48}, [%rd72+-3072];
	add.s64 	%rd56, %rd73, %rd22;
	ld.global.nc.v2.f32 	{%f51, %f52}, [%rd56];
	fma.rn.f32 	%f55, %f47, %f51, %f207;
	fma.rn.f32 	%f56, %f48, %f52, %f55;
	add.s64 	%rd57, %rd73, %rd23;
	ld.global.nc.v2.f32 	{%f57, %f58}, [%rd57];
	fma.rn.f32 	%f61, %f47, %f57, %f206;
	fma.rn.f32 	%f62, %f48, %f58, %f61;
	add.s64 	%rd58, %rd73, %rd20;
	ld.global.nc.v2.f32 	{%f63, %f64}, [%rd58];
	fma.rn.f32 	%f67, %f47, %f63, %f205;
	fma.rn.f32 	%f68, %f48, %f64, %f67;
	ld.global.nc.v2.f32 	{%f69, %f70}, [%rd72+-1536];
	ld.global.nc.v2.f32 	{%f73, %f74}, [%rd56+1536];
	fma.rn.f32 	%f77, %f69, %f73, %f56;
	fma.rn.f32 	%f78, %f70, %f74, %f77;
	add.s64 	%rd59, %rd73, %rd19;
	ld.global.nc.v2.f32 	{%f79, %f80}, [%rd59];
	fma.rn.f32 	%f83, %f69, %f79, %f62;
	fma.rn.f32 	%f84, %f70, %f80, %f83;
	ld.global.nc.v2.f32 	{%f85, %f86}, [%rd58+1536];
	fma.rn.f32 	%f89, %f69, %f85, %f68;
	fma.rn.f32 	%f90, %f70, %f86, %f89;
	ld.global.nc.v2.f32 	{%f91, %f92}, [%rd72];
	ld.global.nc.v2.f32 	{%f95, %f96}, [%rd56+3072];
	fma.rn.f32 	%f99, %f91, %f95, %f78;
	fma.rn.f32 	%f100, %f92, %f96, %f99;
	ld.global.nc.v2.f32 	{%f101, %f102}, [%rd59+1536];
	fma.rn.f32 	%f105, %f91, %f101, %f84;
	fma.rn.f32 	%f106, %f92, %f102, %f105;
	ld.global.nc.v2.f32 	{%f107, %f108}, [%rd58+3072];
	fma.rn.f32 	%f111, %f91, %f107, %f90;
	fma.rn.f32 	%f112, %f92, %f108, %f111;
	ld.global.nc.v2.f32 	{%f113, %f114}, [%rd72+1536];
	ld.global.nc.v2.f32 	{%f117, %f118}, [%rd56+4608];
	fma.rn.f32 	%f121, %f113, %f117, %f100;
	fma.rn.f32 	%f207, %f114, %f118, %f121;
	ld.global.nc.v2.f32 	{%f122, %f123}, [%rd59+3072];
	fma.rn.f32 	%f126, %f113, %f122, %f106;
	fma.rn.f32 	%f206, %f114, %f123, %f126;
	ld.global.nc.v2.f32 	{%f127, %f128}, [%rd58+4608];
	fma.rn.f32 	%f131, %f113, %f127, %f112;
	fma.rn.f32 	%f205, %f114, %f128, %f131;
	add.s64 	%rd73, %rd73, 6144;
	add.s64 	%rd72, %rd72, 6144;
	add.s32 	%r152, %r152, 768;
	setp.lt.s32 	%p6, %r152, %r15;
	@%p6 bra 	$L__BB42_9;

	st.local.f32 	[%rd3], %f207;
	st.local.f32 	[%rd3+4], %f206;
	st.local.f32 	[%rd3+8], %f205;

$L__BB42_11:
	shr.s32 	%r40, %r3, 31;
	shr.u32 	%r41, %r40, 27;
	add.s32 	%r42, %r3, %r41;
	shr.s32 	%r43, %r42, 5;
	shl.b32 	%r44, %r43, 2;
	add.s32 	%r14, %r28, %r44;
	mov.u32 	%r46, 2;
	mov.b32 	%r47, %f207;
	mov.u32 	%r48, 31;
	mov.u32 	%r49, 16;
	mov.u32 	%r50, -1;
	shfl.sync.bfly.b32 	%r51|%p7, %r47, %r49, %r48, %r50;
	mov.b32 	%f132, %r51;
	add.f32 	%f133, %f207, %f132;
	mov.b32 	%r52, %f133;
	mov.u32 	%r53, 8;
	shfl.sync.bfly.b32 	%r54|%p8, %r52, %r53, %r48, %r50;
	mov.b32 	%f134, %r54;
	add.f32 	%f135, %f133, %f134;
	mov.b32 	%r55, %f135;
	mov.u32 	%r56, 4;
	shfl.sync.bfly.b32 	%r57|%p9, %r55, %r56, %r48, %r50;
	mov.b32 	%f136, %r57;
	add.f32 	%f137, %f135, %f136;
	mov.b32 	%r58, %f137;
	shfl.sync.bfly.b32 	%r59|%p10, %r58, %r46, %r48, %r50;
	mov.b32 	%f138, %r59;
	add.f32 	%f139, %f137, %f138;
	mov.b32 	%r60, %f139;
	mov.u32 	%r61, 1;
	shfl.sync.bfly.b32 	%r62|%p11, %r60, %r61, %r48, %r50;
	mov.b32 	%f140, %r62;
	add.f32 	%f141, %f139, %f140;
	st.local.f32 	[%rd3], %f141;
	st.shared.f32 	[%r14], %f141;
	bar.sync 	0;
	@%p1 bra 	$L__BB42_13;

	ld.shared.f32 	%f142, [%r4];
	mov.b32 	%r63, %f142;
	shfl.sync.bfly.b32 	%r67|%p13, %r63, %r49, %r48, %r50;
	mov.b32 	%f143, %r67;
	add.f32 	%f144, %f142, %f143;
	mov.b32 	%r68, %f144;
	shfl.sync.bfly.b32 	%r70|%p14, %r68, %r53, %r48, %r50;
	mov.b32 	%f145, %r70;
	add.f32 	%f146, %f144, %f145;
	mov.b32 	%r71, %f146;
	shfl.sync.bfly.b32 	%r73|%p15, %r71, %r56, %r48, %r50;
	mov.b32 	%f147, %r73;
	add.f32 	%f148, %f146, %f147;
	mov.b32 	%r74, %f148;
	shfl.sync.bfly.b32 	%r76|%p16, %r74, %r46, %r48, %r50;
	mov.b32 	%f149, %r76;
	add.f32 	%f150, %f148, %f149;
	mov.b32 	%r77, %f150;
	shfl.sync.bfly.b32 	%r79|%p17, %r77, %r61, %r48, %r50;
	mov.b32 	%f151, %r79;
	add.f32 	%f152, %f150, %f151;
	st.local.f32 	[%rd3], %f152;

$L__BB42_13:
	bar.sync 	0;
	mov.b32 	%r80, %f206;
	shfl.sync.bfly.b32 	%r84|%p19, %r80, %r49, %r48, %r50;
	mov.b32 	%f153, %r84;
	add.f32 	%f154, %f206, %f153;
	mov.b32 	%r85, %f154;
	shfl.sync.bfly.b32 	%r87|%p20, %r85, %r53, %r48, %r50;
	mov.b32 	%f155, %r87;
	add.f32 	%f156, %f154, %f155;
	mov.b32 	%r88, %f156;
	shfl.sync.bfly.b32 	%r90|%p21, %r88, %r56, %r48, %r50;
	mov.b32 	%f157, %r90;
	add.f32 	%f158, %f156, %f157;
	mov.b32 	%r91, %f158;
	shfl.sync.bfly.b32 	%r93|%p22, %r91, %r46, %r48, %r50;
	mov.b32 	%f159, %r93;
	add.f32 	%f160, %f158, %f159;
	mov.b32 	%r94, %f160;
	shfl.sync.bfly.b32 	%r96|%p23, %r94, %r61, %r48, %r50;
	mov.b32 	%f161, %r96;
	add.f32 	%f162, %f160, %f161;
	st.local.f32 	[%rd3+4], %f162;
	st.shared.f32 	[%r14], %f162;
	bar.sync 	0;
	@%p1 bra 	$L__BB42_15;

	ld.shared.f32 	%f163, [%r4];
	mov.b32 	%r97, %f163;
	mov.u32 	%r98, 31;
	mov.u32 	%r99, 16;
	mov.u32 	%r100, -1;
	shfl.sync.bfly.b32 	%r101|%p24, %r97, %r99, %r98, %r100;
	mov.b32 	%f164, %r101;
	add.f32 	%f165, %f163, %f164;
	mov.b32 	%r102, %f165;
	mov.u32 	%r103, 8;
	shfl.sync.bfly.b32 	%r104|%p25, %r102, %r103, %r98, %r100;
	mov.b32 	%f166, %r104;
	add.f32 	%f167, %f165, %f166;
	mov.b32 	%r105, %f167;
	mov.u32 	%r106, 4;
	shfl.sync.bfly.b32 	%r107|%p26, %r105, %r106, %r98, %r100;
	mov.b32 	%f168, %r107;
	add.f32 	%f169, %f167, %f168;
	mov.b32 	%r108, %f169;
	mov.u32 	%r109, 2;
	shfl.sync.bfly.b32 	%r110|%p27, %r108, %r109, %r98, %r100;
	mov.b32 	%f170, %r110;
	add.f32 	%f171, %f169, %f170;
	mov.b32 	%r111, %f171;
	mov.u32 	%r112, 1;
	shfl.sync.bfly.b32 	%r113|%p28, %r111, %r112, %r98, %r100;
	mov.b32 	%f172, %r113;
	add.f32 	%f173, %f171, %f172;
	st.local.f32 	[%rd3+4], %f173;

$L__BB42_15:
	bar.sync 	0;
	mov.b32 	%r114, %f205;
	mov.u32 	%r115, 31;
	mov.u32 	%r116, 16;
	mov.u32 	%r117, -1;
	shfl.sync.bfly.b32 	%r118|%p30, %r114, %r116, %r115, %r117;
	mov.b32 	%f174, %r118;
	add.f32 	%f175, %f205, %f174;
	mov.b32 	%r119, %f175;
	mov.u32 	%r120, 8;
	shfl.sync.bfly.b32 	%r121|%p31, %r119, %r120, %r115, %r117;
	mov.b32 	%f176, %r121;
	add.f32 	%f177, %f175, %f176;
	mov.b32 	%r122, %f177;
	mov.u32 	%r123, 4;
	shfl.sync.bfly.b32 	%r124|%p32, %r122, %r123, %r115, %r117;
	mov.b32 	%f178, %r124;
	add.f32 	%f179, %f177, %f178;
	mov.b32 	%r125, %f179;
	mov.u32 	%r126, 2;
	shfl.sync.bfly.b32 	%r127|%p33, %r125, %r126, %r115, %r117;
	mov.b32 	%f180, %r127;
	add.f32 	%f181, %f179, %f180;
	mov.b32 	%r128, %f181;
	mov.u32 	%r129, 1;
	shfl.sync.bfly.b32 	%r130|%p34, %r128, %r129, %r115, %r117;
	mov.b32 	%f182, %r130;
	add.f32 	%f183, %f181, %f182;
	st.local.f32 	[%rd3+8], %f183;
	st.shared.f32 	[%r14], %f183;
	bar.sync 	0;
	@%p1 bra 	$L__BB42_17;

	ld.shared.f32 	%f184, [%r4];
	mov.b32 	%r131, %f184;
	shfl.sync.bfly.b32 	%r135|%p35, %r131, %r116, %r115, %r117;
	mov.b32 	%f185, %r135;
	add.f32 	%f186, %f184, %f185;
	mov.b32 	%r136, %f186;
	shfl.sync.bfly.b32 	%r138|%p36, %r136, %r120, %r115, %r117;
	mov.b32 	%f187, %r138;
	add.f32 	%f188, %f186, %f187;
	mov.b32 	%r139, %f188;
	shfl.sync.bfly.b32 	%r141|%p37, %r139, %r123, %r115, %r117;
	mov.b32 	%f189, %r141;
	add.f32 	%f190, %f188, %f189;
	mov.b32 	%r142, %f190;
	shfl.sync.bfly.b32 	%r144|%p38, %r142, %r126, %r115, %r117;
	mov.b32 	%f191, %r144;
	add.f32 	%f192, %f190, %f191;
	mov.b32 	%r145, %f192;
	shfl.sync.bfly.b32 	%r147|%p39, %r145, %r129, %r115, %r117;
	mov.b32 	%f193, %r147;
	add.f32 	%f194, %f192, %f193;
	st.local.f32 	[%rd3+8], %f194;

$L__BB42_17:
	bar.sync 	0;
	setp.gt.s32 	%p40, %r3, 2;
	@%p40 bra 	$L__BB42_19;

	mul.wide.s32 	%rd60, %r3, 4;
	add.s64 	%rd61, %rd3, %rd60;
	ld.local.f32 	%f195, [%rd61];
	mad.lo.s32 	%r148, %r3, %r17, %r2;
	cvt.s64.s32 	%rd62, %r148;
	mul.lo.s32 	%r149, %r1, %r18;
	cvt.s64.s32 	%rd63, %r149;
	add.s64 	%rd64, %rd63, %rd62;
	cvta.to.global.u64 	%rd65, %rd28;
	shl.b64 	%rd66, %rd64, 2;
	add.s64 	%rd67, %rd65, %rd66;
	st.global.f32 	[%rd67], %f195;

$L__BB42_19:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_4_bs_192
.visible .entry ggml_matvec_f32_ncols_4_bs_192(
	.param .u64 ggml_matvec_f32_ncols_4_bs_192_param_0,
	.param .u64 ggml_matvec_f32_ncols_4_bs_192_param_1,
	.param .u64 ggml_matvec_f32_ncols_4_bs_192_param_2,
	.param .u32 ggml_matvec_f32_ncols_4_bs_192_param_3,
	.param .u32 ggml_matvec_f32_ncols_4_bs_192_param_4,
	.param .u32 ggml_matvec_f32_ncols_4_bs_192_param_5,
	.param .u32 ggml_matvec_f32_ncols_4_bs_192_param_6,
	.param .u32 ggml_matvec_f32_ncols_4_bs_192_param_7,
	.param .u32 ggml_matvec_f32_ncols_4_bs_192_param_8,
	.param .u32 ggml_matvec_f32_ncols_4_bs_192_param_9,
	.param .u32 ggml_matvec_f32_ncols_4_bs_192_param_10,
	.param .u32 ggml_matvec_f32_ncols_4_bs_192_param_11
)
{
	.local .align 16 .b8 	__local_depot43[16];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<52>;
	.reg .f32 	%f<270>;
	.reg .b32 	%r<189>;
	.reg .b64 	%rd<85>;


	mov.u64 	%SPL, __local_depot43;
	ld.param.u64 	%rd34, [ggml_matvec_f32_ncols_4_bs_192_param_0];
	ld.param.u64 	%rd35, [ggml_matvec_f32_ncols_4_bs_192_param_1];
	ld.param.u64 	%rd33, [ggml_matvec_f32_ncols_4_bs_192_param_2];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_4_bs_192_param_3];
	ld.param.u32 	%r19, [ggml_matvec_f32_ncols_4_bs_192_param_5];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_4_bs_192_param_6];
	ld.param.u32 	%r17, [ggml_matvec_f32_ncols_4_bs_192_param_7];
	ld.param.u32 	%r20, [ggml_matvec_f32_ncols_4_bs_192_param_8];
	ld.param.u32 	%r21, [ggml_matvec_f32_ncols_4_bs_192_param_9];
	ld.param.u32 	%r22, [ggml_matvec_f32_ncols_4_bs_192_param_10];
	ld.param.u32 	%r18, [ggml_matvec_f32_ncols_4_bs_192_param_11];
	cvta.to.global.u64 	%rd84, %rd35;
	cvta.to.global.u64 	%rd2, %rd34;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r23, %r1, %r20;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r24, %r2, %r19;
	mad.lo.s32 	%r25, %r23, %r21, %r24;
	cvt.s64.s32 	%rd4, %r25;
	mul.lo.s32 	%r26, %r1, %r22;
	cvt.s64.s32 	%rd5, %r26;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r27, %r3, 2;
	mov.u32 	%r28, data_mmv;
	add.s32 	%r4, %r28, %r27;
	@%p1 bra 	$L__BB43_2;

	mov.u32 	%r29, 0;
	st.shared.u32 	[%r4], %r29;

$L__BB43_2:
	bar.sync 	0;
	mov.f32 	%f266, 0f00000000;
	st.local.v4.f32 	[%rd3], {%f266, %f266, %f266, %f266};
	setp.ge.s32 	%p2, %r3, %r15;
	mov.f32 	%f267, %f266;
	mov.f32 	%f268, %f266;
	mov.f32 	%f269, %f266;
	@%p2 bra 	$L__BB43_11;

	not.b32 	%r30, %r3;
	add.s32 	%r5, %r30, %r15;
	mul.wide.u32 	%rd37, %r5, -1431655765;
	shr.u64 	%rd38, %rd37, 39;
	cvt.u32.u64 	%r31, %rd38;
	add.s32 	%r32, %r31, 1;
	and.b32  	%r186, %r32, 3;
	setp.eq.s32 	%p3, %r186, 0;
	mov.f32 	%f266, 0f00000000;
	mov.u32 	%r187, %r3;
	@%p3 bra 	$L__BB43_7;

	shl.b32 	%r33, %r16, 1;
	add.s32 	%r34, %r3, %r33;
	mul.wide.s32 	%rd39, %r34, 2;
	add.s64 	%rd40, %rd39, %rd5;
	shl.b64 	%rd41, %rd40, 2;
	add.s64 	%rd82, %rd84, %rd41;
	mad.lo.s32 	%r35, %r16, 3, %r3;
	mul.wide.s32 	%rd42, %r35, 2;
	add.s64 	%rd43, %rd42, %rd5;
	shl.b64 	%rd44, %rd43, 2;
	add.s64 	%rd81, %rd84, %rd44;
	mul.wide.s32 	%rd45, %r16, 2;
	mul.wide.s32 	%rd46, %r3, 2;
	add.s64 	%rd47, %rd45, %rd46;
	add.s64 	%rd48, %rd47, %rd5;
	shl.b64 	%rd49, %rd48, 2;
	add.s64 	%rd80, %rd84, %rd49;
	add.s64 	%rd50, %rd46, %rd5;
	shl.b64 	%rd51, %rd50, 2;
	add.s64 	%rd79, %rd84, %rd51;
	add.s64 	%rd52, %rd46, %rd4;
	shl.b64 	%rd53, %rd52, 2;
	add.s64 	%rd78, %rd2, %rd53;
	mov.f32 	%f266, 0f00000000;
	mov.f32 	%f267, %f266;
	mov.f32 	%f268, %f266;
	mov.f32 	%f269, %f266;
	mov.u32 	%r187, %r3;

$L__BB43_5:
	.pragma "nounroll";
	ld.global.nc.v2.f32 	{%f37, %f38}, [%rd78];
	ld.global.nc.v2.f32 	{%f41, %f42}, [%rd79];
	fma.rn.f32 	%f45, %f37, %f41, %f269;
	fma.rn.f32 	%f269, %f38, %f42, %f45;
	ld.global.nc.v2.f32 	{%f46, %f47}, [%rd80];
	fma.rn.f32 	%f50, %f37, %f46, %f268;
	fma.rn.f32 	%f268, %f38, %f47, %f50;
	ld.global.nc.v2.f32 	{%f51, %f52}, [%rd82];
	fma.rn.f32 	%f55, %f37, %f51, %f267;
	fma.rn.f32 	%f267, %f38, %f52, %f55;
	ld.global.nc.v2.f32 	{%f56, %f57}, [%rd81];
	fma.rn.f32 	%f60, %f37, %f56, %f266;
	fma.rn.f32 	%f266, %f38, %f57, %f60;
	add.s32 	%r187, %r187, 192;
	add.s64 	%rd82, %rd82, 1536;
	add.s64 	%rd81, %rd81, 1536;
	add.s64 	%rd80, %rd80, 1536;
	add.s64 	%rd79, %rd79, 1536;
	add.s64 	%rd78, %rd78, 1536;
	add.s32 	%r186, %r186, -1;
	setp.ne.s32 	%p4, %r186, 0;
	@%p4 bra 	$L__BB43_5;

	st.local.v4.f32 	[%rd3], {%f269, %f268, %f267, %f266};

$L__BB43_7:
	setp.lt.u32 	%p5, %r5, 576;
	@%p5 bra 	$L__BB43_11;

	add.s32 	%r36, %r187, %r16;
	shl.b32 	%r37, %r16, 1;
	add.s32 	%r38, %r187, %r37;
	mad.lo.s32 	%r39, %r16, 3, %r187;
	add.s32 	%r40, %r36, 192;
	mul.wide.s32 	%rd54, %r40, 8;
	shl.b64 	%rd55, %rd5, 2;
	add.s64 	%rd23, %rd54, %rd55;
	mul.wide.s32 	%rd56, %r38, 8;
	add.s64 	%rd24, %rd56, %rd55;
	mul.wide.s32 	%rd57, %r39, 8;
	add.s64 	%rd25, %rd57, %rd55;
	mul.wide.s32 	%rd58, %r187, 2;
	add.s64 	%rd59, %rd58, %rd4;
	shl.b64 	%rd60, %rd59, 2;
	add.s64 	%rd61, %rd2, %rd60;
	add.s64 	%rd83, %rd61, 3072;
	mul.wide.s32 	%rd62, %r187, 8;
	add.s64 	%rd27, %rd62, %rd55;
	mul.wide.s32 	%rd63, %r16, 8;
	add.s64 	%rd64, %rd62, %rd63;
	add.s64 	%rd28, %rd64, %rd55;

$L__BB43_9:
	ld.global.nc.v2.f32 	{%f61, %f62}, [%rd83+-3072];
	add.s64 	%rd65, %rd84, %rd27;
	ld.global.nc.v2.f32 	{%f65, %f66}, [%rd65];
	fma.rn.f32 	%f69, %f61, %f65, %f269;
	fma.rn.f32 	%f70, %f62, %f66, %f69;
	add.s64 	%rd66, %rd84, %rd28;
	ld.global.nc.v2.f32 	{%f71, %f72}, [%rd66];
	fma.rn.f32 	%f75, %f61, %f71, %f268;
	fma.rn.f32 	%f76, %f62, %f72, %f75;
	add.s64 	%rd67, %rd84, %rd24;
	ld.global.nc.v2.f32 	{%f77, %f78}, [%rd67];
	fma.rn.f32 	%f81, %f61, %f77, %f267;
	fma.rn.f32 	%f82, %f62, %f78, %f81;
	add.s64 	%rd68, %rd84, %rd25;
	ld.global.nc.v2.f32 	{%f83, %f84}, [%rd68];
	fma.rn.f32 	%f87, %f61, %f83, %f266;
	fma.rn.f32 	%f88, %f62, %f84, %f87;
	ld.global.nc.v2.f32 	{%f89, %f90}, [%rd83+-1536];
	ld.global.nc.v2.f32 	{%f93, %f94}, [%rd65+1536];
	fma.rn.f32 	%f97, %f89, %f93, %f70;
	fma.rn.f32 	%f98, %f90, %f94, %f97;
	add.s64 	%rd69, %rd84, %rd23;
	ld.global.nc.v2.f32 	{%f99, %f100}, [%rd69];
	fma.rn.f32 	%f103, %f89, %f99, %f76;
	fma.rn.f32 	%f104, %f90, %f100, %f103;
	ld.global.nc.v2.f32 	{%f105, %f106}, [%rd67+1536];
	fma.rn.f32 	%f109, %f89, %f105, %f82;
	fma.rn.f32 	%f110, %f90, %f106, %f109;
	ld.global.nc.v2.f32 	{%f111, %f112}, [%rd68+1536];
	fma.rn.f32 	%f115, %f89, %f111, %f88;
	fma.rn.f32 	%f116, %f90, %f112, %f115;
	ld.global.nc.v2.f32 	{%f117, %f118}, [%rd83];
	ld.global.nc.v2.f32 	{%f121, %f122}, [%rd65+3072];
	fma.rn.f32 	%f125, %f117, %f121, %f98;
	fma.rn.f32 	%f126, %f118, %f122, %f125;
	ld.global.nc.v2.f32 	{%f127, %f128}, [%rd69+1536];
	fma.rn.f32 	%f131, %f117, %f127, %f104;
	fma.rn.f32 	%f132, %f118, %f128, %f131;
	ld.global.nc.v2.f32 	{%f133, %f134}, [%rd67+3072];
	fma.rn.f32 	%f137, %f117, %f133, %f110;
	fma.rn.f32 	%f138, %f118, %f134, %f137;
	ld.global.nc.v2.f32 	{%f139, %f140}, [%rd68+3072];
	fma.rn.f32 	%f143, %f117, %f139, %f116;
	fma.rn.f32 	%f144, %f118, %f140, %f143;
	ld.global.nc.v2.f32 	{%f145, %f146}, [%rd83+1536];
	ld.global.nc.v2.f32 	{%f149, %f150}, [%rd65+4608];
	fma.rn.f32 	%f153, %f145, %f149, %f126;
	fma.rn.f32 	%f269, %f146, %f150, %f153;
	ld.global.nc.v2.f32 	{%f154, %f155}, [%rd69+3072];
	fma.rn.f32 	%f158, %f145, %f154, %f132;
	fma.rn.f32 	%f268, %f146, %f155, %f158;
	ld.global.nc.v2.f32 	{%f159, %f160}, [%rd67+4608];
	fma.rn.f32 	%f163, %f145, %f159, %f138;
	fma.rn.f32 	%f267, %f146, %f160, %f163;
	ld.global.nc.v2.f32 	{%f164, %f165}, [%rd68+4608];
	fma.rn.f32 	%f168, %f145, %f164, %f144;
	fma.rn.f32 	%f266, %f146, %f165, %f168;
	add.s64 	%rd84, %rd84, 6144;
	add.s64 	%rd83, %rd83, 6144;
	add.s32 	%r187, %r187, 768;
	setp.lt.s32 	%p6, %r187, %r15;
	@%p6 bra 	$L__BB43_9;

	st.local.v4.f32 	[%rd3], {%f269, %f268, %f267, %f266};

$L__BB43_11:
	shr.s32 	%r41, %r3, 31;
	shr.u32 	%r42, %r41, 27;
	add.s32 	%r43, %r3, %r42;
	shr.s32 	%r44, %r43, 5;
	shl.b32 	%r45, %r44, 2;
	add.s32 	%r14, %r28, %r45;
	mov.u32 	%r47, 2;
	mov.b32 	%r48, %f269;
	mov.u32 	%r49, 31;
	mov.u32 	%r50, 16;
	mov.u32 	%r51, -1;
	shfl.sync.bfly.b32 	%r52|%p7, %r48, %r50, %r49, %r51;
	mov.b32 	%f169, %r52;
	add.f32 	%f170, %f269, %f169;
	mov.b32 	%r53, %f170;
	mov.u32 	%r54, 8;
	shfl.sync.bfly.b32 	%r55|%p8, %r53, %r54, %r49, %r51;
	mov.b32 	%f171, %r55;
	add.f32 	%f172, %f170, %f171;
	mov.b32 	%r56, %f172;
	mov.u32 	%r57, 4;
	shfl.sync.bfly.b32 	%r58|%p9, %r56, %r57, %r49, %r51;
	mov.b32 	%f173, %r58;
	add.f32 	%f174, %f172, %f173;
	mov.b32 	%r59, %f174;
	shfl.sync.bfly.b32 	%r60|%p10, %r59, %r47, %r49, %r51;
	mov.b32 	%f175, %r60;
	add.f32 	%f176, %f174, %f175;
	mov.b32 	%r61, %f176;
	mov.u32 	%r62, 1;
	shfl.sync.bfly.b32 	%r63|%p11, %r61, %r62, %r49, %r51;
	mov.b32 	%f177, %r63;
	add.f32 	%f178, %f176, %f177;
	st.local.f32 	[%rd3], %f178;
	st.shared.f32 	[%r14], %f178;
	bar.sync 	0;
	@%p1 bra 	$L__BB43_13;

	ld.shared.f32 	%f179, [%r4];
	mov.b32 	%r64, %f179;
	shfl.sync.bfly.b32 	%r68|%p13, %r64, %r50, %r49, %r51;
	mov.b32 	%f180, %r68;
	add.f32 	%f181, %f179, %f180;
	mov.b32 	%r69, %f181;
	shfl.sync.bfly.b32 	%r71|%p14, %r69, %r54, %r49, %r51;
	mov.b32 	%f182, %r71;
	add.f32 	%f183, %f181, %f182;
	mov.b32 	%r72, %f183;
	shfl.sync.bfly.b32 	%r74|%p15, %r72, %r57, %r49, %r51;
	mov.b32 	%f184, %r74;
	add.f32 	%f185, %f183, %f184;
	mov.b32 	%r75, %f185;
	shfl.sync.bfly.b32 	%r77|%p16, %r75, %r47, %r49, %r51;
	mov.b32 	%f186, %r77;
	add.f32 	%f187, %f185, %f186;
	mov.b32 	%r78, %f187;
	shfl.sync.bfly.b32 	%r80|%p17, %r78, %r62, %r49, %r51;
	mov.b32 	%f188, %r80;
	add.f32 	%f189, %f187, %f188;
	st.local.f32 	[%rd3], %f189;

$L__BB43_13:
	bar.sync 	0;
	mov.b32 	%r81, %f268;
	shfl.sync.bfly.b32 	%r85|%p19, %r81, %r50, %r49, %r51;
	mov.b32 	%f190, %r85;
	add.f32 	%f191, %f268, %f190;
	mov.b32 	%r86, %f191;
	shfl.sync.bfly.b32 	%r88|%p20, %r86, %r54, %r49, %r51;
	mov.b32 	%f192, %r88;
	add.f32 	%f193, %f191, %f192;
	mov.b32 	%r89, %f193;
	shfl.sync.bfly.b32 	%r91|%p21, %r89, %r57, %r49, %r51;
	mov.b32 	%f194, %r91;
	add.f32 	%f195, %f193, %f194;
	mov.b32 	%r92, %f195;
	shfl.sync.bfly.b32 	%r94|%p22, %r92, %r47, %r49, %r51;
	mov.b32 	%f196, %r94;
	add.f32 	%f197, %f195, %f196;
	mov.b32 	%r95, %f197;
	shfl.sync.bfly.b32 	%r97|%p23, %r95, %r62, %r49, %r51;
	mov.b32 	%f198, %r97;
	add.f32 	%f199, %f197, %f198;
	st.local.f32 	[%rd3+4], %f199;
	st.shared.f32 	[%r14], %f199;
	bar.sync 	0;
	@%p1 bra 	$L__BB43_15;

	ld.shared.f32 	%f200, [%r4];
	mov.b32 	%r98, %f200;
	mov.u32 	%r99, 31;
	mov.u32 	%r100, 16;
	mov.u32 	%r101, -1;
	shfl.sync.bfly.b32 	%r102|%p24, %r98, %r100, %r99, %r101;
	mov.b32 	%f201, %r102;
	add.f32 	%f202, %f200, %f201;
	mov.b32 	%r103, %f202;
	mov.u32 	%r104, 8;
	shfl.sync.bfly.b32 	%r105|%p25, %r103, %r104, %r99, %r101;
	mov.b32 	%f203, %r105;
	add.f32 	%f204, %f202, %f203;
	mov.b32 	%r106, %f204;
	mov.u32 	%r107, 4;
	shfl.sync.bfly.b32 	%r108|%p26, %r106, %r107, %r99, %r101;
	mov.b32 	%f205, %r108;
	add.f32 	%f206, %f204, %f205;
	mov.b32 	%r109, %f206;
	mov.u32 	%r110, 2;
	shfl.sync.bfly.b32 	%r111|%p27, %r109, %r110, %r99, %r101;
	mov.b32 	%f207, %r111;
	add.f32 	%f208, %f206, %f207;
	mov.b32 	%r112, %f208;
	mov.u32 	%r113, 1;
	shfl.sync.bfly.b32 	%r114|%p28, %r112, %r113, %r99, %r101;
	mov.b32 	%f209, %r114;
	add.f32 	%f210, %f208, %f209;
	st.local.f32 	[%rd3+4], %f210;

$L__BB43_15:
	bar.sync 	0;
	mov.b32 	%r115, %f267;
	mov.u32 	%r116, 31;
	mov.u32 	%r117, 16;
	mov.u32 	%r118, -1;
	shfl.sync.bfly.b32 	%r119|%p30, %r115, %r117, %r116, %r118;
	mov.b32 	%f211, %r119;
	add.f32 	%f212, %f267, %f211;
	mov.b32 	%r120, %f212;
	mov.u32 	%r121, 8;
	shfl.sync.bfly.b32 	%r122|%p31, %r120, %r121, %r116, %r118;
	mov.b32 	%f213, %r122;
	add.f32 	%f214, %f212, %f213;
	mov.b32 	%r123, %f214;
	mov.u32 	%r124, 4;
	shfl.sync.bfly.b32 	%r125|%p32, %r123, %r124, %r116, %r118;
	mov.b32 	%f215, %r125;
	add.f32 	%f216, %f214, %f215;
	mov.b32 	%r126, %f216;
	mov.u32 	%r127, 2;
	shfl.sync.bfly.b32 	%r128|%p33, %r126, %r127, %r116, %r118;
	mov.b32 	%f217, %r128;
	add.f32 	%f218, %f216, %f217;
	mov.b32 	%r129, %f218;
	mov.u32 	%r130, 1;
	shfl.sync.bfly.b32 	%r131|%p34, %r129, %r130, %r116, %r118;
	mov.b32 	%f219, %r131;
	add.f32 	%f220, %f218, %f219;
	st.local.f32 	[%rd3+8], %f220;
	st.shared.f32 	[%r14], %f220;
	bar.sync 	0;
	@%p1 bra 	$L__BB43_17;

	ld.shared.f32 	%f221, [%r4];
	mov.b32 	%r132, %f221;
	shfl.sync.bfly.b32 	%r136|%p35, %r132, %r117, %r116, %r118;
	mov.b32 	%f222, %r136;
	add.f32 	%f223, %f221, %f222;
	mov.b32 	%r137, %f223;
	shfl.sync.bfly.b32 	%r139|%p36, %r137, %r121, %r116, %r118;
	mov.b32 	%f224, %r139;
	add.f32 	%f225, %f223, %f224;
	mov.b32 	%r140, %f225;
	shfl.sync.bfly.b32 	%r142|%p37, %r140, %r124, %r116, %r118;
	mov.b32 	%f226, %r142;
	add.f32 	%f227, %f225, %f226;
	mov.b32 	%r143, %f227;
	shfl.sync.bfly.b32 	%r145|%p38, %r143, %r127, %r116, %r118;
	mov.b32 	%f228, %r145;
	add.f32 	%f229, %f227, %f228;
	mov.b32 	%r146, %f229;
	shfl.sync.bfly.b32 	%r148|%p39, %r146, %r130, %r116, %r118;
	mov.b32 	%f230, %r148;
	add.f32 	%f231, %f229, %f230;
	st.local.f32 	[%rd3+8], %f231;

$L__BB43_17:
	bar.sync 	0;
	mov.b32 	%r149, %f266;
	shfl.sync.bfly.b32 	%r153|%p41, %r149, %r117, %r116, %r118;
	mov.b32 	%f232, %r153;
	add.f32 	%f233, %f266, %f232;
	mov.b32 	%r154, %f233;
	shfl.sync.bfly.b32 	%r156|%p42, %r154, %r121, %r116, %r118;
	mov.b32 	%f234, %r156;
	add.f32 	%f235, %f233, %f234;
	mov.b32 	%r157, %f235;
	shfl.sync.bfly.b32 	%r159|%p43, %r157, %r124, %r116, %r118;
	mov.b32 	%f236, %r159;
	add.f32 	%f237, %f235, %f236;
	mov.b32 	%r160, %f237;
	shfl.sync.bfly.b32 	%r162|%p44, %r160, %r127, %r116, %r118;
	mov.b32 	%f238, %r162;
	add.f32 	%f239, %f237, %f238;
	mov.b32 	%r163, %f239;
	shfl.sync.bfly.b32 	%r165|%p45, %r163, %r130, %r116, %r118;
	mov.b32 	%f240, %r165;
	add.f32 	%f241, %f239, %f240;
	st.local.f32 	[%rd3+12], %f241;
	st.shared.f32 	[%r14], %f241;
	bar.sync 	0;
	@%p1 bra 	$L__BB43_19;

	ld.shared.f32 	%f242, [%r4];
	mov.b32 	%r166, %f242;
	mov.u32 	%r167, 31;
	mov.u32 	%r168, 16;
	mov.u32 	%r169, -1;
	shfl.sync.bfly.b32 	%r170|%p46, %r166, %r168, %r167, %r169;
	mov.b32 	%f243, %r170;
	add.f32 	%f244, %f242, %f243;
	mov.b32 	%r171, %f244;
	mov.u32 	%r172, 8;
	shfl.sync.bfly.b32 	%r173|%p47, %r171, %r172, %r167, %r169;
	mov.b32 	%f245, %r173;
	add.f32 	%f246, %f244, %f245;
	mov.b32 	%r174, %f246;
	mov.u32 	%r175, 4;
	shfl.sync.bfly.b32 	%r176|%p48, %r174, %r175, %r167, %r169;
	mov.b32 	%f247, %r176;
	add.f32 	%f248, %f246, %f247;
	mov.b32 	%r177, %f248;
	mov.u32 	%r178, 2;
	shfl.sync.bfly.b32 	%r179|%p49, %r177, %r178, %r167, %r169;
	mov.b32 	%f249, %r179;
	add.f32 	%f250, %f248, %f249;
	mov.b32 	%r180, %f250;
	mov.u32 	%r181, 1;
	shfl.sync.bfly.b32 	%r182|%p50, %r180, %r181, %r167, %r169;
	mov.b32 	%f251, %r182;
	add.f32 	%f252, %f250, %f251;
	st.local.f32 	[%rd3+12], %f252;

$L__BB43_19:
	bar.sync 	0;
	setp.gt.s32 	%p51, %r3, 3;
	@%p51 bra 	$L__BB43_21;

	mul.wide.s32 	%rd70, %r3, 4;
	add.s64 	%rd71, %rd3, %rd70;
	ld.local.f32 	%f253, [%rd71];
	mad.lo.s32 	%r183, %r3, %r17, %r2;
	cvt.s64.s32 	%rd72, %r183;
	mul.lo.s32 	%r184, %r1, %r18;
	cvt.s64.s32 	%rd73, %r184;
	add.s64 	%rd74, %rd73, %rd72;
	cvta.to.global.u64 	%rd75, %rd33;
	shl.b64 	%rd76, %rd74, 2;
	add.s64 	%rd77, %rd75, %rd76;
	st.global.f32 	[%rd77], %f253;

$L__BB43_21:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_5_bs_192
.visible .entry ggml_matvec_f32_ncols_5_bs_192(
	.param .u64 ggml_matvec_f32_ncols_5_bs_192_param_0,
	.param .u64 ggml_matvec_f32_ncols_5_bs_192_param_1,
	.param .u64 ggml_matvec_f32_ncols_5_bs_192_param_2,
	.param .u32 ggml_matvec_f32_ncols_5_bs_192_param_3,
	.param .u32 ggml_matvec_f32_ncols_5_bs_192_param_4,
	.param .u32 ggml_matvec_f32_ncols_5_bs_192_param_5,
	.param .u32 ggml_matvec_f32_ncols_5_bs_192_param_6,
	.param .u32 ggml_matvec_f32_ncols_5_bs_192_param_7,
	.param .u32 ggml_matvec_f32_ncols_5_bs_192_param_8,
	.param .u32 ggml_matvec_f32_ncols_5_bs_192_param_9,
	.param .u32 ggml_matvec_f32_ncols_5_bs_192_param_10,
	.param .u32 ggml_matvec_f32_ncols_5_bs_192_param_11
)
{
	.local .align 4 .b8 	__local_depot44[20];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<63>;
	.reg .f32 	%f<332>;
	.reg .b32 	%r<225>;
	.reg .b64 	%rd<76>;


	mov.u64 	%SPL, __local_depot44;
	ld.param.u64 	%rd28, [ggml_matvec_f32_ncols_5_bs_192_param_0];
	ld.param.u64 	%rd29, [ggml_matvec_f32_ncols_5_bs_192_param_1];
	ld.param.u64 	%rd27, [ggml_matvec_f32_ncols_5_bs_192_param_2];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_5_bs_192_param_3];
	ld.param.u32 	%r19, [ggml_matvec_f32_ncols_5_bs_192_param_5];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_5_bs_192_param_6];
	ld.param.u32 	%r17, [ggml_matvec_f32_ncols_5_bs_192_param_7];
	ld.param.u32 	%r20, [ggml_matvec_f32_ncols_5_bs_192_param_8];
	ld.param.u32 	%r21, [ggml_matvec_f32_ncols_5_bs_192_param_9];
	ld.param.u32 	%r22, [ggml_matvec_f32_ncols_5_bs_192_param_10];
	ld.param.u32 	%r18, [ggml_matvec_f32_ncols_5_bs_192_param_11];
	cvta.to.global.u64 	%rd75, %rd29;
	cvta.to.global.u64 	%rd2, %rd28;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r23, %r1, %r20;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r24, %r2, %r19;
	mad.lo.s32 	%r25, %r23, %r21, %r24;
	cvt.s64.s32 	%rd4, %r25;
	mul.lo.s32 	%r26, %r1, %r22;
	cvt.s64.s32 	%rd5, %r26;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r27, %r3, 2;
	mov.u32 	%r28, data_mmv;
	add.s32 	%r4, %r28, %r27;
	@%p1 bra 	$L__BB44_2;

	mov.u32 	%r29, 0;
	st.shared.u32 	[%r4], %r29;

$L__BB44_2:
	bar.sync 	0;
	mov.f32 	%f327, 0f00000000;
	mov.u32 	%r30, 0;
	st.local.u32 	[%rd3], %r30;
	st.local.u32 	[%rd3+4], %r30;
	st.local.u32 	[%rd3+8], %r30;
	st.local.u32 	[%rd3+12], %r30;
	st.local.u32 	[%rd3+16], %r30;
	setp.ge.s32 	%p2, %r3, %r15;
	mov.f32 	%f328, %f327;
	mov.f32 	%f329, %f327;
	mov.f32 	%f330, %f327;
	mov.f32 	%f331, %f327;
	@%p2 bra 	$L__BB44_11;

	not.b32 	%r31, %r3;
	add.s32 	%r5, %r31, %r15;
	mul.wide.u32 	%rd31, %r5, -1431655765;
	shr.u64 	%rd32, %rd31, 39;
	cvt.u32.u64 	%r32, %rd32;
	add.s32 	%r33, %r32, 1;
	and.b32  	%r222, %r33, 3;
	setp.eq.s32 	%p3, %r222, 0;
	mov.f32 	%f327, 0f00000000;
	mov.u32 	%r223, %r3;
	@%p3 bra 	$L__BB44_7;

	shl.b32 	%r34, %r16, 1;
	mad.lo.s32 	%r35, %r16, 3, %r3;
	mul.wide.s32 	%rd33, %r35, 8;
	shl.b64 	%rd34, %rd5, 2;
	add.s64 	%rd7, %rd33, %rd34;
	mul.wide.s32 	%rd35, %r3, 8;
	mul.wide.s32 	%rd36, %r16, 8;
	add.s64 	%rd37, %rd35, %rd36;
	add.s64 	%rd8, %rd37, %rd34;
	add.s64 	%rd9, %rd35, %rd34;
	mul.wide.s32 	%rd38, %r3, 2;
	add.s64 	%rd39, %rd38, %rd4;
	shl.b64 	%rd40, %rd39, 2;
	add.s64 	%rd72, %rd2, %rd40;
	mul.wide.s32 	%rd11, %r34, 8;
	mov.f32 	%f327, 0f00000000;
	mov.u64 	%rd73, %rd75;
	mov.f32 	%f328, %f327;
	mov.f32 	%f329, %f327;
	mov.f32 	%f330, %f327;
	mov.f32 	%f331, %f327;
	mov.u32 	%r223, %r3;

$L__BB44_5:
	.pragma "nounroll";
	ld.global.nc.v2.f32 	{%f46, %f47}, [%rd72];
	add.s64 	%rd41, %rd73, %rd9;
	ld.global.nc.v2.f32 	{%f50, %f51}, [%rd41];
	fma.rn.f32 	%f54, %f46, %f50, %f331;
	fma.rn.f32 	%f331, %f47, %f51, %f54;
	add.s64 	%rd42, %rd73, %rd8;
	ld.global.nc.v2.f32 	{%f55, %f56}, [%rd42];
	fma.rn.f32 	%f59, %f46, %f55, %f330;
	fma.rn.f32 	%f330, %f47, %f56, %f59;
	add.s64 	%rd43, %rd41, %rd11;
	ld.global.nc.v2.f32 	{%f60, %f61}, [%rd43];
	fma.rn.f32 	%f64, %f46, %f60, %f329;
	fma.rn.f32 	%f329, %f47, %f61, %f64;
	add.s64 	%rd44, %rd73, %rd7;
	ld.global.nc.v2.f32 	{%f65, %f66}, [%rd44];
	fma.rn.f32 	%f69, %f46, %f65, %f328;
	fma.rn.f32 	%f328, %f47, %f66, %f69;
	add.s64 	%rd45, %rd43, %rd11;
	ld.global.nc.v2.f32 	{%f70, %f71}, [%rd45];
	fma.rn.f32 	%f74, %f46, %f70, %f327;
	fma.rn.f32 	%f327, %f47, %f71, %f74;
	add.s32 	%r223, %r223, 192;
	add.s64 	%rd73, %rd73, 1536;
	add.s64 	%rd72, %rd72, 1536;
	add.s32 	%r222, %r222, -1;
	setp.ne.s32 	%p4, %r222, 0;
	@%p4 bra 	$L__BB44_5;

	st.local.f32 	[%rd3], %f331;
	st.local.f32 	[%rd3+4], %f330;
	st.local.f32 	[%rd3+8], %f329;
	st.local.f32 	[%rd3+12], %f328;
	st.local.f32 	[%rd3+16], %f327;

$L__BB44_7:
	setp.lt.u32 	%p5, %r5, 576;
	@%p5 bra 	$L__BB44_11;

	add.s32 	%r36, %r223, %r16;
	shl.b32 	%r37, %r16, 1;
	add.s32 	%r38, %r223, %r37;
	mad.lo.s32 	%r39, %r16, 3, %r223;
	shl.b32 	%r40, %r16, 2;
	add.s32 	%r41, %r223, %r40;
	add.s32 	%r42, %r36, 192;
	mul.wide.s32 	%rd46, %r42, 8;
	shl.b64 	%rd47, %rd5, 2;
	add.s64 	%rd16, %rd46, %rd47;
	mul.wide.s32 	%rd48, %r38, 8;
	add.s64 	%rd17, %rd48, %rd47;
	mul.wide.s32 	%rd49, %r39, 8;
	add.s64 	%rd18, %rd49, %rd47;
	mul.wide.s32 	%rd50, %r41, 8;
	add.s64 	%rd19, %rd50, %rd47;
	mul.wide.s32 	%rd51, %r223, 2;
	add.s64 	%rd52, %rd51, %rd4;
	shl.b64 	%rd53, %rd52, 2;
	add.s64 	%rd54, %rd2, %rd53;
	add.s64 	%rd74, %rd54, 3072;
	mul.wide.s32 	%rd55, %r223, 8;
	add.s64 	%rd21, %rd55, %rd47;
	mul.wide.s32 	%rd56, %r16, 8;
	add.s64 	%rd57, %rd55, %rd56;
	add.s64 	%rd22, %rd57, %rd47;

$L__BB44_9:
	ld.global.nc.v2.f32 	{%f75, %f76}, [%rd74+-3072];
	add.s64 	%rd58, %rd75, %rd21;
	ld.global.nc.v2.f32 	{%f79, %f80}, [%rd58];
	fma.rn.f32 	%f83, %f75, %f79, %f331;
	fma.rn.f32 	%f84, %f76, %f80, %f83;
	add.s64 	%rd59, %rd75, %rd22;
	ld.global.nc.v2.f32 	{%f85, %f86}, [%rd59];
	fma.rn.f32 	%f89, %f75, %f85, %f330;
	fma.rn.f32 	%f90, %f76, %f86, %f89;
	add.s64 	%rd60, %rd75, %rd17;
	ld.global.nc.v2.f32 	{%f91, %f92}, [%rd60];
	fma.rn.f32 	%f95, %f75, %f91, %f329;
	fma.rn.f32 	%f96, %f76, %f92, %f95;
	add.s64 	%rd61, %rd75, %rd18;
	ld.global.nc.v2.f32 	{%f97, %f98}, [%rd61];
	fma.rn.f32 	%f101, %f75, %f97, %f328;
	fma.rn.f32 	%f102, %f76, %f98, %f101;
	add.s64 	%rd62, %rd75, %rd19;
	ld.global.nc.v2.f32 	{%f103, %f104}, [%rd62];
	fma.rn.f32 	%f107, %f75, %f103, %f327;
	fma.rn.f32 	%f108, %f76, %f104, %f107;
	ld.global.nc.v2.f32 	{%f109, %f110}, [%rd74+-1536];
	ld.global.nc.v2.f32 	{%f113, %f114}, [%rd58+1536];
	fma.rn.f32 	%f117, %f109, %f113, %f84;
	fma.rn.f32 	%f118, %f110, %f114, %f117;
	add.s64 	%rd63, %rd75, %rd16;
	ld.global.nc.v2.f32 	{%f119, %f120}, [%rd63];
	fma.rn.f32 	%f123, %f109, %f119, %f90;
	fma.rn.f32 	%f124, %f110, %f120, %f123;
	ld.global.nc.v2.f32 	{%f125, %f126}, [%rd60+1536];
	fma.rn.f32 	%f129, %f109, %f125, %f96;
	fma.rn.f32 	%f130, %f110, %f126, %f129;
	ld.global.nc.v2.f32 	{%f131, %f132}, [%rd61+1536];
	fma.rn.f32 	%f135, %f109, %f131, %f102;
	fma.rn.f32 	%f136, %f110, %f132, %f135;
	ld.global.nc.v2.f32 	{%f137, %f138}, [%rd62+1536];
	fma.rn.f32 	%f141, %f109, %f137, %f108;
	fma.rn.f32 	%f142, %f110, %f138, %f141;
	ld.global.nc.v2.f32 	{%f143, %f144}, [%rd74];
	ld.global.nc.v2.f32 	{%f147, %f148}, [%rd58+3072];
	fma.rn.f32 	%f151, %f143, %f147, %f118;
	fma.rn.f32 	%f152, %f144, %f148, %f151;
	ld.global.nc.v2.f32 	{%f153, %f154}, [%rd63+1536];
	fma.rn.f32 	%f157, %f143, %f153, %f124;
	fma.rn.f32 	%f158, %f144, %f154, %f157;
	ld.global.nc.v2.f32 	{%f159, %f160}, [%rd60+3072];
	fma.rn.f32 	%f163, %f143, %f159, %f130;
	fma.rn.f32 	%f164, %f144, %f160, %f163;
	ld.global.nc.v2.f32 	{%f165, %f166}, [%rd61+3072];
	fma.rn.f32 	%f169, %f143, %f165, %f136;
	fma.rn.f32 	%f170, %f144, %f166, %f169;
	ld.global.nc.v2.f32 	{%f171, %f172}, [%rd62+3072];
	fma.rn.f32 	%f175, %f143, %f171, %f142;
	fma.rn.f32 	%f176, %f144, %f172, %f175;
	ld.global.nc.v2.f32 	{%f177, %f178}, [%rd74+1536];
	ld.global.nc.v2.f32 	{%f181, %f182}, [%rd58+4608];
	fma.rn.f32 	%f185, %f177, %f181, %f152;
	fma.rn.f32 	%f331, %f178, %f182, %f185;
	ld.global.nc.v2.f32 	{%f186, %f187}, [%rd63+3072];
	fma.rn.f32 	%f190, %f177, %f186, %f158;
	fma.rn.f32 	%f330, %f178, %f187, %f190;
	ld.global.nc.v2.f32 	{%f191, %f192}, [%rd60+4608];
	fma.rn.f32 	%f195, %f177, %f191, %f164;
	fma.rn.f32 	%f329, %f178, %f192, %f195;
	ld.global.nc.v2.f32 	{%f196, %f197}, [%rd61+4608];
	fma.rn.f32 	%f200, %f177, %f196, %f170;
	fma.rn.f32 	%f328, %f178, %f197, %f200;
	ld.global.nc.v2.f32 	{%f201, %f202}, [%rd62+4608];
	fma.rn.f32 	%f205, %f177, %f201, %f176;
	fma.rn.f32 	%f327, %f178, %f202, %f205;
	add.s64 	%rd75, %rd75, 6144;
	add.s64 	%rd74, %rd74, 6144;
	add.s32 	%r223, %r223, 768;
	setp.lt.s32 	%p6, %r223, %r15;
	@%p6 bra 	$L__BB44_9;

	st.local.f32 	[%rd3], %f331;
	st.local.f32 	[%rd3+4], %f330;
	st.local.f32 	[%rd3+8], %f329;
	st.local.f32 	[%rd3+12], %f328;
	st.local.f32 	[%rd3+16], %f327;

$L__BB44_11:
	shr.s32 	%r43, %r3, 31;
	shr.u32 	%r44, %r43, 27;
	add.s32 	%r45, %r3, %r44;
	shr.s32 	%r46, %r45, 5;
	shl.b32 	%r47, %r46, 2;
	add.s32 	%r14, %r28, %r47;
	mov.u32 	%r49, 2;
	mov.b32 	%r50, %f331;
	mov.u32 	%r51, 31;
	mov.u32 	%r52, 16;
	mov.u32 	%r53, -1;
	shfl.sync.bfly.b32 	%r54|%p7, %r50, %r52, %r51, %r53;
	mov.b32 	%f206, %r54;
	add.f32 	%f207, %f331, %f206;
	mov.b32 	%r55, %f207;
	mov.u32 	%r56, 8;
	shfl.sync.bfly.b32 	%r57|%p8, %r55, %r56, %r51, %r53;
	mov.b32 	%f208, %r57;
	add.f32 	%f209, %f207, %f208;
	mov.b32 	%r58, %f209;
	mov.u32 	%r59, 4;
	shfl.sync.bfly.b32 	%r60|%p9, %r58, %r59, %r51, %r53;
	mov.b32 	%f210, %r60;
	add.f32 	%f211, %f209, %f210;
	mov.b32 	%r61, %f211;
	shfl.sync.bfly.b32 	%r62|%p10, %r61, %r49, %r51, %r53;
	mov.b32 	%f212, %r62;
	add.f32 	%f213, %f211, %f212;
	mov.b32 	%r63, %f213;
	mov.u32 	%r64, 1;
	shfl.sync.bfly.b32 	%r65|%p11, %r63, %r64, %r51, %r53;
	mov.b32 	%f214, %r65;
	add.f32 	%f215, %f213, %f214;
	st.local.f32 	[%rd3], %f215;
	st.shared.f32 	[%r14], %f215;
	bar.sync 	0;
	@%p1 bra 	$L__BB44_13;

	ld.shared.f32 	%f216, [%r4];
	mov.b32 	%r66, %f216;
	shfl.sync.bfly.b32 	%r70|%p13, %r66, %r52, %r51, %r53;
	mov.b32 	%f217, %r70;
	add.f32 	%f218, %f216, %f217;
	mov.b32 	%r71, %f218;
	shfl.sync.bfly.b32 	%r73|%p14, %r71, %r56, %r51, %r53;
	mov.b32 	%f219, %r73;
	add.f32 	%f220, %f218, %f219;
	mov.b32 	%r74, %f220;
	shfl.sync.bfly.b32 	%r76|%p15, %r74, %r59, %r51, %r53;
	mov.b32 	%f221, %r76;
	add.f32 	%f222, %f220, %f221;
	mov.b32 	%r77, %f222;
	shfl.sync.bfly.b32 	%r79|%p16, %r77, %r49, %r51, %r53;
	mov.b32 	%f223, %r79;
	add.f32 	%f224, %f222, %f223;
	mov.b32 	%r80, %f224;
	shfl.sync.bfly.b32 	%r82|%p17, %r80, %r64, %r51, %r53;
	mov.b32 	%f225, %r82;
	add.f32 	%f226, %f224, %f225;
	st.local.f32 	[%rd3], %f226;

$L__BB44_13:
	bar.sync 	0;
	mov.b32 	%r83, %f330;
	shfl.sync.bfly.b32 	%r87|%p19, %r83, %r52, %r51, %r53;
	mov.b32 	%f227, %r87;
	add.f32 	%f228, %f330, %f227;
	mov.b32 	%r88, %f228;
	shfl.sync.bfly.b32 	%r90|%p20, %r88, %r56, %r51, %r53;
	mov.b32 	%f229, %r90;
	add.f32 	%f230, %f228, %f229;
	mov.b32 	%r91, %f230;
	shfl.sync.bfly.b32 	%r93|%p21, %r91, %r59, %r51, %r53;
	mov.b32 	%f231, %r93;
	add.f32 	%f232, %f230, %f231;
	mov.b32 	%r94, %f232;
	shfl.sync.bfly.b32 	%r96|%p22, %r94, %r49, %r51, %r53;
	mov.b32 	%f233, %r96;
	add.f32 	%f234, %f232, %f233;
	mov.b32 	%r97, %f234;
	shfl.sync.bfly.b32 	%r99|%p23, %r97, %r64, %r51, %r53;
	mov.b32 	%f235, %r99;
	add.f32 	%f236, %f234, %f235;
	st.local.f32 	[%rd3+4], %f236;
	st.shared.f32 	[%r14], %f236;
	bar.sync 	0;
	@%p1 bra 	$L__BB44_15;

	ld.shared.f32 	%f237, [%r4];
	mov.b32 	%r100, %f237;
	mov.u32 	%r101, 31;
	mov.u32 	%r102, 16;
	mov.u32 	%r103, -1;
	shfl.sync.bfly.b32 	%r104|%p24, %r100, %r102, %r101, %r103;
	mov.b32 	%f238, %r104;
	add.f32 	%f239, %f237, %f238;
	mov.b32 	%r105, %f239;
	mov.u32 	%r106, 8;
	shfl.sync.bfly.b32 	%r107|%p25, %r105, %r106, %r101, %r103;
	mov.b32 	%f240, %r107;
	add.f32 	%f241, %f239, %f240;
	mov.b32 	%r108, %f241;
	mov.u32 	%r109, 4;
	shfl.sync.bfly.b32 	%r110|%p26, %r108, %r109, %r101, %r103;
	mov.b32 	%f242, %r110;
	add.f32 	%f243, %f241, %f242;
	mov.b32 	%r111, %f243;
	mov.u32 	%r112, 2;
	shfl.sync.bfly.b32 	%r113|%p27, %r111, %r112, %r101, %r103;
	mov.b32 	%f244, %r113;
	add.f32 	%f245, %f243, %f244;
	mov.b32 	%r114, %f245;
	mov.u32 	%r115, 1;
	shfl.sync.bfly.b32 	%r116|%p28, %r114, %r115, %r101, %r103;
	mov.b32 	%f246, %r116;
	add.f32 	%f247, %f245, %f246;
	st.local.f32 	[%rd3+4], %f247;

$L__BB44_15:
	bar.sync 	0;
	mov.b32 	%r117, %f329;
	mov.u32 	%r118, 31;
	mov.u32 	%r119, 16;
	mov.u32 	%r120, -1;
	shfl.sync.bfly.b32 	%r121|%p30, %r117, %r119, %r118, %r120;
	mov.b32 	%f248, %r121;
	add.f32 	%f249, %f329, %f248;
	mov.b32 	%r122, %f249;
	mov.u32 	%r123, 8;
	shfl.sync.bfly.b32 	%r124|%p31, %r122, %r123, %r118, %r120;
	mov.b32 	%f250, %r124;
	add.f32 	%f251, %f249, %f250;
	mov.b32 	%r125, %f251;
	mov.u32 	%r126, 4;
	shfl.sync.bfly.b32 	%r127|%p32, %r125, %r126, %r118, %r120;
	mov.b32 	%f252, %r127;
	add.f32 	%f253, %f251, %f252;
	mov.b32 	%r128, %f253;
	mov.u32 	%r129, 2;
	shfl.sync.bfly.b32 	%r130|%p33, %r128, %r129, %r118, %r120;
	mov.b32 	%f254, %r130;
	add.f32 	%f255, %f253, %f254;
	mov.b32 	%r131, %f255;
	mov.u32 	%r132, 1;
	shfl.sync.bfly.b32 	%r133|%p34, %r131, %r132, %r118, %r120;
	mov.b32 	%f256, %r133;
	add.f32 	%f257, %f255, %f256;
	st.local.f32 	[%rd3+8], %f257;
	st.shared.f32 	[%r14], %f257;
	bar.sync 	0;
	@%p1 bra 	$L__BB44_17;

	ld.shared.f32 	%f258, [%r4];
	mov.b32 	%r134, %f258;
	shfl.sync.bfly.b32 	%r138|%p35, %r134, %r119, %r118, %r120;
	mov.b32 	%f259, %r138;
	add.f32 	%f260, %f258, %f259;
	mov.b32 	%r139, %f260;
	shfl.sync.bfly.b32 	%r141|%p36, %r139, %r123, %r118, %r120;
	mov.b32 	%f261, %r141;
	add.f32 	%f262, %f260, %f261;
	mov.b32 	%r142, %f262;
	shfl.sync.bfly.b32 	%r144|%p37, %r142, %r126, %r118, %r120;
	mov.b32 	%f263, %r144;
	add.f32 	%f264, %f262, %f263;
	mov.b32 	%r145, %f264;
	shfl.sync.bfly.b32 	%r147|%p38, %r145, %r129, %r118, %r120;
	mov.b32 	%f265, %r147;
	add.f32 	%f266, %f264, %f265;
	mov.b32 	%r148, %f266;
	shfl.sync.bfly.b32 	%r150|%p39, %r148, %r132, %r118, %r120;
	mov.b32 	%f267, %r150;
	add.f32 	%f268, %f266, %f267;
	st.local.f32 	[%rd3+8], %f268;

$L__BB44_17:
	bar.sync 	0;
	mov.b32 	%r151, %f328;
	shfl.sync.bfly.b32 	%r155|%p41, %r151, %r119, %r118, %r120;
	mov.b32 	%f269, %r155;
	add.f32 	%f270, %f328, %f269;
	mov.b32 	%r156, %f270;
	shfl.sync.bfly.b32 	%r158|%p42, %r156, %r123, %r118, %r120;
	mov.b32 	%f271, %r158;
	add.f32 	%f272, %f270, %f271;
	mov.b32 	%r159, %f272;
	shfl.sync.bfly.b32 	%r161|%p43, %r159, %r126, %r118, %r120;
	mov.b32 	%f273, %r161;
	add.f32 	%f274, %f272, %f273;
	mov.b32 	%r162, %f274;
	shfl.sync.bfly.b32 	%r164|%p44, %r162, %r129, %r118, %r120;
	mov.b32 	%f275, %r164;
	add.f32 	%f276, %f274, %f275;
	mov.b32 	%r165, %f276;
	shfl.sync.bfly.b32 	%r167|%p45, %r165, %r132, %r118, %r120;
	mov.b32 	%f277, %r167;
	add.f32 	%f278, %f276, %f277;
	st.local.f32 	[%rd3+12], %f278;
	st.shared.f32 	[%r14], %f278;
	bar.sync 	0;
	@%p1 bra 	$L__BB44_19;

	ld.shared.f32 	%f279, [%r4];
	mov.b32 	%r168, %f279;
	mov.u32 	%r169, 31;
	mov.u32 	%r170, 16;
	mov.u32 	%r171, -1;
	shfl.sync.bfly.b32 	%r172|%p46, %r168, %r170, %r169, %r171;
	mov.b32 	%f280, %r172;
	add.f32 	%f281, %f279, %f280;
	mov.b32 	%r173, %f281;
	mov.u32 	%r174, 8;
	shfl.sync.bfly.b32 	%r175|%p47, %r173, %r174, %r169, %r171;
	mov.b32 	%f282, %r175;
	add.f32 	%f283, %f281, %f282;
	mov.b32 	%r176, %f283;
	mov.u32 	%r177, 4;
	shfl.sync.bfly.b32 	%r178|%p48, %r176, %r177, %r169, %r171;
	mov.b32 	%f284, %r178;
	add.f32 	%f285, %f283, %f284;
	mov.b32 	%r179, %f285;
	mov.u32 	%r180, 2;
	shfl.sync.bfly.b32 	%r181|%p49, %r179, %r180, %r169, %r171;
	mov.b32 	%f286, %r181;
	add.f32 	%f287, %f285, %f286;
	mov.b32 	%r182, %f287;
	mov.u32 	%r183, 1;
	shfl.sync.bfly.b32 	%r184|%p50, %r182, %r183, %r169, %r171;
	mov.b32 	%f288, %r184;
	add.f32 	%f289, %f287, %f288;
	st.local.f32 	[%rd3+12], %f289;

$L__BB44_19:
	bar.sync 	0;
	mov.b32 	%r185, %f327;
	mov.u32 	%r186, 31;
	mov.u32 	%r187, 16;
	mov.u32 	%r188, -1;
	shfl.sync.bfly.b32 	%r189|%p52, %r185, %r187, %r186, %r188;
	mov.b32 	%f290, %r189;
	add.f32 	%f291, %f327, %f290;
	mov.b32 	%r190, %f291;
	mov.u32 	%r191, 8;
	shfl.sync.bfly.b32 	%r192|%p53, %r190, %r191, %r186, %r188;
	mov.b32 	%f292, %r192;
	add.f32 	%f293, %f291, %f292;
	mov.b32 	%r193, %f293;
	mov.u32 	%r194, 4;
	shfl.sync.bfly.b32 	%r195|%p54, %r193, %r194, %r186, %r188;
	mov.b32 	%f294, %r195;
	add.f32 	%f295, %f293, %f294;
	mov.b32 	%r196, %f295;
	mov.u32 	%r197, 2;
	shfl.sync.bfly.b32 	%r198|%p55, %r196, %r197, %r186, %r188;
	mov.b32 	%f296, %r198;
	add.f32 	%f297, %f295, %f296;
	mov.b32 	%r199, %f297;
	mov.u32 	%r200, 1;
	shfl.sync.bfly.b32 	%r201|%p56, %r199, %r200, %r186, %r188;
	mov.b32 	%f298, %r201;
	add.f32 	%f299, %f297, %f298;
	st.local.f32 	[%rd3+16], %f299;
	st.shared.f32 	[%r14], %f299;
	bar.sync 	0;
	@%p1 bra 	$L__BB44_21;

	ld.shared.f32 	%f300, [%r4];
	mov.b32 	%r202, %f300;
	shfl.sync.bfly.b32 	%r206|%p57, %r202, %r187, %r186, %r188;
	mov.b32 	%f301, %r206;
	add.f32 	%f302, %f300, %f301;
	mov.b32 	%r207, %f302;
	shfl.sync.bfly.b32 	%r209|%p58, %r207, %r191, %r186, %r188;
	mov.b32 	%f303, %r209;
	add.f32 	%f304, %f302, %f303;
	mov.b32 	%r210, %f304;
	shfl.sync.bfly.b32 	%r212|%p59, %r210, %r194, %r186, %r188;
	mov.b32 	%f305, %r212;
	add.f32 	%f306, %f304, %f305;
	mov.b32 	%r213, %f306;
	shfl.sync.bfly.b32 	%r215|%p60, %r213, %r197, %r186, %r188;
	mov.b32 	%f307, %r215;
	add.f32 	%f308, %f306, %f307;
	mov.b32 	%r216, %f308;
	shfl.sync.bfly.b32 	%r218|%p61, %r216, %r200, %r186, %r188;
	mov.b32 	%f309, %r218;
	add.f32 	%f310, %f308, %f309;
	st.local.f32 	[%rd3+16], %f310;

$L__BB44_21:
	bar.sync 	0;
	setp.gt.s32 	%p62, %r3, 4;
	@%p62 bra 	$L__BB44_23;

	mul.wide.s32 	%rd64, %r3, 4;
	add.s64 	%rd65, %rd3, %rd64;
	ld.local.f32 	%f311, [%rd65];
	mad.lo.s32 	%r219, %r3, %r17, %r2;
	cvt.s64.s32 	%rd66, %r219;
	mul.lo.s32 	%r220, %r1, %r18;
	cvt.s64.s32 	%rd67, %r220;
	add.s64 	%rd68, %rd67, %rd66;
	cvta.to.global.u64 	%rd69, %rd27;
	shl.b64 	%rd70, %rd68, 2;
	add.s64 	%rd71, %rd69, %rd70;
	st.global.f32 	[%rd71], %f311;

$L__BB44_23:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_6_bs_192
.visible .entry ggml_matvec_f32_ncols_6_bs_192(
	.param .u64 ggml_matvec_f32_ncols_6_bs_192_param_0,
	.param .u64 ggml_matvec_f32_ncols_6_bs_192_param_1,
	.param .u64 ggml_matvec_f32_ncols_6_bs_192_param_2,
	.param .u32 ggml_matvec_f32_ncols_6_bs_192_param_3,
	.param .u32 ggml_matvec_f32_ncols_6_bs_192_param_4,
	.param .u32 ggml_matvec_f32_ncols_6_bs_192_param_5,
	.param .u32 ggml_matvec_f32_ncols_6_bs_192_param_6,
	.param .u32 ggml_matvec_f32_ncols_6_bs_192_param_7,
	.param .u32 ggml_matvec_f32_ncols_6_bs_192_param_8,
	.param .u32 ggml_matvec_f32_ncols_6_bs_192_param_9,
	.param .u32 ggml_matvec_f32_ncols_6_bs_192_param_10,
	.param .u32 ggml_matvec_f32_ncols_6_bs_192_param_11
)
{
	.local .align 8 .b8 	__local_depot45[24];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<75>;
	.reg .f32 	%f<296>;
	.reg .b32 	%r<251>;
	.reg .b64 	%rd<70>;


	mov.u64 	%SPL, __local_depot45;
	ld.param.u64 	%rd20, [ggml_matvec_f32_ncols_6_bs_192_param_0];
	ld.param.u64 	%rd21, [ggml_matvec_f32_ncols_6_bs_192_param_1];
	ld.param.u64 	%rd19, [ggml_matvec_f32_ncols_6_bs_192_param_2];
	ld.param.u32 	%r11, [ggml_matvec_f32_ncols_6_bs_192_param_3];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_6_bs_192_param_5];
	ld.param.u32 	%r12, [ggml_matvec_f32_ncols_6_bs_192_param_6];
	ld.param.u32 	%r13, [ggml_matvec_f32_ncols_6_bs_192_param_7];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_6_bs_192_param_8];
	ld.param.u32 	%r17, [ggml_matvec_f32_ncols_6_bs_192_param_9];
	ld.param.u32 	%r18, [ggml_matvec_f32_ncols_6_bs_192_param_10];
	ld.param.u32 	%r14, [ggml_matvec_f32_ncols_6_bs_192_param_11];
	cvta.to.global.u64 	%rd69, %rd21;
	add.u64 	%rd2, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r19, %r1, %r16;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r20, %r2, %r15;
	mad.lo.s32 	%r21, %r19, %r17, %r20;
	cvt.s64.s32 	%rd3, %r21;
	cvta.to.global.u64 	%rd4, %rd20;
	mul.lo.s32 	%r22, %r1, %r18;
	cvt.s64.s32 	%rd5, %r22;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r23, %r3, 2;
	mov.u32 	%r24, data_mmv;
	add.s32 	%r4, %r24, %r23;
	@%p1 bra 	$L__BB45_2;

	mov.u32 	%r25, 0;
	st.shared.u32 	[%r4], %r25;

$L__BB45_2:
	bar.sync 	0;
	mov.f32 	%f290, 0f00000000;
	st.local.v2.f32 	[%rd2], {%f290, %f290};
	st.local.v2.f32 	[%rd2+8], {%f290, %f290};
	st.local.v2.f32 	[%rd2+16], {%f290, %f290};
	setp.ge.s32 	%p2, %r3, %r11;
	mov.f32 	%f291, %f290;
	mov.f32 	%f292, %f290;
	mov.f32 	%f293, %f290;
	mov.f32 	%f294, %f290;
	mov.f32 	%f295, %f290;
	@%p2 bra 	$L__BB45_9;

	not.b32 	%r26, %r3;
	add.s32 	%r5, %r26, %r11;
	mul.wide.u32 	%rd23, %r5, -1431655765;
	shr.u64 	%rd24, %rd23, 39;
	and.b64  	%rd25, %rd24, 1;
	setp.eq.b64 	%p3, %rd25, 1;
	mov.pred 	%p4, 0;
	xor.pred  	%p5, %p3, %p4;
	mov.f32 	%f290, 0f00000000;
	mov.u32 	%r250, %r3;
	@%p5 bra 	$L__BB45_5;

	shl.b64 	%rd26, %rd5, 2;
	add.s64 	%rd27, %rd69, %rd26;
	shl.b64 	%rd28, %rd3, 2;
	add.s64 	%rd29, %rd4, %rd28;
	mul.wide.s32 	%rd30, %r3, 8;
	add.s64 	%rd31, %rd29, %rd30;
	ld.global.nc.v2.f32 	{%f43, %f44}, [%rd31];
	add.s64 	%rd32, %rd27, %rd30;
	ld.global.nc.v2.f32 	{%f47, %f48}, [%rd32];
	fma.rn.f32 	%f51, %f43, %f47, 0f00000000;
	fma.rn.f32 	%f295, %f44, %f48, %f51;
	mul.wide.s32 	%rd33, %r12, 8;
	add.s64 	%rd34, %rd32, %rd33;
	ld.global.nc.v2.f32 	{%f52, %f53}, [%rd34];
	fma.rn.f32 	%f56, %f43, %f52, 0f00000000;
	fma.rn.f32 	%f294, %f44, %f53, %f56;
	st.local.v2.f32 	[%rd2], {%f295, %f294};
	add.s32 	%r27, %r3, %r12;
	add.s32 	%r28, %r27, %r12;
	mul.wide.s32 	%rd35, %r28, 8;
	add.s64 	%rd36, %rd27, %rd35;
	ld.global.nc.v2.f32 	{%f57, %f58}, [%rd36];
	fma.rn.f32 	%f61, %f43, %f57, 0f00000000;
	fma.rn.f32 	%f293, %f44, %f58, %f61;
	add.s64 	%rd37, %rd36, %rd33;
	ld.global.nc.v2.f32 	{%f62, %f63}, [%rd37];
	fma.rn.f32 	%f66, %f43, %f62, 0f00000000;
	fma.rn.f32 	%f292, %f44, %f63, %f66;
	st.local.v2.f32 	[%rd2+8], {%f293, %f292};
	add.s64 	%rd38, %rd37, %rd33;
	ld.global.nc.v2.f32 	{%f67, %f68}, [%rd38];
	fma.rn.f32 	%f71, %f43, %f67, 0f00000000;
	fma.rn.f32 	%f291, %f44, %f68, %f71;
	add.s64 	%rd39, %rd38, %rd33;
	ld.global.nc.v2.f32 	{%f72, %f73}, [%rd39];
	fma.rn.f32 	%f76, %f43, %f72, 0f00000000;
	fma.rn.f32 	%f290, %f44, %f73, %f76;
	st.local.v2.f32 	[%rd2+16], {%f291, %f290};
	add.s32 	%r250, %r3, 192;

$L__BB45_5:
	setp.lt.u32 	%p6, %r5, 192;
	@%p6 bra 	$L__BB45_9;

	add.s32 	%r29, %r250, %r12;
	add.s32 	%r30, %r29, 192;
	mul.wide.s32 	%rd40, %r30, 8;
	shl.b64 	%rd41, %rd5, 2;
	add.s64 	%rd7, %rd40, %rd41;
	shl.b32 	%r31, %r12, 1;
	add.s32 	%r32, %r250, %r31;
	mad.lo.s32 	%r33, %r12, 3, %r250;
	shl.b32 	%r34, %r12, 2;
	add.s32 	%r35, %r250, %r34;
	mad.lo.s32 	%r36, %r12, 5, %r250;
	mul.wide.s32 	%rd42, %r32, 8;
	add.s64 	%rd8, %rd42, %rd41;
	mul.wide.s32 	%rd43, %r33, 8;
	add.s64 	%rd9, %rd43, %rd41;
	mul.wide.s32 	%rd44, %r35, 8;
	add.s64 	%rd10, %rd44, %rd41;
	mul.wide.s32 	%rd45, %r36, 8;
	add.s64 	%rd11, %rd45, %rd41;
	mul.wide.s32 	%rd46, %r250, 2;
	add.s64 	%rd47, %rd46, %rd3;
	shl.b64 	%rd48, %rd47, 2;
	add.s64 	%rd49, %rd4, %rd48;
	add.s64 	%rd68, %rd49, 1536;
	mul.wide.s32 	%rd50, %r250, 8;
	mul.wide.s32 	%rd51, %r12, 8;
	add.s64 	%rd52, %rd50, %rd51;
	add.s64 	%rd13, %rd52, %rd41;
	add.s64 	%rd14, %rd50, %rd41;

$L__BB45_7:
	ld.global.nc.v2.f32 	{%f77, %f78}, [%rd68+-1536];
	add.s64 	%rd53, %rd69, %rd14;
	ld.global.nc.v2.f32 	{%f81, %f82}, [%rd53];
	fma.rn.f32 	%f85, %f77, %f81, %f295;
	fma.rn.f32 	%f86, %f78, %f82, %f85;
	add.s64 	%rd54, %rd69, %rd13;
	ld.global.nc.v2.f32 	{%f87, %f88}, [%rd54];
	fma.rn.f32 	%f91, %f77, %f87, %f294;
	fma.rn.f32 	%f92, %f78, %f88, %f91;
	add.s64 	%rd55, %rd69, %rd8;
	ld.global.nc.v2.f32 	{%f93, %f94}, [%rd55];
	fma.rn.f32 	%f97, %f77, %f93, %f293;
	fma.rn.f32 	%f98, %f78, %f94, %f97;
	add.s64 	%rd56, %rd69, %rd9;
	ld.global.nc.v2.f32 	{%f99, %f100}, [%rd56];
	fma.rn.f32 	%f103, %f77, %f99, %f292;
	fma.rn.f32 	%f104, %f78, %f100, %f103;
	add.s64 	%rd57, %rd69, %rd10;
	ld.global.nc.v2.f32 	{%f105, %f106}, [%rd57];
	fma.rn.f32 	%f109, %f77, %f105, %f291;
	fma.rn.f32 	%f110, %f78, %f106, %f109;
	add.s64 	%rd58, %rd69, %rd11;
	ld.global.nc.v2.f32 	{%f111, %f112}, [%rd58];
	fma.rn.f32 	%f115, %f77, %f111, %f290;
	fma.rn.f32 	%f116, %f78, %f112, %f115;
	ld.global.nc.v2.f32 	{%f117, %f118}, [%rd68];
	ld.global.nc.v2.f32 	{%f121, %f122}, [%rd53+1536];
	fma.rn.f32 	%f125, %f117, %f121, %f86;
	fma.rn.f32 	%f295, %f118, %f122, %f125;
	add.s64 	%rd59, %rd69, %rd7;
	ld.global.nc.v2.f32 	{%f126, %f127}, [%rd59];
	fma.rn.f32 	%f130, %f117, %f126, %f92;
	fma.rn.f32 	%f294, %f118, %f127, %f130;
	ld.global.nc.v2.f32 	{%f131, %f132}, [%rd55+1536];
	fma.rn.f32 	%f135, %f117, %f131, %f98;
	fma.rn.f32 	%f293, %f118, %f132, %f135;
	ld.global.nc.v2.f32 	{%f136, %f137}, [%rd56+1536];
	fma.rn.f32 	%f140, %f117, %f136, %f104;
	fma.rn.f32 	%f292, %f118, %f137, %f140;
	ld.global.nc.v2.f32 	{%f141, %f142}, [%rd57+1536];
	fma.rn.f32 	%f145, %f117, %f141, %f110;
	fma.rn.f32 	%f291, %f118, %f142, %f145;
	ld.global.nc.v2.f32 	{%f146, %f147}, [%rd58+1536];
	fma.rn.f32 	%f150, %f117, %f146, %f116;
	fma.rn.f32 	%f290, %f118, %f147, %f150;
	add.s64 	%rd69, %rd69, 3072;
	add.s64 	%rd68, %rd68, 3072;
	add.s32 	%r250, %r250, 384;
	setp.lt.s32 	%p7, %r250, %r11;
	@%p7 bra 	$L__BB45_7;

	st.local.v2.f32 	[%rd2], {%f295, %f294};
	st.local.v2.f32 	[%rd2+8], {%f293, %f292};
	st.local.v2.f32 	[%rd2+16], {%f291, %f290};

$L__BB45_9:
	shr.s32 	%r37, %r3, 31;
	shr.u32 	%r38, %r37, 27;
	add.s32 	%r39, %r3, %r38;
	shr.s32 	%r40, %r39, 5;
	shl.b32 	%r41, %r40, 2;
	add.s32 	%r10, %r24, %r41;
	mov.u32 	%r43, 2;
	mov.b32 	%r44, %f295;
	mov.u32 	%r45, 31;
	mov.u32 	%r46, 16;
	mov.u32 	%r47, -1;
	shfl.sync.bfly.b32 	%r48|%p8, %r44, %r46, %r45, %r47;
	mov.b32 	%f151, %r48;
	add.f32 	%f152, %f295, %f151;
	mov.b32 	%r49, %f152;
	mov.u32 	%r50, 8;
	shfl.sync.bfly.b32 	%r51|%p9, %r49, %r50, %r45, %r47;
	mov.b32 	%f153, %r51;
	add.f32 	%f154, %f152, %f153;
	mov.b32 	%r52, %f154;
	mov.u32 	%r53, 4;
	shfl.sync.bfly.b32 	%r54|%p10, %r52, %r53, %r45, %r47;
	mov.b32 	%f155, %r54;
	add.f32 	%f156, %f154, %f155;
	mov.b32 	%r55, %f156;
	shfl.sync.bfly.b32 	%r56|%p11, %r55, %r43, %r45, %r47;
	mov.b32 	%f157, %r56;
	add.f32 	%f158, %f156, %f157;
	mov.b32 	%r57, %f158;
	mov.u32 	%r58, 1;
	shfl.sync.bfly.b32 	%r59|%p12, %r57, %r58, %r45, %r47;
	mov.b32 	%f159, %r59;
	add.f32 	%f160, %f158, %f159;
	st.local.f32 	[%rd2], %f160;
	st.shared.f32 	[%r10], %f160;
	bar.sync 	0;
	@%p1 bra 	$L__BB45_11;

	ld.shared.f32 	%f161, [%r4];
	mov.b32 	%r60, %f161;
	shfl.sync.bfly.b32 	%r64|%p14, %r60, %r46, %r45, %r47;
	mov.b32 	%f162, %r64;
	add.f32 	%f163, %f161, %f162;
	mov.b32 	%r65, %f163;
	shfl.sync.bfly.b32 	%r67|%p15, %r65, %r50, %r45, %r47;
	mov.b32 	%f164, %r67;
	add.f32 	%f165, %f163, %f164;
	mov.b32 	%r68, %f165;
	shfl.sync.bfly.b32 	%r70|%p16, %r68, %r53, %r45, %r47;
	mov.b32 	%f166, %r70;
	add.f32 	%f167, %f165, %f166;
	mov.b32 	%r71, %f167;
	shfl.sync.bfly.b32 	%r73|%p17, %r71, %r43, %r45, %r47;
	mov.b32 	%f168, %r73;
	add.f32 	%f169, %f167, %f168;
	mov.b32 	%r74, %f169;
	shfl.sync.bfly.b32 	%r76|%p18, %r74, %r58, %r45, %r47;
	mov.b32 	%f170, %r76;
	add.f32 	%f171, %f169, %f170;
	st.local.f32 	[%rd2], %f171;

$L__BB45_11:
	bar.sync 	0;
	mov.b32 	%r77, %f294;
	shfl.sync.bfly.b32 	%r81|%p20, %r77, %r46, %r45, %r47;
	mov.b32 	%f172, %r81;
	add.f32 	%f173, %f294, %f172;
	mov.b32 	%r82, %f173;
	shfl.sync.bfly.b32 	%r84|%p21, %r82, %r50, %r45, %r47;
	mov.b32 	%f174, %r84;
	add.f32 	%f175, %f173, %f174;
	mov.b32 	%r85, %f175;
	shfl.sync.bfly.b32 	%r87|%p22, %r85, %r53, %r45, %r47;
	mov.b32 	%f176, %r87;
	add.f32 	%f177, %f175, %f176;
	mov.b32 	%r88, %f177;
	shfl.sync.bfly.b32 	%r90|%p23, %r88, %r43, %r45, %r47;
	mov.b32 	%f178, %r90;
	add.f32 	%f179, %f177, %f178;
	mov.b32 	%r91, %f179;
	shfl.sync.bfly.b32 	%r93|%p24, %r91, %r58, %r45, %r47;
	mov.b32 	%f180, %r93;
	add.f32 	%f181, %f179, %f180;
	st.local.f32 	[%rd2+4], %f181;
	st.shared.f32 	[%r10], %f181;
	bar.sync 	0;
	@%p1 bra 	$L__BB45_13;

	ld.shared.f32 	%f182, [%r4];
	mov.b32 	%r94, %f182;
	mov.u32 	%r95, 31;
	mov.u32 	%r96, 16;
	mov.u32 	%r97, -1;
	shfl.sync.bfly.b32 	%r98|%p25, %r94, %r96, %r95, %r97;
	mov.b32 	%f183, %r98;
	add.f32 	%f184, %f182, %f183;
	mov.b32 	%r99, %f184;
	mov.u32 	%r100, 8;
	shfl.sync.bfly.b32 	%r101|%p26, %r99, %r100, %r95, %r97;
	mov.b32 	%f185, %r101;
	add.f32 	%f186, %f184, %f185;
	mov.b32 	%r102, %f186;
	mov.u32 	%r103, 4;
	shfl.sync.bfly.b32 	%r104|%p27, %r102, %r103, %r95, %r97;
	mov.b32 	%f187, %r104;
	add.f32 	%f188, %f186, %f187;
	mov.b32 	%r105, %f188;
	mov.u32 	%r106, 2;
	shfl.sync.bfly.b32 	%r107|%p28, %r105, %r106, %r95, %r97;
	mov.b32 	%f189, %r107;
	add.f32 	%f190, %f188, %f189;
	mov.b32 	%r108, %f190;
	mov.u32 	%r109, 1;
	shfl.sync.bfly.b32 	%r110|%p29, %r108, %r109, %r95, %r97;
	mov.b32 	%f191, %r110;
	add.f32 	%f192, %f190, %f191;
	st.local.f32 	[%rd2+4], %f192;

$L__BB45_13:
	bar.sync 	0;
	mov.b32 	%r111, %f293;
	mov.u32 	%r112, 31;
	mov.u32 	%r113, 16;
	mov.u32 	%r114, -1;
	shfl.sync.bfly.b32 	%r115|%p31, %r111, %r113, %r112, %r114;
	mov.b32 	%f193, %r115;
	add.f32 	%f194, %f293, %f193;
	mov.b32 	%r116, %f194;
	mov.u32 	%r117, 8;
	shfl.sync.bfly.b32 	%r118|%p32, %r116, %r117, %r112, %r114;
	mov.b32 	%f195, %r118;
	add.f32 	%f196, %f194, %f195;
	mov.b32 	%r119, %f196;
	mov.u32 	%r120, 4;
	shfl.sync.bfly.b32 	%r121|%p33, %r119, %r120, %r112, %r114;
	mov.b32 	%f197, %r121;
	add.f32 	%f198, %f196, %f197;
	mov.b32 	%r122, %f198;
	mov.u32 	%r123, 2;
	shfl.sync.bfly.b32 	%r124|%p34, %r122, %r123, %r112, %r114;
	mov.b32 	%f199, %r124;
	add.f32 	%f200, %f198, %f199;
	mov.b32 	%r125, %f200;
	mov.u32 	%r126, 1;
	shfl.sync.bfly.b32 	%r127|%p35, %r125, %r126, %r112, %r114;
	mov.b32 	%f201, %r127;
	add.f32 	%f202, %f200, %f201;
	st.local.f32 	[%rd2+8], %f202;
	st.shared.f32 	[%r10], %f202;
	bar.sync 	0;
	@%p1 bra 	$L__BB45_15;

	ld.shared.f32 	%f203, [%r4];
	mov.b32 	%r128, %f203;
	shfl.sync.bfly.b32 	%r132|%p36, %r128, %r113, %r112, %r114;
	mov.b32 	%f204, %r132;
	add.f32 	%f205, %f203, %f204;
	mov.b32 	%r133, %f205;
	shfl.sync.bfly.b32 	%r135|%p37, %r133, %r117, %r112, %r114;
	mov.b32 	%f206, %r135;
	add.f32 	%f207, %f205, %f206;
	mov.b32 	%r136, %f207;
	shfl.sync.bfly.b32 	%r138|%p38, %r136, %r120, %r112, %r114;
	mov.b32 	%f208, %r138;
	add.f32 	%f209, %f207, %f208;
	mov.b32 	%r139, %f209;
	shfl.sync.bfly.b32 	%r141|%p39, %r139, %r123, %r112, %r114;
	mov.b32 	%f210, %r141;
	add.f32 	%f211, %f209, %f210;
	mov.b32 	%r142, %f211;
	shfl.sync.bfly.b32 	%r144|%p40, %r142, %r126, %r112, %r114;
	mov.b32 	%f212, %r144;
	add.f32 	%f213, %f211, %f212;
	st.local.f32 	[%rd2+8], %f213;

$L__BB45_15:
	bar.sync 	0;
	mov.b32 	%r145, %f292;
	shfl.sync.bfly.b32 	%r149|%p42, %r145, %r113, %r112, %r114;
	mov.b32 	%f214, %r149;
	add.f32 	%f215, %f292, %f214;
	mov.b32 	%r150, %f215;
	shfl.sync.bfly.b32 	%r152|%p43, %r150, %r117, %r112, %r114;
	mov.b32 	%f216, %r152;
	add.f32 	%f217, %f215, %f216;
	mov.b32 	%r153, %f217;
	shfl.sync.bfly.b32 	%r155|%p44, %r153, %r120, %r112, %r114;
	mov.b32 	%f218, %r155;
	add.f32 	%f219, %f217, %f218;
	mov.b32 	%r156, %f219;
	shfl.sync.bfly.b32 	%r158|%p45, %r156, %r123, %r112, %r114;
	mov.b32 	%f220, %r158;
	add.f32 	%f221, %f219, %f220;
	mov.b32 	%r159, %f221;
	shfl.sync.bfly.b32 	%r161|%p46, %r159, %r126, %r112, %r114;
	mov.b32 	%f222, %r161;
	add.f32 	%f223, %f221, %f222;
	st.local.f32 	[%rd2+12], %f223;
	st.shared.f32 	[%r10], %f223;
	bar.sync 	0;
	@%p1 bra 	$L__BB45_17;

	ld.shared.f32 	%f224, [%r4];
	mov.b32 	%r162, %f224;
	mov.u32 	%r163, 31;
	mov.u32 	%r164, 16;
	mov.u32 	%r165, -1;
	shfl.sync.bfly.b32 	%r166|%p47, %r162, %r164, %r163, %r165;
	mov.b32 	%f225, %r166;
	add.f32 	%f226, %f224, %f225;
	mov.b32 	%r167, %f226;
	mov.u32 	%r168, 8;
	shfl.sync.bfly.b32 	%r169|%p48, %r167, %r168, %r163, %r165;
	mov.b32 	%f227, %r169;
	add.f32 	%f228, %f226, %f227;
	mov.b32 	%r170, %f228;
	mov.u32 	%r171, 4;
	shfl.sync.bfly.b32 	%r172|%p49, %r170, %r171, %r163, %r165;
	mov.b32 	%f229, %r172;
	add.f32 	%f230, %f228, %f229;
	mov.b32 	%r173, %f230;
	mov.u32 	%r174, 2;
	shfl.sync.bfly.b32 	%r175|%p50, %r173, %r174, %r163, %r165;
	mov.b32 	%f231, %r175;
	add.f32 	%f232, %f230, %f231;
	mov.b32 	%r176, %f232;
	mov.u32 	%r177, 1;
	shfl.sync.bfly.b32 	%r178|%p51, %r176, %r177, %r163, %r165;
	mov.b32 	%f233, %r178;
	add.f32 	%f234, %f232, %f233;
	st.local.f32 	[%rd2+12], %f234;

$L__BB45_17:
	bar.sync 	0;
	mov.b32 	%r179, %f291;
	mov.u32 	%r180, 31;
	mov.u32 	%r181, 16;
	mov.u32 	%r182, -1;
	shfl.sync.bfly.b32 	%r183|%p53, %r179, %r181, %r180, %r182;
	mov.b32 	%f235, %r183;
	add.f32 	%f236, %f291, %f235;
	mov.b32 	%r184, %f236;
	mov.u32 	%r185, 8;
	shfl.sync.bfly.b32 	%r186|%p54, %r184, %r185, %r180, %r182;
	mov.b32 	%f237, %r186;
	add.f32 	%f238, %f236, %f237;
	mov.b32 	%r187, %f238;
	mov.u32 	%r188, 4;
	shfl.sync.bfly.b32 	%r189|%p55, %r187, %r188, %r180, %r182;
	mov.b32 	%f239, %r189;
	add.f32 	%f240, %f238, %f239;
	mov.b32 	%r190, %f240;
	mov.u32 	%r191, 2;
	shfl.sync.bfly.b32 	%r192|%p56, %r190, %r191, %r180, %r182;
	mov.b32 	%f241, %r192;
	add.f32 	%f242, %f240, %f241;
	mov.b32 	%r193, %f242;
	mov.u32 	%r194, 1;
	shfl.sync.bfly.b32 	%r195|%p57, %r193, %r194, %r180, %r182;
	mov.b32 	%f243, %r195;
	add.f32 	%f244, %f242, %f243;
	st.local.f32 	[%rd2+16], %f244;
	st.shared.f32 	[%r10], %f244;
	bar.sync 	0;
	@%p1 bra 	$L__BB45_19;

	ld.shared.f32 	%f245, [%r4];
	mov.b32 	%r196, %f245;
	shfl.sync.bfly.b32 	%r200|%p58, %r196, %r181, %r180, %r182;
	mov.b32 	%f246, %r200;
	add.f32 	%f247, %f245, %f246;
	mov.b32 	%r201, %f247;
	shfl.sync.bfly.b32 	%r203|%p59, %r201, %r185, %r180, %r182;
	mov.b32 	%f248, %r203;
	add.f32 	%f249, %f247, %f248;
	mov.b32 	%r204, %f249;
	shfl.sync.bfly.b32 	%r206|%p60, %r204, %r188, %r180, %r182;
	mov.b32 	%f250, %r206;
	add.f32 	%f251, %f249, %f250;
	mov.b32 	%r207, %f251;
	shfl.sync.bfly.b32 	%r209|%p61, %r207, %r191, %r180, %r182;
	mov.b32 	%f252, %r209;
	add.f32 	%f253, %f251, %f252;
	mov.b32 	%r210, %f253;
	shfl.sync.bfly.b32 	%r212|%p62, %r210, %r194, %r180, %r182;
	mov.b32 	%f254, %r212;
	add.f32 	%f255, %f253, %f254;
	st.local.f32 	[%rd2+16], %f255;

$L__BB45_19:
	bar.sync 	0;
	mov.b32 	%r213, %f290;
	shfl.sync.bfly.b32 	%r217|%p64, %r213, %r181, %r180, %r182;
	mov.b32 	%f256, %r217;
	add.f32 	%f257, %f290, %f256;
	mov.b32 	%r218, %f257;
	shfl.sync.bfly.b32 	%r220|%p65, %r218, %r185, %r180, %r182;
	mov.b32 	%f258, %r220;
	add.f32 	%f259, %f257, %f258;
	mov.b32 	%r221, %f259;
	shfl.sync.bfly.b32 	%r223|%p66, %r221, %r188, %r180, %r182;
	mov.b32 	%f260, %r223;
	add.f32 	%f261, %f259, %f260;
	mov.b32 	%r224, %f261;
	shfl.sync.bfly.b32 	%r226|%p67, %r224, %r191, %r180, %r182;
	mov.b32 	%f262, %r226;
	add.f32 	%f263, %f261, %f262;
	mov.b32 	%r227, %f263;
	shfl.sync.bfly.b32 	%r229|%p68, %r227, %r194, %r180, %r182;
	mov.b32 	%f264, %r229;
	add.f32 	%f265, %f263, %f264;
	st.local.f32 	[%rd2+20], %f265;
	st.shared.f32 	[%r10], %f265;
	bar.sync 	0;
	@%p1 bra 	$L__BB45_21;

	ld.shared.f32 	%f266, [%r4];
	mov.b32 	%r230, %f266;
	mov.u32 	%r231, 31;
	mov.u32 	%r232, 16;
	mov.u32 	%r233, -1;
	shfl.sync.bfly.b32 	%r234|%p69, %r230, %r232, %r231, %r233;
	mov.b32 	%f267, %r234;
	add.f32 	%f268, %f266, %f267;
	mov.b32 	%r235, %f268;
	mov.u32 	%r236, 8;
	shfl.sync.bfly.b32 	%r237|%p70, %r235, %r236, %r231, %r233;
	mov.b32 	%f269, %r237;
	add.f32 	%f270, %f268, %f269;
	mov.b32 	%r238, %f270;
	mov.u32 	%r239, 4;
	shfl.sync.bfly.b32 	%r240|%p71, %r238, %r239, %r231, %r233;
	mov.b32 	%f271, %r240;
	add.f32 	%f272, %f270, %f271;
	mov.b32 	%r241, %f272;
	mov.u32 	%r242, 2;
	shfl.sync.bfly.b32 	%r243|%p72, %r241, %r242, %r231, %r233;
	mov.b32 	%f273, %r243;
	add.f32 	%f274, %f272, %f273;
	mov.b32 	%r244, %f274;
	mov.u32 	%r245, 1;
	shfl.sync.bfly.b32 	%r246|%p73, %r244, %r245, %r231, %r233;
	mov.b32 	%f275, %r246;
	add.f32 	%f276, %f274, %f275;
	st.local.f32 	[%rd2+20], %f276;

$L__BB45_21:
	bar.sync 	0;
	setp.gt.s32 	%p74, %r3, 5;
	@%p74 bra 	$L__BB45_23;

	mul.wide.s32 	%rd60, %r3, 4;
	add.s64 	%rd61, %rd2, %rd60;
	ld.local.f32 	%f277, [%rd61];
	mad.lo.s32 	%r247, %r3, %r13, %r2;
	cvt.s64.s32 	%rd62, %r247;
	mul.lo.s32 	%r248, %r1, %r14;
	cvt.s64.s32 	%rd63, %r248;
	add.s64 	%rd64, %rd63, %rd62;
	cvta.to.global.u64 	%rd65, %rd19;
	shl.b64 	%rd66, %rd64, 2;
	add.s64 	%rd67, %rd65, %rd66;
	st.global.f32 	[%rd67], %f277;

$L__BB45_23:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_7_bs_192
.visible .entry ggml_matvec_f32_ncols_7_bs_192(
	.param .u64 ggml_matvec_f32_ncols_7_bs_192_param_0,
	.param .u64 ggml_matvec_f32_ncols_7_bs_192_param_1,
	.param .u64 ggml_matvec_f32_ncols_7_bs_192_param_2,
	.param .u32 ggml_matvec_f32_ncols_7_bs_192_param_3,
	.param .u32 ggml_matvec_f32_ncols_7_bs_192_param_4,
	.param .u32 ggml_matvec_f32_ncols_7_bs_192_param_5,
	.param .u32 ggml_matvec_f32_ncols_7_bs_192_param_6,
	.param .u32 ggml_matvec_f32_ncols_7_bs_192_param_7,
	.param .u32 ggml_matvec_f32_ncols_7_bs_192_param_8,
	.param .u32 ggml_matvec_f32_ncols_7_bs_192_param_9,
	.param .u32 ggml_matvec_f32_ncols_7_bs_192_param_10,
	.param .u32 ggml_matvec_f32_ncols_7_bs_192_param_11
)
{
	.local .align 4 .b8 	__local_depot46[28];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<86>;
	.reg .f32 	%f<343>;
	.reg .b32 	%r<287>;
	.reg .b64 	%rd<74>;


	mov.u64 	%SPL, __local_depot46;
	ld.param.u64 	%rd21, [ggml_matvec_f32_ncols_7_bs_192_param_0];
	ld.param.u64 	%rd22, [ggml_matvec_f32_ncols_7_bs_192_param_1];
	ld.param.u64 	%rd20, [ggml_matvec_f32_ncols_7_bs_192_param_2];
	ld.param.u32 	%r11, [ggml_matvec_f32_ncols_7_bs_192_param_3];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_7_bs_192_param_5];
	ld.param.u32 	%r12, [ggml_matvec_f32_ncols_7_bs_192_param_6];
	ld.param.u32 	%r13, [ggml_matvec_f32_ncols_7_bs_192_param_7];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_7_bs_192_param_8];
	ld.param.u32 	%r17, [ggml_matvec_f32_ncols_7_bs_192_param_9];
	ld.param.u32 	%r18, [ggml_matvec_f32_ncols_7_bs_192_param_10];
	ld.param.u32 	%r14, [ggml_matvec_f32_ncols_7_bs_192_param_11];
	cvta.to.global.u64 	%rd73, %rd22;
	add.u64 	%rd2, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r19, %r1, %r16;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r20, %r2, %r15;
	mad.lo.s32 	%r21, %r19, %r17, %r20;
	cvt.s64.s32 	%rd3, %r21;
	cvta.to.global.u64 	%rd4, %rd21;
	mul.lo.s32 	%r22, %r1, %r18;
	cvt.s64.s32 	%rd5, %r22;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r23, %r3, 2;
	mov.u32 	%r24, data_mmv;
	add.s32 	%r4, %r24, %r23;
	@%p1 bra 	$L__BB46_2;

	mov.u32 	%r25, 0;
	st.shared.u32 	[%r4], %r25;

$L__BB46_2:
	bar.sync 	0;
	mov.f32 	%f336, 0f00000000;
	mov.u32 	%r26, 0;
	st.local.u32 	[%rd2], %r26;
	st.local.u32 	[%rd2+4], %r26;
	st.local.u32 	[%rd2+8], %r26;
	st.local.u32 	[%rd2+12], %r26;
	st.local.u32 	[%rd2+16], %r26;
	st.local.u32 	[%rd2+20], %r26;
	st.local.u32 	[%rd2+24], %r26;
	setp.ge.s32 	%p2, %r3, %r11;
	mov.f32 	%f337, %f336;
	mov.f32 	%f338, %f336;
	mov.f32 	%f339, %f336;
	mov.f32 	%f340, %f336;
	mov.f32 	%f341, %f336;
	mov.f32 	%f342, %f336;
	@%p2 bra 	$L__BB46_9;

	not.b32 	%r27, %r3;
	add.s32 	%r5, %r27, %r11;
	mul.wide.u32 	%rd24, %r5, -1431655765;
	shr.u64 	%rd25, %rd24, 39;
	and.b64  	%rd26, %rd25, 1;
	setp.eq.b64 	%p3, %rd26, 1;
	mov.pred 	%p4, 0;
	xor.pred  	%p5, %p3, %p4;
	mov.f32 	%f336, 0f00000000;
	mov.u32 	%r286, %r3;
	@%p5 bra 	$L__BB46_5;

	shl.b64 	%rd27, %rd5, 2;
	add.s64 	%rd28, %rd73, %rd27;
	shl.b64 	%rd29, %rd3, 2;
	add.s64 	%rd30, %rd4, %rd29;
	mul.wide.s32 	%rd31, %r3, 8;
	add.s64 	%rd32, %rd30, %rd31;
	ld.global.nc.v2.f32 	{%f50, %f51}, [%rd32];
	add.s64 	%rd33, %rd28, %rd31;
	ld.global.nc.v2.f32 	{%f54, %f55}, [%rd33];
	fma.rn.f32 	%f58, %f50, %f54, 0f00000000;
	fma.rn.f32 	%f342, %f51, %f55, %f58;
	st.local.f32 	[%rd2], %f342;
	mul.wide.s32 	%rd34, %r12, 8;
	add.s64 	%rd35, %rd33, %rd34;
	ld.global.nc.v2.f32 	{%f59, %f60}, [%rd35];
	fma.rn.f32 	%f63, %f50, %f59, 0f00000000;
	fma.rn.f32 	%f341, %f51, %f60, %f63;
	st.local.f32 	[%rd2+4], %f341;
	add.s32 	%r28, %r3, %r12;
	add.s32 	%r29, %r28, %r12;
	mul.wide.s32 	%rd36, %r29, 8;
	add.s64 	%rd37, %rd28, %rd36;
	ld.global.nc.v2.f32 	{%f64, %f65}, [%rd37];
	fma.rn.f32 	%f68, %f50, %f64, 0f00000000;
	fma.rn.f32 	%f340, %f51, %f65, %f68;
	st.local.f32 	[%rd2+8], %f340;
	add.s64 	%rd38, %rd37, %rd34;
	ld.global.nc.v2.f32 	{%f69, %f70}, [%rd38];
	fma.rn.f32 	%f73, %f50, %f69, 0f00000000;
	fma.rn.f32 	%f339, %f51, %f70, %f73;
	st.local.f32 	[%rd2+12], %f339;
	add.s64 	%rd39, %rd38, %rd34;
	ld.global.nc.v2.f32 	{%f74, %f75}, [%rd39];
	fma.rn.f32 	%f78, %f50, %f74, 0f00000000;
	fma.rn.f32 	%f338, %f51, %f75, %f78;
	st.local.f32 	[%rd2+16], %f338;
	add.s64 	%rd40, %rd39, %rd34;
	ld.global.nc.v2.f32 	{%f79, %f80}, [%rd40];
	fma.rn.f32 	%f83, %f50, %f79, 0f00000000;
	fma.rn.f32 	%f337, %f51, %f80, %f83;
	st.local.f32 	[%rd2+20], %f337;
	add.s64 	%rd41, %rd40, %rd34;
	ld.global.nc.v2.f32 	{%f84, %f85}, [%rd41];
	fma.rn.f32 	%f88, %f50, %f84, 0f00000000;
	fma.rn.f32 	%f336, %f51, %f85, %f88;
	st.local.f32 	[%rd2+24], %f336;
	add.s32 	%r286, %r3, 192;

$L__BB46_5:
	setp.lt.u32 	%p6, %r5, 192;
	@%p6 bra 	$L__BB46_9;

	add.s32 	%r30, %r286, %r12;
	add.s32 	%r31, %r30, 192;
	mul.wide.s32 	%rd42, %r31, 8;
	shl.b64 	%rd43, %rd5, 2;
	add.s64 	%rd7, %rd42, %rd43;
	shl.b32 	%r32, %r12, 1;
	add.s32 	%r33, %r286, %r32;
	mad.lo.s32 	%r34, %r12, 3, %r286;
	shl.b32 	%r35, %r12, 2;
	add.s32 	%r36, %r286, %r35;
	mad.lo.s32 	%r37, %r12, 5, %r286;
	mad.lo.s32 	%r38, %r12, 6, %r286;
	mul.wide.s32 	%rd44, %r33, 8;
	add.s64 	%rd8, %rd44, %rd43;
	mul.wide.s32 	%rd45, %r34, 8;
	add.s64 	%rd9, %rd45, %rd43;
	mul.wide.s32 	%rd46, %r36, 8;
	add.s64 	%rd10, %rd46, %rd43;
	mul.wide.s32 	%rd47, %r37, 8;
	add.s64 	%rd11, %rd47, %rd43;
	mul.wide.s32 	%rd48, %r38, 8;
	add.s64 	%rd12, %rd48, %rd43;
	mul.wide.s32 	%rd49, %r286, 2;
	add.s64 	%rd50, %rd49, %rd3;
	shl.b64 	%rd51, %rd50, 2;
	add.s64 	%rd52, %rd4, %rd51;
	add.s64 	%rd72, %rd52, 1536;
	mul.wide.s32 	%rd53, %r286, 8;
	mul.wide.s32 	%rd54, %r12, 8;
	add.s64 	%rd55, %rd53, %rd54;
	add.s64 	%rd14, %rd55, %rd43;
	add.s64 	%rd15, %rd53, %rd43;

$L__BB46_7:
	ld.global.nc.v2.f32 	{%f89, %f90}, [%rd72+-1536];
	add.s64 	%rd56, %rd73, %rd15;
	ld.global.nc.v2.f32 	{%f93, %f94}, [%rd56];
	fma.rn.f32 	%f97, %f89, %f93, %f342;
	fma.rn.f32 	%f98, %f90, %f94, %f97;
	add.s64 	%rd57, %rd73, %rd14;
	ld.global.nc.v2.f32 	{%f99, %f100}, [%rd57];
	fma.rn.f32 	%f103, %f89, %f99, %f341;
	fma.rn.f32 	%f104, %f90, %f100, %f103;
	add.s64 	%rd58, %rd73, %rd8;
	ld.global.nc.v2.f32 	{%f105, %f106}, [%rd58];
	fma.rn.f32 	%f109, %f89, %f105, %f340;
	fma.rn.f32 	%f110, %f90, %f106, %f109;
	add.s64 	%rd59, %rd73, %rd9;
	ld.global.nc.v2.f32 	{%f111, %f112}, [%rd59];
	fma.rn.f32 	%f115, %f89, %f111, %f339;
	fma.rn.f32 	%f116, %f90, %f112, %f115;
	add.s64 	%rd60, %rd73, %rd10;
	ld.global.nc.v2.f32 	{%f117, %f118}, [%rd60];
	fma.rn.f32 	%f121, %f89, %f117, %f338;
	fma.rn.f32 	%f122, %f90, %f118, %f121;
	add.s64 	%rd61, %rd73, %rd11;
	ld.global.nc.v2.f32 	{%f123, %f124}, [%rd61];
	fma.rn.f32 	%f127, %f89, %f123, %f337;
	fma.rn.f32 	%f128, %f90, %f124, %f127;
	add.s64 	%rd62, %rd73, %rd12;
	ld.global.nc.v2.f32 	{%f129, %f130}, [%rd62];
	fma.rn.f32 	%f133, %f89, %f129, %f336;
	fma.rn.f32 	%f134, %f90, %f130, %f133;
	ld.global.nc.v2.f32 	{%f135, %f136}, [%rd72];
	ld.global.nc.v2.f32 	{%f139, %f140}, [%rd56+1536];
	fma.rn.f32 	%f143, %f135, %f139, %f98;
	fma.rn.f32 	%f342, %f136, %f140, %f143;
	add.s64 	%rd63, %rd73, %rd7;
	ld.global.nc.v2.f32 	{%f144, %f145}, [%rd63];
	fma.rn.f32 	%f148, %f135, %f144, %f104;
	fma.rn.f32 	%f341, %f136, %f145, %f148;
	ld.global.nc.v2.f32 	{%f149, %f150}, [%rd58+1536];
	fma.rn.f32 	%f153, %f135, %f149, %f110;
	fma.rn.f32 	%f340, %f136, %f150, %f153;
	ld.global.nc.v2.f32 	{%f154, %f155}, [%rd59+1536];
	fma.rn.f32 	%f158, %f135, %f154, %f116;
	fma.rn.f32 	%f339, %f136, %f155, %f158;
	ld.global.nc.v2.f32 	{%f159, %f160}, [%rd60+1536];
	fma.rn.f32 	%f163, %f135, %f159, %f122;
	fma.rn.f32 	%f338, %f136, %f160, %f163;
	ld.global.nc.v2.f32 	{%f164, %f165}, [%rd61+1536];
	fma.rn.f32 	%f168, %f135, %f164, %f128;
	fma.rn.f32 	%f337, %f136, %f165, %f168;
	ld.global.nc.v2.f32 	{%f169, %f170}, [%rd62+1536];
	fma.rn.f32 	%f173, %f135, %f169, %f134;
	fma.rn.f32 	%f336, %f136, %f170, %f173;
	add.s64 	%rd73, %rd73, 3072;
	add.s64 	%rd72, %rd72, 3072;
	add.s32 	%r286, %r286, 384;
	setp.lt.s32 	%p7, %r286, %r11;
	@%p7 bra 	$L__BB46_7;

	st.local.f32 	[%rd2], %f342;
	st.local.f32 	[%rd2+4], %f341;
	st.local.f32 	[%rd2+8], %f340;
	st.local.f32 	[%rd2+12], %f339;
	st.local.f32 	[%rd2+16], %f338;
	st.local.f32 	[%rd2+20], %f337;
	st.local.f32 	[%rd2+24], %f336;

$L__BB46_9:
	shr.s32 	%r39, %r3, 31;
	shr.u32 	%r40, %r39, 27;
	add.s32 	%r41, %r3, %r40;
	shr.s32 	%r42, %r41, 5;
	shl.b32 	%r43, %r42, 2;
	add.s32 	%r10, %r24, %r43;
	mov.u32 	%r45, 2;
	mov.b32 	%r46, %f342;
	mov.u32 	%r47, 31;
	mov.u32 	%r48, 16;
	mov.u32 	%r49, -1;
	shfl.sync.bfly.b32 	%r50|%p8, %r46, %r48, %r47, %r49;
	mov.b32 	%f174, %r50;
	add.f32 	%f175, %f342, %f174;
	mov.b32 	%r51, %f175;
	mov.u32 	%r52, 8;
	shfl.sync.bfly.b32 	%r53|%p9, %r51, %r52, %r47, %r49;
	mov.b32 	%f176, %r53;
	add.f32 	%f177, %f175, %f176;
	mov.b32 	%r54, %f177;
	mov.u32 	%r55, 4;
	shfl.sync.bfly.b32 	%r56|%p10, %r54, %r55, %r47, %r49;
	mov.b32 	%f178, %r56;
	add.f32 	%f179, %f177, %f178;
	mov.b32 	%r57, %f179;
	shfl.sync.bfly.b32 	%r58|%p11, %r57, %r45, %r47, %r49;
	mov.b32 	%f180, %r58;
	add.f32 	%f181, %f179, %f180;
	mov.b32 	%r59, %f181;
	mov.u32 	%r60, 1;
	shfl.sync.bfly.b32 	%r61|%p12, %r59, %r60, %r47, %r49;
	mov.b32 	%f182, %r61;
	add.f32 	%f183, %f181, %f182;
	st.local.f32 	[%rd2], %f183;
	st.shared.f32 	[%r10], %f183;
	bar.sync 	0;
	@%p1 bra 	$L__BB46_11;

	ld.shared.f32 	%f184, [%r4];
	mov.b32 	%r62, %f184;
	shfl.sync.bfly.b32 	%r66|%p14, %r62, %r48, %r47, %r49;
	mov.b32 	%f185, %r66;
	add.f32 	%f186, %f184, %f185;
	mov.b32 	%r67, %f186;
	shfl.sync.bfly.b32 	%r69|%p15, %r67, %r52, %r47, %r49;
	mov.b32 	%f187, %r69;
	add.f32 	%f188, %f186, %f187;
	mov.b32 	%r70, %f188;
	shfl.sync.bfly.b32 	%r72|%p16, %r70, %r55, %r47, %r49;
	mov.b32 	%f189, %r72;
	add.f32 	%f190, %f188, %f189;
	mov.b32 	%r73, %f190;
	shfl.sync.bfly.b32 	%r75|%p17, %r73, %r45, %r47, %r49;
	mov.b32 	%f191, %r75;
	add.f32 	%f192, %f190, %f191;
	mov.b32 	%r76, %f192;
	shfl.sync.bfly.b32 	%r78|%p18, %r76, %r60, %r47, %r49;
	mov.b32 	%f193, %r78;
	add.f32 	%f194, %f192, %f193;
	st.local.f32 	[%rd2], %f194;

$L__BB46_11:
	bar.sync 	0;
	mov.b32 	%r79, %f341;
	shfl.sync.bfly.b32 	%r83|%p20, %r79, %r48, %r47, %r49;
	mov.b32 	%f195, %r83;
	add.f32 	%f196, %f341, %f195;
	mov.b32 	%r84, %f196;
	shfl.sync.bfly.b32 	%r86|%p21, %r84, %r52, %r47, %r49;
	mov.b32 	%f197, %r86;
	add.f32 	%f198, %f196, %f197;
	mov.b32 	%r87, %f198;
	shfl.sync.bfly.b32 	%r89|%p22, %r87, %r55, %r47, %r49;
	mov.b32 	%f199, %r89;
	add.f32 	%f200, %f198, %f199;
	mov.b32 	%r90, %f200;
	shfl.sync.bfly.b32 	%r92|%p23, %r90, %r45, %r47, %r49;
	mov.b32 	%f201, %r92;
	add.f32 	%f202, %f200, %f201;
	mov.b32 	%r93, %f202;
	shfl.sync.bfly.b32 	%r95|%p24, %r93, %r60, %r47, %r49;
	mov.b32 	%f203, %r95;
	add.f32 	%f204, %f202, %f203;
	st.local.f32 	[%rd2+4], %f204;
	st.shared.f32 	[%r10], %f204;
	bar.sync 	0;
	@%p1 bra 	$L__BB46_13;

	ld.shared.f32 	%f205, [%r4];
	mov.b32 	%r96, %f205;
	mov.u32 	%r97, 31;
	mov.u32 	%r98, 16;
	mov.u32 	%r99, -1;
	shfl.sync.bfly.b32 	%r100|%p25, %r96, %r98, %r97, %r99;
	mov.b32 	%f206, %r100;
	add.f32 	%f207, %f205, %f206;
	mov.b32 	%r101, %f207;
	mov.u32 	%r102, 8;
	shfl.sync.bfly.b32 	%r103|%p26, %r101, %r102, %r97, %r99;
	mov.b32 	%f208, %r103;
	add.f32 	%f209, %f207, %f208;
	mov.b32 	%r104, %f209;
	mov.u32 	%r105, 4;
	shfl.sync.bfly.b32 	%r106|%p27, %r104, %r105, %r97, %r99;
	mov.b32 	%f210, %r106;
	add.f32 	%f211, %f209, %f210;
	mov.b32 	%r107, %f211;
	mov.u32 	%r108, 2;
	shfl.sync.bfly.b32 	%r109|%p28, %r107, %r108, %r97, %r99;
	mov.b32 	%f212, %r109;
	add.f32 	%f213, %f211, %f212;
	mov.b32 	%r110, %f213;
	mov.u32 	%r111, 1;
	shfl.sync.bfly.b32 	%r112|%p29, %r110, %r111, %r97, %r99;
	mov.b32 	%f214, %r112;
	add.f32 	%f215, %f213, %f214;
	st.local.f32 	[%rd2+4], %f215;

$L__BB46_13:
	bar.sync 	0;
	mov.b32 	%r113, %f340;
	mov.u32 	%r114, 31;
	mov.u32 	%r115, 16;
	mov.u32 	%r116, -1;
	shfl.sync.bfly.b32 	%r117|%p31, %r113, %r115, %r114, %r116;
	mov.b32 	%f216, %r117;
	add.f32 	%f217, %f340, %f216;
	mov.b32 	%r118, %f217;
	mov.u32 	%r119, 8;
	shfl.sync.bfly.b32 	%r120|%p32, %r118, %r119, %r114, %r116;
	mov.b32 	%f218, %r120;
	add.f32 	%f219, %f217, %f218;
	mov.b32 	%r121, %f219;
	mov.u32 	%r122, 4;
	shfl.sync.bfly.b32 	%r123|%p33, %r121, %r122, %r114, %r116;
	mov.b32 	%f220, %r123;
	add.f32 	%f221, %f219, %f220;
	mov.b32 	%r124, %f221;
	mov.u32 	%r125, 2;
	shfl.sync.bfly.b32 	%r126|%p34, %r124, %r125, %r114, %r116;
	mov.b32 	%f222, %r126;
	add.f32 	%f223, %f221, %f222;
	mov.b32 	%r127, %f223;
	mov.u32 	%r128, 1;
	shfl.sync.bfly.b32 	%r129|%p35, %r127, %r128, %r114, %r116;
	mov.b32 	%f224, %r129;
	add.f32 	%f225, %f223, %f224;
	st.local.f32 	[%rd2+8], %f225;
	st.shared.f32 	[%r10], %f225;
	bar.sync 	0;
	@%p1 bra 	$L__BB46_15;

	ld.shared.f32 	%f226, [%r4];
	mov.b32 	%r130, %f226;
	shfl.sync.bfly.b32 	%r134|%p36, %r130, %r115, %r114, %r116;
	mov.b32 	%f227, %r134;
	add.f32 	%f228, %f226, %f227;
	mov.b32 	%r135, %f228;
	shfl.sync.bfly.b32 	%r137|%p37, %r135, %r119, %r114, %r116;
	mov.b32 	%f229, %r137;
	add.f32 	%f230, %f228, %f229;
	mov.b32 	%r138, %f230;
	shfl.sync.bfly.b32 	%r140|%p38, %r138, %r122, %r114, %r116;
	mov.b32 	%f231, %r140;
	add.f32 	%f232, %f230, %f231;
	mov.b32 	%r141, %f232;
	shfl.sync.bfly.b32 	%r143|%p39, %r141, %r125, %r114, %r116;
	mov.b32 	%f233, %r143;
	add.f32 	%f234, %f232, %f233;
	mov.b32 	%r144, %f234;
	shfl.sync.bfly.b32 	%r146|%p40, %r144, %r128, %r114, %r116;
	mov.b32 	%f235, %r146;
	add.f32 	%f236, %f234, %f235;
	st.local.f32 	[%rd2+8], %f236;

$L__BB46_15:
	bar.sync 	0;
	mov.b32 	%r147, %f339;
	shfl.sync.bfly.b32 	%r151|%p42, %r147, %r115, %r114, %r116;
	mov.b32 	%f237, %r151;
	add.f32 	%f238, %f339, %f237;
	mov.b32 	%r152, %f238;
	shfl.sync.bfly.b32 	%r154|%p43, %r152, %r119, %r114, %r116;
	mov.b32 	%f239, %r154;
	add.f32 	%f240, %f238, %f239;
	mov.b32 	%r155, %f240;
	shfl.sync.bfly.b32 	%r157|%p44, %r155, %r122, %r114, %r116;
	mov.b32 	%f241, %r157;
	add.f32 	%f242, %f240, %f241;
	mov.b32 	%r158, %f242;
	shfl.sync.bfly.b32 	%r160|%p45, %r158, %r125, %r114, %r116;
	mov.b32 	%f243, %r160;
	add.f32 	%f244, %f242, %f243;
	mov.b32 	%r161, %f244;
	shfl.sync.bfly.b32 	%r163|%p46, %r161, %r128, %r114, %r116;
	mov.b32 	%f245, %r163;
	add.f32 	%f246, %f244, %f245;
	st.local.f32 	[%rd2+12], %f246;
	st.shared.f32 	[%r10], %f246;
	bar.sync 	0;
	@%p1 bra 	$L__BB46_17;

	ld.shared.f32 	%f247, [%r4];
	mov.b32 	%r164, %f247;
	mov.u32 	%r165, 31;
	mov.u32 	%r166, 16;
	mov.u32 	%r167, -1;
	shfl.sync.bfly.b32 	%r168|%p47, %r164, %r166, %r165, %r167;
	mov.b32 	%f248, %r168;
	add.f32 	%f249, %f247, %f248;
	mov.b32 	%r169, %f249;
	mov.u32 	%r170, 8;
	shfl.sync.bfly.b32 	%r171|%p48, %r169, %r170, %r165, %r167;
	mov.b32 	%f250, %r171;
	add.f32 	%f251, %f249, %f250;
	mov.b32 	%r172, %f251;
	mov.u32 	%r173, 4;
	shfl.sync.bfly.b32 	%r174|%p49, %r172, %r173, %r165, %r167;
	mov.b32 	%f252, %r174;
	add.f32 	%f253, %f251, %f252;
	mov.b32 	%r175, %f253;
	mov.u32 	%r176, 2;
	shfl.sync.bfly.b32 	%r177|%p50, %r175, %r176, %r165, %r167;
	mov.b32 	%f254, %r177;
	add.f32 	%f255, %f253, %f254;
	mov.b32 	%r178, %f255;
	mov.u32 	%r179, 1;
	shfl.sync.bfly.b32 	%r180|%p51, %r178, %r179, %r165, %r167;
	mov.b32 	%f256, %r180;
	add.f32 	%f257, %f255, %f256;
	st.local.f32 	[%rd2+12], %f257;

$L__BB46_17:
	bar.sync 	0;
	mov.b32 	%r181, %f338;
	mov.u32 	%r182, 31;
	mov.u32 	%r183, 16;
	mov.u32 	%r184, -1;
	shfl.sync.bfly.b32 	%r185|%p53, %r181, %r183, %r182, %r184;
	mov.b32 	%f258, %r185;
	add.f32 	%f259, %f338, %f258;
	mov.b32 	%r186, %f259;
	mov.u32 	%r187, 8;
	shfl.sync.bfly.b32 	%r188|%p54, %r186, %r187, %r182, %r184;
	mov.b32 	%f260, %r188;
	add.f32 	%f261, %f259, %f260;
	mov.b32 	%r189, %f261;
	mov.u32 	%r190, 4;
	shfl.sync.bfly.b32 	%r191|%p55, %r189, %r190, %r182, %r184;
	mov.b32 	%f262, %r191;
	add.f32 	%f263, %f261, %f262;
	mov.b32 	%r192, %f263;
	mov.u32 	%r193, 2;
	shfl.sync.bfly.b32 	%r194|%p56, %r192, %r193, %r182, %r184;
	mov.b32 	%f264, %r194;
	add.f32 	%f265, %f263, %f264;
	mov.b32 	%r195, %f265;
	mov.u32 	%r196, 1;
	shfl.sync.bfly.b32 	%r197|%p57, %r195, %r196, %r182, %r184;
	mov.b32 	%f266, %r197;
	add.f32 	%f267, %f265, %f266;
	st.local.f32 	[%rd2+16], %f267;
	st.shared.f32 	[%r10], %f267;
	bar.sync 	0;
	@%p1 bra 	$L__BB46_19;

	ld.shared.f32 	%f268, [%r4];
	mov.b32 	%r198, %f268;
	shfl.sync.bfly.b32 	%r202|%p58, %r198, %r183, %r182, %r184;
	mov.b32 	%f269, %r202;
	add.f32 	%f270, %f268, %f269;
	mov.b32 	%r203, %f270;
	shfl.sync.bfly.b32 	%r205|%p59, %r203, %r187, %r182, %r184;
	mov.b32 	%f271, %r205;
	add.f32 	%f272, %f270, %f271;
	mov.b32 	%r206, %f272;
	shfl.sync.bfly.b32 	%r208|%p60, %r206, %r190, %r182, %r184;
	mov.b32 	%f273, %r208;
	add.f32 	%f274, %f272, %f273;
	mov.b32 	%r209, %f274;
	shfl.sync.bfly.b32 	%r211|%p61, %r209, %r193, %r182, %r184;
	mov.b32 	%f275, %r211;
	add.f32 	%f276, %f274, %f275;
	mov.b32 	%r212, %f276;
	shfl.sync.bfly.b32 	%r214|%p62, %r212, %r196, %r182, %r184;
	mov.b32 	%f277, %r214;
	add.f32 	%f278, %f276, %f277;
	st.local.f32 	[%rd2+16], %f278;

$L__BB46_19:
	bar.sync 	0;
	mov.b32 	%r215, %f337;
	shfl.sync.bfly.b32 	%r219|%p64, %r215, %r183, %r182, %r184;
	mov.b32 	%f279, %r219;
	add.f32 	%f280, %f337, %f279;
	mov.b32 	%r220, %f280;
	shfl.sync.bfly.b32 	%r222|%p65, %r220, %r187, %r182, %r184;
	mov.b32 	%f281, %r222;
	add.f32 	%f282, %f280, %f281;
	mov.b32 	%r223, %f282;
	shfl.sync.bfly.b32 	%r225|%p66, %r223, %r190, %r182, %r184;
	mov.b32 	%f283, %r225;
	add.f32 	%f284, %f282, %f283;
	mov.b32 	%r226, %f284;
	shfl.sync.bfly.b32 	%r228|%p67, %r226, %r193, %r182, %r184;
	mov.b32 	%f285, %r228;
	add.f32 	%f286, %f284, %f285;
	mov.b32 	%r229, %f286;
	shfl.sync.bfly.b32 	%r231|%p68, %r229, %r196, %r182, %r184;
	mov.b32 	%f287, %r231;
	add.f32 	%f288, %f286, %f287;
	st.local.f32 	[%rd2+20], %f288;
	st.shared.f32 	[%r10], %f288;
	bar.sync 	0;
	@%p1 bra 	$L__BB46_21;

	ld.shared.f32 	%f289, [%r4];
	mov.b32 	%r232, %f289;
	mov.u32 	%r233, 31;
	mov.u32 	%r234, 16;
	mov.u32 	%r235, -1;
	shfl.sync.bfly.b32 	%r236|%p69, %r232, %r234, %r233, %r235;
	mov.b32 	%f290, %r236;
	add.f32 	%f291, %f289, %f290;
	mov.b32 	%r237, %f291;
	mov.u32 	%r238, 8;
	shfl.sync.bfly.b32 	%r239|%p70, %r237, %r238, %r233, %r235;
	mov.b32 	%f292, %r239;
	add.f32 	%f293, %f291, %f292;
	mov.b32 	%r240, %f293;
	mov.u32 	%r241, 4;
	shfl.sync.bfly.b32 	%r242|%p71, %r240, %r241, %r233, %r235;
	mov.b32 	%f294, %r242;
	add.f32 	%f295, %f293, %f294;
	mov.b32 	%r243, %f295;
	mov.u32 	%r244, 2;
	shfl.sync.bfly.b32 	%r245|%p72, %r243, %r244, %r233, %r235;
	mov.b32 	%f296, %r245;
	add.f32 	%f297, %f295, %f296;
	mov.b32 	%r246, %f297;
	mov.u32 	%r247, 1;
	shfl.sync.bfly.b32 	%r248|%p73, %r246, %r247, %r233, %r235;
	mov.b32 	%f298, %r248;
	add.f32 	%f299, %f297, %f298;
	st.local.f32 	[%rd2+20], %f299;

$L__BB46_21:
	bar.sync 	0;
	mov.b32 	%r249, %f336;
	mov.u32 	%r250, 31;
	mov.u32 	%r251, 16;
	mov.u32 	%r252, -1;
	shfl.sync.bfly.b32 	%r253|%p75, %r249, %r251, %r250, %r252;
	mov.b32 	%f300, %r253;
	add.f32 	%f301, %f336, %f300;
	mov.b32 	%r254, %f301;
	mov.u32 	%r255, 8;
	shfl.sync.bfly.b32 	%r256|%p76, %r254, %r255, %r250, %r252;
	mov.b32 	%f302, %r256;
	add.f32 	%f303, %f301, %f302;
	mov.b32 	%r257, %f303;
	mov.u32 	%r258, 4;
	shfl.sync.bfly.b32 	%r259|%p77, %r257, %r258, %r250, %r252;
	mov.b32 	%f304, %r259;
	add.f32 	%f305, %f303, %f304;
	mov.b32 	%r260, %f305;
	mov.u32 	%r261, 2;
	shfl.sync.bfly.b32 	%r262|%p78, %r260, %r261, %r250, %r252;
	mov.b32 	%f306, %r262;
	add.f32 	%f307, %f305, %f306;
	mov.b32 	%r263, %f307;
	mov.u32 	%r264, 1;
	shfl.sync.bfly.b32 	%r265|%p79, %r263, %r264, %r250, %r252;
	mov.b32 	%f308, %r265;
	add.f32 	%f309, %f307, %f308;
	st.local.f32 	[%rd2+24], %f309;
	st.shared.f32 	[%r10], %f309;
	bar.sync 	0;
	@%p1 bra 	$L__BB46_23;

	ld.shared.f32 	%f310, [%r4];
	mov.b32 	%r266, %f310;
	shfl.sync.bfly.b32 	%r270|%p80, %r266, %r251, %r250, %r252;
	mov.b32 	%f311, %r270;
	add.f32 	%f312, %f310, %f311;
	mov.b32 	%r271, %f312;
	shfl.sync.bfly.b32 	%r273|%p81, %r271, %r255, %r250, %r252;
	mov.b32 	%f313, %r273;
	add.f32 	%f314, %f312, %f313;
	mov.b32 	%r274, %f314;
	shfl.sync.bfly.b32 	%r276|%p82, %r274, %r258, %r250, %r252;
	mov.b32 	%f315, %r276;
	add.f32 	%f316, %f314, %f315;
	mov.b32 	%r277, %f316;
	shfl.sync.bfly.b32 	%r279|%p83, %r277, %r261, %r250, %r252;
	mov.b32 	%f317, %r279;
	add.f32 	%f318, %f316, %f317;
	mov.b32 	%r280, %f318;
	shfl.sync.bfly.b32 	%r282|%p84, %r280, %r264, %r250, %r252;
	mov.b32 	%f319, %r282;
	add.f32 	%f320, %f318, %f319;
	st.local.f32 	[%rd2+24], %f320;

$L__BB46_23:
	bar.sync 	0;
	setp.gt.s32 	%p85, %r3, 6;
	@%p85 bra 	$L__BB46_25;

	mul.wide.s32 	%rd64, %r3, 4;
	add.s64 	%rd65, %rd2, %rd64;
	ld.local.f32 	%f321, [%rd65];
	mad.lo.s32 	%r283, %r3, %r13, %r2;
	cvt.s64.s32 	%rd66, %r283;
	mul.lo.s32 	%r284, %r1, %r14;
	cvt.s64.s32 	%rd67, %r284;
	add.s64 	%rd68, %rd67, %rd66;
	cvta.to.global.u64 	%rd69, %rd20;
	shl.b64 	%rd70, %rd68, 2;
	add.s64 	%rd71, %rd69, %rd70;
	st.global.f32 	[%rd71], %f321;

$L__BB46_25:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_8_bs_192
.visible .entry ggml_matvec_f32_ncols_8_bs_192(
	.param .u64 ggml_matvec_f32_ncols_8_bs_192_param_0,
	.param .u64 ggml_matvec_f32_ncols_8_bs_192_param_1,
	.param .u64 ggml_matvec_f32_ncols_8_bs_192_param_2,
	.param .u32 ggml_matvec_f32_ncols_8_bs_192_param_3,
	.param .u32 ggml_matvec_f32_ncols_8_bs_192_param_4,
	.param .u32 ggml_matvec_f32_ncols_8_bs_192_param_5,
	.param .u32 ggml_matvec_f32_ncols_8_bs_192_param_6,
	.param .u32 ggml_matvec_f32_ncols_8_bs_192_param_7,
	.param .u32 ggml_matvec_f32_ncols_8_bs_192_param_8,
	.param .u32 ggml_matvec_f32_ncols_8_bs_192_param_9,
	.param .u32 ggml_matvec_f32_ncols_8_bs_192_param_10,
	.param .u32 ggml_matvec_f32_ncols_8_bs_192_param_11
)
{
	.local .align 16 .b8 	__local_depot47[32];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<97>;
	.reg .f32 	%f<390>;
	.reg .b32 	%r<321>;
	.reg .b64 	%rd<78>;


	mov.u64 	%SPL, __local_depot47;
	ld.param.u64 	%rd22, [ggml_matvec_f32_ncols_8_bs_192_param_0];
	ld.param.u64 	%rd23, [ggml_matvec_f32_ncols_8_bs_192_param_1];
	ld.param.u64 	%rd21, [ggml_matvec_f32_ncols_8_bs_192_param_2];
	ld.param.u32 	%r11, [ggml_matvec_f32_ncols_8_bs_192_param_3];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_8_bs_192_param_5];
	ld.param.u32 	%r12, [ggml_matvec_f32_ncols_8_bs_192_param_6];
	ld.param.u32 	%r13, [ggml_matvec_f32_ncols_8_bs_192_param_7];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_8_bs_192_param_8];
	ld.param.u32 	%r17, [ggml_matvec_f32_ncols_8_bs_192_param_9];
	ld.param.u32 	%r18, [ggml_matvec_f32_ncols_8_bs_192_param_10];
	ld.param.u32 	%r14, [ggml_matvec_f32_ncols_8_bs_192_param_11];
	cvta.to.global.u64 	%rd77, %rd23;
	add.u64 	%rd2, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r19, %r1, %r16;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r20, %r2, %r15;
	mad.lo.s32 	%r21, %r19, %r17, %r20;
	cvt.s64.s32 	%rd3, %r21;
	cvta.to.global.u64 	%rd4, %rd22;
	mul.lo.s32 	%r22, %r1, %r18;
	cvt.s64.s32 	%rd5, %r22;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r23, %r3, 2;
	mov.u32 	%r24, data_mmv;
	add.s32 	%r4, %r24, %r23;
	@%p1 bra 	$L__BB47_2;

	mov.u32 	%r25, 0;
	st.shared.u32 	[%r4], %r25;

$L__BB47_2:
	bar.sync 	0;
	mov.f32 	%f382, 0f00000000;
	st.local.v4.f32 	[%rd2], {%f382, %f382, %f382, %f382};
	st.local.v4.f32 	[%rd2+16], {%f382, %f382, %f382, %f382};
	setp.ge.s32 	%p2, %r3, %r11;
	mov.f32 	%f383, %f382;
	mov.f32 	%f384, %f382;
	mov.f32 	%f385, %f382;
	mov.f32 	%f386, %f382;
	mov.f32 	%f387, %f382;
	mov.f32 	%f388, %f382;
	mov.f32 	%f389, %f382;
	@%p2 bra 	$L__BB47_9;

	not.b32 	%r26, %r3;
	add.s32 	%r5, %r26, %r11;
	mul.wide.u32 	%rd25, %r5, -1431655765;
	shr.u64 	%rd26, %rd25, 39;
	and.b64  	%rd27, %rd26, 1;
	setp.eq.b64 	%p3, %rd27, 1;
	mov.pred 	%p4, 0;
	xor.pred  	%p5, %p3, %p4;
	mov.f32 	%f382, 0f00000000;
	mov.u32 	%r320, %r3;
	@%p5 bra 	$L__BB47_5;

	shl.b64 	%rd28, %rd5, 2;
	add.s64 	%rd29, %rd77, %rd28;
	shl.b64 	%rd30, %rd3, 2;
	add.s64 	%rd31, %rd4, %rd30;
	mul.wide.s32 	%rd32, %r3, 8;
	add.s64 	%rd33, %rd31, %rd32;
	ld.global.nc.v2.f32 	{%f57, %f58}, [%rd33];
	add.s64 	%rd34, %rd29, %rd32;
	ld.global.nc.v2.f32 	{%f61, %f62}, [%rd34];
	fma.rn.f32 	%f65, %f57, %f61, 0f00000000;
	fma.rn.f32 	%f389, %f58, %f62, %f65;
	mul.wide.s32 	%rd35, %r12, 8;
	add.s64 	%rd36, %rd34, %rd35;
	ld.global.nc.v2.f32 	{%f66, %f67}, [%rd36];
	fma.rn.f32 	%f70, %f57, %f66, 0f00000000;
	fma.rn.f32 	%f388, %f58, %f67, %f70;
	add.s32 	%r27, %r3, %r12;
	add.s32 	%r28, %r27, %r12;
	mul.wide.s32 	%rd37, %r28, 8;
	add.s64 	%rd38, %rd29, %rd37;
	ld.global.nc.v2.f32 	{%f71, %f72}, [%rd38];
	fma.rn.f32 	%f75, %f57, %f71, 0f00000000;
	fma.rn.f32 	%f387, %f58, %f72, %f75;
	add.s64 	%rd39, %rd38, %rd35;
	ld.global.nc.v2.f32 	{%f76, %f77}, [%rd39];
	fma.rn.f32 	%f80, %f57, %f76, 0f00000000;
	fma.rn.f32 	%f386, %f58, %f77, %f80;
	st.local.v4.f32 	[%rd2], {%f389, %f388, %f387, %f386};
	add.s64 	%rd40, %rd39, %rd35;
	ld.global.nc.v2.f32 	{%f81, %f82}, [%rd40];
	fma.rn.f32 	%f85, %f57, %f81, 0f00000000;
	fma.rn.f32 	%f385, %f58, %f82, %f85;
	add.s64 	%rd41, %rd40, %rd35;
	ld.global.nc.v2.f32 	{%f86, %f87}, [%rd41];
	fma.rn.f32 	%f90, %f57, %f86, 0f00000000;
	fma.rn.f32 	%f384, %f58, %f87, %f90;
	add.s64 	%rd42, %rd41, %rd35;
	ld.global.nc.v2.f32 	{%f91, %f92}, [%rd42];
	fma.rn.f32 	%f95, %f57, %f91, 0f00000000;
	fma.rn.f32 	%f383, %f58, %f92, %f95;
	add.s64 	%rd43, %rd42, %rd35;
	ld.global.nc.v2.f32 	{%f96, %f97}, [%rd43];
	fma.rn.f32 	%f100, %f57, %f96, 0f00000000;
	fma.rn.f32 	%f382, %f58, %f97, %f100;
	st.local.v4.f32 	[%rd2+16], {%f385, %f384, %f383, %f382};
	add.s32 	%r320, %r3, 192;

$L__BB47_5:
	setp.lt.u32 	%p6, %r5, 192;
	@%p6 bra 	$L__BB47_9;

	add.s32 	%r29, %r320, %r12;
	add.s32 	%r30, %r29, 192;
	mul.wide.s32 	%rd44, %r30, 8;
	shl.b64 	%rd45, %rd5, 2;
	add.s64 	%rd7, %rd44, %rd45;
	shl.b32 	%r31, %r12, 1;
	add.s32 	%r32, %r320, %r31;
	mad.lo.s32 	%r33, %r12, 3, %r320;
	shl.b32 	%r34, %r12, 2;
	add.s32 	%r35, %r320, %r34;
	mad.lo.s32 	%r36, %r12, 5, %r320;
	mad.lo.s32 	%r37, %r12, 6, %r320;
	mad.lo.s32 	%r38, %r12, 7, %r320;
	mul.wide.s32 	%rd46, %r32, 8;
	add.s64 	%rd8, %rd46, %rd45;
	mul.wide.s32 	%rd47, %r33, 8;
	add.s64 	%rd9, %rd47, %rd45;
	mul.wide.s32 	%rd48, %r35, 8;
	add.s64 	%rd10, %rd48, %rd45;
	mul.wide.s32 	%rd49, %r36, 8;
	add.s64 	%rd11, %rd49, %rd45;
	mul.wide.s32 	%rd50, %r37, 8;
	add.s64 	%rd12, %rd50, %rd45;
	mul.wide.s32 	%rd51, %r38, 8;
	add.s64 	%rd13, %rd51, %rd45;
	mul.wide.s32 	%rd52, %r320, 2;
	add.s64 	%rd53, %rd52, %rd3;
	shl.b64 	%rd54, %rd53, 2;
	add.s64 	%rd55, %rd4, %rd54;
	add.s64 	%rd76, %rd55, 1536;
	mul.wide.s32 	%rd56, %r320, 8;
	mul.wide.s32 	%rd57, %r12, 8;
	add.s64 	%rd58, %rd56, %rd57;
	add.s64 	%rd15, %rd58, %rd45;
	add.s64 	%rd16, %rd56, %rd45;

$L__BB47_7:
	ld.global.nc.v2.f32 	{%f101, %f102}, [%rd76+-1536];
	add.s64 	%rd59, %rd77, %rd16;
	ld.global.nc.v2.f32 	{%f105, %f106}, [%rd59];
	fma.rn.f32 	%f109, %f101, %f105, %f389;
	fma.rn.f32 	%f110, %f102, %f106, %f109;
	add.s64 	%rd60, %rd77, %rd15;
	ld.global.nc.v2.f32 	{%f111, %f112}, [%rd60];
	fma.rn.f32 	%f115, %f101, %f111, %f388;
	fma.rn.f32 	%f116, %f102, %f112, %f115;
	add.s64 	%rd61, %rd77, %rd8;
	ld.global.nc.v2.f32 	{%f117, %f118}, [%rd61];
	fma.rn.f32 	%f121, %f101, %f117, %f387;
	fma.rn.f32 	%f122, %f102, %f118, %f121;
	add.s64 	%rd62, %rd77, %rd9;
	ld.global.nc.v2.f32 	{%f123, %f124}, [%rd62];
	fma.rn.f32 	%f127, %f101, %f123, %f386;
	fma.rn.f32 	%f128, %f102, %f124, %f127;
	add.s64 	%rd63, %rd77, %rd10;
	ld.global.nc.v2.f32 	{%f129, %f130}, [%rd63];
	fma.rn.f32 	%f133, %f101, %f129, %f385;
	fma.rn.f32 	%f134, %f102, %f130, %f133;
	add.s64 	%rd64, %rd77, %rd11;
	ld.global.nc.v2.f32 	{%f135, %f136}, [%rd64];
	fma.rn.f32 	%f139, %f101, %f135, %f384;
	fma.rn.f32 	%f140, %f102, %f136, %f139;
	add.s64 	%rd65, %rd77, %rd12;
	ld.global.nc.v2.f32 	{%f141, %f142}, [%rd65];
	fma.rn.f32 	%f145, %f101, %f141, %f383;
	fma.rn.f32 	%f146, %f102, %f142, %f145;
	add.s64 	%rd66, %rd77, %rd13;
	ld.global.nc.v2.f32 	{%f147, %f148}, [%rd66];
	fma.rn.f32 	%f151, %f101, %f147, %f382;
	fma.rn.f32 	%f152, %f102, %f148, %f151;
	ld.global.nc.v2.f32 	{%f153, %f154}, [%rd76];
	ld.global.nc.v2.f32 	{%f157, %f158}, [%rd59+1536];
	fma.rn.f32 	%f161, %f153, %f157, %f110;
	fma.rn.f32 	%f389, %f154, %f158, %f161;
	add.s64 	%rd67, %rd77, %rd7;
	ld.global.nc.v2.f32 	{%f162, %f163}, [%rd67];
	fma.rn.f32 	%f166, %f153, %f162, %f116;
	fma.rn.f32 	%f388, %f154, %f163, %f166;
	ld.global.nc.v2.f32 	{%f167, %f168}, [%rd61+1536];
	fma.rn.f32 	%f171, %f153, %f167, %f122;
	fma.rn.f32 	%f387, %f154, %f168, %f171;
	ld.global.nc.v2.f32 	{%f172, %f173}, [%rd62+1536];
	fma.rn.f32 	%f176, %f153, %f172, %f128;
	fma.rn.f32 	%f386, %f154, %f173, %f176;
	ld.global.nc.v2.f32 	{%f177, %f178}, [%rd63+1536];
	fma.rn.f32 	%f181, %f153, %f177, %f134;
	fma.rn.f32 	%f385, %f154, %f178, %f181;
	ld.global.nc.v2.f32 	{%f182, %f183}, [%rd64+1536];
	fma.rn.f32 	%f186, %f153, %f182, %f140;
	fma.rn.f32 	%f384, %f154, %f183, %f186;
	ld.global.nc.v2.f32 	{%f187, %f188}, [%rd65+1536];
	fma.rn.f32 	%f191, %f153, %f187, %f146;
	fma.rn.f32 	%f383, %f154, %f188, %f191;
	ld.global.nc.v2.f32 	{%f192, %f193}, [%rd66+1536];
	fma.rn.f32 	%f196, %f153, %f192, %f152;
	fma.rn.f32 	%f382, %f154, %f193, %f196;
	add.s64 	%rd77, %rd77, 3072;
	add.s64 	%rd76, %rd76, 3072;
	add.s32 	%r320, %r320, 384;
	setp.lt.s32 	%p7, %r320, %r11;
	@%p7 bra 	$L__BB47_7;

	st.local.v4.f32 	[%rd2], {%f389, %f388, %f387, %f386};
	st.local.v4.f32 	[%rd2+16], {%f385, %f384, %f383, %f382};

$L__BB47_9:
	shr.s32 	%r39, %r3, 31;
	shr.u32 	%r40, %r39, 27;
	add.s32 	%r41, %r3, %r40;
	shr.s32 	%r42, %r41, 5;
	shl.b32 	%r43, %r42, 2;
	add.s32 	%r10, %r24, %r43;
	mov.u32 	%r45, 2;
	mov.b32 	%r46, %f389;
	mov.u32 	%r47, 31;
	mov.u32 	%r48, 16;
	mov.u32 	%r49, -1;
	shfl.sync.bfly.b32 	%r50|%p8, %r46, %r48, %r47, %r49;
	mov.b32 	%f197, %r50;
	add.f32 	%f198, %f389, %f197;
	mov.b32 	%r51, %f198;
	mov.u32 	%r52, 8;
	shfl.sync.bfly.b32 	%r53|%p9, %r51, %r52, %r47, %r49;
	mov.b32 	%f199, %r53;
	add.f32 	%f200, %f198, %f199;
	mov.b32 	%r54, %f200;
	mov.u32 	%r55, 4;
	shfl.sync.bfly.b32 	%r56|%p10, %r54, %r55, %r47, %r49;
	mov.b32 	%f201, %r56;
	add.f32 	%f202, %f200, %f201;
	mov.b32 	%r57, %f202;
	shfl.sync.bfly.b32 	%r58|%p11, %r57, %r45, %r47, %r49;
	mov.b32 	%f203, %r58;
	add.f32 	%f204, %f202, %f203;
	mov.b32 	%r59, %f204;
	mov.u32 	%r60, 1;
	shfl.sync.bfly.b32 	%r61|%p12, %r59, %r60, %r47, %r49;
	mov.b32 	%f205, %r61;
	add.f32 	%f206, %f204, %f205;
	st.local.f32 	[%rd2], %f206;
	st.shared.f32 	[%r10], %f206;
	bar.sync 	0;
	@%p1 bra 	$L__BB47_11;

	ld.shared.f32 	%f207, [%r4];
	mov.b32 	%r62, %f207;
	shfl.sync.bfly.b32 	%r66|%p14, %r62, %r48, %r47, %r49;
	mov.b32 	%f208, %r66;
	add.f32 	%f209, %f207, %f208;
	mov.b32 	%r67, %f209;
	shfl.sync.bfly.b32 	%r69|%p15, %r67, %r52, %r47, %r49;
	mov.b32 	%f210, %r69;
	add.f32 	%f211, %f209, %f210;
	mov.b32 	%r70, %f211;
	shfl.sync.bfly.b32 	%r72|%p16, %r70, %r55, %r47, %r49;
	mov.b32 	%f212, %r72;
	add.f32 	%f213, %f211, %f212;
	mov.b32 	%r73, %f213;
	shfl.sync.bfly.b32 	%r75|%p17, %r73, %r45, %r47, %r49;
	mov.b32 	%f214, %r75;
	add.f32 	%f215, %f213, %f214;
	mov.b32 	%r76, %f215;
	shfl.sync.bfly.b32 	%r78|%p18, %r76, %r60, %r47, %r49;
	mov.b32 	%f216, %r78;
	add.f32 	%f217, %f215, %f216;
	st.local.f32 	[%rd2], %f217;

$L__BB47_11:
	bar.sync 	0;
	mov.b32 	%r79, %f388;
	shfl.sync.bfly.b32 	%r83|%p20, %r79, %r48, %r47, %r49;
	mov.b32 	%f218, %r83;
	add.f32 	%f219, %f388, %f218;
	mov.b32 	%r84, %f219;
	shfl.sync.bfly.b32 	%r86|%p21, %r84, %r52, %r47, %r49;
	mov.b32 	%f220, %r86;
	add.f32 	%f221, %f219, %f220;
	mov.b32 	%r87, %f221;
	shfl.sync.bfly.b32 	%r89|%p22, %r87, %r55, %r47, %r49;
	mov.b32 	%f222, %r89;
	add.f32 	%f223, %f221, %f222;
	mov.b32 	%r90, %f223;
	shfl.sync.bfly.b32 	%r92|%p23, %r90, %r45, %r47, %r49;
	mov.b32 	%f224, %r92;
	add.f32 	%f225, %f223, %f224;
	mov.b32 	%r93, %f225;
	shfl.sync.bfly.b32 	%r95|%p24, %r93, %r60, %r47, %r49;
	mov.b32 	%f226, %r95;
	add.f32 	%f227, %f225, %f226;
	st.local.f32 	[%rd2+4], %f227;
	st.shared.f32 	[%r10], %f227;
	bar.sync 	0;
	@%p1 bra 	$L__BB47_13;

	ld.shared.f32 	%f228, [%r4];
	mov.b32 	%r96, %f228;
	mov.u32 	%r97, 31;
	mov.u32 	%r98, 16;
	mov.u32 	%r99, -1;
	shfl.sync.bfly.b32 	%r100|%p25, %r96, %r98, %r97, %r99;
	mov.b32 	%f229, %r100;
	add.f32 	%f230, %f228, %f229;
	mov.b32 	%r101, %f230;
	mov.u32 	%r102, 8;
	shfl.sync.bfly.b32 	%r103|%p26, %r101, %r102, %r97, %r99;
	mov.b32 	%f231, %r103;
	add.f32 	%f232, %f230, %f231;
	mov.b32 	%r104, %f232;
	mov.u32 	%r105, 4;
	shfl.sync.bfly.b32 	%r106|%p27, %r104, %r105, %r97, %r99;
	mov.b32 	%f233, %r106;
	add.f32 	%f234, %f232, %f233;
	mov.b32 	%r107, %f234;
	mov.u32 	%r108, 2;
	shfl.sync.bfly.b32 	%r109|%p28, %r107, %r108, %r97, %r99;
	mov.b32 	%f235, %r109;
	add.f32 	%f236, %f234, %f235;
	mov.b32 	%r110, %f236;
	mov.u32 	%r111, 1;
	shfl.sync.bfly.b32 	%r112|%p29, %r110, %r111, %r97, %r99;
	mov.b32 	%f237, %r112;
	add.f32 	%f238, %f236, %f237;
	st.local.f32 	[%rd2+4], %f238;

$L__BB47_13:
	bar.sync 	0;
	mov.b32 	%r113, %f387;
	mov.u32 	%r114, 31;
	mov.u32 	%r115, 16;
	mov.u32 	%r116, -1;
	shfl.sync.bfly.b32 	%r117|%p31, %r113, %r115, %r114, %r116;
	mov.b32 	%f239, %r117;
	add.f32 	%f240, %f387, %f239;
	mov.b32 	%r118, %f240;
	mov.u32 	%r119, 8;
	shfl.sync.bfly.b32 	%r120|%p32, %r118, %r119, %r114, %r116;
	mov.b32 	%f241, %r120;
	add.f32 	%f242, %f240, %f241;
	mov.b32 	%r121, %f242;
	mov.u32 	%r122, 4;
	shfl.sync.bfly.b32 	%r123|%p33, %r121, %r122, %r114, %r116;
	mov.b32 	%f243, %r123;
	add.f32 	%f244, %f242, %f243;
	mov.b32 	%r124, %f244;
	mov.u32 	%r125, 2;
	shfl.sync.bfly.b32 	%r126|%p34, %r124, %r125, %r114, %r116;
	mov.b32 	%f245, %r126;
	add.f32 	%f246, %f244, %f245;
	mov.b32 	%r127, %f246;
	mov.u32 	%r128, 1;
	shfl.sync.bfly.b32 	%r129|%p35, %r127, %r128, %r114, %r116;
	mov.b32 	%f247, %r129;
	add.f32 	%f248, %f246, %f247;
	st.local.f32 	[%rd2+8], %f248;
	st.shared.f32 	[%r10], %f248;
	bar.sync 	0;
	@%p1 bra 	$L__BB47_15;

	ld.shared.f32 	%f249, [%r4];
	mov.b32 	%r130, %f249;
	shfl.sync.bfly.b32 	%r134|%p36, %r130, %r115, %r114, %r116;
	mov.b32 	%f250, %r134;
	add.f32 	%f251, %f249, %f250;
	mov.b32 	%r135, %f251;
	shfl.sync.bfly.b32 	%r137|%p37, %r135, %r119, %r114, %r116;
	mov.b32 	%f252, %r137;
	add.f32 	%f253, %f251, %f252;
	mov.b32 	%r138, %f253;
	shfl.sync.bfly.b32 	%r140|%p38, %r138, %r122, %r114, %r116;
	mov.b32 	%f254, %r140;
	add.f32 	%f255, %f253, %f254;
	mov.b32 	%r141, %f255;
	shfl.sync.bfly.b32 	%r143|%p39, %r141, %r125, %r114, %r116;
	mov.b32 	%f256, %r143;
	add.f32 	%f257, %f255, %f256;
	mov.b32 	%r144, %f257;
	shfl.sync.bfly.b32 	%r146|%p40, %r144, %r128, %r114, %r116;
	mov.b32 	%f258, %r146;
	add.f32 	%f259, %f257, %f258;
	st.local.f32 	[%rd2+8], %f259;

$L__BB47_15:
	bar.sync 	0;
	mov.b32 	%r147, %f386;
	shfl.sync.bfly.b32 	%r151|%p42, %r147, %r115, %r114, %r116;
	mov.b32 	%f260, %r151;
	add.f32 	%f261, %f386, %f260;
	mov.b32 	%r152, %f261;
	shfl.sync.bfly.b32 	%r154|%p43, %r152, %r119, %r114, %r116;
	mov.b32 	%f262, %r154;
	add.f32 	%f263, %f261, %f262;
	mov.b32 	%r155, %f263;
	shfl.sync.bfly.b32 	%r157|%p44, %r155, %r122, %r114, %r116;
	mov.b32 	%f264, %r157;
	add.f32 	%f265, %f263, %f264;
	mov.b32 	%r158, %f265;
	shfl.sync.bfly.b32 	%r160|%p45, %r158, %r125, %r114, %r116;
	mov.b32 	%f266, %r160;
	add.f32 	%f267, %f265, %f266;
	mov.b32 	%r161, %f267;
	shfl.sync.bfly.b32 	%r163|%p46, %r161, %r128, %r114, %r116;
	mov.b32 	%f268, %r163;
	add.f32 	%f269, %f267, %f268;
	st.local.f32 	[%rd2+12], %f269;
	st.shared.f32 	[%r10], %f269;
	bar.sync 	0;
	@%p1 bra 	$L__BB47_17;

	ld.shared.f32 	%f270, [%r4];
	mov.b32 	%r164, %f270;
	mov.u32 	%r165, 31;
	mov.u32 	%r166, 16;
	mov.u32 	%r167, -1;
	shfl.sync.bfly.b32 	%r168|%p47, %r164, %r166, %r165, %r167;
	mov.b32 	%f271, %r168;
	add.f32 	%f272, %f270, %f271;
	mov.b32 	%r169, %f272;
	mov.u32 	%r170, 8;
	shfl.sync.bfly.b32 	%r171|%p48, %r169, %r170, %r165, %r167;
	mov.b32 	%f273, %r171;
	add.f32 	%f274, %f272, %f273;
	mov.b32 	%r172, %f274;
	mov.u32 	%r173, 4;
	shfl.sync.bfly.b32 	%r174|%p49, %r172, %r173, %r165, %r167;
	mov.b32 	%f275, %r174;
	add.f32 	%f276, %f274, %f275;
	mov.b32 	%r175, %f276;
	mov.u32 	%r176, 2;
	shfl.sync.bfly.b32 	%r177|%p50, %r175, %r176, %r165, %r167;
	mov.b32 	%f277, %r177;
	add.f32 	%f278, %f276, %f277;
	mov.b32 	%r178, %f278;
	mov.u32 	%r179, 1;
	shfl.sync.bfly.b32 	%r180|%p51, %r178, %r179, %r165, %r167;
	mov.b32 	%f279, %r180;
	add.f32 	%f280, %f278, %f279;
	st.local.f32 	[%rd2+12], %f280;

$L__BB47_17:
	bar.sync 	0;
	mov.b32 	%r181, %f385;
	mov.u32 	%r182, 31;
	mov.u32 	%r183, 16;
	mov.u32 	%r184, -1;
	shfl.sync.bfly.b32 	%r185|%p53, %r181, %r183, %r182, %r184;
	mov.b32 	%f281, %r185;
	add.f32 	%f282, %f385, %f281;
	mov.b32 	%r186, %f282;
	mov.u32 	%r187, 8;
	shfl.sync.bfly.b32 	%r188|%p54, %r186, %r187, %r182, %r184;
	mov.b32 	%f283, %r188;
	add.f32 	%f284, %f282, %f283;
	mov.b32 	%r189, %f284;
	mov.u32 	%r190, 4;
	shfl.sync.bfly.b32 	%r191|%p55, %r189, %r190, %r182, %r184;
	mov.b32 	%f285, %r191;
	add.f32 	%f286, %f284, %f285;
	mov.b32 	%r192, %f286;
	mov.u32 	%r193, 2;
	shfl.sync.bfly.b32 	%r194|%p56, %r192, %r193, %r182, %r184;
	mov.b32 	%f287, %r194;
	add.f32 	%f288, %f286, %f287;
	mov.b32 	%r195, %f288;
	mov.u32 	%r196, 1;
	shfl.sync.bfly.b32 	%r197|%p57, %r195, %r196, %r182, %r184;
	mov.b32 	%f289, %r197;
	add.f32 	%f290, %f288, %f289;
	st.local.f32 	[%rd2+16], %f290;
	st.shared.f32 	[%r10], %f290;
	bar.sync 	0;
	@%p1 bra 	$L__BB47_19;

	ld.shared.f32 	%f291, [%r4];
	mov.b32 	%r198, %f291;
	shfl.sync.bfly.b32 	%r202|%p58, %r198, %r183, %r182, %r184;
	mov.b32 	%f292, %r202;
	add.f32 	%f293, %f291, %f292;
	mov.b32 	%r203, %f293;
	shfl.sync.bfly.b32 	%r205|%p59, %r203, %r187, %r182, %r184;
	mov.b32 	%f294, %r205;
	add.f32 	%f295, %f293, %f294;
	mov.b32 	%r206, %f295;
	shfl.sync.bfly.b32 	%r208|%p60, %r206, %r190, %r182, %r184;
	mov.b32 	%f296, %r208;
	add.f32 	%f297, %f295, %f296;
	mov.b32 	%r209, %f297;
	shfl.sync.bfly.b32 	%r211|%p61, %r209, %r193, %r182, %r184;
	mov.b32 	%f298, %r211;
	add.f32 	%f299, %f297, %f298;
	mov.b32 	%r212, %f299;
	shfl.sync.bfly.b32 	%r214|%p62, %r212, %r196, %r182, %r184;
	mov.b32 	%f300, %r214;
	add.f32 	%f301, %f299, %f300;
	st.local.f32 	[%rd2+16], %f301;

$L__BB47_19:
	bar.sync 	0;
	mov.b32 	%r215, %f384;
	shfl.sync.bfly.b32 	%r219|%p64, %r215, %r183, %r182, %r184;
	mov.b32 	%f302, %r219;
	add.f32 	%f303, %f384, %f302;
	mov.b32 	%r220, %f303;
	shfl.sync.bfly.b32 	%r222|%p65, %r220, %r187, %r182, %r184;
	mov.b32 	%f304, %r222;
	add.f32 	%f305, %f303, %f304;
	mov.b32 	%r223, %f305;
	shfl.sync.bfly.b32 	%r225|%p66, %r223, %r190, %r182, %r184;
	mov.b32 	%f306, %r225;
	add.f32 	%f307, %f305, %f306;
	mov.b32 	%r226, %f307;
	shfl.sync.bfly.b32 	%r228|%p67, %r226, %r193, %r182, %r184;
	mov.b32 	%f308, %r228;
	add.f32 	%f309, %f307, %f308;
	mov.b32 	%r229, %f309;
	shfl.sync.bfly.b32 	%r231|%p68, %r229, %r196, %r182, %r184;
	mov.b32 	%f310, %r231;
	add.f32 	%f311, %f309, %f310;
	st.local.f32 	[%rd2+20], %f311;
	st.shared.f32 	[%r10], %f311;
	bar.sync 	0;
	@%p1 bra 	$L__BB47_21;

	ld.shared.f32 	%f312, [%r4];
	mov.b32 	%r232, %f312;
	mov.u32 	%r233, 31;
	mov.u32 	%r234, 16;
	mov.u32 	%r235, -1;
	shfl.sync.bfly.b32 	%r236|%p69, %r232, %r234, %r233, %r235;
	mov.b32 	%f313, %r236;
	add.f32 	%f314, %f312, %f313;
	mov.b32 	%r237, %f314;
	mov.u32 	%r238, 8;
	shfl.sync.bfly.b32 	%r239|%p70, %r237, %r238, %r233, %r235;
	mov.b32 	%f315, %r239;
	add.f32 	%f316, %f314, %f315;
	mov.b32 	%r240, %f316;
	mov.u32 	%r241, 4;
	shfl.sync.bfly.b32 	%r242|%p71, %r240, %r241, %r233, %r235;
	mov.b32 	%f317, %r242;
	add.f32 	%f318, %f316, %f317;
	mov.b32 	%r243, %f318;
	mov.u32 	%r244, 2;
	shfl.sync.bfly.b32 	%r245|%p72, %r243, %r244, %r233, %r235;
	mov.b32 	%f319, %r245;
	add.f32 	%f320, %f318, %f319;
	mov.b32 	%r246, %f320;
	mov.u32 	%r247, 1;
	shfl.sync.bfly.b32 	%r248|%p73, %r246, %r247, %r233, %r235;
	mov.b32 	%f321, %r248;
	add.f32 	%f322, %f320, %f321;
	st.local.f32 	[%rd2+20], %f322;

$L__BB47_21:
	bar.sync 	0;
	mov.b32 	%r249, %f383;
	mov.u32 	%r250, 31;
	mov.u32 	%r251, 16;
	mov.u32 	%r252, -1;
	shfl.sync.bfly.b32 	%r253|%p75, %r249, %r251, %r250, %r252;
	mov.b32 	%f323, %r253;
	add.f32 	%f324, %f383, %f323;
	mov.b32 	%r254, %f324;
	mov.u32 	%r255, 8;
	shfl.sync.bfly.b32 	%r256|%p76, %r254, %r255, %r250, %r252;
	mov.b32 	%f325, %r256;
	add.f32 	%f326, %f324, %f325;
	mov.b32 	%r257, %f326;
	mov.u32 	%r258, 4;
	shfl.sync.bfly.b32 	%r259|%p77, %r257, %r258, %r250, %r252;
	mov.b32 	%f327, %r259;
	add.f32 	%f328, %f326, %f327;
	mov.b32 	%r260, %f328;
	mov.u32 	%r261, 2;
	shfl.sync.bfly.b32 	%r262|%p78, %r260, %r261, %r250, %r252;
	mov.b32 	%f329, %r262;
	add.f32 	%f330, %f328, %f329;
	mov.b32 	%r263, %f330;
	mov.u32 	%r264, 1;
	shfl.sync.bfly.b32 	%r265|%p79, %r263, %r264, %r250, %r252;
	mov.b32 	%f331, %r265;
	add.f32 	%f332, %f330, %f331;
	st.local.f32 	[%rd2+24], %f332;
	st.shared.f32 	[%r10], %f332;
	bar.sync 	0;
	@%p1 bra 	$L__BB47_23;

	ld.shared.f32 	%f333, [%r4];
	mov.b32 	%r266, %f333;
	shfl.sync.bfly.b32 	%r270|%p80, %r266, %r251, %r250, %r252;
	mov.b32 	%f334, %r270;
	add.f32 	%f335, %f333, %f334;
	mov.b32 	%r271, %f335;
	shfl.sync.bfly.b32 	%r273|%p81, %r271, %r255, %r250, %r252;
	mov.b32 	%f336, %r273;
	add.f32 	%f337, %f335, %f336;
	mov.b32 	%r274, %f337;
	shfl.sync.bfly.b32 	%r276|%p82, %r274, %r258, %r250, %r252;
	mov.b32 	%f338, %r276;
	add.f32 	%f339, %f337, %f338;
	mov.b32 	%r277, %f339;
	shfl.sync.bfly.b32 	%r279|%p83, %r277, %r261, %r250, %r252;
	mov.b32 	%f340, %r279;
	add.f32 	%f341, %f339, %f340;
	mov.b32 	%r280, %f341;
	shfl.sync.bfly.b32 	%r282|%p84, %r280, %r264, %r250, %r252;
	mov.b32 	%f342, %r282;
	add.f32 	%f343, %f341, %f342;
	st.local.f32 	[%rd2+24], %f343;

$L__BB47_23:
	bar.sync 	0;
	mov.b32 	%r283, %f382;
	shfl.sync.bfly.b32 	%r287|%p86, %r283, %r251, %r250, %r252;
	mov.b32 	%f344, %r287;
	add.f32 	%f345, %f382, %f344;
	mov.b32 	%r288, %f345;
	shfl.sync.bfly.b32 	%r290|%p87, %r288, %r255, %r250, %r252;
	mov.b32 	%f346, %r290;
	add.f32 	%f347, %f345, %f346;
	mov.b32 	%r291, %f347;
	shfl.sync.bfly.b32 	%r293|%p88, %r291, %r258, %r250, %r252;
	mov.b32 	%f348, %r293;
	add.f32 	%f349, %f347, %f348;
	mov.b32 	%r294, %f349;
	shfl.sync.bfly.b32 	%r296|%p89, %r294, %r261, %r250, %r252;
	mov.b32 	%f350, %r296;
	add.f32 	%f351, %f349, %f350;
	mov.b32 	%r297, %f351;
	shfl.sync.bfly.b32 	%r299|%p90, %r297, %r264, %r250, %r252;
	mov.b32 	%f352, %r299;
	add.f32 	%f353, %f351, %f352;
	st.local.f32 	[%rd2+28], %f353;
	st.shared.f32 	[%r10], %f353;
	bar.sync 	0;
	@%p1 bra 	$L__BB47_25;

	ld.shared.f32 	%f354, [%r4];
	mov.b32 	%r300, %f354;
	mov.u32 	%r301, 31;
	mov.u32 	%r302, 16;
	mov.u32 	%r303, -1;
	shfl.sync.bfly.b32 	%r304|%p91, %r300, %r302, %r301, %r303;
	mov.b32 	%f355, %r304;
	add.f32 	%f356, %f354, %f355;
	mov.b32 	%r305, %f356;
	mov.u32 	%r306, 8;
	shfl.sync.bfly.b32 	%r307|%p92, %r305, %r306, %r301, %r303;
	mov.b32 	%f357, %r307;
	add.f32 	%f358, %f356, %f357;
	mov.b32 	%r308, %f358;
	mov.u32 	%r309, 4;
	shfl.sync.bfly.b32 	%r310|%p93, %r308, %r309, %r301, %r303;
	mov.b32 	%f359, %r310;
	add.f32 	%f360, %f358, %f359;
	mov.b32 	%r311, %f360;
	mov.u32 	%r312, 2;
	shfl.sync.bfly.b32 	%r313|%p94, %r311, %r312, %r301, %r303;
	mov.b32 	%f361, %r313;
	add.f32 	%f362, %f360, %f361;
	mov.b32 	%r314, %f362;
	mov.u32 	%r315, 1;
	shfl.sync.bfly.b32 	%r316|%p95, %r314, %r315, %r301, %r303;
	mov.b32 	%f363, %r316;
	add.f32 	%f364, %f362, %f363;
	st.local.f32 	[%rd2+28], %f364;

$L__BB47_25:
	bar.sync 	0;
	setp.gt.s32 	%p96, %r3, 7;
	@%p96 bra 	$L__BB47_27;

	mul.wide.s32 	%rd68, %r3, 4;
	add.s64 	%rd69, %rd2, %rd68;
	ld.local.f32 	%f365, [%rd69];
	mad.lo.s32 	%r317, %r3, %r13, %r2;
	cvt.s64.s32 	%rd70, %r317;
	mul.lo.s32 	%r318, %r1, %r14;
	cvt.s64.s32 	%rd71, %r318;
	add.s64 	%rd72, %rd71, %rd70;
	cvta.to.global.u64 	%rd73, %rd21;
	shl.b64 	%rd74, %rd72, 2;
	add.s64 	%rd75, %rd73, %rd74;
	st.global.f32 	[%rd75], %f365;

$L__BB47_27:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_1_bs_224
.visible .entry ggml_matvec_f32_ncols_1_bs_224(
	.param .u64 ggml_matvec_f32_ncols_1_bs_224_param_0,
	.param .u64 ggml_matvec_f32_ncols_1_bs_224_param_1,
	.param .u64 ggml_matvec_f32_ncols_1_bs_224_param_2,
	.param .u32 ggml_matvec_f32_ncols_1_bs_224_param_3,
	.param .u32 ggml_matvec_f32_ncols_1_bs_224_param_4,
	.param .u32 ggml_matvec_f32_ncols_1_bs_224_param_5,
	.param .u32 ggml_matvec_f32_ncols_1_bs_224_param_6,
	.param .u32 ggml_matvec_f32_ncols_1_bs_224_param_7,
	.param .u32 ggml_matvec_f32_ncols_1_bs_224_param_8,
	.param .u32 ggml_matvec_f32_ncols_1_bs_224_param_9,
	.param .u32 ggml_matvec_f32_ncols_1_bs_224_param_10,
	.param .u32 ggml_matvec_f32_ncols_1_bs_224_param_11
)
{
	.reg .pred 	%p<19>;
	.reg .f32 	%f<88>;
	.reg .b32 	%r<80>;
	.reg .b64 	%rd<44>;


	ld.param.u64 	%rd18, [ggml_matvec_f32_ncols_1_bs_224_param_0];
	ld.param.u64 	%rd19, [ggml_matvec_f32_ncols_1_bs_224_param_1];
	ld.param.u64 	%rd17, [ggml_matvec_f32_ncols_1_bs_224_param_2];
	ld.param.u32 	%r13, [ggml_matvec_f32_ncols_1_bs_224_param_3];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_1_bs_224_param_5];
	ld.param.u32 	%r14, [ggml_matvec_f32_ncols_1_bs_224_param_7];
	ld.param.u32 	%r17, [ggml_matvec_f32_ncols_1_bs_224_param_8];
	ld.param.u32 	%r18, [ggml_matvec_f32_ncols_1_bs_224_param_9];
	ld.param.u32 	%r19, [ggml_matvec_f32_ncols_1_bs_224_param_10];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_1_bs_224_param_11];
	cvta.to.global.u64 	%rd1, %rd19;
	cvta.to.global.u64 	%rd2, %rd18;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r20, %r1, %r17;
	mov.u32 	%r21, %ctaid.x;
	mul.lo.s32 	%r22, %r21, %r16;
	mad.lo.s32 	%r23, %r20, %r18, %r22;
	cvt.s64.s32 	%rd3, %r23;
	mul.lo.s32 	%r24, %r1, %r19;
	cvt.s64.s32 	%rd4, %r24;
	mov.u32 	%r2, %tid.x;
	setp.gt.s32 	%p1, %r2, 31;
	shl.b32 	%r25, %r2, 2;
	mov.u32 	%r26, data_mmv;
	add.s32 	%r3, %r26, %r25;
	@%p1 bra 	$L__BB48_2;

	mov.u32 	%r27, 0;
	st.shared.u32 	[%r3], %r27;

$L__BB48_2:
	bar.sync 	0;
	setp.ge.s32 	%p2, %r2, %r13;
	mov.f32 	%f86, 0f00000000;
	@%p2 bra 	$L__BB48_9;

	not.b32 	%r28, %r2;
	add.s32 	%r4, %r28, %r13;
	shr.u32 	%r29, %r4, 5;
	mul.wide.u32 	%rd20, %r29, 613566757;
	shr.u64 	%rd21, %rd20, 32;
	cvt.u32.u64 	%r30, %rd21;
	add.s32 	%r31, %r30, 1;
	and.b32  	%r77, %r31, 3;
	setp.eq.s32 	%p3, %r77, 0;
	mov.f32 	%f86, 0f00000000;
	mov.u32 	%r78, %r2;
	@%p3 bra 	$L__BB48_6;

	mul.wide.s32 	%rd22, %r2, 2;
	add.s64 	%rd23, %rd22, %rd4;
	shl.b64 	%rd24, %rd23, 2;
	add.s64 	%rd41, %rd1, %rd24;
	add.s64 	%rd25, %rd22, %rd3;
	shl.b64 	%rd26, %rd25, 2;
	add.s64 	%rd40, %rd2, %rd26;
	mov.f32 	%f86, 0f00000000;
	mov.u32 	%r78, %r2;

$L__BB48_5:
	.pragma "nounroll";
	ld.global.nc.v2.f32 	{%f15, %f16}, [%rd40];
	ld.global.nc.v2.f32 	{%f19, %f20}, [%rd41];
	fma.rn.f32 	%f23, %f15, %f19, %f86;
	fma.rn.f32 	%f86, %f16, %f20, %f23;
	add.s32 	%r78, %r78, 224;
	add.s64 	%rd41, %rd41, 1792;
	add.s64 	%rd40, %rd40, 1792;
	add.s32 	%r77, %r77, -1;
	setp.ne.s32 	%p4, %r77, 0;
	@%p4 bra 	$L__BB48_5;

$L__BB48_6:
	setp.lt.u32 	%p5, %r4, 672;
	@%p5 bra 	$L__BB48_9;

	mul.wide.s32 	%rd27, %r78, 2;
	add.s64 	%rd28, %rd27, %rd3;
	shl.b64 	%rd29, %rd28, 2;
	add.s64 	%rd30, %rd2, %rd29;
	add.s64 	%rd43, %rd30, 3584;
	add.s64 	%rd31, %rd27, %rd4;
	shl.b64 	%rd32, %rd31, 2;
	add.s64 	%rd33, %rd1, %rd32;
	add.s64 	%rd42, %rd33, 3584;

$L__BB48_8:
	ld.global.nc.v2.f32 	{%f24, %f25}, [%rd43+-3584];
	ld.global.nc.v2.f32 	{%f28, %f29}, [%rd42+-3584];
	fma.rn.f32 	%f32, %f24, %f28, %f86;
	fma.rn.f32 	%f33, %f25, %f29, %f32;
	ld.global.nc.v2.f32 	{%f34, %f35}, [%rd43+-1792];
	ld.global.nc.v2.f32 	{%f38, %f39}, [%rd42+-1792];
	fma.rn.f32 	%f42, %f34, %f38, %f33;
	fma.rn.f32 	%f43, %f35, %f39, %f42;
	ld.global.nc.v2.f32 	{%f44, %f45}, [%rd43];
	ld.global.nc.v2.f32 	{%f48, %f49}, [%rd42];
	fma.rn.f32 	%f52, %f44, %f48, %f43;
	fma.rn.f32 	%f53, %f45, %f49, %f52;
	ld.global.nc.v2.f32 	{%f54, %f55}, [%rd43+1792];
	ld.global.nc.v2.f32 	{%f58, %f59}, [%rd42+1792];
	fma.rn.f32 	%f62, %f54, %f58, %f53;
	fma.rn.f32 	%f86, %f55, %f59, %f62;
	add.s64 	%rd43, %rd43, 7168;
	add.s64 	%rd42, %rd42, 7168;
	add.s32 	%r78, %r78, 896;
	setp.lt.s32 	%p6, %r78, %r13;
	@%p6 bra 	$L__BB48_8;

$L__BB48_9:
	mov.b32 	%r32, %f86;
	mov.u32 	%r33, 31;
	mov.u32 	%r34, 16;
	mov.u32 	%r35, -1;
	shfl.sync.bfly.b32 	%r36|%p7, %r32, %r34, %r33, %r35;
	mov.b32 	%f63, %r36;
	add.f32 	%f64, %f86, %f63;
	mov.b32 	%r37, %f64;
	mov.u32 	%r38, 8;
	shfl.sync.bfly.b32 	%r39|%p8, %r37, %r38, %r33, %r35;
	mov.b32 	%f65, %r39;
	add.f32 	%f66, %f64, %f65;
	mov.b32 	%r40, %f66;
	mov.u32 	%r41, 4;
	shfl.sync.bfly.b32 	%r42|%p9, %r40, %r41, %r33, %r35;
	mov.b32 	%f67, %r42;
	add.f32 	%f68, %f66, %f67;
	mov.b32 	%r43, %f68;
	mov.u32 	%r44, 2;
	shfl.sync.bfly.b32 	%r45|%p10, %r43, %r44, %r33, %r35;
	mov.b32 	%f69, %r45;
	add.f32 	%f70, %f68, %f69;
	mov.b32 	%r46, %f70;
	mov.u32 	%r47, 1;
	shfl.sync.bfly.b32 	%r48|%p11, %r46, %r47, %r33, %r35;
	mov.b32 	%f71, %r48;
	add.f32 	%f87, %f70, %f71;
	shr.s32 	%r49, %r2, 31;
	shr.u32 	%r50, %r49, 27;
	add.s32 	%r51, %r2, %r50;
	shr.s32 	%r52, %r51, 5;
	shl.b32 	%r53, %r52, 2;
	add.s32 	%r55, %r26, %r53;
	st.shared.f32 	[%r55], %f87;
	bar.sync 	0;
	@%p1 bra 	$L__BB48_11;

	ld.shared.f32 	%f72, [%r3];
	mov.b32 	%r56, %f72;
	shfl.sync.bfly.b32 	%r60|%p13, %r56, %r34, %r33, %r35;
	mov.b32 	%f73, %r60;
	add.f32 	%f74, %f72, %f73;
	mov.b32 	%r61, %f74;
	shfl.sync.bfly.b32 	%r63|%p14, %r61, %r38, %r33, %r35;
	mov.b32 	%f75, %r63;
	add.f32 	%f76, %f74, %f75;
	mov.b32 	%r64, %f76;
	shfl.sync.bfly.b32 	%r66|%p15, %r64, %r41, %r33, %r35;
	mov.b32 	%f77, %r66;
	add.f32 	%f78, %f76, %f77;
	mov.b32 	%r67, %f78;
	shfl.sync.bfly.b32 	%r69|%p16, %r67, %r44, %r33, %r35;
	mov.b32 	%f79, %r69;
	add.f32 	%f80, %f78, %f79;
	mov.b32 	%r70, %f80;
	shfl.sync.bfly.b32 	%r72|%p17, %r70, %r47, %r33, %r35;
	mov.b32 	%f81, %r72;
	add.f32 	%f87, %f80, %f81;

$L__BB48_11:
	bar.sync 	0;
	setp.gt.s32 	%p18, %r2, 0;
	@%p18 bra 	$L__BB48_13;

	mad.lo.s32 	%r74, %r2, %r14, %r21;
	cvt.s64.s32 	%rd34, %r74;
	mul.lo.s32 	%r75, %r1, %r15;
	cvt.s64.s32 	%rd35, %r75;
	add.s64 	%rd36, %rd35, %rd34;
	cvta.to.global.u64 	%rd37, %rd17;
	shl.b64 	%rd38, %rd36, 2;
	add.s64 	%rd39, %rd37, %rd38;
	st.global.f32 	[%rd39], %f87;

$L__BB48_13:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_2_bs_224
.visible .entry ggml_matvec_f32_ncols_2_bs_224(
	.param .u64 ggml_matvec_f32_ncols_2_bs_224_param_0,
	.param .u64 ggml_matvec_f32_ncols_2_bs_224_param_1,
	.param .u64 ggml_matvec_f32_ncols_2_bs_224_param_2,
	.param .u32 ggml_matvec_f32_ncols_2_bs_224_param_3,
	.param .u32 ggml_matvec_f32_ncols_2_bs_224_param_4,
	.param .u32 ggml_matvec_f32_ncols_2_bs_224_param_5,
	.param .u32 ggml_matvec_f32_ncols_2_bs_224_param_6,
	.param .u32 ggml_matvec_f32_ncols_2_bs_224_param_7,
	.param .u32 ggml_matvec_f32_ncols_2_bs_224_param_8,
	.param .u32 ggml_matvec_f32_ncols_2_bs_224_param_9,
	.param .u32 ggml_matvec_f32_ncols_2_bs_224_param_10,
	.param .u32 ggml_matvec_f32_ncols_2_bs_224_param_11
)
{
	.local .align 8 .b8 	__local_depot49[8];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<30>;
	.reg .f32 	%f<146>;
	.reg .b32 	%r<114>;
	.reg .b64 	%rd<66>;


	mov.u64 	%SPL, __local_depot49;
	ld.param.u64 	%rd27, [ggml_matvec_f32_ncols_2_bs_224_param_0];
	ld.param.u64 	%rd28, [ggml_matvec_f32_ncols_2_bs_224_param_1];
	ld.param.u64 	%rd26, [ggml_matvec_f32_ncols_2_bs_224_param_2];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_2_bs_224_param_3];
	ld.param.u32 	%r19, [ggml_matvec_f32_ncols_2_bs_224_param_5];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_2_bs_224_param_6];
	ld.param.u32 	%r17, [ggml_matvec_f32_ncols_2_bs_224_param_7];
	ld.param.u32 	%r20, [ggml_matvec_f32_ncols_2_bs_224_param_8];
	ld.param.u32 	%r21, [ggml_matvec_f32_ncols_2_bs_224_param_9];
	ld.param.u32 	%r22, [ggml_matvec_f32_ncols_2_bs_224_param_10];
	ld.param.u32 	%r18, [ggml_matvec_f32_ncols_2_bs_224_param_11];
	cvta.to.global.u64 	%rd1, %rd28;
	cvta.to.global.u64 	%rd2, %rd27;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r23, %r1, %r20;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r24, %r2, %r19;
	mad.lo.s32 	%r25, %r23, %r21, %r24;
	cvt.s64.s32 	%rd4, %r25;
	mul.lo.s32 	%r26, %r1, %r22;
	cvt.s64.s32 	%rd5, %r26;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r27, %r3, 2;
	mov.u32 	%r28, data_mmv;
	add.s32 	%r4, %r28, %r27;
	@%p1 bra 	$L__BB49_2;

	mov.u32 	%r29, 0;
	st.shared.u32 	[%r4], %r29;

$L__BB49_2:
	bar.sync 	0;
	mov.f32 	%f144, 0f00000000;
	st.local.v2.f32 	[%rd3], {%f144, %f144};
	setp.ge.s32 	%p2, %r3, %r15;
	mov.f32 	%f145, %f144;
	@%p2 bra 	$L__BB49_11;

	not.b32 	%r30, %r3;
	add.s32 	%r5, %r30, %r15;
	shr.u32 	%r31, %r5, 5;
	mul.wide.u32 	%rd30, %r31, 613566757;
	shr.u64 	%rd31, %rd30, 32;
	cvt.u32.u64 	%r32, %rd31;
	add.s32 	%r33, %r32, 1;
	and.b32  	%r111, %r33, 3;
	setp.eq.s32 	%p3, %r111, 0;
	mov.f32 	%f144, 0f00000000;
	mov.u32 	%r112, %r3;
	@%p3 bra 	$L__BB49_7;

	mul.wide.s32 	%rd32, %r16, 2;
	mul.wide.s32 	%rd33, %r3, 2;
	add.s64 	%rd34, %rd32, %rd33;
	add.s64 	%rd35, %rd34, %rd5;
	shl.b64 	%rd36, %rd35, 2;
	add.s64 	%rd62, %rd1, %rd36;
	add.s64 	%rd37, %rd33, %rd5;
	shl.b64 	%rd38, %rd37, 2;
	add.s64 	%rd61, %rd1, %rd38;
	add.s64 	%rd39, %rd33, %rd4;
	shl.b64 	%rd40, %rd39, 2;
	add.s64 	%rd60, %rd2, %rd40;
	mov.f32 	%f144, 0f00000000;
	mov.f32 	%f145, %f144;
	mov.u32 	%r112, %r3;

$L__BB49_5:
	.pragma "nounroll";
	ld.global.nc.v2.f32 	{%f19, %f20}, [%rd60];
	ld.global.nc.v2.f32 	{%f23, %f24}, [%rd61];
	fma.rn.f32 	%f27, %f19, %f23, %f145;
	fma.rn.f32 	%f145, %f20, %f24, %f27;
	ld.global.nc.v2.f32 	{%f28, %f29}, [%rd62];
	fma.rn.f32 	%f32, %f19, %f28, %f144;
	fma.rn.f32 	%f144, %f20, %f29, %f32;
	add.s32 	%r112, %r112, 224;
	add.s64 	%rd62, %rd62, 1792;
	add.s64 	%rd61, %rd61, 1792;
	add.s64 	%rd60, %rd60, 1792;
	add.s32 	%r111, %r111, -1;
	setp.ne.s32 	%p4, %r111, 0;
	@%p4 bra 	$L__BB49_5;

	st.local.v2.f32 	[%rd3], {%f145, %f144};

$L__BB49_7:
	setp.lt.u32 	%p5, %r5, 672;
	@%p5 bra 	$L__BB49_11;

	mul.wide.s32 	%rd41, %r112, 2;
	add.s64 	%rd42, %rd41, %rd4;
	shl.b64 	%rd43, %rd42, 2;
	add.s64 	%rd44, %rd2, %rd43;
	add.s64 	%rd65, %rd44, 3584;
	add.s64 	%rd45, %rd41, %rd5;
	shl.b64 	%rd46, %rd45, 2;
	add.s64 	%rd47, %rd1, %rd46;
	add.s64 	%rd64, %rd47, 5376;
	mul.wide.s32 	%rd48, %r16, 2;
	add.s64 	%rd49, %rd45, %rd48;
	shl.b64 	%rd50, %rd49, 2;
	add.s64 	%rd51, %rd1, %rd50;
	add.s64 	%rd63, %rd51, 3584;

$L__BB49_9:
	ld.global.nc.v2.f32 	{%f33, %f34}, [%rd65+-3584];
	ld.global.nc.v2.f32 	{%f37, %f38}, [%rd64+-5376];
	fma.rn.f32 	%f41, %f33, %f37, %f145;
	fma.rn.f32 	%f42, %f34, %f38, %f41;
	ld.global.nc.v2.f32 	{%f43, %f44}, [%rd63+-3584];
	fma.rn.f32 	%f47, %f33, %f43, %f144;
	fma.rn.f32 	%f48, %f34, %f44, %f47;
	ld.global.nc.v2.f32 	{%f49, %f50}, [%rd65+-1792];
	ld.global.nc.v2.f32 	{%f53, %f54}, [%rd64+-3584];
	fma.rn.f32 	%f57, %f49, %f53, %f42;
	fma.rn.f32 	%f58, %f50, %f54, %f57;
	ld.global.nc.v2.f32 	{%f59, %f60}, [%rd63+-1792];
	fma.rn.f32 	%f63, %f49, %f59, %f48;
	fma.rn.f32 	%f64, %f50, %f60, %f63;
	ld.global.nc.v2.f32 	{%f65, %f66}, [%rd65];
	ld.global.nc.v2.f32 	{%f69, %f70}, [%rd64+-1792];
	fma.rn.f32 	%f73, %f65, %f69, %f58;
	fma.rn.f32 	%f74, %f66, %f70, %f73;
	ld.global.nc.v2.f32 	{%f75, %f76}, [%rd63];
	fma.rn.f32 	%f79, %f65, %f75, %f64;
	fma.rn.f32 	%f80, %f66, %f76, %f79;
	ld.global.nc.v2.f32 	{%f81, %f82}, [%rd65+1792];
	ld.global.nc.v2.f32 	{%f85, %f86}, [%rd64];
	fma.rn.f32 	%f89, %f81, %f85, %f74;
	fma.rn.f32 	%f145, %f82, %f86, %f89;
	ld.global.nc.v2.f32 	{%f90, %f91}, [%rd63+1792];
	fma.rn.f32 	%f94, %f81, %f90, %f80;
	fma.rn.f32 	%f144, %f82, %f91, %f94;
	add.s64 	%rd65, %rd65, 7168;
	add.s64 	%rd64, %rd64, 7168;
	add.s64 	%rd63, %rd63, 7168;
	add.s32 	%r112, %r112, 896;
	setp.lt.s32 	%p6, %r112, %r15;
	@%p6 bra 	$L__BB49_9;

	st.local.v2.f32 	[%rd3], {%f145, %f144};

$L__BB49_11:
	shr.s32 	%r34, %r3, 31;
	shr.u32 	%r35, %r34, 27;
	add.s32 	%r36, %r3, %r35;
	shr.s32 	%r37, %r36, 5;
	shl.b32 	%r38, %r37, 2;
	add.s32 	%r14, %r28, %r38;
	mov.u32 	%r40, 2;
	mov.b32 	%r41, %f145;
	mov.u32 	%r42, 31;
	mov.u32 	%r43, 16;
	mov.u32 	%r44, -1;
	shfl.sync.bfly.b32 	%r45|%p7, %r41, %r43, %r42, %r44;
	mov.b32 	%f95, %r45;
	add.f32 	%f96, %f145, %f95;
	mov.b32 	%r46, %f96;
	mov.u32 	%r47, 8;
	shfl.sync.bfly.b32 	%r48|%p8, %r46, %r47, %r42, %r44;
	mov.b32 	%f97, %r48;
	add.f32 	%f98, %f96, %f97;
	mov.b32 	%r49, %f98;
	mov.u32 	%r50, 4;
	shfl.sync.bfly.b32 	%r51|%p9, %r49, %r50, %r42, %r44;
	mov.b32 	%f99, %r51;
	add.f32 	%f100, %f98, %f99;
	mov.b32 	%r52, %f100;
	shfl.sync.bfly.b32 	%r53|%p10, %r52, %r40, %r42, %r44;
	mov.b32 	%f101, %r53;
	add.f32 	%f102, %f100, %f101;
	mov.b32 	%r54, %f102;
	mov.u32 	%r55, 1;
	shfl.sync.bfly.b32 	%r56|%p11, %r54, %r55, %r42, %r44;
	mov.b32 	%f103, %r56;
	add.f32 	%f104, %f102, %f103;
	st.local.f32 	[%rd3], %f104;
	st.shared.f32 	[%r14], %f104;
	bar.sync 	0;
	@%p1 bra 	$L__BB49_13;

	ld.shared.f32 	%f105, [%r4];
	mov.b32 	%r57, %f105;
	shfl.sync.bfly.b32 	%r61|%p13, %r57, %r43, %r42, %r44;
	mov.b32 	%f106, %r61;
	add.f32 	%f107, %f105, %f106;
	mov.b32 	%r62, %f107;
	shfl.sync.bfly.b32 	%r64|%p14, %r62, %r47, %r42, %r44;
	mov.b32 	%f108, %r64;
	add.f32 	%f109, %f107, %f108;
	mov.b32 	%r65, %f109;
	shfl.sync.bfly.b32 	%r67|%p15, %r65, %r50, %r42, %r44;
	mov.b32 	%f110, %r67;
	add.f32 	%f111, %f109, %f110;
	mov.b32 	%r68, %f111;
	shfl.sync.bfly.b32 	%r70|%p16, %r68, %r40, %r42, %r44;
	mov.b32 	%f112, %r70;
	add.f32 	%f113, %f111, %f112;
	mov.b32 	%r71, %f113;
	shfl.sync.bfly.b32 	%r73|%p17, %r71, %r55, %r42, %r44;
	mov.b32 	%f114, %r73;
	add.f32 	%f115, %f113, %f114;
	st.local.f32 	[%rd3], %f115;

$L__BB49_13:
	bar.sync 	0;
	mov.b32 	%r74, %f144;
	shfl.sync.bfly.b32 	%r78|%p19, %r74, %r43, %r42, %r44;
	mov.b32 	%f116, %r78;
	add.f32 	%f117, %f144, %f116;
	mov.b32 	%r79, %f117;
	shfl.sync.bfly.b32 	%r81|%p20, %r79, %r47, %r42, %r44;
	mov.b32 	%f118, %r81;
	add.f32 	%f119, %f117, %f118;
	mov.b32 	%r82, %f119;
	shfl.sync.bfly.b32 	%r84|%p21, %r82, %r50, %r42, %r44;
	mov.b32 	%f120, %r84;
	add.f32 	%f121, %f119, %f120;
	mov.b32 	%r85, %f121;
	shfl.sync.bfly.b32 	%r87|%p22, %r85, %r40, %r42, %r44;
	mov.b32 	%f122, %r87;
	add.f32 	%f123, %f121, %f122;
	mov.b32 	%r88, %f123;
	shfl.sync.bfly.b32 	%r90|%p23, %r88, %r55, %r42, %r44;
	mov.b32 	%f124, %r90;
	add.f32 	%f125, %f123, %f124;
	st.local.f32 	[%rd3+4], %f125;
	st.shared.f32 	[%r14], %f125;
	bar.sync 	0;
	@%p1 bra 	$L__BB49_15;

	ld.shared.f32 	%f126, [%r4];
	mov.b32 	%r91, %f126;
	mov.u32 	%r92, 31;
	mov.u32 	%r93, 16;
	mov.u32 	%r94, -1;
	shfl.sync.bfly.b32 	%r95|%p24, %r91, %r93, %r92, %r94;
	mov.b32 	%f127, %r95;
	add.f32 	%f128, %f126, %f127;
	mov.b32 	%r96, %f128;
	mov.u32 	%r97, 8;
	shfl.sync.bfly.b32 	%r98|%p25, %r96, %r97, %r92, %r94;
	mov.b32 	%f129, %r98;
	add.f32 	%f130, %f128, %f129;
	mov.b32 	%r99, %f130;
	mov.u32 	%r100, 4;
	shfl.sync.bfly.b32 	%r101|%p26, %r99, %r100, %r92, %r94;
	mov.b32 	%f131, %r101;
	add.f32 	%f132, %f130, %f131;
	mov.b32 	%r102, %f132;
	mov.u32 	%r103, 2;
	shfl.sync.bfly.b32 	%r104|%p27, %r102, %r103, %r92, %r94;
	mov.b32 	%f133, %r104;
	add.f32 	%f134, %f132, %f133;
	mov.b32 	%r105, %f134;
	mov.u32 	%r106, 1;
	shfl.sync.bfly.b32 	%r107|%p28, %r105, %r106, %r92, %r94;
	mov.b32 	%f135, %r107;
	add.f32 	%f136, %f134, %f135;
	st.local.f32 	[%rd3+4], %f136;

$L__BB49_15:
	bar.sync 	0;
	setp.gt.s32 	%p29, %r3, 1;
	@%p29 bra 	$L__BB49_17;

	mul.wide.s32 	%rd52, %r3, 4;
	add.s64 	%rd53, %rd3, %rd52;
	ld.local.f32 	%f137, [%rd53];
	mad.lo.s32 	%r108, %r3, %r17, %r2;
	cvt.s64.s32 	%rd54, %r108;
	mul.lo.s32 	%r109, %r1, %r18;
	cvt.s64.s32 	%rd55, %r109;
	add.s64 	%rd56, %rd55, %rd54;
	cvta.to.global.u64 	%rd57, %rd26;
	shl.b64 	%rd58, %rd56, 2;
	add.s64 	%rd59, %rd57, %rd58;
	st.global.f32 	[%rd59], %f137;

$L__BB49_17:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_3_bs_224
.visible .entry ggml_matvec_f32_ncols_3_bs_224(
	.param .u64 ggml_matvec_f32_ncols_3_bs_224_param_0,
	.param .u64 ggml_matvec_f32_ncols_3_bs_224_param_1,
	.param .u64 ggml_matvec_f32_ncols_3_bs_224_param_2,
	.param .u32 ggml_matvec_f32_ncols_3_bs_224_param_3,
	.param .u32 ggml_matvec_f32_ncols_3_bs_224_param_4,
	.param .u32 ggml_matvec_f32_ncols_3_bs_224_param_5,
	.param .u32 ggml_matvec_f32_ncols_3_bs_224_param_6,
	.param .u32 ggml_matvec_f32_ncols_3_bs_224_param_7,
	.param .u32 ggml_matvec_f32_ncols_3_bs_224_param_8,
	.param .u32 ggml_matvec_f32_ncols_3_bs_224_param_9,
	.param .u32 ggml_matvec_f32_ncols_3_bs_224_param_10,
	.param .u32 ggml_matvec_f32_ncols_3_bs_224_param_11
)
{
	.local .align 4 .b8 	__local_depot50[12];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<41>;
	.reg .f32 	%f<208>;
	.reg .b32 	%r<155>;
	.reg .b64 	%rd<74>;


	mov.u64 	%SPL, __local_depot50;
	ld.param.u64 	%rd29, [ggml_matvec_f32_ncols_3_bs_224_param_0];
	ld.param.u64 	%rd30, [ggml_matvec_f32_ncols_3_bs_224_param_1];
	ld.param.u64 	%rd28, [ggml_matvec_f32_ncols_3_bs_224_param_2];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_3_bs_224_param_3];
	ld.param.u32 	%r19, [ggml_matvec_f32_ncols_3_bs_224_param_5];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_3_bs_224_param_6];
	ld.param.u32 	%r17, [ggml_matvec_f32_ncols_3_bs_224_param_7];
	ld.param.u32 	%r20, [ggml_matvec_f32_ncols_3_bs_224_param_8];
	ld.param.u32 	%r21, [ggml_matvec_f32_ncols_3_bs_224_param_9];
	ld.param.u32 	%r22, [ggml_matvec_f32_ncols_3_bs_224_param_10];
	ld.param.u32 	%r18, [ggml_matvec_f32_ncols_3_bs_224_param_11];
	cvta.to.global.u64 	%rd73, %rd30;
	cvta.to.global.u64 	%rd2, %rd29;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r23, %r1, %r20;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r24, %r2, %r19;
	mad.lo.s32 	%r25, %r23, %r21, %r24;
	cvt.s64.s32 	%rd4, %r25;
	mul.lo.s32 	%r26, %r1, %r22;
	cvt.s64.s32 	%rd5, %r26;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r27, %r3, 2;
	mov.u32 	%r28, data_mmv;
	add.s32 	%r4, %r28, %r27;
	@%p1 bra 	$L__BB50_2;

	mov.u32 	%r29, 0;
	st.shared.u32 	[%r4], %r29;

$L__BB50_2:
	bar.sync 	0;
	mov.f32 	%f205, 0f00000000;
	mov.u32 	%r30, 0;
	st.local.u32 	[%rd3], %r30;
	st.local.u32 	[%rd3+4], %r30;
	st.local.u32 	[%rd3+8], %r30;
	setp.ge.s32 	%p2, %r3, %r15;
	mov.f32 	%f206, %f205;
	mov.f32 	%f207, %f205;
	@%p2 bra 	$L__BB50_11;

	not.b32 	%r31, %r3;
	add.s32 	%r5, %r31, %r15;
	shr.u32 	%r32, %r5, 5;
	mul.wide.u32 	%rd32, %r32, 613566757;
	shr.u64 	%rd33, %rd32, 32;
	cvt.u32.u64 	%r33, %rd33;
	add.s32 	%r34, %r33, 1;
	and.b32  	%r152, %r34, 3;
	setp.eq.s32 	%p3, %r152, 0;
	mov.f32 	%f205, 0f00000000;
	mov.u32 	%r153, %r3;
	@%p3 bra 	$L__BB50_7;

	shl.b32 	%r35, %r16, 1;
	add.s32 	%r36, %r3, %r35;
	mul.wide.s32 	%rd34, %r36, 2;
	add.s64 	%rd35, %rd34, %rd5;
	shl.b64 	%rd36, %rd35, 2;
	add.s64 	%rd71, %rd73, %rd36;
	mul.wide.s32 	%rd37, %r16, 2;
	mul.wide.s32 	%rd38, %r3, 2;
	add.s64 	%rd39, %rd37, %rd38;
	add.s64 	%rd40, %rd39, %rd5;
	shl.b64 	%rd41, %rd40, 2;
	add.s64 	%rd70, %rd73, %rd41;
	add.s64 	%rd42, %rd38, %rd5;
	shl.b64 	%rd43, %rd42, 2;
	add.s64 	%rd69, %rd73, %rd43;
	add.s64 	%rd44, %rd38, %rd4;
	shl.b64 	%rd45, %rd44, 2;
	add.s64 	%rd68, %rd2, %rd45;
	mov.f32 	%f205, 0f00000000;
	mov.f32 	%f206, %f205;
	mov.f32 	%f207, %f205;
	mov.u32 	%r153, %r3;

$L__BB50_5:
	.pragma "nounroll";
	ld.global.nc.v2.f32 	{%f28, %f29}, [%rd68];
	ld.global.nc.v2.f32 	{%f32, %f33}, [%rd69];
	fma.rn.f32 	%f36, %f28, %f32, %f207;
	fma.rn.f32 	%f207, %f29, %f33, %f36;
	ld.global.nc.v2.f32 	{%f37, %f38}, [%rd70];
	fma.rn.f32 	%f41, %f28, %f37, %f206;
	fma.rn.f32 	%f206, %f29, %f38, %f41;
	ld.global.nc.v2.f32 	{%f42, %f43}, [%rd71];
	fma.rn.f32 	%f46, %f28, %f42, %f205;
	fma.rn.f32 	%f205, %f29, %f43, %f46;
	add.s32 	%r153, %r153, 224;
	add.s64 	%rd71, %rd71, 1792;
	add.s64 	%rd70, %rd70, 1792;
	add.s64 	%rd69, %rd69, 1792;
	add.s64 	%rd68, %rd68, 1792;
	add.s32 	%r152, %r152, -1;
	setp.ne.s32 	%p4, %r152, 0;
	@%p4 bra 	$L__BB50_5;

	st.local.f32 	[%rd3], %f207;
	st.local.f32 	[%rd3+4], %f206;
	st.local.f32 	[%rd3+8], %f205;

$L__BB50_7:
	setp.lt.u32 	%p5, %r5, 672;
	@%p5 bra 	$L__BB50_11;

	add.s32 	%r37, %r153, %r16;
	shl.b32 	%r38, %r16, 1;
	add.s32 	%r39, %r153, %r38;
	add.s32 	%r40, %r37, 224;
	mul.wide.s32 	%rd46, %r40, 8;
	shl.b64 	%rd47, %rd5, 2;
	add.s64 	%rd19, %rd46, %rd47;
	mul.wide.s32 	%rd48, %r39, 8;
	add.s64 	%rd20, %rd48, %rd47;
	mul.wide.s32 	%rd49, %r153, 2;
	add.s64 	%rd50, %rd49, %rd4;
	shl.b64 	%rd51, %rd50, 2;
	add.s64 	%rd52, %rd2, %rd51;
	add.s64 	%rd72, %rd52, 3584;
	mul.wide.s32 	%rd53, %r153, 8;
	add.s64 	%rd22, %rd53, %rd47;
	mul.wide.s32 	%rd54, %r16, 8;
	add.s64 	%rd55, %rd53, %rd54;
	add.s64 	%rd23, %rd55, %rd47;

$L__BB50_9:
	ld.global.nc.v2.f32 	{%f47, %f48}, [%rd72+-3584];
	add.s64 	%rd56, %rd73, %rd22;
	ld.global.nc.v2.f32 	{%f51, %f52}, [%rd56];
	fma.rn.f32 	%f55, %f47, %f51, %f207;
	fma.rn.f32 	%f56, %f48, %f52, %f55;
	add.s64 	%rd57, %rd73, %rd23;
	ld.global.nc.v2.f32 	{%f57, %f58}, [%rd57];
	fma.rn.f32 	%f61, %f47, %f57, %f206;
	fma.rn.f32 	%f62, %f48, %f58, %f61;
	add.s64 	%rd58, %rd73, %rd20;
	ld.global.nc.v2.f32 	{%f63, %f64}, [%rd58];
	fma.rn.f32 	%f67, %f47, %f63, %f205;
	fma.rn.f32 	%f68, %f48, %f64, %f67;
	ld.global.nc.v2.f32 	{%f69, %f70}, [%rd72+-1792];
	ld.global.nc.v2.f32 	{%f73, %f74}, [%rd56+1792];
	fma.rn.f32 	%f77, %f69, %f73, %f56;
	fma.rn.f32 	%f78, %f70, %f74, %f77;
	add.s64 	%rd59, %rd73, %rd19;
	ld.global.nc.v2.f32 	{%f79, %f80}, [%rd59];
	fma.rn.f32 	%f83, %f69, %f79, %f62;
	fma.rn.f32 	%f84, %f70, %f80, %f83;
	ld.global.nc.v2.f32 	{%f85, %f86}, [%rd58+1792];
	fma.rn.f32 	%f89, %f69, %f85, %f68;
	fma.rn.f32 	%f90, %f70, %f86, %f89;
	ld.global.nc.v2.f32 	{%f91, %f92}, [%rd72];
	ld.global.nc.v2.f32 	{%f95, %f96}, [%rd56+3584];
	fma.rn.f32 	%f99, %f91, %f95, %f78;
	fma.rn.f32 	%f100, %f92, %f96, %f99;
	ld.global.nc.v2.f32 	{%f101, %f102}, [%rd59+1792];
	fma.rn.f32 	%f105, %f91, %f101, %f84;
	fma.rn.f32 	%f106, %f92, %f102, %f105;
	ld.global.nc.v2.f32 	{%f107, %f108}, [%rd58+3584];
	fma.rn.f32 	%f111, %f91, %f107, %f90;
	fma.rn.f32 	%f112, %f92, %f108, %f111;
	ld.global.nc.v2.f32 	{%f113, %f114}, [%rd72+1792];
	ld.global.nc.v2.f32 	{%f117, %f118}, [%rd56+5376];
	fma.rn.f32 	%f121, %f113, %f117, %f100;
	fma.rn.f32 	%f207, %f114, %f118, %f121;
	ld.global.nc.v2.f32 	{%f122, %f123}, [%rd59+3584];
	fma.rn.f32 	%f126, %f113, %f122, %f106;
	fma.rn.f32 	%f206, %f114, %f123, %f126;
	ld.global.nc.v2.f32 	{%f127, %f128}, [%rd58+5376];
	fma.rn.f32 	%f131, %f113, %f127, %f112;
	fma.rn.f32 	%f205, %f114, %f128, %f131;
	add.s64 	%rd73, %rd73, 7168;
	add.s64 	%rd72, %rd72, 7168;
	add.s32 	%r153, %r153, 896;
	setp.lt.s32 	%p6, %r153, %r15;
	@%p6 bra 	$L__BB50_9;

	st.local.f32 	[%rd3], %f207;
	st.local.f32 	[%rd3+4], %f206;
	st.local.f32 	[%rd3+8], %f205;

$L__BB50_11:
	shr.s32 	%r41, %r3, 31;
	shr.u32 	%r42, %r41, 27;
	add.s32 	%r43, %r3, %r42;
	shr.s32 	%r44, %r43, 5;
	shl.b32 	%r45, %r44, 2;
	add.s32 	%r14, %r28, %r45;
	mov.u32 	%r47, 2;
	mov.b32 	%r48, %f207;
	mov.u32 	%r49, 31;
	mov.u32 	%r50, 16;
	mov.u32 	%r51, -1;
	shfl.sync.bfly.b32 	%r52|%p7, %r48, %r50, %r49, %r51;
	mov.b32 	%f132, %r52;
	add.f32 	%f133, %f207, %f132;
	mov.b32 	%r53, %f133;
	mov.u32 	%r54, 8;
	shfl.sync.bfly.b32 	%r55|%p8, %r53, %r54, %r49, %r51;
	mov.b32 	%f134, %r55;
	add.f32 	%f135, %f133, %f134;
	mov.b32 	%r56, %f135;
	mov.u32 	%r57, 4;
	shfl.sync.bfly.b32 	%r58|%p9, %r56, %r57, %r49, %r51;
	mov.b32 	%f136, %r58;
	add.f32 	%f137, %f135, %f136;
	mov.b32 	%r59, %f137;
	shfl.sync.bfly.b32 	%r60|%p10, %r59, %r47, %r49, %r51;
	mov.b32 	%f138, %r60;
	add.f32 	%f139, %f137, %f138;
	mov.b32 	%r61, %f139;
	mov.u32 	%r62, 1;
	shfl.sync.bfly.b32 	%r63|%p11, %r61, %r62, %r49, %r51;
	mov.b32 	%f140, %r63;
	add.f32 	%f141, %f139, %f140;
	st.local.f32 	[%rd3], %f141;
	st.shared.f32 	[%r14], %f141;
	bar.sync 	0;
	@%p1 bra 	$L__BB50_13;

	ld.shared.f32 	%f142, [%r4];
	mov.b32 	%r64, %f142;
	shfl.sync.bfly.b32 	%r68|%p13, %r64, %r50, %r49, %r51;
	mov.b32 	%f143, %r68;
	add.f32 	%f144, %f142, %f143;
	mov.b32 	%r69, %f144;
	shfl.sync.bfly.b32 	%r71|%p14, %r69, %r54, %r49, %r51;
	mov.b32 	%f145, %r71;
	add.f32 	%f146, %f144, %f145;
	mov.b32 	%r72, %f146;
	shfl.sync.bfly.b32 	%r74|%p15, %r72, %r57, %r49, %r51;
	mov.b32 	%f147, %r74;
	add.f32 	%f148, %f146, %f147;
	mov.b32 	%r75, %f148;
	shfl.sync.bfly.b32 	%r77|%p16, %r75, %r47, %r49, %r51;
	mov.b32 	%f149, %r77;
	add.f32 	%f150, %f148, %f149;
	mov.b32 	%r78, %f150;
	shfl.sync.bfly.b32 	%r80|%p17, %r78, %r62, %r49, %r51;
	mov.b32 	%f151, %r80;
	add.f32 	%f152, %f150, %f151;
	st.local.f32 	[%rd3], %f152;

$L__BB50_13:
	bar.sync 	0;
	mov.b32 	%r81, %f206;
	shfl.sync.bfly.b32 	%r85|%p19, %r81, %r50, %r49, %r51;
	mov.b32 	%f153, %r85;
	add.f32 	%f154, %f206, %f153;
	mov.b32 	%r86, %f154;
	shfl.sync.bfly.b32 	%r88|%p20, %r86, %r54, %r49, %r51;
	mov.b32 	%f155, %r88;
	add.f32 	%f156, %f154, %f155;
	mov.b32 	%r89, %f156;
	shfl.sync.bfly.b32 	%r91|%p21, %r89, %r57, %r49, %r51;
	mov.b32 	%f157, %r91;
	add.f32 	%f158, %f156, %f157;
	mov.b32 	%r92, %f158;
	shfl.sync.bfly.b32 	%r94|%p22, %r92, %r47, %r49, %r51;
	mov.b32 	%f159, %r94;
	add.f32 	%f160, %f158, %f159;
	mov.b32 	%r95, %f160;
	shfl.sync.bfly.b32 	%r97|%p23, %r95, %r62, %r49, %r51;
	mov.b32 	%f161, %r97;
	add.f32 	%f162, %f160, %f161;
	st.local.f32 	[%rd3+4], %f162;
	st.shared.f32 	[%r14], %f162;
	bar.sync 	0;
	@%p1 bra 	$L__BB50_15;

	ld.shared.f32 	%f163, [%r4];
	mov.b32 	%r98, %f163;
	mov.u32 	%r99, 31;
	mov.u32 	%r100, 16;
	mov.u32 	%r101, -1;
	shfl.sync.bfly.b32 	%r102|%p24, %r98, %r100, %r99, %r101;
	mov.b32 	%f164, %r102;
	add.f32 	%f165, %f163, %f164;
	mov.b32 	%r103, %f165;
	mov.u32 	%r104, 8;
	shfl.sync.bfly.b32 	%r105|%p25, %r103, %r104, %r99, %r101;
	mov.b32 	%f166, %r105;
	add.f32 	%f167, %f165, %f166;
	mov.b32 	%r106, %f167;
	mov.u32 	%r107, 4;
	shfl.sync.bfly.b32 	%r108|%p26, %r106, %r107, %r99, %r101;
	mov.b32 	%f168, %r108;
	add.f32 	%f169, %f167, %f168;
	mov.b32 	%r109, %f169;
	mov.u32 	%r110, 2;
	shfl.sync.bfly.b32 	%r111|%p27, %r109, %r110, %r99, %r101;
	mov.b32 	%f170, %r111;
	add.f32 	%f171, %f169, %f170;
	mov.b32 	%r112, %f171;
	mov.u32 	%r113, 1;
	shfl.sync.bfly.b32 	%r114|%p28, %r112, %r113, %r99, %r101;
	mov.b32 	%f172, %r114;
	add.f32 	%f173, %f171, %f172;
	st.local.f32 	[%rd3+4], %f173;

$L__BB50_15:
	bar.sync 	0;
	mov.b32 	%r115, %f205;
	mov.u32 	%r116, 31;
	mov.u32 	%r117, 16;
	mov.u32 	%r118, -1;
	shfl.sync.bfly.b32 	%r119|%p30, %r115, %r117, %r116, %r118;
	mov.b32 	%f174, %r119;
	add.f32 	%f175, %f205, %f174;
	mov.b32 	%r120, %f175;
	mov.u32 	%r121, 8;
	shfl.sync.bfly.b32 	%r122|%p31, %r120, %r121, %r116, %r118;
	mov.b32 	%f176, %r122;
	add.f32 	%f177, %f175, %f176;
	mov.b32 	%r123, %f177;
	mov.u32 	%r124, 4;
	shfl.sync.bfly.b32 	%r125|%p32, %r123, %r124, %r116, %r118;
	mov.b32 	%f178, %r125;
	add.f32 	%f179, %f177, %f178;
	mov.b32 	%r126, %f179;
	mov.u32 	%r127, 2;
	shfl.sync.bfly.b32 	%r128|%p33, %r126, %r127, %r116, %r118;
	mov.b32 	%f180, %r128;
	add.f32 	%f181, %f179, %f180;
	mov.b32 	%r129, %f181;
	mov.u32 	%r130, 1;
	shfl.sync.bfly.b32 	%r131|%p34, %r129, %r130, %r116, %r118;
	mov.b32 	%f182, %r131;
	add.f32 	%f183, %f181, %f182;
	st.local.f32 	[%rd3+8], %f183;
	st.shared.f32 	[%r14], %f183;
	bar.sync 	0;
	@%p1 bra 	$L__BB50_17;

	ld.shared.f32 	%f184, [%r4];
	mov.b32 	%r132, %f184;
	shfl.sync.bfly.b32 	%r136|%p35, %r132, %r117, %r116, %r118;
	mov.b32 	%f185, %r136;
	add.f32 	%f186, %f184, %f185;
	mov.b32 	%r137, %f186;
	shfl.sync.bfly.b32 	%r139|%p36, %r137, %r121, %r116, %r118;
	mov.b32 	%f187, %r139;
	add.f32 	%f188, %f186, %f187;
	mov.b32 	%r140, %f188;
	shfl.sync.bfly.b32 	%r142|%p37, %r140, %r124, %r116, %r118;
	mov.b32 	%f189, %r142;
	add.f32 	%f190, %f188, %f189;
	mov.b32 	%r143, %f190;
	shfl.sync.bfly.b32 	%r145|%p38, %r143, %r127, %r116, %r118;
	mov.b32 	%f191, %r145;
	add.f32 	%f192, %f190, %f191;
	mov.b32 	%r146, %f192;
	shfl.sync.bfly.b32 	%r148|%p39, %r146, %r130, %r116, %r118;
	mov.b32 	%f193, %r148;
	add.f32 	%f194, %f192, %f193;
	st.local.f32 	[%rd3+8], %f194;

$L__BB50_17:
	bar.sync 	0;
	setp.gt.s32 	%p40, %r3, 2;
	@%p40 bra 	$L__BB50_19;

	mul.wide.s32 	%rd60, %r3, 4;
	add.s64 	%rd61, %rd3, %rd60;
	ld.local.f32 	%f195, [%rd61];
	mad.lo.s32 	%r149, %r3, %r17, %r2;
	cvt.s64.s32 	%rd62, %r149;
	mul.lo.s32 	%r150, %r1, %r18;
	cvt.s64.s32 	%rd63, %r150;
	add.s64 	%rd64, %rd63, %rd62;
	cvta.to.global.u64 	%rd65, %rd28;
	shl.b64 	%rd66, %rd64, 2;
	add.s64 	%rd67, %rd65, %rd66;
	st.global.f32 	[%rd67], %f195;

$L__BB50_19:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_4_bs_224
.visible .entry ggml_matvec_f32_ncols_4_bs_224(
	.param .u64 ggml_matvec_f32_ncols_4_bs_224_param_0,
	.param .u64 ggml_matvec_f32_ncols_4_bs_224_param_1,
	.param .u64 ggml_matvec_f32_ncols_4_bs_224_param_2,
	.param .u32 ggml_matvec_f32_ncols_4_bs_224_param_3,
	.param .u32 ggml_matvec_f32_ncols_4_bs_224_param_4,
	.param .u32 ggml_matvec_f32_ncols_4_bs_224_param_5,
	.param .u32 ggml_matvec_f32_ncols_4_bs_224_param_6,
	.param .u32 ggml_matvec_f32_ncols_4_bs_224_param_7,
	.param .u32 ggml_matvec_f32_ncols_4_bs_224_param_8,
	.param .u32 ggml_matvec_f32_ncols_4_bs_224_param_9,
	.param .u32 ggml_matvec_f32_ncols_4_bs_224_param_10,
	.param .u32 ggml_matvec_f32_ncols_4_bs_224_param_11
)
{
	.local .align 16 .b8 	__local_depot51[16];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<52>;
	.reg .f32 	%f<270>;
	.reg .b32 	%r<190>;
	.reg .b64 	%rd<85>;


	mov.u64 	%SPL, __local_depot51;
	ld.param.u64 	%rd34, [ggml_matvec_f32_ncols_4_bs_224_param_0];
	ld.param.u64 	%rd35, [ggml_matvec_f32_ncols_4_bs_224_param_1];
	ld.param.u64 	%rd33, [ggml_matvec_f32_ncols_4_bs_224_param_2];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_4_bs_224_param_3];
	ld.param.u32 	%r19, [ggml_matvec_f32_ncols_4_bs_224_param_5];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_4_bs_224_param_6];
	ld.param.u32 	%r17, [ggml_matvec_f32_ncols_4_bs_224_param_7];
	ld.param.u32 	%r20, [ggml_matvec_f32_ncols_4_bs_224_param_8];
	ld.param.u32 	%r21, [ggml_matvec_f32_ncols_4_bs_224_param_9];
	ld.param.u32 	%r22, [ggml_matvec_f32_ncols_4_bs_224_param_10];
	ld.param.u32 	%r18, [ggml_matvec_f32_ncols_4_bs_224_param_11];
	cvta.to.global.u64 	%rd84, %rd35;
	cvta.to.global.u64 	%rd2, %rd34;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r23, %r1, %r20;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r24, %r2, %r19;
	mad.lo.s32 	%r25, %r23, %r21, %r24;
	cvt.s64.s32 	%rd4, %r25;
	mul.lo.s32 	%r26, %r1, %r22;
	cvt.s64.s32 	%rd5, %r26;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r27, %r3, 2;
	mov.u32 	%r28, data_mmv;
	add.s32 	%r4, %r28, %r27;
	@%p1 bra 	$L__BB51_2;

	mov.u32 	%r29, 0;
	st.shared.u32 	[%r4], %r29;

$L__BB51_2:
	bar.sync 	0;
	mov.f32 	%f266, 0f00000000;
	st.local.v4.f32 	[%rd3], {%f266, %f266, %f266, %f266};
	setp.ge.s32 	%p2, %r3, %r15;
	mov.f32 	%f267, %f266;
	mov.f32 	%f268, %f266;
	mov.f32 	%f269, %f266;
	@%p2 bra 	$L__BB51_11;

	not.b32 	%r30, %r3;
	add.s32 	%r5, %r30, %r15;
	shr.u32 	%r31, %r5, 5;
	mul.wide.u32 	%rd37, %r31, 613566757;
	shr.u64 	%rd38, %rd37, 32;
	cvt.u32.u64 	%r32, %rd38;
	add.s32 	%r33, %r32, 1;
	and.b32  	%r187, %r33, 3;
	setp.eq.s32 	%p3, %r187, 0;
	mov.f32 	%f266, 0f00000000;
	mov.u32 	%r188, %r3;
	@%p3 bra 	$L__BB51_7;

	shl.b32 	%r34, %r16, 1;
	add.s32 	%r35, %r3, %r34;
	mul.wide.s32 	%rd39, %r35, 2;
	add.s64 	%rd40, %rd39, %rd5;
	shl.b64 	%rd41, %rd40, 2;
	add.s64 	%rd82, %rd84, %rd41;
	mad.lo.s32 	%r36, %r16, 3, %r3;
	mul.wide.s32 	%rd42, %r36, 2;
	add.s64 	%rd43, %rd42, %rd5;
	shl.b64 	%rd44, %rd43, 2;
	add.s64 	%rd81, %rd84, %rd44;
	mul.wide.s32 	%rd45, %r16, 2;
	mul.wide.s32 	%rd46, %r3, 2;
	add.s64 	%rd47, %rd45, %rd46;
	add.s64 	%rd48, %rd47, %rd5;
	shl.b64 	%rd49, %rd48, 2;
	add.s64 	%rd80, %rd84, %rd49;
	add.s64 	%rd50, %rd46, %rd5;
	shl.b64 	%rd51, %rd50, 2;
	add.s64 	%rd79, %rd84, %rd51;
	add.s64 	%rd52, %rd46, %rd4;
	shl.b64 	%rd53, %rd52, 2;
	add.s64 	%rd78, %rd2, %rd53;
	mov.f32 	%f266, 0f00000000;
	mov.f32 	%f267, %f266;
	mov.f32 	%f268, %f266;
	mov.f32 	%f269, %f266;
	mov.u32 	%r188, %r3;

$L__BB51_5:
	.pragma "nounroll";
	ld.global.nc.v2.f32 	{%f37, %f38}, [%rd78];
	ld.global.nc.v2.f32 	{%f41, %f42}, [%rd79];
	fma.rn.f32 	%f45, %f37, %f41, %f269;
	fma.rn.f32 	%f269, %f38, %f42, %f45;
	ld.global.nc.v2.f32 	{%f46, %f47}, [%rd80];
	fma.rn.f32 	%f50, %f37, %f46, %f268;
	fma.rn.f32 	%f268, %f38, %f47, %f50;
	ld.global.nc.v2.f32 	{%f51, %f52}, [%rd82];
	fma.rn.f32 	%f55, %f37, %f51, %f267;
	fma.rn.f32 	%f267, %f38, %f52, %f55;
	ld.global.nc.v2.f32 	{%f56, %f57}, [%rd81];
	fma.rn.f32 	%f60, %f37, %f56, %f266;
	fma.rn.f32 	%f266, %f38, %f57, %f60;
	add.s32 	%r188, %r188, 224;
	add.s64 	%rd82, %rd82, 1792;
	add.s64 	%rd81, %rd81, 1792;
	add.s64 	%rd80, %rd80, 1792;
	add.s64 	%rd79, %rd79, 1792;
	add.s64 	%rd78, %rd78, 1792;
	add.s32 	%r187, %r187, -1;
	setp.ne.s32 	%p4, %r187, 0;
	@%p4 bra 	$L__BB51_5;

	st.local.v4.f32 	[%rd3], {%f269, %f268, %f267, %f266};

$L__BB51_7:
	setp.lt.u32 	%p5, %r5, 672;
	@%p5 bra 	$L__BB51_11;

	add.s32 	%r37, %r188, %r16;
	shl.b32 	%r38, %r16, 1;
	add.s32 	%r39, %r188, %r38;
	mad.lo.s32 	%r40, %r16, 3, %r188;
	add.s32 	%r41, %r37, 224;
	mul.wide.s32 	%rd54, %r41, 8;
	shl.b64 	%rd55, %rd5, 2;
	add.s64 	%rd23, %rd54, %rd55;
	mul.wide.s32 	%rd56, %r39, 8;
	add.s64 	%rd24, %rd56, %rd55;
	mul.wide.s32 	%rd57, %r40, 8;
	add.s64 	%rd25, %rd57, %rd55;
	mul.wide.s32 	%rd58, %r188, 2;
	add.s64 	%rd59, %rd58, %rd4;
	shl.b64 	%rd60, %rd59, 2;
	add.s64 	%rd61, %rd2, %rd60;
	add.s64 	%rd83, %rd61, 3584;
	mul.wide.s32 	%rd62, %r188, 8;
	add.s64 	%rd27, %rd62, %rd55;
	mul.wide.s32 	%rd63, %r16, 8;
	add.s64 	%rd64, %rd62, %rd63;
	add.s64 	%rd28, %rd64, %rd55;

$L__BB51_9:
	ld.global.nc.v2.f32 	{%f61, %f62}, [%rd83+-3584];
	add.s64 	%rd65, %rd84, %rd27;
	ld.global.nc.v2.f32 	{%f65, %f66}, [%rd65];
	fma.rn.f32 	%f69, %f61, %f65, %f269;
	fma.rn.f32 	%f70, %f62, %f66, %f69;
	add.s64 	%rd66, %rd84, %rd28;
	ld.global.nc.v2.f32 	{%f71, %f72}, [%rd66];
	fma.rn.f32 	%f75, %f61, %f71, %f268;
	fma.rn.f32 	%f76, %f62, %f72, %f75;
	add.s64 	%rd67, %rd84, %rd24;
	ld.global.nc.v2.f32 	{%f77, %f78}, [%rd67];
	fma.rn.f32 	%f81, %f61, %f77, %f267;
	fma.rn.f32 	%f82, %f62, %f78, %f81;
	add.s64 	%rd68, %rd84, %rd25;
	ld.global.nc.v2.f32 	{%f83, %f84}, [%rd68];
	fma.rn.f32 	%f87, %f61, %f83, %f266;
	fma.rn.f32 	%f88, %f62, %f84, %f87;
	ld.global.nc.v2.f32 	{%f89, %f90}, [%rd83+-1792];
	ld.global.nc.v2.f32 	{%f93, %f94}, [%rd65+1792];
	fma.rn.f32 	%f97, %f89, %f93, %f70;
	fma.rn.f32 	%f98, %f90, %f94, %f97;
	add.s64 	%rd69, %rd84, %rd23;
	ld.global.nc.v2.f32 	{%f99, %f100}, [%rd69];
	fma.rn.f32 	%f103, %f89, %f99, %f76;
	fma.rn.f32 	%f104, %f90, %f100, %f103;
	ld.global.nc.v2.f32 	{%f105, %f106}, [%rd67+1792];
	fma.rn.f32 	%f109, %f89, %f105, %f82;
	fma.rn.f32 	%f110, %f90, %f106, %f109;
	ld.global.nc.v2.f32 	{%f111, %f112}, [%rd68+1792];
	fma.rn.f32 	%f115, %f89, %f111, %f88;
	fma.rn.f32 	%f116, %f90, %f112, %f115;
	ld.global.nc.v2.f32 	{%f117, %f118}, [%rd83];
	ld.global.nc.v2.f32 	{%f121, %f122}, [%rd65+3584];
	fma.rn.f32 	%f125, %f117, %f121, %f98;
	fma.rn.f32 	%f126, %f118, %f122, %f125;
	ld.global.nc.v2.f32 	{%f127, %f128}, [%rd69+1792];
	fma.rn.f32 	%f131, %f117, %f127, %f104;
	fma.rn.f32 	%f132, %f118, %f128, %f131;
	ld.global.nc.v2.f32 	{%f133, %f134}, [%rd67+3584];
	fma.rn.f32 	%f137, %f117, %f133, %f110;
	fma.rn.f32 	%f138, %f118, %f134, %f137;
	ld.global.nc.v2.f32 	{%f139, %f140}, [%rd68+3584];
	fma.rn.f32 	%f143, %f117, %f139, %f116;
	fma.rn.f32 	%f144, %f118, %f140, %f143;
	ld.global.nc.v2.f32 	{%f145, %f146}, [%rd83+1792];
	ld.global.nc.v2.f32 	{%f149, %f150}, [%rd65+5376];
	fma.rn.f32 	%f153, %f145, %f149, %f126;
	fma.rn.f32 	%f269, %f146, %f150, %f153;
	ld.global.nc.v2.f32 	{%f154, %f155}, [%rd69+3584];
	fma.rn.f32 	%f158, %f145, %f154, %f132;
	fma.rn.f32 	%f268, %f146, %f155, %f158;
	ld.global.nc.v2.f32 	{%f159, %f160}, [%rd67+5376];
	fma.rn.f32 	%f163, %f145, %f159, %f138;
	fma.rn.f32 	%f267, %f146, %f160, %f163;
	ld.global.nc.v2.f32 	{%f164, %f165}, [%rd68+5376];
	fma.rn.f32 	%f168, %f145, %f164, %f144;
	fma.rn.f32 	%f266, %f146, %f165, %f168;
	add.s64 	%rd84, %rd84, 7168;
	add.s64 	%rd83, %rd83, 7168;
	add.s32 	%r188, %r188, 896;
	setp.lt.s32 	%p6, %r188, %r15;
	@%p6 bra 	$L__BB51_9;

	st.local.v4.f32 	[%rd3], {%f269, %f268, %f267, %f266};

$L__BB51_11:
	shr.s32 	%r42, %r3, 31;
	shr.u32 	%r43, %r42, 27;
	add.s32 	%r44, %r3, %r43;
	shr.s32 	%r45, %r44, 5;
	shl.b32 	%r46, %r45, 2;
	add.s32 	%r14, %r28, %r46;
	mov.u32 	%r48, 2;
	mov.b32 	%r49, %f269;
	mov.u32 	%r50, 31;
	mov.u32 	%r51, 16;
	mov.u32 	%r52, -1;
	shfl.sync.bfly.b32 	%r53|%p7, %r49, %r51, %r50, %r52;
	mov.b32 	%f169, %r53;
	add.f32 	%f170, %f269, %f169;
	mov.b32 	%r54, %f170;
	mov.u32 	%r55, 8;
	shfl.sync.bfly.b32 	%r56|%p8, %r54, %r55, %r50, %r52;
	mov.b32 	%f171, %r56;
	add.f32 	%f172, %f170, %f171;
	mov.b32 	%r57, %f172;
	mov.u32 	%r58, 4;
	shfl.sync.bfly.b32 	%r59|%p9, %r57, %r58, %r50, %r52;
	mov.b32 	%f173, %r59;
	add.f32 	%f174, %f172, %f173;
	mov.b32 	%r60, %f174;
	shfl.sync.bfly.b32 	%r61|%p10, %r60, %r48, %r50, %r52;
	mov.b32 	%f175, %r61;
	add.f32 	%f176, %f174, %f175;
	mov.b32 	%r62, %f176;
	mov.u32 	%r63, 1;
	shfl.sync.bfly.b32 	%r64|%p11, %r62, %r63, %r50, %r52;
	mov.b32 	%f177, %r64;
	add.f32 	%f178, %f176, %f177;
	st.local.f32 	[%rd3], %f178;
	st.shared.f32 	[%r14], %f178;
	bar.sync 	0;
	@%p1 bra 	$L__BB51_13;

	ld.shared.f32 	%f179, [%r4];
	mov.b32 	%r65, %f179;
	shfl.sync.bfly.b32 	%r69|%p13, %r65, %r51, %r50, %r52;
	mov.b32 	%f180, %r69;
	add.f32 	%f181, %f179, %f180;
	mov.b32 	%r70, %f181;
	shfl.sync.bfly.b32 	%r72|%p14, %r70, %r55, %r50, %r52;
	mov.b32 	%f182, %r72;
	add.f32 	%f183, %f181, %f182;
	mov.b32 	%r73, %f183;
	shfl.sync.bfly.b32 	%r75|%p15, %r73, %r58, %r50, %r52;
	mov.b32 	%f184, %r75;
	add.f32 	%f185, %f183, %f184;
	mov.b32 	%r76, %f185;
	shfl.sync.bfly.b32 	%r78|%p16, %r76, %r48, %r50, %r52;
	mov.b32 	%f186, %r78;
	add.f32 	%f187, %f185, %f186;
	mov.b32 	%r79, %f187;
	shfl.sync.bfly.b32 	%r81|%p17, %r79, %r63, %r50, %r52;
	mov.b32 	%f188, %r81;
	add.f32 	%f189, %f187, %f188;
	st.local.f32 	[%rd3], %f189;

$L__BB51_13:
	bar.sync 	0;
	mov.b32 	%r82, %f268;
	shfl.sync.bfly.b32 	%r86|%p19, %r82, %r51, %r50, %r52;
	mov.b32 	%f190, %r86;
	add.f32 	%f191, %f268, %f190;
	mov.b32 	%r87, %f191;
	shfl.sync.bfly.b32 	%r89|%p20, %r87, %r55, %r50, %r52;
	mov.b32 	%f192, %r89;
	add.f32 	%f193, %f191, %f192;
	mov.b32 	%r90, %f193;
	shfl.sync.bfly.b32 	%r92|%p21, %r90, %r58, %r50, %r52;
	mov.b32 	%f194, %r92;
	add.f32 	%f195, %f193, %f194;
	mov.b32 	%r93, %f195;
	shfl.sync.bfly.b32 	%r95|%p22, %r93, %r48, %r50, %r52;
	mov.b32 	%f196, %r95;
	add.f32 	%f197, %f195, %f196;
	mov.b32 	%r96, %f197;
	shfl.sync.bfly.b32 	%r98|%p23, %r96, %r63, %r50, %r52;
	mov.b32 	%f198, %r98;
	add.f32 	%f199, %f197, %f198;
	st.local.f32 	[%rd3+4], %f199;
	st.shared.f32 	[%r14], %f199;
	bar.sync 	0;
	@%p1 bra 	$L__BB51_15;

	ld.shared.f32 	%f200, [%r4];
	mov.b32 	%r99, %f200;
	mov.u32 	%r100, 31;
	mov.u32 	%r101, 16;
	mov.u32 	%r102, -1;
	shfl.sync.bfly.b32 	%r103|%p24, %r99, %r101, %r100, %r102;
	mov.b32 	%f201, %r103;
	add.f32 	%f202, %f200, %f201;
	mov.b32 	%r104, %f202;
	mov.u32 	%r105, 8;
	shfl.sync.bfly.b32 	%r106|%p25, %r104, %r105, %r100, %r102;
	mov.b32 	%f203, %r106;
	add.f32 	%f204, %f202, %f203;
	mov.b32 	%r107, %f204;
	mov.u32 	%r108, 4;
	shfl.sync.bfly.b32 	%r109|%p26, %r107, %r108, %r100, %r102;
	mov.b32 	%f205, %r109;
	add.f32 	%f206, %f204, %f205;
	mov.b32 	%r110, %f206;
	mov.u32 	%r111, 2;
	shfl.sync.bfly.b32 	%r112|%p27, %r110, %r111, %r100, %r102;
	mov.b32 	%f207, %r112;
	add.f32 	%f208, %f206, %f207;
	mov.b32 	%r113, %f208;
	mov.u32 	%r114, 1;
	shfl.sync.bfly.b32 	%r115|%p28, %r113, %r114, %r100, %r102;
	mov.b32 	%f209, %r115;
	add.f32 	%f210, %f208, %f209;
	st.local.f32 	[%rd3+4], %f210;

$L__BB51_15:
	bar.sync 	0;
	mov.b32 	%r116, %f267;
	mov.u32 	%r117, 31;
	mov.u32 	%r118, 16;
	mov.u32 	%r119, -1;
	shfl.sync.bfly.b32 	%r120|%p30, %r116, %r118, %r117, %r119;
	mov.b32 	%f211, %r120;
	add.f32 	%f212, %f267, %f211;
	mov.b32 	%r121, %f212;
	mov.u32 	%r122, 8;
	shfl.sync.bfly.b32 	%r123|%p31, %r121, %r122, %r117, %r119;
	mov.b32 	%f213, %r123;
	add.f32 	%f214, %f212, %f213;
	mov.b32 	%r124, %f214;
	mov.u32 	%r125, 4;
	shfl.sync.bfly.b32 	%r126|%p32, %r124, %r125, %r117, %r119;
	mov.b32 	%f215, %r126;
	add.f32 	%f216, %f214, %f215;
	mov.b32 	%r127, %f216;
	mov.u32 	%r128, 2;
	shfl.sync.bfly.b32 	%r129|%p33, %r127, %r128, %r117, %r119;
	mov.b32 	%f217, %r129;
	add.f32 	%f218, %f216, %f217;
	mov.b32 	%r130, %f218;
	mov.u32 	%r131, 1;
	shfl.sync.bfly.b32 	%r132|%p34, %r130, %r131, %r117, %r119;
	mov.b32 	%f219, %r132;
	add.f32 	%f220, %f218, %f219;
	st.local.f32 	[%rd3+8], %f220;
	st.shared.f32 	[%r14], %f220;
	bar.sync 	0;
	@%p1 bra 	$L__BB51_17;

	ld.shared.f32 	%f221, [%r4];
	mov.b32 	%r133, %f221;
	shfl.sync.bfly.b32 	%r137|%p35, %r133, %r118, %r117, %r119;
	mov.b32 	%f222, %r137;
	add.f32 	%f223, %f221, %f222;
	mov.b32 	%r138, %f223;
	shfl.sync.bfly.b32 	%r140|%p36, %r138, %r122, %r117, %r119;
	mov.b32 	%f224, %r140;
	add.f32 	%f225, %f223, %f224;
	mov.b32 	%r141, %f225;
	shfl.sync.bfly.b32 	%r143|%p37, %r141, %r125, %r117, %r119;
	mov.b32 	%f226, %r143;
	add.f32 	%f227, %f225, %f226;
	mov.b32 	%r144, %f227;
	shfl.sync.bfly.b32 	%r146|%p38, %r144, %r128, %r117, %r119;
	mov.b32 	%f228, %r146;
	add.f32 	%f229, %f227, %f228;
	mov.b32 	%r147, %f229;
	shfl.sync.bfly.b32 	%r149|%p39, %r147, %r131, %r117, %r119;
	mov.b32 	%f230, %r149;
	add.f32 	%f231, %f229, %f230;
	st.local.f32 	[%rd3+8], %f231;

$L__BB51_17:
	bar.sync 	0;
	mov.b32 	%r150, %f266;
	shfl.sync.bfly.b32 	%r154|%p41, %r150, %r118, %r117, %r119;
	mov.b32 	%f232, %r154;
	add.f32 	%f233, %f266, %f232;
	mov.b32 	%r155, %f233;
	shfl.sync.bfly.b32 	%r157|%p42, %r155, %r122, %r117, %r119;
	mov.b32 	%f234, %r157;
	add.f32 	%f235, %f233, %f234;
	mov.b32 	%r158, %f235;
	shfl.sync.bfly.b32 	%r160|%p43, %r158, %r125, %r117, %r119;
	mov.b32 	%f236, %r160;
	add.f32 	%f237, %f235, %f236;
	mov.b32 	%r161, %f237;
	shfl.sync.bfly.b32 	%r163|%p44, %r161, %r128, %r117, %r119;
	mov.b32 	%f238, %r163;
	add.f32 	%f239, %f237, %f238;
	mov.b32 	%r164, %f239;
	shfl.sync.bfly.b32 	%r166|%p45, %r164, %r131, %r117, %r119;
	mov.b32 	%f240, %r166;
	add.f32 	%f241, %f239, %f240;
	st.local.f32 	[%rd3+12], %f241;
	st.shared.f32 	[%r14], %f241;
	bar.sync 	0;
	@%p1 bra 	$L__BB51_19;

	ld.shared.f32 	%f242, [%r4];
	mov.b32 	%r167, %f242;
	mov.u32 	%r168, 31;
	mov.u32 	%r169, 16;
	mov.u32 	%r170, -1;
	shfl.sync.bfly.b32 	%r171|%p46, %r167, %r169, %r168, %r170;
	mov.b32 	%f243, %r171;
	add.f32 	%f244, %f242, %f243;
	mov.b32 	%r172, %f244;
	mov.u32 	%r173, 8;
	shfl.sync.bfly.b32 	%r174|%p47, %r172, %r173, %r168, %r170;
	mov.b32 	%f245, %r174;
	add.f32 	%f246, %f244, %f245;
	mov.b32 	%r175, %f246;
	mov.u32 	%r176, 4;
	shfl.sync.bfly.b32 	%r177|%p48, %r175, %r176, %r168, %r170;
	mov.b32 	%f247, %r177;
	add.f32 	%f248, %f246, %f247;
	mov.b32 	%r178, %f248;
	mov.u32 	%r179, 2;
	shfl.sync.bfly.b32 	%r180|%p49, %r178, %r179, %r168, %r170;
	mov.b32 	%f249, %r180;
	add.f32 	%f250, %f248, %f249;
	mov.b32 	%r181, %f250;
	mov.u32 	%r182, 1;
	shfl.sync.bfly.b32 	%r183|%p50, %r181, %r182, %r168, %r170;
	mov.b32 	%f251, %r183;
	add.f32 	%f252, %f250, %f251;
	st.local.f32 	[%rd3+12], %f252;

$L__BB51_19:
	bar.sync 	0;
	setp.gt.s32 	%p51, %r3, 3;
	@%p51 bra 	$L__BB51_21;

	mul.wide.s32 	%rd70, %r3, 4;
	add.s64 	%rd71, %rd3, %rd70;
	ld.local.f32 	%f253, [%rd71];
	mad.lo.s32 	%r184, %r3, %r17, %r2;
	cvt.s64.s32 	%rd72, %r184;
	mul.lo.s32 	%r185, %r1, %r18;
	cvt.s64.s32 	%rd73, %r185;
	add.s64 	%rd74, %rd73, %rd72;
	cvta.to.global.u64 	%rd75, %rd33;
	shl.b64 	%rd76, %rd74, 2;
	add.s64 	%rd77, %rd75, %rd76;
	st.global.f32 	[%rd77], %f253;

$L__BB51_21:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_5_bs_224
.visible .entry ggml_matvec_f32_ncols_5_bs_224(
	.param .u64 ggml_matvec_f32_ncols_5_bs_224_param_0,
	.param .u64 ggml_matvec_f32_ncols_5_bs_224_param_1,
	.param .u64 ggml_matvec_f32_ncols_5_bs_224_param_2,
	.param .u32 ggml_matvec_f32_ncols_5_bs_224_param_3,
	.param .u32 ggml_matvec_f32_ncols_5_bs_224_param_4,
	.param .u32 ggml_matvec_f32_ncols_5_bs_224_param_5,
	.param .u32 ggml_matvec_f32_ncols_5_bs_224_param_6,
	.param .u32 ggml_matvec_f32_ncols_5_bs_224_param_7,
	.param .u32 ggml_matvec_f32_ncols_5_bs_224_param_8,
	.param .u32 ggml_matvec_f32_ncols_5_bs_224_param_9,
	.param .u32 ggml_matvec_f32_ncols_5_bs_224_param_10,
	.param .u32 ggml_matvec_f32_ncols_5_bs_224_param_11
)
{
	.local .align 4 .b8 	__local_depot52[20];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<63>;
	.reg .f32 	%f<332>;
	.reg .b32 	%r<226>;
	.reg .b64 	%rd<76>;


	mov.u64 	%SPL, __local_depot52;
	ld.param.u64 	%rd28, [ggml_matvec_f32_ncols_5_bs_224_param_0];
	ld.param.u64 	%rd29, [ggml_matvec_f32_ncols_5_bs_224_param_1];
	ld.param.u64 	%rd27, [ggml_matvec_f32_ncols_5_bs_224_param_2];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_5_bs_224_param_3];
	ld.param.u32 	%r19, [ggml_matvec_f32_ncols_5_bs_224_param_5];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_5_bs_224_param_6];
	ld.param.u32 	%r17, [ggml_matvec_f32_ncols_5_bs_224_param_7];
	ld.param.u32 	%r20, [ggml_matvec_f32_ncols_5_bs_224_param_8];
	ld.param.u32 	%r21, [ggml_matvec_f32_ncols_5_bs_224_param_9];
	ld.param.u32 	%r22, [ggml_matvec_f32_ncols_5_bs_224_param_10];
	ld.param.u32 	%r18, [ggml_matvec_f32_ncols_5_bs_224_param_11];
	cvta.to.global.u64 	%rd75, %rd29;
	cvta.to.global.u64 	%rd2, %rd28;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r23, %r1, %r20;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r24, %r2, %r19;
	mad.lo.s32 	%r25, %r23, %r21, %r24;
	cvt.s64.s32 	%rd4, %r25;
	mul.lo.s32 	%r26, %r1, %r22;
	cvt.s64.s32 	%rd5, %r26;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r27, %r3, 2;
	mov.u32 	%r28, data_mmv;
	add.s32 	%r4, %r28, %r27;
	@%p1 bra 	$L__BB52_2;

	mov.u32 	%r29, 0;
	st.shared.u32 	[%r4], %r29;

$L__BB52_2:
	bar.sync 	0;
	mov.f32 	%f327, 0f00000000;
	mov.u32 	%r30, 0;
	st.local.u32 	[%rd3], %r30;
	st.local.u32 	[%rd3+4], %r30;
	st.local.u32 	[%rd3+8], %r30;
	st.local.u32 	[%rd3+12], %r30;
	st.local.u32 	[%rd3+16], %r30;
	setp.ge.s32 	%p2, %r3, %r15;
	mov.f32 	%f328, %f327;
	mov.f32 	%f329, %f327;
	mov.f32 	%f330, %f327;
	mov.f32 	%f331, %f327;
	@%p2 bra 	$L__BB52_11;

	not.b32 	%r31, %r3;
	add.s32 	%r5, %r31, %r15;
	shr.u32 	%r32, %r5, 5;
	mul.wide.u32 	%rd31, %r32, 613566757;
	shr.u64 	%rd32, %rd31, 32;
	cvt.u32.u64 	%r33, %rd32;
	add.s32 	%r34, %r33, 1;
	and.b32  	%r223, %r34, 3;
	setp.eq.s32 	%p3, %r223, 0;
	mov.f32 	%f327, 0f00000000;
	mov.u32 	%r224, %r3;
	@%p3 bra 	$L__BB52_7;

	shl.b32 	%r35, %r16, 1;
	mad.lo.s32 	%r36, %r16, 3, %r3;
	mul.wide.s32 	%rd33, %r36, 8;
	shl.b64 	%rd34, %rd5, 2;
	add.s64 	%rd7, %rd33, %rd34;
	mul.wide.s32 	%rd35, %r3, 8;
	mul.wide.s32 	%rd36, %r16, 8;
	add.s64 	%rd37, %rd35, %rd36;
	add.s64 	%rd8, %rd37, %rd34;
	add.s64 	%rd9, %rd35, %rd34;
	mul.wide.s32 	%rd38, %r3, 2;
	add.s64 	%rd39, %rd38, %rd4;
	shl.b64 	%rd40, %rd39, 2;
	add.s64 	%rd72, %rd2, %rd40;
	mul.wide.s32 	%rd11, %r35, 8;
	mov.f32 	%f327, 0f00000000;
	mov.u64 	%rd73, %rd75;
	mov.f32 	%f328, %f327;
	mov.f32 	%f329, %f327;
	mov.f32 	%f330, %f327;
	mov.f32 	%f331, %f327;
	mov.u32 	%r224, %r3;

$L__BB52_5:
	.pragma "nounroll";
	ld.global.nc.v2.f32 	{%f46, %f47}, [%rd72];
	add.s64 	%rd41, %rd73, %rd9;
	ld.global.nc.v2.f32 	{%f50, %f51}, [%rd41];
	fma.rn.f32 	%f54, %f46, %f50, %f331;
	fma.rn.f32 	%f331, %f47, %f51, %f54;
	add.s64 	%rd42, %rd73, %rd8;
	ld.global.nc.v2.f32 	{%f55, %f56}, [%rd42];
	fma.rn.f32 	%f59, %f46, %f55, %f330;
	fma.rn.f32 	%f330, %f47, %f56, %f59;
	add.s64 	%rd43, %rd41, %rd11;
	ld.global.nc.v2.f32 	{%f60, %f61}, [%rd43];
	fma.rn.f32 	%f64, %f46, %f60, %f329;
	fma.rn.f32 	%f329, %f47, %f61, %f64;
	add.s64 	%rd44, %rd73, %rd7;
	ld.global.nc.v2.f32 	{%f65, %f66}, [%rd44];
	fma.rn.f32 	%f69, %f46, %f65, %f328;
	fma.rn.f32 	%f328, %f47, %f66, %f69;
	add.s64 	%rd45, %rd43, %rd11;
	ld.global.nc.v2.f32 	{%f70, %f71}, [%rd45];
	fma.rn.f32 	%f74, %f46, %f70, %f327;
	fma.rn.f32 	%f327, %f47, %f71, %f74;
	add.s32 	%r224, %r224, 224;
	add.s64 	%rd73, %rd73, 1792;
	add.s64 	%rd72, %rd72, 1792;
	add.s32 	%r223, %r223, -1;
	setp.ne.s32 	%p4, %r223, 0;
	@%p4 bra 	$L__BB52_5;

	st.local.f32 	[%rd3], %f331;
	st.local.f32 	[%rd3+4], %f330;
	st.local.f32 	[%rd3+8], %f329;
	st.local.f32 	[%rd3+12], %f328;
	st.local.f32 	[%rd3+16], %f327;

$L__BB52_7:
	setp.lt.u32 	%p5, %r5, 672;
	@%p5 bra 	$L__BB52_11;

	add.s32 	%r37, %r224, %r16;
	shl.b32 	%r38, %r16, 1;
	add.s32 	%r39, %r224, %r38;
	mad.lo.s32 	%r40, %r16, 3, %r224;
	shl.b32 	%r41, %r16, 2;
	add.s32 	%r42, %r224, %r41;
	add.s32 	%r43, %r37, 224;
	mul.wide.s32 	%rd46, %r43, 8;
	shl.b64 	%rd47, %rd5, 2;
	add.s64 	%rd16, %rd46, %rd47;
	mul.wide.s32 	%rd48, %r39, 8;
	add.s64 	%rd17, %rd48, %rd47;
	mul.wide.s32 	%rd49, %r40, 8;
	add.s64 	%rd18, %rd49, %rd47;
	mul.wide.s32 	%rd50, %r42, 8;
	add.s64 	%rd19, %rd50, %rd47;
	mul.wide.s32 	%rd51, %r224, 2;
	add.s64 	%rd52, %rd51, %rd4;
	shl.b64 	%rd53, %rd52, 2;
	add.s64 	%rd54, %rd2, %rd53;
	add.s64 	%rd74, %rd54, 3584;
	mul.wide.s32 	%rd55, %r224, 8;
	add.s64 	%rd21, %rd55, %rd47;
	mul.wide.s32 	%rd56, %r16, 8;
	add.s64 	%rd57, %rd55, %rd56;
	add.s64 	%rd22, %rd57, %rd47;

$L__BB52_9:
	ld.global.nc.v2.f32 	{%f75, %f76}, [%rd74+-3584];
	add.s64 	%rd58, %rd75, %rd21;
	ld.global.nc.v2.f32 	{%f79, %f80}, [%rd58];
	fma.rn.f32 	%f83, %f75, %f79, %f331;
	fma.rn.f32 	%f84, %f76, %f80, %f83;
	add.s64 	%rd59, %rd75, %rd22;
	ld.global.nc.v2.f32 	{%f85, %f86}, [%rd59];
	fma.rn.f32 	%f89, %f75, %f85, %f330;
	fma.rn.f32 	%f90, %f76, %f86, %f89;
	add.s64 	%rd60, %rd75, %rd17;
	ld.global.nc.v2.f32 	{%f91, %f92}, [%rd60];
	fma.rn.f32 	%f95, %f75, %f91, %f329;
	fma.rn.f32 	%f96, %f76, %f92, %f95;
	add.s64 	%rd61, %rd75, %rd18;
	ld.global.nc.v2.f32 	{%f97, %f98}, [%rd61];
	fma.rn.f32 	%f101, %f75, %f97, %f328;
	fma.rn.f32 	%f102, %f76, %f98, %f101;
	add.s64 	%rd62, %rd75, %rd19;
	ld.global.nc.v2.f32 	{%f103, %f104}, [%rd62];
	fma.rn.f32 	%f107, %f75, %f103, %f327;
	fma.rn.f32 	%f108, %f76, %f104, %f107;
	ld.global.nc.v2.f32 	{%f109, %f110}, [%rd74+-1792];
	ld.global.nc.v2.f32 	{%f113, %f114}, [%rd58+1792];
	fma.rn.f32 	%f117, %f109, %f113, %f84;
	fma.rn.f32 	%f118, %f110, %f114, %f117;
	add.s64 	%rd63, %rd75, %rd16;
	ld.global.nc.v2.f32 	{%f119, %f120}, [%rd63];
	fma.rn.f32 	%f123, %f109, %f119, %f90;
	fma.rn.f32 	%f124, %f110, %f120, %f123;
	ld.global.nc.v2.f32 	{%f125, %f126}, [%rd60+1792];
	fma.rn.f32 	%f129, %f109, %f125, %f96;
	fma.rn.f32 	%f130, %f110, %f126, %f129;
	ld.global.nc.v2.f32 	{%f131, %f132}, [%rd61+1792];
	fma.rn.f32 	%f135, %f109, %f131, %f102;
	fma.rn.f32 	%f136, %f110, %f132, %f135;
	ld.global.nc.v2.f32 	{%f137, %f138}, [%rd62+1792];
	fma.rn.f32 	%f141, %f109, %f137, %f108;
	fma.rn.f32 	%f142, %f110, %f138, %f141;
	ld.global.nc.v2.f32 	{%f143, %f144}, [%rd74];
	ld.global.nc.v2.f32 	{%f147, %f148}, [%rd58+3584];
	fma.rn.f32 	%f151, %f143, %f147, %f118;
	fma.rn.f32 	%f152, %f144, %f148, %f151;
	ld.global.nc.v2.f32 	{%f153, %f154}, [%rd63+1792];
	fma.rn.f32 	%f157, %f143, %f153, %f124;
	fma.rn.f32 	%f158, %f144, %f154, %f157;
	ld.global.nc.v2.f32 	{%f159, %f160}, [%rd60+3584];
	fma.rn.f32 	%f163, %f143, %f159, %f130;
	fma.rn.f32 	%f164, %f144, %f160, %f163;
	ld.global.nc.v2.f32 	{%f165, %f166}, [%rd61+3584];
	fma.rn.f32 	%f169, %f143, %f165, %f136;
	fma.rn.f32 	%f170, %f144, %f166, %f169;
	ld.global.nc.v2.f32 	{%f171, %f172}, [%rd62+3584];
	fma.rn.f32 	%f175, %f143, %f171, %f142;
	fma.rn.f32 	%f176, %f144, %f172, %f175;
	ld.global.nc.v2.f32 	{%f177, %f178}, [%rd74+1792];
	ld.global.nc.v2.f32 	{%f181, %f182}, [%rd58+5376];
	fma.rn.f32 	%f185, %f177, %f181, %f152;
	fma.rn.f32 	%f331, %f178, %f182, %f185;
	ld.global.nc.v2.f32 	{%f186, %f187}, [%rd63+3584];
	fma.rn.f32 	%f190, %f177, %f186, %f158;
	fma.rn.f32 	%f330, %f178, %f187, %f190;
	ld.global.nc.v2.f32 	{%f191, %f192}, [%rd60+5376];
	fma.rn.f32 	%f195, %f177, %f191, %f164;
	fma.rn.f32 	%f329, %f178, %f192, %f195;
	ld.global.nc.v2.f32 	{%f196, %f197}, [%rd61+5376];
	fma.rn.f32 	%f200, %f177, %f196, %f170;
	fma.rn.f32 	%f328, %f178, %f197, %f200;
	ld.global.nc.v2.f32 	{%f201, %f202}, [%rd62+5376];
	fma.rn.f32 	%f205, %f177, %f201, %f176;
	fma.rn.f32 	%f327, %f178, %f202, %f205;
	add.s64 	%rd75, %rd75, 7168;
	add.s64 	%rd74, %rd74, 7168;
	add.s32 	%r224, %r224, 896;
	setp.lt.s32 	%p6, %r224, %r15;
	@%p6 bra 	$L__BB52_9;

	st.local.f32 	[%rd3], %f331;
	st.local.f32 	[%rd3+4], %f330;
	st.local.f32 	[%rd3+8], %f329;
	st.local.f32 	[%rd3+12], %f328;
	st.local.f32 	[%rd3+16], %f327;

$L__BB52_11:
	shr.s32 	%r44, %r3, 31;
	shr.u32 	%r45, %r44, 27;
	add.s32 	%r46, %r3, %r45;
	shr.s32 	%r47, %r46, 5;
	shl.b32 	%r48, %r47, 2;
	add.s32 	%r14, %r28, %r48;
	mov.u32 	%r50, 2;
	mov.b32 	%r51, %f331;
	mov.u32 	%r52, 31;
	mov.u32 	%r53, 16;
	mov.u32 	%r54, -1;
	shfl.sync.bfly.b32 	%r55|%p7, %r51, %r53, %r52, %r54;
	mov.b32 	%f206, %r55;
	add.f32 	%f207, %f331, %f206;
	mov.b32 	%r56, %f207;
	mov.u32 	%r57, 8;
	shfl.sync.bfly.b32 	%r58|%p8, %r56, %r57, %r52, %r54;
	mov.b32 	%f208, %r58;
	add.f32 	%f209, %f207, %f208;
	mov.b32 	%r59, %f209;
	mov.u32 	%r60, 4;
	shfl.sync.bfly.b32 	%r61|%p9, %r59, %r60, %r52, %r54;
	mov.b32 	%f210, %r61;
	add.f32 	%f211, %f209, %f210;
	mov.b32 	%r62, %f211;
	shfl.sync.bfly.b32 	%r63|%p10, %r62, %r50, %r52, %r54;
	mov.b32 	%f212, %r63;
	add.f32 	%f213, %f211, %f212;
	mov.b32 	%r64, %f213;
	mov.u32 	%r65, 1;
	shfl.sync.bfly.b32 	%r66|%p11, %r64, %r65, %r52, %r54;
	mov.b32 	%f214, %r66;
	add.f32 	%f215, %f213, %f214;
	st.local.f32 	[%rd3], %f215;
	st.shared.f32 	[%r14], %f215;
	bar.sync 	0;
	@%p1 bra 	$L__BB52_13;

	ld.shared.f32 	%f216, [%r4];
	mov.b32 	%r67, %f216;
	shfl.sync.bfly.b32 	%r71|%p13, %r67, %r53, %r52, %r54;
	mov.b32 	%f217, %r71;
	add.f32 	%f218, %f216, %f217;
	mov.b32 	%r72, %f218;
	shfl.sync.bfly.b32 	%r74|%p14, %r72, %r57, %r52, %r54;
	mov.b32 	%f219, %r74;
	add.f32 	%f220, %f218, %f219;
	mov.b32 	%r75, %f220;
	shfl.sync.bfly.b32 	%r77|%p15, %r75, %r60, %r52, %r54;
	mov.b32 	%f221, %r77;
	add.f32 	%f222, %f220, %f221;
	mov.b32 	%r78, %f222;
	shfl.sync.bfly.b32 	%r80|%p16, %r78, %r50, %r52, %r54;
	mov.b32 	%f223, %r80;
	add.f32 	%f224, %f222, %f223;
	mov.b32 	%r81, %f224;
	shfl.sync.bfly.b32 	%r83|%p17, %r81, %r65, %r52, %r54;
	mov.b32 	%f225, %r83;
	add.f32 	%f226, %f224, %f225;
	st.local.f32 	[%rd3], %f226;

$L__BB52_13:
	bar.sync 	0;
	mov.b32 	%r84, %f330;
	shfl.sync.bfly.b32 	%r88|%p19, %r84, %r53, %r52, %r54;
	mov.b32 	%f227, %r88;
	add.f32 	%f228, %f330, %f227;
	mov.b32 	%r89, %f228;
	shfl.sync.bfly.b32 	%r91|%p20, %r89, %r57, %r52, %r54;
	mov.b32 	%f229, %r91;
	add.f32 	%f230, %f228, %f229;
	mov.b32 	%r92, %f230;
	shfl.sync.bfly.b32 	%r94|%p21, %r92, %r60, %r52, %r54;
	mov.b32 	%f231, %r94;
	add.f32 	%f232, %f230, %f231;
	mov.b32 	%r95, %f232;
	shfl.sync.bfly.b32 	%r97|%p22, %r95, %r50, %r52, %r54;
	mov.b32 	%f233, %r97;
	add.f32 	%f234, %f232, %f233;
	mov.b32 	%r98, %f234;
	shfl.sync.bfly.b32 	%r100|%p23, %r98, %r65, %r52, %r54;
	mov.b32 	%f235, %r100;
	add.f32 	%f236, %f234, %f235;
	st.local.f32 	[%rd3+4], %f236;
	st.shared.f32 	[%r14], %f236;
	bar.sync 	0;
	@%p1 bra 	$L__BB52_15;

	ld.shared.f32 	%f237, [%r4];
	mov.b32 	%r101, %f237;
	mov.u32 	%r102, 31;
	mov.u32 	%r103, 16;
	mov.u32 	%r104, -1;
	shfl.sync.bfly.b32 	%r105|%p24, %r101, %r103, %r102, %r104;
	mov.b32 	%f238, %r105;
	add.f32 	%f239, %f237, %f238;
	mov.b32 	%r106, %f239;
	mov.u32 	%r107, 8;
	shfl.sync.bfly.b32 	%r108|%p25, %r106, %r107, %r102, %r104;
	mov.b32 	%f240, %r108;
	add.f32 	%f241, %f239, %f240;
	mov.b32 	%r109, %f241;
	mov.u32 	%r110, 4;
	shfl.sync.bfly.b32 	%r111|%p26, %r109, %r110, %r102, %r104;
	mov.b32 	%f242, %r111;
	add.f32 	%f243, %f241, %f242;
	mov.b32 	%r112, %f243;
	mov.u32 	%r113, 2;
	shfl.sync.bfly.b32 	%r114|%p27, %r112, %r113, %r102, %r104;
	mov.b32 	%f244, %r114;
	add.f32 	%f245, %f243, %f244;
	mov.b32 	%r115, %f245;
	mov.u32 	%r116, 1;
	shfl.sync.bfly.b32 	%r117|%p28, %r115, %r116, %r102, %r104;
	mov.b32 	%f246, %r117;
	add.f32 	%f247, %f245, %f246;
	st.local.f32 	[%rd3+4], %f247;

$L__BB52_15:
	bar.sync 	0;
	mov.b32 	%r118, %f329;
	mov.u32 	%r119, 31;
	mov.u32 	%r120, 16;
	mov.u32 	%r121, -1;
	shfl.sync.bfly.b32 	%r122|%p30, %r118, %r120, %r119, %r121;
	mov.b32 	%f248, %r122;
	add.f32 	%f249, %f329, %f248;
	mov.b32 	%r123, %f249;
	mov.u32 	%r124, 8;
	shfl.sync.bfly.b32 	%r125|%p31, %r123, %r124, %r119, %r121;
	mov.b32 	%f250, %r125;
	add.f32 	%f251, %f249, %f250;
	mov.b32 	%r126, %f251;
	mov.u32 	%r127, 4;
	shfl.sync.bfly.b32 	%r128|%p32, %r126, %r127, %r119, %r121;
	mov.b32 	%f252, %r128;
	add.f32 	%f253, %f251, %f252;
	mov.b32 	%r129, %f253;
	mov.u32 	%r130, 2;
	shfl.sync.bfly.b32 	%r131|%p33, %r129, %r130, %r119, %r121;
	mov.b32 	%f254, %r131;
	add.f32 	%f255, %f253, %f254;
	mov.b32 	%r132, %f255;
	mov.u32 	%r133, 1;
	shfl.sync.bfly.b32 	%r134|%p34, %r132, %r133, %r119, %r121;
	mov.b32 	%f256, %r134;
	add.f32 	%f257, %f255, %f256;
	st.local.f32 	[%rd3+8], %f257;
	st.shared.f32 	[%r14], %f257;
	bar.sync 	0;
	@%p1 bra 	$L__BB52_17;

	ld.shared.f32 	%f258, [%r4];
	mov.b32 	%r135, %f258;
	shfl.sync.bfly.b32 	%r139|%p35, %r135, %r120, %r119, %r121;
	mov.b32 	%f259, %r139;
	add.f32 	%f260, %f258, %f259;
	mov.b32 	%r140, %f260;
	shfl.sync.bfly.b32 	%r142|%p36, %r140, %r124, %r119, %r121;
	mov.b32 	%f261, %r142;
	add.f32 	%f262, %f260, %f261;
	mov.b32 	%r143, %f262;
	shfl.sync.bfly.b32 	%r145|%p37, %r143, %r127, %r119, %r121;
	mov.b32 	%f263, %r145;
	add.f32 	%f264, %f262, %f263;
	mov.b32 	%r146, %f264;
	shfl.sync.bfly.b32 	%r148|%p38, %r146, %r130, %r119, %r121;
	mov.b32 	%f265, %r148;
	add.f32 	%f266, %f264, %f265;
	mov.b32 	%r149, %f266;
	shfl.sync.bfly.b32 	%r151|%p39, %r149, %r133, %r119, %r121;
	mov.b32 	%f267, %r151;
	add.f32 	%f268, %f266, %f267;
	st.local.f32 	[%rd3+8], %f268;

$L__BB52_17:
	bar.sync 	0;
	mov.b32 	%r152, %f328;
	shfl.sync.bfly.b32 	%r156|%p41, %r152, %r120, %r119, %r121;
	mov.b32 	%f269, %r156;
	add.f32 	%f270, %f328, %f269;
	mov.b32 	%r157, %f270;
	shfl.sync.bfly.b32 	%r159|%p42, %r157, %r124, %r119, %r121;
	mov.b32 	%f271, %r159;
	add.f32 	%f272, %f270, %f271;
	mov.b32 	%r160, %f272;
	shfl.sync.bfly.b32 	%r162|%p43, %r160, %r127, %r119, %r121;
	mov.b32 	%f273, %r162;
	add.f32 	%f274, %f272, %f273;
	mov.b32 	%r163, %f274;
	shfl.sync.bfly.b32 	%r165|%p44, %r163, %r130, %r119, %r121;
	mov.b32 	%f275, %r165;
	add.f32 	%f276, %f274, %f275;
	mov.b32 	%r166, %f276;
	shfl.sync.bfly.b32 	%r168|%p45, %r166, %r133, %r119, %r121;
	mov.b32 	%f277, %r168;
	add.f32 	%f278, %f276, %f277;
	st.local.f32 	[%rd3+12], %f278;
	st.shared.f32 	[%r14], %f278;
	bar.sync 	0;
	@%p1 bra 	$L__BB52_19;

	ld.shared.f32 	%f279, [%r4];
	mov.b32 	%r169, %f279;
	mov.u32 	%r170, 31;
	mov.u32 	%r171, 16;
	mov.u32 	%r172, -1;
	shfl.sync.bfly.b32 	%r173|%p46, %r169, %r171, %r170, %r172;
	mov.b32 	%f280, %r173;
	add.f32 	%f281, %f279, %f280;
	mov.b32 	%r174, %f281;
	mov.u32 	%r175, 8;
	shfl.sync.bfly.b32 	%r176|%p47, %r174, %r175, %r170, %r172;
	mov.b32 	%f282, %r176;
	add.f32 	%f283, %f281, %f282;
	mov.b32 	%r177, %f283;
	mov.u32 	%r178, 4;
	shfl.sync.bfly.b32 	%r179|%p48, %r177, %r178, %r170, %r172;
	mov.b32 	%f284, %r179;
	add.f32 	%f285, %f283, %f284;
	mov.b32 	%r180, %f285;
	mov.u32 	%r181, 2;
	shfl.sync.bfly.b32 	%r182|%p49, %r180, %r181, %r170, %r172;
	mov.b32 	%f286, %r182;
	add.f32 	%f287, %f285, %f286;
	mov.b32 	%r183, %f287;
	mov.u32 	%r184, 1;
	shfl.sync.bfly.b32 	%r185|%p50, %r183, %r184, %r170, %r172;
	mov.b32 	%f288, %r185;
	add.f32 	%f289, %f287, %f288;
	st.local.f32 	[%rd3+12], %f289;

$L__BB52_19:
	bar.sync 	0;
	mov.b32 	%r186, %f327;
	mov.u32 	%r187, 31;
	mov.u32 	%r188, 16;
	mov.u32 	%r189, -1;
	shfl.sync.bfly.b32 	%r190|%p52, %r186, %r188, %r187, %r189;
	mov.b32 	%f290, %r190;
	add.f32 	%f291, %f327, %f290;
	mov.b32 	%r191, %f291;
	mov.u32 	%r192, 8;
	shfl.sync.bfly.b32 	%r193|%p53, %r191, %r192, %r187, %r189;
	mov.b32 	%f292, %r193;
	add.f32 	%f293, %f291, %f292;
	mov.b32 	%r194, %f293;
	mov.u32 	%r195, 4;
	shfl.sync.bfly.b32 	%r196|%p54, %r194, %r195, %r187, %r189;
	mov.b32 	%f294, %r196;
	add.f32 	%f295, %f293, %f294;
	mov.b32 	%r197, %f295;
	mov.u32 	%r198, 2;
	shfl.sync.bfly.b32 	%r199|%p55, %r197, %r198, %r187, %r189;
	mov.b32 	%f296, %r199;
	add.f32 	%f297, %f295, %f296;
	mov.b32 	%r200, %f297;
	mov.u32 	%r201, 1;
	shfl.sync.bfly.b32 	%r202|%p56, %r200, %r201, %r187, %r189;
	mov.b32 	%f298, %r202;
	add.f32 	%f299, %f297, %f298;
	st.local.f32 	[%rd3+16], %f299;
	st.shared.f32 	[%r14], %f299;
	bar.sync 	0;
	@%p1 bra 	$L__BB52_21;

	ld.shared.f32 	%f300, [%r4];
	mov.b32 	%r203, %f300;
	shfl.sync.bfly.b32 	%r207|%p57, %r203, %r188, %r187, %r189;
	mov.b32 	%f301, %r207;
	add.f32 	%f302, %f300, %f301;
	mov.b32 	%r208, %f302;
	shfl.sync.bfly.b32 	%r210|%p58, %r208, %r192, %r187, %r189;
	mov.b32 	%f303, %r210;
	add.f32 	%f304, %f302, %f303;
	mov.b32 	%r211, %f304;
	shfl.sync.bfly.b32 	%r213|%p59, %r211, %r195, %r187, %r189;
	mov.b32 	%f305, %r213;
	add.f32 	%f306, %f304, %f305;
	mov.b32 	%r214, %f306;
	shfl.sync.bfly.b32 	%r216|%p60, %r214, %r198, %r187, %r189;
	mov.b32 	%f307, %r216;
	add.f32 	%f308, %f306, %f307;
	mov.b32 	%r217, %f308;
	shfl.sync.bfly.b32 	%r219|%p61, %r217, %r201, %r187, %r189;
	mov.b32 	%f309, %r219;
	add.f32 	%f310, %f308, %f309;
	st.local.f32 	[%rd3+16], %f310;

$L__BB52_21:
	bar.sync 	0;
	setp.gt.s32 	%p62, %r3, 4;
	@%p62 bra 	$L__BB52_23;

	mul.wide.s32 	%rd64, %r3, 4;
	add.s64 	%rd65, %rd3, %rd64;
	ld.local.f32 	%f311, [%rd65];
	mad.lo.s32 	%r220, %r3, %r17, %r2;
	cvt.s64.s32 	%rd66, %r220;
	mul.lo.s32 	%r221, %r1, %r18;
	cvt.s64.s32 	%rd67, %r221;
	add.s64 	%rd68, %rd67, %rd66;
	cvta.to.global.u64 	%rd69, %rd27;
	shl.b64 	%rd70, %rd68, 2;
	add.s64 	%rd71, %rd69, %rd70;
	st.global.f32 	[%rd71], %f311;

$L__BB52_23:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_6_bs_224
.visible .entry ggml_matvec_f32_ncols_6_bs_224(
	.param .u64 ggml_matvec_f32_ncols_6_bs_224_param_0,
	.param .u64 ggml_matvec_f32_ncols_6_bs_224_param_1,
	.param .u64 ggml_matvec_f32_ncols_6_bs_224_param_2,
	.param .u32 ggml_matvec_f32_ncols_6_bs_224_param_3,
	.param .u32 ggml_matvec_f32_ncols_6_bs_224_param_4,
	.param .u32 ggml_matvec_f32_ncols_6_bs_224_param_5,
	.param .u32 ggml_matvec_f32_ncols_6_bs_224_param_6,
	.param .u32 ggml_matvec_f32_ncols_6_bs_224_param_7,
	.param .u32 ggml_matvec_f32_ncols_6_bs_224_param_8,
	.param .u32 ggml_matvec_f32_ncols_6_bs_224_param_9,
	.param .u32 ggml_matvec_f32_ncols_6_bs_224_param_10,
	.param .u32 ggml_matvec_f32_ncols_6_bs_224_param_11
)
{
	.local .align 8 .b8 	__local_depot53[24];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<75>;
	.reg .f32 	%f<296>;
	.reg .b32 	%r<252>;
	.reg .b64 	%rd<70>;


	mov.u64 	%SPL, __local_depot53;
	ld.param.u64 	%rd20, [ggml_matvec_f32_ncols_6_bs_224_param_0];
	ld.param.u64 	%rd21, [ggml_matvec_f32_ncols_6_bs_224_param_1];
	ld.param.u64 	%rd19, [ggml_matvec_f32_ncols_6_bs_224_param_2];
	ld.param.u32 	%r11, [ggml_matvec_f32_ncols_6_bs_224_param_3];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_6_bs_224_param_5];
	ld.param.u32 	%r12, [ggml_matvec_f32_ncols_6_bs_224_param_6];
	ld.param.u32 	%r13, [ggml_matvec_f32_ncols_6_bs_224_param_7];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_6_bs_224_param_8];
	ld.param.u32 	%r17, [ggml_matvec_f32_ncols_6_bs_224_param_9];
	ld.param.u32 	%r18, [ggml_matvec_f32_ncols_6_bs_224_param_10];
	ld.param.u32 	%r14, [ggml_matvec_f32_ncols_6_bs_224_param_11];
	cvta.to.global.u64 	%rd69, %rd21;
	add.u64 	%rd2, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r19, %r1, %r16;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r20, %r2, %r15;
	mad.lo.s32 	%r21, %r19, %r17, %r20;
	cvt.s64.s32 	%rd3, %r21;
	cvta.to.global.u64 	%rd4, %rd20;
	mul.lo.s32 	%r22, %r1, %r18;
	cvt.s64.s32 	%rd5, %r22;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r23, %r3, 2;
	mov.u32 	%r24, data_mmv;
	add.s32 	%r4, %r24, %r23;
	@%p1 bra 	$L__BB53_2;

	mov.u32 	%r25, 0;
	st.shared.u32 	[%r4], %r25;

$L__BB53_2:
	bar.sync 	0;
	mov.f32 	%f290, 0f00000000;
	st.local.v2.f32 	[%rd2], {%f290, %f290};
	st.local.v2.f32 	[%rd2+8], {%f290, %f290};
	st.local.v2.f32 	[%rd2+16], {%f290, %f290};
	setp.ge.s32 	%p2, %r3, %r11;
	mov.f32 	%f291, %f290;
	mov.f32 	%f292, %f290;
	mov.f32 	%f293, %f290;
	mov.f32 	%f294, %f290;
	mov.f32 	%f295, %f290;
	@%p2 bra 	$L__BB53_9;

	not.b32 	%r26, %r3;
	add.s32 	%r5, %r26, %r11;
	shr.u32 	%r27, %r5, 5;
	mul.wide.u32 	%rd23, %r27, 613566757;
	shr.u64 	%rd24, %rd23, 32;
	and.b64  	%rd25, %rd24, 1;
	setp.eq.b64 	%p3, %rd25, 1;
	mov.pred 	%p4, 0;
	xor.pred  	%p5, %p3, %p4;
	mov.f32 	%f290, 0f00000000;
	mov.u32 	%r251, %r3;
	@%p5 bra 	$L__BB53_5;

	shl.b64 	%rd26, %rd5, 2;
	add.s64 	%rd27, %rd69, %rd26;
	shl.b64 	%rd28, %rd3, 2;
	add.s64 	%rd29, %rd4, %rd28;
	mul.wide.s32 	%rd30, %r3, 8;
	add.s64 	%rd31, %rd29, %rd30;
	ld.global.nc.v2.f32 	{%f43, %f44}, [%rd31];
	add.s64 	%rd32, %rd27, %rd30;
	ld.global.nc.v2.f32 	{%f47, %f48}, [%rd32];
	fma.rn.f32 	%f51, %f43, %f47, 0f00000000;
	fma.rn.f32 	%f295, %f44, %f48, %f51;
	mul.wide.s32 	%rd33, %r12, 8;
	add.s64 	%rd34, %rd32, %rd33;
	ld.global.nc.v2.f32 	{%f52, %f53}, [%rd34];
	fma.rn.f32 	%f56, %f43, %f52, 0f00000000;
	fma.rn.f32 	%f294, %f44, %f53, %f56;
	st.local.v2.f32 	[%rd2], {%f295, %f294};
	add.s32 	%r28, %r3, %r12;
	add.s32 	%r29, %r28, %r12;
	mul.wide.s32 	%rd35, %r29, 8;
	add.s64 	%rd36, %rd27, %rd35;
	ld.global.nc.v2.f32 	{%f57, %f58}, [%rd36];
	fma.rn.f32 	%f61, %f43, %f57, 0f00000000;
	fma.rn.f32 	%f293, %f44, %f58, %f61;
	add.s64 	%rd37, %rd36, %rd33;
	ld.global.nc.v2.f32 	{%f62, %f63}, [%rd37];
	fma.rn.f32 	%f66, %f43, %f62, 0f00000000;
	fma.rn.f32 	%f292, %f44, %f63, %f66;
	st.local.v2.f32 	[%rd2+8], {%f293, %f292};
	add.s64 	%rd38, %rd37, %rd33;
	ld.global.nc.v2.f32 	{%f67, %f68}, [%rd38];
	fma.rn.f32 	%f71, %f43, %f67, 0f00000000;
	fma.rn.f32 	%f291, %f44, %f68, %f71;
	add.s64 	%rd39, %rd38, %rd33;
	ld.global.nc.v2.f32 	{%f72, %f73}, [%rd39];
	fma.rn.f32 	%f76, %f43, %f72, 0f00000000;
	fma.rn.f32 	%f290, %f44, %f73, %f76;
	st.local.v2.f32 	[%rd2+16], {%f291, %f290};
	add.s32 	%r251, %r3, 224;

$L__BB53_5:
	setp.lt.u32 	%p6, %r5, 224;
	@%p6 bra 	$L__BB53_9;

	add.s32 	%r30, %r251, %r12;
	add.s32 	%r31, %r30, 224;
	mul.wide.s32 	%rd40, %r31, 8;
	shl.b64 	%rd41, %rd5, 2;
	add.s64 	%rd7, %rd40, %rd41;
	shl.b32 	%r32, %r12, 1;
	add.s32 	%r33, %r251, %r32;
	mad.lo.s32 	%r34, %r12, 3, %r251;
	shl.b32 	%r35, %r12, 2;
	add.s32 	%r36, %r251, %r35;
	mad.lo.s32 	%r37, %r12, 5, %r251;
	mul.wide.s32 	%rd42, %r33, 8;
	add.s64 	%rd8, %rd42, %rd41;
	mul.wide.s32 	%rd43, %r34, 8;
	add.s64 	%rd9, %rd43, %rd41;
	mul.wide.s32 	%rd44, %r36, 8;
	add.s64 	%rd10, %rd44, %rd41;
	mul.wide.s32 	%rd45, %r37, 8;
	add.s64 	%rd11, %rd45, %rd41;
	mul.wide.s32 	%rd46, %r251, 2;
	add.s64 	%rd47, %rd46, %rd3;
	shl.b64 	%rd48, %rd47, 2;
	add.s64 	%rd49, %rd4, %rd48;
	add.s64 	%rd68, %rd49, 1792;
	mul.wide.s32 	%rd50, %r251, 8;
	mul.wide.s32 	%rd51, %r12, 8;
	add.s64 	%rd52, %rd50, %rd51;
	add.s64 	%rd13, %rd52, %rd41;
	add.s64 	%rd14, %rd50, %rd41;

$L__BB53_7:
	ld.global.nc.v2.f32 	{%f77, %f78}, [%rd68+-1792];
	add.s64 	%rd53, %rd69, %rd14;
	ld.global.nc.v2.f32 	{%f81, %f82}, [%rd53];
	fma.rn.f32 	%f85, %f77, %f81, %f295;
	fma.rn.f32 	%f86, %f78, %f82, %f85;
	add.s64 	%rd54, %rd69, %rd13;
	ld.global.nc.v2.f32 	{%f87, %f88}, [%rd54];
	fma.rn.f32 	%f91, %f77, %f87, %f294;
	fma.rn.f32 	%f92, %f78, %f88, %f91;
	add.s64 	%rd55, %rd69, %rd8;
	ld.global.nc.v2.f32 	{%f93, %f94}, [%rd55];
	fma.rn.f32 	%f97, %f77, %f93, %f293;
	fma.rn.f32 	%f98, %f78, %f94, %f97;
	add.s64 	%rd56, %rd69, %rd9;
	ld.global.nc.v2.f32 	{%f99, %f100}, [%rd56];
	fma.rn.f32 	%f103, %f77, %f99, %f292;
	fma.rn.f32 	%f104, %f78, %f100, %f103;
	add.s64 	%rd57, %rd69, %rd10;
	ld.global.nc.v2.f32 	{%f105, %f106}, [%rd57];
	fma.rn.f32 	%f109, %f77, %f105, %f291;
	fma.rn.f32 	%f110, %f78, %f106, %f109;
	add.s64 	%rd58, %rd69, %rd11;
	ld.global.nc.v2.f32 	{%f111, %f112}, [%rd58];
	fma.rn.f32 	%f115, %f77, %f111, %f290;
	fma.rn.f32 	%f116, %f78, %f112, %f115;
	ld.global.nc.v2.f32 	{%f117, %f118}, [%rd68];
	ld.global.nc.v2.f32 	{%f121, %f122}, [%rd53+1792];
	fma.rn.f32 	%f125, %f117, %f121, %f86;
	fma.rn.f32 	%f295, %f118, %f122, %f125;
	add.s64 	%rd59, %rd69, %rd7;
	ld.global.nc.v2.f32 	{%f126, %f127}, [%rd59];
	fma.rn.f32 	%f130, %f117, %f126, %f92;
	fma.rn.f32 	%f294, %f118, %f127, %f130;
	ld.global.nc.v2.f32 	{%f131, %f132}, [%rd55+1792];
	fma.rn.f32 	%f135, %f117, %f131, %f98;
	fma.rn.f32 	%f293, %f118, %f132, %f135;
	ld.global.nc.v2.f32 	{%f136, %f137}, [%rd56+1792];
	fma.rn.f32 	%f140, %f117, %f136, %f104;
	fma.rn.f32 	%f292, %f118, %f137, %f140;
	ld.global.nc.v2.f32 	{%f141, %f142}, [%rd57+1792];
	fma.rn.f32 	%f145, %f117, %f141, %f110;
	fma.rn.f32 	%f291, %f118, %f142, %f145;
	ld.global.nc.v2.f32 	{%f146, %f147}, [%rd58+1792];
	fma.rn.f32 	%f150, %f117, %f146, %f116;
	fma.rn.f32 	%f290, %f118, %f147, %f150;
	add.s64 	%rd69, %rd69, 3584;
	add.s64 	%rd68, %rd68, 3584;
	add.s32 	%r251, %r251, 448;
	setp.lt.s32 	%p7, %r251, %r11;
	@%p7 bra 	$L__BB53_7;

	st.local.v2.f32 	[%rd2], {%f295, %f294};
	st.local.v2.f32 	[%rd2+8], {%f293, %f292};
	st.local.v2.f32 	[%rd2+16], {%f291, %f290};

$L__BB53_9:
	shr.s32 	%r38, %r3, 31;
	shr.u32 	%r39, %r38, 27;
	add.s32 	%r40, %r3, %r39;
	shr.s32 	%r41, %r40, 5;
	shl.b32 	%r42, %r41, 2;
	add.s32 	%r10, %r24, %r42;
	mov.u32 	%r44, 2;
	mov.b32 	%r45, %f295;
	mov.u32 	%r46, 31;
	mov.u32 	%r47, 16;
	mov.u32 	%r48, -1;
	shfl.sync.bfly.b32 	%r49|%p8, %r45, %r47, %r46, %r48;
	mov.b32 	%f151, %r49;
	add.f32 	%f152, %f295, %f151;
	mov.b32 	%r50, %f152;
	mov.u32 	%r51, 8;
	shfl.sync.bfly.b32 	%r52|%p9, %r50, %r51, %r46, %r48;
	mov.b32 	%f153, %r52;
	add.f32 	%f154, %f152, %f153;
	mov.b32 	%r53, %f154;
	mov.u32 	%r54, 4;
	shfl.sync.bfly.b32 	%r55|%p10, %r53, %r54, %r46, %r48;
	mov.b32 	%f155, %r55;
	add.f32 	%f156, %f154, %f155;
	mov.b32 	%r56, %f156;
	shfl.sync.bfly.b32 	%r57|%p11, %r56, %r44, %r46, %r48;
	mov.b32 	%f157, %r57;
	add.f32 	%f158, %f156, %f157;
	mov.b32 	%r58, %f158;
	mov.u32 	%r59, 1;
	shfl.sync.bfly.b32 	%r60|%p12, %r58, %r59, %r46, %r48;
	mov.b32 	%f159, %r60;
	add.f32 	%f160, %f158, %f159;
	st.local.f32 	[%rd2], %f160;
	st.shared.f32 	[%r10], %f160;
	bar.sync 	0;
	@%p1 bra 	$L__BB53_11;

	ld.shared.f32 	%f161, [%r4];
	mov.b32 	%r61, %f161;
	shfl.sync.bfly.b32 	%r65|%p14, %r61, %r47, %r46, %r48;
	mov.b32 	%f162, %r65;
	add.f32 	%f163, %f161, %f162;
	mov.b32 	%r66, %f163;
	shfl.sync.bfly.b32 	%r68|%p15, %r66, %r51, %r46, %r48;
	mov.b32 	%f164, %r68;
	add.f32 	%f165, %f163, %f164;
	mov.b32 	%r69, %f165;
	shfl.sync.bfly.b32 	%r71|%p16, %r69, %r54, %r46, %r48;
	mov.b32 	%f166, %r71;
	add.f32 	%f167, %f165, %f166;
	mov.b32 	%r72, %f167;
	shfl.sync.bfly.b32 	%r74|%p17, %r72, %r44, %r46, %r48;
	mov.b32 	%f168, %r74;
	add.f32 	%f169, %f167, %f168;
	mov.b32 	%r75, %f169;
	shfl.sync.bfly.b32 	%r77|%p18, %r75, %r59, %r46, %r48;
	mov.b32 	%f170, %r77;
	add.f32 	%f171, %f169, %f170;
	st.local.f32 	[%rd2], %f171;

$L__BB53_11:
	bar.sync 	0;
	mov.b32 	%r78, %f294;
	shfl.sync.bfly.b32 	%r82|%p20, %r78, %r47, %r46, %r48;
	mov.b32 	%f172, %r82;
	add.f32 	%f173, %f294, %f172;
	mov.b32 	%r83, %f173;
	shfl.sync.bfly.b32 	%r85|%p21, %r83, %r51, %r46, %r48;
	mov.b32 	%f174, %r85;
	add.f32 	%f175, %f173, %f174;
	mov.b32 	%r86, %f175;
	shfl.sync.bfly.b32 	%r88|%p22, %r86, %r54, %r46, %r48;
	mov.b32 	%f176, %r88;
	add.f32 	%f177, %f175, %f176;
	mov.b32 	%r89, %f177;
	shfl.sync.bfly.b32 	%r91|%p23, %r89, %r44, %r46, %r48;
	mov.b32 	%f178, %r91;
	add.f32 	%f179, %f177, %f178;
	mov.b32 	%r92, %f179;
	shfl.sync.bfly.b32 	%r94|%p24, %r92, %r59, %r46, %r48;
	mov.b32 	%f180, %r94;
	add.f32 	%f181, %f179, %f180;
	st.local.f32 	[%rd2+4], %f181;
	st.shared.f32 	[%r10], %f181;
	bar.sync 	0;
	@%p1 bra 	$L__BB53_13;

	ld.shared.f32 	%f182, [%r4];
	mov.b32 	%r95, %f182;
	mov.u32 	%r96, 31;
	mov.u32 	%r97, 16;
	mov.u32 	%r98, -1;
	shfl.sync.bfly.b32 	%r99|%p25, %r95, %r97, %r96, %r98;
	mov.b32 	%f183, %r99;
	add.f32 	%f184, %f182, %f183;
	mov.b32 	%r100, %f184;
	mov.u32 	%r101, 8;
	shfl.sync.bfly.b32 	%r102|%p26, %r100, %r101, %r96, %r98;
	mov.b32 	%f185, %r102;
	add.f32 	%f186, %f184, %f185;
	mov.b32 	%r103, %f186;
	mov.u32 	%r104, 4;
	shfl.sync.bfly.b32 	%r105|%p27, %r103, %r104, %r96, %r98;
	mov.b32 	%f187, %r105;
	add.f32 	%f188, %f186, %f187;
	mov.b32 	%r106, %f188;
	mov.u32 	%r107, 2;
	shfl.sync.bfly.b32 	%r108|%p28, %r106, %r107, %r96, %r98;
	mov.b32 	%f189, %r108;
	add.f32 	%f190, %f188, %f189;
	mov.b32 	%r109, %f190;
	mov.u32 	%r110, 1;
	shfl.sync.bfly.b32 	%r111|%p29, %r109, %r110, %r96, %r98;
	mov.b32 	%f191, %r111;
	add.f32 	%f192, %f190, %f191;
	st.local.f32 	[%rd2+4], %f192;

$L__BB53_13:
	bar.sync 	0;
	mov.b32 	%r112, %f293;
	mov.u32 	%r113, 31;
	mov.u32 	%r114, 16;
	mov.u32 	%r115, -1;
	shfl.sync.bfly.b32 	%r116|%p31, %r112, %r114, %r113, %r115;
	mov.b32 	%f193, %r116;
	add.f32 	%f194, %f293, %f193;
	mov.b32 	%r117, %f194;
	mov.u32 	%r118, 8;
	shfl.sync.bfly.b32 	%r119|%p32, %r117, %r118, %r113, %r115;
	mov.b32 	%f195, %r119;
	add.f32 	%f196, %f194, %f195;
	mov.b32 	%r120, %f196;
	mov.u32 	%r121, 4;
	shfl.sync.bfly.b32 	%r122|%p33, %r120, %r121, %r113, %r115;
	mov.b32 	%f197, %r122;
	add.f32 	%f198, %f196, %f197;
	mov.b32 	%r123, %f198;
	mov.u32 	%r124, 2;
	shfl.sync.bfly.b32 	%r125|%p34, %r123, %r124, %r113, %r115;
	mov.b32 	%f199, %r125;
	add.f32 	%f200, %f198, %f199;
	mov.b32 	%r126, %f200;
	mov.u32 	%r127, 1;
	shfl.sync.bfly.b32 	%r128|%p35, %r126, %r127, %r113, %r115;
	mov.b32 	%f201, %r128;
	add.f32 	%f202, %f200, %f201;
	st.local.f32 	[%rd2+8], %f202;
	st.shared.f32 	[%r10], %f202;
	bar.sync 	0;
	@%p1 bra 	$L__BB53_15;

	ld.shared.f32 	%f203, [%r4];
	mov.b32 	%r129, %f203;
	shfl.sync.bfly.b32 	%r133|%p36, %r129, %r114, %r113, %r115;
	mov.b32 	%f204, %r133;
	add.f32 	%f205, %f203, %f204;
	mov.b32 	%r134, %f205;
	shfl.sync.bfly.b32 	%r136|%p37, %r134, %r118, %r113, %r115;
	mov.b32 	%f206, %r136;
	add.f32 	%f207, %f205, %f206;
	mov.b32 	%r137, %f207;
	shfl.sync.bfly.b32 	%r139|%p38, %r137, %r121, %r113, %r115;
	mov.b32 	%f208, %r139;
	add.f32 	%f209, %f207, %f208;
	mov.b32 	%r140, %f209;
	shfl.sync.bfly.b32 	%r142|%p39, %r140, %r124, %r113, %r115;
	mov.b32 	%f210, %r142;
	add.f32 	%f211, %f209, %f210;
	mov.b32 	%r143, %f211;
	shfl.sync.bfly.b32 	%r145|%p40, %r143, %r127, %r113, %r115;
	mov.b32 	%f212, %r145;
	add.f32 	%f213, %f211, %f212;
	st.local.f32 	[%rd2+8], %f213;

$L__BB53_15:
	bar.sync 	0;
	mov.b32 	%r146, %f292;
	shfl.sync.bfly.b32 	%r150|%p42, %r146, %r114, %r113, %r115;
	mov.b32 	%f214, %r150;
	add.f32 	%f215, %f292, %f214;
	mov.b32 	%r151, %f215;
	shfl.sync.bfly.b32 	%r153|%p43, %r151, %r118, %r113, %r115;
	mov.b32 	%f216, %r153;
	add.f32 	%f217, %f215, %f216;
	mov.b32 	%r154, %f217;
	shfl.sync.bfly.b32 	%r156|%p44, %r154, %r121, %r113, %r115;
	mov.b32 	%f218, %r156;
	add.f32 	%f219, %f217, %f218;
	mov.b32 	%r157, %f219;
	shfl.sync.bfly.b32 	%r159|%p45, %r157, %r124, %r113, %r115;
	mov.b32 	%f220, %r159;
	add.f32 	%f221, %f219, %f220;
	mov.b32 	%r160, %f221;
	shfl.sync.bfly.b32 	%r162|%p46, %r160, %r127, %r113, %r115;
	mov.b32 	%f222, %r162;
	add.f32 	%f223, %f221, %f222;
	st.local.f32 	[%rd2+12], %f223;
	st.shared.f32 	[%r10], %f223;
	bar.sync 	0;
	@%p1 bra 	$L__BB53_17;

	ld.shared.f32 	%f224, [%r4];
	mov.b32 	%r163, %f224;
	mov.u32 	%r164, 31;
	mov.u32 	%r165, 16;
	mov.u32 	%r166, -1;
	shfl.sync.bfly.b32 	%r167|%p47, %r163, %r165, %r164, %r166;
	mov.b32 	%f225, %r167;
	add.f32 	%f226, %f224, %f225;
	mov.b32 	%r168, %f226;
	mov.u32 	%r169, 8;
	shfl.sync.bfly.b32 	%r170|%p48, %r168, %r169, %r164, %r166;
	mov.b32 	%f227, %r170;
	add.f32 	%f228, %f226, %f227;
	mov.b32 	%r171, %f228;
	mov.u32 	%r172, 4;
	shfl.sync.bfly.b32 	%r173|%p49, %r171, %r172, %r164, %r166;
	mov.b32 	%f229, %r173;
	add.f32 	%f230, %f228, %f229;
	mov.b32 	%r174, %f230;
	mov.u32 	%r175, 2;
	shfl.sync.bfly.b32 	%r176|%p50, %r174, %r175, %r164, %r166;
	mov.b32 	%f231, %r176;
	add.f32 	%f232, %f230, %f231;
	mov.b32 	%r177, %f232;
	mov.u32 	%r178, 1;
	shfl.sync.bfly.b32 	%r179|%p51, %r177, %r178, %r164, %r166;
	mov.b32 	%f233, %r179;
	add.f32 	%f234, %f232, %f233;
	st.local.f32 	[%rd2+12], %f234;

$L__BB53_17:
	bar.sync 	0;
	mov.b32 	%r180, %f291;
	mov.u32 	%r181, 31;
	mov.u32 	%r182, 16;
	mov.u32 	%r183, -1;
	shfl.sync.bfly.b32 	%r184|%p53, %r180, %r182, %r181, %r183;
	mov.b32 	%f235, %r184;
	add.f32 	%f236, %f291, %f235;
	mov.b32 	%r185, %f236;
	mov.u32 	%r186, 8;
	shfl.sync.bfly.b32 	%r187|%p54, %r185, %r186, %r181, %r183;
	mov.b32 	%f237, %r187;
	add.f32 	%f238, %f236, %f237;
	mov.b32 	%r188, %f238;
	mov.u32 	%r189, 4;
	shfl.sync.bfly.b32 	%r190|%p55, %r188, %r189, %r181, %r183;
	mov.b32 	%f239, %r190;
	add.f32 	%f240, %f238, %f239;
	mov.b32 	%r191, %f240;
	mov.u32 	%r192, 2;
	shfl.sync.bfly.b32 	%r193|%p56, %r191, %r192, %r181, %r183;
	mov.b32 	%f241, %r193;
	add.f32 	%f242, %f240, %f241;
	mov.b32 	%r194, %f242;
	mov.u32 	%r195, 1;
	shfl.sync.bfly.b32 	%r196|%p57, %r194, %r195, %r181, %r183;
	mov.b32 	%f243, %r196;
	add.f32 	%f244, %f242, %f243;
	st.local.f32 	[%rd2+16], %f244;
	st.shared.f32 	[%r10], %f244;
	bar.sync 	0;
	@%p1 bra 	$L__BB53_19;

	ld.shared.f32 	%f245, [%r4];
	mov.b32 	%r197, %f245;
	shfl.sync.bfly.b32 	%r201|%p58, %r197, %r182, %r181, %r183;
	mov.b32 	%f246, %r201;
	add.f32 	%f247, %f245, %f246;
	mov.b32 	%r202, %f247;
	shfl.sync.bfly.b32 	%r204|%p59, %r202, %r186, %r181, %r183;
	mov.b32 	%f248, %r204;
	add.f32 	%f249, %f247, %f248;
	mov.b32 	%r205, %f249;
	shfl.sync.bfly.b32 	%r207|%p60, %r205, %r189, %r181, %r183;
	mov.b32 	%f250, %r207;
	add.f32 	%f251, %f249, %f250;
	mov.b32 	%r208, %f251;
	shfl.sync.bfly.b32 	%r210|%p61, %r208, %r192, %r181, %r183;
	mov.b32 	%f252, %r210;
	add.f32 	%f253, %f251, %f252;
	mov.b32 	%r211, %f253;
	shfl.sync.bfly.b32 	%r213|%p62, %r211, %r195, %r181, %r183;
	mov.b32 	%f254, %r213;
	add.f32 	%f255, %f253, %f254;
	st.local.f32 	[%rd2+16], %f255;

$L__BB53_19:
	bar.sync 	0;
	mov.b32 	%r214, %f290;
	shfl.sync.bfly.b32 	%r218|%p64, %r214, %r182, %r181, %r183;
	mov.b32 	%f256, %r218;
	add.f32 	%f257, %f290, %f256;
	mov.b32 	%r219, %f257;
	shfl.sync.bfly.b32 	%r221|%p65, %r219, %r186, %r181, %r183;
	mov.b32 	%f258, %r221;
	add.f32 	%f259, %f257, %f258;
	mov.b32 	%r222, %f259;
	shfl.sync.bfly.b32 	%r224|%p66, %r222, %r189, %r181, %r183;
	mov.b32 	%f260, %r224;
	add.f32 	%f261, %f259, %f260;
	mov.b32 	%r225, %f261;
	shfl.sync.bfly.b32 	%r227|%p67, %r225, %r192, %r181, %r183;
	mov.b32 	%f262, %r227;
	add.f32 	%f263, %f261, %f262;
	mov.b32 	%r228, %f263;
	shfl.sync.bfly.b32 	%r230|%p68, %r228, %r195, %r181, %r183;
	mov.b32 	%f264, %r230;
	add.f32 	%f265, %f263, %f264;
	st.local.f32 	[%rd2+20], %f265;
	st.shared.f32 	[%r10], %f265;
	bar.sync 	0;
	@%p1 bra 	$L__BB53_21;

	ld.shared.f32 	%f266, [%r4];
	mov.b32 	%r231, %f266;
	mov.u32 	%r232, 31;
	mov.u32 	%r233, 16;
	mov.u32 	%r234, -1;
	shfl.sync.bfly.b32 	%r235|%p69, %r231, %r233, %r232, %r234;
	mov.b32 	%f267, %r235;
	add.f32 	%f268, %f266, %f267;
	mov.b32 	%r236, %f268;
	mov.u32 	%r237, 8;
	shfl.sync.bfly.b32 	%r238|%p70, %r236, %r237, %r232, %r234;
	mov.b32 	%f269, %r238;
	add.f32 	%f270, %f268, %f269;
	mov.b32 	%r239, %f270;
	mov.u32 	%r240, 4;
	shfl.sync.bfly.b32 	%r241|%p71, %r239, %r240, %r232, %r234;
	mov.b32 	%f271, %r241;
	add.f32 	%f272, %f270, %f271;
	mov.b32 	%r242, %f272;
	mov.u32 	%r243, 2;
	shfl.sync.bfly.b32 	%r244|%p72, %r242, %r243, %r232, %r234;
	mov.b32 	%f273, %r244;
	add.f32 	%f274, %f272, %f273;
	mov.b32 	%r245, %f274;
	mov.u32 	%r246, 1;
	shfl.sync.bfly.b32 	%r247|%p73, %r245, %r246, %r232, %r234;
	mov.b32 	%f275, %r247;
	add.f32 	%f276, %f274, %f275;
	st.local.f32 	[%rd2+20], %f276;

$L__BB53_21:
	bar.sync 	0;
	setp.gt.s32 	%p74, %r3, 5;
	@%p74 bra 	$L__BB53_23;

	mul.wide.s32 	%rd60, %r3, 4;
	add.s64 	%rd61, %rd2, %rd60;
	ld.local.f32 	%f277, [%rd61];
	mad.lo.s32 	%r248, %r3, %r13, %r2;
	cvt.s64.s32 	%rd62, %r248;
	mul.lo.s32 	%r249, %r1, %r14;
	cvt.s64.s32 	%rd63, %r249;
	add.s64 	%rd64, %rd63, %rd62;
	cvta.to.global.u64 	%rd65, %rd19;
	shl.b64 	%rd66, %rd64, 2;
	add.s64 	%rd67, %rd65, %rd66;
	st.global.f32 	[%rd67], %f277;

$L__BB53_23:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_7_bs_224
.visible .entry ggml_matvec_f32_ncols_7_bs_224(
	.param .u64 ggml_matvec_f32_ncols_7_bs_224_param_0,
	.param .u64 ggml_matvec_f32_ncols_7_bs_224_param_1,
	.param .u64 ggml_matvec_f32_ncols_7_bs_224_param_2,
	.param .u32 ggml_matvec_f32_ncols_7_bs_224_param_3,
	.param .u32 ggml_matvec_f32_ncols_7_bs_224_param_4,
	.param .u32 ggml_matvec_f32_ncols_7_bs_224_param_5,
	.param .u32 ggml_matvec_f32_ncols_7_bs_224_param_6,
	.param .u32 ggml_matvec_f32_ncols_7_bs_224_param_7,
	.param .u32 ggml_matvec_f32_ncols_7_bs_224_param_8,
	.param .u32 ggml_matvec_f32_ncols_7_bs_224_param_9,
	.param .u32 ggml_matvec_f32_ncols_7_bs_224_param_10,
	.param .u32 ggml_matvec_f32_ncols_7_bs_224_param_11
)
{
	.local .align 4 .b8 	__local_depot54[28];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<86>;
	.reg .f32 	%f<343>;
	.reg .b32 	%r<288>;
	.reg .b64 	%rd<74>;


	mov.u64 	%SPL, __local_depot54;
	ld.param.u64 	%rd21, [ggml_matvec_f32_ncols_7_bs_224_param_0];
	ld.param.u64 	%rd22, [ggml_matvec_f32_ncols_7_bs_224_param_1];
	ld.param.u64 	%rd20, [ggml_matvec_f32_ncols_7_bs_224_param_2];
	ld.param.u32 	%r11, [ggml_matvec_f32_ncols_7_bs_224_param_3];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_7_bs_224_param_5];
	ld.param.u32 	%r12, [ggml_matvec_f32_ncols_7_bs_224_param_6];
	ld.param.u32 	%r13, [ggml_matvec_f32_ncols_7_bs_224_param_7];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_7_bs_224_param_8];
	ld.param.u32 	%r17, [ggml_matvec_f32_ncols_7_bs_224_param_9];
	ld.param.u32 	%r18, [ggml_matvec_f32_ncols_7_bs_224_param_10];
	ld.param.u32 	%r14, [ggml_matvec_f32_ncols_7_bs_224_param_11];
	cvta.to.global.u64 	%rd73, %rd22;
	add.u64 	%rd2, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r19, %r1, %r16;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r20, %r2, %r15;
	mad.lo.s32 	%r21, %r19, %r17, %r20;
	cvt.s64.s32 	%rd3, %r21;
	cvta.to.global.u64 	%rd4, %rd21;
	mul.lo.s32 	%r22, %r1, %r18;
	cvt.s64.s32 	%rd5, %r22;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r23, %r3, 2;
	mov.u32 	%r24, data_mmv;
	add.s32 	%r4, %r24, %r23;
	@%p1 bra 	$L__BB54_2;

	mov.u32 	%r25, 0;
	st.shared.u32 	[%r4], %r25;

$L__BB54_2:
	bar.sync 	0;
	mov.f32 	%f336, 0f00000000;
	mov.u32 	%r26, 0;
	st.local.u32 	[%rd2], %r26;
	st.local.u32 	[%rd2+4], %r26;
	st.local.u32 	[%rd2+8], %r26;
	st.local.u32 	[%rd2+12], %r26;
	st.local.u32 	[%rd2+16], %r26;
	st.local.u32 	[%rd2+20], %r26;
	st.local.u32 	[%rd2+24], %r26;
	setp.ge.s32 	%p2, %r3, %r11;
	mov.f32 	%f337, %f336;
	mov.f32 	%f338, %f336;
	mov.f32 	%f339, %f336;
	mov.f32 	%f340, %f336;
	mov.f32 	%f341, %f336;
	mov.f32 	%f342, %f336;
	@%p2 bra 	$L__BB54_9;

	not.b32 	%r27, %r3;
	add.s32 	%r5, %r27, %r11;
	shr.u32 	%r28, %r5, 5;
	mul.wide.u32 	%rd24, %r28, 613566757;
	shr.u64 	%rd25, %rd24, 32;
	and.b64  	%rd26, %rd25, 1;
	setp.eq.b64 	%p3, %rd26, 1;
	mov.pred 	%p4, 0;
	xor.pred  	%p5, %p3, %p4;
	mov.f32 	%f336, 0f00000000;
	mov.u32 	%r287, %r3;
	@%p5 bra 	$L__BB54_5;

	shl.b64 	%rd27, %rd5, 2;
	add.s64 	%rd28, %rd73, %rd27;
	shl.b64 	%rd29, %rd3, 2;
	add.s64 	%rd30, %rd4, %rd29;
	mul.wide.s32 	%rd31, %r3, 8;
	add.s64 	%rd32, %rd30, %rd31;
	ld.global.nc.v2.f32 	{%f50, %f51}, [%rd32];
	add.s64 	%rd33, %rd28, %rd31;
	ld.global.nc.v2.f32 	{%f54, %f55}, [%rd33];
	fma.rn.f32 	%f58, %f50, %f54, 0f00000000;
	fma.rn.f32 	%f342, %f51, %f55, %f58;
	st.local.f32 	[%rd2], %f342;
	mul.wide.s32 	%rd34, %r12, 8;
	add.s64 	%rd35, %rd33, %rd34;
	ld.global.nc.v2.f32 	{%f59, %f60}, [%rd35];
	fma.rn.f32 	%f63, %f50, %f59, 0f00000000;
	fma.rn.f32 	%f341, %f51, %f60, %f63;
	st.local.f32 	[%rd2+4], %f341;
	add.s32 	%r29, %r3, %r12;
	add.s32 	%r30, %r29, %r12;
	mul.wide.s32 	%rd36, %r30, 8;
	add.s64 	%rd37, %rd28, %rd36;
	ld.global.nc.v2.f32 	{%f64, %f65}, [%rd37];
	fma.rn.f32 	%f68, %f50, %f64, 0f00000000;
	fma.rn.f32 	%f340, %f51, %f65, %f68;
	st.local.f32 	[%rd2+8], %f340;
	add.s64 	%rd38, %rd37, %rd34;
	ld.global.nc.v2.f32 	{%f69, %f70}, [%rd38];
	fma.rn.f32 	%f73, %f50, %f69, 0f00000000;
	fma.rn.f32 	%f339, %f51, %f70, %f73;
	st.local.f32 	[%rd2+12], %f339;
	add.s64 	%rd39, %rd38, %rd34;
	ld.global.nc.v2.f32 	{%f74, %f75}, [%rd39];
	fma.rn.f32 	%f78, %f50, %f74, 0f00000000;
	fma.rn.f32 	%f338, %f51, %f75, %f78;
	st.local.f32 	[%rd2+16], %f338;
	add.s64 	%rd40, %rd39, %rd34;
	ld.global.nc.v2.f32 	{%f79, %f80}, [%rd40];
	fma.rn.f32 	%f83, %f50, %f79, 0f00000000;
	fma.rn.f32 	%f337, %f51, %f80, %f83;
	st.local.f32 	[%rd2+20], %f337;
	add.s64 	%rd41, %rd40, %rd34;
	ld.global.nc.v2.f32 	{%f84, %f85}, [%rd41];
	fma.rn.f32 	%f88, %f50, %f84, 0f00000000;
	fma.rn.f32 	%f336, %f51, %f85, %f88;
	st.local.f32 	[%rd2+24], %f336;
	add.s32 	%r287, %r3, 224;

$L__BB54_5:
	setp.lt.u32 	%p6, %r5, 224;
	@%p6 bra 	$L__BB54_9;

	add.s32 	%r31, %r287, %r12;
	add.s32 	%r32, %r31, 224;
	mul.wide.s32 	%rd42, %r32, 8;
	shl.b64 	%rd43, %rd5, 2;
	add.s64 	%rd7, %rd42, %rd43;
	shl.b32 	%r33, %r12, 1;
	add.s32 	%r34, %r287, %r33;
	mad.lo.s32 	%r35, %r12, 3, %r287;
	shl.b32 	%r36, %r12, 2;
	add.s32 	%r37, %r287, %r36;
	mad.lo.s32 	%r38, %r12, 5, %r287;
	mad.lo.s32 	%r39, %r12, 6, %r287;
	mul.wide.s32 	%rd44, %r34, 8;
	add.s64 	%rd8, %rd44, %rd43;
	mul.wide.s32 	%rd45, %r35, 8;
	add.s64 	%rd9, %rd45, %rd43;
	mul.wide.s32 	%rd46, %r37, 8;
	add.s64 	%rd10, %rd46, %rd43;
	mul.wide.s32 	%rd47, %r38, 8;
	add.s64 	%rd11, %rd47, %rd43;
	mul.wide.s32 	%rd48, %r39, 8;
	add.s64 	%rd12, %rd48, %rd43;
	mul.wide.s32 	%rd49, %r287, 2;
	add.s64 	%rd50, %rd49, %rd3;
	shl.b64 	%rd51, %rd50, 2;
	add.s64 	%rd52, %rd4, %rd51;
	add.s64 	%rd72, %rd52, 1792;
	mul.wide.s32 	%rd53, %r287, 8;
	mul.wide.s32 	%rd54, %r12, 8;
	add.s64 	%rd55, %rd53, %rd54;
	add.s64 	%rd14, %rd55, %rd43;
	add.s64 	%rd15, %rd53, %rd43;

$L__BB54_7:
	ld.global.nc.v2.f32 	{%f89, %f90}, [%rd72+-1792];
	add.s64 	%rd56, %rd73, %rd15;
	ld.global.nc.v2.f32 	{%f93, %f94}, [%rd56];
	fma.rn.f32 	%f97, %f89, %f93, %f342;
	fma.rn.f32 	%f98, %f90, %f94, %f97;
	add.s64 	%rd57, %rd73, %rd14;
	ld.global.nc.v2.f32 	{%f99, %f100}, [%rd57];
	fma.rn.f32 	%f103, %f89, %f99, %f341;
	fma.rn.f32 	%f104, %f90, %f100, %f103;
	add.s64 	%rd58, %rd73, %rd8;
	ld.global.nc.v2.f32 	{%f105, %f106}, [%rd58];
	fma.rn.f32 	%f109, %f89, %f105, %f340;
	fma.rn.f32 	%f110, %f90, %f106, %f109;
	add.s64 	%rd59, %rd73, %rd9;
	ld.global.nc.v2.f32 	{%f111, %f112}, [%rd59];
	fma.rn.f32 	%f115, %f89, %f111, %f339;
	fma.rn.f32 	%f116, %f90, %f112, %f115;
	add.s64 	%rd60, %rd73, %rd10;
	ld.global.nc.v2.f32 	{%f117, %f118}, [%rd60];
	fma.rn.f32 	%f121, %f89, %f117, %f338;
	fma.rn.f32 	%f122, %f90, %f118, %f121;
	add.s64 	%rd61, %rd73, %rd11;
	ld.global.nc.v2.f32 	{%f123, %f124}, [%rd61];
	fma.rn.f32 	%f127, %f89, %f123, %f337;
	fma.rn.f32 	%f128, %f90, %f124, %f127;
	add.s64 	%rd62, %rd73, %rd12;
	ld.global.nc.v2.f32 	{%f129, %f130}, [%rd62];
	fma.rn.f32 	%f133, %f89, %f129, %f336;
	fma.rn.f32 	%f134, %f90, %f130, %f133;
	ld.global.nc.v2.f32 	{%f135, %f136}, [%rd72];
	ld.global.nc.v2.f32 	{%f139, %f140}, [%rd56+1792];
	fma.rn.f32 	%f143, %f135, %f139, %f98;
	fma.rn.f32 	%f342, %f136, %f140, %f143;
	add.s64 	%rd63, %rd73, %rd7;
	ld.global.nc.v2.f32 	{%f144, %f145}, [%rd63];
	fma.rn.f32 	%f148, %f135, %f144, %f104;
	fma.rn.f32 	%f341, %f136, %f145, %f148;
	ld.global.nc.v2.f32 	{%f149, %f150}, [%rd58+1792];
	fma.rn.f32 	%f153, %f135, %f149, %f110;
	fma.rn.f32 	%f340, %f136, %f150, %f153;
	ld.global.nc.v2.f32 	{%f154, %f155}, [%rd59+1792];
	fma.rn.f32 	%f158, %f135, %f154, %f116;
	fma.rn.f32 	%f339, %f136, %f155, %f158;
	ld.global.nc.v2.f32 	{%f159, %f160}, [%rd60+1792];
	fma.rn.f32 	%f163, %f135, %f159, %f122;
	fma.rn.f32 	%f338, %f136, %f160, %f163;
	ld.global.nc.v2.f32 	{%f164, %f165}, [%rd61+1792];
	fma.rn.f32 	%f168, %f135, %f164, %f128;
	fma.rn.f32 	%f337, %f136, %f165, %f168;
	ld.global.nc.v2.f32 	{%f169, %f170}, [%rd62+1792];
	fma.rn.f32 	%f173, %f135, %f169, %f134;
	fma.rn.f32 	%f336, %f136, %f170, %f173;
	add.s64 	%rd73, %rd73, 3584;
	add.s64 	%rd72, %rd72, 3584;
	add.s32 	%r287, %r287, 448;
	setp.lt.s32 	%p7, %r287, %r11;
	@%p7 bra 	$L__BB54_7;

	st.local.f32 	[%rd2], %f342;
	st.local.f32 	[%rd2+4], %f341;
	st.local.f32 	[%rd2+8], %f340;
	st.local.f32 	[%rd2+12], %f339;
	st.local.f32 	[%rd2+16], %f338;
	st.local.f32 	[%rd2+20], %f337;
	st.local.f32 	[%rd2+24], %f336;

$L__BB54_9:
	shr.s32 	%r40, %r3, 31;
	shr.u32 	%r41, %r40, 27;
	add.s32 	%r42, %r3, %r41;
	shr.s32 	%r43, %r42, 5;
	shl.b32 	%r44, %r43, 2;
	add.s32 	%r10, %r24, %r44;
	mov.u32 	%r46, 2;
	mov.b32 	%r47, %f342;
	mov.u32 	%r48, 31;
	mov.u32 	%r49, 16;
	mov.u32 	%r50, -1;
	shfl.sync.bfly.b32 	%r51|%p8, %r47, %r49, %r48, %r50;
	mov.b32 	%f174, %r51;
	add.f32 	%f175, %f342, %f174;
	mov.b32 	%r52, %f175;
	mov.u32 	%r53, 8;
	shfl.sync.bfly.b32 	%r54|%p9, %r52, %r53, %r48, %r50;
	mov.b32 	%f176, %r54;
	add.f32 	%f177, %f175, %f176;
	mov.b32 	%r55, %f177;
	mov.u32 	%r56, 4;
	shfl.sync.bfly.b32 	%r57|%p10, %r55, %r56, %r48, %r50;
	mov.b32 	%f178, %r57;
	add.f32 	%f179, %f177, %f178;
	mov.b32 	%r58, %f179;
	shfl.sync.bfly.b32 	%r59|%p11, %r58, %r46, %r48, %r50;
	mov.b32 	%f180, %r59;
	add.f32 	%f181, %f179, %f180;
	mov.b32 	%r60, %f181;
	mov.u32 	%r61, 1;
	shfl.sync.bfly.b32 	%r62|%p12, %r60, %r61, %r48, %r50;
	mov.b32 	%f182, %r62;
	add.f32 	%f183, %f181, %f182;
	st.local.f32 	[%rd2], %f183;
	st.shared.f32 	[%r10], %f183;
	bar.sync 	0;
	@%p1 bra 	$L__BB54_11;

	ld.shared.f32 	%f184, [%r4];
	mov.b32 	%r63, %f184;
	shfl.sync.bfly.b32 	%r67|%p14, %r63, %r49, %r48, %r50;
	mov.b32 	%f185, %r67;
	add.f32 	%f186, %f184, %f185;
	mov.b32 	%r68, %f186;
	shfl.sync.bfly.b32 	%r70|%p15, %r68, %r53, %r48, %r50;
	mov.b32 	%f187, %r70;
	add.f32 	%f188, %f186, %f187;
	mov.b32 	%r71, %f188;
	shfl.sync.bfly.b32 	%r73|%p16, %r71, %r56, %r48, %r50;
	mov.b32 	%f189, %r73;
	add.f32 	%f190, %f188, %f189;
	mov.b32 	%r74, %f190;
	shfl.sync.bfly.b32 	%r76|%p17, %r74, %r46, %r48, %r50;
	mov.b32 	%f191, %r76;
	add.f32 	%f192, %f190, %f191;
	mov.b32 	%r77, %f192;
	shfl.sync.bfly.b32 	%r79|%p18, %r77, %r61, %r48, %r50;
	mov.b32 	%f193, %r79;
	add.f32 	%f194, %f192, %f193;
	st.local.f32 	[%rd2], %f194;

$L__BB54_11:
	bar.sync 	0;
	mov.b32 	%r80, %f341;
	shfl.sync.bfly.b32 	%r84|%p20, %r80, %r49, %r48, %r50;
	mov.b32 	%f195, %r84;
	add.f32 	%f196, %f341, %f195;
	mov.b32 	%r85, %f196;
	shfl.sync.bfly.b32 	%r87|%p21, %r85, %r53, %r48, %r50;
	mov.b32 	%f197, %r87;
	add.f32 	%f198, %f196, %f197;
	mov.b32 	%r88, %f198;
	shfl.sync.bfly.b32 	%r90|%p22, %r88, %r56, %r48, %r50;
	mov.b32 	%f199, %r90;
	add.f32 	%f200, %f198, %f199;
	mov.b32 	%r91, %f200;
	shfl.sync.bfly.b32 	%r93|%p23, %r91, %r46, %r48, %r50;
	mov.b32 	%f201, %r93;
	add.f32 	%f202, %f200, %f201;
	mov.b32 	%r94, %f202;
	shfl.sync.bfly.b32 	%r96|%p24, %r94, %r61, %r48, %r50;
	mov.b32 	%f203, %r96;
	add.f32 	%f204, %f202, %f203;
	st.local.f32 	[%rd2+4], %f204;
	st.shared.f32 	[%r10], %f204;
	bar.sync 	0;
	@%p1 bra 	$L__BB54_13;

	ld.shared.f32 	%f205, [%r4];
	mov.b32 	%r97, %f205;
	mov.u32 	%r98, 31;
	mov.u32 	%r99, 16;
	mov.u32 	%r100, -1;
	shfl.sync.bfly.b32 	%r101|%p25, %r97, %r99, %r98, %r100;
	mov.b32 	%f206, %r101;
	add.f32 	%f207, %f205, %f206;
	mov.b32 	%r102, %f207;
	mov.u32 	%r103, 8;
	shfl.sync.bfly.b32 	%r104|%p26, %r102, %r103, %r98, %r100;
	mov.b32 	%f208, %r104;
	add.f32 	%f209, %f207, %f208;
	mov.b32 	%r105, %f209;
	mov.u32 	%r106, 4;
	shfl.sync.bfly.b32 	%r107|%p27, %r105, %r106, %r98, %r100;
	mov.b32 	%f210, %r107;
	add.f32 	%f211, %f209, %f210;
	mov.b32 	%r108, %f211;
	mov.u32 	%r109, 2;
	shfl.sync.bfly.b32 	%r110|%p28, %r108, %r109, %r98, %r100;
	mov.b32 	%f212, %r110;
	add.f32 	%f213, %f211, %f212;
	mov.b32 	%r111, %f213;
	mov.u32 	%r112, 1;
	shfl.sync.bfly.b32 	%r113|%p29, %r111, %r112, %r98, %r100;
	mov.b32 	%f214, %r113;
	add.f32 	%f215, %f213, %f214;
	st.local.f32 	[%rd2+4], %f215;

$L__BB54_13:
	bar.sync 	0;
	mov.b32 	%r114, %f340;
	mov.u32 	%r115, 31;
	mov.u32 	%r116, 16;
	mov.u32 	%r117, -1;
	shfl.sync.bfly.b32 	%r118|%p31, %r114, %r116, %r115, %r117;
	mov.b32 	%f216, %r118;
	add.f32 	%f217, %f340, %f216;
	mov.b32 	%r119, %f217;
	mov.u32 	%r120, 8;
	shfl.sync.bfly.b32 	%r121|%p32, %r119, %r120, %r115, %r117;
	mov.b32 	%f218, %r121;
	add.f32 	%f219, %f217, %f218;
	mov.b32 	%r122, %f219;
	mov.u32 	%r123, 4;
	shfl.sync.bfly.b32 	%r124|%p33, %r122, %r123, %r115, %r117;
	mov.b32 	%f220, %r124;
	add.f32 	%f221, %f219, %f220;
	mov.b32 	%r125, %f221;
	mov.u32 	%r126, 2;
	shfl.sync.bfly.b32 	%r127|%p34, %r125, %r126, %r115, %r117;
	mov.b32 	%f222, %r127;
	add.f32 	%f223, %f221, %f222;
	mov.b32 	%r128, %f223;
	mov.u32 	%r129, 1;
	shfl.sync.bfly.b32 	%r130|%p35, %r128, %r129, %r115, %r117;
	mov.b32 	%f224, %r130;
	add.f32 	%f225, %f223, %f224;
	st.local.f32 	[%rd2+8], %f225;
	st.shared.f32 	[%r10], %f225;
	bar.sync 	0;
	@%p1 bra 	$L__BB54_15;

	ld.shared.f32 	%f226, [%r4];
	mov.b32 	%r131, %f226;
	shfl.sync.bfly.b32 	%r135|%p36, %r131, %r116, %r115, %r117;
	mov.b32 	%f227, %r135;
	add.f32 	%f228, %f226, %f227;
	mov.b32 	%r136, %f228;
	shfl.sync.bfly.b32 	%r138|%p37, %r136, %r120, %r115, %r117;
	mov.b32 	%f229, %r138;
	add.f32 	%f230, %f228, %f229;
	mov.b32 	%r139, %f230;
	shfl.sync.bfly.b32 	%r141|%p38, %r139, %r123, %r115, %r117;
	mov.b32 	%f231, %r141;
	add.f32 	%f232, %f230, %f231;
	mov.b32 	%r142, %f232;
	shfl.sync.bfly.b32 	%r144|%p39, %r142, %r126, %r115, %r117;
	mov.b32 	%f233, %r144;
	add.f32 	%f234, %f232, %f233;
	mov.b32 	%r145, %f234;
	shfl.sync.bfly.b32 	%r147|%p40, %r145, %r129, %r115, %r117;
	mov.b32 	%f235, %r147;
	add.f32 	%f236, %f234, %f235;
	st.local.f32 	[%rd2+8], %f236;

$L__BB54_15:
	bar.sync 	0;
	mov.b32 	%r148, %f339;
	shfl.sync.bfly.b32 	%r152|%p42, %r148, %r116, %r115, %r117;
	mov.b32 	%f237, %r152;
	add.f32 	%f238, %f339, %f237;
	mov.b32 	%r153, %f238;
	shfl.sync.bfly.b32 	%r155|%p43, %r153, %r120, %r115, %r117;
	mov.b32 	%f239, %r155;
	add.f32 	%f240, %f238, %f239;
	mov.b32 	%r156, %f240;
	shfl.sync.bfly.b32 	%r158|%p44, %r156, %r123, %r115, %r117;
	mov.b32 	%f241, %r158;
	add.f32 	%f242, %f240, %f241;
	mov.b32 	%r159, %f242;
	shfl.sync.bfly.b32 	%r161|%p45, %r159, %r126, %r115, %r117;
	mov.b32 	%f243, %r161;
	add.f32 	%f244, %f242, %f243;
	mov.b32 	%r162, %f244;
	shfl.sync.bfly.b32 	%r164|%p46, %r162, %r129, %r115, %r117;
	mov.b32 	%f245, %r164;
	add.f32 	%f246, %f244, %f245;
	st.local.f32 	[%rd2+12], %f246;
	st.shared.f32 	[%r10], %f246;
	bar.sync 	0;
	@%p1 bra 	$L__BB54_17;

	ld.shared.f32 	%f247, [%r4];
	mov.b32 	%r165, %f247;
	mov.u32 	%r166, 31;
	mov.u32 	%r167, 16;
	mov.u32 	%r168, -1;
	shfl.sync.bfly.b32 	%r169|%p47, %r165, %r167, %r166, %r168;
	mov.b32 	%f248, %r169;
	add.f32 	%f249, %f247, %f248;
	mov.b32 	%r170, %f249;
	mov.u32 	%r171, 8;
	shfl.sync.bfly.b32 	%r172|%p48, %r170, %r171, %r166, %r168;
	mov.b32 	%f250, %r172;
	add.f32 	%f251, %f249, %f250;
	mov.b32 	%r173, %f251;
	mov.u32 	%r174, 4;
	shfl.sync.bfly.b32 	%r175|%p49, %r173, %r174, %r166, %r168;
	mov.b32 	%f252, %r175;
	add.f32 	%f253, %f251, %f252;
	mov.b32 	%r176, %f253;
	mov.u32 	%r177, 2;
	shfl.sync.bfly.b32 	%r178|%p50, %r176, %r177, %r166, %r168;
	mov.b32 	%f254, %r178;
	add.f32 	%f255, %f253, %f254;
	mov.b32 	%r179, %f255;
	mov.u32 	%r180, 1;
	shfl.sync.bfly.b32 	%r181|%p51, %r179, %r180, %r166, %r168;
	mov.b32 	%f256, %r181;
	add.f32 	%f257, %f255, %f256;
	st.local.f32 	[%rd2+12], %f257;

$L__BB54_17:
	bar.sync 	0;
	mov.b32 	%r182, %f338;
	mov.u32 	%r183, 31;
	mov.u32 	%r184, 16;
	mov.u32 	%r185, -1;
	shfl.sync.bfly.b32 	%r186|%p53, %r182, %r184, %r183, %r185;
	mov.b32 	%f258, %r186;
	add.f32 	%f259, %f338, %f258;
	mov.b32 	%r187, %f259;
	mov.u32 	%r188, 8;
	shfl.sync.bfly.b32 	%r189|%p54, %r187, %r188, %r183, %r185;
	mov.b32 	%f260, %r189;
	add.f32 	%f261, %f259, %f260;
	mov.b32 	%r190, %f261;
	mov.u32 	%r191, 4;
	shfl.sync.bfly.b32 	%r192|%p55, %r190, %r191, %r183, %r185;
	mov.b32 	%f262, %r192;
	add.f32 	%f263, %f261, %f262;
	mov.b32 	%r193, %f263;
	mov.u32 	%r194, 2;
	shfl.sync.bfly.b32 	%r195|%p56, %r193, %r194, %r183, %r185;
	mov.b32 	%f264, %r195;
	add.f32 	%f265, %f263, %f264;
	mov.b32 	%r196, %f265;
	mov.u32 	%r197, 1;
	shfl.sync.bfly.b32 	%r198|%p57, %r196, %r197, %r183, %r185;
	mov.b32 	%f266, %r198;
	add.f32 	%f267, %f265, %f266;
	st.local.f32 	[%rd2+16], %f267;
	st.shared.f32 	[%r10], %f267;
	bar.sync 	0;
	@%p1 bra 	$L__BB54_19;

	ld.shared.f32 	%f268, [%r4];
	mov.b32 	%r199, %f268;
	shfl.sync.bfly.b32 	%r203|%p58, %r199, %r184, %r183, %r185;
	mov.b32 	%f269, %r203;
	add.f32 	%f270, %f268, %f269;
	mov.b32 	%r204, %f270;
	shfl.sync.bfly.b32 	%r206|%p59, %r204, %r188, %r183, %r185;
	mov.b32 	%f271, %r206;
	add.f32 	%f272, %f270, %f271;
	mov.b32 	%r207, %f272;
	shfl.sync.bfly.b32 	%r209|%p60, %r207, %r191, %r183, %r185;
	mov.b32 	%f273, %r209;
	add.f32 	%f274, %f272, %f273;
	mov.b32 	%r210, %f274;
	shfl.sync.bfly.b32 	%r212|%p61, %r210, %r194, %r183, %r185;
	mov.b32 	%f275, %r212;
	add.f32 	%f276, %f274, %f275;
	mov.b32 	%r213, %f276;
	shfl.sync.bfly.b32 	%r215|%p62, %r213, %r197, %r183, %r185;
	mov.b32 	%f277, %r215;
	add.f32 	%f278, %f276, %f277;
	st.local.f32 	[%rd2+16], %f278;

$L__BB54_19:
	bar.sync 	0;
	mov.b32 	%r216, %f337;
	shfl.sync.bfly.b32 	%r220|%p64, %r216, %r184, %r183, %r185;
	mov.b32 	%f279, %r220;
	add.f32 	%f280, %f337, %f279;
	mov.b32 	%r221, %f280;
	shfl.sync.bfly.b32 	%r223|%p65, %r221, %r188, %r183, %r185;
	mov.b32 	%f281, %r223;
	add.f32 	%f282, %f280, %f281;
	mov.b32 	%r224, %f282;
	shfl.sync.bfly.b32 	%r226|%p66, %r224, %r191, %r183, %r185;
	mov.b32 	%f283, %r226;
	add.f32 	%f284, %f282, %f283;
	mov.b32 	%r227, %f284;
	shfl.sync.bfly.b32 	%r229|%p67, %r227, %r194, %r183, %r185;
	mov.b32 	%f285, %r229;
	add.f32 	%f286, %f284, %f285;
	mov.b32 	%r230, %f286;
	shfl.sync.bfly.b32 	%r232|%p68, %r230, %r197, %r183, %r185;
	mov.b32 	%f287, %r232;
	add.f32 	%f288, %f286, %f287;
	st.local.f32 	[%rd2+20], %f288;
	st.shared.f32 	[%r10], %f288;
	bar.sync 	0;
	@%p1 bra 	$L__BB54_21;

	ld.shared.f32 	%f289, [%r4];
	mov.b32 	%r233, %f289;
	mov.u32 	%r234, 31;
	mov.u32 	%r235, 16;
	mov.u32 	%r236, -1;
	shfl.sync.bfly.b32 	%r237|%p69, %r233, %r235, %r234, %r236;
	mov.b32 	%f290, %r237;
	add.f32 	%f291, %f289, %f290;
	mov.b32 	%r238, %f291;
	mov.u32 	%r239, 8;
	shfl.sync.bfly.b32 	%r240|%p70, %r238, %r239, %r234, %r236;
	mov.b32 	%f292, %r240;
	add.f32 	%f293, %f291, %f292;
	mov.b32 	%r241, %f293;
	mov.u32 	%r242, 4;
	shfl.sync.bfly.b32 	%r243|%p71, %r241, %r242, %r234, %r236;
	mov.b32 	%f294, %r243;
	add.f32 	%f295, %f293, %f294;
	mov.b32 	%r244, %f295;
	mov.u32 	%r245, 2;
	shfl.sync.bfly.b32 	%r246|%p72, %r244, %r245, %r234, %r236;
	mov.b32 	%f296, %r246;
	add.f32 	%f297, %f295, %f296;
	mov.b32 	%r247, %f297;
	mov.u32 	%r248, 1;
	shfl.sync.bfly.b32 	%r249|%p73, %r247, %r248, %r234, %r236;
	mov.b32 	%f298, %r249;
	add.f32 	%f299, %f297, %f298;
	st.local.f32 	[%rd2+20], %f299;

$L__BB54_21:
	bar.sync 	0;
	mov.b32 	%r250, %f336;
	mov.u32 	%r251, 31;
	mov.u32 	%r252, 16;
	mov.u32 	%r253, -1;
	shfl.sync.bfly.b32 	%r254|%p75, %r250, %r252, %r251, %r253;
	mov.b32 	%f300, %r254;
	add.f32 	%f301, %f336, %f300;
	mov.b32 	%r255, %f301;
	mov.u32 	%r256, 8;
	shfl.sync.bfly.b32 	%r257|%p76, %r255, %r256, %r251, %r253;
	mov.b32 	%f302, %r257;
	add.f32 	%f303, %f301, %f302;
	mov.b32 	%r258, %f303;
	mov.u32 	%r259, 4;
	shfl.sync.bfly.b32 	%r260|%p77, %r258, %r259, %r251, %r253;
	mov.b32 	%f304, %r260;
	add.f32 	%f305, %f303, %f304;
	mov.b32 	%r261, %f305;
	mov.u32 	%r262, 2;
	shfl.sync.bfly.b32 	%r263|%p78, %r261, %r262, %r251, %r253;
	mov.b32 	%f306, %r263;
	add.f32 	%f307, %f305, %f306;
	mov.b32 	%r264, %f307;
	mov.u32 	%r265, 1;
	shfl.sync.bfly.b32 	%r266|%p79, %r264, %r265, %r251, %r253;
	mov.b32 	%f308, %r266;
	add.f32 	%f309, %f307, %f308;
	st.local.f32 	[%rd2+24], %f309;
	st.shared.f32 	[%r10], %f309;
	bar.sync 	0;
	@%p1 bra 	$L__BB54_23;

	ld.shared.f32 	%f310, [%r4];
	mov.b32 	%r267, %f310;
	shfl.sync.bfly.b32 	%r271|%p80, %r267, %r252, %r251, %r253;
	mov.b32 	%f311, %r271;
	add.f32 	%f312, %f310, %f311;
	mov.b32 	%r272, %f312;
	shfl.sync.bfly.b32 	%r274|%p81, %r272, %r256, %r251, %r253;
	mov.b32 	%f313, %r274;
	add.f32 	%f314, %f312, %f313;
	mov.b32 	%r275, %f314;
	shfl.sync.bfly.b32 	%r277|%p82, %r275, %r259, %r251, %r253;
	mov.b32 	%f315, %r277;
	add.f32 	%f316, %f314, %f315;
	mov.b32 	%r278, %f316;
	shfl.sync.bfly.b32 	%r280|%p83, %r278, %r262, %r251, %r253;
	mov.b32 	%f317, %r280;
	add.f32 	%f318, %f316, %f317;
	mov.b32 	%r281, %f318;
	shfl.sync.bfly.b32 	%r283|%p84, %r281, %r265, %r251, %r253;
	mov.b32 	%f319, %r283;
	add.f32 	%f320, %f318, %f319;
	st.local.f32 	[%rd2+24], %f320;

$L__BB54_23:
	bar.sync 	0;
	setp.gt.s32 	%p85, %r3, 6;
	@%p85 bra 	$L__BB54_25;

	mul.wide.s32 	%rd64, %r3, 4;
	add.s64 	%rd65, %rd2, %rd64;
	ld.local.f32 	%f321, [%rd65];
	mad.lo.s32 	%r284, %r3, %r13, %r2;
	cvt.s64.s32 	%rd66, %r284;
	mul.lo.s32 	%r285, %r1, %r14;
	cvt.s64.s32 	%rd67, %r285;
	add.s64 	%rd68, %rd67, %rd66;
	cvta.to.global.u64 	%rd69, %rd20;
	shl.b64 	%rd70, %rd68, 2;
	add.s64 	%rd71, %rd69, %rd70;
	st.global.f32 	[%rd71], %f321;

$L__BB54_25:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_8_bs_224
.visible .entry ggml_matvec_f32_ncols_8_bs_224(
	.param .u64 ggml_matvec_f32_ncols_8_bs_224_param_0,
	.param .u64 ggml_matvec_f32_ncols_8_bs_224_param_1,
	.param .u64 ggml_matvec_f32_ncols_8_bs_224_param_2,
	.param .u32 ggml_matvec_f32_ncols_8_bs_224_param_3,
	.param .u32 ggml_matvec_f32_ncols_8_bs_224_param_4,
	.param .u32 ggml_matvec_f32_ncols_8_bs_224_param_5,
	.param .u32 ggml_matvec_f32_ncols_8_bs_224_param_6,
	.param .u32 ggml_matvec_f32_ncols_8_bs_224_param_7,
	.param .u32 ggml_matvec_f32_ncols_8_bs_224_param_8,
	.param .u32 ggml_matvec_f32_ncols_8_bs_224_param_9,
	.param .u32 ggml_matvec_f32_ncols_8_bs_224_param_10,
	.param .u32 ggml_matvec_f32_ncols_8_bs_224_param_11
)
{
	.local .align 16 .b8 	__local_depot55[32];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<97>;
	.reg .f32 	%f<390>;
	.reg .b32 	%r<322>;
	.reg .b64 	%rd<78>;


	mov.u64 	%SPL, __local_depot55;
	ld.param.u64 	%rd22, [ggml_matvec_f32_ncols_8_bs_224_param_0];
	ld.param.u64 	%rd23, [ggml_matvec_f32_ncols_8_bs_224_param_1];
	ld.param.u64 	%rd21, [ggml_matvec_f32_ncols_8_bs_224_param_2];
	ld.param.u32 	%r11, [ggml_matvec_f32_ncols_8_bs_224_param_3];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_8_bs_224_param_5];
	ld.param.u32 	%r12, [ggml_matvec_f32_ncols_8_bs_224_param_6];
	ld.param.u32 	%r13, [ggml_matvec_f32_ncols_8_bs_224_param_7];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_8_bs_224_param_8];
	ld.param.u32 	%r17, [ggml_matvec_f32_ncols_8_bs_224_param_9];
	ld.param.u32 	%r18, [ggml_matvec_f32_ncols_8_bs_224_param_10];
	ld.param.u32 	%r14, [ggml_matvec_f32_ncols_8_bs_224_param_11];
	cvta.to.global.u64 	%rd77, %rd23;
	add.u64 	%rd2, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r19, %r1, %r16;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r20, %r2, %r15;
	mad.lo.s32 	%r21, %r19, %r17, %r20;
	cvt.s64.s32 	%rd3, %r21;
	cvta.to.global.u64 	%rd4, %rd22;
	mul.lo.s32 	%r22, %r1, %r18;
	cvt.s64.s32 	%rd5, %r22;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r23, %r3, 2;
	mov.u32 	%r24, data_mmv;
	add.s32 	%r4, %r24, %r23;
	@%p1 bra 	$L__BB55_2;

	mov.u32 	%r25, 0;
	st.shared.u32 	[%r4], %r25;

$L__BB55_2:
	bar.sync 	0;
	mov.f32 	%f382, 0f00000000;
	st.local.v4.f32 	[%rd2], {%f382, %f382, %f382, %f382};
	st.local.v4.f32 	[%rd2+16], {%f382, %f382, %f382, %f382};
	setp.ge.s32 	%p2, %r3, %r11;
	mov.f32 	%f383, %f382;
	mov.f32 	%f384, %f382;
	mov.f32 	%f385, %f382;
	mov.f32 	%f386, %f382;
	mov.f32 	%f387, %f382;
	mov.f32 	%f388, %f382;
	mov.f32 	%f389, %f382;
	@%p2 bra 	$L__BB55_9;

	not.b32 	%r26, %r3;
	add.s32 	%r5, %r26, %r11;
	shr.u32 	%r27, %r5, 5;
	mul.wide.u32 	%rd25, %r27, 613566757;
	shr.u64 	%rd26, %rd25, 32;
	and.b64  	%rd27, %rd26, 1;
	setp.eq.b64 	%p3, %rd27, 1;
	mov.pred 	%p4, 0;
	xor.pred  	%p5, %p3, %p4;
	mov.f32 	%f382, 0f00000000;
	mov.u32 	%r321, %r3;
	@%p5 bra 	$L__BB55_5;

	shl.b64 	%rd28, %rd5, 2;
	add.s64 	%rd29, %rd77, %rd28;
	shl.b64 	%rd30, %rd3, 2;
	add.s64 	%rd31, %rd4, %rd30;
	mul.wide.s32 	%rd32, %r3, 8;
	add.s64 	%rd33, %rd31, %rd32;
	ld.global.nc.v2.f32 	{%f57, %f58}, [%rd33];
	add.s64 	%rd34, %rd29, %rd32;
	ld.global.nc.v2.f32 	{%f61, %f62}, [%rd34];
	fma.rn.f32 	%f65, %f57, %f61, 0f00000000;
	fma.rn.f32 	%f389, %f58, %f62, %f65;
	mul.wide.s32 	%rd35, %r12, 8;
	add.s64 	%rd36, %rd34, %rd35;
	ld.global.nc.v2.f32 	{%f66, %f67}, [%rd36];
	fma.rn.f32 	%f70, %f57, %f66, 0f00000000;
	fma.rn.f32 	%f388, %f58, %f67, %f70;
	add.s32 	%r28, %r3, %r12;
	add.s32 	%r29, %r28, %r12;
	mul.wide.s32 	%rd37, %r29, 8;
	add.s64 	%rd38, %rd29, %rd37;
	ld.global.nc.v2.f32 	{%f71, %f72}, [%rd38];
	fma.rn.f32 	%f75, %f57, %f71, 0f00000000;
	fma.rn.f32 	%f387, %f58, %f72, %f75;
	add.s64 	%rd39, %rd38, %rd35;
	ld.global.nc.v2.f32 	{%f76, %f77}, [%rd39];
	fma.rn.f32 	%f80, %f57, %f76, 0f00000000;
	fma.rn.f32 	%f386, %f58, %f77, %f80;
	st.local.v4.f32 	[%rd2], {%f389, %f388, %f387, %f386};
	add.s64 	%rd40, %rd39, %rd35;
	ld.global.nc.v2.f32 	{%f81, %f82}, [%rd40];
	fma.rn.f32 	%f85, %f57, %f81, 0f00000000;
	fma.rn.f32 	%f385, %f58, %f82, %f85;
	add.s64 	%rd41, %rd40, %rd35;
	ld.global.nc.v2.f32 	{%f86, %f87}, [%rd41];
	fma.rn.f32 	%f90, %f57, %f86, 0f00000000;
	fma.rn.f32 	%f384, %f58, %f87, %f90;
	add.s64 	%rd42, %rd41, %rd35;
	ld.global.nc.v2.f32 	{%f91, %f92}, [%rd42];
	fma.rn.f32 	%f95, %f57, %f91, 0f00000000;
	fma.rn.f32 	%f383, %f58, %f92, %f95;
	add.s64 	%rd43, %rd42, %rd35;
	ld.global.nc.v2.f32 	{%f96, %f97}, [%rd43];
	fma.rn.f32 	%f100, %f57, %f96, 0f00000000;
	fma.rn.f32 	%f382, %f58, %f97, %f100;
	st.local.v4.f32 	[%rd2+16], {%f385, %f384, %f383, %f382};
	add.s32 	%r321, %r3, 224;

$L__BB55_5:
	setp.lt.u32 	%p6, %r5, 224;
	@%p6 bra 	$L__BB55_9;

	add.s32 	%r30, %r321, %r12;
	add.s32 	%r31, %r30, 224;
	mul.wide.s32 	%rd44, %r31, 8;
	shl.b64 	%rd45, %rd5, 2;
	add.s64 	%rd7, %rd44, %rd45;
	shl.b32 	%r32, %r12, 1;
	add.s32 	%r33, %r321, %r32;
	mad.lo.s32 	%r34, %r12, 3, %r321;
	shl.b32 	%r35, %r12, 2;
	add.s32 	%r36, %r321, %r35;
	mad.lo.s32 	%r37, %r12, 5, %r321;
	mad.lo.s32 	%r38, %r12, 6, %r321;
	mad.lo.s32 	%r39, %r12, 7, %r321;
	mul.wide.s32 	%rd46, %r33, 8;
	add.s64 	%rd8, %rd46, %rd45;
	mul.wide.s32 	%rd47, %r34, 8;
	add.s64 	%rd9, %rd47, %rd45;
	mul.wide.s32 	%rd48, %r36, 8;
	add.s64 	%rd10, %rd48, %rd45;
	mul.wide.s32 	%rd49, %r37, 8;
	add.s64 	%rd11, %rd49, %rd45;
	mul.wide.s32 	%rd50, %r38, 8;
	add.s64 	%rd12, %rd50, %rd45;
	mul.wide.s32 	%rd51, %r39, 8;
	add.s64 	%rd13, %rd51, %rd45;
	mul.wide.s32 	%rd52, %r321, 2;
	add.s64 	%rd53, %rd52, %rd3;
	shl.b64 	%rd54, %rd53, 2;
	add.s64 	%rd55, %rd4, %rd54;
	add.s64 	%rd76, %rd55, 1792;
	mul.wide.s32 	%rd56, %r321, 8;
	mul.wide.s32 	%rd57, %r12, 8;
	add.s64 	%rd58, %rd56, %rd57;
	add.s64 	%rd15, %rd58, %rd45;
	add.s64 	%rd16, %rd56, %rd45;

$L__BB55_7:
	ld.global.nc.v2.f32 	{%f101, %f102}, [%rd76+-1792];
	add.s64 	%rd59, %rd77, %rd16;
	ld.global.nc.v2.f32 	{%f105, %f106}, [%rd59];
	fma.rn.f32 	%f109, %f101, %f105, %f389;
	fma.rn.f32 	%f110, %f102, %f106, %f109;
	add.s64 	%rd60, %rd77, %rd15;
	ld.global.nc.v2.f32 	{%f111, %f112}, [%rd60];
	fma.rn.f32 	%f115, %f101, %f111, %f388;
	fma.rn.f32 	%f116, %f102, %f112, %f115;
	add.s64 	%rd61, %rd77, %rd8;
	ld.global.nc.v2.f32 	{%f117, %f118}, [%rd61];
	fma.rn.f32 	%f121, %f101, %f117, %f387;
	fma.rn.f32 	%f122, %f102, %f118, %f121;
	add.s64 	%rd62, %rd77, %rd9;
	ld.global.nc.v2.f32 	{%f123, %f124}, [%rd62];
	fma.rn.f32 	%f127, %f101, %f123, %f386;
	fma.rn.f32 	%f128, %f102, %f124, %f127;
	add.s64 	%rd63, %rd77, %rd10;
	ld.global.nc.v2.f32 	{%f129, %f130}, [%rd63];
	fma.rn.f32 	%f133, %f101, %f129, %f385;
	fma.rn.f32 	%f134, %f102, %f130, %f133;
	add.s64 	%rd64, %rd77, %rd11;
	ld.global.nc.v2.f32 	{%f135, %f136}, [%rd64];
	fma.rn.f32 	%f139, %f101, %f135, %f384;
	fma.rn.f32 	%f140, %f102, %f136, %f139;
	add.s64 	%rd65, %rd77, %rd12;
	ld.global.nc.v2.f32 	{%f141, %f142}, [%rd65];
	fma.rn.f32 	%f145, %f101, %f141, %f383;
	fma.rn.f32 	%f146, %f102, %f142, %f145;
	add.s64 	%rd66, %rd77, %rd13;
	ld.global.nc.v2.f32 	{%f147, %f148}, [%rd66];
	fma.rn.f32 	%f151, %f101, %f147, %f382;
	fma.rn.f32 	%f152, %f102, %f148, %f151;
	ld.global.nc.v2.f32 	{%f153, %f154}, [%rd76];
	ld.global.nc.v2.f32 	{%f157, %f158}, [%rd59+1792];
	fma.rn.f32 	%f161, %f153, %f157, %f110;
	fma.rn.f32 	%f389, %f154, %f158, %f161;
	add.s64 	%rd67, %rd77, %rd7;
	ld.global.nc.v2.f32 	{%f162, %f163}, [%rd67];
	fma.rn.f32 	%f166, %f153, %f162, %f116;
	fma.rn.f32 	%f388, %f154, %f163, %f166;
	ld.global.nc.v2.f32 	{%f167, %f168}, [%rd61+1792];
	fma.rn.f32 	%f171, %f153, %f167, %f122;
	fma.rn.f32 	%f387, %f154, %f168, %f171;
	ld.global.nc.v2.f32 	{%f172, %f173}, [%rd62+1792];
	fma.rn.f32 	%f176, %f153, %f172, %f128;
	fma.rn.f32 	%f386, %f154, %f173, %f176;
	ld.global.nc.v2.f32 	{%f177, %f178}, [%rd63+1792];
	fma.rn.f32 	%f181, %f153, %f177, %f134;
	fma.rn.f32 	%f385, %f154, %f178, %f181;
	ld.global.nc.v2.f32 	{%f182, %f183}, [%rd64+1792];
	fma.rn.f32 	%f186, %f153, %f182, %f140;
	fma.rn.f32 	%f384, %f154, %f183, %f186;
	ld.global.nc.v2.f32 	{%f187, %f188}, [%rd65+1792];
	fma.rn.f32 	%f191, %f153, %f187, %f146;
	fma.rn.f32 	%f383, %f154, %f188, %f191;
	ld.global.nc.v2.f32 	{%f192, %f193}, [%rd66+1792];
	fma.rn.f32 	%f196, %f153, %f192, %f152;
	fma.rn.f32 	%f382, %f154, %f193, %f196;
	add.s64 	%rd77, %rd77, 3584;
	add.s64 	%rd76, %rd76, 3584;
	add.s32 	%r321, %r321, 448;
	setp.lt.s32 	%p7, %r321, %r11;
	@%p7 bra 	$L__BB55_7;

	st.local.v4.f32 	[%rd2], {%f389, %f388, %f387, %f386};
	st.local.v4.f32 	[%rd2+16], {%f385, %f384, %f383, %f382};

$L__BB55_9:
	shr.s32 	%r40, %r3, 31;
	shr.u32 	%r41, %r40, 27;
	add.s32 	%r42, %r3, %r41;
	shr.s32 	%r43, %r42, 5;
	shl.b32 	%r44, %r43, 2;
	add.s32 	%r10, %r24, %r44;
	mov.u32 	%r46, 2;
	mov.b32 	%r47, %f389;
	mov.u32 	%r48, 31;
	mov.u32 	%r49, 16;
	mov.u32 	%r50, -1;
	shfl.sync.bfly.b32 	%r51|%p8, %r47, %r49, %r48, %r50;
	mov.b32 	%f197, %r51;
	add.f32 	%f198, %f389, %f197;
	mov.b32 	%r52, %f198;
	mov.u32 	%r53, 8;
	shfl.sync.bfly.b32 	%r54|%p9, %r52, %r53, %r48, %r50;
	mov.b32 	%f199, %r54;
	add.f32 	%f200, %f198, %f199;
	mov.b32 	%r55, %f200;
	mov.u32 	%r56, 4;
	shfl.sync.bfly.b32 	%r57|%p10, %r55, %r56, %r48, %r50;
	mov.b32 	%f201, %r57;
	add.f32 	%f202, %f200, %f201;
	mov.b32 	%r58, %f202;
	shfl.sync.bfly.b32 	%r59|%p11, %r58, %r46, %r48, %r50;
	mov.b32 	%f203, %r59;
	add.f32 	%f204, %f202, %f203;
	mov.b32 	%r60, %f204;
	mov.u32 	%r61, 1;
	shfl.sync.bfly.b32 	%r62|%p12, %r60, %r61, %r48, %r50;
	mov.b32 	%f205, %r62;
	add.f32 	%f206, %f204, %f205;
	st.local.f32 	[%rd2], %f206;
	st.shared.f32 	[%r10], %f206;
	bar.sync 	0;
	@%p1 bra 	$L__BB55_11;

	ld.shared.f32 	%f207, [%r4];
	mov.b32 	%r63, %f207;
	shfl.sync.bfly.b32 	%r67|%p14, %r63, %r49, %r48, %r50;
	mov.b32 	%f208, %r67;
	add.f32 	%f209, %f207, %f208;
	mov.b32 	%r68, %f209;
	shfl.sync.bfly.b32 	%r70|%p15, %r68, %r53, %r48, %r50;
	mov.b32 	%f210, %r70;
	add.f32 	%f211, %f209, %f210;
	mov.b32 	%r71, %f211;
	shfl.sync.bfly.b32 	%r73|%p16, %r71, %r56, %r48, %r50;
	mov.b32 	%f212, %r73;
	add.f32 	%f213, %f211, %f212;
	mov.b32 	%r74, %f213;
	shfl.sync.bfly.b32 	%r76|%p17, %r74, %r46, %r48, %r50;
	mov.b32 	%f214, %r76;
	add.f32 	%f215, %f213, %f214;
	mov.b32 	%r77, %f215;
	shfl.sync.bfly.b32 	%r79|%p18, %r77, %r61, %r48, %r50;
	mov.b32 	%f216, %r79;
	add.f32 	%f217, %f215, %f216;
	st.local.f32 	[%rd2], %f217;

$L__BB55_11:
	bar.sync 	0;
	mov.b32 	%r80, %f388;
	shfl.sync.bfly.b32 	%r84|%p20, %r80, %r49, %r48, %r50;
	mov.b32 	%f218, %r84;
	add.f32 	%f219, %f388, %f218;
	mov.b32 	%r85, %f219;
	shfl.sync.bfly.b32 	%r87|%p21, %r85, %r53, %r48, %r50;
	mov.b32 	%f220, %r87;
	add.f32 	%f221, %f219, %f220;
	mov.b32 	%r88, %f221;
	shfl.sync.bfly.b32 	%r90|%p22, %r88, %r56, %r48, %r50;
	mov.b32 	%f222, %r90;
	add.f32 	%f223, %f221, %f222;
	mov.b32 	%r91, %f223;
	shfl.sync.bfly.b32 	%r93|%p23, %r91, %r46, %r48, %r50;
	mov.b32 	%f224, %r93;
	add.f32 	%f225, %f223, %f224;
	mov.b32 	%r94, %f225;
	shfl.sync.bfly.b32 	%r96|%p24, %r94, %r61, %r48, %r50;
	mov.b32 	%f226, %r96;
	add.f32 	%f227, %f225, %f226;
	st.local.f32 	[%rd2+4], %f227;
	st.shared.f32 	[%r10], %f227;
	bar.sync 	0;
	@%p1 bra 	$L__BB55_13;

	ld.shared.f32 	%f228, [%r4];
	mov.b32 	%r97, %f228;
	mov.u32 	%r98, 31;
	mov.u32 	%r99, 16;
	mov.u32 	%r100, -1;
	shfl.sync.bfly.b32 	%r101|%p25, %r97, %r99, %r98, %r100;
	mov.b32 	%f229, %r101;
	add.f32 	%f230, %f228, %f229;
	mov.b32 	%r102, %f230;
	mov.u32 	%r103, 8;
	shfl.sync.bfly.b32 	%r104|%p26, %r102, %r103, %r98, %r100;
	mov.b32 	%f231, %r104;
	add.f32 	%f232, %f230, %f231;
	mov.b32 	%r105, %f232;
	mov.u32 	%r106, 4;
	shfl.sync.bfly.b32 	%r107|%p27, %r105, %r106, %r98, %r100;
	mov.b32 	%f233, %r107;
	add.f32 	%f234, %f232, %f233;
	mov.b32 	%r108, %f234;
	mov.u32 	%r109, 2;
	shfl.sync.bfly.b32 	%r110|%p28, %r108, %r109, %r98, %r100;
	mov.b32 	%f235, %r110;
	add.f32 	%f236, %f234, %f235;
	mov.b32 	%r111, %f236;
	mov.u32 	%r112, 1;
	shfl.sync.bfly.b32 	%r113|%p29, %r111, %r112, %r98, %r100;
	mov.b32 	%f237, %r113;
	add.f32 	%f238, %f236, %f237;
	st.local.f32 	[%rd2+4], %f238;

$L__BB55_13:
	bar.sync 	0;
	mov.b32 	%r114, %f387;
	mov.u32 	%r115, 31;
	mov.u32 	%r116, 16;
	mov.u32 	%r117, -1;
	shfl.sync.bfly.b32 	%r118|%p31, %r114, %r116, %r115, %r117;
	mov.b32 	%f239, %r118;
	add.f32 	%f240, %f387, %f239;
	mov.b32 	%r119, %f240;
	mov.u32 	%r120, 8;
	shfl.sync.bfly.b32 	%r121|%p32, %r119, %r120, %r115, %r117;
	mov.b32 	%f241, %r121;
	add.f32 	%f242, %f240, %f241;
	mov.b32 	%r122, %f242;
	mov.u32 	%r123, 4;
	shfl.sync.bfly.b32 	%r124|%p33, %r122, %r123, %r115, %r117;
	mov.b32 	%f243, %r124;
	add.f32 	%f244, %f242, %f243;
	mov.b32 	%r125, %f244;
	mov.u32 	%r126, 2;
	shfl.sync.bfly.b32 	%r127|%p34, %r125, %r126, %r115, %r117;
	mov.b32 	%f245, %r127;
	add.f32 	%f246, %f244, %f245;
	mov.b32 	%r128, %f246;
	mov.u32 	%r129, 1;
	shfl.sync.bfly.b32 	%r130|%p35, %r128, %r129, %r115, %r117;
	mov.b32 	%f247, %r130;
	add.f32 	%f248, %f246, %f247;
	st.local.f32 	[%rd2+8], %f248;
	st.shared.f32 	[%r10], %f248;
	bar.sync 	0;
	@%p1 bra 	$L__BB55_15;

	ld.shared.f32 	%f249, [%r4];
	mov.b32 	%r131, %f249;
	shfl.sync.bfly.b32 	%r135|%p36, %r131, %r116, %r115, %r117;
	mov.b32 	%f250, %r135;
	add.f32 	%f251, %f249, %f250;
	mov.b32 	%r136, %f251;
	shfl.sync.bfly.b32 	%r138|%p37, %r136, %r120, %r115, %r117;
	mov.b32 	%f252, %r138;
	add.f32 	%f253, %f251, %f252;
	mov.b32 	%r139, %f253;
	shfl.sync.bfly.b32 	%r141|%p38, %r139, %r123, %r115, %r117;
	mov.b32 	%f254, %r141;
	add.f32 	%f255, %f253, %f254;
	mov.b32 	%r142, %f255;
	shfl.sync.bfly.b32 	%r144|%p39, %r142, %r126, %r115, %r117;
	mov.b32 	%f256, %r144;
	add.f32 	%f257, %f255, %f256;
	mov.b32 	%r145, %f257;
	shfl.sync.bfly.b32 	%r147|%p40, %r145, %r129, %r115, %r117;
	mov.b32 	%f258, %r147;
	add.f32 	%f259, %f257, %f258;
	st.local.f32 	[%rd2+8], %f259;

$L__BB55_15:
	bar.sync 	0;
	mov.b32 	%r148, %f386;
	shfl.sync.bfly.b32 	%r152|%p42, %r148, %r116, %r115, %r117;
	mov.b32 	%f260, %r152;
	add.f32 	%f261, %f386, %f260;
	mov.b32 	%r153, %f261;
	shfl.sync.bfly.b32 	%r155|%p43, %r153, %r120, %r115, %r117;
	mov.b32 	%f262, %r155;
	add.f32 	%f263, %f261, %f262;
	mov.b32 	%r156, %f263;
	shfl.sync.bfly.b32 	%r158|%p44, %r156, %r123, %r115, %r117;
	mov.b32 	%f264, %r158;
	add.f32 	%f265, %f263, %f264;
	mov.b32 	%r159, %f265;
	shfl.sync.bfly.b32 	%r161|%p45, %r159, %r126, %r115, %r117;
	mov.b32 	%f266, %r161;
	add.f32 	%f267, %f265, %f266;
	mov.b32 	%r162, %f267;
	shfl.sync.bfly.b32 	%r164|%p46, %r162, %r129, %r115, %r117;
	mov.b32 	%f268, %r164;
	add.f32 	%f269, %f267, %f268;
	st.local.f32 	[%rd2+12], %f269;
	st.shared.f32 	[%r10], %f269;
	bar.sync 	0;
	@%p1 bra 	$L__BB55_17;

	ld.shared.f32 	%f270, [%r4];
	mov.b32 	%r165, %f270;
	mov.u32 	%r166, 31;
	mov.u32 	%r167, 16;
	mov.u32 	%r168, -1;
	shfl.sync.bfly.b32 	%r169|%p47, %r165, %r167, %r166, %r168;
	mov.b32 	%f271, %r169;
	add.f32 	%f272, %f270, %f271;
	mov.b32 	%r170, %f272;
	mov.u32 	%r171, 8;
	shfl.sync.bfly.b32 	%r172|%p48, %r170, %r171, %r166, %r168;
	mov.b32 	%f273, %r172;
	add.f32 	%f274, %f272, %f273;
	mov.b32 	%r173, %f274;
	mov.u32 	%r174, 4;
	shfl.sync.bfly.b32 	%r175|%p49, %r173, %r174, %r166, %r168;
	mov.b32 	%f275, %r175;
	add.f32 	%f276, %f274, %f275;
	mov.b32 	%r176, %f276;
	mov.u32 	%r177, 2;
	shfl.sync.bfly.b32 	%r178|%p50, %r176, %r177, %r166, %r168;
	mov.b32 	%f277, %r178;
	add.f32 	%f278, %f276, %f277;
	mov.b32 	%r179, %f278;
	mov.u32 	%r180, 1;
	shfl.sync.bfly.b32 	%r181|%p51, %r179, %r180, %r166, %r168;
	mov.b32 	%f279, %r181;
	add.f32 	%f280, %f278, %f279;
	st.local.f32 	[%rd2+12], %f280;

$L__BB55_17:
	bar.sync 	0;
	mov.b32 	%r182, %f385;
	mov.u32 	%r183, 31;
	mov.u32 	%r184, 16;
	mov.u32 	%r185, -1;
	shfl.sync.bfly.b32 	%r186|%p53, %r182, %r184, %r183, %r185;
	mov.b32 	%f281, %r186;
	add.f32 	%f282, %f385, %f281;
	mov.b32 	%r187, %f282;
	mov.u32 	%r188, 8;
	shfl.sync.bfly.b32 	%r189|%p54, %r187, %r188, %r183, %r185;
	mov.b32 	%f283, %r189;
	add.f32 	%f284, %f282, %f283;
	mov.b32 	%r190, %f284;
	mov.u32 	%r191, 4;
	shfl.sync.bfly.b32 	%r192|%p55, %r190, %r191, %r183, %r185;
	mov.b32 	%f285, %r192;
	add.f32 	%f286, %f284, %f285;
	mov.b32 	%r193, %f286;
	mov.u32 	%r194, 2;
	shfl.sync.bfly.b32 	%r195|%p56, %r193, %r194, %r183, %r185;
	mov.b32 	%f287, %r195;
	add.f32 	%f288, %f286, %f287;
	mov.b32 	%r196, %f288;
	mov.u32 	%r197, 1;
	shfl.sync.bfly.b32 	%r198|%p57, %r196, %r197, %r183, %r185;
	mov.b32 	%f289, %r198;
	add.f32 	%f290, %f288, %f289;
	st.local.f32 	[%rd2+16], %f290;
	st.shared.f32 	[%r10], %f290;
	bar.sync 	0;
	@%p1 bra 	$L__BB55_19;

	ld.shared.f32 	%f291, [%r4];
	mov.b32 	%r199, %f291;
	shfl.sync.bfly.b32 	%r203|%p58, %r199, %r184, %r183, %r185;
	mov.b32 	%f292, %r203;
	add.f32 	%f293, %f291, %f292;
	mov.b32 	%r204, %f293;
	shfl.sync.bfly.b32 	%r206|%p59, %r204, %r188, %r183, %r185;
	mov.b32 	%f294, %r206;
	add.f32 	%f295, %f293, %f294;
	mov.b32 	%r207, %f295;
	shfl.sync.bfly.b32 	%r209|%p60, %r207, %r191, %r183, %r185;
	mov.b32 	%f296, %r209;
	add.f32 	%f297, %f295, %f296;
	mov.b32 	%r210, %f297;
	shfl.sync.bfly.b32 	%r212|%p61, %r210, %r194, %r183, %r185;
	mov.b32 	%f298, %r212;
	add.f32 	%f299, %f297, %f298;
	mov.b32 	%r213, %f299;
	shfl.sync.bfly.b32 	%r215|%p62, %r213, %r197, %r183, %r185;
	mov.b32 	%f300, %r215;
	add.f32 	%f301, %f299, %f300;
	st.local.f32 	[%rd2+16], %f301;

$L__BB55_19:
	bar.sync 	0;
	mov.b32 	%r216, %f384;
	shfl.sync.bfly.b32 	%r220|%p64, %r216, %r184, %r183, %r185;
	mov.b32 	%f302, %r220;
	add.f32 	%f303, %f384, %f302;
	mov.b32 	%r221, %f303;
	shfl.sync.bfly.b32 	%r223|%p65, %r221, %r188, %r183, %r185;
	mov.b32 	%f304, %r223;
	add.f32 	%f305, %f303, %f304;
	mov.b32 	%r224, %f305;
	shfl.sync.bfly.b32 	%r226|%p66, %r224, %r191, %r183, %r185;
	mov.b32 	%f306, %r226;
	add.f32 	%f307, %f305, %f306;
	mov.b32 	%r227, %f307;
	shfl.sync.bfly.b32 	%r229|%p67, %r227, %r194, %r183, %r185;
	mov.b32 	%f308, %r229;
	add.f32 	%f309, %f307, %f308;
	mov.b32 	%r230, %f309;
	shfl.sync.bfly.b32 	%r232|%p68, %r230, %r197, %r183, %r185;
	mov.b32 	%f310, %r232;
	add.f32 	%f311, %f309, %f310;
	st.local.f32 	[%rd2+20], %f311;
	st.shared.f32 	[%r10], %f311;
	bar.sync 	0;
	@%p1 bra 	$L__BB55_21;

	ld.shared.f32 	%f312, [%r4];
	mov.b32 	%r233, %f312;
	mov.u32 	%r234, 31;
	mov.u32 	%r235, 16;
	mov.u32 	%r236, -1;
	shfl.sync.bfly.b32 	%r237|%p69, %r233, %r235, %r234, %r236;
	mov.b32 	%f313, %r237;
	add.f32 	%f314, %f312, %f313;
	mov.b32 	%r238, %f314;
	mov.u32 	%r239, 8;
	shfl.sync.bfly.b32 	%r240|%p70, %r238, %r239, %r234, %r236;
	mov.b32 	%f315, %r240;
	add.f32 	%f316, %f314, %f315;
	mov.b32 	%r241, %f316;
	mov.u32 	%r242, 4;
	shfl.sync.bfly.b32 	%r243|%p71, %r241, %r242, %r234, %r236;
	mov.b32 	%f317, %r243;
	add.f32 	%f318, %f316, %f317;
	mov.b32 	%r244, %f318;
	mov.u32 	%r245, 2;
	shfl.sync.bfly.b32 	%r246|%p72, %r244, %r245, %r234, %r236;
	mov.b32 	%f319, %r246;
	add.f32 	%f320, %f318, %f319;
	mov.b32 	%r247, %f320;
	mov.u32 	%r248, 1;
	shfl.sync.bfly.b32 	%r249|%p73, %r247, %r248, %r234, %r236;
	mov.b32 	%f321, %r249;
	add.f32 	%f322, %f320, %f321;
	st.local.f32 	[%rd2+20], %f322;

$L__BB55_21:
	bar.sync 	0;
	mov.b32 	%r250, %f383;
	mov.u32 	%r251, 31;
	mov.u32 	%r252, 16;
	mov.u32 	%r253, -1;
	shfl.sync.bfly.b32 	%r254|%p75, %r250, %r252, %r251, %r253;
	mov.b32 	%f323, %r254;
	add.f32 	%f324, %f383, %f323;
	mov.b32 	%r255, %f324;
	mov.u32 	%r256, 8;
	shfl.sync.bfly.b32 	%r257|%p76, %r255, %r256, %r251, %r253;
	mov.b32 	%f325, %r257;
	add.f32 	%f326, %f324, %f325;
	mov.b32 	%r258, %f326;
	mov.u32 	%r259, 4;
	shfl.sync.bfly.b32 	%r260|%p77, %r258, %r259, %r251, %r253;
	mov.b32 	%f327, %r260;
	add.f32 	%f328, %f326, %f327;
	mov.b32 	%r261, %f328;
	mov.u32 	%r262, 2;
	shfl.sync.bfly.b32 	%r263|%p78, %r261, %r262, %r251, %r253;
	mov.b32 	%f329, %r263;
	add.f32 	%f330, %f328, %f329;
	mov.b32 	%r264, %f330;
	mov.u32 	%r265, 1;
	shfl.sync.bfly.b32 	%r266|%p79, %r264, %r265, %r251, %r253;
	mov.b32 	%f331, %r266;
	add.f32 	%f332, %f330, %f331;
	st.local.f32 	[%rd2+24], %f332;
	st.shared.f32 	[%r10], %f332;
	bar.sync 	0;
	@%p1 bra 	$L__BB55_23;

	ld.shared.f32 	%f333, [%r4];
	mov.b32 	%r267, %f333;
	shfl.sync.bfly.b32 	%r271|%p80, %r267, %r252, %r251, %r253;
	mov.b32 	%f334, %r271;
	add.f32 	%f335, %f333, %f334;
	mov.b32 	%r272, %f335;
	shfl.sync.bfly.b32 	%r274|%p81, %r272, %r256, %r251, %r253;
	mov.b32 	%f336, %r274;
	add.f32 	%f337, %f335, %f336;
	mov.b32 	%r275, %f337;
	shfl.sync.bfly.b32 	%r277|%p82, %r275, %r259, %r251, %r253;
	mov.b32 	%f338, %r277;
	add.f32 	%f339, %f337, %f338;
	mov.b32 	%r278, %f339;
	shfl.sync.bfly.b32 	%r280|%p83, %r278, %r262, %r251, %r253;
	mov.b32 	%f340, %r280;
	add.f32 	%f341, %f339, %f340;
	mov.b32 	%r281, %f341;
	shfl.sync.bfly.b32 	%r283|%p84, %r281, %r265, %r251, %r253;
	mov.b32 	%f342, %r283;
	add.f32 	%f343, %f341, %f342;
	st.local.f32 	[%rd2+24], %f343;

$L__BB55_23:
	bar.sync 	0;
	mov.b32 	%r284, %f382;
	shfl.sync.bfly.b32 	%r288|%p86, %r284, %r252, %r251, %r253;
	mov.b32 	%f344, %r288;
	add.f32 	%f345, %f382, %f344;
	mov.b32 	%r289, %f345;
	shfl.sync.bfly.b32 	%r291|%p87, %r289, %r256, %r251, %r253;
	mov.b32 	%f346, %r291;
	add.f32 	%f347, %f345, %f346;
	mov.b32 	%r292, %f347;
	shfl.sync.bfly.b32 	%r294|%p88, %r292, %r259, %r251, %r253;
	mov.b32 	%f348, %r294;
	add.f32 	%f349, %f347, %f348;
	mov.b32 	%r295, %f349;
	shfl.sync.bfly.b32 	%r297|%p89, %r295, %r262, %r251, %r253;
	mov.b32 	%f350, %r297;
	add.f32 	%f351, %f349, %f350;
	mov.b32 	%r298, %f351;
	shfl.sync.bfly.b32 	%r300|%p90, %r298, %r265, %r251, %r253;
	mov.b32 	%f352, %r300;
	add.f32 	%f353, %f351, %f352;
	st.local.f32 	[%rd2+28], %f353;
	st.shared.f32 	[%r10], %f353;
	bar.sync 	0;
	@%p1 bra 	$L__BB55_25;

	ld.shared.f32 	%f354, [%r4];
	mov.b32 	%r301, %f354;
	mov.u32 	%r302, 31;
	mov.u32 	%r303, 16;
	mov.u32 	%r304, -1;
	shfl.sync.bfly.b32 	%r305|%p91, %r301, %r303, %r302, %r304;
	mov.b32 	%f355, %r305;
	add.f32 	%f356, %f354, %f355;
	mov.b32 	%r306, %f356;
	mov.u32 	%r307, 8;
	shfl.sync.bfly.b32 	%r308|%p92, %r306, %r307, %r302, %r304;
	mov.b32 	%f357, %r308;
	add.f32 	%f358, %f356, %f357;
	mov.b32 	%r309, %f358;
	mov.u32 	%r310, 4;
	shfl.sync.bfly.b32 	%r311|%p93, %r309, %r310, %r302, %r304;
	mov.b32 	%f359, %r311;
	add.f32 	%f360, %f358, %f359;
	mov.b32 	%r312, %f360;
	mov.u32 	%r313, 2;
	shfl.sync.bfly.b32 	%r314|%p94, %r312, %r313, %r302, %r304;
	mov.b32 	%f361, %r314;
	add.f32 	%f362, %f360, %f361;
	mov.b32 	%r315, %f362;
	mov.u32 	%r316, 1;
	shfl.sync.bfly.b32 	%r317|%p95, %r315, %r316, %r302, %r304;
	mov.b32 	%f363, %r317;
	add.f32 	%f364, %f362, %f363;
	st.local.f32 	[%rd2+28], %f364;

$L__BB55_25:
	bar.sync 	0;
	setp.gt.s32 	%p96, %r3, 7;
	@%p96 bra 	$L__BB55_27;

	mul.wide.s32 	%rd68, %r3, 4;
	add.s64 	%rd69, %rd2, %rd68;
	ld.local.f32 	%f365, [%rd69];
	mad.lo.s32 	%r318, %r3, %r13, %r2;
	cvt.s64.s32 	%rd70, %r318;
	mul.lo.s32 	%r319, %r1, %r14;
	cvt.s64.s32 	%rd71, %r319;
	add.s64 	%rd72, %rd71, %rd70;
	cvta.to.global.u64 	%rd73, %rd21;
	shl.b64 	%rd74, %rd72, 2;
	add.s64 	%rd75, %rd73, %rd74;
	st.global.f32 	[%rd75], %f365;

$L__BB55_27:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_1_bs_256
.visible .entry ggml_matvec_f32_ncols_1_bs_256(
	.param .u64 ggml_matvec_f32_ncols_1_bs_256_param_0,
	.param .u64 ggml_matvec_f32_ncols_1_bs_256_param_1,
	.param .u64 ggml_matvec_f32_ncols_1_bs_256_param_2,
	.param .u32 ggml_matvec_f32_ncols_1_bs_256_param_3,
	.param .u32 ggml_matvec_f32_ncols_1_bs_256_param_4,
	.param .u32 ggml_matvec_f32_ncols_1_bs_256_param_5,
	.param .u32 ggml_matvec_f32_ncols_1_bs_256_param_6,
	.param .u32 ggml_matvec_f32_ncols_1_bs_256_param_7,
	.param .u32 ggml_matvec_f32_ncols_1_bs_256_param_8,
	.param .u32 ggml_matvec_f32_ncols_1_bs_256_param_9,
	.param .u32 ggml_matvec_f32_ncols_1_bs_256_param_10,
	.param .u32 ggml_matvec_f32_ncols_1_bs_256_param_11
)
{
	.reg .pred 	%p<19>;
	.reg .f32 	%f<88>;
	.reg .b32 	%r<79>;
	.reg .b64 	%rd<42>;


	ld.param.u64 	%rd18, [ggml_matvec_f32_ncols_1_bs_256_param_0];
	ld.param.u64 	%rd19, [ggml_matvec_f32_ncols_1_bs_256_param_1];
	ld.param.u64 	%rd17, [ggml_matvec_f32_ncols_1_bs_256_param_2];
	ld.param.u32 	%r13, [ggml_matvec_f32_ncols_1_bs_256_param_3];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_1_bs_256_param_5];
	ld.param.u32 	%r14, [ggml_matvec_f32_ncols_1_bs_256_param_7];
	ld.param.u32 	%r17, [ggml_matvec_f32_ncols_1_bs_256_param_8];
	ld.param.u32 	%r18, [ggml_matvec_f32_ncols_1_bs_256_param_9];
	ld.param.u32 	%r19, [ggml_matvec_f32_ncols_1_bs_256_param_10];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_1_bs_256_param_11];
	cvta.to.global.u64 	%rd1, %rd19;
	cvta.to.global.u64 	%rd2, %rd18;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r20, %r1, %r17;
	mov.u32 	%r21, %ctaid.x;
	mul.lo.s32 	%r22, %r21, %r16;
	mad.lo.s32 	%r23, %r20, %r18, %r22;
	cvt.s64.s32 	%rd3, %r23;
	mul.lo.s32 	%r24, %r1, %r19;
	cvt.s64.s32 	%rd4, %r24;
	mov.u32 	%r2, %tid.x;
	setp.gt.s32 	%p1, %r2, 31;
	shl.b32 	%r25, %r2, 2;
	mov.u32 	%r26, data_mmv;
	add.s32 	%r3, %r26, %r25;
	@%p1 bra 	$L__BB56_2;

	mov.u32 	%r27, 0;
	st.shared.u32 	[%r3], %r27;

$L__BB56_2:
	bar.sync 	0;
	setp.ge.s32 	%p2, %r2, %r13;
	mov.f32 	%f86, 0f00000000;
	@%p2 bra 	$L__BB56_9;

	not.b32 	%r28, %r2;
	add.s32 	%r4, %r28, %r13;
	shr.u32 	%r29, %r4, 8;
	add.s32 	%r30, %r29, 1;
	and.b32  	%r76, %r30, 3;
	setp.eq.s32 	%p3, %r76, 0;
	mov.f32 	%f86, 0f00000000;
	mov.u32 	%r77, %r2;
	@%p3 bra 	$L__BB56_6;

	mul.wide.s32 	%rd20, %r2, 2;
	add.s64 	%rd21, %rd20, %rd4;
	shl.b64 	%rd22, %rd21, 2;
	add.s64 	%rd39, %rd1, %rd22;
	add.s64 	%rd23, %rd20, %rd3;
	shl.b64 	%rd24, %rd23, 2;
	add.s64 	%rd38, %rd2, %rd24;
	mov.f32 	%f86, 0f00000000;
	mov.u32 	%r77, %r2;

$L__BB56_5:
	.pragma "nounroll";
	ld.global.nc.v2.f32 	{%f15, %f16}, [%rd38];
	ld.global.nc.v2.f32 	{%f19, %f20}, [%rd39];
	fma.rn.f32 	%f23, %f15, %f19, %f86;
	fma.rn.f32 	%f86, %f16, %f20, %f23;
	add.s32 	%r77, %r77, 256;
	add.s64 	%rd39, %rd39, 2048;
	add.s64 	%rd38, %rd38, 2048;
	add.s32 	%r76, %r76, -1;
	setp.ne.s32 	%p4, %r76, 0;
	@%p4 bra 	$L__BB56_5;

$L__BB56_6:
	setp.lt.u32 	%p5, %r4, 768;
	@%p5 bra 	$L__BB56_9;

	mul.wide.s32 	%rd25, %r77, 2;
	add.s64 	%rd26, %rd25, %rd3;
	shl.b64 	%rd27, %rd26, 2;
	add.s64 	%rd28, %rd2, %rd27;
	add.s64 	%rd41, %rd28, 4096;
	add.s64 	%rd29, %rd25, %rd4;
	shl.b64 	%rd30, %rd29, 2;
	add.s64 	%rd31, %rd1, %rd30;
	add.s64 	%rd40, %rd31, 4096;

$L__BB56_8:
	ld.global.nc.v2.f32 	{%f24, %f25}, [%rd41+-4096];
	ld.global.nc.v2.f32 	{%f28, %f29}, [%rd40+-4096];
	fma.rn.f32 	%f32, %f24, %f28, %f86;
	fma.rn.f32 	%f33, %f25, %f29, %f32;
	ld.global.nc.v2.f32 	{%f34, %f35}, [%rd41+-2048];
	ld.global.nc.v2.f32 	{%f38, %f39}, [%rd40+-2048];
	fma.rn.f32 	%f42, %f34, %f38, %f33;
	fma.rn.f32 	%f43, %f35, %f39, %f42;
	ld.global.nc.v2.f32 	{%f44, %f45}, [%rd41];
	ld.global.nc.v2.f32 	{%f48, %f49}, [%rd40];
	fma.rn.f32 	%f52, %f44, %f48, %f43;
	fma.rn.f32 	%f53, %f45, %f49, %f52;
	ld.global.nc.v2.f32 	{%f54, %f55}, [%rd41+2048];
	ld.global.nc.v2.f32 	{%f58, %f59}, [%rd40+2048];
	fma.rn.f32 	%f62, %f54, %f58, %f53;
	fma.rn.f32 	%f86, %f55, %f59, %f62;
	add.s64 	%rd41, %rd41, 8192;
	add.s64 	%rd40, %rd40, 8192;
	add.s32 	%r77, %r77, 1024;
	setp.lt.s32 	%p6, %r77, %r13;
	@%p6 bra 	$L__BB56_8;

$L__BB56_9:
	mov.b32 	%r31, %f86;
	mov.u32 	%r32, 31;
	mov.u32 	%r33, 16;
	mov.u32 	%r34, -1;
	shfl.sync.bfly.b32 	%r35|%p7, %r31, %r33, %r32, %r34;
	mov.b32 	%f63, %r35;
	add.f32 	%f64, %f86, %f63;
	mov.b32 	%r36, %f64;
	mov.u32 	%r37, 8;
	shfl.sync.bfly.b32 	%r38|%p8, %r36, %r37, %r32, %r34;
	mov.b32 	%f65, %r38;
	add.f32 	%f66, %f64, %f65;
	mov.b32 	%r39, %f66;
	mov.u32 	%r40, 4;
	shfl.sync.bfly.b32 	%r41|%p9, %r39, %r40, %r32, %r34;
	mov.b32 	%f67, %r41;
	add.f32 	%f68, %f66, %f67;
	mov.b32 	%r42, %f68;
	mov.u32 	%r43, 2;
	shfl.sync.bfly.b32 	%r44|%p10, %r42, %r43, %r32, %r34;
	mov.b32 	%f69, %r44;
	add.f32 	%f70, %f68, %f69;
	mov.b32 	%r45, %f70;
	mov.u32 	%r46, 1;
	shfl.sync.bfly.b32 	%r47|%p11, %r45, %r46, %r32, %r34;
	mov.b32 	%f71, %r47;
	add.f32 	%f87, %f70, %f71;
	shr.s32 	%r48, %r2, 31;
	shr.u32 	%r49, %r48, 27;
	add.s32 	%r50, %r2, %r49;
	shr.s32 	%r51, %r50, 5;
	shl.b32 	%r52, %r51, 2;
	add.s32 	%r54, %r26, %r52;
	st.shared.f32 	[%r54], %f87;
	bar.sync 	0;
	@%p1 bra 	$L__BB56_11;

	ld.shared.f32 	%f72, [%r3];
	mov.b32 	%r55, %f72;
	shfl.sync.bfly.b32 	%r59|%p13, %r55, %r33, %r32, %r34;
	mov.b32 	%f73, %r59;
	add.f32 	%f74, %f72, %f73;
	mov.b32 	%r60, %f74;
	shfl.sync.bfly.b32 	%r62|%p14, %r60, %r37, %r32, %r34;
	mov.b32 	%f75, %r62;
	add.f32 	%f76, %f74, %f75;
	mov.b32 	%r63, %f76;
	shfl.sync.bfly.b32 	%r65|%p15, %r63, %r40, %r32, %r34;
	mov.b32 	%f77, %r65;
	add.f32 	%f78, %f76, %f77;
	mov.b32 	%r66, %f78;
	shfl.sync.bfly.b32 	%r68|%p16, %r66, %r43, %r32, %r34;
	mov.b32 	%f79, %r68;
	add.f32 	%f80, %f78, %f79;
	mov.b32 	%r69, %f80;
	shfl.sync.bfly.b32 	%r71|%p17, %r69, %r46, %r32, %r34;
	mov.b32 	%f81, %r71;
	add.f32 	%f87, %f80, %f81;

$L__BB56_11:
	bar.sync 	0;
	setp.gt.s32 	%p18, %r2, 0;
	@%p18 bra 	$L__BB56_13;

	mad.lo.s32 	%r73, %r2, %r14, %r21;
	cvt.s64.s32 	%rd32, %r73;
	mul.lo.s32 	%r74, %r1, %r15;
	cvt.s64.s32 	%rd33, %r74;
	add.s64 	%rd34, %rd33, %rd32;
	cvta.to.global.u64 	%rd35, %rd17;
	shl.b64 	%rd36, %rd34, 2;
	add.s64 	%rd37, %rd35, %rd36;
	st.global.f32 	[%rd37], %f87;

$L__BB56_13:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_2_bs_256
.visible .entry ggml_matvec_f32_ncols_2_bs_256(
	.param .u64 ggml_matvec_f32_ncols_2_bs_256_param_0,
	.param .u64 ggml_matvec_f32_ncols_2_bs_256_param_1,
	.param .u64 ggml_matvec_f32_ncols_2_bs_256_param_2,
	.param .u32 ggml_matvec_f32_ncols_2_bs_256_param_3,
	.param .u32 ggml_matvec_f32_ncols_2_bs_256_param_4,
	.param .u32 ggml_matvec_f32_ncols_2_bs_256_param_5,
	.param .u32 ggml_matvec_f32_ncols_2_bs_256_param_6,
	.param .u32 ggml_matvec_f32_ncols_2_bs_256_param_7,
	.param .u32 ggml_matvec_f32_ncols_2_bs_256_param_8,
	.param .u32 ggml_matvec_f32_ncols_2_bs_256_param_9,
	.param .u32 ggml_matvec_f32_ncols_2_bs_256_param_10,
	.param .u32 ggml_matvec_f32_ncols_2_bs_256_param_11
)
{
	.local .align 8 .b8 	__local_depot57[8];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<30>;
	.reg .f32 	%f<146>;
	.reg .b32 	%r<113>;
	.reg .b64 	%rd<64>;


	mov.u64 	%SPL, __local_depot57;
	ld.param.u64 	%rd27, [ggml_matvec_f32_ncols_2_bs_256_param_0];
	ld.param.u64 	%rd28, [ggml_matvec_f32_ncols_2_bs_256_param_1];
	ld.param.u64 	%rd26, [ggml_matvec_f32_ncols_2_bs_256_param_2];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_2_bs_256_param_3];
	ld.param.u32 	%r19, [ggml_matvec_f32_ncols_2_bs_256_param_5];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_2_bs_256_param_6];
	ld.param.u32 	%r17, [ggml_matvec_f32_ncols_2_bs_256_param_7];
	ld.param.u32 	%r20, [ggml_matvec_f32_ncols_2_bs_256_param_8];
	ld.param.u32 	%r21, [ggml_matvec_f32_ncols_2_bs_256_param_9];
	ld.param.u32 	%r22, [ggml_matvec_f32_ncols_2_bs_256_param_10];
	ld.param.u32 	%r18, [ggml_matvec_f32_ncols_2_bs_256_param_11];
	cvta.to.global.u64 	%rd1, %rd28;
	cvta.to.global.u64 	%rd2, %rd27;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r23, %r1, %r20;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r24, %r2, %r19;
	mad.lo.s32 	%r25, %r23, %r21, %r24;
	cvt.s64.s32 	%rd4, %r25;
	mul.lo.s32 	%r26, %r1, %r22;
	cvt.s64.s32 	%rd5, %r26;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r27, %r3, 2;
	mov.u32 	%r28, data_mmv;
	add.s32 	%r4, %r28, %r27;
	@%p1 bra 	$L__BB57_2;

	mov.u32 	%r29, 0;
	st.shared.u32 	[%r4], %r29;

$L__BB57_2:
	bar.sync 	0;
	mov.f32 	%f144, 0f00000000;
	st.local.v2.f32 	[%rd3], {%f144, %f144};
	setp.ge.s32 	%p2, %r3, %r15;
	mov.f32 	%f145, %f144;
	@%p2 bra 	$L__BB57_11;

	not.b32 	%r30, %r3;
	add.s32 	%r5, %r30, %r15;
	shr.u32 	%r31, %r5, 8;
	add.s32 	%r32, %r31, 1;
	and.b32  	%r110, %r32, 3;
	setp.eq.s32 	%p3, %r110, 0;
	mov.f32 	%f144, 0f00000000;
	mov.u32 	%r111, %r3;
	@%p3 bra 	$L__BB57_7;

	mul.wide.s32 	%rd30, %r16, 2;
	mul.wide.s32 	%rd31, %r3, 2;
	add.s64 	%rd32, %rd30, %rd31;
	add.s64 	%rd33, %rd32, %rd5;
	shl.b64 	%rd34, %rd33, 2;
	add.s64 	%rd60, %rd1, %rd34;
	add.s64 	%rd35, %rd31, %rd5;
	shl.b64 	%rd36, %rd35, 2;
	add.s64 	%rd59, %rd1, %rd36;
	add.s64 	%rd37, %rd31, %rd4;
	shl.b64 	%rd38, %rd37, 2;
	add.s64 	%rd58, %rd2, %rd38;
	mov.f32 	%f144, 0f00000000;
	mov.f32 	%f145, %f144;
	mov.u32 	%r111, %r3;

$L__BB57_5:
	.pragma "nounroll";
	ld.global.nc.v2.f32 	{%f19, %f20}, [%rd58];
	ld.global.nc.v2.f32 	{%f23, %f24}, [%rd59];
	fma.rn.f32 	%f27, %f19, %f23, %f145;
	fma.rn.f32 	%f145, %f20, %f24, %f27;
	ld.global.nc.v2.f32 	{%f28, %f29}, [%rd60];
	fma.rn.f32 	%f32, %f19, %f28, %f144;
	fma.rn.f32 	%f144, %f20, %f29, %f32;
	add.s32 	%r111, %r111, 256;
	add.s64 	%rd60, %rd60, 2048;
	add.s64 	%rd59, %rd59, 2048;
	add.s64 	%rd58, %rd58, 2048;
	add.s32 	%r110, %r110, -1;
	setp.ne.s32 	%p4, %r110, 0;
	@%p4 bra 	$L__BB57_5;

	st.local.v2.f32 	[%rd3], {%f145, %f144};

$L__BB57_7:
	setp.lt.u32 	%p5, %r5, 768;
	@%p5 bra 	$L__BB57_11;

	mul.wide.s32 	%rd39, %r111, 2;
	add.s64 	%rd40, %rd39, %rd4;
	shl.b64 	%rd41, %rd40, 2;
	add.s64 	%rd42, %rd2, %rd41;
	add.s64 	%rd63, %rd42, 4096;
	add.s64 	%rd43, %rd39, %rd5;
	shl.b64 	%rd44, %rd43, 2;
	add.s64 	%rd45, %rd1, %rd44;
	add.s64 	%rd62, %rd45, 6144;
	mul.wide.s32 	%rd46, %r16, 2;
	add.s64 	%rd47, %rd43, %rd46;
	shl.b64 	%rd48, %rd47, 2;
	add.s64 	%rd49, %rd1, %rd48;
	add.s64 	%rd61, %rd49, 4096;

$L__BB57_9:
	ld.global.nc.v2.f32 	{%f33, %f34}, [%rd63+-4096];
	ld.global.nc.v2.f32 	{%f37, %f38}, [%rd62+-6144];
	fma.rn.f32 	%f41, %f33, %f37, %f145;
	fma.rn.f32 	%f42, %f34, %f38, %f41;
	ld.global.nc.v2.f32 	{%f43, %f44}, [%rd61+-4096];
	fma.rn.f32 	%f47, %f33, %f43, %f144;
	fma.rn.f32 	%f48, %f34, %f44, %f47;
	ld.global.nc.v2.f32 	{%f49, %f50}, [%rd63+-2048];
	ld.global.nc.v2.f32 	{%f53, %f54}, [%rd62+-4096];
	fma.rn.f32 	%f57, %f49, %f53, %f42;
	fma.rn.f32 	%f58, %f50, %f54, %f57;
	ld.global.nc.v2.f32 	{%f59, %f60}, [%rd61+-2048];
	fma.rn.f32 	%f63, %f49, %f59, %f48;
	fma.rn.f32 	%f64, %f50, %f60, %f63;
	ld.global.nc.v2.f32 	{%f65, %f66}, [%rd63];
	ld.global.nc.v2.f32 	{%f69, %f70}, [%rd62+-2048];
	fma.rn.f32 	%f73, %f65, %f69, %f58;
	fma.rn.f32 	%f74, %f66, %f70, %f73;
	ld.global.nc.v2.f32 	{%f75, %f76}, [%rd61];
	fma.rn.f32 	%f79, %f65, %f75, %f64;
	fma.rn.f32 	%f80, %f66, %f76, %f79;
	ld.global.nc.v2.f32 	{%f81, %f82}, [%rd63+2048];
	ld.global.nc.v2.f32 	{%f85, %f86}, [%rd62];
	fma.rn.f32 	%f89, %f81, %f85, %f74;
	fma.rn.f32 	%f145, %f82, %f86, %f89;
	ld.global.nc.v2.f32 	{%f90, %f91}, [%rd61+2048];
	fma.rn.f32 	%f94, %f81, %f90, %f80;
	fma.rn.f32 	%f144, %f82, %f91, %f94;
	add.s64 	%rd63, %rd63, 8192;
	add.s64 	%rd62, %rd62, 8192;
	add.s64 	%rd61, %rd61, 8192;
	add.s32 	%r111, %r111, 1024;
	setp.lt.s32 	%p6, %r111, %r15;
	@%p6 bra 	$L__BB57_9;

	st.local.v2.f32 	[%rd3], {%f145, %f144};

$L__BB57_11:
	shr.s32 	%r33, %r3, 31;
	shr.u32 	%r34, %r33, 27;
	add.s32 	%r35, %r3, %r34;
	shr.s32 	%r36, %r35, 5;
	shl.b32 	%r37, %r36, 2;
	add.s32 	%r14, %r28, %r37;
	mov.u32 	%r39, 2;
	mov.b32 	%r40, %f145;
	mov.u32 	%r41, 31;
	mov.u32 	%r42, 16;
	mov.u32 	%r43, -1;
	shfl.sync.bfly.b32 	%r44|%p7, %r40, %r42, %r41, %r43;
	mov.b32 	%f95, %r44;
	add.f32 	%f96, %f145, %f95;
	mov.b32 	%r45, %f96;
	mov.u32 	%r46, 8;
	shfl.sync.bfly.b32 	%r47|%p8, %r45, %r46, %r41, %r43;
	mov.b32 	%f97, %r47;
	add.f32 	%f98, %f96, %f97;
	mov.b32 	%r48, %f98;
	mov.u32 	%r49, 4;
	shfl.sync.bfly.b32 	%r50|%p9, %r48, %r49, %r41, %r43;
	mov.b32 	%f99, %r50;
	add.f32 	%f100, %f98, %f99;
	mov.b32 	%r51, %f100;
	shfl.sync.bfly.b32 	%r52|%p10, %r51, %r39, %r41, %r43;
	mov.b32 	%f101, %r52;
	add.f32 	%f102, %f100, %f101;
	mov.b32 	%r53, %f102;
	mov.u32 	%r54, 1;
	shfl.sync.bfly.b32 	%r55|%p11, %r53, %r54, %r41, %r43;
	mov.b32 	%f103, %r55;
	add.f32 	%f104, %f102, %f103;
	st.local.f32 	[%rd3], %f104;
	st.shared.f32 	[%r14], %f104;
	bar.sync 	0;
	@%p1 bra 	$L__BB57_13;

	ld.shared.f32 	%f105, [%r4];
	mov.b32 	%r56, %f105;
	shfl.sync.bfly.b32 	%r60|%p13, %r56, %r42, %r41, %r43;
	mov.b32 	%f106, %r60;
	add.f32 	%f107, %f105, %f106;
	mov.b32 	%r61, %f107;
	shfl.sync.bfly.b32 	%r63|%p14, %r61, %r46, %r41, %r43;
	mov.b32 	%f108, %r63;
	add.f32 	%f109, %f107, %f108;
	mov.b32 	%r64, %f109;
	shfl.sync.bfly.b32 	%r66|%p15, %r64, %r49, %r41, %r43;
	mov.b32 	%f110, %r66;
	add.f32 	%f111, %f109, %f110;
	mov.b32 	%r67, %f111;
	shfl.sync.bfly.b32 	%r69|%p16, %r67, %r39, %r41, %r43;
	mov.b32 	%f112, %r69;
	add.f32 	%f113, %f111, %f112;
	mov.b32 	%r70, %f113;
	shfl.sync.bfly.b32 	%r72|%p17, %r70, %r54, %r41, %r43;
	mov.b32 	%f114, %r72;
	add.f32 	%f115, %f113, %f114;
	st.local.f32 	[%rd3], %f115;

$L__BB57_13:
	bar.sync 	0;
	mov.b32 	%r73, %f144;
	shfl.sync.bfly.b32 	%r77|%p19, %r73, %r42, %r41, %r43;
	mov.b32 	%f116, %r77;
	add.f32 	%f117, %f144, %f116;
	mov.b32 	%r78, %f117;
	shfl.sync.bfly.b32 	%r80|%p20, %r78, %r46, %r41, %r43;
	mov.b32 	%f118, %r80;
	add.f32 	%f119, %f117, %f118;
	mov.b32 	%r81, %f119;
	shfl.sync.bfly.b32 	%r83|%p21, %r81, %r49, %r41, %r43;
	mov.b32 	%f120, %r83;
	add.f32 	%f121, %f119, %f120;
	mov.b32 	%r84, %f121;
	shfl.sync.bfly.b32 	%r86|%p22, %r84, %r39, %r41, %r43;
	mov.b32 	%f122, %r86;
	add.f32 	%f123, %f121, %f122;
	mov.b32 	%r87, %f123;
	shfl.sync.bfly.b32 	%r89|%p23, %r87, %r54, %r41, %r43;
	mov.b32 	%f124, %r89;
	add.f32 	%f125, %f123, %f124;
	st.local.f32 	[%rd3+4], %f125;
	st.shared.f32 	[%r14], %f125;
	bar.sync 	0;
	@%p1 bra 	$L__BB57_15;

	ld.shared.f32 	%f126, [%r4];
	mov.b32 	%r90, %f126;
	mov.u32 	%r91, 31;
	mov.u32 	%r92, 16;
	mov.u32 	%r93, -1;
	shfl.sync.bfly.b32 	%r94|%p24, %r90, %r92, %r91, %r93;
	mov.b32 	%f127, %r94;
	add.f32 	%f128, %f126, %f127;
	mov.b32 	%r95, %f128;
	mov.u32 	%r96, 8;
	shfl.sync.bfly.b32 	%r97|%p25, %r95, %r96, %r91, %r93;
	mov.b32 	%f129, %r97;
	add.f32 	%f130, %f128, %f129;
	mov.b32 	%r98, %f130;
	mov.u32 	%r99, 4;
	shfl.sync.bfly.b32 	%r100|%p26, %r98, %r99, %r91, %r93;
	mov.b32 	%f131, %r100;
	add.f32 	%f132, %f130, %f131;
	mov.b32 	%r101, %f132;
	mov.u32 	%r102, 2;
	shfl.sync.bfly.b32 	%r103|%p27, %r101, %r102, %r91, %r93;
	mov.b32 	%f133, %r103;
	add.f32 	%f134, %f132, %f133;
	mov.b32 	%r104, %f134;
	mov.u32 	%r105, 1;
	shfl.sync.bfly.b32 	%r106|%p28, %r104, %r105, %r91, %r93;
	mov.b32 	%f135, %r106;
	add.f32 	%f136, %f134, %f135;
	st.local.f32 	[%rd3+4], %f136;

$L__BB57_15:
	bar.sync 	0;
	setp.gt.s32 	%p29, %r3, 1;
	@%p29 bra 	$L__BB57_17;

	mul.wide.s32 	%rd50, %r3, 4;
	add.s64 	%rd51, %rd3, %rd50;
	ld.local.f32 	%f137, [%rd51];
	mad.lo.s32 	%r107, %r3, %r17, %r2;
	cvt.s64.s32 	%rd52, %r107;
	mul.lo.s32 	%r108, %r1, %r18;
	cvt.s64.s32 	%rd53, %r108;
	add.s64 	%rd54, %rd53, %rd52;
	cvta.to.global.u64 	%rd55, %rd26;
	shl.b64 	%rd56, %rd54, 2;
	add.s64 	%rd57, %rd55, %rd56;
	st.global.f32 	[%rd57], %f137;

$L__BB57_17:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_3_bs_256
.visible .entry ggml_matvec_f32_ncols_3_bs_256(
	.param .u64 ggml_matvec_f32_ncols_3_bs_256_param_0,
	.param .u64 ggml_matvec_f32_ncols_3_bs_256_param_1,
	.param .u64 ggml_matvec_f32_ncols_3_bs_256_param_2,
	.param .u32 ggml_matvec_f32_ncols_3_bs_256_param_3,
	.param .u32 ggml_matvec_f32_ncols_3_bs_256_param_4,
	.param .u32 ggml_matvec_f32_ncols_3_bs_256_param_5,
	.param .u32 ggml_matvec_f32_ncols_3_bs_256_param_6,
	.param .u32 ggml_matvec_f32_ncols_3_bs_256_param_7,
	.param .u32 ggml_matvec_f32_ncols_3_bs_256_param_8,
	.param .u32 ggml_matvec_f32_ncols_3_bs_256_param_9,
	.param .u32 ggml_matvec_f32_ncols_3_bs_256_param_10,
	.param .u32 ggml_matvec_f32_ncols_3_bs_256_param_11
)
{
	.local .align 4 .b8 	__local_depot58[12];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<41>;
	.reg .f32 	%f<208>;
	.reg .b32 	%r<154>;
	.reg .b64 	%rd<72>;


	mov.u64 	%SPL, __local_depot58;
	ld.param.u64 	%rd29, [ggml_matvec_f32_ncols_3_bs_256_param_0];
	ld.param.u64 	%rd30, [ggml_matvec_f32_ncols_3_bs_256_param_1];
	ld.param.u64 	%rd28, [ggml_matvec_f32_ncols_3_bs_256_param_2];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_3_bs_256_param_3];
	ld.param.u32 	%r19, [ggml_matvec_f32_ncols_3_bs_256_param_5];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_3_bs_256_param_6];
	ld.param.u32 	%r17, [ggml_matvec_f32_ncols_3_bs_256_param_7];
	ld.param.u32 	%r20, [ggml_matvec_f32_ncols_3_bs_256_param_8];
	ld.param.u32 	%r21, [ggml_matvec_f32_ncols_3_bs_256_param_9];
	ld.param.u32 	%r22, [ggml_matvec_f32_ncols_3_bs_256_param_10];
	ld.param.u32 	%r18, [ggml_matvec_f32_ncols_3_bs_256_param_11];
	cvta.to.global.u64 	%rd71, %rd30;
	cvta.to.global.u64 	%rd2, %rd29;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r23, %r1, %r20;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r24, %r2, %r19;
	mad.lo.s32 	%r25, %r23, %r21, %r24;
	cvt.s64.s32 	%rd4, %r25;
	mul.lo.s32 	%r26, %r1, %r22;
	cvt.s64.s32 	%rd5, %r26;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r27, %r3, 2;
	mov.u32 	%r28, data_mmv;
	add.s32 	%r4, %r28, %r27;
	@%p1 bra 	$L__BB58_2;

	mov.u32 	%r29, 0;
	st.shared.u32 	[%r4], %r29;

$L__BB58_2:
	bar.sync 	0;
	mov.f32 	%f205, 0f00000000;
	mov.u32 	%r30, 0;
	st.local.u32 	[%rd3], %r30;
	st.local.u32 	[%rd3+4], %r30;
	st.local.u32 	[%rd3+8], %r30;
	setp.ge.s32 	%p2, %r3, %r15;
	mov.f32 	%f206, %f205;
	mov.f32 	%f207, %f205;
	@%p2 bra 	$L__BB58_11;

	not.b32 	%r31, %r3;
	add.s32 	%r5, %r31, %r15;
	shr.u32 	%r32, %r5, 8;
	add.s32 	%r33, %r32, 1;
	and.b32  	%r151, %r33, 3;
	setp.eq.s32 	%p3, %r151, 0;
	mov.f32 	%f205, 0f00000000;
	mov.u32 	%r152, %r3;
	@%p3 bra 	$L__BB58_7;

	shl.b32 	%r34, %r16, 1;
	add.s32 	%r35, %r3, %r34;
	mul.wide.s32 	%rd32, %r35, 2;
	add.s64 	%rd33, %rd32, %rd5;
	shl.b64 	%rd34, %rd33, 2;
	add.s64 	%rd69, %rd71, %rd34;
	mul.wide.s32 	%rd35, %r16, 2;
	mul.wide.s32 	%rd36, %r3, 2;
	add.s64 	%rd37, %rd35, %rd36;
	add.s64 	%rd38, %rd37, %rd5;
	shl.b64 	%rd39, %rd38, 2;
	add.s64 	%rd68, %rd71, %rd39;
	add.s64 	%rd40, %rd36, %rd5;
	shl.b64 	%rd41, %rd40, 2;
	add.s64 	%rd67, %rd71, %rd41;
	add.s64 	%rd42, %rd36, %rd4;
	shl.b64 	%rd43, %rd42, 2;
	add.s64 	%rd66, %rd2, %rd43;
	mov.f32 	%f205, 0f00000000;
	mov.f32 	%f206, %f205;
	mov.f32 	%f207, %f205;
	mov.u32 	%r152, %r3;

$L__BB58_5:
	.pragma "nounroll";
	ld.global.nc.v2.f32 	{%f28, %f29}, [%rd66];
	ld.global.nc.v2.f32 	{%f32, %f33}, [%rd67];
	fma.rn.f32 	%f36, %f28, %f32, %f207;
	fma.rn.f32 	%f207, %f29, %f33, %f36;
	ld.global.nc.v2.f32 	{%f37, %f38}, [%rd68];
	fma.rn.f32 	%f41, %f28, %f37, %f206;
	fma.rn.f32 	%f206, %f29, %f38, %f41;
	ld.global.nc.v2.f32 	{%f42, %f43}, [%rd69];
	fma.rn.f32 	%f46, %f28, %f42, %f205;
	fma.rn.f32 	%f205, %f29, %f43, %f46;
	add.s32 	%r152, %r152, 256;
	add.s64 	%rd69, %rd69, 2048;
	add.s64 	%rd68, %rd68, 2048;
	add.s64 	%rd67, %rd67, 2048;
	add.s64 	%rd66, %rd66, 2048;
	add.s32 	%r151, %r151, -1;
	setp.ne.s32 	%p4, %r151, 0;
	@%p4 bra 	$L__BB58_5;

	st.local.f32 	[%rd3], %f207;
	st.local.f32 	[%rd3+4], %f206;
	st.local.f32 	[%rd3+8], %f205;

$L__BB58_7:
	setp.lt.u32 	%p5, %r5, 768;
	@%p5 bra 	$L__BB58_11;

	add.s32 	%r36, %r152, %r16;
	shl.b32 	%r37, %r16, 1;
	add.s32 	%r38, %r152, %r37;
	add.s32 	%r39, %r36, 256;
	mul.wide.s32 	%rd44, %r39, 8;
	shl.b64 	%rd45, %rd5, 2;
	add.s64 	%rd19, %rd44, %rd45;
	mul.wide.s32 	%rd46, %r38, 8;
	add.s64 	%rd20, %rd46, %rd45;
	mul.wide.s32 	%rd47, %r152, 2;
	add.s64 	%rd48, %rd47, %rd4;
	shl.b64 	%rd49, %rd48, 2;
	add.s64 	%rd50, %rd2, %rd49;
	add.s64 	%rd70, %rd50, 4096;
	mul.wide.s32 	%rd51, %r152, 8;
	add.s64 	%rd22, %rd51, %rd45;
	mul.wide.s32 	%rd52, %r16, 8;
	add.s64 	%rd53, %rd51, %rd52;
	add.s64 	%rd23, %rd53, %rd45;

$L__BB58_9:
	ld.global.nc.v2.f32 	{%f47, %f48}, [%rd70+-4096];
	add.s64 	%rd54, %rd71, %rd22;
	ld.global.nc.v2.f32 	{%f51, %f52}, [%rd54];
	fma.rn.f32 	%f55, %f47, %f51, %f207;
	fma.rn.f32 	%f56, %f48, %f52, %f55;
	add.s64 	%rd55, %rd71, %rd23;
	ld.global.nc.v2.f32 	{%f57, %f58}, [%rd55];
	fma.rn.f32 	%f61, %f47, %f57, %f206;
	fma.rn.f32 	%f62, %f48, %f58, %f61;
	add.s64 	%rd56, %rd71, %rd20;
	ld.global.nc.v2.f32 	{%f63, %f64}, [%rd56];
	fma.rn.f32 	%f67, %f47, %f63, %f205;
	fma.rn.f32 	%f68, %f48, %f64, %f67;
	ld.global.nc.v2.f32 	{%f69, %f70}, [%rd70+-2048];
	ld.global.nc.v2.f32 	{%f73, %f74}, [%rd54+2048];
	fma.rn.f32 	%f77, %f69, %f73, %f56;
	fma.rn.f32 	%f78, %f70, %f74, %f77;
	add.s64 	%rd57, %rd71, %rd19;
	ld.global.nc.v2.f32 	{%f79, %f80}, [%rd57];
	fma.rn.f32 	%f83, %f69, %f79, %f62;
	fma.rn.f32 	%f84, %f70, %f80, %f83;
	ld.global.nc.v2.f32 	{%f85, %f86}, [%rd56+2048];
	fma.rn.f32 	%f89, %f69, %f85, %f68;
	fma.rn.f32 	%f90, %f70, %f86, %f89;
	ld.global.nc.v2.f32 	{%f91, %f92}, [%rd70];
	ld.global.nc.v2.f32 	{%f95, %f96}, [%rd54+4096];
	fma.rn.f32 	%f99, %f91, %f95, %f78;
	fma.rn.f32 	%f100, %f92, %f96, %f99;
	ld.global.nc.v2.f32 	{%f101, %f102}, [%rd57+2048];
	fma.rn.f32 	%f105, %f91, %f101, %f84;
	fma.rn.f32 	%f106, %f92, %f102, %f105;
	ld.global.nc.v2.f32 	{%f107, %f108}, [%rd56+4096];
	fma.rn.f32 	%f111, %f91, %f107, %f90;
	fma.rn.f32 	%f112, %f92, %f108, %f111;
	ld.global.nc.v2.f32 	{%f113, %f114}, [%rd70+2048];
	ld.global.nc.v2.f32 	{%f117, %f118}, [%rd54+6144];
	fma.rn.f32 	%f121, %f113, %f117, %f100;
	fma.rn.f32 	%f207, %f114, %f118, %f121;
	ld.global.nc.v2.f32 	{%f122, %f123}, [%rd57+4096];
	fma.rn.f32 	%f126, %f113, %f122, %f106;
	fma.rn.f32 	%f206, %f114, %f123, %f126;
	ld.global.nc.v2.f32 	{%f127, %f128}, [%rd56+6144];
	fma.rn.f32 	%f131, %f113, %f127, %f112;
	fma.rn.f32 	%f205, %f114, %f128, %f131;
	add.s64 	%rd71, %rd71, 8192;
	add.s64 	%rd70, %rd70, 8192;
	add.s32 	%r152, %r152, 1024;
	setp.lt.s32 	%p6, %r152, %r15;
	@%p6 bra 	$L__BB58_9;

	st.local.f32 	[%rd3], %f207;
	st.local.f32 	[%rd3+4], %f206;
	st.local.f32 	[%rd3+8], %f205;

$L__BB58_11:
	shr.s32 	%r40, %r3, 31;
	shr.u32 	%r41, %r40, 27;
	add.s32 	%r42, %r3, %r41;
	shr.s32 	%r43, %r42, 5;
	shl.b32 	%r44, %r43, 2;
	add.s32 	%r14, %r28, %r44;
	mov.u32 	%r46, 2;
	mov.b32 	%r47, %f207;
	mov.u32 	%r48, 31;
	mov.u32 	%r49, 16;
	mov.u32 	%r50, -1;
	shfl.sync.bfly.b32 	%r51|%p7, %r47, %r49, %r48, %r50;
	mov.b32 	%f132, %r51;
	add.f32 	%f133, %f207, %f132;
	mov.b32 	%r52, %f133;
	mov.u32 	%r53, 8;
	shfl.sync.bfly.b32 	%r54|%p8, %r52, %r53, %r48, %r50;
	mov.b32 	%f134, %r54;
	add.f32 	%f135, %f133, %f134;
	mov.b32 	%r55, %f135;
	mov.u32 	%r56, 4;
	shfl.sync.bfly.b32 	%r57|%p9, %r55, %r56, %r48, %r50;
	mov.b32 	%f136, %r57;
	add.f32 	%f137, %f135, %f136;
	mov.b32 	%r58, %f137;
	shfl.sync.bfly.b32 	%r59|%p10, %r58, %r46, %r48, %r50;
	mov.b32 	%f138, %r59;
	add.f32 	%f139, %f137, %f138;
	mov.b32 	%r60, %f139;
	mov.u32 	%r61, 1;
	shfl.sync.bfly.b32 	%r62|%p11, %r60, %r61, %r48, %r50;
	mov.b32 	%f140, %r62;
	add.f32 	%f141, %f139, %f140;
	st.local.f32 	[%rd3], %f141;
	st.shared.f32 	[%r14], %f141;
	bar.sync 	0;
	@%p1 bra 	$L__BB58_13;

	ld.shared.f32 	%f142, [%r4];
	mov.b32 	%r63, %f142;
	shfl.sync.bfly.b32 	%r67|%p13, %r63, %r49, %r48, %r50;
	mov.b32 	%f143, %r67;
	add.f32 	%f144, %f142, %f143;
	mov.b32 	%r68, %f144;
	shfl.sync.bfly.b32 	%r70|%p14, %r68, %r53, %r48, %r50;
	mov.b32 	%f145, %r70;
	add.f32 	%f146, %f144, %f145;
	mov.b32 	%r71, %f146;
	shfl.sync.bfly.b32 	%r73|%p15, %r71, %r56, %r48, %r50;
	mov.b32 	%f147, %r73;
	add.f32 	%f148, %f146, %f147;
	mov.b32 	%r74, %f148;
	shfl.sync.bfly.b32 	%r76|%p16, %r74, %r46, %r48, %r50;
	mov.b32 	%f149, %r76;
	add.f32 	%f150, %f148, %f149;
	mov.b32 	%r77, %f150;
	shfl.sync.bfly.b32 	%r79|%p17, %r77, %r61, %r48, %r50;
	mov.b32 	%f151, %r79;
	add.f32 	%f152, %f150, %f151;
	st.local.f32 	[%rd3], %f152;

$L__BB58_13:
	bar.sync 	0;
	mov.b32 	%r80, %f206;
	shfl.sync.bfly.b32 	%r84|%p19, %r80, %r49, %r48, %r50;
	mov.b32 	%f153, %r84;
	add.f32 	%f154, %f206, %f153;
	mov.b32 	%r85, %f154;
	shfl.sync.bfly.b32 	%r87|%p20, %r85, %r53, %r48, %r50;
	mov.b32 	%f155, %r87;
	add.f32 	%f156, %f154, %f155;
	mov.b32 	%r88, %f156;
	shfl.sync.bfly.b32 	%r90|%p21, %r88, %r56, %r48, %r50;
	mov.b32 	%f157, %r90;
	add.f32 	%f158, %f156, %f157;
	mov.b32 	%r91, %f158;
	shfl.sync.bfly.b32 	%r93|%p22, %r91, %r46, %r48, %r50;
	mov.b32 	%f159, %r93;
	add.f32 	%f160, %f158, %f159;
	mov.b32 	%r94, %f160;
	shfl.sync.bfly.b32 	%r96|%p23, %r94, %r61, %r48, %r50;
	mov.b32 	%f161, %r96;
	add.f32 	%f162, %f160, %f161;
	st.local.f32 	[%rd3+4], %f162;
	st.shared.f32 	[%r14], %f162;
	bar.sync 	0;
	@%p1 bra 	$L__BB58_15;

	ld.shared.f32 	%f163, [%r4];
	mov.b32 	%r97, %f163;
	mov.u32 	%r98, 31;
	mov.u32 	%r99, 16;
	mov.u32 	%r100, -1;
	shfl.sync.bfly.b32 	%r101|%p24, %r97, %r99, %r98, %r100;
	mov.b32 	%f164, %r101;
	add.f32 	%f165, %f163, %f164;
	mov.b32 	%r102, %f165;
	mov.u32 	%r103, 8;
	shfl.sync.bfly.b32 	%r104|%p25, %r102, %r103, %r98, %r100;
	mov.b32 	%f166, %r104;
	add.f32 	%f167, %f165, %f166;
	mov.b32 	%r105, %f167;
	mov.u32 	%r106, 4;
	shfl.sync.bfly.b32 	%r107|%p26, %r105, %r106, %r98, %r100;
	mov.b32 	%f168, %r107;
	add.f32 	%f169, %f167, %f168;
	mov.b32 	%r108, %f169;
	mov.u32 	%r109, 2;
	shfl.sync.bfly.b32 	%r110|%p27, %r108, %r109, %r98, %r100;
	mov.b32 	%f170, %r110;
	add.f32 	%f171, %f169, %f170;
	mov.b32 	%r111, %f171;
	mov.u32 	%r112, 1;
	shfl.sync.bfly.b32 	%r113|%p28, %r111, %r112, %r98, %r100;
	mov.b32 	%f172, %r113;
	add.f32 	%f173, %f171, %f172;
	st.local.f32 	[%rd3+4], %f173;

$L__BB58_15:
	bar.sync 	0;
	mov.b32 	%r114, %f205;
	mov.u32 	%r115, 31;
	mov.u32 	%r116, 16;
	mov.u32 	%r117, -1;
	shfl.sync.bfly.b32 	%r118|%p30, %r114, %r116, %r115, %r117;
	mov.b32 	%f174, %r118;
	add.f32 	%f175, %f205, %f174;
	mov.b32 	%r119, %f175;
	mov.u32 	%r120, 8;
	shfl.sync.bfly.b32 	%r121|%p31, %r119, %r120, %r115, %r117;
	mov.b32 	%f176, %r121;
	add.f32 	%f177, %f175, %f176;
	mov.b32 	%r122, %f177;
	mov.u32 	%r123, 4;
	shfl.sync.bfly.b32 	%r124|%p32, %r122, %r123, %r115, %r117;
	mov.b32 	%f178, %r124;
	add.f32 	%f179, %f177, %f178;
	mov.b32 	%r125, %f179;
	mov.u32 	%r126, 2;
	shfl.sync.bfly.b32 	%r127|%p33, %r125, %r126, %r115, %r117;
	mov.b32 	%f180, %r127;
	add.f32 	%f181, %f179, %f180;
	mov.b32 	%r128, %f181;
	mov.u32 	%r129, 1;
	shfl.sync.bfly.b32 	%r130|%p34, %r128, %r129, %r115, %r117;
	mov.b32 	%f182, %r130;
	add.f32 	%f183, %f181, %f182;
	st.local.f32 	[%rd3+8], %f183;
	st.shared.f32 	[%r14], %f183;
	bar.sync 	0;
	@%p1 bra 	$L__BB58_17;

	ld.shared.f32 	%f184, [%r4];
	mov.b32 	%r131, %f184;
	shfl.sync.bfly.b32 	%r135|%p35, %r131, %r116, %r115, %r117;
	mov.b32 	%f185, %r135;
	add.f32 	%f186, %f184, %f185;
	mov.b32 	%r136, %f186;
	shfl.sync.bfly.b32 	%r138|%p36, %r136, %r120, %r115, %r117;
	mov.b32 	%f187, %r138;
	add.f32 	%f188, %f186, %f187;
	mov.b32 	%r139, %f188;
	shfl.sync.bfly.b32 	%r141|%p37, %r139, %r123, %r115, %r117;
	mov.b32 	%f189, %r141;
	add.f32 	%f190, %f188, %f189;
	mov.b32 	%r142, %f190;
	shfl.sync.bfly.b32 	%r144|%p38, %r142, %r126, %r115, %r117;
	mov.b32 	%f191, %r144;
	add.f32 	%f192, %f190, %f191;
	mov.b32 	%r145, %f192;
	shfl.sync.bfly.b32 	%r147|%p39, %r145, %r129, %r115, %r117;
	mov.b32 	%f193, %r147;
	add.f32 	%f194, %f192, %f193;
	st.local.f32 	[%rd3+8], %f194;

$L__BB58_17:
	bar.sync 	0;
	setp.gt.s32 	%p40, %r3, 2;
	@%p40 bra 	$L__BB58_19;

	mul.wide.s32 	%rd58, %r3, 4;
	add.s64 	%rd59, %rd3, %rd58;
	ld.local.f32 	%f195, [%rd59];
	mad.lo.s32 	%r148, %r3, %r17, %r2;
	cvt.s64.s32 	%rd60, %r148;
	mul.lo.s32 	%r149, %r1, %r18;
	cvt.s64.s32 	%rd61, %r149;
	add.s64 	%rd62, %rd61, %rd60;
	cvta.to.global.u64 	%rd63, %rd28;
	shl.b64 	%rd64, %rd62, 2;
	add.s64 	%rd65, %rd63, %rd64;
	st.global.f32 	[%rd65], %f195;

$L__BB58_19:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_4_bs_256
.visible .entry ggml_matvec_f32_ncols_4_bs_256(
	.param .u64 ggml_matvec_f32_ncols_4_bs_256_param_0,
	.param .u64 ggml_matvec_f32_ncols_4_bs_256_param_1,
	.param .u64 ggml_matvec_f32_ncols_4_bs_256_param_2,
	.param .u32 ggml_matvec_f32_ncols_4_bs_256_param_3,
	.param .u32 ggml_matvec_f32_ncols_4_bs_256_param_4,
	.param .u32 ggml_matvec_f32_ncols_4_bs_256_param_5,
	.param .u32 ggml_matvec_f32_ncols_4_bs_256_param_6,
	.param .u32 ggml_matvec_f32_ncols_4_bs_256_param_7,
	.param .u32 ggml_matvec_f32_ncols_4_bs_256_param_8,
	.param .u32 ggml_matvec_f32_ncols_4_bs_256_param_9,
	.param .u32 ggml_matvec_f32_ncols_4_bs_256_param_10,
	.param .u32 ggml_matvec_f32_ncols_4_bs_256_param_11
)
{
	.local .align 16 .b8 	__local_depot59[16];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<52>;
	.reg .f32 	%f<270>;
	.reg .b32 	%r<189>;
	.reg .b64 	%rd<83>;


	mov.u64 	%SPL, __local_depot59;
	ld.param.u64 	%rd34, [ggml_matvec_f32_ncols_4_bs_256_param_0];
	ld.param.u64 	%rd35, [ggml_matvec_f32_ncols_4_bs_256_param_1];
	ld.param.u64 	%rd33, [ggml_matvec_f32_ncols_4_bs_256_param_2];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_4_bs_256_param_3];
	ld.param.u32 	%r19, [ggml_matvec_f32_ncols_4_bs_256_param_5];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_4_bs_256_param_6];
	ld.param.u32 	%r17, [ggml_matvec_f32_ncols_4_bs_256_param_7];
	ld.param.u32 	%r20, [ggml_matvec_f32_ncols_4_bs_256_param_8];
	ld.param.u32 	%r21, [ggml_matvec_f32_ncols_4_bs_256_param_9];
	ld.param.u32 	%r22, [ggml_matvec_f32_ncols_4_bs_256_param_10];
	ld.param.u32 	%r18, [ggml_matvec_f32_ncols_4_bs_256_param_11];
	cvta.to.global.u64 	%rd82, %rd35;
	cvta.to.global.u64 	%rd2, %rd34;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r23, %r1, %r20;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r24, %r2, %r19;
	mad.lo.s32 	%r25, %r23, %r21, %r24;
	cvt.s64.s32 	%rd4, %r25;
	mul.lo.s32 	%r26, %r1, %r22;
	cvt.s64.s32 	%rd5, %r26;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r27, %r3, 2;
	mov.u32 	%r28, data_mmv;
	add.s32 	%r4, %r28, %r27;
	@%p1 bra 	$L__BB59_2;

	mov.u32 	%r29, 0;
	st.shared.u32 	[%r4], %r29;

$L__BB59_2:
	bar.sync 	0;
	mov.f32 	%f266, 0f00000000;
	st.local.v4.f32 	[%rd3], {%f266, %f266, %f266, %f266};
	setp.ge.s32 	%p2, %r3, %r15;
	mov.f32 	%f267, %f266;
	mov.f32 	%f268, %f266;
	mov.f32 	%f269, %f266;
	@%p2 bra 	$L__BB59_11;

	not.b32 	%r30, %r3;
	add.s32 	%r5, %r30, %r15;
	shr.u32 	%r31, %r5, 8;
	add.s32 	%r32, %r31, 1;
	and.b32  	%r186, %r32, 3;
	setp.eq.s32 	%p3, %r186, 0;
	mov.f32 	%f266, 0f00000000;
	mov.u32 	%r187, %r3;
	@%p3 bra 	$L__BB59_7;

	shl.b32 	%r33, %r16, 1;
	add.s32 	%r34, %r3, %r33;
	mul.wide.s32 	%rd37, %r34, 2;
	add.s64 	%rd38, %rd37, %rd5;
	shl.b64 	%rd39, %rd38, 2;
	add.s64 	%rd80, %rd82, %rd39;
	mad.lo.s32 	%r35, %r16, 3, %r3;
	mul.wide.s32 	%rd40, %r35, 2;
	add.s64 	%rd41, %rd40, %rd5;
	shl.b64 	%rd42, %rd41, 2;
	add.s64 	%rd79, %rd82, %rd42;
	mul.wide.s32 	%rd43, %r16, 2;
	mul.wide.s32 	%rd44, %r3, 2;
	add.s64 	%rd45, %rd43, %rd44;
	add.s64 	%rd46, %rd45, %rd5;
	shl.b64 	%rd47, %rd46, 2;
	add.s64 	%rd78, %rd82, %rd47;
	add.s64 	%rd48, %rd44, %rd5;
	shl.b64 	%rd49, %rd48, 2;
	add.s64 	%rd77, %rd82, %rd49;
	add.s64 	%rd50, %rd44, %rd4;
	shl.b64 	%rd51, %rd50, 2;
	add.s64 	%rd76, %rd2, %rd51;
	mov.f32 	%f266, 0f00000000;
	mov.f32 	%f267, %f266;
	mov.f32 	%f268, %f266;
	mov.f32 	%f269, %f266;
	mov.u32 	%r187, %r3;

$L__BB59_5:
	.pragma "nounroll";
	ld.global.nc.v2.f32 	{%f37, %f38}, [%rd76];
	ld.global.nc.v2.f32 	{%f41, %f42}, [%rd77];
	fma.rn.f32 	%f45, %f37, %f41, %f269;
	fma.rn.f32 	%f269, %f38, %f42, %f45;
	ld.global.nc.v2.f32 	{%f46, %f47}, [%rd78];
	fma.rn.f32 	%f50, %f37, %f46, %f268;
	fma.rn.f32 	%f268, %f38, %f47, %f50;
	ld.global.nc.v2.f32 	{%f51, %f52}, [%rd80];
	fma.rn.f32 	%f55, %f37, %f51, %f267;
	fma.rn.f32 	%f267, %f38, %f52, %f55;
	ld.global.nc.v2.f32 	{%f56, %f57}, [%rd79];
	fma.rn.f32 	%f60, %f37, %f56, %f266;
	fma.rn.f32 	%f266, %f38, %f57, %f60;
	add.s32 	%r187, %r187, 256;
	add.s64 	%rd80, %rd80, 2048;
	add.s64 	%rd79, %rd79, 2048;
	add.s64 	%rd78, %rd78, 2048;
	add.s64 	%rd77, %rd77, 2048;
	add.s64 	%rd76, %rd76, 2048;
	add.s32 	%r186, %r186, -1;
	setp.ne.s32 	%p4, %r186, 0;
	@%p4 bra 	$L__BB59_5;

	st.local.v4.f32 	[%rd3], {%f269, %f268, %f267, %f266};

$L__BB59_7:
	setp.lt.u32 	%p5, %r5, 768;
	@%p5 bra 	$L__BB59_11;

	add.s32 	%r36, %r187, %r16;
	shl.b32 	%r37, %r16, 1;
	add.s32 	%r38, %r187, %r37;
	mad.lo.s32 	%r39, %r16, 3, %r187;
	add.s32 	%r40, %r36, 256;
	mul.wide.s32 	%rd52, %r40, 8;
	shl.b64 	%rd53, %rd5, 2;
	add.s64 	%rd23, %rd52, %rd53;
	mul.wide.s32 	%rd54, %r38, 8;
	add.s64 	%rd24, %rd54, %rd53;
	mul.wide.s32 	%rd55, %r39, 8;
	add.s64 	%rd25, %rd55, %rd53;
	mul.wide.s32 	%rd56, %r187, 2;
	add.s64 	%rd57, %rd56, %rd4;
	shl.b64 	%rd58, %rd57, 2;
	add.s64 	%rd59, %rd2, %rd58;
	add.s64 	%rd81, %rd59, 4096;
	mul.wide.s32 	%rd60, %r187, 8;
	add.s64 	%rd27, %rd60, %rd53;
	mul.wide.s32 	%rd61, %r16, 8;
	add.s64 	%rd62, %rd60, %rd61;
	add.s64 	%rd28, %rd62, %rd53;

$L__BB59_9:
	ld.global.nc.v2.f32 	{%f61, %f62}, [%rd81+-4096];
	add.s64 	%rd63, %rd82, %rd27;
	ld.global.nc.v2.f32 	{%f65, %f66}, [%rd63];
	fma.rn.f32 	%f69, %f61, %f65, %f269;
	fma.rn.f32 	%f70, %f62, %f66, %f69;
	add.s64 	%rd64, %rd82, %rd28;
	ld.global.nc.v2.f32 	{%f71, %f72}, [%rd64];
	fma.rn.f32 	%f75, %f61, %f71, %f268;
	fma.rn.f32 	%f76, %f62, %f72, %f75;
	add.s64 	%rd65, %rd82, %rd24;
	ld.global.nc.v2.f32 	{%f77, %f78}, [%rd65];
	fma.rn.f32 	%f81, %f61, %f77, %f267;
	fma.rn.f32 	%f82, %f62, %f78, %f81;
	add.s64 	%rd66, %rd82, %rd25;
	ld.global.nc.v2.f32 	{%f83, %f84}, [%rd66];
	fma.rn.f32 	%f87, %f61, %f83, %f266;
	fma.rn.f32 	%f88, %f62, %f84, %f87;
	ld.global.nc.v2.f32 	{%f89, %f90}, [%rd81+-2048];
	ld.global.nc.v2.f32 	{%f93, %f94}, [%rd63+2048];
	fma.rn.f32 	%f97, %f89, %f93, %f70;
	fma.rn.f32 	%f98, %f90, %f94, %f97;
	add.s64 	%rd67, %rd82, %rd23;
	ld.global.nc.v2.f32 	{%f99, %f100}, [%rd67];
	fma.rn.f32 	%f103, %f89, %f99, %f76;
	fma.rn.f32 	%f104, %f90, %f100, %f103;
	ld.global.nc.v2.f32 	{%f105, %f106}, [%rd65+2048];
	fma.rn.f32 	%f109, %f89, %f105, %f82;
	fma.rn.f32 	%f110, %f90, %f106, %f109;
	ld.global.nc.v2.f32 	{%f111, %f112}, [%rd66+2048];
	fma.rn.f32 	%f115, %f89, %f111, %f88;
	fma.rn.f32 	%f116, %f90, %f112, %f115;
	ld.global.nc.v2.f32 	{%f117, %f118}, [%rd81];
	ld.global.nc.v2.f32 	{%f121, %f122}, [%rd63+4096];
	fma.rn.f32 	%f125, %f117, %f121, %f98;
	fma.rn.f32 	%f126, %f118, %f122, %f125;
	ld.global.nc.v2.f32 	{%f127, %f128}, [%rd67+2048];
	fma.rn.f32 	%f131, %f117, %f127, %f104;
	fma.rn.f32 	%f132, %f118, %f128, %f131;
	ld.global.nc.v2.f32 	{%f133, %f134}, [%rd65+4096];
	fma.rn.f32 	%f137, %f117, %f133, %f110;
	fma.rn.f32 	%f138, %f118, %f134, %f137;
	ld.global.nc.v2.f32 	{%f139, %f140}, [%rd66+4096];
	fma.rn.f32 	%f143, %f117, %f139, %f116;
	fma.rn.f32 	%f144, %f118, %f140, %f143;
	ld.global.nc.v2.f32 	{%f145, %f146}, [%rd81+2048];
	ld.global.nc.v2.f32 	{%f149, %f150}, [%rd63+6144];
	fma.rn.f32 	%f153, %f145, %f149, %f126;
	fma.rn.f32 	%f269, %f146, %f150, %f153;
	ld.global.nc.v2.f32 	{%f154, %f155}, [%rd67+4096];
	fma.rn.f32 	%f158, %f145, %f154, %f132;
	fma.rn.f32 	%f268, %f146, %f155, %f158;
	ld.global.nc.v2.f32 	{%f159, %f160}, [%rd65+6144];
	fma.rn.f32 	%f163, %f145, %f159, %f138;
	fma.rn.f32 	%f267, %f146, %f160, %f163;
	ld.global.nc.v2.f32 	{%f164, %f165}, [%rd66+6144];
	fma.rn.f32 	%f168, %f145, %f164, %f144;
	fma.rn.f32 	%f266, %f146, %f165, %f168;
	add.s64 	%rd82, %rd82, 8192;
	add.s64 	%rd81, %rd81, 8192;
	add.s32 	%r187, %r187, 1024;
	setp.lt.s32 	%p6, %r187, %r15;
	@%p6 bra 	$L__BB59_9;

	st.local.v4.f32 	[%rd3], {%f269, %f268, %f267, %f266};

$L__BB59_11:
	shr.s32 	%r41, %r3, 31;
	shr.u32 	%r42, %r41, 27;
	add.s32 	%r43, %r3, %r42;
	shr.s32 	%r44, %r43, 5;
	shl.b32 	%r45, %r44, 2;
	add.s32 	%r14, %r28, %r45;
	mov.u32 	%r47, 2;
	mov.b32 	%r48, %f269;
	mov.u32 	%r49, 31;
	mov.u32 	%r50, 16;
	mov.u32 	%r51, -1;
	shfl.sync.bfly.b32 	%r52|%p7, %r48, %r50, %r49, %r51;
	mov.b32 	%f169, %r52;
	add.f32 	%f170, %f269, %f169;
	mov.b32 	%r53, %f170;
	mov.u32 	%r54, 8;
	shfl.sync.bfly.b32 	%r55|%p8, %r53, %r54, %r49, %r51;
	mov.b32 	%f171, %r55;
	add.f32 	%f172, %f170, %f171;
	mov.b32 	%r56, %f172;
	mov.u32 	%r57, 4;
	shfl.sync.bfly.b32 	%r58|%p9, %r56, %r57, %r49, %r51;
	mov.b32 	%f173, %r58;
	add.f32 	%f174, %f172, %f173;
	mov.b32 	%r59, %f174;
	shfl.sync.bfly.b32 	%r60|%p10, %r59, %r47, %r49, %r51;
	mov.b32 	%f175, %r60;
	add.f32 	%f176, %f174, %f175;
	mov.b32 	%r61, %f176;
	mov.u32 	%r62, 1;
	shfl.sync.bfly.b32 	%r63|%p11, %r61, %r62, %r49, %r51;
	mov.b32 	%f177, %r63;
	add.f32 	%f178, %f176, %f177;
	st.local.f32 	[%rd3], %f178;
	st.shared.f32 	[%r14], %f178;
	bar.sync 	0;
	@%p1 bra 	$L__BB59_13;

	ld.shared.f32 	%f179, [%r4];
	mov.b32 	%r64, %f179;
	shfl.sync.bfly.b32 	%r68|%p13, %r64, %r50, %r49, %r51;
	mov.b32 	%f180, %r68;
	add.f32 	%f181, %f179, %f180;
	mov.b32 	%r69, %f181;
	shfl.sync.bfly.b32 	%r71|%p14, %r69, %r54, %r49, %r51;
	mov.b32 	%f182, %r71;
	add.f32 	%f183, %f181, %f182;
	mov.b32 	%r72, %f183;
	shfl.sync.bfly.b32 	%r74|%p15, %r72, %r57, %r49, %r51;
	mov.b32 	%f184, %r74;
	add.f32 	%f185, %f183, %f184;
	mov.b32 	%r75, %f185;
	shfl.sync.bfly.b32 	%r77|%p16, %r75, %r47, %r49, %r51;
	mov.b32 	%f186, %r77;
	add.f32 	%f187, %f185, %f186;
	mov.b32 	%r78, %f187;
	shfl.sync.bfly.b32 	%r80|%p17, %r78, %r62, %r49, %r51;
	mov.b32 	%f188, %r80;
	add.f32 	%f189, %f187, %f188;
	st.local.f32 	[%rd3], %f189;

$L__BB59_13:
	bar.sync 	0;
	mov.b32 	%r81, %f268;
	shfl.sync.bfly.b32 	%r85|%p19, %r81, %r50, %r49, %r51;
	mov.b32 	%f190, %r85;
	add.f32 	%f191, %f268, %f190;
	mov.b32 	%r86, %f191;
	shfl.sync.bfly.b32 	%r88|%p20, %r86, %r54, %r49, %r51;
	mov.b32 	%f192, %r88;
	add.f32 	%f193, %f191, %f192;
	mov.b32 	%r89, %f193;
	shfl.sync.bfly.b32 	%r91|%p21, %r89, %r57, %r49, %r51;
	mov.b32 	%f194, %r91;
	add.f32 	%f195, %f193, %f194;
	mov.b32 	%r92, %f195;
	shfl.sync.bfly.b32 	%r94|%p22, %r92, %r47, %r49, %r51;
	mov.b32 	%f196, %r94;
	add.f32 	%f197, %f195, %f196;
	mov.b32 	%r95, %f197;
	shfl.sync.bfly.b32 	%r97|%p23, %r95, %r62, %r49, %r51;
	mov.b32 	%f198, %r97;
	add.f32 	%f199, %f197, %f198;
	st.local.f32 	[%rd3+4], %f199;
	st.shared.f32 	[%r14], %f199;
	bar.sync 	0;
	@%p1 bra 	$L__BB59_15;

	ld.shared.f32 	%f200, [%r4];
	mov.b32 	%r98, %f200;
	mov.u32 	%r99, 31;
	mov.u32 	%r100, 16;
	mov.u32 	%r101, -1;
	shfl.sync.bfly.b32 	%r102|%p24, %r98, %r100, %r99, %r101;
	mov.b32 	%f201, %r102;
	add.f32 	%f202, %f200, %f201;
	mov.b32 	%r103, %f202;
	mov.u32 	%r104, 8;
	shfl.sync.bfly.b32 	%r105|%p25, %r103, %r104, %r99, %r101;
	mov.b32 	%f203, %r105;
	add.f32 	%f204, %f202, %f203;
	mov.b32 	%r106, %f204;
	mov.u32 	%r107, 4;
	shfl.sync.bfly.b32 	%r108|%p26, %r106, %r107, %r99, %r101;
	mov.b32 	%f205, %r108;
	add.f32 	%f206, %f204, %f205;
	mov.b32 	%r109, %f206;
	mov.u32 	%r110, 2;
	shfl.sync.bfly.b32 	%r111|%p27, %r109, %r110, %r99, %r101;
	mov.b32 	%f207, %r111;
	add.f32 	%f208, %f206, %f207;
	mov.b32 	%r112, %f208;
	mov.u32 	%r113, 1;
	shfl.sync.bfly.b32 	%r114|%p28, %r112, %r113, %r99, %r101;
	mov.b32 	%f209, %r114;
	add.f32 	%f210, %f208, %f209;
	st.local.f32 	[%rd3+4], %f210;

$L__BB59_15:
	bar.sync 	0;
	mov.b32 	%r115, %f267;
	mov.u32 	%r116, 31;
	mov.u32 	%r117, 16;
	mov.u32 	%r118, -1;
	shfl.sync.bfly.b32 	%r119|%p30, %r115, %r117, %r116, %r118;
	mov.b32 	%f211, %r119;
	add.f32 	%f212, %f267, %f211;
	mov.b32 	%r120, %f212;
	mov.u32 	%r121, 8;
	shfl.sync.bfly.b32 	%r122|%p31, %r120, %r121, %r116, %r118;
	mov.b32 	%f213, %r122;
	add.f32 	%f214, %f212, %f213;
	mov.b32 	%r123, %f214;
	mov.u32 	%r124, 4;
	shfl.sync.bfly.b32 	%r125|%p32, %r123, %r124, %r116, %r118;
	mov.b32 	%f215, %r125;
	add.f32 	%f216, %f214, %f215;
	mov.b32 	%r126, %f216;
	mov.u32 	%r127, 2;
	shfl.sync.bfly.b32 	%r128|%p33, %r126, %r127, %r116, %r118;
	mov.b32 	%f217, %r128;
	add.f32 	%f218, %f216, %f217;
	mov.b32 	%r129, %f218;
	mov.u32 	%r130, 1;
	shfl.sync.bfly.b32 	%r131|%p34, %r129, %r130, %r116, %r118;
	mov.b32 	%f219, %r131;
	add.f32 	%f220, %f218, %f219;
	st.local.f32 	[%rd3+8], %f220;
	st.shared.f32 	[%r14], %f220;
	bar.sync 	0;
	@%p1 bra 	$L__BB59_17;

	ld.shared.f32 	%f221, [%r4];
	mov.b32 	%r132, %f221;
	shfl.sync.bfly.b32 	%r136|%p35, %r132, %r117, %r116, %r118;
	mov.b32 	%f222, %r136;
	add.f32 	%f223, %f221, %f222;
	mov.b32 	%r137, %f223;
	shfl.sync.bfly.b32 	%r139|%p36, %r137, %r121, %r116, %r118;
	mov.b32 	%f224, %r139;
	add.f32 	%f225, %f223, %f224;
	mov.b32 	%r140, %f225;
	shfl.sync.bfly.b32 	%r142|%p37, %r140, %r124, %r116, %r118;
	mov.b32 	%f226, %r142;
	add.f32 	%f227, %f225, %f226;
	mov.b32 	%r143, %f227;
	shfl.sync.bfly.b32 	%r145|%p38, %r143, %r127, %r116, %r118;
	mov.b32 	%f228, %r145;
	add.f32 	%f229, %f227, %f228;
	mov.b32 	%r146, %f229;
	shfl.sync.bfly.b32 	%r148|%p39, %r146, %r130, %r116, %r118;
	mov.b32 	%f230, %r148;
	add.f32 	%f231, %f229, %f230;
	st.local.f32 	[%rd3+8], %f231;

$L__BB59_17:
	bar.sync 	0;
	mov.b32 	%r149, %f266;
	shfl.sync.bfly.b32 	%r153|%p41, %r149, %r117, %r116, %r118;
	mov.b32 	%f232, %r153;
	add.f32 	%f233, %f266, %f232;
	mov.b32 	%r154, %f233;
	shfl.sync.bfly.b32 	%r156|%p42, %r154, %r121, %r116, %r118;
	mov.b32 	%f234, %r156;
	add.f32 	%f235, %f233, %f234;
	mov.b32 	%r157, %f235;
	shfl.sync.bfly.b32 	%r159|%p43, %r157, %r124, %r116, %r118;
	mov.b32 	%f236, %r159;
	add.f32 	%f237, %f235, %f236;
	mov.b32 	%r160, %f237;
	shfl.sync.bfly.b32 	%r162|%p44, %r160, %r127, %r116, %r118;
	mov.b32 	%f238, %r162;
	add.f32 	%f239, %f237, %f238;
	mov.b32 	%r163, %f239;
	shfl.sync.bfly.b32 	%r165|%p45, %r163, %r130, %r116, %r118;
	mov.b32 	%f240, %r165;
	add.f32 	%f241, %f239, %f240;
	st.local.f32 	[%rd3+12], %f241;
	st.shared.f32 	[%r14], %f241;
	bar.sync 	0;
	@%p1 bra 	$L__BB59_19;

	ld.shared.f32 	%f242, [%r4];
	mov.b32 	%r166, %f242;
	mov.u32 	%r167, 31;
	mov.u32 	%r168, 16;
	mov.u32 	%r169, -1;
	shfl.sync.bfly.b32 	%r170|%p46, %r166, %r168, %r167, %r169;
	mov.b32 	%f243, %r170;
	add.f32 	%f244, %f242, %f243;
	mov.b32 	%r171, %f244;
	mov.u32 	%r172, 8;
	shfl.sync.bfly.b32 	%r173|%p47, %r171, %r172, %r167, %r169;
	mov.b32 	%f245, %r173;
	add.f32 	%f246, %f244, %f245;
	mov.b32 	%r174, %f246;
	mov.u32 	%r175, 4;
	shfl.sync.bfly.b32 	%r176|%p48, %r174, %r175, %r167, %r169;
	mov.b32 	%f247, %r176;
	add.f32 	%f248, %f246, %f247;
	mov.b32 	%r177, %f248;
	mov.u32 	%r178, 2;
	shfl.sync.bfly.b32 	%r179|%p49, %r177, %r178, %r167, %r169;
	mov.b32 	%f249, %r179;
	add.f32 	%f250, %f248, %f249;
	mov.b32 	%r180, %f250;
	mov.u32 	%r181, 1;
	shfl.sync.bfly.b32 	%r182|%p50, %r180, %r181, %r167, %r169;
	mov.b32 	%f251, %r182;
	add.f32 	%f252, %f250, %f251;
	st.local.f32 	[%rd3+12], %f252;

$L__BB59_19:
	bar.sync 	0;
	setp.gt.s32 	%p51, %r3, 3;
	@%p51 bra 	$L__BB59_21;

	mul.wide.s32 	%rd68, %r3, 4;
	add.s64 	%rd69, %rd3, %rd68;
	ld.local.f32 	%f253, [%rd69];
	mad.lo.s32 	%r183, %r3, %r17, %r2;
	cvt.s64.s32 	%rd70, %r183;
	mul.lo.s32 	%r184, %r1, %r18;
	cvt.s64.s32 	%rd71, %r184;
	add.s64 	%rd72, %rd71, %rd70;
	cvta.to.global.u64 	%rd73, %rd33;
	shl.b64 	%rd74, %rd72, 2;
	add.s64 	%rd75, %rd73, %rd74;
	st.global.f32 	[%rd75], %f253;

$L__BB59_21:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_5_bs_256
.visible .entry ggml_matvec_f32_ncols_5_bs_256(
	.param .u64 ggml_matvec_f32_ncols_5_bs_256_param_0,
	.param .u64 ggml_matvec_f32_ncols_5_bs_256_param_1,
	.param .u64 ggml_matvec_f32_ncols_5_bs_256_param_2,
	.param .u32 ggml_matvec_f32_ncols_5_bs_256_param_3,
	.param .u32 ggml_matvec_f32_ncols_5_bs_256_param_4,
	.param .u32 ggml_matvec_f32_ncols_5_bs_256_param_5,
	.param .u32 ggml_matvec_f32_ncols_5_bs_256_param_6,
	.param .u32 ggml_matvec_f32_ncols_5_bs_256_param_7,
	.param .u32 ggml_matvec_f32_ncols_5_bs_256_param_8,
	.param .u32 ggml_matvec_f32_ncols_5_bs_256_param_9,
	.param .u32 ggml_matvec_f32_ncols_5_bs_256_param_10,
	.param .u32 ggml_matvec_f32_ncols_5_bs_256_param_11
)
{
	.local .align 4 .b8 	__local_depot60[20];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<63>;
	.reg .f32 	%f<332>;
	.reg .b32 	%r<225>;
	.reg .b64 	%rd<74>;


	mov.u64 	%SPL, __local_depot60;
	ld.param.u64 	%rd28, [ggml_matvec_f32_ncols_5_bs_256_param_0];
	ld.param.u64 	%rd29, [ggml_matvec_f32_ncols_5_bs_256_param_1];
	ld.param.u64 	%rd27, [ggml_matvec_f32_ncols_5_bs_256_param_2];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_5_bs_256_param_3];
	ld.param.u32 	%r19, [ggml_matvec_f32_ncols_5_bs_256_param_5];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_5_bs_256_param_6];
	ld.param.u32 	%r17, [ggml_matvec_f32_ncols_5_bs_256_param_7];
	ld.param.u32 	%r20, [ggml_matvec_f32_ncols_5_bs_256_param_8];
	ld.param.u32 	%r21, [ggml_matvec_f32_ncols_5_bs_256_param_9];
	ld.param.u32 	%r22, [ggml_matvec_f32_ncols_5_bs_256_param_10];
	ld.param.u32 	%r18, [ggml_matvec_f32_ncols_5_bs_256_param_11];
	cvta.to.global.u64 	%rd73, %rd29;
	cvta.to.global.u64 	%rd2, %rd28;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r23, %r1, %r20;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r24, %r2, %r19;
	mad.lo.s32 	%r25, %r23, %r21, %r24;
	cvt.s64.s32 	%rd4, %r25;
	mul.lo.s32 	%r26, %r1, %r22;
	cvt.s64.s32 	%rd5, %r26;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r27, %r3, 2;
	mov.u32 	%r28, data_mmv;
	add.s32 	%r4, %r28, %r27;
	@%p1 bra 	$L__BB60_2;

	mov.u32 	%r29, 0;
	st.shared.u32 	[%r4], %r29;

$L__BB60_2:
	bar.sync 	0;
	mov.f32 	%f327, 0f00000000;
	mov.u32 	%r30, 0;
	st.local.u32 	[%rd3], %r30;
	st.local.u32 	[%rd3+4], %r30;
	st.local.u32 	[%rd3+8], %r30;
	st.local.u32 	[%rd3+12], %r30;
	st.local.u32 	[%rd3+16], %r30;
	setp.ge.s32 	%p2, %r3, %r15;
	mov.f32 	%f328, %f327;
	mov.f32 	%f329, %f327;
	mov.f32 	%f330, %f327;
	mov.f32 	%f331, %f327;
	@%p2 bra 	$L__BB60_11;

	not.b32 	%r31, %r3;
	add.s32 	%r5, %r31, %r15;
	shr.u32 	%r32, %r5, 8;
	add.s32 	%r33, %r32, 1;
	and.b32  	%r222, %r33, 3;
	setp.eq.s32 	%p3, %r222, 0;
	mov.f32 	%f327, 0f00000000;
	mov.u32 	%r223, %r3;
	@%p3 bra 	$L__BB60_7;

	shl.b32 	%r34, %r16, 1;
	mad.lo.s32 	%r35, %r16, 3, %r3;
	mul.wide.s32 	%rd31, %r35, 8;
	shl.b64 	%rd32, %rd5, 2;
	add.s64 	%rd7, %rd31, %rd32;
	mul.wide.s32 	%rd33, %r3, 8;
	mul.wide.s32 	%rd34, %r16, 8;
	add.s64 	%rd35, %rd33, %rd34;
	add.s64 	%rd8, %rd35, %rd32;
	add.s64 	%rd9, %rd33, %rd32;
	mul.wide.s32 	%rd36, %r3, 2;
	add.s64 	%rd37, %rd36, %rd4;
	shl.b64 	%rd38, %rd37, 2;
	add.s64 	%rd70, %rd2, %rd38;
	mul.wide.s32 	%rd11, %r34, 8;
	mov.f32 	%f327, 0f00000000;
	mov.u64 	%rd71, %rd73;
	mov.f32 	%f328, %f327;
	mov.f32 	%f329, %f327;
	mov.f32 	%f330, %f327;
	mov.f32 	%f331, %f327;
	mov.u32 	%r223, %r3;

$L__BB60_5:
	.pragma "nounroll";
	ld.global.nc.v2.f32 	{%f46, %f47}, [%rd70];
	add.s64 	%rd39, %rd71, %rd9;
	ld.global.nc.v2.f32 	{%f50, %f51}, [%rd39];
	fma.rn.f32 	%f54, %f46, %f50, %f331;
	fma.rn.f32 	%f331, %f47, %f51, %f54;
	add.s64 	%rd40, %rd71, %rd8;
	ld.global.nc.v2.f32 	{%f55, %f56}, [%rd40];
	fma.rn.f32 	%f59, %f46, %f55, %f330;
	fma.rn.f32 	%f330, %f47, %f56, %f59;
	add.s64 	%rd41, %rd39, %rd11;
	ld.global.nc.v2.f32 	{%f60, %f61}, [%rd41];
	fma.rn.f32 	%f64, %f46, %f60, %f329;
	fma.rn.f32 	%f329, %f47, %f61, %f64;
	add.s64 	%rd42, %rd71, %rd7;
	ld.global.nc.v2.f32 	{%f65, %f66}, [%rd42];
	fma.rn.f32 	%f69, %f46, %f65, %f328;
	fma.rn.f32 	%f328, %f47, %f66, %f69;
	add.s64 	%rd43, %rd41, %rd11;
	ld.global.nc.v2.f32 	{%f70, %f71}, [%rd43];
	fma.rn.f32 	%f74, %f46, %f70, %f327;
	fma.rn.f32 	%f327, %f47, %f71, %f74;
	add.s32 	%r223, %r223, 256;
	add.s64 	%rd71, %rd71, 2048;
	add.s64 	%rd70, %rd70, 2048;
	add.s32 	%r222, %r222, -1;
	setp.ne.s32 	%p4, %r222, 0;
	@%p4 bra 	$L__BB60_5;

	st.local.f32 	[%rd3], %f331;
	st.local.f32 	[%rd3+4], %f330;
	st.local.f32 	[%rd3+8], %f329;
	st.local.f32 	[%rd3+12], %f328;
	st.local.f32 	[%rd3+16], %f327;

$L__BB60_7:
	setp.lt.u32 	%p5, %r5, 768;
	@%p5 bra 	$L__BB60_11;

	add.s32 	%r36, %r223, %r16;
	shl.b32 	%r37, %r16, 1;
	add.s32 	%r38, %r223, %r37;
	mad.lo.s32 	%r39, %r16, 3, %r223;
	shl.b32 	%r40, %r16, 2;
	add.s32 	%r41, %r223, %r40;
	add.s32 	%r42, %r36, 256;
	mul.wide.s32 	%rd44, %r42, 8;
	shl.b64 	%rd45, %rd5, 2;
	add.s64 	%rd16, %rd44, %rd45;
	mul.wide.s32 	%rd46, %r38, 8;
	add.s64 	%rd17, %rd46, %rd45;
	mul.wide.s32 	%rd47, %r39, 8;
	add.s64 	%rd18, %rd47, %rd45;
	mul.wide.s32 	%rd48, %r41, 8;
	add.s64 	%rd19, %rd48, %rd45;
	mul.wide.s32 	%rd49, %r223, 2;
	add.s64 	%rd50, %rd49, %rd4;
	shl.b64 	%rd51, %rd50, 2;
	add.s64 	%rd52, %rd2, %rd51;
	add.s64 	%rd72, %rd52, 4096;
	mul.wide.s32 	%rd53, %r223, 8;
	add.s64 	%rd21, %rd53, %rd45;
	mul.wide.s32 	%rd54, %r16, 8;
	add.s64 	%rd55, %rd53, %rd54;
	add.s64 	%rd22, %rd55, %rd45;

$L__BB60_9:
	ld.global.nc.v2.f32 	{%f75, %f76}, [%rd72+-4096];
	add.s64 	%rd56, %rd73, %rd21;
	ld.global.nc.v2.f32 	{%f79, %f80}, [%rd56];
	fma.rn.f32 	%f83, %f75, %f79, %f331;
	fma.rn.f32 	%f84, %f76, %f80, %f83;
	add.s64 	%rd57, %rd73, %rd22;
	ld.global.nc.v2.f32 	{%f85, %f86}, [%rd57];
	fma.rn.f32 	%f89, %f75, %f85, %f330;
	fma.rn.f32 	%f90, %f76, %f86, %f89;
	add.s64 	%rd58, %rd73, %rd17;
	ld.global.nc.v2.f32 	{%f91, %f92}, [%rd58];
	fma.rn.f32 	%f95, %f75, %f91, %f329;
	fma.rn.f32 	%f96, %f76, %f92, %f95;
	add.s64 	%rd59, %rd73, %rd18;
	ld.global.nc.v2.f32 	{%f97, %f98}, [%rd59];
	fma.rn.f32 	%f101, %f75, %f97, %f328;
	fma.rn.f32 	%f102, %f76, %f98, %f101;
	add.s64 	%rd60, %rd73, %rd19;
	ld.global.nc.v2.f32 	{%f103, %f104}, [%rd60];
	fma.rn.f32 	%f107, %f75, %f103, %f327;
	fma.rn.f32 	%f108, %f76, %f104, %f107;
	ld.global.nc.v2.f32 	{%f109, %f110}, [%rd72+-2048];
	ld.global.nc.v2.f32 	{%f113, %f114}, [%rd56+2048];
	fma.rn.f32 	%f117, %f109, %f113, %f84;
	fma.rn.f32 	%f118, %f110, %f114, %f117;
	add.s64 	%rd61, %rd73, %rd16;
	ld.global.nc.v2.f32 	{%f119, %f120}, [%rd61];
	fma.rn.f32 	%f123, %f109, %f119, %f90;
	fma.rn.f32 	%f124, %f110, %f120, %f123;
	ld.global.nc.v2.f32 	{%f125, %f126}, [%rd58+2048];
	fma.rn.f32 	%f129, %f109, %f125, %f96;
	fma.rn.f32 	%f130, %f110, %f126, %f129;
	ld.global.nc.v2.f32 	{%f131, %f132}, [%rd59+2048];
	fma.rn.f32 	%f135, %f109, %f131, %f102;
	fma.rn.f32 	%f136, %f110, %f132, %f135;
	ld.global.nc.v2.f32 	{%f137, %f138}, [%rd60+2048];
	fma.rn.f32 	%f141, %f109, %f137, %f108;
	fma.rn.f32 	%f142, %f110, %f138, %f141;
	ld.global.nc.v2.f32 	{%f143, %f144}, [%rd72];
	ld.global.nc.v2.f32 	{%f147, %f148}, [%rd56+4096];
	fma.rn.f32 	%f151, %f143, %f147, %f118;
	fma.rn.f32 	%f152, %f144, %f148, %f151;
	ld.global.nc.v2.f32 	{%f153, %f154}, [%rd61+2048];
	fma.rn.f32 	%f157, %f143, %f153, %f124;
	fma.rn.f32 	%f158, %f144, %f154, %f157;
	ld.global.nc.v2.f32 	{%f159, %f160}, [%rd58+4096];
	fma.rn.f32 	%f163, %f143, %f159, %f130;
	fma.rn.f32 	%f164, %f144, %f160, %f163;
	ld.global.nc.v2.f32 	{%f165, %f166}, [%rd59+4096];
	fma.rn.f32 	%f169, %f143, %f165, %f136;
	fma.rn.f32 	%f170, %f144, %f166, %f169;
	ld.global.nc.v2.f32 	{%f171, %f172}, [%rd60+4096];
	fma.rn.f32 	%f175, %f143, %f171, %f142;
	fma.rn.f32 	%f176, %f144, %f172, %f175;
	ld.global.nc.v2.f32 	{%f177, %f178}, [%rd72+2048];
	ld.global.nc.v2.f32 	{%f181, %f182}, [%rd56+6144];
	fma.rn.f32 	%f185, %f177, %f181, %f152;
	fma.rn.f32 	%f331, %f178, %f182, %f185;
	ld.global.nc.v2.f32 	{%f186, %f187}, [%rd61+4096];
	fma.rn.f32 	%f190, %f177, %f186, %f158;
	fma.rn.f32 	%f330, %f178, %f187, %f190;
	ld.global.nc.v2.f32 	{%f191, %f192}, [%rd58+6144];
	fma.rn.f32 	%f195, %f177, %f191, %f164;
	fma.rn.f32 	%f329, %f178, %f192, %f195;
	ld.global.nc.v2.f32 	{%f196, %f197}, [%rd59+6144];
	fma.rn.f32 	%f200, %f177, %f196, %f170;
	fma.rn.f32 	%f328, %f178, %f197, %f200;
	ld.global.nc.v2.f32 	{%f201, %f202}, [%rd60+6144];
	fma.rn.f32 	%f205, %f177, %f201, %f176;
	fma.rn.f32 	%f327, %f178, %f202, %f205;
	add.s64 	%rd73, %rd73, 8192;
	add.s64 	%rd72, %rd72, 8192;
	add.s32 	%r223, %r223, 1024;
	setp.lt.s32 	%p6, %r223, %r15;
	@%p6 bra 	$L__BB60_9;

	st.local.f32 	[%rd3], %f331;
	st.local.f32 	[%rd3+4], %f330;
	st.local.f32 	[%rd3+8], %f329;
	st.local.f32 	[%rd3+12], %f328;
	st.local.f32 	[%rd3+16], %f327;

$L__BB60_11:
	shr.s32 	%r43, %r3, 31;
	shr.u32 	%r44, %r43, 27;
	add.s32 	%r45, %r3, %r44;
	shr.s32 	%r46, %r45, 5;
	shl.b32 	%r47, %r46, 2;
	add.s32 	%r14, %r28, %r47;
	mov.u32 	%r49, 2;
	mov.b32 	%r50, %f331;
	mov.u32 	%r51, 31;
	mov.u32 	%r52, 16;
	mov.u32 	%r53, -1;
	shfl.sync.bfly.b32 	%r54|%p7, %r50, %r52, %r51, %r53;
	mov.b32 	%f206, %r54;
	add.f32 	%f207, %f331, %f206;
	mov.b32 	%r55, %f207;
	mov.u32 	%r56, 8;
	shfl.sync.bfly.b32 	%r57|%p8, %r55, %r56, %r51, %r53;
	mov.b32 	%f208, %r57;
	add.f32 	%f209, %f207, %f208;
	mov.b32 	%r58, %f209;
	mov.u32 	%r59, 4;
	shfl.sync.bfly.b32 	%r60|%p9, %r58, %r59, %r51, %r53;
	mov.b32 	%f210, %r60;
	add.f32 	%f211, %f209, %f210;
	mov.b32 	%r61, %f211;
	shfl.sync.bfly.b32 	%r62|%p10, %r61, %r49, %r51, %r53;
	mov.b32 	%f212, %r62;
	add.f32 	%f213, %f211, %f212;
	mov.b32 	%r63, %f213;
	mov.u32 	%r64, 1;
	shfl.sync.bfly.b32 	%r65|%p11, %r63, %r64, %r51, %r53;
	mov.b32 	%f214, %r65;
	add.f32 	%f215, %f213, %f214;
	st.local.f32 	[%rd3], %f215;
	st.shared.f32 	[%r14], %f215;
	bar.sync 	0;
	@%p1 bra 	$L__BB60_13;

	ld.shared.f32 	%f216, [%r4];
	mov.b32 	%r66, %f216;
	shfl.sync.bfly.b32 	%r70|%p13, %r66, %r52, %r51, %r53;
	mov.b32 	%f217, %r70;
	add.f32 	%f218, %f216, %f217;
	mov.b32 	%r71, %f218;
	shfl.sync.bfly.b32 	%r73|%p14, %r71, %r56, %r51, %r53;
	mov.b32 	%f219, %r73;
	add.f32 	%f220, %f218, %f219;
	mov.b32 	%r74, %f220;
	shfl.sync.bfly.b32 	%r76|%p15, %r74, %r59, %r51, %r53;
	mov.b32 	%f221, %r76;
	add.f32 	%f222, %f220, %f221;
	mov.b32 	%r77, %f222;
	shfl.sync.bfly.b32 	%r79|%p16, %r77, %r49, %r51, %r53;
	mov.b32 	%f223, %r79;
	add.f32 	%f224, %f222, %f223;
	mov.b32 	%r80, %f224;
	shfl.sync.bfly.b32 	%r82|%p17, %r80, %r64, %r51, %r53;
	mov.b32 	%f225, %r82;
	add.f32 	%f226, %f224, %f225;
	st.local.f32 	[%rd3], %f226;

$L__BB60_13:
	bar.sync 	0;
	mov.b32 	%r83, %f330;
	shfl.sync.bfly.b32 	%r87|%p19, %r83, %r52, %r51, %r53;
	mov.b32 	%f227, %r87;
	add.f32 	%f228, %f330, %f227;
	mov.b32 	%r88, %f228;
	shfl.sync.bfly.b32 	%r90|%p20, %r88, %r56, %r51, %r53;
	mov.b32 	%f229, %r90;
	add.f32 	%f230, %f228, %f229;
	mov.b32 	%r91, %f230;
	shfl.sync.bfly.b32 	%r93|%p21, %r91, %r59, %r51, %r53;
	mov.b32 	%f231, %r93;
	add.f32 	%f232, %f230, %f231;
	mov.b32 	%r94, %f232;
	shfl.sync.bfly.b32 	%r96|%p22, %r94, %r49, %r51, %r53;
	mov.b32 	%f233, %r96;
	add.f32 	%f234, %f232, %f233;
	mov.b32 	%r97, %f234;
	shfl.sync.bfly.b32 	%r99|%p23, %r97, %r64, %r51, %r53;
	mov.b32 	%f235, %r99;
	add.f32 	%f236, %f234, %f235;
	st.local.f32 	[%rd3+4], %f236;
	st.shared.f32 	[%r14], %f236;
	bar.sync 	0;
	@%p1 bra 	$L__BB60_15;

	ld.shared.f32 	%f237, [%r4];
	mov.b32 	%r100, %f237;
	mov.u32 	%r101, 31;
	mov.u32 	%r102, 16;
	mov.u32 	%r103, -1;
	shfl.sync.bfly.b32 	%r104|%p24, %r100, %r102, %r101, %r103;
	mov.b32 	%f238, %r104;
	add.f32 	%f239, %f237, %f238;
	mov.b32 	%r105, %f239;
	mov.u32 	%r106, 8;
	shfl.sync.bfly.b32 	%r107|%p25, %r105, %r106, %r101, %r103;
	mov.b32 	%f240, %r107;
	add.f32 	%f241, %f239, %f240;
	mov.b32 	%r108, %f241;
	mov.u32 	%r109, 4;
	shfl.sync.bfly.b32 	%r110|%p26, %r108, %r109, %r101, %r103;
	mov.b32 	%f242, %r110;
	add.f32 	%f243, %f241, %f242;
	mov.b32 	%r111, %f243;
	mov.u32 	%r112, 2;
	shfl.sync.bfly.b32 	%r113|%p27, %r111, %r112, %r101, %r103;
	mov.b32 	%f244, %r113;
	add.f32 	%f245, %f243, %f244;
	mov.b32 	%r114, %f245;
	mov.u32 	%r115, 1;
	shfl.sync.bfly.b32 	%r116|%p28, %r114, %r115, %r101, %r103;
	mov.b32 	%f246, %r116;
	add.f32 	%f247, %f245, %f246;
	st.local.f32 	[%rd3+4], %f247;

$L__BB60_15:
	bar.sync 	0;
	mov.b32 	%r117, %f329;
	mov.u32 	%r118, 31;
	mov.u32 	%r119, 16;
	mov.u32 	%r120, -1;
	shfl.sync.bfly.b32 	%r121|%p30, %r117, %r119, %r118, %r120;
	mov.b32 	%f248, %r121;
	add.f32 	%f249, %f329, %f248;
	mov.b32 	%r122, %f249;
	mov.u32 	%r123, 8;
	shfl.sync.bfly.b32 	%r124|%p31, %r122, %r123, %r118, %r120;
	mov.b32 	%f250, %r124;
	add.f32 	%f251, %f249, %f250;
	mov.b32 	%r125, %f251;
	mov.u32 	%r126, 4;
	shfl.sync.bfly.b32 	%r127|%p32, %r125, %r126, %r118, %r120;
	mov.b32 	%f252, %r127;
	add.f32 	%f253, %f251, %f252;
	mov.b32 	%r128, %f253;
	mov.u32 	%r129, 2;
	shfl.sync.bfly.b32 	%r130|%p33, %r128, %r129, %r118, %r120;
	mov.b32 	%f254, %r130;
	add.f32 	%f255, %f253, %f254;
	mov.b32 	%r131, %f255;
	mov.u32 	%r132, 1;
	shfl.sync.bfly.b32 	%r133|%p34, %r131, %r132, %r118, %r120;
	mov.b32 	%f256, %r133;
	add.f32 	%f257, %f255, %f256;
	st.local.f32 	[%rd3+8], %f257;
	st.shared.f32 	[%r14], %f257;
	bar.sync 	0;
	@%p1 bra 	$L__BB60_17;

	ld.shared.f32 	%f258, [%r4];
	mov.b32 	%r134, %f258;
	shfl.sync.bfly.b32 	%r138|%p35, %r134, %r119, %r118, %r120;
	mov.b32 	%f259, %r138;
	add.f32 	%f260, %f258, %f259;
	mov.b32 	%r139, %f260;
	shfl.sync.bfly.b32 	%r141|%p36, %r139, %r123, %r118, %r120;
	mov.b32 	%f261, %r141;
	add.f32 	%f262, %f260, %f261;
	mov.b32 	%r142, %f262;
	shfl.sync.bfly.b32 	%r144|%p37, %r142, %r126, %r118, %r120;
	mov.b32 	%f263, %r144;
	add.f32 	%f264, %f262, %f263;
	mov.b32 	%r145, %f264;
	shfl.sync.bfly.b32 	%r147|%p38, %r145, %r129, %r118, %r120;
	mov.b32 	%f265, %r147;
	add.f32 	%f266, %f264, %f265;
	mov.b32 	%r148, %f266;
	shfl.sync.bfly.b32 	%r150|%p39, %r148, %r132, %r118, %r120;
	mov.b32 	%f267, %r150;
	add.f32 	%f268, %f266, %f267;
	st.local.f32 	[%rd3+8], %f268;

$L__BB60_17:
	bar.sync 	0;
	mov.b32 	%r151, %f328;
	shfl.sync.bfly.b32 	%r155|%p41, %r151, %r119, %r118, %r120;
	mov.b32 	%f269, %r155;
	add.f32 	%f270, %f328, %f269;
	mov.b32 	%r156, %f270;
	shfl.sync.bfly.b32 	%r158|%p42, %r156, %r123, %r118, %r120;
	mov.b32 	%f271, %r158;
	add.f32 	%f272, %f270, %f271;
	mov.b32 	%r159, %f272;
	shfl.sync.bfly.b32 	%r161|%p43, %r159, %r126, %r118, %r120;
	mov.b32 	%f273, %r161;
	add.f32 	%f274, %f272, %f273;
	mov.b32 	%r162, %f274;
	shfl.sync.bfly.b32 	%r164|%p44, %r162, %r129, %r118, %r120;
	mov.b32 	%f275, %r164;
	add.f32 	%f276, %f274, %f275;
	mov.b32 	%r165, %f276;
	shfl.sync.bfly.b32 	%r167|%p45, %r165, %r132, %r118, %r120;
	mov.b32 	%f277, %r167;
	add.f32 	%f278, %f276, %f277;
	st.local.f32 	[%rd3+12], %f278;
	st.shared.f32 	[%r14], %f278;
	bar.sync 	0;
	@%p1 bra 	$L__BB60_19;

	ld.shared.f32 	%f279, [%r4];
	mov.b32 	%r168, %f279;
	mov.u32 	%r169, 31;
	mov.u32 	%r170, 16;
	mov.u32 	%r171, -1;
	shfl.sync.bfly.b32 	%r172|%p46, %r168, %r170, %r169, %r171;
	mov.b32 	%f280, %r172;
	add.f32 	%f281, %f279, %f280;
	mov.b32 	%r173, %f281;
	mov.u32 	%r174, 8;
	shfl.sync.bfly.b32 	%r175|%p47, %r173, %r174, %r169, %r171;
	mov.b32 	%f282, %r175;
	add.f32 	%f283, %f281, %f282;
	mov.b32 	%r176, %f283;
	mov.u32 	%r177, 4;
	shfl.sync.bfly.b32 	%r178|%p48, %r176, %r177, %r169, %r171;
	mov.b32 	%f284, %r178;
	add.f32 	%f285, %f283, %f284;
	mov.b32 	%r179, %f285;
	mov.u32 	%r180, 2;
	shfl.sync.bfly.b32 	%r181|%p49, %r179, %r180, %r169, %r171;
	mov.b32 	%f286, %r181;
	add.f32 	%f287, %f285, %f286;
	mov.b32 	%r182, %f287;
	mov.u32 	%r183, 1;
	shfl.sync.bfly.b32 	%r184|%p50, %r182, %r183, %r169, %r171;
	mov.b32 	%f288, %r184;
	add.f32 	%f289, %f287, %f288;
	st.local.f32 	[%rd3+12], %f289;

$L__BB60_19:
	bar.sync 	0;
	mov.b32 	%r185, %f327;
	mov.u32 	%r186, 31;
	mov.u32 	%r187, 16;
	mov.u32 	%r188, -1;
	shfl.sync.bfly.b32 	%r189|%p52, %r185, %r187, %r186, %r188;
	mov.b32 	%f290, %r189;
	add.f32 	%f291, %f327, %f290;
	mov.b32 	%r190, %f291;
	mov.u32 	%r191, 8;
	shfl.sync.bfly.b32 	%r192|%p53, %r190, %r191, %r186, %r188;
	mov.b32 	%f292, %r192;
	add.f32 	%f293, %f291, %f292;
	mov.b32 	%r193, %f293;
	mov.u32 	%r194, 4;
	shfl.sync.bfly.b32 	%r195|%p54, %r193, %r194, %r186, %r188;
	mov.b32 	%f294, %r195;
	add.f32 	%f295, %f293, %f294;
	mov.b32 	%r196, %f295;
	mov.u32 	%r197, 2;
	shfl.sync.bfly.b32 	%r198|%p55, %r196, %r197, %r186, %r188;
	mov.b32 	%f296, %r198;
	add.f32 	%f297, %f295, %f296;
	mov.b32 	%r199, %f297;
	mov.u32 	%r200, 1;
	shfl.sync.bfly.b32 	%r201|%p56, %r199, %r200, %r186, %r188;
	mov.b32 	%f298, %r201;
	add.f32 	%f299, %f297, %f298;
	st.local.f32 	[%rd3+16], %f299;
	st.shared.f32 	[%r14], %f299;
	bar.sync 	0;
	@%p1 bra 	$L__BB60_21;

	ld.shared.f32 	%f300, [%r4];
	mov.b32 	%r202, %f300;
	shfl.sync.bfly.b32 	%r206|%p57, %r202, %r187, %r186, %r188;
	mov.b32 	%f301, %r206;
	add.f32 	%f302, %f300, %f301;
	mov.b32 	%r207, %f302;
	shfl.sync.bfly.b32 	%r209|%p58, %r207, %r191, %r186, %r188;
	mov.b32 	%f303, %r209;
	add.f32 	%f304, %f302, %f303;
	mov.b32 	%r210, %f304;
	shfl.sync.bfly.b32 	%r212|%p59, %r210, %r194, %r186, %r188;
	mov.b32 	%f305, %r212;
	add.f32 	%f306, %f304, %f305;
	mov.b32 	%r213, %f306;
	shfl.sync.bfly.b32 	%r215|%p60, %r213, %r197, %r186, %r188;
	mov.b32 	%f307, %r215;
	add.f32 	%f308, %f306, %f307;
	mov.b32 	%r216, %f308;
	shfl.sync.bfly.b32 	%r218|%p61, %r216, %r200, %r186, %r188;
	mov.b32 	%f309, %r218;
	add.f32 	%f310, %f308, %f309;
	st.local.f32 	[%rd3+16], %f310;

$L__BB60_21:
	bar.sync 	0;
	setp.gt.s32 	%p62, %r3, 4;
	@%p62 bra 	$L__BB60_23;

	mul.wide.s32 	%rd62, %r3, 4;
	add.s64 	%rd63, %rd3, %rd62;
	ld.local.f32 	%f311, [%rd63];
	mad.lo.s32 	%r219, %r3, %r17, %r2;
	cvt.s64.s32 	%rd64, %r219;
	mul.lo.s32 	%r220, %r1, %r18;
	cvt.s64.s32 	%rd65, %r220;
	add.s64 	%rd66, %rd65, %rd64;
	cvta.to.global.u64 	%rd67, %rd27;
	shl.b64 	%rd68, %rd66, 2;
	add.s64 	%rd69, %rd67, %rd68;
	st.global.f32 	[%rd69], %f311;

$L__BB60_23:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_6_bs_256
.visible .entry ggml_matvec_f32_ncols_6_bs_256(
	.param .u64 ggml_matvec_f32_ncols_6_bs_256_param_0,
	.param .u64 ggml_matvec_f32_ncols_6_bs_256_param_1,
	.param .u64 ggml_matvec_f32_ncols_6_bs_256_param_2,
	.param .u32 ggml_matvec_f32_ncols_6_bs_256_param_3,
	.param .u32 ggml_matvec_f32_ncols_6_bs_256_param_4,
	.param .u32 ggml_matvec_f32_ncols_6_bs_256_param_5,
	.param .u32 ggml_matvec_f32_ncols_6_bs_256_param_6,
	.param .u32 ggml_matvec_f32_ncols_6_bs_256_param_7,
	.param .u32 ggml_matvec_f32_ncols_6_bs_256_param_8,
	.param .u32 ggml_matvec_f32_ncols_6_bs_256_param_9,
	.param .u32 ggml_matvec_f32_ncols_6_bs_256_param_10,
	.param .u32 ggml_matvec_f32_ncols_6_bs_256_param_11
)
{
	.local .align 8 .b8 	__local_depot61[24];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<73>;
	.reg .f32 	%f<296>;
	.reg .b32 	%r<253>;
	.reg .b64 	%rd<67>;


	mov.u64 	%SPL, __local_depot61;
	ld.param.u64 	%rd20, [ggml_matvec_f32_ncols_6_bs_256_param_0];
	ld.param.u64 	%rd21, [ggml_matvec_f32_ncols_6_bs_256_param_1];
	ld.param.u64 	%rd19, [ggml_matvec_f32_ncols_6_bs_256_param_2];
	ld.param.u32 	%r11, [ggml_matvec_f32_ncols_6_bs_256_param_3];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_6_bs_256_param_5];
	ld.param.u32 	%r12, [ggml_matvec_f32_ncols_6_bs_256_param_6];
	ld.param.u32 	%r13, [ggml_matvec_f32_ncols_6_bs_256_param_7];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_6_bs_256_param_8];
	ld.param.u32 	%r17, [ggml_matvec_f32_ncols_6_bs_256_param_9];
	ld.param.u32 	%r18, [ggml_matvec_f32_ncols_6_bs_256_param_10];
	ld.param.u32 	%r14, [ggml_matvec_f32_ncols_6_bs_256_param_11];
	cvta.to.global.u64 	%rd66, %rd21;
	add.u64 	%rd2, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r19, %r1, %r16;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r20, %r2, %r15;
	mad.lo.s32 	%r21, %r19, %r17, %r20;
	cvt.s64.s32 	%rd3, %r21;
	cvta.to.global.u64 	%rd4, %rd20;
	mul.lo.s32 	%r22, %r1, %r18;
	cvt.s64.s32 	%rd5, %r22;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r23, %r3, 2;
	mov.u32 	%r24, data_mmv;
	add.s32 	%r4, %r24, %r23;
	@%p1 bra 	$L__BB61_2;

	mov.u32 	%r25, 0;
	st.shared.u32 	[%r4], %r25;

$L__BB61_2:
	bar.sync 	0;
	mov.f32 	%f290, 0f00000000;
	st.local.v2.f32 	[%rd2], {%f290, %f290};
	st.local.v2.f32 	[%rd2+8], {%f290, %f290};
	st.local.v2.f32 	[%rd2+16], {%f290, %f290};
	setp.ge.s32 	%p2, %r3, %r11;
	mov.f32 	%f291, %f290;
	mov.f32 	%f292, %f290;
	mov.f32 	%f293, %f290;
	mov.f32 	%f294, %f290;
	mov.f32 	%f295, %f290;
	@%p2 bra 	$L__BB61_9;

	not.b32 	%r26, %r3;
	add.s32 	%r5, %r26, %r11;
	and.b32  	%r27, %r5, 256;
	setp.ne.s32 	%p3, %r27, 0;
	mov.f32 	%f290, 0f00000000;
	mov.u32 	%r252, %r3;
	@%p3 bra 	$L__BB61_5;

	shl.b64 	%rd23, %rd5, 2;
	add.s64 	%rd24, %rd66, %rd23;
	shl.b64 	%rd25, %rd3, 2;
	add.s64 	%rd26, %rd4, %rd25;
	mul.wide.s32 	%rd27, %r3, 8;
	add.s64 	%rd28, %rd26, %rd27;
	ld.global.nc.v2.f32 	{%f43, %f44}, [%rd28];
	add.s64 	%rd29, %rd24, %rd27;
	ld.global.nc.v2.f32 	{%f47, %f48}, [%rd29];
	fma.rn.f32 	%f51, %f43, %f47, 0f00000000;
	fma.rn.f32 	%f295, %f44, %f48, %f51;
	mul.wide.s32 	%rd30, %r12, 8;
	add.s64 	%rd31, %rd29, %rd30;
	ld.global.nc.v2.f32 	{%f52, %f53}, [%rd31];
	fma.rn.f32 	%f56, %f43, %f52, 0f00000000;
	fma.rn.f32 	%f294, %f44, %f53, %f56;
	st.local.v2.f32 	[%rd2], {%f295, %f294};
	add.s32 	%r28, %r3, %r12;
	add.s32 	%r29, %r28, %r12;
	mul.wide.s32 	%rd32, %r29, 8;
	add.s64 	%rd33, %rd24, %rd32;
	ld.global.nc.v2.f32 	{%f57, %f58}, [%rd33];
	fma.rn.f32 	%f61, %f43, %f57, 0f00000000;
	fma.rn.f32 	%f293, %f44, %f58, %f61;
	add.s64 	%rd34, %rd33, %rd30;
	ld.global.nc.v2.f32 	{%f62, %f63}, [%rd34];
	fma.rn.f32 	%f66, %f43, %f62, 0f00000000;
	fma.rn.f32 	%f292, %f44, %f63, %f66;
	st.local.v2.f32 	[%rd2+8], {%f293, %f292};
	add.s64 	%rd35, %rd34, %rd30;
	ld.global.nc.v2.f32 	{%f67, %f68}, [%rd35];
	fma.rn.f32 	%f71, %f43, %f67, 0f00000000;
	fma.rn.f32 	%f291, %f44, %f68, %f71;
	add.s64 	%rd36, %rd35, %rd30;
	ld.global.nc.v2.f32 	{%f72, %f73}, [%rd36];
	fma.rn.f32 	%f76, %f43, %f72, 0f00000000;
	fma.rn.f32 	%f290, %f44, %f73, %f76;
	st.local.v2.f32 	[%rd2+16], {%f291, %f290};
	add.s32 	%r252, %r3, 256;

$L__BB61_5:
	and.b32  	%r30, %r5, -256;
	setp.eq.s32 	%p4, %r30, 0;
	@%p4 bra 	$L__BB61_9;

	add.s32 	%r31, %r252, %r12;
	add.s32 	%r32, %r31, 256;
	mul.wide.s32 	%rd37, %r32, 8;
	shl.b64 	%rd38, %rd5, 2;
	add.s64 	%rd7, %rd37, %rd38;
	shl.b32 	%r33, %r12, 1;
	add.s32 	%r34, %r252, %r33;
	mad.lo.s32 	%r35, %r12, 3, %r252;
	shl.b32 	%r36, %r12, 2;
	add.s32 	%r37, %r252, %r36;
	mad.lo.s32 	%r38, %r12, 5, %r252;
	mul.wide.s32 	%rd39, %r34, 8;
	add.s64 	%rd8, %rd39, %rd38;
	mul.wide.s32 	%rd40, %r35, 8;
	add.s64 	%rd9, %rd40, %rd38;
	mul.wide.s32 	%rd41, %r37, 8;
	add.s64 	%rd10, %rd41, %rd38;
	mul.wide.s32 	%rd42, %r38, 8;
	add.s64 	%rd11, %rd42, %rd38;
	mul.wide.s32 	%rd43, %r252, 2;
	add.s64 	%rd44, %rd43, %rd3;
	shl.b64 	%rd45, %rd44, 2;
	add.s64 	%rd46, %rd4, %rd45;
	add.s64 	%rd65, %rd46, 2048;
	mul.wide.s32 	%rd47, %r252, 8;
	mul.wide.s32 	%rd48, %r12, 8;
	add.s64 	%rd49, %rd47, %rd48;
	add.s64 	%rd13, %rd49, %rd38;
	add.s64 	%rd14, %rd47, %rd38;

$L__BB61_7:
	ld.global.nc.v2.f32 	{%f77, %f78}, [%rd65+-2048];
	add.s64 	%rd50, %rd66, %rd14;
	ld.global.nc.v2.f32 	{%f81, %f82}, [%rd50];
	fma.rn.f32 	%f85, %f77, %f81, %f295;
	fma.rn.f32 	%f86, %f78, %f82, %f85;
	add.s64 	%rd51, %rd66, %rd13;
	ld.global.nc.v2.f32 	{%f87, %f88}, [%rd51];
	fma.rn.f32 	%f91, %f77, %f87, %f294;
	fma.rn.f32 	%f92, %f78, %f88, %f91;
	add.s64 	%rd52, %rd66, %rd8;
	ld.global.nc.v2.f32 	{%f93, %f94}, [%rd52];
	fma.rn.f32 	%f97, %f77, %f93, %f293;
	fma.rn.f32 	%f98, %f78, %f94, %f97;
	add.s64 	%rd53, %rd66, %rd9;
	ld.global.nc.v2.f32 	{%f99, %f100}, [%rd53];
	fma.rn.f32 	%f103, %f77, %f99, %f292;
	fma.rn.f32 	%f104, %f78, %f100, %f103;
	add.s64 	%rd54, %rd66, %rd10;
	ld.global.nc.v2.f32 	{%f105, %f106}, [%rd54];
	fma.rn.f32 	%f109, %f77, %f105, %f291;
	fma.rn.f32 	%f110, %f78, %f106, %f109;
	add.s64 	%rd55, %rd66, %rd11;
	ld.global.nc.v2.f32 	{%f111, %f112}, [%rd55];
	fma.rn.f32 	%f115, %f77, %f111, %f290;
	fma.rn.f32 	%f116, %f78, %f112, %f115;
	ld.global.nc.v2.f32 	{%f117, %f118}, [%rd65];
	ld.global.nc.v2.f32 	{%f121, %f122}, [%rd50+2048];
	fma.rn.f32 	%f125, %f117, %f121, %f86;
	fma.rn.f32 	%f295, %f118, %f122, %f125;
	add.s64 	%rd56, %rd66, %rd7;
	ld.global.nc.v2.f32 	{%f126, %f127}, [%rd56];
	fma.rn.f32 	%f130, %f117, %f126, %f92;
	fma.rn.f32 	%f294, %f118, %f127, %f130;
	ld.global.nc.v2.f32 	{%f131, %f132}, [%rd52+2048];
	fma.rn.f32 	%f135, %f117, %f131, %f98;
	fma.rn.f32 	%f293, %f118, %f132, %f135;
	ld.global.nc.v2.f32 	{%f136, %f137}, [%rd53+2048];
	fma.rn.f32 	%f140, %f117, %f136, %f104;
	fma.rn.f32 	%f292, %f118, %f137, %f140;
	ld.global.nc.v2.f32 	{%f141, %f142}, [%rd54+2048];
	fma.rn.f32 	%f145, %f117, %f141, %f110;
	fma.rn.f32 	%f291, %f118, %f142, %f145;
	ld.global.nc.v2.f32 	{%f146, %f147}, [%rd55+2048];
	fma.rn.f32 	%f150, %f117, %f146, %f116;
	fma.rn.f32 	%f290, %f118, %f147, %f150;
	add.s64 	%rd66, %rd66, 4096;
	add.s64 	%rd65, %rd65, 4096;
	add.s32 	%r252, %r252, 512;
	setp.lt.s32 	%p5, %r252, %r11;
	@%p5 bra 	$L__BB61_7;

	st.local.v2.f32 	[%rd2], {%f295, %f294};
	st.local.v2.f32 	[%rd2+8], {%f293, %f292};
	st.local.v2.f32 	[%rd2+16], {%f291, %f290};

$L__BB61_9:
	shr.s32 	%r39, %r3, 31;
	shr.u32 	%r40, %r39, 27;
	add.s32 	%r41, %r3, %r40;
	shr.s32 	%r42, %r41, 5;
	shl.b32 	%r43, %r42, 2;
	add.s32 	%r10, %r24, %r43;
	mov.u32 	%r45, 2;
	mov.b32 	%r46, %f295;
	mov.u32 	%r47, 31;
	mov.u32 	%r48, 16;
	mov.u32 	%r49, -1;
	shfl.sync.bfly.b32 	%r50|%p6, %r46, %r48, %r47, %r49;
	mov.b32 	%f151, %r50;
	add.f32 	%f152, %f295, %f151;
	mov.b32 	%r51, %f152;
	mov.u32 	%r52, 8;
	shfl.sync.bfly.b32 	%r53|%p7, %r51, %r52, %r47, %r49;
	mov.b32 	%f153, %r53;
	add.f32 	%f154, %f152, %f153;
	mov.b32 	%r54, %f154;
	mov.u32 	%r55, 4;
	shfl.sync.bfly.b32 	%r56|%p8, %r54, %r55, %r47, %r49;
	mov.b32 	%f155, %r56;
	add.f32 	%f156, %f154, %f155;
	mov.b32 	%r57, %f156;
	shfl.sync.bfly.b32 	%r58|%p9, %r57, %r45, %r47, %r49;
	mov.b32 	%f157, %r58;
	add.f32 	%f158, %f156, %f157;
	mov.b32 	%r59, %f158;
	mov.u32 	%r60, 1;
	shfl.sync.bfly.b32 	%r61|%p10, %r59, %r60, %r47, %r49;
	mov.b32 	%f159, %r61;
	add.f32 	%f160, %f158, %f159;
	st.local.f32 	[%rd2], %f160;
	st.shared.f32 	[%r10], %f160;
	bar.sync 	0;
	@%p1 bra 	$L__BB61_11;

	ld.shared.f32 	%f161, [%r4];
	mov.b32 	%r62, %f161;
	shfl.sync.bfly.b32 	%r66|%p12, %r62, %r48, %r47, %r49;
	mov.b32 	%f162, %r66;
	add.f32 	%f163, %f161, %f162;
	mov.b32 	%r67, %f163;
	shfl.sync.bfly.b32 	%r69|%p13, %r67, %r52, %r47, %r49;
	mov.b32 	%f164, %r69;
	add.f32 	%f165, %f163, %f164;
	mov.b32 	%r70, %f165;
	shfl.sync.bfly.b32 	%r72|%p14, %r70, %r55, %r47, %r49;
	mov.b32 	%f166, %r72;
	add.f32 	%f167, %f165, %f166;
	mov.b32 	%r73, %f167;
	shfl.sync.bfly.b32 	%r75|%p15, %r73, %r45, %r47, %r49;
	mov.b32 	%f168, %r75;
	add.f32 	%f169, %f167, %f168;
	mov.b32 	%r76, %f169;
	shfl.sync.bfly.b32 	%r78|%p16, %r76, %r60, %r47, %r49;
	mov.b32 	%f170, %r78;
	add.f32 	%f171, %f169, %f170;
	st.local.f32 	[%rd2], %f171;

$L__BB61_11:
	bar.sync 	0;
	mov.b32 	%r79, %f294;
	shfl.sync.bfly.b32 	%r83|%p18, %r79, %r48, %r47, %r49;
	mov.b32 	%f172, %r83;
	add.f32 	%f173, %f294, %f172;
	mov.b32 	%r84, %f173;
	shfl.sync.bfly.b32 	%r86|%p19, %r84, %r52, %r47, %r49;
	mov.b32 	%f174, %r86;
	add.f32 	%f175, %f173, %f174;
	mov.b32 	%r87, %f175;
	shfl.sync.bfly.b32 	%r89|%p20, %r87, %r55, %r47, %r49;
	mov.b32 	%f176, %r89;
	add.f32 	%f177, %f175, %f176;
	mov.b32 	%r90, %f177;
	shfl.sync.bfly.b32 	%r92|%p21, %r90, %r45, %r47, %r49;
	mov.b32 	%f178, %r92;
	add.f32 	%f179, %f177, %f178;
	mov.b32 	%r93, %f179;
	shfl.sync.bfly.b32 	%r95|%p22, %r93, %r60, %r47, %r49;
	mov.b32 	%f180, %r95;
	add.f32 	%f181, %f179, %f180;
	st.local.f32 	[%rd2+4], %f181;
	st.shared.f32 	[%r10], %f181;
	bar.sync 	0;
	@%p1 bra 	$L__BB61_13;

	ld.shared.f32 	%f182, [%r4];
	mov.b32 	%r96, %f182;
	mov.u32 	%r97, 31;
	mov.u32 	%r98, 16;
	mov.u32 	%r99, -1;
	shfl.sync.bfly.b32 	%r100|%p23, %r96, %r98, %r97, %r99;
	mov.b32 	%f183, %r100;
	add.f32 	%f184, %f182, %f183;
	mov.b32 	%r101, %f184;
	mov.u32 	%r102, 8;
	shfl.sync.bfly.b32 	%r103|%p24, %r101, %r102, %r97, %r99;
	mov.b32 	%f185, %r103;
	add.f32 	%f186, %f184, %f185;
	mov.b32 	%r104, %f186;
	mov.u32 	%r105, 4;
	shfl.sync.bfly.b32 	%r106|%p25, %r104, %r105, %r97, %r99;
	mov.b32 	%f187, %r106;
	add.f32 	%f188, %f186, %f187;
	mov.b32 	%r107, %f188;
	mov.u32 	%r108, 2;
	shfl.sync.bfly.b32 	%r109|%p26, %r107, %r108, %r97, %r99;
	mov.b32 	%f189, %r109;
	add.f32 	%f190, %f188, %f189;
	mov.b32 	%r110, %f190;
	mov.u32 	%r111, 1;
	shfl.sync.bfly.b32 	%r112|%p27, %r110, %r111, %r97, %r99;
	mov.b32 	%f191, %r112;
	add.f32 	%f192, %f190, %f191;
	st.local.f32 	[%rd2+4], %f192;

$L__BB61_13:
	bar.sync 	0;
	mov.b32 	%r113, %f293;
	mov.u32 	%r114, 31;
	mov.u32 	%r115, 16;
	mov.u32 	%r116, -1;
	shfl.sync.bfly.b32 	%r117|%p29, %r113, %r115, %r114, %r116;
	mov.b32 	%f193, %r117;
	add.f32 	%f194, %f293, %f193;
	mov.b32 	%r118, %f194;
	mov.u32 	%r119, 8;
	shfl.sync.bfly.b32 	%r120|%p30, %r118, %r119, %r114, %r116;
	mov.b32 	%f195, %r120;
	add.f32 	%f196, %f194, %f195;
	mov.b32 	%r121, %f196;
	mov.u32 	%r122, 4;
	shfl.sync.bfly.b32 	%r123|%p31, %r121, %r122, %r114, %r116;
	mov.b32 	%f197, %r123;
	add.f32 	%f198, %f196, %f197;
	mov.b32 	%r124, %f198;
	mov.u32 	%r125, 2;
	shfl.sync.bfly.b32 	%r126|%p32, %r124, %r125, %r114, %r116;
	mov.b32 	%f199, %r126;
	add.f32 	%f200, %f198, %f199;
	mov.b32 	%r127, %f200;
	mov.u32 	%r128, 1;
	shfl.sync.bfly.b32 	%r129|%p33, %r127, %r128, %r114, %r116;
	mov.b32 	%f201, %r129;
	add.f32 	%f202, %f200, %f201;
	st.local.f32 	[%rd2+8], %f202;
	st.shared.f32 	[%r10], %f202;
	bar.sync 	0;
	@%p1 bra 	$L__BB61_15;

	ld.shared.f32 	%f203, [%r4];
	mov.b32 	%r130, %f203;
	shfl.sync.bfly.b32 	%r134|%p34, %r130, %r115, %r114, %r116;
	mov.b32 	%f204, %r134;
	add.f32 	%f205, %f203, %f204;
	mov.b32 	%r135, %f205;
	shfl.sync.bfly.b32 	%r137|%p35, %r135, %r119, %r114, %r116;
	mov.b32 	%f206, %r137;
	add.f32 	%f207, %f205, %f206;
	mov.b32 	%r138, %f207;
	shfl.sync.bfly.b32 	%r140|%p36, %r138, %r122, %r114, %r116;
	mov.b32 	%f208, %r140;
	add.f32 	%f209, %f207, %f208;
	mov.b32 	%r141, %f209;
	shfl.sync.bfly.b32 	%r143|%p37, %r141, %r125, %r114, %r116;
	mov.b32 	%f210, %r143;
	add.f32 	%f211, %f209, %f210;
	mov.b32 	%r144, %f211;
	shfl.sync.bfly.b32 	%r146|%p38, %r144, %r128, %r114, %r116;
	mov.b32 	%f212, %r146;
	add.f32 	%f213, %f211, %f212;
	st.local.f32 	[%rd2+8], %f213;

$L__BB61_15:
	bar.sync 	0;
	mov.b32 	%r147, %f292;
	shfl.sync.bfly.b32 	%r151|%p40, %r147, %r115, %r114, %r116;
	mov.b32 	%f214, %r151;
	add.f32 	%f215, %f292, %f214;
	mov.b32 	%r152, %f215;
	shfl.sync.bfly.b32 	%r154|%p41, %r152, %r119, %r114, %r116;
	mov.b32 	%f216, %r154;
	add.f32 	%f217, %f215, %f216;
	mov.b32 	%r155, %f217;
	shfl.sync.bfly.b32 	%r157|%p42, %r155, %r122, %r114, %r116;
	mov.b32 	%f218, %r157;
	add.f32 	%f219, %f217, %f218;
	mov.b32 	%r158, %f219;
	shfl.sync.bfly.b32 	%r160|%p43, %r158, %r125, %r114, %r116;
	mov.b32 	%f220, %r160;
	add.f32 	%f221, %f219, %f220;
	mov.b32 	%r161, %f221;
	shfl.sync.bfly.b32 	%r163|%p44, %r161, %r128, %r114, %r116;
	mov.b32 	%f222, %r163;
	add.f32 	%f223, %f221, %f222;
	st.local.f32 	[%rd2+12], %f223;
	st.shared.f32 	[%r10], %f223;
	bar.sync 	0;
	@%p1 bra 	$L__BB61_17;

	ld.shared.f32 	%f224, [%r4];
	mov.b32 	%r164, %f224;
	mov.u32 	%r165, 31;
	mov.u32 	%r166, 16;
	mov.u32 	%r167, -1;
	shfl.sync.bfly.b32 	%r168|%p45, %r164, %r166, %r165, %r167;
	mov.b32 	%f225, %r168;
	add.f32 	%f226, %f224, %f225;
	mov.b32 	%r169, %f226;
	mov.u32 	%r170, 8;
	shfl.sync.bfly.b32 	%r171|%p46, %r169, %r170, %r165, %r167;
	mov.b32 	%f227, %r171;
	add.f32 	%f228, %f226, %f227;
	mov.b32 	%r172, %f228;
	mov.u32 	%r173, 4;
	shfl.sync.bfly.b32 	%r174|%p47, %r172, %r173, %r165, %r167;
	mov.b32 	%f229, %r174;
	add.f32 	%f230, %f228, %f229;
	mov.b32 	%r175, %f230;
	mov.u32 	%r176, 2;
	shfl.sync.bfly.b32 	%r177|%p48, %r175, %r176, %r165, %r167;
	mov.b32 	%f231, %r177;
	add.f32 	%f232, %f230, %f231;
	mov.b32 	%r178, %f232;
	mov.u32 	%r179, 1;
	shfl.sync.bfly.b32 	%r180|%p49, %r178, %r179, %r165, %r167;
	mov.b32 	%f233, %r180;
	add.f32 	%f234, %f232, %f233;
	st.local.f32 	[%rd2+12], %f234;

$L__BB61_17:
	bar.sync 	0;
	mov.b32 	%r181, %f291;
	mov.u32 	%r182, 31;
	mov.u32 	%r183, 16;
	mov.u32 	%r184, -1;
	shfl.sync.bfly.b32 	%r185|%p51, %r181, %r183, %r182, %r184;
	mov.b32 	%f235, %r185;
	add.f32 	%f236, %f291, %f235;
	mov.b32 	%r186, %f236;
	mov.u32 	%r187, 8;
	shfl.sync.bfly.b32 	%r188|%p52, %r186, %r187, %r182, %r184;
	mov.b32 	%f237, %r188;
	add.f32 	%f238, %f236, %f237;
	mov.b32 	%r189, %f238;
	mov.u32 	%r190, 4;
	shfl.sync.bfly.b32 	%r191|%p53, %r189, %r190, %r182, %r184;
	mov.b32 	%f239, %r191;
	add.f32 	%f240, %f238, %f239;
	mov.b32 	%r192, %f240;
	mov.u32 	%r193, 2;
	shfl.sync.bfly.b32 	%r194|%p54, %r192, %r193, %r182, %r184;
	mov.b32 	%f241, %r194;
	add.f32 	%f242, %f240, %f241;
	mov.b32 	%r195, %f242;
	mov.u32 	%r196, 1;
	shfl.sync.bfly.b32 	%r197|%p55, %r195, %r196, %r182, %r184;
	mov.b32 	%f243, %r197;
	add.f32 	%f244, %f242, %f243;
	st.local.f32 	[%rd2+16], %f244;
	st.shared.f32 	[%r10], %f244;
	bar.sync 	0;
	@%p1 bra 	$L__BB61_19;

	ld.shared.f32 	%f245, [%r4];
	mov.b32 	%r198, %f245;
	shfl.sync.bfly.b32 	%r202|%p56, %r198, %r183, %r182, %r184;
	mov.b32 	%f246, %r202;
	add.f32 	%f247, %f245, %f246;
	mov.b32 	%r203, %f247;
	shfl.sync.bfly.b32 	%r205|%p57, %r203, %r187, %r182, %r184;
	mov.b32 	%f248, %r205;
	add.f32 	%f249, %f247, %f248;
	mov.b32 	%r206, %f249;
	shfl.sync.bfly.b32 	%r208|%p58, %r206, %r190, %r182, %r184;
	mov.b32 	%f250, %r208;
	add.f32 	%f251, %f249, %f250;
	mov.b32 	%r209, %f251;
	shfl.sync.bfly.b32 	%r211|%p59, %r209, %r193, %r182, %r184;
	mov.b32 	%f252, %r211;
	add.f32 	%f253, %f251, %f252;
	mov.b32 	%r212, %f253;
	shfl.sync.bfly.b32 	%r214|%p60, %r212, %r196, %r182, %r184;
	mov.b32 	%f254, %r214;
	add.f32 	%f255, %f253, %f254;
	st.local.f32 	[%rd2+16], %f255;

$L__BB61_19:
	bar.sync 	0;
	mov.b32 	%r215, %f290;
	shfl.sync.bfly.b32 	%r219|%p62, %r215, %r183, %r182, %r184;
	mov.b32 	%f256, %r219;
	add.f32 	%f257, %f290, %f256;
	mov.b32 	%r220, %f257;
	shfl.sync.bfly.b32 	%r222|%p63, %r220, %r187, %r182, %r184;
	mov.b32 	%f258, %r222;
	add.f32 	%f259, %f257, %f258;
	mov.b32 	%r223, %f259;
	shfl.sync.bfly.b32 	%r225|%p64, %r223, %r190, %r182, %r184;
	mov.b32 	%f260, %r225;
	add.f32 	%f261, %f259, %f260;
	mov.b32 	%r226, %f261;
	shfl.sync.bfly.b32 	%r228|%p65, %r226, %r193, %r182, %r184;
	mov.b32 	%f262, %r228;
	add.f32 	%f263, %f261, %f262;
	mov.b32 	%r229, %f263;
	shfl.sync.bfly.b32 	%r231|%p66, %r229, %r196, %r182, %r184;
	mov.b32 	%f264, %r231;
	add.f32 	%f265, %f263, %f264;
	st.local.f32 	[%rd2+20], %f265;
	st.shared.f32 	[%r10], %f265;
	bar.sync 	0;
	@%p1 bra 	$L__BB61_21;

	ld.shared.f32 	%f266, [%r4];
	mov.b32 	%r232, %f266;
	mov.u32 	%r233, 31;
	mov.u32 	%r234, 16;
	mov.u32 	%r235, -1;
	shfl.sync.bfly.b32 	%r236|%p67, %r232, %r234, %r233, %r235;
	mov.b32 	%f267, %r236;
	add.f32 	%f268, %f266, %f267;
	mov.b32 	%r237, %f268;
	mov.u32 	%r238, 8;
	shfl.sync.bfly.b32 	%r239|%p68, %r237, %r238, %r233, %r235;
	mov.b32 	%f269, %r239;
	add.f32 	%f270, %f268, %f269;
	mov.b32 	%r240, %f270;
	mov.u32 	%r241, 4;
	shfl.sync.bfly.b32 	%r242|%p69, %r240, %r241, %r233, %r235;
	mov.b32 	%f271, %r242;
	add.f32 	%f272, %f270, %f271;
	mov.b32 	%r243, %f272;
	mov.u32 	%r244, 2;
	shfl.sync.bfly.b32 	%r245|%p70, %r243, %r244, %r233, %r235;
	mov.b32 	%f273, %r245;
	add.f32 	%f274, %f272, %f273;
	mov.b32 	%r246, %f274;
	mov.u32 	%r247, 1;
	shfl.sync.bfly.b32 	%r248|%p71, %r246, %r247, %r233, %r235;
	mov.b32 	%f275, %r248;
	add.f32 	%f276, %f274, %f275;
	st.local.f32 	[%rd2+20], %f276;

$L__BB61_21:
	bar.sync 	0;
	setp.gt.s32 	%p72, %r3, 5;
	@%p72 bra 	$L__BB61_23;

	mul.wide.s32 	%rd57, %r3, 4;
	add.s64 	%rd58, %rd2, %rd57;
	ld.local.f32 	%f277, [%rd58];
	mad.lo.s32 	%r249, %r3, %r13, %r2;
	cvt.s64.s32 	%rd59, %r249;
	mul.lo.s32 	%r250, %r1, %r14;
	cvt.s64.s32 	%rd60, %r250;
	add.s64 	%rd61, %rd60, %rd59;
	cvta.to.global.u64 	%rd62, %rd19;
	shl.b64 	%rd63, %rd61, 2;
	add.s64 	%rd64, %rd62, %rd63;
	st.global.f32 	[%rd64], %f277;

$L__BB61_23:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_7_bs_256
.visible .entry ggml_matvec_f32_ncols_7_bs_256(
	.param .u64 ggml_matvec_f32_ncols_7_bs_256_param_0,
	.param .u64 ggml_matvec_f32_ncols_7_bs_256_param_1,
	.param .u64 ggml_matvec_f32_ncols_7_bs_256_param_2,
	.param .u32 ggml_matvec_f32_ncols_7_bs_256_param_3,
	.param .u32 ggml_matvec_f32_ncols_7_bs_256_param_4,
	.param .u32 ggml_matvec_f32_ncols_7_bs_256_param_5,
	.param .u32 ggml_matvec_f32_ncols_7_bs_256_param_6,
	.param .u32 ggml_matvec_f32_ncols_7_bs_256_param_7,
	.param .u32 ggml_matvec_f32_ncols_7_bs_256_param_8,
	.param .u32 ggml_matvec_f32_ncols_7_bs_256_param_9,
	.param .u32 ggml_matvec_f32_ncols_7_bs_256_param_10,
	.param .u32 ggml_matvec_f32_ncols_7_bs_256_param_11
)
{
	.local .align 4 .b8 	__local_depot62[28];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<84>;
	.reg .f32 	%f<343>;
	.reg .b32 	%r<289>;
	.reg .b64 	%rd<71>;


	mov.u64 	%SPL, __local_depot62;
	ld.param.u64 	%rd21, [ggml_matvec_f32_ncols_7_bs_256_param_0];
	ld.param.u64 	%rd22, [ggml_matvec_f32_ncols_7_bs_256_param_1];
	ld.param.u64 	%rd20, [ggml_matvec_f32_ncols_7_bs_256_param_2];
	ld.param.u32 	%r11, [ggml_matvec_f32_ncols_7_bs_256_param_3];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_7_bs_256_param_5];
	ld.param.u32 	%r12, [ggml_matvec_f32_ncols_7_bs_256_param_6];
	ld.param.u32 	%r13, [ggml_matvec_f32_ncols_7_bs_256_param_7];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_7_bs_256_param_8];
	ld.param.u32 	%r17, [ggml_matvec_f32_ncols_7_bs_256_param_9];
	ld.param.u32 	%r18, [ggml_matvec_f32_ncols_7_bs_256_param_10];
	ld.param.u32 	%r14, [ggml_matvec_f32_ncols_7_bs_256_param_11];
	cvta.to.global.u64 	%rd70, %rd22;
	add.u64 	%rd2, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r19, %r1, %r16;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r20, %r2, %r15;
	mad.lo.s32 	%r21, %r19, %r17, %r20;
	cvt.s64.s32 	%rd3, %r21;
	cvta.to.global.u64 	%rd4, %rd21;
	mul.lo.s32 	%r22, %r1, %r18;
	cvt.s64.s32 	%rd5, %r22;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r23, %r3, 2;
	mov.u32 	%r24, data_mmv;
	add.s32 	%r4, %r24, %r23;
	@%p1 bra 	$L__BB62_2;

	mov.u32 	%r25, 0;
	st.shared.u32 	[%r4], %r25;

$L__BB62_2:
	bar.sync 	0;
	mov.f32 	%f336, 0f00000000;
	mov.u32 	%r26, 0;
	st.local.u32 	[%rd2], %r26;
	st.local.u32 	[%rd2+4], %r26;
	st.local.u32 	[%rd2+8], %r26;
	st.local.u32 	[%rd2+12], %r26;
	st.local.u32 	[%rd2+16], %r26;
	st.local.u32 	[%rd2+20], %r26;
	st.local.u32 	[%rd2+24], %r26;
	setp.ge.s32 	%p2, %r3, %r11;
	mov.f32 	%f337, %f336;
	mov.f32 	%f338, %f336;
	mov.f32 	%f339, %f336;
	mov.f32 	%f340, %f336;
	mov.f32 	%f341, %f336;
	mov.f32 	%f342, %f336;
	@%p2 bra 	$L__BB62_9;

	not.b32 	%r27, %r3;
	add.s32 	%r5, %r27, %r11;
	and.b32  	%r28, %r5, 256;
	setp.ne.s32 	%p3, %r28, 0;
	mov.f32 	%f336, 0f00000000;
	mov.u32 	%r288, %r3;
	@%p3 bra 	$L__BB62_5;

	shl.b64 	%rd24, %rd5, 2;
	add.s64 	%rd25, %rd70, %rd24;
	shl.b64 	%rd26, %rd3, 2;
	add.s64 	%rd27, %rd4, %rd26;
	mul.wide.s32 	%rd28, %r3, 8;
	add.s64 	%rd29, %rd27, %rd28;
	ld.global.nc.v2.f32 	{%f50, %f51}, [%rd29];
	add.s64 	%rd30, %rd25, %rd28;
	ld.global.nc.v2.f32 	{%f54, %f55}, [%rd30];
	fma.rn.f32 	%f58, %f50, %f54, 0f00000000;
	fma.rn.f32 	%f342, %f51, %f55, %f58;
	st.local.f32 	[%rd2], %f342;
	mul.wide.s32 	%rd31, %r12, 8;
	add.s64 	%rd32, %rd30, %rd31;
	ld.global.nc.v2.f32 	{%f59, %f60}, [%rd32];
	fma.rn.f32 	%f63, %f50, %f59, 0f00000000;
	fma.rn.f32 	%f341, %f51, %f60, %f63;
	st.local.f32 	[%rd2+4], %f341;
	add.s32 	%r29, %r3, %r12;
	add.s32 	%r30, %r29, %r12;
	mul.wide.s32 	%rd33, %r30, 8;
	add.s64 	%rd34, %rd25, %rd33;
	ld.global.nc.v2.f32 	{%f64, %f65}, [%rd34];
	fma.rn.f32 	%f68, %f50, %f64, 0f00000000;
	fma.rn.f32 	%f340, %f51, %f65, %f68;
	st.local.f32 	[%rd2+8], %f340;
	add.s64 	%rd35, %rd34, %rd31;
	ld.global.nc.v2.f32 	{%f69, %f70}, [%rd35];
	fma.rn.f32 	%f73, %f50, %f69, 0f00000000;
	fma.rn.f32 	%f339, %f51, %f70, %f73;
	st.local.f32 	[%rd2+12], %f339;
	add.s64 	%rd36, %rd35, %rd31;
	ld.global.nc.v2.f32 	{%f74, %f75}, [%rd36];
	fma.rn.f32 	%f78, %f50, %f74, 0f00000000;
	fma.rn.f32 	%f338, %f51, %f75, %f78;
	st.local.f32 	[%rd2+16], %f338;
	add.s64 	%rd37, %rd36, %rd31;
	ld.global.nc.v2.f32 	{%f79, %f80}, [%rd37];
	fma.rn.f32 	%f83, %f50, %f79, 0f00000000;
	fma.rn.f32 	%f337, %f51, %f80, %f83;
	st.local.f32 	[%rd2+20], %f337;
	add.s64 	%rd38, %rd37, %rd31;
	ld.global.nc.v2.f32 	{%f84, %f85}, [%rd38];
	fma.rn.f32 	%f88, %f50, %f84, 0f00000000;
	fma.rn.f32 	%f336, %f51, %f85, %f88;
	st.local.f32 	[%rd2+24], %f336;
	add.s32 	%r288, %r3, 256;

$L__BB62_5:
	and.b32  	%r31, %r5, -256;
	setp.eq.s32 	%p4, %r31, 0;
	@%p4 bra 	$L__BB62_9;

	add.s32 	%r32, %r288, %r12;
	add.s32 	%r33, %r32, 256;
	mul.wide.s32 	%rd39, %r33, 8;
	shl.b64 	%rd40, %rd5, 2;
	add.s64 	%rd7, %rd39, %rd40;
	shl.b32 	%r34, %r12, 1;
	add.s32 	%r35, %r288, %r34;
	mad.lo.s32 	%r36, %r12, 3, %r288;
	shl.b32 	%r37, %r12, 2;
	add.s32 	%r38, %r288, %r37;
	mad.lo.s32 	%r39, %r12, 5, %r288;
	mad.lo.s32 	%r40, %r12, 6, %r288;
	mul.wide.s32 	%rd41, %r35, 8;
	add.s64 	%rd8, %rd41, %rd40;
	mul.wide.s32 	%rd42, %r36, 8;
	add.s64 	%rd9, %rd42, %rd40;
	mul.wide.s32 	%rd43, %r38, 8;
	add.s64 	%rd10, %rd43, %rd40;
	mul.wide.s32 	%rd44, %r39, 8;
	add.s64 	%rd11, %rd44, %rd40;
	mul.wide.s32 	%rd45, %r40, 8;
	add.s64 	%rd12, %rd45, %rd40;
	mul.wide.s32 	%rd46, %r288, 2;
	add.s64 	%rd47, %rd46, %rd3;
	shl.b64 	%rd48, %rd47, 2;
	add.s64 	%rd49, %rd4, %rd48;
	add.s64 	%rd69, %rd49, 2048;
	mul.wide.s32 	%rd50, %r288, 8;
	mul.wide.s32 	%rd51, %r12, 8;
	add.s64 	%rd52, %rd50, %rd51;
	add.s64 	%rd14, %rd52, %rd40;
	add.s64 	%rd15, %rd50, %rd40;

$L__BB62_7:
	ld.global.nc.v2.f32 	{%f89, %f90}, [%rd69+-2048];
	add.s64 	%rd53, %rd70, %rd15;
	ld.global.nc.v2.f32 	{%f93, %f94}, [%rd53];
	fma.rn.f32 	%f97, %f89, %f93, %f342;
	fma.rn.f32 	%f98, %f90, %f94, %f97;
	add.s64 	%rd54, %rd70, %rd14;
	ld.global.nc.v2.f32 	{%f99, %f100}, [%rd54];
	fma.rn.f32 	%f103, %f89, %f99, %f341;
	fma.rn.f32 	%f104, %f90, %f100, %f103;
	add.s64 	%rd55, %rd70, %rd8;
	ld.global.nc.v2.f32 	{%f105, %f106}, [%rd55];
	fma.rn.f32 	%f109, %f89, %f105, %f340;
	fma.rn.f32 	%f110, %f90, %f106, %f109;
	add.s64 	%rd56, %rd70, %rd9;
	ld.global.nc.v2.f32 	{%f111, %f112}, [%rd56];
	fma.rn.f32 	%f115, %f89, %f111, %f339;
	fma.rn.f32 	%f116, %f90, %f112, %f115;
	add.s64 	%rd57, %rd70, %rd10;
	ld.global.nc.v2.f32 	{%f117, %f118}, [%rd57];
	fma.rn.f32 	%f121, %f89, %f117, %f338;
	fma.rn.f32 	%f122, %f90, %f118, %f121;
	add.s64 	%rd58, %rd70, %rd11;
	ld.global.nc.v2.f32 	{%f123, %f124}, [%rd58];
	fma.rn.f32 	%f127, %f89, %f123, %f337;
	fma.rn.f32 	%f128, %f90, %f124, %f127;
	add.s64 	%rd59, %rd70, %rd12;
	ld.global.nc.v2.f32 	{%f129, %f130}, [%rd59];
	fma.rn.f32 	%f133, %f89, %f129, %f336;
	fma.rn.f32 	%f134, %f90, %f130, %f133;
	ld.global.nc.v2.f32 	{%f135, %f136}, [%rd69];
	ld.global.nc.v2.f32 	{%f139, %f140}, [%rd53+2048];
	fma.rn.f32 	%f143, %f135, %f139, %f98;
	fma.rn.f32 	%f342, %f136, %f140, %f143;
	add.s64 	%rd60, %rd70, %rd7;
	ld.global.nc.v2.f32 	{%f144, %f145}, [%rd60];
	fma.rn.f32 	%f148, %f135, %f144, %f104;
	fma.rn.f32 	%f341, %f136, %f145, %f148;
	ld.global.nc.v2.f32 	{%f149, %f150}, [%rd55+2048];
	fma.rn.f32 	%f153, %f135, %f149, %f110;
	fma.rn.f32 	%f340, %f136, %f150, %f153;
	ld.global.nc.v2.f32 	{%f154, %f155}, [%rd56+2048];
	fma.rn.f32 	%f158, %f135, %f154, %f116;
	fma.rn.f32 	%f339, %f136, %f155, %f158;
	ld.global.nc.v2.f32 	{%f159, %f160}, [%rd57+2048];
	fma.rn.f32 	%f163, %f135, %f159, %f122;
	fma.rn.f32 	%f338, %f136, %f160, %f163;
	ld.global.nc.v2.f32 	{%f164, %f165}, [%rd58+2048];
	fma.rn.f32 	%f168, %f135, %f164, %f128;
	fma.rn.f32 	%f337, %f136, %f165, %f168;
	ld.global.nc.v2.f32 	{%f169, %f170}, [%rd59+2048];
	fma.rn.f32 	%f173, %f135, %f169, %f134;
	fma.rn.f32 	%f336, %f136, %f170, %f173;
	add.s64 	%rd70, %rd70, 4096;
	add.s64 	%rd69, %rd69, 4096;
	add.s32 	%r288, %r288, 512;
	setp.lt.s32 	%p5, %r288, %r11;
	@%p5 bra 	$L__BB62_7;

	st.local.f32 	[%rd2], %f342;
	st.local.f32 	[%rd2+4], %f341;
	st.local.f32 	[%rd2+8], %f340;
	st.local.f32 	[%rd2+12], %f339;
	st.local.f32 	[%rd2+16], %f338;
	st.local.f32 	[%rd2+20], %f337;
	st.local.f32 	[%rd2+24], %f336;

$L__BB62_9:
	shr.s32 	%r41, %r3, 31;
	shr.u32 	%r42, %r41, 27;
	add.s32 	%r43, %r3, %r42;
	shr.s32 	%r44, %r43, 5;
	shl.b32 	%r45, %r44, 2;
	add.s32 	%r10, %r24, %r45;
	mov.u32 	%r47, 2;
	mov.b32 	%r48, %f342;
	mov.u32 	%r49, 31;
	mov.u32 	%r50, 16;
	mov.u32 	%r51, -1;
	shfl.sync.bfly.b32 	%r52|%p6, %r48, %r50, %r49, %r51;
	mov.b32 	%f174, %r52;
	add.f32 	%f175, %f342, %f174;
	mov.b32 	%r53, %f175;
	mov.u32 	%r54, 8;
	shfl.sync.bfly.b32 	%r55|%p7, %r53, %r54, %r49, %r51;
	mov.b32 	%f176, %r55;
	add.f32 	%f177, %f175, %f176;
	mov.b32 	%r56, %f177;
	mov.u32 	%r57, 4;
	shfl.sync.bfly.b32 	%r58|%p8, %r56, %r57, %r49, %r51;
	mov.b32 	%f178, %r58;
	add.f32 	%f179, %f177, %f178;
	mov.b32 	%r59, %f179;
	shfl.sync.bfly.b32 	%r60|%p9, %r59, %r47, %r49, %r51;
	mov.b32 	%f180, %r60;
	add.f32 	%f181, %f179, %f180;
	mov.b32 	%r61, %f181;
	mov.u32 	%r62, 1;
	shfl.sync.bfly.b32 	%r63|%p10, %r61, %r62, %r49, %r51;
	mov.b32 	%f182, %r63;
	add.f32 	%f183, %f181, %f182;
	st.local.f32 	[%rd2], %f183;
	st.shared.f32 	[%r10], %f183;
	bar.sync 	0;
	@%p1 bra 	$L__BB62_11;

	ld.shared.f32 	%f184, [%r4];
	mov.b32 	%r64, %f184;
	shfl.sync.bfly.b32 	%r68|%p12, %r64, %r50, %r49, %r51;
	mov.b32 	%f185, %r68;
	add.f32 	%f186, %f184, %f185;
	mov.b32 	%r69, %f186;
	shfl.sync.bfly.b32 	%r71|%p13, %r69, %r54, %r49, %r51;
	mov.b32 	%f187, %r71;
	add.f32 	%f188, %f186, %f187;
	mov.b32 	%r72, %f188;
	shfl.sync.bfly.b32 	%r74|%p14, %r72, %r57, %r49, %r51;
	mov.b32 	%f189, %r74;
	add.f32 	%f190, %f188, %f189;
	mov.b32 	%r75, %f190;
	shfl.sync.bfly.b32 	%r77|%p15, %r75, %r47, %r49, %r51;
	mov.b32 	%f191, %r77;
	add.f32 	%f192, %f190, %f191;
	mov.b32 	%r78, %f192;
	shfl.sync.bfly.b32 	%r80|%p16, %r78, %r62, %r49, %r51;
	mov.b32 	%f193, %r80;
	add.f32 	%f194, %f192, %f193;
	st.local.f32 	[%rd2], %f194;

$L__BB62_11:
	bar.sync 	0;
	mov.b32 	%r81, %f341;
	shfl.sync.bfly.b32 	%r85|%p18, %r81, %r50, %r49, %r51;
	mov.b32 	%f195, %r85;
	add.f32 	%f196, %f341, %f195;
	mov.b32 	%r86, %f196;
	shfl.sync.bfly.b32 	%r88|%p19, %r86, %r54, %r49, %r51;
	mov.b32 	%f197, %r88;
	add.f32 	%f198, %f196, %f197;
	mov.b32 	%r89, %f198;
	shfl.sync.bfly.b32 	%r91|%p20, %r89, %r57, %r49, %r51;
	mov.b32 	%f199, %r91;
	add.f32 	%f200, %f198, %f199;
	mov.b32 	%r92, %f200;
	shfl.sync.bfly.b32 	%r94|%p21, %r92, %r47, %r49, %r51;
	mov.b32 	%f201, %r94;
	add.f32 	%f202, %f200, %f201;
	mov.b32 	%r95, %f202;
	shfl.sync.bfly.b32 	%r97|%p22, %r95, %r62, %r49, %r51;
	mov.b32 	%f203, %r97;
	add.f32 	%f204, %f202, %f203;
	st.local.f32 	[%rd2+4], %f204;
	st.shared.f32 	[%r10], %f204;
	bar.sync 	0;
	@%p1 bra 	$L__BB62_13;

	ld.shared.f32 	%f205, [%r4];
	mov.b32 	%r98, %f205;
	mov.u32 	%r99, 31;
	mov.u32 	%r100, 16;
	mov.u32 	%r101, -1;
	shfl.sync.bfly.b32 	%r102|%p23, %r98, %r100, %r99, %r101;
	mov.b32 	%f206, %r102;
	add.f32 	%f207, %f205, %f206;
	mov.b32 	%r103, %f207;
	mov.u32 	%r104, 8;
	shfl.sync.bfly.b32 	%r105|%p24, %r103, %r104, %r99, %r101;
	mov.b32 	%f208, %r105;
	add.f32 	%f209, %f207, %f208;
	mov.b32 	%r106, %f209;
	mov.u32 	%r107, 4;
	shfl.sync.bfly.b32 	%r108|%p25, %r106, %r107, %r99, %r101;
	mov.b32 	%f210, %r108;
	add.f32 	%f211, %f209, %f210;
	mov.b32 	%r109, %f211;
	mov.u32 	%r110, 2;
	shfl.sync.bfly.b32 	%r111|%p26, %r109, %r110, %r99, %r101;
	mov.b32 	%f212, %r111;
	add.f32 	%f213, %f211, %f212;
	mov.b32 	%r112, %f213;
	mov.u32 	%r113, 1;
	shfl.sync.bfly.b32 	%r114|%p27, %r112, %r113, %r99, %r101;
	mov.b32 	%f214, %r114;
	add.f32 	%f215, %f213, %f214;
	st.local.f32 	[%rd2+4], %f215;

$L__BB62_13:
	bar.sync 	0;
	mov.b32 	%r115, %f340;
	mov.u32 	%r116, 31;
	mov.u32 	%r117, 16;
	mov.u32 	%r118, -1;
	shfl.sync.bfly.b32 	%r119|%p29, %r115, %r117, %r116, %r118;
	mov.b32 	%f216, %r119;
	add.f32 	%f217, %f340, %f216;
	mov.b32 	%r120, %f217;
	mov.u32 	%r121, 8;
	shfl.sync.bfly.b32 	%r122|%p30, %r120, %r121, %r116, %r118;
	mov.b32 	%f218, %r122;
	add.f32 	%f219, %f217, %f218;
	mov.b32 	%r123, %f219;
	mov.u32 	%r124, 4;
	shfl.sync.bfly.b32 	%r125|%p31, %r123, %r124, %r116, %r118;
	mov.b32 	%f220, %r125;
	add.f32 	%f221, %f219, %f220;
	mov.b32 	%r126, %f221;
	mov.u32 	%r127, 2;
	shfl.sync.bfly.b32 	%r128|%p32, %r126, %r127, %r116, %r118;
	mov.b32 	%f222, %r128;
	add.f32 	%f223, %f221, %f222;
	mov.b32 	%r129, %f223;
	mov.u32 	%r130, 1;
	shfl.sync.bfly.b32 	%r131|%p33, %r129, %r130, %r116, %r118;
	mov.b32 	%f224, %r131;
	add.f32 	%f225, %f223, %f224;
	st.local.f32 	[%rd2+8], %f225;
	st.shared.f32 	[%r10], %f225;
	bar.sync 	0;
	@%p1 bra 	$L__BB62_15;

	ld.shared.f32 	%f226, [%r4];
	mov.b32 	%r132, %f226;
	shfl.sync.bfly.b32 	%r136|%p34, %r132, %r117, %r116, %r118;
	mov.b32 	%f227, %r136;
	add.f32 	%f228, %f226, %f227;
	mov.b32 	%r137, %f228;
	shfl.sync.bfly.b32 	%r139|%p35, %r137, %r121, %r116, %r118;
	mov.b32 	%f229, %r139;
	add.f32 	%f230, %f228, %f229;
	mov.b32 	%r140, %f230;
	shfl.sync.bfly.b32 	%r142|%p36, %r140, %r124, %r116, %r118;
	mov.b32 	%f231, %r142;
	add.f32 	%f232, %f230, %f231;
	mov.b32 	%r143, %f232;
	shfl.sync.bfly.b32 	%r145|%p37, %r143, %r127, %r116, %r118;
	mov.b32 	%f233, %r145;
	add.f32 	%f234, %f232, %f233;
	mov.b32 	%r146, %f234;
	shfl.sync.bfly.b32 	%r148|%p38, %r146, %r130, %r116, %r118;
	mov.b32 	%f235, %r148;
	add.f32 	%f236, %f234, %f235;
	st.local.f32 	[%rd2+8], %f236;

$L__BB62_15:
	bar.sync 	0;
	mov.b32 	%r149, %f339;
	shfl.sync.bfly.b32 	%r153|%p40, %r149, %r117, %r116, %r118;
	mov.b32 	%f237, %r153;
	add.f32 	%f238, %f339, %f237;
	mov.b32 	%r154, %f238;
	shfl.sync.bfly.b32 	%r156|%p41, %r154, %r121, %r116, %r118;
	mov.b32 	%f239, %r156;
	add.f32 	%f240, %f238, %f239;
	mov.b32 	%r157, %f240;
	shfl.sync.bfly.b32 	%r159|%p42, %r157, %r124, %r116, %r118;
	mov.b32 	%f241, %r159;
	add.f32 	%f242, %f240, %f241;
	mov.b32 	%r160, %f242;
	shfl.sync.bfly.b32 	%r162|%p43, %r160, %r127, %r116, %r118;
	mov.b32 	%f243, %r162;
	add.f32 	%f244, %f242, %f243;
	mov.b32 	%r163, %f244;
	shfl.sync.bfly.b32 	%r165|%p44, %r163, %r130, %r116, %r118;
	mov.b32 	%f245, %r165;
	add.f32 	%f246, %f244, %f245;
	st.local.f32 	[%rd2+12], %f246;
	st.shared.f32 	[%r10], %f246;
	bar.sync 	0;
	@%p1 bra 	$L__BB62_17;

	ld.shared.f32 	%f247, [%r4];
	mov.b32 	%r166, %f247;
	mov.u32 	%r167, 31;
	mov.u32 	%r168, 16;
	mov.u32 	%r169, -1;
	shfl.sync.bfly.b32 	%r170|%p45, %r166, %r168, %r167, %r169;
	mov.b32 	%f248, %r170;
	add.f32 	%f249, %f247, %f248;
	mov.b32 	%r171, %f249;
	mov.u32 	%r172, 8;
	shfl.sync.bfly.b32 	%r173|%p46, %r171, %r172, %r167, %r169;
	mov.b32 	%f250, %r173;
	add.f32 	%f251, %f249, %f250;
	mov.b32 	%r174, %f251;
	mov.u32 	%r175, 4;
	shfl.sync.bfly.b32 	%r176|%p47, %r174, %r175, %r167, %r169;
	mov.b32 	%f252, %r176;
	add.f32 	%f253, %f251, %f252;
	mov.b32 	%r177, %f253;
	mov.u32 	%r178, 2;
	shfl.sync.bfly.b32 	%r179|%p48, %r177, %r178, %r167, %r169;
	mov.b32 	%f254, %r179;
	add.f32 	%f255, %f253, %f254;
	mov.b32 	%r180, %f255;
	mov.u32 	%r181, 1;
	shfl.sync.bfly.b32 	%r182|%p49, %r180, %r181, %r167, %r169;
	mov.b32 	%f256, %r182;
	add.f32 	%f257, %f255, %f256;
	st.local.f32 	[%rd2+12], %f257;

$L__BB62_17:
	bar.sync 	0;
	mov.b32 	%r183, %f338;
	mov.u32 	%r184, 31;
	mov.u32 	%r185, 16;
	mov.u32 	%r186, -1;
	shfl.sync.bfly.b32 	%r187|%p51, %r183, %r185, %r184, %r186;
	mov.b32 	%f258, %r187;
	add.f32 	%f259, %f338, %f258;
	mov.b32 	%r188, %f259;
	mov.u32 	%r189, 8;
	shfl.sync.bfly.b32 	%r190|%p52, %r188, %r189, %r184, %r186;
	mov.b32 	%f260, %r190;
	add.f32 	%f261, %f259, %f260;
	mov.b32 	%r191, %f261;
	mov.u32 	%r192, 4;
	shfl.sync.bfly.b32 	%r193|%p53, %r191, %r192, %r184, %r186;
	mov.b32 	%f262, %r193;
	add.f32 	%f263, %f261, %f262;
	mov.b32 	%r194, %f263;
	mov.u32 	%r195, 2;
	shfl.sync.bfly.b32 	%r196|%p54, %r194, %r195, %r184, %r186;
	mov.b32 	%f264, %r196;
	add.f32 	%f265, %f263, %f264;
	mov.b32 	%r197, %f265;
	mov.u32 	%r198, 1;
	shfl.sync.bfly.b32 	%r199|%p55, %r197, %r198, %r184, %r186;
	mov.b32 	%f266, %r199;
	add.f32 	%f267, %f265, %f266;
	st.local.f32 	[%rd2+16], %f267;
	st.shared.f32 	[%r10], %f267;
	bar.sync 	0;
	@%p1 bra 	$L__BB62_19;

	ld.shared.f32 	%f268, [%r4];
	mov.b32 	%r200, %f268;
	shfl.sync.bfly.b32 	%r204|%p56, %r200, %r185, %r184, %r186;
	mov.b32 	%f269, %r204;
	add.f32 	%f270, %f268, %f269;
	mov.b32 	%r205, %f270;
	shfl.sync.bfly.b32 	%r207|%p57, %r205, %r189, %r184, %r186;
	mov.b32 	%f271, %r207;
	add.f32 	%f272, %f270, %f271;
	mov.b32 	%r208, %f272;
	shfl.sync.bfly.b32 	%r210|%p58, %r208, %r192, %r184, %r186;
	mov.b32 	%f273, %r210;
	add.f32 	%f274, %f272, %f273;
	mov.b32 	%r211, %f274;
	shfl.sync.bfly.b32 	%r213|%p59, %r211, %r195, %r184, %r186;
	mov.b32 	%f275, %r213;
	add.f32 	%f276, %f274, %f275;
	mov.b32 	%r214, %f276;
	shfl.sync.bfly.b32 	%r216|%p60, %r214, %r198, %r184, %r186;
	mov.b32 	%f277, %r216;
	add.f32 	%f278, %f276, %f277;
	st.local.f32 	[%rd2+16], %f278;

$L__BB62_19:
	bar.sync 	0;
	mov.b32 	%r217, %f337;
	shfl.sync.bfly.b32 	%r221|%p62, %r217, %r185, %r184, %r186;
	mov.b32 	%f279, %r221;
	add.f32 	%f280, %f337, %f279;
	mov.b32 	%r222, %f280;
	shfl.sync.bfly.b32 	%r224|%p63, %r222, %r189, %r184, %r186;
	mov.b32 	%f281, %r224;
	add.f32 	%f282, %f280, %f281;
	mov.b32 	%r225, %f282;
	shfl.sync.bfly.b32 	%r227|%p64, %r225, %r192, %r184, %r186;
	mov.b32 	%f283, %r227;
	add.f32 	%f284, %f282, %f283;
	mov.b32 	%r228, %f284;
	shfl.sync.bfly.b32 	%r230|%p65, %r228, %r195, %r184, %r186;
	mov.b32 	%f285, %r230;
	add.f32 	%f286, %f284, %f285;
	mov.b32 	%r231, %f286;
	shfl.sync.bfly.b32 	%r233|%p66, %r231, %r198, %r184, %r186;
	mov.b32 	%f287, %r233;
	add.f32 	%f288, %f286, %f287;
	st.local.f32 	[%rd2+20], %f288;
	st.shared.f32 	[%r10], %f288;
	bar.sync 	0;
	@%p1 bra 	$L__BB62_21;

	ld.shared.f32 	%f289, [%r4];
	mov.b32 	%r234, %f289;
	mov.u32 	%r235, 31;
	mov.u32 	%r236, 16;
	mov.u32 	%r237, -1;
	shfl.sync.bfly.b32 	%r238|%p67, %r234, %r236, %r235, %r237;
	mov.b32 	%f290, %r238;
	add.f32 	%f291, %f289, %f290;
	mov.b32 	%r239, %f291;
	mov.u32 	%r240, 8;
	shfl.sync.bfly.b32 	%r241|%p68, %r239, %r240, %r235, %r237;
	mov.b32 	%f292, %r241;
	add.f32 	%f293, %f291, %f292;
	mov.b32 	%r242, %f293;
	mov.u32 	%r243, 4;
	shfl.sync.bfly.b32 	%r244|%p69, %r242, %r243, %r235, %r237;
	mov.b32 	%f294, %r244;
	add.f32 	%f295, %f293, %f294;
	mov.b32 	%r245, %f295;
	mov.u32 	%r246, 2;
	shfl.sync.bfly.b32 	%r247|%p70, %r245, %r246, %r235, %r237;
	mov.b32 	%f296, %r247;
	add.f32 	%f297, %f295, %f296;
	mov.b32 	%r248, %f297;
	mov.u32 	%r249, 1;
	shfl.sync.bfly.b32 	%r250|%p71, %r248, %r249, %r235, %r237;
	mov.b32 	%f298, %r250;
	add.f32 	%f299, %f297, %f298;
	st.local.f32 	[%rd2+20], %f299;

$L__BB62_21:
	bar.sync 	0;
	mov.b32 	%r251, %f336;
	mov.u32 	%r252, 31;
	mov.u32 	%r253, 16;
	mov.u32 	%r254, -1;
	shfl.sync.bfly.b32 	%r255|%p73, %r251, %r253, %r252, %r254;
	mov.b32 	%f300, %r255;
	add.f32 	%f301, %f336, %f300;
	mov.b32 	%r256, %f301;
	mov.u32 	%r257, 8;
	shfl.sync.bfly.b32 	%r258|%p74, %r256, %r257, %r252, %r254;
	mov.b32 	%f302, %r258;
	add.f32 	%f303, %f301, %f302;
	mov.b32 	%r259, %f303;
	mov.u32 	%r260, 4;
	shfl.sync.bfly.b32 	%r261|%p75, %r259, %r260, %r252, %r254;
	mov.b32 	%f304, %r261;
	add.f32 	%f305, %f303, %f304;
	mov.b32 	%r262, %f305;
	mov.u32 	%r263, 2;
	shfl.sync.bfly.b32 	%r264|%p76, %r262, %r263, %r252, %r254;
	mov.b32 	%f306, %r264;
	add.f32 	%f307, %f305, %f306;
	mov.b32 	%r265, %f307;
	mov.u32 	%r266, 1;
	shfl.sync.bfly.b32 	%r267|%p77, %r265, %r266, %r252, %r254;
	mov.b32 	%f308, %r267;
	add.f32 	%f309, %f307, %f308;
	st.local.f32 	[%rd2+24], %f309;
	st.shared.f32 	[%r10], %f309;
	bar.sync 	0;
	@%p1 bra 	$L__BB62_23;

	ld.shared.f32 	%f310, [%r4];
	mov.b32 	%r268, %f310;
	shfl.sync.bfly.b32 	%r272|%p78, %r268, %r253, %r252, %r254;
	mov.b32 	%f311, %r272;
	add.f32 	%f312, %f310, %f311;
	mov.b32 	%r273, %f312;
	shfl.sync.bfly.b32 	%r275|%p79, %r273, %r257, %r252, %r254;
	mov.b32 	%f313, %r275;
	add.f32 	%f314, %f312, %f313;
	mov.b32 	%r276, %f314;
	shfl.sync.bfly.b32 	%r278|%p80, %r276, %r260, %r252, %r254;
	mov.b32 	%f315, %r278;
	add.f32 	%f316, %f314, %f315;
	mov.b32 	%r279, %f316;
	shfl.sync.bfly.b32 	%r281|%p81, %r279, %r263, %r252, %r254;
	mov.b32 	%f317, %r281;
	add.f32 	%f318, %f316, %f317;
	mov.b32 	%r282, %f318;
	shfl.sync.bfly.b32 	%r284|%p82, %r282, %r266, %r252, %r254;
	mov.b32 	%f319, %r284;
	add.f32 	%f320, %f318, %f319;
	st.local.f32 	[%rd2+24], %f320;

$L__BB62_23:
	bar.sync 	0;
	setp.gt.s32 	%p83, %r3, 6;
	@%p83 bra 	$L__BB62_25;

	mul.wide.s32 	%rd61, %r3, 4;
	add.s64 	%rd62, %rd2, %rd61;
	ld.local.f32 	%f321, [%rd62];
	mad.lo.s32 	%r285, %r3, %r13, %r2;
	cvt.s64.s32 	%rd63, %r285;
	mul.lo.s32 	%r286, %r1, %r14;
	cvt.s64.s32 	%rd64, %r286;
	add.s64 	%rd65, %rd64, %rd63;
	cvta.to.global.u64 	%rd66, %rd20;
	shl.b64 	%rd67, %rd65, 2;
	add.s64 	%rd68, %rd66, %rd67;
	st.global.f32 	[%rd68], %f321;

$L__BB62_25:
	ret;

}
	// .globl	ggml_matvec_f32_ncols_8_bs_256
.visible .entry ggml_matvec_f32_ncols_8_bs_256(
	.param .u64 ggml_matvec_f32_ncols_8_bs_256_param_0,
	.param .u64 ggml_matvec_f32_ncols_8_bs_256_param_1,
	.param .u64 ggml_matvec_f32_ncols_8_bs_256_param_2,
	.param .u32 ggml_matvec_f32_ncols_8_bs_256_param_3,
	.param .u32 ggml_matvec_f32_ncols_8_bs_256_param_4,
	.param .u32 ggml_matvec_f32_ncols_8_bs_256_param_5,
	.param .u32 ggml_matvec_f32_ncols_8_bs_256_param_6,
	.param .u32 ggml_matvec_f32_ncols_8_bs_256_param_7,
	.param .u32 ggml_matvec_f32_ncols_8_bs_256_param_8,
	.param .u32 ggml_matvec_f32_ncols_8_bs_256_param_9,
	.param .u32 ggml_matvec_f32_ncols_8_bs_256_param_10,
	.param .u32 ggml_matvec_f32_ncols_8_bs_256_param_11
)
{
	.local .align 16 .b8 	__local_depot63[32];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<95>;
	.reg .f32 	%f<390>;
	.reg .b32 	%r<323>;
	.reg .b64 	%rd<75>;


	mov.u64 	%SPL, __local_depot63;
	ld.param.u64 	%rd22, [ggml_matvec_f32_ncols_8_bs_256_param_0];
	ld.param.u64 	%rd23, [ggml_matvec_f32_ncols_8_bs_256_param_1];
	ld.param.u64 	%rd21, [ggml_matvec_f32_ncols_8_bs_256_param_2];
	ld.param.u32 	%r11, [ggml_matvec_f32_ncols_8_bs_256_param_3];
	ld.param.u32 	%r15, [ggml_matvec_f32_ncols_8_bs_256_param_5];
	ld.param.u32 	%r12, [ggml_matvec_f32_ncols_8_bs_256_param_6];
	ld.param.u32 	%r13, [ggml_matvec_f32_ncols_8_bs_256_param_7];
	ld.param.u32 	%r16, [ggml_matvec_f32_ncols_8_bs_256_param_8];
	ld.param.u32 	%r17, [ggml_matvec_f32_ncols_8_bs_256_param_9];
	ld.param.u32 	%r18, [ggml_matvec_f32_ncols_8_bs_256_param_10];
	ld.param.u32 	%r14, [ggml_matvec_f32_ncols_8_bs_256_param_11];
	cvta.to.global.u64 	%rd74, %rd23;
	add.u64 	%rd2, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r19, %r1, %r16;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r20, %r2, %r15;
	mad.lo.s32 	%r21, %r19, %r17, %r20;
	cvt.s64.s32 	%rd3, %r21;
	cvta.to.global.u64 	%rd4, %rd22;
	mul.lo.s32 	%r22, %r1, %r18;
	cvt.s64.s32 	%rd5, %r22;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r23, %r3, 2;
	mov.u32 	%r24, data_mmv;
	add.s32 	%r4, %r24, %r23;
	@%p1 bra 	$L__BB63_2;

	mov.u32 	%r25, 0;
	st.shared.u32 	[%r4], %r25;

$L__BB63_2:
	bar.sync 	0;
	mov.f32 	%f382, 0f00000000;
	st.local.v4.f32 	[%rd2], {%f382, %f382, %f382, %f382};
	st.local.v4.f32 	[%rd2+16], {%f382, %f382, %f382, %f382};
	setp.ge.s32 	%p2, %r3, %r11;
	mov.f32 	%f383, %f382;
	mov.f32 	%f384, %f382;
	mov.f32 	%f385, %f382;
	mov.f32 	%f386, %f382;
	mov.f32 	%f387, %f382;
	mov.f32 	%f388, %f382;
	mov.f32 	%f389, %f382;
	@%p2 bra 	$L__BB63_9;

	not.b32 	%r26, %r3;
	add.s32 	%r5, %r26, %r11;
	and.b32  	%r27, %r5, 256;
	setp.ne.s32 	%p3, %r27, 0;
	mov.f32 	%f382, 0f00000000;
	mov.u32 	%r322, %r3;
	@%p3 bra 	$L__BB63_5;

	shl.b64 	%rd25, %rd5, 2;
	add.s64 	%rd26, %rd74, %rd25;
	shl.b64 	%rd27, %rd3, 2;
	add.s64 	%rd28, %rd4, %rd27;
	mul.wide.s32 	%rd29, %r3, 8;
	add.s64 	%rd30, %rd28, %rd29;
	ld.global.nc.v2.f32 	{%f57, %f58}, [%rd30];
	add.s64 	%rd31, %rd26, %rd29;
	ld.global.nc.v2.f32 	{%f61, %f62}, [%rd31];
	fma.rn.f32 	%f65, %f57, %f61, 0f00000000;
	fma.rn.f32 	%f389, %f58, %f62, %f65;
	mul.wide.s32 	%rd32, %r12, 8;
	add.s64 	%rd33, %rd31, %rd32;
	ld.global.nc.v2.f32 	{%f66, %f67}, [%rd33];
	fma.rn.f32 	%f70, %f57, %f66, 0f00000000;
	fma.rn.f32 	%f388, %f58, %f67, %f70;
	add.s32 	%r28, %r3, %r12;
	add.s32 	%r29, %r28, %r12;
	mul.wide.s32 	%rd34, %r29, 8;
	add.s64 	%rd35, %rd26, %rd34;
	ld.global.nc.v2.f32 	{%f71, %f72}, [%rd35];
	fma.rn.f32 	%f75, %f57, %f71, 0f00000000;
	fma.rn.f32 	%f387, %f58, %f72, %f75;
	add.s64 	%rd36, %rd35, %rd32;
	ld.global.nc.v2.f32 	{%f76, %f77}, [%rd36];
	fma.rn.f32 	%f80, %f57, %f76, 0f00000000;
	fma.rn.f32 	%f386, %f58, %f77, %f80;
	st.local.v4.f32 	[%rd2], {%f389, %f388, %f387, %f386};
	add.s64 	%rd37, %rd36, %rd32;
	ld.global.nc.v2.f32 	{%f81, %f82}, [%rd37];
	fma.rn.f32 	%f85, %f57, %f81, 0f00000000;
	fma.rn.f32 	%f385, %f58, %f82, %f85;
	add.s64 	%rd38, %rd37, %rd32;
	ld.global.nc.v2.f32 	{%f86, %f87}, [%rd38];
	fma.rn.f32 	%f90, %f57, %f86, 0f00000000;
	fma.rn.f32 	%f384, %f58, %f87, %f90;
	add.s64 	%rd39, %rd38, %rd32;
	ld.global.nc.v2.f32 	{%f91, %f92}, [%rd39];
	fma.rn.f32 	%f95, %f57, %f91, 0f00000000;
	fma.rn.f32 	%f383, %f58, %f92, %f95;
	add.s64 	%rd40, %rd39, %rd32;
	ld.global.nc.v2.f32 	{%f96, %f97}, [%rd40];
	fma.rn.f32 	%f100, %f57, %f96, 0f00000000;
	fma.rn.f32 	%f382, %f58, %f97, %f100;
	st.local.v4.f32 	[%rd2+16], {%f385, %f384, %f383, %f382};
	add.s32 	%r322, %r3, 256;

$L__BB63_5:
	and.b32  	%r30, %r5, -256;
	setp.eq.s32 	%p4, %r30, 0;
	@%p4 bra 	$L__BB63_9;

	add.s32 	%r31, %r322, %r12;
	add.s32 	%r32, %r31, 256;
	mul.wide.s32 	%rd41, %r32, 8;
	shl.b64 	%rd42, %rd5, 2;
	add.s64 	%rd7, %rd41, %rd42;
	shl.b32 	%r33, %r12, 1;
	add.s32 	%r34, %r322, %r33;
	mad.lo.s32 	%r35, %r12, 3, %r322;
	shl.b32 	%r36, %r12, 2;
	add.s32 	%r37, %r322, %r36;
	mad.lo.s32 	%r38, %r12, 5, %r322;
	mad.lo.s32 	%r39, %r12, 6, %r322;
	mad.lo.s32 	%r40, %r12, 7, %r322;
	mul.wide.s32 	%rd43, %r34, 8;
	add.s64 	%rd8, %rd43, %rd42;
	mul.wide.s32 	%rd44, %r35, 8;
	add.s64 	%rd9, %rd44, %rd42;
	mul.wide.s32 	%rd45, %r37, 8;
	add.s64 	%rd10, %rd45, %rd42;
	mul.wide.s32 	%rd46, %r38, 8;
	add.s64 	%rd11, %rd46, %rd42;
	mul.wide.s32 	%rd47, %r39, 8;
	add.s64 	%rd12, %rd47, %rd42;
	mul.wide.s32 	%rd48, %r40, 8;
	add.s64 	%rd13, %rd48, %rd42;
	mul.wide.s32 	%rd49, %r322, 2;
	add.s64 	%rd50, %rd49, %rd3;
	shl.b64 	%rd51, %rd50, 2;
	add.s64 	%rd52, %rd4, %rd51;
	add.s64 	%rd73, %rd52, 2048;
	mul.wide.s32 	%rd53, %r322, 8;
	mul.wide.s32 	%rd54, %r12, 8;
	add.s64 	%rd55, %rd53, %rd54;
	add.s64 	%rd15, %rd55, %rd42;
	add.s64 	%rd16, %rd53, %rd42;

$L__BB63_7:
	ld.global.nc.v2.f32 	{%f101, %f102}, [%rd73+-2048];
	add.s64 	%rd56, %rd74, %rd16;
	ld.global.nc.v2.f32 	{%f105, %f106}, [%rd56];
	fma.rn.f32 	%f109, %f101, %f105, %f389;
	fma.rn.f32 	%f110, %f102, %f106, %f109;
	add.s64 	%rd57, %rd74, %rd15;
	ld.global.nc.v2.f32 	{%f111, %f112}, [%rd57];
	fma.rn.f32 	%f115, %f101, %f111, %f388;
	fma.rn.f32 	%f116, %f102, %f112, %f115;
	add.s64 	%rd58, %rd74, %rd8;
	ld.global.nc.v2.f32 	{%f117, %f118}, [%rd58];
	fma.rn.f32 	%f121, %f101, %f117, %f387;
	fma.rn.f32 	%f122, %f102, %f118, %f121;
	add.s64 	%rd59, %rd74, %rd9;
	ld.global.nc.v2.f32 	{%f123, %f124}, [%rd59];
	fma.rn.f32 	%f127, %f101, %f123, %f386;
	fma.rn.f32 	%f128, %f102, %f124, %f127;
	add.s64 	%rd60, %rd74, %rd10;
	ld.global.nc.v2.f32 	{%f129, %f130}, [%rd60];
	fma.rn.f32 	%f133, %f101, %f129, %f385;
	fma.rn.f32 	%f134, %f102, %f130, %f133;
	add.s64 	%rd61, %rd74, %rd11;
	ld.global.nc.v2.f32 	{%f135, %f136}, [%rd61];
	fma.rn.f32 	%f139, %f101, %f135, %f384;
	fma.rn.f32 	%f140, %f102, %f136, %f139;
	add.s64 	%rd62, %rd74, %rd12;
	ld.global.nc.v2.f32 	{%f141, %f142}, [%rd62];
	fma.rn.f32 	%f145, %f101, %f141, %f383;
	fma.rn.f32 	%f146, %f102, %f142, %f145;
	add.s64 	%rd63, %rd74, %rd13;
	ld.global.nc.v2.f32 	{%f147, %f148}, [%rd63];
	fma.rn.f32 	%f151, %f101, %f147, %f382;
	fma.rn.f32 	%f152, %f102, %f148, %f151;
	ld.global.nc.v2.f32 	{%f153, %f154}, [%rd73];
	ld.global.nc.v2.f32 	{%f157, %f158}, [%rd56+2048];
	fma.rn.f32 	%f161, %f153, %f157, %f110;
	fma.rn.f32 	%f389, %f154, %f158, %f161;
	add.s64 	%rd64, %rd74, %rd7;
	ld.global.nc.v2.f32 	{%f162, %f163}, [%rd64];
	fma.rn.f32 	%f166, %f153, %f162, %f116;
	fma.rn.f32 	%f388, %f154, %f163, %f166;
	ld.global.nc.v2.f32 	{%f167, %f168}, [%rd58+2048];
	fma.rn.f32 	%f171, %f153, %f167, %f122;
	fma.rn.f32 	%f387, %f154, %f168, %f171;
	ld.global.nc.v2.f32 	{%f172, %f173}, [%rd59+2048];
	fma.rn.f32 	%f176, %f153, %f172, %f128;
	fma.rn.f32 	%f386, %f154, %f173, %f176;
	ld.global.nc.v2.f32 	{%f177, %f178}, [%rd60+2048];
	fma.rn.f32 	%f181, %f153, %f177, %f134;
	fma.rn.f32 	%f385, %f154, %f178, %f181;
	ld.global.nc.v2.f32 	{%f182, %f183}, [%rd61+2048];
	fma.rn.f32 	%f186, %f153, %f182, %f140;
	fma.rn.f32 	%f384, %f154, %f183, %f186;
	ld.global.nc.v2.f32 	{%f187, %f188}, [%rd62+2048];
	fma.rn.f32 	%f191, %f153, %f187, %f146;
	fma.rn.f32 	%f383, %f154, %f188, %f191;
	ld.global.nc.v2.f32 	{%f192, %f193}, [%rd63+2048];
	fma.rn.f32 	%f196, %f153, %f192, %f152;
	fma.rn.f32 	%f382, %f154, %f193, %f196;
	add.s64 	%rd74, %rd74, 4096;
	add.s64 	%rd73, %rd73, 4096;
	add.s32 	%r322, %r322, 512;
	setp.lt.s32 	%p5, %r322, %r11;
	@%p5 bra 	$L__BB63_7;

	st.local.v4.f32 	[%rd2], {%f389, %f388, %f387, %f386};
	st.local.v4.f32 	[%rd2+16], {%f385, %f384, %f383, %f382};

$L__BB63_9:
	shr.s32 	%r41, %r3, 31;
	shr.u32 	%r42, %r41, 27;
	add.s32 	%r43, %r3, %r42;
	shr.s32 	%r44, %r43, 5;
	shl.b32 	%r45, %r44, 2;
	add.s32 	%r10, %r24, %r45;
	mov.u32 	%r47, 2;
	mov.b32 	%r48, %f389;
	mov.u32 	%r49, 31;
	mov.u32 	%r50, 16;
	mov.u32 	%r51, -1;
	shfl.sync.bfly.b32 	%r52|%p6, %r48, %r50, %r49, %r51;
	mov.b32 	%f197, %r52;
	add.f32 	%f198, %f389, %f197;
	mov.b32 	%r53, %f198;
	mov.u32 	%r54, 8;
	shfl.sync.bfly.b32 	%r55|%p7, %r53, %r54, %r49, %r51;
	mov.b32 	%f199, %r55;
	add.f32 	%f200, %f198, %f199;
	mov.b32 	%r56, %f200;
	mov.u32 	%r57, 4;
	shfl.sync.bfly.b32 	%r58|%p8, %r56, %r57, %r49, %r51;
	mov.b32 	%f201, %r58;
	add.f32 	%f202, %f200, %f201;
	mov.b32 	%r59, %f202;
	shfl.sync.bfly.b32 	%r60|%p9, %r59, %r47, %r49, %r51;
	mov.b32 	%f203, %r60;
	add.f32 	%f204, %f202, %f203;
	mov.b32 	%r61, %f204;
	mov.u32 	%r62, 1;
	shfl.sync.bfly.b32 	%r63|%p10, %r61, %r62, %r49, %r51;
	mov.b32 	%f205, %r63;
	add.f32 	%f206, %f204, %f205;
	st.local.f32 	[%rd2], %f206;
	st.shared.f32 	[%r10], %f206;
	bar.sync 	0;
	@%p1 bra 	$L__BB63_11;

	ld.shared.f32 	%f207, [%r4];
	mov.b32 	%r64, %f207;
	shfl.sync.bfly.b32 	%r68|%p12, %r64, %r50, %r49, %r51;
	mov.b32 	%f208, %r68;
	add.f32 	%f209, %f207, %f208;
	mov.b32 	%r69, %f209;
	shfl.sync.bfly.b32 	%r71|%p13, %r69, %r54, %r49, %r51;
	mov.b32 	%f210, %r71;
	add.f32 	%f211, %f209, %f210;
	mov.b32 	%r72, %f211;
	shfl.sync.bfly.b32 	%r74|%p14, %r72, %r57, %r49, %r51;
	mov.b32 	%f212, %r74;
	add.f32 	%f213, %f211, %f212;
	mov.b32 	%r75, %f213;
	shfl.sync.bfly.b32 	%r77|%p15, %r75, %r47, %r49, %r51;
	mov.b32 	%f214, %r77;
	add.f32 	%f215, %f213, %f214;
	mov.b32 	%r78, %f215;
	shfl.sync.bfly.b32 	%r80|%p16, %r78, %r62, %r49, %r51;
	mov.b32 	%f216, %r80;
	add.f32 	%f217, %f215, %f216;
	st.local.f32 	[%rd2], %f217;

$L__BB63_11:
	bar.sync 	0;
	mov.b32 	%r81, %f388;
	shfl.sync.bfly.b32 	%r85|%p18, %r81, %r50, %r49, %r51;
	mov.b32 	%f218, %r85;
	add.f32 	%f219, %f388, %f218;
	mov.b32 	%r86, %f219;
	shfl.sync.bfly.b32 	%r88|%p19, %r86, %r54, %r49, %r51;
	mov.b32 	%f220, %r88;
	add.f32 	%f221, %f219, %f220;
	mov.b32 	%r89, %f221;
	shfl.sync.bfly.b32 	%r91|%p20, %r89, %r57, %r49, %r51;
	mov.b32 	%f222, %r91;
	add.f32 	%f223, %f221, %f222;
	mov.b32 	%r92, %f223;
	shfl.sync.bfly.b32 	%r94|%p21, %r92, %r47, %r49, %r51;
	mov.b32 	%f224, %r94;
	add.f32 	%f225, %f223, %f224;
	mov.b32 	%r95, %f225;
	shfl.sync.bfly.b32 	%r97|%p22, %r95, %r62, %r49, %r51;
	mov.b32 	%f226, %r97;
	add.f32 	%f227, %f225, %f226;
	st.local.f32 	[%rd2+4], %f227;
	st.shared.f32 	[%r10], %f227;
	bar.sync 	0;
	@%p1 bra 	$L__BB63_13;

	ld.shared.f32 	%f228, [%r4];
	mov.b32 	%r98, %f228;
	mov.u32 	%r99, 31;
	mov.u32 	%r100, 16;
	mov.u32 	%r101, -1;
	shfl.sync.bfly.b32 	%r102|%p23, %r98, %r100, %r99, %r101;
	mov.b32 	%f229, %r102;
	add.f32 	%f230, %f228, %f229;
	mov.b32 	%r103, %f230;
	mov.u32 	%r104, 8;
	shfl.sync.bfly.b32 	%r105|%p24, %r103, %r104, %r99, %r101;
	mov.b32 	%f231, %r105;
	add.f32 	%f232, %f230, %f231;
	mov.b32 	%r106, %f232;
	mov.u32 	%r107, 4;
	shfl.sync.bfly.b32 	%r108|%p25, %r106, %r107, %r99, %r101;
	mov.b32 	%f233, %r108;
	add.f32 	%f234, %f232, %f233;
	mov.b32 	%r109, %f234;
	mov.u32 	%r110, 2;
	shfl.sync.bfly.b32 	%r111|%p26, %r109, %r110, %r99, %r101;
	mov.b32 	%f235, %r111;
	add.f32 	%f236, %f234, %f235;
	mov.b32 	%r112, %f236;
	mov.u32 	%r113, 1;
	shfl.sync.bfly.b32 	%r114|%p27, %r112, %r113, %r99, %r101;
	mov.b32 	%f237, %r114;
	add.f32 	%f238, %f236, %f237;
	st.local.f32 	[%rd2+4], %f238;

$L__BB63_13:
	bar.sync 	0;
	mov.b32 	%r115, %f387;
	mov.u32 	%r116, 31;
	mov.u32 	%r117, 16;
	mov.u32 	%r118, -1;
	shfl.sync.bfly.b32 	%r119|%p29, %r115, %r117, %r116, %r118;
	mov.b32 	%f239, %r119;
	add.f32 	%f240, %f387, %f239;
	mov.b32 	%r120, %f240;
	mov.u32 	%r121, 8;
	shfl.sync.bfly.b32 	%r122|%p30, %r120, %r121, %r116, %r118;
	mov.b32 	%f241, %r122;
	add.f32 	%f242, %f240, %f241;
	mov.b32 	%r123, %f242;
	mov.u32 	%r124, 4;
	shfl.sync.bfly.b32 	%r125|%p31, %r123, %r124, %r116, %r118;
	mov.b32 	%f243, %r125;
	add.f32 	%f244, %f242, %f243;
	mov.b32 	%r126, %f244;
	mov.u32 	%r127, 2;
	shfl.sync.bfly.b32 	%r128|%p32, %r126, %r127, %r116, %r118;
	mov.b32 	%f245, %r128;
	add.f32 	%f246, %f244, %f245;
	mov.b32 	%r129, %f246;
	mov.u32 	%r130, 1;
	shfl.sync.bfly.b32 	%r131|%p33, %r129, %r130, %r116, %r118;
	mov.b32 	%f247, %r131;
	add.f32 	%f248, %f246, %f247;
	st.local.f32 	[%rd2+8], %f248;
	st.shared.f32 	[%r10], %f248;
	bar.sync 	0;
	@%p1 bra 	$L__BB63_15;

	ld.shared.f32 	%f249, [%r4];
	mov.b32 	%r132, %f249;
	shfl.sync.bfly.b32 	%r136|%p34, %r132, %r117, %r116, %r118;
	mov.b32 	%f250, %r136;
	add.f32 	%f251, %f249, %f250;
	mov.b32 	%r137, %f251;
	shfl.sync.bfly.b32 	%r139|%p35, %r137, %r121, %r116, %r118;
	mov.b32 	%f252, %r139;
	add.f32 	%f253, %f251, %f252;
	mov.b32 	%r140, %f253;
	shfl.sync.bfly.b32 	%r142|%p36, %r140, %r124, %r116, %r118;
	mov.b32 	%f254, %r142;
	add.f32 	%f255, %f253, %f254;
	mov.b32 	%r143, %f255;
	shfl.sync.bfly.b32 	%r145|%p37, %r143, %r127, %r116, %r118;
	mov.b32 	%f256, %r145;
	add.f32 	%f257, %f255, %f256;
	mov.b32 	%r146, %f257;
	shfl.sync.bfly.b32 	%r148|%p38, %r146, %r130, %r116, %r118;
	mov.b32 	%f258, %r148;
	add.f32 	%f259, %f257, %f258;
	st.local.f32 	[%rd2+8], %f259;

$L__BB63_15:
	bar.sync 	0;
	mov.b32 	%r149, %f386;
	shfl.sync.bfly.b32 	%r153|%p40, %r149, %r117, %r116, %r118;
	mov.b32 	%f260, %r153;
	add.f32 	%f261, %f386, %f260;
	mov.b32 	%r154, %f261;
	shfl.sync.bfly.b32 	%r156|%p41, %r154, %r121, %r116, %r118;
	mov.b32 	%f262, %r156;
	add.f32 	%f263, %f261, %f262;
	mov.b32 	%r157, %f263;
	shfl.sync.bfly.b32 	%r159|%p42, %r157, %r124, %r116, %r118;
	mov.b32 	%f264, %r159;
	add.f32 	%f265, %f263, %f264;
	mov.b32 	%r160, %f265;
	shfl.sync.bfly.b32 	%r162|%p43, %r160, %r127, %r116, %r118;
	mov.b32 	%f266, %r162;
	add.f32 	%f267, %f265, %f266;
	mov.b32 	%r163, %f267;
	shfl.sync.bfly.b32 	%r165|%p44, %r163, %r130, %r116, %r118;
	mov.b32 	%f268, %r165;
	add.f32 	%f269, %f267, %f268;
	st.local.f32 	[%rd2+12], %f269;
	st.shared.f32 	[%r10], %f269;
	bar.sync 	0;
	@%p1 bra 	$L__BB63_17;

	ld.shared.f32 	%f270, [%r4];
	mov.b32 	%r166, %f270;
	mov.u32 	%r167, 31;
	mov.u32 	%r168, 16;
	mov.u32 	%r169, -1;
	shfl.sync.bfly.b32 	%r170|%p45, %r166, %r168, %r167, %r169;
	mov.b32 	%f271, %r170;
	add.f32 	%f272, %f270, %f271;
	mov.b32 	%r171, %f272;
	mov.u32 	%r172, 8;
	shfl.sync.bfly.b32 	%r173|%p46, %r171, %r172, %r167, %r169;
	mov.b32 	%f273, %r173;
	add.f32 	%f274, %f272, %f273;
	mov.b32 	%r174, %f274;
	mov.u32 	%r175, 4;
	shfl.sync.bfly.b32 	%r176|%p47, %r174, %r175, %r167, %r169;
	mov.b32 	%f275, %r176;
	add.f32 	%f276, %f274, %f275;
	mov.b32 	%r177, %f276;
	mov.u32 	%r178, 2;
	shfl.sync.bfly.b32 	%r179|%p48, %r177, %r178, %r167, %r169;
	mov.b32 	%f277, %r179;
	add.f32 	%f278, %f276, %f277;
	mov.b32 	%r180, %f278;
	mov.u32 	%r181, 1;
	shfl.sync.bfly.b32 	%r182|%p49, %r180, %r181, %r167, %r169;
	mov.b32 	%f279, %r182;
	add.f32 	%f280, %f278, %f279;
	st.local.f32 	[%rd2+12], %f280;

$L__BB63_17:
	bar.sync 	0;
	mov.b32 	%r183, %f385;
	mov.u32 	%r184, 31;
	mov.u32 	%r185, 16;
	mov.u32 	%r186, -1;
	shfl.sync.bfly.b32 	%r187|%p51, %r183, %r185, %r184, %r186;
	mov.b32 	%f281, %r187;
	add.f32 	%f282, %f385, %f281;
	mov.b32 	%r188, %f282;
	mov.u32 	%r189, 8;
	shfl.sync.bfly.b32 	%r190|%p52, %r188, %r189, %r184, %r186;
	mov.b32 	%f283, %r190;
	add.f32 	%f284, %f282, %f283;
	mov.b32 	%r191, %f284;
	mov.u32 	%r192, 4;
	shfl.sync.bfly.b32 	%r193|%p53, %r191, %r192, %r184, %r186;
	mov.b32 	%f285, %r193;
	add.f32 	%f286, %f284, %f285;
	mov.b32 	%r194, %f286;
	mov.u32 	%r195, 2;
	shfl.sync.bfly.b32 	%r196|%p54, %r194, %r195, %r184, %r186;
	mov.b32 	%f287, %r196;
	add.f32 	%f288, %f286, %f287;
	mov.b32 	%r197, %f288;
	mov.u32 	%r198, 1;
	shfl.sync.bfly.b32 	%r199|%p55, %r197, %r198, %r184, %r186;
	mov.b32 	%f289, %r199;
	add.f32 	%f290, %f288, %f289;
	st.local.f32 	[%rd2+16], %f290;
	st.shared.f32 	[%r10], %f290;
	bar.sync 	0;
	@%p1 bra 	$L__BB63_19;

	ld.shared.f32 	%f291, [%r4];
	mov.b32 	%r200, %f291;
	shfl.sync.bfly.b32 	%r204|%p56, %r200, %r185, %r184, %r186;
	mov.b32 	%f292, %r204;
	add.f32 	%f293, %f291, %f292;
	mov.b32 	%r205, %f293;
	shfl.sync.bfly.b32 	%r207|%p57, %r205, %r189, %r184, %r186;
	mov.b32 	%f294, %r207;
	add.f32 	%f295, %f293, %f294;
	mov.b32 	%r208, %f295;
	shfl.sync.bfly.b32 	%r210|%p58, %r208, %r192, %r184, %r186;
	mov.b32 	%f296, %r210;
	add.f32 	%f297, %f295, %f296;
	mov.b32 	%r211, %f297;
	shfl.sync.bfly.b32 	%r213|%p59, %r211, %r195, %r184, %r186;
	mov.b32 	%f298, %r213;
	add.f32 	%f299, %f297, %f298;
	mov.b32 	%r214, %f299;
	shfl.sync.bfly.b32 	%r216|%p60, %r214, %r198, %r184, %r186;
	mov.b32 	%f300, %r216;
	add.f32 	%f301, %f299, %f300;
	st.local.f32 	[%rd2+16], %f301;

$L__BB63_19:
	bar.sync 	0;
	mov.b32 	%r217, %f384;
	shfl.sync.bfly.b32 	%r221|%p62, %r217, %r185, %r184, %r186;
	mov.b32 	%f302, %r221;
	add.f32 	%f303, %f384, %f302;
	mov.b32 	%r222, %f303;
	shfl.sync.bfly.b32 	%r224|%p63, %r222, %r189, %r184, %r186;
	mov.b32 	%f304, %r224;
	add.f32 	%f305, %f303, %f304;
	mov.b32 	%r225, %f305;
	shfl.sync.bfly.b32 	%r227|%p64, %r225, %r192, %r184, %r186;
	mov.b32 	%f306, %r227;
	add.f32 	%f307, %f305, %f306;
	mov.b32 	%r228, %f307;
	shfl.sync.bfly.b32 	%r230|%p65, %r228, %r195, %r184, %r186;
	mov.b32 	%f308, %r230;
	add.f32 	%f309, %f307, %f308;
	mov.b32 	%r231, %f309;
	shfl.sync.bfly.b32 	%r233|%p66, %r231, %r198, %r184, %r186;
	mov.b32 	%f310, %r233;
	add.f32 	%f311, %f309, %f310;
	st.local.f32 	[%rd2+20], %f311;
	st.shared.f32 	[%r10], %f311;
	bar.sync 	0;
	@%p1 bra 	$L__BB63_21;

	ld.shared.f32 	%f312, [%r4];
	mov.b32 	%r234, %f312;
	mov.u32 	%r235, 31;
	mov.u32 	%r236, 16;
	mov.u32 	%r237, -1;
	shfl.sync.bfly.b32 	%r238|%p67, %r234, %r236, %r235, %r237;
	mov.b32 	%f313, %r238;
	add.f32 	%f314, %f312, %f313;
	mov.b32 	%r239, %f314;
	mov.u32 	%r240, 8;
	shfl.sync.bfly.b32 	%r241|%p68, %r239, %r240, %r235, %r237;
	mov.b32 	%f315, %r241;
	add.f32 	%f316, %f314, %f315;
	mov.b32 	%r242, %f316;
	mov.u32 	%r243, 4;
	shfl.sync.bfly.b32 	%r244|%p69, %r242, %r243, %r235, %r237;
	mov.b32 	%f317, %r244;
	add.f32 	%f318, %f316, %f317;
	mov.b32 	%r245, %f318;
	mov.u32 	%r246, 2;
	shfl.sync.bfly.b32 	%r247|%p70, %r245, %r246, %r235, %r237;
	mov.b32 	%f319, %r247;
	add.f32 	%f320, %f318, %f319;
	mov.b32 	%r248, %f320;
	mov.u32 	%r249, 1;
	shfl.sync.bfly.b32 	%r250|%p71, %r248, %r249, %r235, %r237;
	mov.b32 	%f321, %r250;
	add.f32 	%f322, %f320, %f321;
	st.local.f32 	[%rd2+20], %f322;

$L__BB63_21:
	bar.sync 	0;
	mov.b32 	%r251, %f383;
	mov.u32 	%r252, 31;
	mov.u32 	%r253, 16;
	mov.u32 	%r254, -1;
	shfl.sync.bfly.b32 	%r255|%p73, %r251, %r253, %r252, %r254;
	mov.b32 	%f323, %r255;
	add.f32 	%f324, %f383, %f323;
	mov.b32 	%r256, %f324;
	mov.u32 	%r257, 8;
	shfl.sync.bfly.b32 	%r258|%p74, %r256, %r257, %r252, %r254;
	mov.b32 	%f325, %r258;
	add.f32 	%f326, %f324, %f325;
	mov.b32 	%r259, %f326;
	mov.u32 	%r260, 4;
	shfl.sync.bfly.b32 	%r261|%p75, %r259, %r260, %r252, %r254;
	mov.b32 	%f327, %r261;
	add.f32 	%f328, %f326, %f327;
	mov.b32 	%r262, %f328;
	mov.u32 	%r263, 2;
	shfl.sync.bfly.b32 	%r264|%p76, %r262, %r263, %r252, %r254;
	mov.b32 	%f329, %r264;
	add.f32 	%f330, %f328, %f329;
	mov.b32 	%r265, %f330;
	mov.u32 	%r266, 1;
	shfl.sync.bfly.b32 	%r267|%p77, %r265, %r266, %r252, %r254;
	mov.b32 	%f331, %r267;
	add.f32 	%f332, %f330, %f331;
	st.local.f32 	[%rd2+24], %f332;
	st.shared.f32 	[%r10], %f332;
	bar.sync 	0;
	@%p1 bra 	$L__BB63_23;

	ld.shared.f32 	%f333, [%r4];
	mov.b32 	%r268, %f333;
	shfl.sync.bfly.b32 	%r272|%p78, %r268, %r253, %r252, %r254;
	mov.b32 	%f334, %r272;
	add.f32 	%f335, %f333, %f334;
	mov.b32 	%r273, %f335;
	shfl.sync.bfly.b32 	%r275|%p79, %r273, %r257, %r252, %r254;
	mov.b32 	%f336, %r275;
	add.f32 	%f337, %f335, %f336;
	mov.b32 	%r276, %f337;
	shfl.sync.bfly.b32 	%r278|%p80, %r276, %r260, %r252, %r254;
	mov.b32 	%f338, %r278;
	add.f32 	%f339, %f337, %f338;
	mov.b32 	%r279, %f339;
	shfl.sync.bfly.b32 	%r281|%p81, %r279, %r263, %r252, %r254;
	mov.b32 	%f340, %r281;
	add.f32 	%f341, %f339, %f340;
	mov.b32 	%r282, %f341;
	shfl.sync.bfly.b32 	%r284|%p82, %r282, %r266, %r252, %r254;
	mov.b32 	%f342, %r284;
	add.f32 	%f343, %f341, %f342;
	st.local.f32 	[%rd2+24], %f343;

$L__BB63_23:
	bar.sync 	0;
	mov.b32 	%r285, %f382;
	shfl.sync.bfly.b32 	%r289|%p84, %r285, %r253, %r252, %r254;
	mov.b32 	%f344, %r289;
	add.f32 	%f345, %f382, %f344;
	mov.b32 	%r290, %f345;
	shfl.sync.bfly.b32 	%r292|%p85, %r290, %r257, %r252, %r254;
	mov.b32 	%f346, %r292;
	add.f32 	%f347, %f345, %f346;
	mov.b32 	%r293, %f347;
	shfl.sync.bfly.b32 	%r295|%p86, %r293, %r260, %r252, %r254;
	mov.b32 	%f348, %r295;
	add.f32 	%f349, %f347, %f348;
	mov.b32 	%r296, %f349;
	shfl.sync.bfly.b32 	%r298|%p87, %r296, %r263, %r252, %r254;
	mov.b32 	%f350, %r298;
	add.f32 	%f351, %f349, %f350;
	mov.b32 	%r299, %f351;
	shfl.sync.bfly.b32 	%r301|%p88, %r299, %r266, %r252, %r254;
	mov.b32 	%f352, %r301;
	add.f32 	%f353, %f351, %f352;
	st.local.f32 	[%rd2+28], %f353;
	st.shared.f32 	[%r10], %f353;
	bar.sync 	0;
	@%p1 bra 	$L__BB63_25;

	ld.shared.f32 	%f354, [%r4];
	mov.b32 	%r302, %f354;
	mov.u32 	%r303, 31;
	mov.u32 	%r304, 16;
	mov.u32 	%r305, -1;
	shfl.sync.bfly.b32 	%r306|%p89, %r302, %r304, %r303, %r305;
	mov.b32 	%f355, %r306;
	add.f32 	%f356, %f354, %f355;
	mov.b32 	%r307, %f356;
	mov.u32 	%r308, 8;
	shfl.sync.bfly.b32 	%r309|%p90, %r307, %r308, %r303, %r305;
	mov.b32 	%f357, %r309;
	add.f32 	%f358, %f356, %f357;
	mov.b32 	%r310, %f358;
	mov.u32 	%r311, 4;
	shfl.sync.bfly.b32 	%r312|%p91, %r310, %r311, %r303, %r305;
	mov.b32 	%f359, %r312;
	add.f32 	%f360, %f358, %f359;
	mov.b32 	%r313, %f360;
	mov.u32 	%r314, 2;
	shfl.sync.bfly.b32 	%r315|%p92, %r313, %r314, %r303, %r305;
	mov.b32 	%f361, %r315;
	add.f32 	%f362, %f360, %f361;
	mov.b32 	%r316, %f362;
	mov.u32 	%r317, 1;
	shfl.sync.bfly.b32 	%r318|%p93, %r316, %r317, %r303, %r305;
	mov.b32 	%f363, %r318;
	add.f32 	%f364, %f362, %f363;
	st.local.f32 	[%rd2+28], %f364;

$L__BB63_25:
	bar.sync 	0;
	setp.gt.s32 	%p94, %r3, 7;
	@%p94 bra 	$L__BB63_27;

	mul.wide.s32 	%rd65, %r3, 4;
	add.s64 	%rd66, %rd2, %rd65;
	ld.local.f32 	%f365, [%rd66];
	mad.lo.s32 	%r319, %r3, %r13, %r2;
	cvt.s64.s32 	%rd67, %r319;
	mul.lo.s32 	%r320, %r1, %r14;
	cvt.s64.s32 	%rd68, %r320;
	add.s64 	%rd69, %rd68, %rd67;
	cvta.to.global.u64 	%rd70, %rd21;
	shl.b64 	%rd71, %rd69, 2;
	add.s64 	%rd72, %rd70, %rd71;
	st.global.f32 	[%rd72], %f365;

$L__BB63_27:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_1_bs_32
.visible .entry ggml_matvec_f16_ncols_1_bs_32(
	.param .u64 ggml_matvec_f16_ncols_1_bs_32_param_0,
	.param .u64 ggml_matvec_f16_ncols_1_bs_32_param_1,
	.param .u64 ggml_matvec_f16_ncols_1_bs_32_param_2,
	.param .u32 ggml_matvec_f16_ncols_1_bs_32_param_3,
	.param .u32 ggml_matvec_f16_ncols_1_bs_32_param_4,
	.param .u32 ggml_matvec_f16_ncols_1_bs_32_param_5,
	.param .u32 ggml_matvec_f16_ncols_1_bs_32_param_6,
	.param .u32 ggml_matvec_f16_ncols_1_bs_32_param_7,
	.param .u32 ggml_matvec_f16_ncols_1_bs_32_param_8,
	.param .u32 ggml_matvec_f16_ncols_1_bs_32_param_9,
	.param .u32 ggml_matvec_f16_ncols_1_bs_32_param_10,
	.param .u32 ggml_matvec_f16_ncols_1_bs_32_param_11
)
{
	.reg .pred 	%p<12>;
	.reg .b16 	%rs<2>;
	.reg .f32 	%f<56>;
	.reg .b32 	%r<71>;
	.reg .b64 	%rd<42>;


	ld.param.u64 	%rd18, [ggml_matvec_f16_ncols_1_bs_32_param_0];
	ld.param.u64 	%rd19, [ggml_matvec_f16_ncols_1_bs_32_param_1];
	ld.param.u64 	%rd17, [ggml_matvec_f16_ncols_1_bs_32_param_2];
	ld.param.u32 	%r12, [ggml_matvec_f16_ncols_1_bs_32_param_3];
	ld.param.u32 	%r15, [ggml_matvec_f16_ncols_1_bs_32_param_5];
	ld.param.u32 	%r13, [ggml_matvec_f16_ncols_1_bs_32_param_7];
	ld.param.u32 	%r16, [ggml_matvec_f16_ncols_1_bs_32_param_8];
	ld.param.u32 	%r17, [ggml_matvec_f16_ncols_1_bs_32_param_9];
	ld.param.u32 	%r18, [ggml_matvec_f16_ncols_1_bs_32_param_10];
	ld.param.u32 	%r14, [ggml_matvec_f16_ncols_1_bs_32_param_11];
	cvta.to.global.u64 	%rd1, %rd19;
	cvta.to.global.u64 	%rd2, %rd18;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r19, %r1, %r16;
	mov.u32 	%r20, %ctaid.x;
	mul.lo.s32 	%r21, %r20, %r15;
	mad.lo.s32 	%r22, %r19, %r17, %r21;
	cvt.s64.s32 	%rd3, %r22;
	mul.lo.s32 	%r23, %r1, %r18;
	cvt.s64.s32 	%rd4, %r23;
	mov.u32 	%r2, %tid.x;
	setp.ge.s32 	%p1, %r2, %r12;
	mov.f32 	%f55, 0f00000000;
	@%p1 bra 	$L__BB64_7;

	not.b32 	%r24, %r2;
	add.s32 	%r3, %r24, %r12;
	shr.u32 	%r25, %r3, 5;
	add.s32 	%r26, %r25, 1;
	and.b32  	%r68, %r26, 3;
	setp.eq.s32 	%p2, %r68, 0;
	mov.f32 	%f55, 0f00000000;
	mov.u32 	%r69, %r2;
	@%p2 bra 	$L__BB64_4;

	mul.wide.s32 	%rd20, %r2, 2;
	add.s64 	%rd21, %rd20, %rd4;
	shl.b64 	%rd22, %rd21, 1;
	add.s64 	%rd39, %rd1, %rd22;
	add.s64 	%rd23, %rd20, %rd3;
	shl.b64 	%rd24, %rd23, 1;
	add.s64 	%rd38, %rd2, %rd24;
	mov.f32 	%f55, 0f00000000;
	mov.u32 	%r69, %r2;

$L__BB64_3:
	.pragma "nounroll";
	ld.global.nc.u32 	%r27, [%rd38];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r27;
  cvt.f32.f16 %f13, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r27;
  cvt.f32.f16 %f14, high;}

	// end inline asm
	ld.global.nc.u32 	%r29, [%rd39];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r29;
  cvt.f32.f16 %f15, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r29;
  cvt.f32.f16 %f16, high;}

	// end inline asm
	fma.rn.f32 	%f17, %f13, %f15, %f55;
	fma.rn.f32 	%f55, %f14, %f16, %f17;
	add.s32 	%r69, %r69, 32;
	add.s64 	%rd39, %rd39, 128;
	add.s64 	%rd38, %rd38, 128;
	add.s32 	%r68, %r68, -1;
	setp.ne.s32 	%p3, %r68, 0;
	@%p3 bra 	$L__BB64_3;

$L__BB64_4:
	setp.lt.u32 	%p4, %r3, 96;
	@%p4 bra 	$L__BB64_7;

	mul.wide.s32 	%rd25, %r69, 2;
	add.s64 	%rd26, %rd25, %rd3;
	shl.b64 	%rd27, %rd26, 1;
	add.s64 	%rd28, %rd2, %rd27;
	add.s64 	%rd41, %rd28, 256;
	add.s64 	%rd29, %rd25, %rd4;
	shl.b64 	%rd30, %rd29, 1;
	add.s64 	%rd31, %rd1, %rd30;
	add.s64 	%rd40, %rd31, 256;

$L__BB64_6:
	ld.global.nc.u32 	%r31, [%rd41+-256];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r31;
  cvt.f32.f16 %f18, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r31;
  cvt.f32.f16 %f19, high;}

	// end inline asm
	ld.global.nc.u32 	%r33, [%rd40+-256];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r33;
  cvt.f32.f16 %f20, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r33;
  cvt.f32.f16 %f21, high;}

	// end inline asm
	fma.rn.f32 	%f34, %f18, %f20, %f55;
	fma.rn.f32 	%f35, %f19, %f21, %f34;
	ld.global.nc.u32 	%r35, [%rd41+-128];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r35;
  cvt.f32.f16 %f22, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r35;
  cvt.f32.f16 %f23, high;}

	// end inline asm
	ld.global.nc.u32 	%r37, [%rd40+-128];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r37;
  cvt.f32.f16 %f24, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r37;
  cvt.f32.f16 %f25, high;}

	// end inline asm
	fma.rn.f32 	%f36, %f22, %f24, %f35;
	fma.rn.f32 	%f37, %f23, %f25, %f36;
	ld.global.nc.u32 	%r39, [%rd41];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r39;
  cvt.f32.f16 %f26, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r39;
  cvt.f32.f16 %f27, high;}

	// end inline asm
	ld.global.nc.u32 	%r41, [%rd40];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r41;
  cvt.f32.f16 %f28, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r41;
  cvt.f32.f16 %f29, high;}

	// end inline asm
	fma.rn.f32 	%f38, %f26, %f28, %f37;
	fma.rn.f32 	%f39, %f27, %f29, %f38;
	ld.global.nc.u32 	%r43, [%rd41+128];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r43;
  cvt.f32.f16 %f30, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r43;
  cvt.f32.f16 %f31, high;}

	// end inline asm
	ld.global.nc.u32 	%r45, [%rd40+128];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r45;
  cvt.f32.f16 %f32, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r45;
  cvt.f32.f16 %f33, high;}

	// end inline asm
	fma.rn.f32 	%f40, %f30, %f32, %f39;
	fma.rn.f32 	%f55, %f31, %f33, %f40;
	add.s64 	%rd41, %rd41, 512;
	add.s64 	%rd40, %rd40, 512;
	add.s32 	%r69, %r69, 128;
	setp.lt.s32 	%p5, %r69, %r12;
	@%p5 bra 	$L__BB64_6;

$L__BB64_7:
	mov.b32 	%r47, %f55;
	mov.u32 	%r48, 31;
	mov.u32 	%r49, 16;
	mov.u32 	%r50, -1;
	shfl.sync.bfly.b32 	%r51|%p6, %r47, %r49, %r48, %r50;
	mov.b32 	%f41, %r51;
	add.f32 	%f42, %f55, %f41;
	mov.b32 	%r52, %f42;
	mov.u32 	%r53, 8;
	shfl.sync.bfly.b32 	%r54|%p7, %r52, %r53, %r48, %r50;
	mov.b32 	%f43, %r54;
	add.f32 	%f44, %f42, %f43;
	mov.b32 	%r55, %f44;
	mov.u32 	%r56, 4;
	shfl.sync.bfly.b32 	%r57|%p8, %r55, %r56, %r48, %r50;
	mov.b32 	%f45, %r57;
	add.f32 	%f46, %f44, %f45;
	mov.b32 	%r58, %f46;
	mov.u32 	%r59, 2;
	shfl.sync.bfly.b32 	%r60|%p9, %r58, %r59, %r48, %r50;
	mov.b32 	%f47, %r60;
	add.f32 	%f48, %f46, %f47;
	mov.b32 	%r61, %f48;
	mov.u32 	%r62, 1;
	shfl.sync.bfly.b32 	%r63|%p10, %r61, %r62, %r48, %r50;
	mov.b32 	%f49, %r63;
	add.f32 	%f8, %f48, %f49;
	setp.gt.s32 	%p11, %r2, 0;
	@%p11 bra 	$L__BB64_9;

	mad.lo.s32 	%r65, %r2, %r13, %r20;
	cvt.s64.s32 	%rd32, %r65;
	mul.lo.s32 	%r66, %r1, %r14;
	cvt.s64.s32 	%rd33, %r66;
	add.s64 	%rd34, %rd33, %rd32;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f8;}

	// end inline asm
	cvta.to.global.u64 	%rd35, %rd17;
	shl.b64 	%rd36, %rd34, 1;
	add.s64 	%rd37, %rd35, %rd36;
	st.global.u16 	[%rd37], %rs1;

$L__BB64_9:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_2_bs_32
.visible .entry ggml_matvec_f16_ncols_2_bs_32(
	.param .u64 ggml_matvec_f16_ncols_2_bs_32_param_0,
	.param .u64 ggml_matvec_f16_ncols_2_bs_32_param_1,
	.param .u64 ggml_matvec_f16_ncols_2_bs_32_param_2,
	.param .u32 ggml_matvec_f16_ncols_2_bs_32_param_3,
	.param .u32 ggml_matvec_f16_ncols_2_bs_32_param_4,
	.param .u32 ggml_matvec_f16_ncols_2_bs_32_param_5,
	.param .u32 ggml_matvec_f16_ncols_2_bs_32_param_6,
	.param .u32 ggml_matvec_f16_ncols_2_bs_32_param_7,
	.param .u32 ggml_matvec_f16_ncols_2_bs_32_param_8,
	.param .u32 ggml_matvec_f16_ncols_2_bs_32_param_9,
	.param .u32 ggml_matvec_f16_ncols_2_bs_32_param_10,
	.param .u32 ggml_matvec_f16_ncols_2_bs_32_param_11
)
{
	.local .align 8 .b8 	__local_depot65[8];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<17>;
	.reg .b16 	%rs<2>;
	.reg .f32 	%f<94>;
	.reg .b32 	%r<91>;
	.reg .b64 	%rd<63>;


	mov.u64 	%SPL, __local_depot65;
	ld.param.u64 	%rd26, [ggml_matvec_f16_ncols_2_bs_32_param_0];
	ld.param.u64 	%rd27, [ggml_matvec_f16_ncols_2_bs_32_param_1];
	ld.param.u64 	%rd25, [ggml_matvec_f16_ncols_2_bs_32_param_2];
	ld.param.u32 	%r13, [ggml_matvec_f16_ncols_2_bs_32_param_3];
	ld.param.u32 	%r17, [ggml_matvec_f16_ncols_2_bs_32_param_5];
	ld.param.u32 	%r14, [ggml_matvec_f16_ncols_2_bs_32_param_6];
	ld.param.u32 	%r15, [ggml_matvec_f16_ncols_2_bs_32_param_7];
	ld.param.u32 	%r18, [ggml_matvec_f16_ncols_2_bs_32_param_8];
	ld.param.u32 	%r19, [ggml_matvec_f16_ncols_2_bs_32_param_9];
	ld.param.u32 	%r20, [ggml_matvec_f16_ncols_2_bs_32_param_10];
	ld.param.u32 	%r16, [ggml_matvec_f16_ncols_2_bs_32_param_11];
	cvta.to.global.u64 	%rd1, %rd27;
	cvta.to.global.u64 	%rd2, %rd26;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r21, %r1, %r18;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r22, %r2, %r17;
	mad.lo.s32 	%r23, %r21, %r19, %r22;
	cvt.s64.s32 	%rd4, %r23;
	mul.lo.s32 	%r24, %r1, %r20;
	cvt.s64.s32 	%rd5, %r24;
	mov.f32 	%f92, 0f00000000;
	st.local.v2.f32 	[%rd3], {%f92, %f92};
	mov.u32 	%r3, %tid.x;
	setp.ge.s32 	%p1, %r3, %r13;
	mov.f32 	%f93, %f92;
	@%p1 bra 	$L__BB65_9;

	not.b32 	%r25, %r3;
	add.s32 	%r4, %r25, %r13;
	shr.u32 	%r26, %r4, 5;
	add.s32 	%r27, %r26, 1;
	and.b32  	%r88, %r27, 3;
	setp.eq.s32 	%p2, %r88, 0;
	mov.f32 	%f92, 0f00000000;
	mov.u32 	%r89, %r3;
	@%p2 bra 	$L__BB65_5;

	mul.wide.s32 	%rd29, %r14, 2;
	mul.wide.s32 	%rd30, %r3, 2;
	add.s64 	%rd31, %rd29, %rd30;
	add.s64 	%rd32, %rd31, %rd5;
	shl.b64 	%rd33, %rd32, 1;
	add.s64 	%rd59, %rd1, %rd33;
	add.s64 	%rd34, %rd30, %rd5;
	shl.b64 	%rd35, %rd34, 1;
	add.s64 	%rd58, %rd1, %rd35;
	add.s64 	%rd36, %rd30, %rd4;
	shl.b64 	%rd37, %rd36, 1;
	add.s64 	%rd57, %rd2, %rd37;
	mov.f32 	%f92, 0f00000000;
	mov.f32 	%f93, %f92;
	mov.u32 	%r89, %r3;

$L__BB65_3:
	.pragma "nounroll";
	ld.global.nc.u32 	%r28, [%rd57];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r28;
  cvt.f32.f16 %f19, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r28;
  cvt.f32.f16 %f20, high;}

	// end inline asm
	ld.global.nc.u32 	%r30, [%rd58];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r30;
  cvt.f32.f16 %f21, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r30;
  cvt.f32.f16 %f22, high;}

	// end inline asm
	fma.rn.f32 	%f25, %f19, %f21, %f93;
	fma.rn.f32 	%f93, %f20, %f22, %f25;
	ld.global.nc.u32 	%r32, [%rd59];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r32;
  cvt.f32.f16 %f23, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r32;
  cvt.f32.f16 %f24, high;}

	// end inline asm
	fma.rn.f32 	%f26, %f19, %f23, %f92;
	fma.rn.f32 	%f92, %f20, %f24, %f26;
	add.s32 	%r89, %r89, 32;
	add.s64 	%rd59, %rd59, 128;
	add.s64 	%rd58, %rd58, 128;
	add.s64 	%rd57, %rd57, 128;
	add.s32 	%r88, %r88, -1;
	setp.ne.s32 	%p3, %r88, 0;
	@%p3 bra 	$L__BB65_3;

	st.local.v2.f32 	[%rd3], {%f93, %f92};

$L__BB65_5:
	setp.lt.u32 	%p4, %r4, 96;
	@%p4 bra 	$L__BB65_9;

	mul.wide.s32 	%rd38, %r89, 2;
	add.s64 	%rd39, %rd38, %rd4;
	shl.b64 	%rd40, %rd39, 1;
	add.s64 	%rd41, %rd2, %rd40;
	add.s64 	%rd62, %rd41, 256;
	add.s64 	%rd42, %rd38, %rd5;
	shl.b64 	%rd43, %rd42, 1;
	add.s64 	%rd44, %rd1, %rd43;
	add.s64 	%rd61, %rd44, 384;
	mul.wide.s32 	%rd45, %r14, 2;
	add.s64 	%rd46, %rd42, %rd45;
	shl.b64 	%rd47, %rd46, 1;
	add.s64 	%rd48, %rd1, %rd47;
	add.s64 	%rd60, %rd48, 256;

$L__BB65_7:
	ld.global.nc.u32 	%r34, [%rd62+-256];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r34;
  cvt.f32.f16 %f27, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r34;
  cvt.f32.f16 %f28, high;}

	// end inline asm
	ld.global.nc.u32 	%r36, [%rd61+-384];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r36;
  cvt.f32.f16 %f29, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r36;
  cvt.f32.f16 %f30, high;}

	// end inline asm
	fma.rn.f32 	%f51, %f27, %f29, %f93;
	fma.rn.f32 	%f52, %f28, %f30, %f51;
	ld.global.nc.u32 	%r38, [%rd60+-256];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r38;
  cvt.f32.f16 %f31, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r38;
  cvt.f32.f16 %f32, high;}

	// end inline asm
	fma.rn.f32 	%f53, %f27, %f31, %f92;
	fma.rn.f32 	%f54, %f28, %f32, %f53;
	ld.global.nc.u32 	%r40, [%rd62+-128];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r40;
  cvt.f32.f16 %f33, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r40;
  cvt.f32.f16 %f34, high;}

	// end inline asm
	ld.global.nc.u32 	%r42, [%rd61+-256];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r42;
  cvt.f32.f16 %f35, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r42;
  cvt.f32.f16 %f36, high;}

	// end inline asm
	fma.rn.f32 	%f55, %f33, %f35, %f52;
	fma.rn.f32 	%f56, %f34, %f36, %f55;
	ld.global.nc.u32 	%r44, [%rd60+-128];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r44;
  cvt.f32.f16 %f37, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r44;
  cvt.f32.f16 %f38, high;}

	// end inline asm
	fma.rn.f32 	%f57, %f33, %f37, %f54;
	fma.rn.f32 	%f58, %f34, %f38, %f57;
	ld.global.nc.u32 	%r46, [%rd62];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r46;
  cvt.f32.f16 %f39, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r46;
  cvt.f32.f16 %f40, high;}

	// end inline asm
	ld.global.nc.u32 	%r48, [%rd61+-128];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r48;
  cvt.f32.f16 %f41, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r48;
  cvt.f32.f16 %f42, high;}

	// end inline asm
	fma.rn.f32 	%f59, %f39, %f41, %f56;
	fma.rn.f32 	%f60, %f40, %f42, %f59;
	ld.global.nc.u32 	%r50, [%rd60];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r50;
  cvt.f32.f16 %f43, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r50;
  cvt.f32.f16 %f44, high;}

	// end inline asm
	fma.rn.f32 	%f61, %f39, %f43, %f58;
	fma.rn.f32 	%f62, %f40, %f44, %f61;
	ld.global.nc.u32 	%r52, [%rd62+128];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r52;
  cvt.f32.f16 %f45, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r52;
  cvt.f32.f16 %f46, high;}

	// end inline asm
	ld.global.nc.u32 	%r54, [%rd61];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r54;
  cvt.f32.f16 %f47, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r54;
  cvt.f32.f16 %f48, high;}

	// end inline asm
	fma.rn.f32 	%f63, %f45, %f47, %f60;
	fma.rn.f32 	%f93, %f46, %f48, %f63;
	ld.global.nc.u32 	%r56, [%rd60+128];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r56;
  cvt.f32.f16 %f49, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r56;
  cvt.f32.f16 %f50, high;}

	// end inline asm
	fma.rn.f32 	%f64, %f45, %f49, %f62;
	fma.rn.f32 	%f92, %f46, %f50, %f64;
	add.s64 	%rd62, %rd62, 512;
	add.s64 	%rd61, %rd61, 512;
	add.s64 	%rd60, %rd60, 512;
	add.s32 	%r89, %r89, 128;
	setp.lt.s32 	%p5, %r89, %r13;
	@%p5 bra 	$L__BB65_7;

	st.local.v2.f32 	[%rd3], {%f93, %f92};

$L__BB65_9:
	mov.b32 	%r58, %f93;
	mov.u32 	%r59, 31;
	mov.u32 	%r60, 16;
	mov.u32 	%r61, -1;
	shfl.sync.bfly.b32 	%r62|%p6, %r58, %r60, %r59, %r61;
	mov.b32 	%f65, %r62;
	add.f32 	%f66, %f93, %f65;
	mov.b32 	%r63, %f66;
	mov.u32 	%r64, 8;
	shfl.sync.bfly.b32 	%r65|%p7, %r63, %r64, %r59, %r61;
	mov.b32 	%f67, %r65;
	add.f32 	%f68, %f66, %f67;
	mov.b32 	%r66, %f68;
	mov.u32 	%r67, 4;
	shfl.sync.bfly.b32 	%r68|%p8, %r66, %r67, %r59, %r61;
	mov.b32 	%f69, %r68;
	add.f32 	%f70, %f68, %f69;
	mov.b32 	%r69, %f70;
	mov.u32 	%r70, 2;
	shfl.sync.bfly.b32 	%r71|%p9, %r69, %r70, %r59, %r61;
	mov.b32 	%f71, %r71;
	add.f32 	%f72, %f70, %f71;
	mov.b32 	%r72, %f72;
	mov.u32 	%r73, 1;
	shfl.sync.bfly.b32 	%r74|%p10, %r72, %r73, %r59, %r61;
	mov.b32 	%f73, %r74;
	add.f32 	%f74, %f72, %f73;
	st.local.f32 	[%rd3], %f74;
	mov.b32 	%r75, %f92;
	shfl.sync.bfly.b32 	%r76|%p11, %r75, %r60, %r59, %r61;
	mov.b32 	%f75, %r76;
	add.f32 	%f76, %f92, %f75;
	mov.b32 	%r77, %f76;
	shfl.sync.bfly.b32 	%r78|%p12, %r77, %r64, %r59, %r61;
	mov.b32 	%f77, %r78;
	add.f32 	%f78, %f76, %f77;
	mov.b32 	%r79, %f78;
	shfl.sync.bfly.b32 	%r80|%p13, %r79, %r67, %r59, %r61;
	mov.b32 	%f79, %r80;
	add.f32 	%f80, %f78, %f79;
	mov.b32 	%r81, %f80;
	shfl.sync.bfly.b32 	%r82|%p14, %r81, %r70, %r59, %r61;
	mov.b32 	%f81, %r82;
	add.f32 	%f82, %f80, %f81;
	mov.b32 	%r83, %f82;
	shfl.sync.bfly.b32 	%r84|%p15, %r83, %r73, %r59, %r61;
	mov.b32 	%f83, %r84;
	add.f32 	%f84, %f82, %f83;
	st.local.f32 	[%rd3+4], %f84;
	setp.gt.s32 	%p16, %r3, 1;
	@%p16 bra 	$L__BB65_11;

	mad.lo.s32 	%r85, %r3, %r15, %r2;
	cvt.s64.s32 	%rd49, %r85;
	mul.lo.s32 	%r86, %r1, %r16;
	cvt.s64.s32 	%rd50, %r86;
	add.s64 	%rd51, %rd50, %rd49;
	mul.wide.s32 	%rd52, %r3, 4;
	add.s64 	%rd53, %rd3, %rd52;
	ld.local.f32 	%f85, [%rd53];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f85;}

	// end inline asm
	cvta.to.global.u64 	%rd54, %rd25;
	shl.b64 	%rd55, %rd51, 1;
	add.s64 	%rd56, %rd54, %rd55;
	st.global.u16 	[%rd56], %rs1;

$L__BB65_11:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_3_bs_32
.visible .entry ggml_matvec_f16_ncols_3_bs_32(
	.param .u64 ggml_matvec_f16_ncols_3_bs_32_param_0,
	.param .u64 ggml_matvec_f16_ncols_3_bs_32_param_1,
	.param .u64 ggml_matvec_f16_ncols_3_bs_32_param_2,
	.param .u32 ggml_matvec_f16_ncols_3_bs_32_param_3,
	.param .u32 ggml_matvec_f16_ncols_3_bs_32_param_4,
	.param .u32 ggml_matvec_f16_ncols_3_bs_32_param_5,
	.param .u32 ggml_matvec_f16_ncols_3_bs_32_param_6,
	.param .u32 ggml_matvec_f16_ncols_3_bs_32_param_7,
	.param .u32 ggml_matvec_f16_ncols_3_bs_32_param_8,
	.param .u32 ggml_matvec_f16_ncols_3_bs_32_param_9,
	.param .u32 ggml_matvec_f16_ncols_3_bs_32_param_10,
	.param .u32 ggml_matvec_f16_ncols_3_bs_32_param_11
)
{
	.local .align 4 .b8 	__local_depot66[12];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<22>;
	.reg .b16 	%rs<2>;
	.reg .f32 	%f<135>;
	.reg .b32 	%r<118>;
	.reg .b64 	%rd<72>;


	mov.u64 	%SPL, __local_depot66;
	ld.param.u64 	%rd29, [ggml_matvec_f16_ncols_3_bs_32_param_0];
	ld.param.u64 	%rd30, [ggml_matvec_f16_ncols_3_bs_32_param_1];
	ld.param.u64 	%rd28, [ggml_matvec_f16_ncols_3_bs_32_param_2];
	ld.param.u32 	%r13, [ggml_matvec_f16_ncols_3_bs_32_param_3];
	ld.param.u32 	%r17, [ggml_matvec_f16_ncols_3_bs_32_param_5];
	ld.param.u32 	%r14, [ggml_matvec_f16_ncols_3_bs_32_param_6];
	ld.param.u32 	%r15, [ggml_matvec_f16_ncols_3_bs_32_param_7];
	ld.param.u32 	%r18, [ggml_matvec_f16_ncols_3_bs_32_param_8];
	ld.param.u32 	%r19, [ggml_matvec_f16_ncols_3_bs_32_param_9];
	ld.param.u32 	%r20, [ggml_matvec_f16_ncols_3_bs_32_param_10];
	ld.param.u32 	%r16, [ggml_matvec_f16_ncols_3_bs_32_param_11];
	cvta.to.global.u64 	%rd71, %rd30;
	cvta.to.global.u64 	%rd2, %rd29;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r21, %r1, %r18;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r22, %r2, %r17;
	mad.lo.s32 	%r23, %r21, %r19, %r22;
	cvt.s64.s32 	%rd4, %r23;
	mul.lo.s32 	%r24, %r1, %r20;
	cvt.s64.s32 	%rd5, %r24;
	mov.f32 	%f132, 0f00000000;
	mov.u32 	%r25, 0;
	st.local.u32 	[%rd3], %r25;
	st.local.u32 	[%rd3+4], %r25;
	st.local.u32 	[%rd3+8], %r25;
	mov.u32 	%r3, %tid.x;
	setp.ge.s32 	%p1, %r3, %r13;
	mov.f32 	%f133, %f132;
	mov.f32 	%f134, %f132;
	@%p1 bra 	$L__BB66_9;

	not.b32 	%r26, %r3;
	add.s32 	%r4, %r26, %r13;
	shr.u32 	%r27, %r4, 5;
	add.s32 	%r28, %r27, 1;
	and.b32  	%r115, %r28, 3;
	setp.eq.s32 	%p2, %r115, 0;
	mov.f32 	%f132, 0f00000000;
	mov.u32 	%r116, %r3;
	@%p2 bra 	$L__BB66_5;

	shl.b32 	%r29, %r14, 1;
	add.s32 	%r30, %r3, %r29;
	mul.wide.s32 	%rd32, %r30, 2;
	add.s64 	%rd33, %rd32, %rd5;
	shl.b64 	%rd34, %rd33, 1;
	add.s64 	%rd69, %rd71, %rd34;
	mul.wide.s32 	%rd35, %r14, 2;
	mul.wide.s32 	%rd36, %r3, 2;
	add.s64 	%rd37, %rd35, %rd36;
	add.s64 	%rd38, %rd37, %rd5;
	shl.b64 	%rd39, %rd38, 1;
	add.s64 	%rd68, %rd71, %rd39;
	add.s64 	%rd40, %rd36, %rd5;
	shl.b64 	%rd41, %rd40, 1;
	add.s64 	%rd67, %rd71, %rd41;
	add.s64 	%rd42, %rd36, %rd4;
	shl.b64 	%rd43, %rd42, 1;
	add.s64 	%rd66, %rd2, %rd43;
	mov.f32 	%f132, 0f00000000;
	mov.f32 	%f133, %f132;
	mov.f32 	%f134, %f132;
	mov.u32 	%r116, %r3;

$L__BB66_3:
	.pragma "nounroll";
	ld.global.nc.u32 	%r31, [%rd66];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r31;
  cvt.f32.f16 %f28, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r31;
  cvt.f32.f16 %f29, high;}

	// end inline asm
	ld.global.nc.u32 	%r33, [%rd67];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r33;
  cvt.f32.f16 %f30, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r33;
  cvt.f32.f16 %f31, high;}

	// end inline asm
	fma.rn.f32 	%f36, %f28, %f30, %f134;
	fma.rn.f32 	%f134, %f29, %f31, %f36;
	ld.global.nc.u32 	%r35, [%rd68];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r35;
  cvt.f32.f16 %f32, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r35;
  cvt.f32.f16 %f33, high;}

	// end inline asm
	fma.rn.f32 	%f37, %f28, %f32, %f133;
	fma.rn.f32 	%f133, %f29, %f33, %f37;
	ld.global.nc.u32 	%r37, [%rd69];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r37;
  cvt.f32.f16 %f34, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r37;
  cvt.f32.f16 %f35, high;}

	// end inline asm
	fma.rn.f32 	%f38, %f28, %f34, %f132;
	fma.rn.f32 	%f132, %f29, %f35, %f38;
	add.s32 	%r116, %r116, 32;
	add.s64 	%rd69, %rd69, 128;
	add.s64 	%rd68, %rd68, 128;
	add.s64 	%rd67, %rd67, 128;
	add.s64 	%rd66, %rd66, 128;
	add.s32 	%r115, %r115, -1;
	setp.ne.s32 	%p3, %r115, 0;
	@%p3 bra 	$L__BB66_3;

	st.local.f32 	[%rd3], %f134;
	st.local.f32 	[%rd3+4], %f133;
	st.local.f32 	[%rd3+8], %f132;

$L__BB66_5:
	setp.lt.u32 	%p4, %r4, 96;
	@%p4 bra 	$L__BB66_9;

	add.s32 	%r39, %r116, %r14;
	shl.b32 	%r40, %r14, 1;
	add.s32 	%r41, %r116, %r40;
	add.s32 	%r42, %r39, 32;
	mul.wide.s32 	%rd44, %r42, 4;
	shl.b64 	%rd45, %rd5, 1;
	add.s64 	%rd19, %rd44, %rd45;
	mul.wide.s32 	%rd46, %r41, 4;
	add.s64 	%rd20, %rd46, %rd45;
	mul.wide.s32 	%rd47, %r116, 2;
	add.s64 	%rd48, %rd47, %rd4;
	shl.b64 	%rd49, %rd48, 1;
	add.s64 	%rd50, %rd2, %rd49;
	add.s64 	%rd70, %rd50, 256;
	mul.wide.s32 	%rd51, %r116, 4;
	add.s64 	%rd22, %rd51, %rd45;
	mul.wide.s32 	%rd52, %r14, 4;
	add.s64 	%rd53, %rd51, %rd52;
	add.s64 	%rd23, %rd53, %rd45;

$L__BB66_7:
	ld.global.nc.u32 	%r43, [%rd70+-256];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r43;
  cvt.f32.f16 %f39, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r43;
  cvt.f32.f16 %f40, high;}

	// end inline asm
	add.s64 	%rd54, %rd71, %rd22;
	ld.global.nc.u32 	%r45, [%rd54];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r45;
  cvt.f32.f16 %f41, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r45;
  cvt.f32.f16 %f42, high;}

	// end inline asm
	fma.rn.f32 	%f71, %f39, %f41, %f134;
	fma.rn.f32 	%f72, %f40, %f42, %f71;
	add.s64 	%rd55, %rd71, %rd23;
	ld.global.nc.u32 	%r47, [%rd55];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r47;
  cvt.f32.f16 %f43, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r47;
  cvt.f32.f16 %f44, high;}

	// end inline asm
	fma.rn.f32 	%f73, %f39, %f43, %f133;
	fma.rn.f32 	%f74, %f40, %f44, %f73;
	add.s64 	%rd56, %rd71, %rd20;
	ld.global.nc.u32 	%r49, [%rd56];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r49;
  cvt.f32.f16 %f45, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r49;
  cvt.f32.f16 %f46, high;}

	// end inline asm
	fma.rn.f32 	%f75, %f39, %f45, %f132;
	fma.rn.f32 	%f76, %f40, %f46, %f75;
	ld.global.nc.u32 	%r51, [%rd70+-128];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r51;
  cvt.f32.f16 %f47, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r51;
  cvt.f32.f16 %f48, high;}

	// end inline asm
	ld.global.nc.u32 	%r53, [%rd54+128];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r53;
  cvt.f32.f16 %f49, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r53;
  cvt.f32.f16 %f50, high;}

	// end inline asm
	fma.rn.f32 	%f77, %f47, %f49, %f72;
	fma.rn.f32 	%f78, %f48, %f50, %f77;
	add.s64 	%rd57, %rd71, %rd19;
	ld.global.nc.u32 	%r55, [%rd57];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r55;
  cvt.f32.f16 %f51, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r55;
  cvt.f32.f16 %f52, high;}

	// end inline asm
	fma.rn.f32 	%f79, %f47, %f51, %f74;
	fma.rn.f32 	%f80, %f48, %f52, %f79;
	ld.global.nc.u32 	%r57, [%rd56+128];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r57;
  cvt.f32.f16 %f53, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r57;
  cvt.f32.f16 %f54, high;}

	// end inline asm
	fma.rn.f32 	%f81, %f47, %f53, %f76;
	fma.rn.f32 	%f82, %f48, %f54, %f81;
	ld.global.nc.u32 	%r59, [%rd70];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r59;
  cvt.f32.f16 %f55, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r59;
  cvt.f32.f16 %f56, high;}

	// end inline asm
	ld.global.nc.u32 	%r61, [%rd54+256];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r61;
  cvt.f32.f16 %f57, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r61;
  cvt.f32.f16 %f58, high;}

	// end inline asm
	fma.rn.f32 	%f83, %f55, %f57, %f78;
	fma.rn.f32 	%f84, %f56, %f58, %f83;
	ld.global.nc.u32 	%r63, [%rd57+128];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r63;
  cvt.f32.f16 %f59, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r63;
  cvt.f32.f16 %f60, high;}

	// end inline asm
	fma.rn.f32 	%f85, %f55, %f59, %f80;
	fma.rn.f32 	%f86, %f56, %f60, %f85;
	ld.global.nc.u32 	%r65, [%rd56+256];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r65;
  cvt.f32.f16 %f61, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r65;
  cvt.f32.f16 %f62, high;}

	// end inline asm
	fma.rn.f32 	%f87, %f55, %f61, %f82;
	fma.rn.f32 	%f88, %f56, %f62, %f87;
	ld.global.nc.u32 	%r67, [%rd70+128];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r67;
  cvt.f32.f16 %f63, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r67;
  cvt.f32.f16 %f64, high;}

	// end inline asm
	ld.global.nc.u32 	%r69, [%rd54+384];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r69;
  cvt.f32.f16 %f65, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r69;
  cvt.f32.f16 %f66, high;}

	// end inline asm
	fma.rn.f32 	%f89, %f63, %f65, %f84;
	fma.rn.f32 	%f134, %f64, %f66, %f89;
	ld.global.nc.u32 	%r71, [%rd57+256];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r71;
  cvt.f32.f16 %f67, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r71;
  cvt.f32.f16 %f68, high;}

	// end inline asm
	fma.rn.f32 	%f90, %f63, %f67, %f86;
	fma.rn.f32 	%f133, %f64, %f68, %f90;
	ld.global.nc.u32 	%r73, [%rd56+384];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r73;
  cvt.f32.f16 %f69, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r73;
  cvt.f32.f16 %f70, high;}

	// end inline asm
	fma.rn.f32 	%f91, %f63, %f69, %f88;
	fma.rn.f32 	%f132, %f64, %f70, %f91;
	add.s64 	%rd71, %rd71, 512;
	add.s64 	%rd70, %rd70, 512;
	add.s32 	%r116, %r116, 128;
	setp.lt.s32 	%p5, %r116, %r13;
	@%p5 bra 	$L__BB66_7;

	st.local.f32 	[%rd3], %f134;
	st.local.f32 	[%rd3+4], %f133;
	st.local.f32 	[%rd3+8], %f132;

$L__BB66_9:
	mov.b32 	%r75, %f134;
	mov.u32 	%r76, 31;
	mov.u32 	%r77, 16;
	mov.u32 	%r78, -1;
	shfl.sync.bfly.b32 	%r79|%p6, %r75, %r77, %r76, %r78;
	mov.b32 	%f92, %r79;
	add.f32 	%f93, %f134, %f92;
	mov.b32 	%r80, %f93;
	mov.u32 	%r81, 8;
	shfl.sync.bfly.b32 	%r82|%p7, %r80, %r81, %r76, %r78;
	mov.b32 	%f94, %r82;
	add.f32 	%f95, %f93, %f94;
	mov.b32 	%r83, %f95;
	mov.u32 	%r84, 4;
	shfl.sync.bfly.b32 	%r85|%p8, %r83, %r84, %r76, %r78;
	mov.b32 	%f96, %r85;
	add.f32 	%f97, %f95, %f96;
	mov.b32 	%r86, %f97;
	mov.u32 	%r87, 2;
	shfl.sync.bfly.b32 	%r88|%p9, %r86, %r87, %r76, %r78;
	mov.b32 	%f98, %r88;
	add.f32 	%f99, %f97, %f98;
	mov.b32 	%r89, %f99;
	mov.u32 	%r90, 1;
	shfl.sync.bfly.b32 	%r91|%p10, %r89, %r90, %r76, %r78;
	mov.b32 	%f100, %r91;
	add.f32 	%f101, %f99, %f100;
	st.local.f32 	[%rd3], %f101;
	mov.b32 	%r92, %f133;
	shfl.sync.bfly.b32 	%r93|%p11, %r92, %r77, %r76, %r78;
	mov.b32 	%f102, %r93;
	add.f32 	%f103, %f133, %f102;
	mov.b32 	%r94, %f103;
	shfl.sync.bfly.b32 	%r95|%p12, %r94, %r81, %r76, %r78;
	mov.b32 	%f104, %r95;
	add.f32 	%f105, %f103, %f104;
	mov.b32 	%r96, %f105;
	shfl.sync.bfly.b32 	%r97|%p13, %r96, %r84, %r76, %r78;
	mov.b32 	%f106, %r97;
	add.f32 	%f107, %f105, %f106;
	mov.b32 	%r98, %f107;
	shfl.sync.bfly.b32 	%r99|%p14, %r98, %r87, %r76, %r78;
	mov.b32 	%f108, %r99;
	add.f32 	%f109, %f107, %f108;
	mov.b32 	%r100, %f109;
	shfl.sync.bfly.b32 	%r101|%p15, %r100, %r90, %r76, %r78;
	mov.b32 	%f110, %r101;
	add.f32 	%f111, %f109, %f110;
	st.local.f32 	[%rd3+4], %f111;
	mov.b32 	%r102, %f132;
	shfl.sync.bfly.b32 	%r103|%p16, %r102, %r77, %r76, %r78;
	mov.b32 	%f112, %r103;
	add.f32 	%f113, %f132, %f112;
	mov.b32 	%r104, %f113;
	shfl.sync.bfly.b32 	%r105|%p17, %r104, %r81, %r76, %r78;
	mov.b32 	%f114, %r105;
	add.f32 	%f115, %f113, %f114;
	mov.b32 	%r106, %f115;
	shfl.sync.bfly.b32 	%r107|%p18, %r106, %r84, %r76, %r78;
	mov.b32 	%f116, %r107;
	add.f32 	%f117, %f115, %f116;
	mov.b32 	%r108, %f117;
	shfl.sync.bfly.b32 	%r109|%p19, %r108, %r87, %r76, %r78;
	mov.b32 	%f118, %r109;
	add.f32 	%f119, %f117, %f118;
	mov.b32 	%r110, %f119;
	shfl.sync.bfly.b32 	%r111|%p20, %r110, %r90, %r76, %r78;
	mov.b32 	%f120, %r111;
	add.f32 	%f121, %f119, %f120;
	st.local.f32 	[%rd3+8], %f121;
	setp.gt.s32 	%p21, %r3, 2;
	@%p21 bra 	$L__BB66_11;

	mad.lo.s32 	%r112, %r3, %r15, %r2;
	cvt.s64.s32 	%rd58, %r112;
	mul.lo.s32 	%r113, %r1, %r16;
	cvt.s64.s32 	%rd59, %r113;
	add.s64 	%rd60, %rd59, %rd58;
	mul.wide.s32 	%rd61, %r3, 4;
	add.s64 	%rd62, %rd3, %rd61;
	ld.local.f32 	%f122, [%rd62];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f122;}

	// end inline asm
	cvta.to.global.u64 	%rd63, %rd28;
	shl.b64 	%rd64, %rd60, 1;
	add.s64 	%rd65, %rd63, %rd64;
	st.global.u16 	[%rd65], %rs1;

$L__BB66_11:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_4_bs_32
.visible .entry ggml_matvec_f16_ncols_4_bs_32(
	.param .u64 ggml_matvec_f16_ncols_4_bs_32_param_0,
	.param .u64 ggml_matvec_f16_ncols_4_bs_32_param_1,
	.param .u64 ggml_matvec_f16_ncols_4_bs_32_param_2,
	.param .u32 ggml_matvec_f16_ncols_4_bs_32_param_3,
	.param .u32 ggml_matvec_f16_ncols_4_bs_32_param_4,
	.param .u32 ggml_matvec_f16_ncols_4_bs_32_param_5,
	.param .u32 ggml_matvec_f16_ncols_4_bs_32_param_6,
	.param .u32 ggml_matvec_f16_ncols_4_bs_32_param_7,
	.param .u32 ggml_matvec_f16_ncols_4_bs_32_param_8,
	.param .u32 ggml_matvec_f16_ncols_4_bs_32_param_9,
	.param .u32 ggml_matvec_f16_ncols_4_bs_32_param_10,
	.param .u32 ggml_matvec_f16_ncols_4_bs_32_param_11
)
{
	.local .align 16 .b8 	__local_depot67[16];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<26>;
	.reg .b16 	%rs<2>;
	.reg .f32 	%f<128>;
	.reg .b32 	%r<113>;
	.reg .b64 	%rd<60>;


	mov.u64 	%SPL, __local_depot67;
	ld.param.u64 	%rd18, [ggml_matvec_f16_ncols_4_bs_32_param_0];
	ld.param.u64 	%rd19, [ggml_matvec_f16_ncols_4_bs_32_param_1];
	ld.param.u64 	%rd17, [ggml_matvec_f16_ncols_4_bs_32_param_2];
	ld.param.u32 	%r9, [ggml_matvec_f16_ncols_4_bs_32_param_3];
	ld.param.u32 	%r13, [ggml_matvec_f16_ncols_4_bs_32_param_5];
	ld.param.u32 	%r10, [ggml_matvec_f16_ncols_4_bs_32_param_6];
	ld.param.u32 	%r11, [ggml_matvec_f16_ncols_4_bs_32_param_7];
	ld.param.u32 	%r14, [ggml_matvec_f16_ncols_4_bs_32_param_8];
	ld.param.u32 	%r15, [ggml_matvec_f16_ncols_4_bs_32_param_9];
	ld.param.u32 	%r16, [ggml_matvec_f16_ncols_4_bs_32_param_10];
	ld.param.u32 	%r12, [ggml_matvec_f16_ncols_4_bs_32_param_11];
	cvta.to.global.u64 	%rd59, %rd19;
	add.u64 	%rd2, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r17, %r1, %r14;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r18, %r2, %r13;
	mad.lo.s32 	%r19, %r17, %r15, %r18;
	cvt.s64.s32 	%rd3, %r19;
	cvta.to.global.u64 	%rd4, %rd18;
	mul.lo.s32 	%r20, %r1, %r16;
	cvt.s64.s32 	%rd5, %r20;
	mov.f32 	%f124, 0f00000000;
	st.local.v4.f32 	[%rd2], {%f124, %f124, %f124, %f124};
	mov.u32 	%r3, %tid.x;
	setp.ge.s32 	%p1, %r3, %r9;
	mov.f32 	%f125, %f124;
	mov.f32 	%f126, %f124;
	mov.f32 	%f127, %f124;
	@%p1 bra 	$L__BB67_7;

	not.b32 	%r21, %r3;
	add.s32 	%r4, %r21, %r9;
	and.b32  	%r22, %r4, 32;
	setp.ne.s32 	%p2, %r22, 0;
	mov.f32 	%f124, 0f00000000;
	mov.u32 	%r112, %r3;
	@%p2 bra 	$L__BB67_3;

	shl.b64 	%rd21, %rd5, 1;
	add.s64 	%rd22, %rd59, %rd21;
	shl.b64 	%rd23, %rd3, 1;
	add.s64 	%rd24, %rd4, %rd23;
	mul.wide.s32 	%rd25, %r3, 4;
	add.s64 	%rd26, %rd24, %rd25;
	ld.global.nc.u32 	%r23, [%rd26];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r23;
  cvt.f32.f16 %f29, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r23;
  cvt.f32.f16 %f30, high;}

	// end inline asm
	add.s64 	%rd27, %rd22, %rd25;
	ld.global.nc.u32 	%r25, [%rd27];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r25;
  cvt.f32.f16 %f31, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r25;
  cvt.f32.f16 %f32, high;}

	// end inline asm
	fma.rn.f32 	%f39, %f29, %f31, 0f00000000;
	fma.rn.f32 	%f127, %f30, %f32, %f39;
	st.local.f32 	[%rd2], %f127;
	mul.wide.s32 	%rd28, %r10, 4;
	add.s64 	%rd29, %rd27, %rd28;
	ld.global.nc.u32 	%r27, [%rd29];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r27;
  cvt.f32.f16 %f33, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r27;
  cvt.f32.f16 %f34, high;}

	// end inline asm
	fma.rn.f32 	%f40, %f29, %f33, 0f00000000;
	fma.rn.f32 	%f126, %f30, %f34, %f40;
	st.local.f32 	[%rd2+4], %f126;
	add.s32 	%r33, %r3, %r10;
	add.s32 	%r34, %r33, %r10;
	mul.wide.s32 	%rd30, %r34, 4;
	add.s64 	%rd31, %rd22, %rd30;
	ld.global.nc.u32 	%r29, [%rd31];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r29;
  cvt.f32.f16 %f35, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r29;
  cvt.f32.f16 %f36, high;}

	// end inline asm
	fma.rn.f32 	%f41, %f29, %f35, 0f00000000;
	fma.rn.f32 	%f125, %f30, %f36, %f41;
	st.local.f32 	[%rd2+8], %f125;
	add.s32 	%r35, %r34, %r10;
	mul.wide.s32 	%rd32, %r35, 4;
	add.s64 	%rd33, %rd22, %rd32;
	ld.global.nc.u32 	%r31, [%rd33];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r31;
  cvt.f32.f16 %f37, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r31;
  cvt.f32.f16 %f38, high;}

	// end inline asm
	fma.rn.f32 	%f42, %f29, %f37, 0f00000000;
	fma.rn.f32 	%f124, %f30, %f38, %f42;
	st.local.f32 	[%rd2+12], %f124;
	add.s32 	%r112, %r3, 32;

$L__BB67_3:
	and.b32  	%r36, %r4, -32;
	setp.eq.s32 	%p3, %r36, 0;
	@%p3 bra 	$L__BB67_7;

	add.s32 	%r37, %r112, %r10;
	add.s32 	%r38, %r37, 32;
	mul.wide.s32 	%rd34, %r38, 4;
	shl.b64 	%rd35, %rd5, 1;
	add.s64 	%rd7, %rd34, %rd35;
	shl.b32 	%r39, %r10, 1;
	add.s32 	%r40, %r112, %r39;
	mad.lo.s32 	%r41, %r10, 3, %r112;
	mul.wide.s32 	%rd36, %r40, 4;
	add.s64 	%rd8, %rd36, %rd35;
	mul.wide.s32 	%rd37, %r41, 4;
	add.s64 	%rd9, %rd37, %rd35;
	mul.wide.s32 	%rd38, %r112, 2;
	add.s64 	%rd39, %rd38, %rd3;
	shl.b64 	%rd40, %rd39, 1;
	add.s64 	%rd41, %rd4, %rd40;
	add.s64 	%rd58, %rd41, 128;
	mul.wide.s32 	%rd42, %r112, 4;
	mul.wide.s32 	%rd43, %r10, 4;
	add.s64 	%rd44, %rd42, %rd43;
	add.s64 	%rd11, %rd44, %rd35;
	add.s64 	%rd12, %rd42, %rd35;

$L__BB67_5:
	ld.global.nc.u32 	%r42, [%rd58+-128];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r42;
  cvt.f32.f16 %f43, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r42;
  cvt.f32.f16 %f44, high;}

	// end inline asm
	add.s64 	%rd45, %rd59, %rd12;
	ld.global.nc.u32 	%r44, [%rd45];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r44;
  cvt.f32.f16 %f45, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r44;
  cvt.f32.f16 %f46, high;}

	// end inline asm
	fma.rn.f32 	%f63, %f43, %f45, %f127;
	fma.rn.f32 	%f64, %f44, %f46, %f63;
	add.s64 	%rd46, %rd59, %rd11;
	ld.global.nc.u32 	%r46, [%rd46];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r46;
  cvt.f32.f16 %f47, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r46;
  cvt.f32.f16 %f48, high;}

	// end inline asm
	fma.rn.f32 	%f65, %f43, %f47, %f126;
	fma.rn.f32 	%f66, %f44, %f48, %f65;
	add.s64 	%rd47, %rd59, %rd8;
	ld.global.nc.u32 	%r48, [%rd47];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r48;
  cvt.f32.f16 %f49, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r48;
  cvt.f32.f16 %f50, high;}

	// end inline asm
	fma.rn.f32 	%f67, %f43, %f49, %f125;
	fma.rn.f32 	%f68, %f44, %f50, %f67;
	add.s64 	%rd48, %rd59, %rd9;
	ld.global.nc.u32 	%r50, [%rd48];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r50;
  cvt.f32.f16 %f51, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r50;
  cvt.f32.f16 %f52, high;}

	// end inline asm
	fma.rn.f32 	%f69, %f43, %f51, %f124;
	fma.rn.f32 	%f70, %f44, %f52, %f69;
	ld.global.nc.u32 	%r52, [%rd58];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r52;
  cvt.f32.f16 %f53, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r52;
  cvt.f32.f16 %f54, high;}

	// end inline asm
	ld.global.nc.u32 	%r54, [%rd45+128];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r54;
  cvt.f32.f16 %f55, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r54;
  cvt.f32.f16 %f56, high;}

	// end inline asm
	fma.rn.f32 	%f71, %f53, %f55, %f64;
	fma.rn.f32 	%f127, %f54, %f56, %f71;
	add.s64 	%rd49, %rd59, %rd7;
	ld.global.nc.u32 	%r56, [%rd49];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r56;
  cvt.f32.f16 %f57, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r56;
  cvt.f32.f16 %f58, high;}

	// end inline asm
	fma.rn.f32 	%f72, %f53, %f57, %f66;
	fma.rn.f32 	%f126, %f54, %f58, %f72;
	ld.global.nc.u32 	%r58, [%rd47+128];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r58;
  cvt.f32.f16 %f59, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r58;
  cvt.f32.f16 %f60, high;}

	// end inline asm
	fma.rn.f32 	%f73, %f53, %f59, %f68;
	fma.rn.f32 	%f125, %f54, %f60, %f73;
	ld.global.nc.u32 	%r60, [%rd48+128];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r60;
  cvt.f32.f16 %f61, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r60;
  cvt.f32.f16 %f62, high;}

	// end inline asm
	fma.rn.f32 	%f74, %f53, %f61, %f70;
	fma.rn.f32 	%f124, %f54, %f62, %f74;
	add.s64 	%rd59, %rd59, 256;
	add.s64 	%rd58, %rd58, 256;
	add.s32 	%r112, %r112, 64;
	setp.lt.s32 	%p4, %r112, %r9;
	@%p4 bra 	$L__BB67_5;

	st.local.v4.f32 	[%rd2], {%f127, %f126, %f125, %f124};

$L__BB67_7:
	mov.b32 	%r62, %f127;
	mov.u32 	%r63, 31;
	mov.u32 	%r64, 16;
	mov.u32 	%r65, -1;
	shfl.sync.bfly.b32 	%r66|%p5, %r62, %r64, %r63, %r65;
	mov.b32 	%f75, %r66;
	add.f32 	%f76, %f127, %f75;
	mov.b32 	%r67, %f76;
	mov.u32 	%r68, 8;
	shfl.sync.bfly.b32 	%r69|%p6, %r67, %r68, %r63, %r65;
	mov.b32 	%f77, %r69;
	add.f32 	%f78, %f76, %f77;
	mov.b32 	%r70, %f78;
	mov.u32 	%r71, 4;
	shfl.sync.bfly.b32 	%r72|%p7, %r70, %r71, %r63, %r65;
	mov.b32 	%f79, %r72;
	add.f32 	%f80, %f78, %f79;
	mov.b32 	%r73, %f80;
	mov.u32 	%r74, 2;
	shfl.sync.bfly.b32 	%r75|%p8, %r73, %r74, %r63, %r65;
	mov.b32 	%f81, %r75;
	add.f32 	%f82, %f80, %f81;
	mov.b32 	%r76, %f82;
	mov.u32 	%r77, 1;
	shfl.sync.bfly.b32 	%r78|%p9, %r76, %r77, %r63, %r65;
	mov.b32 	%f83, %r78;
	add.f32 	%f84, %f82, %f83;
	st.local.f32 	[%rd2], %f84;
	mov.b32 	%r79, %f126;
	shfl.sync.bfly.b32 	%r80|%p10, %r79, %r64, %r63, %r65;
	mov.b32 	%f85, %r80;
	add.f32 	%f86, %f126, %f85;
	mov.b32 	%r81, %f86;
	shfl.sync.bfly.b32 	%r82|%p11, %r81, %r68, %r63, %r65;
	mov.b32 	%f87, %r82;
	add.f32 	%f88, %f86, %f87;
	mov.b32 	%r83, %f88;
	shfl.sync.bfly.b32 	%r84|%p12, %r83, %r71, %r63, %r65;
	mov.b32 	%f89, %r84;
	add.f32 	%f90, %f88, %f89;
	mov.b32 	%r85, %f90;
	shfl.sync.bfly.b32 	%r86|%p13, %r85, %r74, %r63, %r65;
	mov.b32 	%f91, %r86;
	add.f32 	%f92, %f90, %f91;
	mov.b32 	%r87, %f92;
	shfl.sync.bfly.b32 	%r88|%p14, %r87, %r77, %r63, %r65;
	mov.b32 	%f93, %r88;
	add.f32 	%f94, %f92, %f93;
	st.local.f32 	[%rd2+4], %f94;
	mov.b32 	%r89, %f125;
	shfl.sync.bfly.b32 	%r90|%p15, %r89, %r64, %r63, %r65;
	mov.b32 	%f95, %r90;
	add.f32 	%f96, %f125, %f95;
	mov.b32 	%r91, %f96;
	shfl.sync.bfly.b32 	%r92|%p16, %r91, %r68, %r63, %r65;
	mov.b32 	%f97, %r92;
	add.f32 	%f98, %f96, %f97;
	mov.b32 	%r93, %f98;
	shfl.sync.bfly.b32 	%r94|%p17, %r93, %r71, %r63, %r65;
	mov.b32 	%f99, %r94;
	add.f32 	%f100, %f98, %f99;
	mov.b32 	%r95, %f100;
	shfl.sync.bfly.b32 	%r96|%p18, %r95, %r74, %r63, %r65;
	mov.b32 	%f101, %r96;
	add.f32 	%f102, %f100, %f101;
	mov.b32 	%r97, %f102;
	shfl.sync.bfly.b32 	%r98|%p19, %r97, %r77, %r63, %r65;
	mov.b32 	%f103, %r98;
	add.f32 	%f104, %f102, %f103;
	st.local.f32 	[%rd2+8], %f104;
	mov.b32 	%r99, %f124;
	shfl.sync.bfly.b32 	%r100|%p20, %r99, %r64, %r63, %r65;
	mov.b32 	%f105, %r100;
	add.f32 	%f106, %f124, %f105;
	mov.b32 	%r101, %f106;
	shfl.sync.bfly.b32 	%r102|%p21, %r101, %r68, %r63, %r65;
	mov.b32 	%f107, %r102;
	add.f32 	%f108, %f106, %f107;
	mov.b32 	%r103, %f108;
	shfl.sync.bfly.b32 	%r104|%p22, %r103, %r71, %r63, %r65;
	mov.b32 	%f109, %r104;
	add.f32 	%f110, %f108, %f109;
	mov.b32 	%r105, %f110;
	shfl.sync.bfly.b32 	%r106|%p23, %r105, %r74, %r63, %r65;
	mov.b32 	%f111, %r106;
	add.f32 	%f112, %f110, %f111;
	mov.b32 	%r107, %f112;
	shfl.sync.bfly.b32 	%r108|%p24, %r107, %r77, %r63, %r65;
	mov.b32 	%f113, %r108;
	add.f32 	%f114, %f112, %f113;
	st.local.f32 	[%rd2+12], %f114;
	setp.gt.s32 	%p25, %r3, 3;
	@%p25 bra 	$L__BB67_9;

	mad.lo.s32 	%r109, %r3, %r11, %r2;
	cvt.s64.s32 	%rd50, %r109;
	mul.lo.s32 	%r110, %r1, %r12;
	cvt.s64.s32 	%rd51, %r110;
	add.s64 	%rd52, %rd51, %rd50;
	mul.wide.s32 	%rd53, %r3, 4;
	add.s64 	%rd54, %rd2, %rd53;
	ld.local.f32 	%f115, [%rd54];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f115;}

	// end inline asm
	cvta.to.global.u64 	%rd55, %rd17;
	shl.b64 	%rd56, %rd52, 1;
	add.s64 	%rd57, %rd55, %rd56;
	st.global.u16 	[%rd57], %rs1;

$L__BB67_9:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_5_bs_32
.visible .entry ggml_matvec_f16_ncols_5_bs_32(
	.param .u64 ggml_matvec_f16_ncols_5_bs_32_param_0,
	.param .u64 ggml_matvec_f16_ncols_5_bs_32_param_1,
	.param .u64 ggml_matvec_f16_ncols_5_bs_32_param_2,
	.param .u32 ggml_matvec_f16_ncols_5_bs_32_param_3,
	.param .u32 ggml_matvec_f16_ncols_5_bs_32_param_4,
	.param .u32 ggml_matvec_f16_ncols_5_bs_32_param_5,
	.param .u32 ggml_matvec_f16_ncols_5_bs_32_param_6,
	.param .u32 ggml_matvec_f16_ncols_5_bs_32_param_7,
	.param .u32 ggml_matvec_f16_ncols_5_bs_32_param_8,
	.param .u32 ggml_matvec_f16_ncols_5_bs_32_param_9,
	.param .u32 ggml_matvec_f16_ncols_5_bs_32_param_10,
	.param .u32 ggml_matvec_f16_ncols_5_bs_32_param_11
)
{
	.local .align 4 .b8 	__local_depot68[20];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<31>;
	.reg .b16 	%rs<2>;
	.reg .f32 	%f<158>;
	.reg .b32 	%r<133>;
	.reg .b64 	%rd<64>;


	mov.u64 	%SPL, __local_depot68;
	ld.param.u64 	%rd19, [ggml_matvec_f16_ncols_5_bs_32_param_0];
	ld.param.u64 	%rd20, [ggml_matvec_f16_ncols_5_bs_32_param_1];
	ld.param.u64 	%rd18, [ggml_matvec_f16_ncols_5_bs_32_param_2];
	ld.param.u32 	%r9, [ggml_matvec_f16_ncols_5_bs_32_param_3];
	ld.param.u32 	%r13, [ggml_matvec_f16_ncols_5_bs_32_param_5];
	ld.param.u32 	%r10, [ggml_matvec_f16_ncols_5_bs_32_param_6];
	ld.param.u32 	%r11, [ggml_matvec_f16_ncols_5_bs_32_param_7];
	ld.param.u32 	%r14, [ggml_matvec_f16_ncols_5_bs_32_param_8];
	ld.param.u32 	%r15, [ggml_matvec_f16_ncols_5_bs_32_param_9];
	ld.param.u32 	%r16, [ggml_matvec_f16_ncols_5_bs_32_param_10];
	ld.param.u32 	%r12, [ggml_matvec_f16_ncols_5_bs_32_param_11];
	cvta.to.global.u64 	%rd63, %rd20;
	add.u64 	%rd2, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r17, %r1, %r14;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r18, %r2, %r13;
	mad.lo.s32 	%r19, %r17, %r15, %r18;
	cvt.s64.s32 	%rd3, %r19;
	cvta.to.global.u64 	%rd4, %rd19;
	mul.lo.s32 	%r20, %r1, %r16;
	cvt.s64.s32 	%rd5, %r20;
	mov.f32 	%f153, 0f00000000;
	mov.u32 	%r21, 0;
	st.local.u32 	[%rd2], %r21;
	st.local.u32 	[%rd2+4], %r21;
	st.local.u32 	[%rd2+8], %r21;
	st.local.u32 	[%rd2+12], %r21;
	st.local.u32 	[%rd2+16], %r21;
	mov.u32 	%r3, %tid.x;
	setp.ge.s32 	%p1, %r3, %r9;
	mov.f32 	%f154, %f153;
	mov.f32 	%f155, %f153;
	mov.f32 	%f156, %f153;
	mov.f32 	%f157, %f153;
	@%p1 bra 	$L__BB68_7;

	not.b32 	%r22, %r3;
	add.s32 	%r4, %r22, %r9;
	and.b32  	%r23, %r4, 32;
	setp.ne.s32 	%p2, %r23, 0;
	mov.f32 	%f153, 0f00000000;
	mov.u32 	%r132, %r3;
	@%p2 bra 	$L__BB68_3;

	shl.b64 	%rd22, %rd5, 1;
	add.s64 	%rd23, %rd63, %rd22;
	shl.b64 	%rd24, %rd3, 1;
	add.s64 	%rd25, %rd4, %rd24;
	mul.wide.s32 	%rd26, %r3, 4;
	add.s64 	%rd27, %rd25, %rd26;
	ld.global.nc.u32 	%r24, [%rd27];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r24;
  cvt.f32.f16 %f36, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r24;
  cvt.f32.f16 %f37, high;}

	// end inline asm
	add.s64 	%rd28, %rd23, %rd26;
	ld.global.nc.u32 	%r26, [%rd28];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r26;
  cvt.f32.f16 %f38, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r26;
  cvt.f32.f16 %f39, high;}

	// end inline asm
	fma.rn.f32 	%f48, %f36, %f38, 0f00000000;
	fma.rn.f32 	%f157, %f37, %f39, %f48;
	st.local.f32 	[%rd2], %f157;
	mul.wide.s32 	%rd29, %r10, 4;
	add.s64 	%rd30, %rd28, %rd29;
	ld.global.nc.u32 	%r28, [%rd30];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r28;
  cvt.f32.f16 %f40, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r28;
  cvt.f32.f16 %f41, high;}

	// end inline asm
	fma.rn.f32 	%f49, %f36, %f40, 0f00000000;
	fma.rn.f32 	%f156, %f37, %f41, %f49;
	st.local.f32 	[%rd2+4], %f156;
	add.s32 	%r36, %r3, %r10;
	add.s32 	%r37, %r36, %r10;
	shl.b32 	%r38, %r10, 1;
	mul.wide.s32 	%rd31, %r38, 4;
	add.s64 	%rd32, %rd28, %rd31;
	ld.global.nc.u32 	%r30, [%rd32];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r30;
  cvt.f32.f16 %f42, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r30;
  cvt.f32.f16 %f43, high;}

	// end inline asm
	fma.rn.f32 	%f50, %f36, %f42, 0f00000000;
	fma.rn.f32 	%f155, %f37, %f43, %f50;
	st.local.f32 	[%rd2+8], %f155;
	add.s32 	%r39, %r37, %r10;
	mul.wide.s32 	%rd33, %r39, 4;
	add.s64 	%rd34, %rd23, %rd33;
	ld.global.nc.u32 	%r32, [%rd34];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r32;
  cvt.f32.f16 %f44, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r32;
  cvt.f32.f16 %f45, high;}

	// end inline asm
	fma.rn.f32 	%f51, %f36, %f44, 0f00000000;
	fma.rn.f32 	%f154, %f37, %f45, %f51;
	st.local.f32 	[%rd2+12], %f154;
	add.s64 	%rd35, %rd32, %rd31;
	ld.global.nc.u32 	%r34, [%rd35];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r34;
  cvt.f32.f16 %f46, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r34;
  cvt.f32.f16 %f47, high;}

	// end inline asm
	fma.rn.f32 	%f52, %f36, %f46, 0f00000000;
	fma.rn.f32 	%f153, %f37, %f47, %f52;
	st.local.f32 	[%rd2+16], %f153;
	add.s32 	%r132, %r3, 32;

$L__BB68_3:
	and.b32  	%r40, %r4, -32;
	setp.eq.s32 	%p3, %r40, 0;
	@%p3 bra 	$L__BB68_7;

	add.s32 	%r41, %r132, %r10;
	add.s32 	%r42, %r41, 32;
	mul.wide.s32 	%rd36, %r42, 4;
	shl.b64 	%rd37, %rd5, 1;
	add.s64 	%rd7, %rd36, %rd37;
	shl.b32 	%r43, %r10, 1;
	add.s32 	%r44, %r132, %r43;
	mad.lo.s32 	%r45, %r10, 3, %r132;
	shl.b32 	%r46, %r10, 2;
	add.s32 	%r47, %r132, %r46;
	mul.wide.s32 	%rd38, %r44, 4;
	add.s64 	%rd8, %rd38, %rd37;
	mul.wide.s32 	%rd39, %r45, 4;
	add.s64 	%rd9, %rd39, %rd37;
	mul.wide.s32 	%rd40, %r47, 4;
	add.s64 	%rd10, %rd40, %rd37;
	mul.wide.s32 	%rd41, %r132, 2;
	add.s64 	%rd42, %rd41, %rd3;
	shl.b64 	%rd43, %rd42, 1;
	add.s64 	%rd44, %rd4, %rd43;
	add.s64 	%rd62, %rd44, 128;
	mul.wide.s32 	%rd45, %r132, 4;
	mul.wide.s32 	%rd46, %r10, 4;
	add.s64 	%rd47, %rd45, %rd46;
	add.s64 	%rd12, %rd47, %rd37;
	add.s64 	%rd13, %rd45, %rd37;

$L__BB68_5:
	ld.global.nc.u32 	%r48, [%rd62+-128];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r48;
  cvt.f32.f16 %f53, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r48;
  cvt.f32.f16 %f54, high;}

	// end inline asm
	add.s64 	%rd48, %rd63, %rd13;
	ld.global.nc.u32 	%r50, [%rd48];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r50;
  cvt.f32.f16 %f55, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r50;
  cvt.f32.f16 %f56, high;}

	// end inline asm
	fma.rn.f32 	%f77, %f53, %f55, %f157;
	fma.rn.f32 	%f78, %f54, %f56, %f77;
	add.s64 	%rd49, %rd63, %rd12;
	ld.global.nc.u32 	%r52, [%rd49];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r52;
  cvt.f32.f16 %f57, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r52;
  cvt.f32.f16 %f58, high;}

	// end inline asm
	fma.rn.f32 	%f79, %f53, %f57, %f156;
	fma.rn.f32 	%f80, %f54, %f58, %f79;
	add.s64 	%rd50, %rd63, %rd8;
	ld.global.nc.u32 	%r54, [%rd50];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r54;
  cvt.f32.f16 %f59, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r54;
  cvt.f32.f16 %f60, high;}

	// end inline asm
	fma.rn.f32 	%f81, %f53, %f59, %f155;
	fma.rn.f32 	%f82, %f54, %f60, %f81;
	add.s64 	%rd51, %rd63, %rd9;
	ld.global.nc.u32 	%r56, [%rd51];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r56;
  cvt.f32.f16 %f61, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r56;
  cvt.f32.f16 %f62, high;}

	// end inline asm
	fma.rn.f32 	%f83, %f53, %f61, %f154;
	fma.rn.f32 	%f84, %f54, %f62, %f83;
	add.s64 	%rd52, %rd63, %rd10;
	ld.global.nc.u32 	%r58, [%rd52];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r58;
  cvt.f32.f16 %f63, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r58;
  cvt.f32.f16 %f64, high;}

	// end inline asm
	fma.rn.f32 	%f85, %f53, %f63, %f153;
	fma.rn.f32 	%f86, %f54, %f64, %f85;
	ld.global.nc.u32 	%r60, [%rd62];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r60;
  cvt.f32.f16 %f65, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r60;
  cvt.f32.f16 %f66, high;}

	// end inline asm
	ld.global.nc.u32 	%r62, [%rd48+128];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r62;
  cvt.f32.f16 %f67, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r62;
  cvt.f32.f16 %f68, high;}

	// end inline asm
	fma.rn.f32 	%f87, %f65, %f67, %f78;
	fma.rn.f32 	%f157, %f66, %f68, %f87;
	add.s64 	%rd53, %rd63, %rd7;
	ld.global.nc.u32 	%r64, [%rd53];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r64;
  cvt.f32.f16 %f69, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r64;
  cvt.f32.f16 %f70, high;}

	// end inline asm
	fma.rn.f32 	%f88, %f65, %f69, %f80;
	fma.rn.f32 	%f156, %f66, %f70, %f88;
	ld.global.nc.u32 	%r66, [%rd50+128];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r66;
  cvt.f32.f16 %f71, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r66;
  cvt.f32.f16 %f72, high;}

	// end inline asm
	fma.rn.f32 	%f89, %f65, %f71, %f82;
	fma.rn.f32 	%f155, %f66, %f72, %f89;
	ld.global.nc.u32 	%r68, [%rd51+128];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r68;
  cvt.f32.f16 %f73, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r68;
  cvt.f32.f16 %f74, high;}

	// end inline asm
	fma.rn.f32 	%f90, %f65, %f73, %f84;
	fma.rn.f32 	%f154, %f66, %f74, %f90;
	ld.global.nc.u32 	%r70, [%rd52+128];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r70;
  cvt.f32.f16 %f75, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r70;
  cvt.f32.f16 %f76, high;}

	// end inline asm
	fma.rn.f32 	%f91, %f65, %f75, %f86;
	fma.rn.f32 	%f153, %f66, %f76, %f91;
	add.s64 	%rd63, %rd63, 256;
	add.s64 	%rd62, %rd62, 256;
	add.s32 	%r132, %r132, 64;
	setp.lt.s32 	%p4, %r132, %r9;
	@%p4 bra 	$L__BB68_5;

	st.local.f32 	[%rd2], %f157;
	st.local.f32 	[%rd2+4], %f156;
	st.local.f32 	[%rd2+8], %f155;
	st.local.f32 	[%rd2+12], %f154;
	st.local.f32 	[%rd2+16], %f153;

$L__BB68_7:
	mov.b32 	%r72, %f157;
	mov.u32 	%r73, 31;
	mov.u32 	%r74, 16;
	mov.u32 	%r75, -1;
	shfl.sync.bfly.b32 	%r76|%p5, %r72, %r74, %r73, %r75;
	mov.b32 	%f92, %r76;
	add.f32 	%f93, %f157, %f92;
	mov.b32 	%r77, %f93;
	mov.u32 	%r78, 8;
	shfl.sync.bfly.b32 	%r79|%p6, %r77, %r78, %r73, %r75;
	mov.b32 	%f94, %r79;
	add.f32 	%f95, %f93, %f94;
	mov.b32 	%r80, %f95;
	mov.u32 	%r81, 4;
	shfl.sync.bfly.b32 	%r82|%p7, %r80, %r81, %r73, %r75;
	mov.b32 	%f96, %r82;
	add.f32 	%f97, %f95, %f96;
	mov.b32 	%r83, %f97;
	mov.u32 	%r84, 2;
	shfl.sync.bfly.b32 	%r85|%p8, %r83, %r84, %r73, %r75;
	mov.b32 	%f98, %r85;
	add.f32 	%f99, %f97, %f98;
	mov.b32 	%r86, %f99;
	mov.u32 	%r87, 1;
	shfl.sync.bfly.b32 	%r88|%p9, %r86, %r87, %r73, %r75;
	mov.b32 	%f100, %r88;
	add.f32 	%f101, %f99, %f100;
	st.local.f32 	[%rd2], %f101;
	mov.b32 	%r89, %f156;
	shfl.sync.bfly.b32 	%r90|%p10, %r89, %r74, %r73, %r75;
	mov.b32 	%f102, %r90;
	add.f32 	%f103, %f156, %f102;
	mov.b32 	%r91, %f103;
	shfl.sync.bfly.b32 	%r92|%p11, %r91, %r78, %r73, %r75;
	mov.b32 	%f104, %r92;
	add.f32 	%f105, %f103, %f104;
	mov.b32 	%r93, %f105;
	shfl.sync.bfly.b32 	%r94|%p12, %r93, %r81, %r73, %r75;
	mov.b32 	%f106, %r94;
	add.f32 	%f107, %f105, %f106;
	mov.b32 	%r95, %f107;
	shfl.sync.bfly.b32 	%r96|%p13, %r95, %r84, %r73, %r75;
	mov.b32 	%f108, %r96;
	add.f32 	%f109, %f107, %f108;
	mov.b32 	%r97, %f109;
	shfl.sync.bfly.b32 	%r98|%p14, %r97, %r87, %r73, %r75;
	mov.b32 	%f110, %r98;
	add.f32 	%f111, %f109, %f110;
	st.local.f32 	[%rd2+4], %f111;
	mov.b32 	%r99, %f155;
	shfl.sync.bfly.b32 	%r100|%p15, %r99, %r74, %r73, %r75;
	mov.b32 	%f112, %r100;
	add.f32 	%f113, %f155, %f112;
	mov.b32 	%r101, %f113;
	shfl.sync.bfly.b32 	%r102|%p16, %r101, %r78, %r73, %r75;
	mov.b32 	%f114, %r102;
	add.f32 	%f115, %f113, %f114;
	mov.b32 	%r103, %f115;
	shfl.sync.bfly.b32 	%r104|%p17, %r103, %r81, %r73, %r75;
	mov.b32 	%f116, %r104;
	add.f32 	%f117, %f115, %f116;
	mov.b32 	%r105, %f117;
	shfl.sync.bfly.b32 	%r106|%p18, %r105, %r84, %r73, %r75;
	mov.b32 	%f118, %r106;
	add.f32 	%f119, %f117, %f118;
	mov.b32 	%r107, %f119;
	shfl.sync.bfly.b32 	%r108|%p19, %r107, %r87, %r73, %r75;
	mov.b32 	%f120, %r108;
	add.f32 	%f121, %f119, %f120;
	st.local.f32 	[%rd2+8], %f121;
	mov.b32 	%r109, %f154;
	shfl.sync.bfly.b32 	%r110|%p20, %r109, %r74, %r73, %r75;
	mov.b32 	%f122, %r110;
	add.f32 	%f123, %f154, %f122;
	mov.b32 	%r111, %f123;
	shfl.sync.bfly.b32 	%r112|%p21, %r111, %r78, %r73, %r75;
	mov.b32 	%f124, %r112;
	add.f32 	%f125, %f123, %f124;
	mov.b32 	%r113, %f125;
	shfl.sync.bfly.b32 	%r114|%p22, %r113, %r81, %r73, %r75;
	mov.b32 	%f126, %r114;
	add.f32 	%f127, %f125, %f126;
	mov.b32 	%r115, %f127;
	shfl.sync.bfly.b32 	%r116|%p23, %r115, %r84, %r73, %r75;
	mov.b32 	%f128, %r116;
	add.f32 	%f129, %f127, %f128;
	mov.b32 	%r117, %f129;
	shfl.sync.bfly.b32 	%r118|%p24, %r117, %r87, %r73, %r75;
	mov.b32 	%f130, %r118;
	add.f32 	%f131, %f129, %f130;
	st.local.f32 	[%rd2+12], %f131;
	mov.b32 	%r119, %f153;
	shfl.sync.bfly.b32 	%r120|%p25, %r119, %r74, %r73, %r75;
	mov.b32 	%f132, %r120;
	add.f32 	%f133, %f153, %f132;
	mov.b32 	%r121, %f133;
	shfl.sync.bfly.b32 	%r122|%p26, %r121, %r78, %r73, %r75;
	mov.b32 	%f134, %r122;
	add.f32 	%f135, %f133, %f134;
	mov.b32 	%r123, %f135;
	shfl.sync.bfly.b32 	%r124|%p27, %r123, %r81, %r73, %r75;
	mov.b32 	%f136, %r124;
	add.f32 	%f137, %f135, %f136;
	mov.b32 	%r125, %f137;
	shfl.sync.bfly.b32 	%r126|%p28, %r125, %r84, %r73, %r75;
	mov.b32 	%f138, %r126;
	add.f32 	%f139, %f137, %f138;
	mov.b32 	%r127, %f139;
	shfl.sync.bfly.b32 	%r128|%p29, %r127, %r87, %r73, %r75;
	mov.b32 	%f140, %r128;
	add.f32 	%f141, %f139, %f140;
	st.local.f32 	[%rd2+16], %f141;
	setp.gt.s32 	%p30, %r3, 4;
	@%p30 bra 	$L__BB68_9;

	mad.lo.s32 	%r129, %r3, %r11, %r2;
	cvt.s64.s32 	%rd54, %r129;
	mul.lo.s32 	%r130, %r1, %r12;
	cvt.s64.s32 	%rd55, %r130;
	add.s64 	%rd56, %rd55, %rd54;
	mul.wide.s32 	%rd57, %r3, 4;
	add.s64 	%rd58, %rd2, %rd57;
	ld.local.f32 	%f142, [%rd58];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f142;}

	// end inline asm
	cvta.to.global.u64 	%rd59, %rd18;
	shl.b64 	%rd60, %rd56, 1;
	add.s64 	%rd61, %rd59, %rd60;
	st.global.u16 	[%rd61], %rs1;

$L__BB68_9:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_6_bs_32
.visible .entry ggml_matvec_f16_ncols_6_bs_32(
	.param .u64 ggml_matvec_f16_ncols_6_bs_32_param_0,
	.param .u64 ggml_matvec_f16_ncols_6_bs_32_param_1,
	.param .u64 ggml_matvec_f16_ncols_6_bs_32_param_2,
	.param .u32 ggml_matvec_f16_ncols_6_bs_32_param_3,
	.param .u32 ggml_matvec_f16_ncols_6_bs_32_param_4,
	.param .u32 ggml_matvec_f16_ncols_6_bs_32_param_5,
	.param .u32 ggml_matvec_f16_ncols_6_bs_32_param_6,
	.param .u32 ggml_matvec_f16_ncols_6_bs_32_param_7,
	.param .u32 ggml_matvec_f16_ncols_6_bs_32_param_8,
	.param .u32 ggml_matvec_f16_ncols_6_bs_32_param_9,
	.param .u32 ggml_matvec_f16_ncols_6_bs_32_param_10,
	.param .u32 ggml_matvec_f16_ncols_6_bs_32_param_11
)
{
	.local .align 8 .b8 	__local_depot69[24];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<36>;
	.reg .b16 	%rs<2>;
	.reg .f32 	%f<188>;
	.reg .b32 	%r<147>;
	.reg .b64 	%rd<67>;


	mov.u64 	%SPL, __local_depot69;
	ld.param.u64 	%rd20, [ggml_matvec_f16_ncols_6_bs_32_param_0];
	ld.param.u64 	%rd21, [ggml_matvec_f16_ncols_6_bs_32_param_1];
	ld.param.u64 	%rd19, [ggml_matvec_f16_ncols_6_bs_32_param_2];
	ld.param.u32 	%r9, [ggml_matvec_f16_ncols_6_bs_32_param_3];
	ld.param.u32 	%r13, [ggml_matvec_f16_ncols_6_bs_32_param_5];
	ld.param.u32 	%r10, [ggml_matvec_f16_ncols_6_bs_32_param_6];
	ld.param.u32 	%r11, [ggml_matvec_f16_ncols_6_bs_32_param_7];
	ld.param.u32 	%r14, [ggml_matvec_f16_ncols_6_bs_32_param_8];
	ld.param.u32 	%r15, [ggml_matvec_f16_ncols_6_bs_32_param_9];
	ld.param.u32 	%r16, [ggml_matvec_f16_ncols_6_bs_32_param_10];
	ld.param.u32 	%r12, [ggml_matvec_f16_ncols_6_bs_32_param_11];
	cvta.to.global.u64 	%rd66, %rd21;
	add.u64 	%rd2, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r17, %r1, %r14;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r18, %r2, %r13;
	mad.lo.s32 	%r19, %r17, %r15, %r18;
	cvt.s64.s32 	%rd3, %r19;
	cvta.to.global.u64 	%rd4, %rd20;
	mul.lo.s32 	%r20, %r1, %r16;
	cvt.s64.s32 	%rd5, %r20;
	mov.f32 	%f182, 0f00000000;
	st.local.v2.f32 	[%rd2], {%f182, %f182};
	st.local.v2.f32 	[%rd2+8], {%f182, %f182};
	st.local.v2.f32 	[%rd2+16], {%f182, %f182};
	mov.u32 	%r3, %tid.x;
	setp.ge.s32 	%p1, %r3, %r9;
	mov.f32 	%f183, %f182;
	mov.f32 	%f184, %f182;
	mov.f32 	%f185, %f182;
	mov.f32 	%f186, %f182;
	mov.f32 	%f187, %f182;
	@%p1 bra 	$L__BB69_7;

	not.b32 	%r21, %r3;
	add.s32 	%r4, %r21, %r9;
	and.b32  	%r22, %r4, 32;
	setp.ne.s32 	%p2, %r22, 0;
	mov.f32 	%f182, 0f00000000;
	mov.u32 	%r146, %r3;
	@%p2 bra 	$L__BB69_3;

	shl.b64 	%rd23, %rd5, 1;
	add.s64 	%rd24, %rd66, %rd23;
	shl.b64 	%rd25, %rd3, 1;
	add.s64 	%rd26, %rd4, %rd25;
	mul.wide.s32 	%rd27, %r3, 4;
	add.s64 	%rd28, %rd26, %rd27;
	ld.global.nc.u32 	%r23, [%rd28];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r23;
  cvt.f32.f16 %f43, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r23;
  cvt.f32.f16 %f44, high;}

	// end inline asm
	add.s64 	%rd29, %rd24, %rd27;
	ld.global.nc.u32 	%r25, [%rd29];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r25;
  cvt.f32.f16 %f45, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r25;
  cvt.f32.f16 %f46, high;}

	// end inline asm
	fma.rn.f32 	%f57, %f43, %f45, 0f00000000;
	fma.rn.f32 	%f187, %f44, %f46, %f57;
	st.local.f32 	[%rd2], %f187;
	mul.wide.s32 	%rd30, %r10, 4;
	add.s64 	%rd31, %rd29, %rd30;
	ld.global.nc.u32 	%r27, [%rd31];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r27;
  cvt.f32.f16 %f47, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r27;
  cvt.f32.f16 %f48, high;}

	// end inline asm
	fma.rn.f32 	%f58, %f43, %f47, 0f00000000;
	fma.rn.f32 	%f186, %f44, %f48, %f58;
	st.local.f32 	[%rd2+4], %f186;
	add.s32 	%r37, %r3, %r10;
	add.s32 	%r38, %r37, %r10;
	mul.wide.s32 	%rd32, %r38, 4;
	add.s64 	%rd33, %rd24, %rd32;
	ld.global.nc.u32 	%r29, [%rd33];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r29;
  cvt.f32.f16 %f49, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r29;
  cvt.f32.f16 %f50, high;}

	// end inline asm
	fma.rn.f32 	%f59, %f43, %f49, 0f00000000;
	fma.rn.f32 	%f185, %f44, %f50, %f59;
	st.local.f32 	[%rd2+8], %f185;
	add.s64 	%rd34, %rd33, %rd30;
	ld.global.nc.u32 	%r31, [%rd34];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r31;
  cvt.f32.f16 %f51, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r31;
  cvt.f32.f16 %f52, high;}

	// end inline asm
	fma.rn.f32 	%f60, %f43, %f51, 0f00000000;
	fma.rn.f32 	%f184, %f44, %f52, %f60;
	st.local.f32 	[%rd2+12], %f184;
	add.s64 	%rd35, %rd34, %rd30;
	ld.global.nc.u32 	%r33, [%rd35];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r33;
  cvt.f32.f16 %f53, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r33;
  cvt.f32.f16 %f54, high;}

	// end inline asm
	fma.rn.f32 	%f61, %f43, %f53, 0f00000000;
	fma.rn.f32 	%f183, %f44, %f54, %f61;
	st.local.f32 	[%rd2+16], %f183;
	add.s64 	%rd36, %rd35, %rd30;
	ld.global.nc.u32 	%r35, [%rd36];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r35;
  cvt.f32.f16 %f55, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r35;
  cvt.f32.f16 %f56, high;}

	// end inline asm
	fma.rn.f32 	%f62, %f43, %f55, 0f00000000;
	fma.rn.f32 	%f182, %f44, %f56, %f62;
	st.local.f32 	[%rd2+20], %f182;
	add.s32 	%r146, %r3, 32;

$L__BB69_3:
	and.b32  	%r39, %r4, -32;
	setp.eq.s32 	%p3, %r39, 0;
	@%p3 bra 	$L__BB69_7;

	add.s32 	%r40, %r146, %r10;
	add.s32 	%r41, %r40, 32;
	mul.wide.s32 	%rd37, %r41, 4;
	shl.b64 	%rd38, %rd5, 1;
	add.s64 	%rd7, %rd37, %rd38;
	shl.b32 	%r42, %r10, 1;
	add.s32 	%r43, %r146, %r42;
	mad.lo.s32 	%r44, %r10, 3, %r146;
	shl.b32 	%r45, %r10, 2;
	add.s32 	%r46, %r146, %r45;
	mad.lo.s32 	%r47, %r10, 5, %r146;
	mul.wide.s32 	%rd39, %r43, 4;
	add.s64 	%rd8, %rd39, %rd38;
	mul.wide.s32 	%rd40, %r44, 4;
	add.s64 	%rd9, %rd40, %rd38;
	mul.wide.s32 	%rd41, %r46, 4;
	add.s64 	%rd10, %rd41, %rd38;
	mul.wide.s32 	%rd42, %r47, 4;
	add.s64 	%rd11, %rd42, %rd38;
	mul.wide.s32 	%rd43, %r146, 2;
	add.s64 	%rd44, %rd43, %rd3;
	shl.b64 	%rd45, %rd44, 1;
	add.s64 	%rd46, %rd4, %rd45;
	add.s64 	%rd65, %rd46, 128;
	mul.wide.s32 	%rd47, %r146, 4;
	mul.wide.s32 	%rd48, %r10, 4;
	add.s64 	%rd49, %rd47, %rd48;
	add.s64 	%rd13, %rd49, %rd38;
	add.s64 	%rd14, %rd47, %rd38;

$L__BB69_5:
	ld.global.nc.u32 	%r48, [%rd65+-128];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r48;
  cvt.f32.f16 %f63, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r48;
  cvt.f32.f16 %f64, high;}

	// end inline asm
	add.s64 	%rd50, %rd66, %rd14;
	ld.global.nc.u32 	%r50, [%rd50];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r50;
  cvt.f32.f16 %f65, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r50;
  cvt.f32.f16 %f66, high;}

	// end inline asm
	fma.rn.f32 	%f91, %f63, %f65, %f187;
	fma.rn.f32 	%f92, %f64, %f66, %f91;
	add.s64 	%rd51, %rd66, %rd13;
	ld.global.nc.u32 	%r52, [%rd51];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r52;
  cvt.f32.f16 %f67, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r52;
  cvt.f32.f16 %f68, high;}

	// end inline asm
	fma.rn.f32 	%f93, %f63, %f67, %f186;
	fma.rn.f32 	%f94, %f64, %f68, %f93;
	add.s64 	%rd52, %rd66, %rd8;
	ld.global.nc.u32 	%r54, [%rd52];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r54;
  cvt.f32.f16 %f69, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r54;
  cvt.f32.f16 %f70, high;}

	// end inline asm
	fma.rn.f32 	%f95, %f63, %f69, %f185;
	fma.rn.f32 	%f96, %f64, %f70, %f95;
	add.s64 	%rd53, %rd66, %rd9;
	ld.global.nc.u32 	%r56, [%rd53];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r56;
  cvt.f32.f16 %f71, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r56;
  cvt.f32.f16 %f72, high;}

	// end inline asm
	fma.rn.f32 	%f97, %f63, %f71, %f184;
	fma.rn.f32 	%f98, %f64, %f72, %f97;
	add.s64 	%rd54, %rd66, %rd10;
	ld.global.nc.u32 	%r58, [%rd54];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r58;
  cvt.f32.f16 %f73, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r58;
  cvt.f32.f16 %f74, high;}

	// end inline asm
	fma.rn.f32 	%f99, %f63, %f73, %f183;
	fma.rn.f32 	%f100, %f64, %f74, %f99;
	add.s64 	%rd55, %rd66, %rd11;
	ld.global.nc.u32 	%r60, [%rd55];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r60;
  cvt.f32.f16 %f75, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r60;
  cvt.f32.f16 %f76, high;}

	// end inline asm
	fma.rn.f32 	%f101, %f63, %f75, %f182;
	fma.rn.f32 	%f102, %f64, %f76, %f101;
	ld.global.nc.u32 	%r62, [%rd65];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r62;
  cvt.f32.f16 %f77, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r62;
  cvt.f32.f16 %f78, high;}

	// end inline asm
	ld.global.nc.u32 	%r64, [%rd50+128];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r64;
  cvt.f32.f16 %f79, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r64;
  cvt.f32.f16 %f80, high;}

	// end inline asm
	fma.rn.f32 	%f103, %f77, %f79, %f92;
	fma.rn.f32 	%f187, %f78, %f80, %f103;
	add.s64 	%rd56, %rd66, %rd7;
	ld.global.nc.u32 	%r66, [%rd56];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r66;
  cvt.f32.f16 %f81, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r66;
  cvt.f32.f16 %f82, high;}

	// end inline asm
	fma.rn.f32 	%f104, %f77, %f81, %f94;
	fma.rn.f32 	%f186, %f78, %f82, %f104;
	ld.global.nc.u32 	%r68, [%rd52+128];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r68;
  cvt.f32.f16 %f83, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r68;
  cvt.f32.f16 %f84, high;}

	// end inline asm
	fma.rn.f32 	%f105, %f77, %f83, %f96;
	fma.rn.f32 	%f185, %f78, %f84, %f105;
	ld.global.nc.u32 	%r70, [%rd53+128];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r70;
  cvt.f32.f16 %f85, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r70;
  cvt.f32.f16 %f86, high;}

	// end inline asm
	fma.rn.f32 	%f106, %f77, %f85, %f98;
	fma.rn.f32 	%f184, %f78, %f86, %f106;
	ld.global.nc.u32 	%r72, [%rd54+128];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r72;
  cvt.f32.f16 %f87, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r72;
  cvt.f32.f16 %f88, high;}

	// end inline asm
	fma.rn.f32 	%f107, %f77, %f87, %f100;
	fma.rn.f32 	%f183, %f78, %f88, %f107;
	ld.global.nc.u32 	%r74, [%rd55+128];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r74;
  cvt.f32.f16 %f89, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r74;
  cvt.f32.f16 %f90, high;}

	// end inline asm
	fma.rn.f32 	%f108, %f77, %f89, %f102;
	fma.rn.f32 	%f182, %f78, %f90, %f108;
	add.s64 	%rd66, %rd66, 256;
	add.s64 	%rd65, %rd65, 256;
	add.s32 	%r146, %r146, 64;
	setp.lt.s32 	%p4, %r146, %r9;
	@%p4 bra 	$L__BB69_5;

	st.local.v2.f32 	[%rd2], {%f187, %f186};
	st.local.v2.f32 	[%rd2+8], {%f185, %f184};
	st.local.v2.f32 	[%rd2+16], {%f183, %f182};

$L__BB69_7:
	mov.b32 	%r76, %f187;
	mov.u32 	%r77, 31;
	mov.u32 	%r78, 16;
	mov.u32 	%r79, -1;
	shfl.sync.bfly.b32 	%r80|%p5, %r76, %r78, %r77, %r79;
	mov.b32 	%f109, %r80;
	add.f32 	%f110, %f187, %f109;
	mov.b32 	%r81, %f110;
	mov.u32 	%r82, 8;
	shfl.sync.bfly.b32 	%r83|%p6, %r81, %r82, %r77, %r79;
	mov.b32 	%f111, %r83;
	add.f32 	%f112, %f110, %f111;
	mov.b32 	%r84, %f112;
	mov.u32 	%r85, 4;
	shfl.sync.bfly.b32 	%r86|%p7, %r84, %r85, %r77, %r79;
	mov.b32 	%f113, %r86;
	add.f32 	%f114, %f112, %f113;
	mov.b32 	%r87, %f114;
	mov.u32 	%r88, 2;
	shfl.sync.bfly.b32 	%r89|%p8, %r87, %r88, %r77, %r79;
	mov.b32 	%f115, %r89;
	add.f32 	%f116, %f114, %f115;
	mov.b32 	%r90, %f116;
	mov.u32 	%r91, 1;
	shfl.sync.bfly.b32 	%r92|%p9, %r90, %r91, %r77, %r79;
	mov.b32 	%f117, %r92;
	add.f32 	%f118, %f116, %f117;
	st.local.f32 	[%rd2], %f118;
	mov.b32 	%r93, %f186;
	shfl.sync.bfly.b32 	%r94|%p10, %r93, %r78, %r77, %r79;
	mov.b32 	%f119, %r94;
	add.f32 	%f120, %f186, %f119;
	mov.b32 	%r95, %f120;
	shfl.sync.bfly.b32 	%r96|%p11, %r95, %r82, %r77, %r79;
	mov.b32 	%f121, %r96;
	add.f32 	%f122, %f120, %f121;
	mov.b32 	%r97, %f122;
	shfl.sync.bfly.b32 	%r98|%p12, %r97, %r85, %r77, %r79;
	mov.b32 	%f123, %r98;
	add.f32 	%f124, %f122, %f123;
	mov.b32 	%r99, %f124;
	shfl.sync.bfly.b32 	%r100|%p13, %r99, %r88, %r77, %r79;
	mov.b32 	%f125, %r100;
	add.f32 	%f126, %f124, %f125;
	mov.b32 	%r101, %f126;
	shfl.sync.bfly.b32 	%r102|%p14, %r101, %r91, %r77, %r79;
	mov.b32 	%f127, %r102;
	add.f32 	%f128, %f126, %f127;
	st.local.f32 	[%rd2+4], %f128;
	mov.b32 	%r103, %f185;
	shfl.sync.bfly.b32 	%r104|%p15, %r103, %r78, %r77, %r79;
	mov.b32 	%f129, %r104;
	add.f32 	%f130, %f185, %f129;
	mov.b32 	%r105, %f130;
	shfl.sync.bfly.b32 	%r106|%p16, %r105, %r82, %r77, %r79;
	mov.b32 	%f131, %r106;
	add.f32 	%f132, %f130, %f131;
	mov.b32 	%r107, %f132;
	shfl.sync.bfly.b32 	%r108|%p17, %r107, %r85, %r77, %r79;
	mov.b32 	%f133, %r108;
	add.f32 	%f134, %f132, %f133;
	mov.b32 	%r109, %f134;
	shfl.sync.bfly.b32 	%r110|%p18, %r109, %r88, %r77, %r79;
	mov.b32 	%f135, %r110;
	add.f32 	%f136, %f134, %f135;
	mov.b32 	%r111, %f136;
	shfl.sync.bfly.b32 	%r112|%p19, %r111, %r91, %r77, %r79;
	mov.b32 	%f137, %r112;
	add.f32 	%f138, %f136, %f137;
	st.local.f32 	[%rd2+8], %f138;
	mov.b32 	%r113, %f184;
	shfl.sync.bfly.b32 	%r114|%p20, %r113, %r78, %r77, %r79;
	mov.b32 	%f139, %r114;
	add.f32 	%f140, %f184, %f139;
	mov.b32 	%r115, %f140;
	shfl.sync.bfly.b32 	%r116|%p21, %r115, %r82, %r77, %r79;
	mov.b32 	%f141, %r116;
	add.f32 	%f142, %f140, %f141;
	mov.b32 	%r117, %f142;
	shfl.sync.bfly.b32 	%r118|%p22, %r117, %r85, %r77, %r79;
	mov.b32 	%f143, %r118;
	add.f32 	%f144, %f142, %f143;
	mov.b32 	%r119, %f144;
	shfl.sync.bfly.b32 	%r120|%p23, %r119, %r88, %r77, %r79;
	mov.b32 	%f145, %r120;
	add.f32 	%f146, %f144, %f145;
	mov.b32 	%r121, %f146;
	shfl.sync.bfly.b32 	%r122|%p24, %r121, %r91, %r77, %r79;
	mov.b32 	%f147, %r122;
	add.f32 	%f148, %f146, %f147;
	st.local.f32 	[%rd2+12], %f148;
	mov.b32 	%r123, %f183;
	shfl.sync.bfly.b32 	%r124|%p25, %r123, %r78, %r77, %r79;
	mov.b32 	%f149, %r124;
	add.f32 	%f150, %f183, %f149;
	mov.b32 	%r125, %f150;
	shfl.sync.bfly.b32 	%r126|%p26, %r125, %r82, %r77, %r79;
	mov.b32 	%f151, %r126;
	add.f32 	%f152, %f150, %f151;
	mov.b32 	%r127, %f152;
	shfl.sync.bfly.b32 	%r128|%p27, %r127, %r85, %r77, %r79;
	mov.b32 	%f153, %r128;
	add.f32 	%f154, %f152, %f153;
	mov.b32 	%r129, %f154;
	shfl.sync.bfly.b32 	%r130|%p28, %r129, %r88, %r77, %r79;
	mov.b32 	%f155, %r130;
	add.f32 	%f156, %f154, %f155;
	mov.b32 	%r131, %f156;
	shfl.sync.bfly.b32 	%r132|%p29, %r131, %r91, %r77, %r79;
	mov.b32 	%f157, %r132;
	add.f32 	%f158, %f156, %f157;
	st.local.f32 	[%rd2+16], %f158;
	mov.b32 	%r133, %f182;
	shfl.sync.bfly.b32 	%r134|%p30, %r133, %r78, %r77, %r79;
	mov.b32 	%f159, %r134;
	add.f32 	%f160, %f182, %f159;
	mov.b32 	%r135, %f160;
	shfl.sync.bfly.b32 	%r136|%p31, %r135, %r82, %r77, %r79;
	mov.b32 	%f161, %r136;
	add.f32 	%f162, %f160, %f161;
	mov.b32 	%r137, %f162;
	shfl.sync.bfly.b32 	%r138|%p32, %r137, %r85, %r77, %r79;
	mov.b32 	%f163, %r138;
	add.f32 	%f164, %f162, %f163;
	mov.b32 	%r139, %f164;
	shfl.sync.bfly.b32 	%r140|%p33, %r139, %r88, %r77, %r79;
	mov.b32 	%f165, %r140;
	add.f32 	%f166, %f164, %f165;
	mov.b32 	%r141, %f166;
	shfl.sync.bfly.b32 	%r142|%p34, %r141, %r91, %r77, %r79;
	mov.b32 	%f167, %r142;
	add.f32 	%f168, %f166, %f167;
	st.local.f32 	[%rd2+20], %f168;
	setp.gt.s32 	%p35, %r3, 5;
	@%p35 bra 	$L__BB69_9;

	mad.lo.s32 	%r143, %r3, %r11, %r2;
	cvt.s64.s32 	%rd57, %r143;
	mul.lo.s32 	%r144, %r1, %r12;
	cvt.s64.s32 	%rd58, %r144;
	add.s64 	%rd59, %rd58, %rd57;
	mul.wide.s32 	%rd60, %r3, 4;
	add.s64 	%rd61, %rd2, %rd60;
	ld.local.f32 	%f169, [%rd61];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f169;}

	// end inline asm
	cvta.to.global.u64 	%rd62, %rd19;
	shl.b64 	%rd63, %rd59, 1;
	add.s64 	%rd64, %rd62, %rd63;
	st.global.u16 	[%rd64], %rs1;

$L__BB69_9:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_7_bs_32
.visible .entry ggml_matvec_f16_ncols_7_bs_32(
	.param .u64 ggml_matvec_f16_ncols_7_bs_32_param_0,
	.param .u64 ggml_matvec_f16_ncols_7_bs_32_param_1,
	.param .u64 ggml_matvec_f16_ncols_7_bs_32_param_2,
	.param .u32 ggml_matvec_f16_ncols_7_bs_32_param_3,
	.param .u32 ggml_matvec_f16_ncols_7_bs_32_param_4,
	.param .u32 ggml_matvec_f16_ncols_7_bs_32_param_5,
	.param .u32 ggml_matvec_f16_ncols_7_bs_32_param_6,
	.param .u32 ggml_matvec_f16_ncols_7_bs_32_param_7,
	.param .u32 ggml_matvec_f16_ncols_7_bs_32_param_8,
	.param .u32 ggml_matvec_f16_ncols_7_bs_32_param_9,
	.param .u32 ggml_matvec_f16_ncols_7_bs_32_param_10,
	.param .u32 ggml_matvec_f16_ncols_7_bs_32_param_11
)
{
	.local .align 4 .b8 	__local_depot70[28];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<39>;
	.reg .b16 	%rs<2>;
	.reg .f32 	%f<144>;
	.reg .b32 	%r<117>;
	.reg .b64 	%rd<43>;


	mov.u64 	%SPL, __local_depot70;
	ld.param.u64 	%rd13, [ggml_matvec_f16_ncols_7_bs_32_param_0];
	ld.param.u64 	%rd14, [ggml_matvec_f16_ncols_7_bs_32_param_1];
	ld.param.u64 	%rd15, [ggml_matvec_f16_ncols_7_bs_32_param_2];
	ld.param.u32 	%r6, [ggml_matvec_f16_ncols_7_bs_32_param_3];
	ld.param.u32 	%r7, [ggml_matvec_f16_ncols_7_bs_32_param_5];
	ld.param.u32 	%r8, [ggml_matvec_f16_ncols_7_bs_32_param_6];
	ld.param.u32 	%r9, [ggml_matvec_f16_ncols_7_bs_32_param_7];
	ld.param.u32 	%r10, [ggml_matvec_f16_ncols_7_bs_32_param_8];
	ld.param.u32 	%r11, [ggml_matvec_f16_ncols_7_bs_32_param_9];
	ld.param.u32 	%r12, [ggml_matvec_f16_ncols_7_bs_32_param_10];
	ld.param.u32 	%r13, [ggml_matvec_f16_ncols_7_bs_32_param_11];
	add.u64 	%rd1, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	mov.u32 	%r2, %ctaid.x;
	mov.f32 	%f137, 0f00000000;
	mov.u32 	%r14, 0;
	st.local.u32 	[%rd1], %r14;
	st.local.u32 	[%rd1+4], %r14;
	st.local.u32 	[%rd1+8], %r14;
	st.local.u32 	[%rd1+12], %r14;
	st.local.u32 	[%rd1+16], %r14;
	st.local.u32 	[%rd1+20], %r14;
	st.local.u32 	[%rd1+24], %r14;
	mov.u32 	%r3, %tid.x;
	setp.ge.s32 	%p1, %r3, %r6;
	mov.f32 	%f138, %f137;
	mov.f32 	%f139, %f137;
	mov.f32 	%f140, %f137;
	mov.f32 	%f141, %f137;
	mov.f32 	%f142, %f137;
	mov.f32 	%f143, %f137;
	@%p1 bra 	$L__BB70_4;

	shl.b32 	%r15, %r8, 1;
	add.s32 	%r16, %r3, %r15;
	mul.wide.s32 	%rd17, %r16, 4;
	mul.lo.s32 	%r17, %r1, %r12;
	mul.wide.s32 	%rd18, %r17, 2;
	add.s64 	%rd3, %rd17, %rd18;
	mul.wide.s32 	%rd19, %r3, 4;
	mul.wide.s32 	%rd4, %r8, 4;
	add.s64 	%rd20, %rd19, %rd4;
	add.s64 	%rd5, %rd20, %rd18;
	add.s64 	%rd6, %rd19, %rd18;
	mul.wide.s32 	%rd21, %r3, 2;
	mul.lo.s32 	%r18, %r2, %r7;
	div.s32 	%r19, %r1, %r10;
	mad.lo.s32 	%r20, %r19, %r11, %r18;
	cvt.s64.s32 	%rd22, %r20;
	add.s64 	%rd23, %rd21, %rd22;
	cvta.to.global.u64 	%rd24, %rd13;
	shl.b64 	%rd25, %rd23, 1;
	add.s64 	%rd41, %rd24, %rd25;
	cvta.to.global.u64 	%rd42, %rd14;
	mov.f32 	%f137, 0f00000000;
	mov.u32 	%r116, %r3;

$L__BB70_2:
	ld.global.nc.u32 	%r21, [%rd41];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r21;
  cvt.f32.f16 %f36, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r21;
  cvt.f32.f16 %f37, high;}

	// end inline asm
	add.s64 	%rd26, %rd42, %rd6;
	ld.global.nc.u32 	%r23, [%rd26];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r23;
  cvt.f32.f16 %f38, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r23;
  cvt.f32.f16 %f39, high;}

	// end inline asm
	fma.rn.f32 	%f52, %f36, %f38, %f143;
	fma.rn.f32 	%f143, %f37, %f39, %f52;
	add.s64 	%rd27, %rd42, %rd5;
	ld.global.nc.u32 	%r25, [%rd27];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r25;
  cvt.f32.f16 %f40, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r25;
  cvt.f32.f16 %f41, high;}

	// end inline asm
	fma.rn.f32 	%f53, %f36, %f40, %f142;
	fma.rn.f32 	%f142, %f37, %f41, %f53;
	add.s64 	%rd28, %rd42, %rd3;
	ld.global.nc.u32 	%r27, [%rd28];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r27;
  cvt.f32.f16 %f42, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r27;
  cvt.f32.f16 %f43, high;}

	// end inline asm
	fma.rn.f32 	%f54, %f36, %f42, %f141;
	fma.rn.f32 	%f141, %f37, %f43, %f54;
	add.s64 	%rd29, %rd28, %rd4;
	ld.global.nc.u32 	%r29, [%rd29];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r29;
  cvt.f32.f16 %f44, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r29;
  cvt.f32.f16 %f45, high;}

	// end inline asm
	fma.rn.f32 	%f55, %f36, %f44, %f140;
	fma.rn.f32 	%f140, %f37, %f45, %f55;
	add.s64 	%rd30, %rd29, %rd4;
	ld.global.nc.u32 	%r31, [%rd30];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r31;
  cvt.f32.f16 %f46, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r31;
  cvt.f32.f16 %f47, high;}

	// end inline asm
	fma.rn.f32 	%f56, %f36, %f46, %f139;
	fma.rn.f32 	%f139, %f37, %f47, %f56;
	add.s64 	%rd31, %rd30, %rd4;
	ld.global.nc.u32 	%r33, [%rd31];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r33;
  cvt.f32.f16 %f48, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r33;
  cvt.f32.f16 %f49, high;}

	// end inline asm
	fma.rn.f32 	%f57, %f36, %f48, %f138;
	fma.rn.f32 	%f138, %f37, %f49, %f57;
	add.s64 	%rd32, %rd31, %rd4;
	ld.global.nc.u32 	%r35, [%rd32];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r35;
  cvt.f32.f16 %f50, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r35;
  cvt.f32.f16 %f51, high;}

	// end inline asm
	fma.rn.f32 	%f58, %f36, %f50, %f137;
	fma.rn.f32 	%f137, %f37, %f51, %f58;
	add.s64 	%rd42, %rd42, 128;
	add.s64 	%rd41, %rd41, 128;
	add.s32 	%r116, %r116, 32;
	setp.lt.s32 	%p2, %r116, %r6;
	@%p2 bra 	$L__BB70_2;

	st.local.f32 	[%rd1], %f143;
	st.local.f32 	[%rd1+4], %f142;
	st.local.f32 	[%rd1+8], %f141;
	st.local.f32 	[%rd1+12], %f140;
	st.local.f32 	[%rd1+16], %f139;
	st.local.f32 	[%rd1+20], %f138;
	st.local.f32 	[%rd1+24], %f137;

$L__BB70_4:
	mov.b32 	%r37, %f143;
	mov.u32 	%r38, 31;
	mov.u32 	%r39, 16;
	mov.u32 	%r40, -1;
	shfl.sync.bfly.b32 	%r41|%p3, %r37, %r39, %r38, %r40;
	mov.b32 	%f59, %r41;
	add.f32 	%f60, %f143, %f59;
	mov.b32 	%r42, %f60;
	mov.u32 	%r43, 8;
	shfl.sync.bfly.b32 	%r44|%p4, %r42, %r43, %r38, %r40;
	mov.b32 	%f61, %r44;
	add.f32 	%f62, %f60, %f61;
	mov.b32 	%r45, %f62;
	mov.u32 	%r46, 4;
	shfl.sync.bfly.b32 	%r47|%p5, %r45, %r46, %r38, %r40;
	mov.b32 	%f63, %r47;
	add.f32 	%f64, %f62, %f63;
	mov.b32 	%r48, %f64;
	mov.u32 	%r49, 2;
	shfl.sync.bfly.b32 	%r50|%p6, %r48, %r49, %r38, %r40;
	mov.b32 	%f65, %r50;
	add.f32 	%f66, %f64, %f65;
	mov.b32 	%r51, %f66;
	mov.u32 	%r52, 1;
	shfl.sync.bfly.b32 	%r53|%p7, %r51, %r52, %r38, %r40;
	mov.b32 	%f67, %r53;
	add.f32 	%f68, %f66, %f67;
	st.local.f32 	[%rd1], %f68;
	mov.b32 	%r54, %f142;
	shfl.sync.bfly.b32 	%r55|%p8, %r54, %r39, %r38, %r40;
	mov.b32 	%f69, %r55;
	add.f32 	%f70, %f142, %f69;
	mov.b32 	%r56, %f70;
	shfl.sync.bfly.b32 	%r57|%p9, %r56, %r43, %r38, %r40;
	mov.b32 	%f71, %r57;
	add.f32 	%f72, %f70, %f71;
	mov.b32 	%r58, %f72;
	shfl.sync.bfly.b32 	%r59|%p10, %r58, %r46, %r38, %r40;
	mov.b32 	%f73, %r59;
	add.f32 	%f74, %f72, %f73;
	mov.b32 	%r60, %f74;
	shfl.sync.bfly.b32 	%r61|%p11, %r60, %r49, %r38, %r40;
	mov.b32 	%f75, %r61;
	add.f32 	%f76, %f74, %f75;
	mov.b32 	%r62, %f76;
	shfl.sync.bfly.b32 	%r63|%p12, %r62, %r52, %r38, %r40;
	mov.b32 	%f77, %r63;
	add.f32 	%f78, %f76, %f77;
	st.local.f32 	[%rd1+4], %f78;
	mov.b32 	%r64, %f141;
	shfl.sync.bfly.b32 	%r65|%p13, %r64, %r39, %r38, %r40;
	mov.b32 	%f79, %r65;
	add.f32 	%f80, %f141, %f79;
	mov.b32 	%r66, %f80;
	shfl.sync.bfly.b32 	%r67|%p14, %r66, %r43, %r38, %r40;
	mov.b32 	%f81, %r67;
	add.f32 	%f82, %f80, %f81;
	mov.b32 	%r68, %f82;
	shfl.sync.bfly.b32 	%r69|%p15, %r68, %r46, %r38, %r40;
	mov.b32 	%f83, %r69;
	add.f32 	%f84, %f82, %f83;
	mov.b32 	%r70, %f84;
	shfl.sync.bfly.b32 	%r71|%p16, %r70, %r49, %r38, %r40;
	mov.b32 	%f85, %r71;
	add.f32 	%f86, %f84, %f85;
	mov.b32 	%r72, %f86;
	shfl.sync.bfly.b32 	%r73|%p17, %r72, %r52, %r38, %r40;
	mov.b32 	%f87, %r73;
	add.f32 	%f88, %f86, %f87;
	st.local.f32 	[%rd1+8], %f88;
	mov.b32 	%r74, %f140;
	shfl.sync.bfly.b32 	%r75|%p18, %r74, %r39, %r38, %r40;
	mov.b32 	%f89, %r75;
	add.f32 	%f90, %f140, %f89;
	mov.b32 	%r76, %f90;
	shfl.sync.bfly.b32 	%r77|%p19, %r76, %r43, %r38, %r40;
	mov.b32 	%f91, %r77;
	add.f32 	%f92, %f90, %f91;
	mov.b32 	%r78, %f92;
	shfl.sync.bfly.b32 	%r79|%p20, %r78, %r46, %r38, %r40;
	mov.b32 	%f93, %r79;
	add.f32 	%f94, %f92, %f93;
	mov.b32 	%r80, %f94;
	shfl.sync.bfly.b32 	%r81|%p21, %r80, %r49, %r38, %r40;
	mov.b32 	%f95, %r81;
	add.f32 	%f96, %f94, %f95;
	mov.b32 	%r82, %f96;
	shfl.sync.bfly.b32 	%r83|%p22, %r82, %r52, %r38, %r40;
	mov.b32 	%f97, %r83;
	add.f32 	%f98, %f96, %f97;
	st.local.f32 	[%rd1+12], %f98;
	mov.b32 	%r84, %f139;
	shfl.sync.bfly.b32 	%r85|%p23, %r84, %r39, %r38, %r40;
	mov.b32 	%f99, %r85;
	add.f32 	%f100, %f139, %f99;
	mov.b32 	%r86, %f100;
	shfl.sync.bfly.b32 	%r87|%p24, %r86, %r43, %r38, %r40;
	mov.b32 	%f101, %r87;
	add.f32 	%f102, %f100, %f101;
	mov.b32 	%r88, %f102;
	shfl.sync.bfly.b32 	%r89|%p25, %r88, %r46, %r38, %r40;
	mov.b32 	%f103, %r89;
	add.f32 	%f104, %f102, %f103;
	mov.b32 	%r90, %f104;
	shfl.sync.bfly.b32 	%r91|%p26, %r90, %r49, %r38, %r40;
	mov.b32 	%f105, %r91;
	add.f32 	%f106, %f104, %f105;
	mov.b32 	%r92, %f106;
	shfl.sync.bfly.b32 	%r93|%p27, %r92, %r52, %r38, %r40;
	mov.b32 	%f107, %r93;
	add.f32 	%f108, %f106, %f107;
	st.local.f32 	[%rd1+16], %f108;
	mov.b32 	%r94, %f138;
	shfl.sync.bfly.b32 	%r95|%p28, %r94, %r39, %r38, %r40;
	mov.b32 	%f109, %r95;
	add.f32 	%f110, %f138, %f109;
	mov.b32 	%r96, %f110;
	shfl.sync.bfly.b32 	%r97|%p29, %r96, %r43, %r38, %r40;
	mov.b32 	%f111, %r97;
	add.f32 	%f112, %f110, %f111;
	mov.b32 	%r98, %f112;
	shfl.sync.bfly.b32 	%r99|%p30, %r98, %r46, %r38, %r40;
	mov.b32 	%f113, %r99;
	add.f32 	%f114, %f112, %f113;
	mov.b32 	%r100, %f114;
	shfl.sync.bfly.b32 	%r101|%p31, %r100, %r49, %r38, %r40;
	mov.b32 	%f115, %r101;
	add.f32 	%f116, %f114, %f115;
	mov.b32 	%r102, %f116;
	shfl.sync.bfly.b32 	%r103|%p32, %r102, %r52, %r38, %r40;
	mov.b32 	%f117, %r103;
	add.f32 	%f118, %f116, %f117;
	st.local.f32 	[%rd1+20], %f118;
	mov.b32 	%r104, %f137;
	shfl.sync.bfly.b32 	%r105|%p33, %r104, %r39, %r38, %r40;
	mov.b32 	%f119, %r105;
	add.f32 	%f120, %f137, %f119;
	mov.b32 	%r106, %f120;
	shfl.sync.bfly.b32 	%r107|%p34, %r106, %r43, %r38, %r40;
	mov.b32 	%f121, %r107;
	add.f32 	%f122, %f120, %f121;
	mov.b32 	%r108, %f122;
	shfl.sync.bfly.b32 	%r109|%p35, %r108, %r46, %r38, %r40;
	mov.b32 	%f123, %r109;
	add.f32 	%f124, %f122, %f123;
	mov.b32 	%r110, %f124;
	shfl.sync.bfly.b32 	%r111|%p36, %r110, %r49, %r38, %r40;
	mov.b32 	%f125, %r111;
	add.f32 	%f126, %f124, %f125;
	mov.b32 	%r112, %f126;
	shfl.sync.bfly.b32 	%r113|%p37, %r112, %r52, %r38, %r40;
	mov.b32 	%f127, %r113;
	add.f32 	%f128, %f126, %f127;
	st.local.f32 	[%rd1+24], %f128;
	setp.gt.s32 	%p38, %r3, 6;
	@%p38 bra 	$L__BB70_6;

	mad.lo.s32 	%r114, %r3, %r9, %r2;
	cvt.s64.s32 	%rd33, %r114;
	mul.lo.s32 	%r115, %r1, %r13;
	cvt.s64.s32 	%rd34, %r115;
	add.s64 	%rd35, %rd34, %rd33;
	mul.wide.s32 	%rd36, %r3, 4;
	add.s64 	%rd37, %rd1, %rd36;
	ld.local.f32 	%f129, [%rd37];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f129;}

	// end inline asm
	cvta.to.global.u64 	%rd38, %rd15;
	shl.b64 	%rd39, %rd35, 1;
	add.s64 	%rd40, %rd38, %rd39;
	st.global.u16 	[%rd40], %rs1;

$L__BB70_6:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_8_bs_32
.visible .entry ggml_matvec_f16_ncols_8_bs_32(
	.param .u64 ggml_matvec_f16_ncols_8_bs_32_param_0,
	.param .u64 ggml_matvec_f16_ncols_8_bs_32_param_1,
	.param .u64 ggml_matvec_f16_ncols_8_bs_32_param_2,
	.param .u32 ggml_matvec_f16_ncols_8_bs_32_param_3,
	.param .u32 ggml_matvec_f16_ncols_8_bs_32_param_4,
	.param .u32 ggml_matvec_f16_ncols_8_bs_32_param_5,
	.param .u32 ggml_matvec_f16_ncols_8_bs_32_param_6,
	.param .u32 ggml_matvec_f16_ncols_8_bs_32_param_7,
	.param .u32 ggml_matvec_f16_ncols_8_bs_32_param_8,
	.param .u32 ggml_matvec_f16_ncols_8_bs_32_param_9,
	.param .u32 ggml_matvec_f16_ncols_8_bs_32_param_10,
	.param .u32 ggml_matvec_f16_ncols_8_bs_32_param_11
)
{
	.local .align 16 .b8 	__local_depot71[32];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<44>;
	.reg .b16 	%rs<2>;
	.reg .f32 	%f<164>;
	.reg .b32 	%r<128>;
	.reg .b64 	%rd<44>;


	mov.u64 	%SPL, __local_depot71;
	ld.param.u64 	%rd13, [ggml_matvec_f16_ncols_8_bs_32_param_0];
	ld.param.u64 	%rd14, [ggml_matvec_f16_ncols_8_bs_32_param_1];
	ld.param.u64 	%rd15, [ggml_matvec_f16_ncols_8_bs_32_param_2];
	ld.param.u32 	%r6, [ggml_matvec_f16_ncols_8_bs_32_param_3];
	ld.param.u32 	%r7, [ggml_matvec_f16_ncols_8_bs_32_param_5];
	ld.param.u32 	%r8, [ggml_matvec_f16_ncols_8_bs_32_param_6];
	ld.param.u32 	%r9, [ggml_matvec_f16_ncols_8_bs_32_param_7];
	ld.param.u32 	%r10, [ggml_matvec_f16_ncols_8_bs_32_param_8];
	ld.param.u32 	%r11, [ggml_matvec_f16_ncols_8_bs_32_param_9];
	ld.param.u32 	%r12, [ggml_matvec_f16_ncols_8_bs_32_param_10];
	ld.param.u32 	%r13, [ggml_matvec_f16_ncols_8_bs_32_param_11];
	add.u64 	%rd1, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	mov.u32 	%r2, %ctaid.x;
	mov.f32 	%f156, 0f00000000;
	st.local.v4.f32 	[%rd1], {%f156, %f156, %f156, %f156};
	st.local.v4.f32 	[%rd1+16], {%f156, %f156, %f156, %f156};
	mov.u32 	%r3, %tid.x;
	setp.ge.s32 	%p1, %r3, %r6;
	mov.f32 	%f157, %f156;
	mov.f32 	%f158, %f156;
	mov.f32 	%f159, %f156;
	mov.f32 	%f160, %f156;
	mov.f32 	%f161, %f156;
	mov.f32 	%f162, %f156;
	mov.f32 	%f163, %f156;
	@%p1 bra 	$L__BB71_4;

	shl.b32 	%r14, %r8, 1;
	add.s32 	%r15, %r3, %r14;
	mul.wide.s32 	%rd17, %r15, 4;
	mul.lo.s32 	%r16, %r1, %r12;
	mul.wide.s32 	%rd18, %r16, 2;
	add.s64 	%rd3, %rd17, %rd18;
	mul.wide.s32 	%rd19, %r3, 4;
	mul.wide.s32 	%rd4, %r8, 4;
	add.s64 	%rd20, %rd19, %rd4;
	add.s64 	%rd5, %rd20, %rd18;
	add.s64 	%rd6, %rd19, %rd18;
	mul.wide.s32 	%rd21, %r3, 2;
	mul.lo.s32 	%r17, %r2, %r7;
	div.s32 	%r18, %r1, %r10;
	mad.lo.s32 	%r19, %r18, %r11, %r17;
	cvt.s64.s32 	%rd22, %r19;
	add.s64 	%rd23, %rd21, %rd22;
	cvta.to.global.u64 	%rd24, %rd13;
	shl.b64 	%rd25, %rd23, 1;
	add.s64 	%rd42, %rd24, %rd25;
	cvta.to.global.u64 	%rd43, %rd14;
	mov.f32 	%f156, 0f00000000;
	mov.u32 	%r127, %r3;

$L__BB71_2:
	ld.global.nc.u32 	%r20, [%rd42];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r20;
  cvt.f32.f16 %f41, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r20;
  cvt.f32.f16 %f42, high;}

	// end inline asm
	add.s64 	%rd26, %rd43, %rd6;
	ld.global.nc.u32 	%r22, [%rd26];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r22;
  cvt.f32.f16 %f43, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r22;
  cvt.f32.f16 %f44, high;}

	// end inline asm
	fma.rn.f32 	%f59, %f41, %f43, %f163;
	fma.rn.f32 	%f163, %f42, %f44, %f59;
	add.s64 	%rd27, %rd43, %rd5;
	ld.global.nc.u32 	%r24, [%rd27];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r24;
  cvt.f32.f16 %f45, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r24;
  cvt.f32.f16 %f46, high;}

	// end inline asm
	fma.rn.f32 	%f60, %f41, %f45, %f162;
	fma.rn.f32 	%f162, %f42, %f46, %f60;
	add.s64 	%rd28, %rd43, %rd3;
	ld.global.nc.u32 	%r26, [%rd28];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r26;
  cvt.f32.f16 %f47, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r26;
  cvt.f32.f16 %f48, high;}

	// end inline asm
	fma.rn.f32 	%f61, %f41, %f47, %f161;
	fma.rn.f32 	%f161, %f42, %f48, %f61;
	add.s64 	%rd29, %rd28, %rd4;
	ld.global.nc.u32 	%r28, [%rd29];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r28;
  cvt.f32.f16 %f49, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r28;
  cvt.f32.f16 %f50, high;}

	// end inline asm
	fma.rn.f32 	%f62, %f41, %f49, %f160;
	fma.rn.f32 	%f160, %f42, %f50, %f62;
	add.s64 	%rd30, %rd29, %rd4;
	ld.global.nc.u32 	%r30, [%rd30];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r30;
  cvt.f32.f16 %f51, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r30;
  cvt.f32.f16 %f52, high;}

	// end inline asm
	fma.rn.f32 	%f63, %f41, %f51, %f159;
	fma.rn.f32 	%f159, %f42, %f52, %f63;
	add.s64 	%rd31, %rd30, %rd4;
	ld.global.nc.u32 	%r32, [%rd31];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r32;
  cvt.f32.f16 %f53, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r32;
  cvt.f32.f16 %f54, high;}

	// end inline asm
	fma.rn.f32 	%f64, %f41, %f53, %f158;
	fma.rn.f32 	%f158, %f42, %f54, %f64;
	add.s64 	%rd32, %rd31, %rd4;
	ld.global.nc.u32 	%r34, [%rd32];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r34;
  cvt.f32.f16 %f55, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r34;
  cvt.f32.f16 %f56, high;}

	// end inline asm
	fma.rn.f32 	%f65, %f41, %f55, %f157;
	fma.rn.f32 	%f157, %f42, %f56, %f65;
	add.s64 	%rd33, %rd32, %rd4;
	ld.global.nc.u32 	%r36, [%rd33];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r36;
  cvt.f32.f16 %f57, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r36;
  cvt.f32.f16 %f58, high;}

	// end inline asm
	fma.rn.f32 	%f66, %f41, %f57, %f156;
	fma.rn.f32 	%f156, %f42, %f58, %f66;
	add.s64 	%rd43, %rd43, 128;
	add.s64 	%rd42, %rd42, 128;
	add.s32 	%r127, %r127, 32;
	setp.lt.s32 	%p2, %r127, %r6;
	@%p2 bra 	$L__BB71_2;

	st.local.v4.f32 	[%rd1], {%f163, %f162, %f161, %f160};
	st.local.v4.f32 	[%rd1+16], {%f159, %f158, %f157, %f156};

$L__BB71_4:
	mov.b32 	%r38, %f163;
	mov.u32 	%r39, 31;
	mov.u32 	%r40, 16;
	mov.u32 	%r41, -1;
	shfl.sync.bfly.b32 	%r42|%p3, %r38, %r40, %r39, %r41;
	mov.b32 	%f67, %r42;
	add.f32 	%f68, %f163, %f67;
	mov.b32 	%r43, %f68;
	mov.u32 	%r44, 8;
	shfl.sync.bfly.b32 	%r45|%p4, %r43, %r44, %r39, %r41;
	mov.b32 	%f69, %r45;
	add.f32 	%f70, %f68, %f69;
	mov.b32 	%r46, %f70;
	mov.u32 	%r47, 4;
	shfl.sync.bfly.b32 	%r48|%p5, %r46, %r47, %r39, %r41;
	mov.b32 	%f71, %r48;
	add.f32 	%f72, %f70, %f71;
	mov.b32 	%r49, %f72;
	mov.u32 	%r50, 2;
	shfl.sync.bfly.b32 	%r51|%p6, %r49, %r50, %r39, %r41;
	mov.b32 	%f73, %r51;
	add.f32 	%f74, %f72, %f73;
	mov.b32 	%r52, %f74;
	mov.u32 	%r53, 1;
	shfl.sync.bfly.b32 	%r54|%p7, %r52, %r53, %r39, %r41;
	mov.b32 	%f75, %r54;
	add.f32 	%f76, %f74, %f75;
	st.local.f32 	[%rd1], %f76;
	mov.b32 	%r55, %f162;
	shfl.sync.bfly.b32 	%r56|%p8, %r55, %r40, %r39, %r41;
	mov.b32 	%f77, %r56;
	add.f32 	%f78, %f162, %f77;
	mov.b32 	%r57, %f78;
	shfl.sync.bfly.b32 	%r58|%p9, %r57, %r44, %r39, %r41;
	mov.b32 	%f79, %r58;
	add.f32 	%f80, %f78, %f79;
	mov.b32 	%r59, %f80;
	shfl.sync.bfly.b32 	%r60|%p10, %r59, %r47, %r39, %r41;
	mov.b32 	%f81, %r60;
	add.f32 	%f82, %f80, %f81;
	mov.b32 	%r61, %f82;
	shfl.sync.bfly.b32 	%r62|%p11, %r61, %r50, %r39, %r41;
	mov.b32 	%f83, %r62;
	add.f32 	%f84, %f82, %f83;
	mov.b32 	%r63, %f84;
	shfl.sync.bfly.b32 	%r64|%p12, %r63, %r53, %r39, %r41;
	mov.b32 	%f85, %r64;
	add.f32 	%f86, %f84, %f85;
	st.local.f32 	[%rd1+4], %f86;
	mov.b32 	%r65, %f161;
	shfl.sync.bfly.b32 	%r66|%p13, %r65, %r40, %r39, %r41;
	mov.b32 	%f87, %r66;
	add.f32 	%f88, %f161, %f87;
	mov.b32 	%r67, %f88;
	shfl.sync.bfly.b32 	%r68|%p14, %r67, %r44, %r39, %r41;
	mov.b32 	%f89, %r68;
	add.f32 	%f90, %f88, %f89;
	mov.b32 	%r69, %f90;
	shfl.sync.bfly.b32 	%r70|%p15, %r69, %r47, %r39, %r41;
	mov.b32 	%f91, %r70;
	add.f32 	%f92, %f90, %f91;
	mov.b32 	%r71, %f92;
	shfl.sync.bfly.b32 	%r72|%p16, %r71, %r50, %r39, %r41;
	mov.b32 	%f93, %r72;
	add.f32 	%f94, %f92, %f93;
	mov.b32 	%r73, %f94;
	shfl.sync.bfly.b32 	%r74|%p17, %r73, %r53, %r39, %r41;
	mov.b32 	%f95, %r74;
	add.f32 	%f96, %f94, %f95;
	st.local.f32 	[%rd1+8], %f96;
	mov.b32 	%r75, %f160;
	shfl.sync.bfly.b32 	%r76|%p18, %r75, %r40, %r39, %r41;
	mov.b32 	%f97, %r76;
	add.f32 	%f98, %f160, %f97;
	mov.b32 	%r77, %f98;
	shfl.sync.bfly.b32 	%r78|%p19, %r77, %r44, %r39, %r41;
	mov.b32 	%f99, %r78;
	add.f32 	%f100, %f98, %f99;
	mov.b32 	%r79, %f100;
	shfl.sync.bfly.b32 	%r80|%p20, %r79, %r47, %r39, %r41;
	mov.b32 	%f101, %r80;
	add.f32 	%f102, %f100, %f101;
	mov.b32 	%r81, %f102;
	shfl.sync.bfly.b32 	%r82|%p21, %r81, %r50, %r39, %r41;
	mov.b32 	%f103, %r82;
	add.f32 	%f104, %f102, %f103;
	mov.b32 	%r83, %f104;
	shfl.sync.bfly.b32 	%r84|%p22, %r83, %r53, %r39, %r41;
	mov.b32 	%f105, %r84;
	add.f32 	%f106, %f104, %f105;
	st.local.f32 	[%rd1+12], %f106;
	mov.b32 	%r85, %f159;
	shfl.sync.bfly.b32 	%r86|%p23, %r85, %r40, %r39, %r41;
	mov.b32 	%f107, %r86;
	add.f32 	%f108, %f159, %f107;
	mov.b32 	%r87, %f108;
	shfl.sync.bfly.b32 	%r88|%p24, %r87, %r44, %r39, %r41;
	mov.b32 	%f109, %r88;
	add.f32 	%f110, %f108, %f109;
	mov.b32 	%r89, %f110;
	shfl.sync.bfly.b32 	%r90|%p25, %r89, %r47, %r39, %r41;
	mov.b32 	%f111, %r90;
	add.f32 	%f112, %f110, %f111;
	mov.b32 	%r91, %f112;
	shfl.sync.bfly.b32 	%r92|%p26, %r91, %r50, %r39, %r41;
	mov.b32 	%f113, %r92;
	add.f32 	%f114, %f112, %f113;
	mov.b32 	%r93, %f114;
	shfl.sync.bfly.b32 	%r94|%p27, %r93, %r53, %r39, %r41;
	mov.b32 	%f115, %r94;
	add.f32 	%f116, %f114, %f115;
	st.local.f32 	[%rd1+16], %f116;
	mov.b32 	%r95, %f158;
	shfl.sync.bfly.b32 	%r96|%p28, %r95, %r40, %r39, %r41;
	mov.b32 	%f117, %r96;
	add.f32 	%f118, %f158, %f117;
	mov.b32 	%r97, %f118;
	shfl.sync.bfly.b32 	%r98|%p29, %r97, %r44, %r39, %r41;
	mov.b32 	%f119, %r98;
	add.f32 	%f120, %f118, %f119;
	mov.b32 	%r99, %f120;
	shfl.sync.bfly.b32 	%r100|%p30, %r99, %r47, %r39, %r41;
	mov.b32 	%f121, %r100;
	add.f32 	%f122, %f120, %f121;
	mov.b32 	%r101, %f122;
	shfl.sync.bfly.b32 	%r102|%p31, %r101, %r50, %r39, %r41;
	mov.b32 	%f123, %r102;
	add.f32 	%f124, %f122, %f123;
	mov.b32 	%r103, %f124;
	shfl.sync.bfly.b32 	%r104|%p32, %r103, %r53, %r39, %r41;
	mov.b32 	%f125, %r104;
	add.f32 	%f126, %f124, %f125;
	st.local.f32 	[%rd1+20], %f126;
	mov.b32 	%r105, %f157;
	shfl.sync.bfly.b32 	%r106|%p33, %r105, %r40, %r39, %r41;
	mov.b32 	%f127, %r106;
	add.f32 	%f128, %f157, %f127;
	mov.b32 	%r107, %f128;
	shfl.sync.bfly.b32 	%r108|%p34, %r107, %r44, %r39, %r41;
	mov.b32 	%f129, %r108;
	add.f32 	%f130, %f128, %f129;
	mov.b32 	%r109, %f130;
	shfl.sync.bfly.b32 	%r110|%p35, %r109, %r47, %r39, %r41;
	mov.b32 	%f131, %r110;
	add.f32 	%f132, %f130, %f131;
	mov.b32 	%r111, %f132;
	shfl.sync.bfly.b32 	%r112|%p36, %r111, %r50, %r39, %r41;
	mov.b32 	%f133, %r112;
	add.f32 	%f134, %f132, %f133;
	mov.b32 	%r113, %f134;
	shfl.sync.bfly.b32 	%r114|%p37, %r113, %r53, %r39, %r41;
	mov.b32 	%f135, %r114;
	add.f32 	%f136, %f134, %f135;
	st.local.f32 	[%rd1+24], %f136;
	mov.b32 	%r115, %f156;
	shfl.sync.bfly.b32 	%r116|%p38, %r115, %r40, %r39, %r41;
	mov.b32 	%f137, %r116;
	add.f32 	%f138, %f156, %f137;
	mov.b32 	%r117, %f138;
	shfl.sync.bfly.b32 	%r118|%p39, %r117, %r44, %r39, %r41;
	mov.b32 	%f139, %r118;
	add.f32 	%f140, %f138, %f139;
	mov.b32 	%r119, %f140;
	shfl.sync.bfly.b32 	%r120|%p40, %r119, %r47, %r39, %r41;
	mov.b32 	%f141, %r120;
	add.f32 	%f142, %f140, %f141;
	mov.b32 	%r121, %f142;
	shfl.sync.bfly.b32 	%r122|%p41, %r121, %r50, %r39, %r41;
	mov.b32 	%f143, %r122;
	add.f32 	%f144, %f142, %f143;
	mov.b32 	%r123, %f144;
	shfl.sync.bfly.b32 	%r124|%p42, %r123, %r53, %r39, %r41;
	mov.b32 	%f145, %r124;
	add.f32 	%f146, %f144, %f145;
	st.local.f32 	[%rd1+28], %f146;
	setp.gt.s32 	%p43, %r3, 7;
	@%p43 bra 	$L__BB71_6;

	mad.lo.s32 	%r125, %r3, %r9, %r2;
	cvt.s64.s32 	%rd34, %r125;
	mul.lo.s32 	%r126, %r1, %r13;
	cvt.s64.s32 	%rd35, %r126;
	add.s64 	%rd36, %rd35, %rd34;
	mul.wide.s32 	%rd37, %r3, 4;
	add.s64 	%rd38, %rd1, %rd37;
	ld.local.f32 	%f147, [%rd38];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f147;}

	// end inline asm
	cvta.to.global.u64 	%rd39, %rd15;
	shl.b64 	%rd40, %rd36, 1;
	add.s64 	%rd41, %rd39, %rd40;
	st.global.u16 	[%rd41], %rs1;

$L__BB71_6:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_1_bs_64
.visible .entry ggml_matvec_f16_ncols_1_bs_64(
	.param .u64 ggml_matvec_f16_ncols_1_bs_64_param_0,
	.param .u64 ggml_matvec_f16_ncols_1_bs_64_param_1,
	.param .u64 ggml_matvec_f16_ncols_1_bs_64_param_2,
	.param .u32 ggml_matvec_f16_ncols_1_bs_64_param_3,
	.param .u32 ggml_matvec_f16_ncols_1_bs_64_param_4,
	.param .u32 ggml_matvec_f16_ncols_1_bs_64_param_5,
	.param .u32 ggml_matvec_f16_ncols_1_bs_64_param_6,
	.param .u32 ggml_matvec_f16_ncols_1_bs_64_param_7,
	.param .u32 ggml_matvec_f16_ncols_1_bs_64_param_8,
	.param .u32 ggml_matvec_f16_ncols_1_bs_64_param_9,
	.param .u32 ggml_matvec_f16_ncols_1_bs_64_param_10,
	.param .u32 ggml_matvec_f16_ncols_1_bs_64_param_11
)
{
	.reg .pred 	%p<19>;
	.reg .b16 	%rs<2>;
	.reg .f32 	%f<69>;
	.reg .b32 	%r<99>;
	.reg .b64 	%rd<42>;


	ld.param.u64 	%rd18, [ggml_matvec_f16_ncols_1_bs_64_param_0];
	ld.param.u64 	%rd19, [ggml_matvec_f16_ncols_1_bs_64_param_1];
	ld.param.u64 	%rd17, [ggml_matvec_f16_ncols_1_bs_64_param_2];
	ld.param.u32 	%r13, [ggml_matvec_f16_ncols_1_bs_64_param_3];
	ld.param.u32 	%r16, [ggml_matvec_f16_ncols_1_bs_64_param_5];
	ld.param.u32 	%r14, [ggml_matvec_f16_ncols_1_bs_64_param_7];
	ld.param.u32 	%r17, [ggml_matvec_f16_ncols_1_bs_64_param_8];
	ld.param.u32 	%r18, [ggml_matvec_f16_ncols_1_bs_64_param_9];
	ld.param.u32 	%r19, [ggml_matvec_f16_ncols_1_bs_64_param_10];
	ld.param.u32 	%r15, [ggml_matvec_f16_ncols_1_bs_64_param_11];
	cvta.to.global.u64 	%rd1, %rd19;
	cvta.to.global.u64 	%rd2, %rd18;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r20, %r1, %r17;
	mov.u32 	%r21, %ctaid.x;
	mul.lo.s32 	%r22, %r21, %r16;
	mad.lo.s32 	%r23, %r20, %r18, %r22;
	cvt.s64.s32 	%rd3, %r23;
	mul.lo.s32 	%r24, %r1, %r19;
	cvt.s64.s32 	%rd4, %r24;
	mov.u32 	%r2, %tid.x;
	setp.gt.s32 	%p1, %r2, 31;
	shl.b32 	%r25, %r2, 2;
	mov.u32 	%r26, data_mmv;
	add.s32 	%r3, %r26, %r25;
	@%p1 bra 	$L__BB72_2;

	mov.u32 	%r27, 0;
	st.shared.u32 	[%r3], %r27;

$L__BB72_2:
	bar.sync 	0;
	setp.ge.s32 	%p2, %r2, %r13;
	mov.f32 	%f67, 0f00000000;
	@%p2 bra 	$L__BB72_9;

	not.b32 	%r28, %r2;
	add.s32 	%r4, %r28, %r13;
	shr.u32 	%r29, %r4, 6;
	add.s32 	%r30, %r29, 1;
	and.b32  	%r96, %r30, 3;
	setp.eq.s32 	%p3, %r96, 0;
	mov.f32 	%f67, 0f00000000;
	mov.u32 	%r97, %r2;
	@%p3 bra 	$L__BB72_6;

	mul.wide.s32 	%rd20, %r2, 2;
	add.s64 	%rd21, %rd20, %rd4;
	shl.b64 	%rd22, %rd21, 1;
	add.s64 	%rd39, %rd1, %rd22;
	add.s64 	%rd23, %rd20, %rd3;
	shl.b64 	%rd24, %rd23, 1;
	add.s64 	%rd38, %rd2, %rd24;
	mov.f32 	%f67, 0f00000000;
	mov.u32 	%r97, %r2;

$L__BB72_5:
	.pragma "nounroll";
	ld.global.nc.u32 	%r31, [%rd38];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r31;
  cvt.f32.f16 %f15, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r31;
  cvt.f32.f16 %f16, high;}

	// end inline asm
	ld.global.nc.u32 	%r33, [%rd39];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r33;
  cvt.f32.f16 %f17, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r33;
  cvt.f32.f16 %f18, high;}

	// end inline asm
	fma.rn.f32 	%f19, %f15, %f17, %f67;
	fma.rn.f32 	%f67, %f16, %f18, %f19;
	add.s32 	%r97, %r97, 64;
	add.s64 	%rd39, %rd39, 256;
	add.s64 	%rd38, %rd38, 256;
	add.s32 	%r96, %r96, -1;
	setp.ne.s32 	%p4, %r96, 0;
	@%p4 bra 	$L__BB72_5;

$L__BB72_6:
	setp.lt.u32 	%p5, %r4, 192;
	@%p5 bra 	$L__BB72_9;

	mul.wide.s32 	%rd25, %r97, 2;
	add.s64 	%rd26, %rd25, %rd3;
	shl.b64 	%rd27, %rd26, 1;
	add.s64 	%rd28, %rd2, %rd27;
	add.s64 	%rd41, %rd28, 512;
	add.s64 	%rd29, %rd25, %rd4;
	shl.b64 	%rd30, %rd29, 1;
	add.s64 	%rd31, %rd1, %rd30;
	add.s64 	%rd40, %rd31, 512;

$L__BB72_8:
	ld.global.nc.u32 	%r35, [%rd41+-512];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r35;
  cvt.f32.f16 %f20, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r35;
  cvt.f32.f16 %f21, high;}

	// end inline asm
	ld.global.nc.u32 	%r37, [%rd40+-512];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r37;
  cvt.f32.f16 %f22, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r37;
  cvt.f32.f16 %f23, high;}

	// end inline asm
	fma.rn.f32 	%f36, %f20, %f22, %f67;
	fma.rn.f32 	%f37, %f21, %f23, %f36;
	ld.global.nc.u32 	%r39, [%rd41+-256];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r39;
  cvt.f32.f16 %f24, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r39;
  cvt.f32.f16 %f25, high;}

	// end inline asm
	ld.global.nc.u32 	%r41, [%rd40+-256];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r41;
  cvt.f32.f16 %f26, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r41;
  cvt.f32.f16 %f27, high;}

	// end inline asm
	fma.rn.f32 	%f38, %f24, %f26, %f37;
	fma.rn.f32 	%f39, %f25, %f27, %f38;
	ld.global.nc.u32 	%r43, [%rd41];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r43;
  cvt.f32.f16 %f28, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r43;
  cvt.f32.f16 %f29, high;}

	// end inline asm
	ld.global.nc.u32 	%r45, [%rd40];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r45;
  cvt.f32.f16 %f30, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r45;
  cvt.f32.f16 %f31, high;}

	// end inline asm
	fma.rn.f32 	%f40, %f28, %f30, %f39;
	fma.rn.f32 	%f41, %f29, %f31, %f40;
	ld.global.nc.u32 	%r47, [%rd41+256];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r47;
  cvt.f32.f16 %f32, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r47;
  cvt.f32.f16 %f33, high;}

	// end inline asm
	ld.global.nc.u32 	%r49, [%rd40+256];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r49;
  cvt.f32.f16 %f34, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r49;
  cvt.f32.f16 %f35, high;}

	// end inline asm
	fma.rn.f32 	%f42, %f32, %f34, %f41;
	fma.rn.f32 	%f67, %f33, %f35, %f42;
	add.s64 	%rd41, %rd41, 1024;
	add.s64 	%rd40, %rd40, 1024;
	add.s32 	%r97, %r97, 256;
	setp.lt.s32 	%p6, %r97, %r13;
	@%p6 bra 	$L__BB72_8;

$L__BB72_9:
	mov.b32 	%r51, %f67;
	mov.u32 	%r52, 31;
	mov.u32 	%r53, 16;
	mov.u32 	%r54, -1;
	shfl.sync.bfly.b32 	%r55|%p7, %r51, %r53, %r52, %r54;
	mov.b32 	%f43, %r55;
	add.f32 	%f44, %f67, %f43;
	mov.b32 	%r56, %f44;
	mov.u32 	%r57, 8;
	shfl.sync.bfly.b32 	%r58|%p8, %r56, %r57, %r52, %r54;
	mov.b32 	%f45, %r58;
	add.f32 	%f46, %f44, %f45;
	mov.b32 	%r59, %f46;
	mov.u32 	%r60, 4;
	shfl.sync.bfly.b32 	%r61|%p9, %r59, %r60, %r52, %r54;
	mov.b32 	%f47, %r61;
	add.f32 	%f48, %f46, %f47;
	mov.b32 	%r62, %f48;
	mov.u32 	%r63, 2;
	shfl.sync.bfly.b32 	%r64|%p10, %r62, %r63, %r52, %r54;
	mov.b32 	%f49, %r64;
	add.f32 	%f50, %f48, %f49;
	mov.b32 	%r65, %f50;
	mov.u32 	%r66, 1;
	shfl.sync.bfly.b32 	%r67|%p11, %r65, %r66, %r52, %r54;
	mov.b32 	%f51, %r67;
	add.f32 	%f68, %f50, %f51;
	shr.s32 	%r68, %r2, 31;
	shr.u32 	%r69, %r68, 27;
	add.s32 	%r70, %r2, %r69;
	shr.s32 	%r71, %r70, 5;
	shl.b32 	%r72, %r71, 2;
	add.s32 	%r74, %r26, %r72;
	st.shared.f32 	[%r74], %f68;
	bar.sync 	0;
	@%p1 bra 	$L__BB72_11;

	ld.shared.f32 	%f52, [%r3];
	mov.b32 	%r75, %f52;
	shfl.sync.bfly.b32 	%r79|%p13, %r75, %r53, %r52, %r54;
	mov.b32 	%f53, %r79;
	add.f32 	%f54, %f52, %f53;
	mov.b32 	%r80, %f54;
	shfl.sync.bfly.b32 	%r82|%p14, %r80, %r57, %r52, %r54;
	mov.b32 	%f55, %r82;
	add.f32 	%f56, %f54, %f55;
	mov.b32 	%r83, %f56;
	shfl.sync.bfly.b32 	%r85|%p15, %r83, %r60, %r52, %r54;
	mov.b32 	%f57, %r85;
	add.f32 	%f58, %f56, %f57;
	mov.b32 	%r86, %f58;
	shfl.sync.bfly.b32 	%r88|%p16, %r86, %r63, %r52, %r54;
	mov.b32 	%f59, %r88;
	add.f32 	%f60, %f58, %f59;
	mov.b32 	%r89, %f60;
	shfl.sync.bfly.b32 	%r91|%p17, %r89, %r66, %r52, %r54;
	mov.b32 	%f61, %r91;
	add.f32 	%f68, %f60, %f61;

$L__BB72_11:
	bar.sync 	0;
	setp.gt.s32 	%p18, %r2, 0;
	@%p18 bra 	$L__BB72_13;

	mad.lo.s32 	%r93, %r2, %r14, %r21;
	cvt.s64.s32 	%rd32, %r93;
	mul.lo.s32 	%r94, %r1, %r15;
	cvt.s64.s32 	%rd33, %r94;
	add.s64 	%rd34, %rd33, %rd32;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f68;}

	// end inline asm
	cvta.to.global.u64 	%rd35, %rd17;
	shl.b64 	%rd36, %rd34, 1;
	add.s64 	%rd37, %rd35, %rd36;
	st.global.u16 	[%rd37], %rs1;

$L__BB72_13:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_2_bs_64
.visible .entry ggml_matvec_f16_ncols_2_bs_64(
	.param .u64 ggml_matvec_f16_ncols_2_bs_64_param_0,
	.param .u64 ggml_matvec_f16_ncols_2_bs_64_param_1,
	.param .u64 ggml_matvec_f16_ncols_2_bs_64_param_2,
	.param .u32 ggml_matvec_f16_ncols_2_bs_64_param_3,
	.param .u32 ggml_matvec_f16_ncols_2_bs_64_param_4,
	.param .u32 ggml_matvec_f16_ncols_2_bs_64_param_5,
	.param .u32 ggml_matvec_f16_ncols_2_bs_64_param_6,
	.param .u32 ggml_matvec_f16_ncols_2_bs_64_param_7,
	.param .u32 ggml_matvec_f16_ncols_2_bs_64_param_8,
	.param .u32 ggml_matvec_f16_ncols_2_bs_64_param_9,
	.param .u32 ggml_matvec_f16_ncols_2_bs_64_param_10,
	.param .u32 ggml_matvec_f16_ncols_2_bs_64_param_11
)
{
	.local .align 8 .b8 	__local_depot73[8];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<30>;
	.reg .b16 	%rs<2>;
	.reg .f32 	%f<116>;
	.reg .b32 	%r<143>;
	.reg .b64 	%rd<64>;


	mov.u64 	%SPL, __local_depot73;
	ld.param.u64 	%rd27, [ggml_matvec_f16_ncols_2_bs_64_param_0];
	ld.param.u64 	%rd28, [ggml_matvec_f16_ncols_2_bs_64_param_1];
	ld.param.u64 	%rd26, [ggml_matvec_f16_ncols_2_bs_64_param_2];
	ld.param.u32 	%r15, [ggml_matvec_f16_ncols_2_bs_64_param_3];
	ld.param.u32 	%r19, [ggml_matvec_f16_ncols_2_bs_64_param_5];
	ld.param.u32 	%r16, [ggml_matvec_f16_ncols_2_bs_64_param_6];
	ld.param.u32 	%r17, [ggml_matvec_f16_ncols_2_bs_64_param_7];
	ld.param.u32 	%r20, [ggml_matvec_f16_ncols_2_bs_64_param_8];
	ld.param.u32 	%r21, [ggml_matvec_f16_ncols_2_bs_64_param_9];
	ld.param.u32 	%r22, [ggml_matvec_f16_ncols_2_bs_64_param_10];
	ld.param.u32 	%r18, [ggml_matvec_f16_ncols_2_bs_64_param_11];
	cvta.to.global.u64 	%rd1, %rd28;
	cvta.to.global.u64 	%rd2, %rd27;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r23, %r1, %r20;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r24, %r2, %r19;
	mad.lo.s32 	%r25, %r23, %r21, %r24;
	cvt.s64.s32 	%rd4, %r25;
	mul.lo.s32 	%r26, %r1, %r22;
	cvt.s64.s32 	%rd5, %r26;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r27, %r3, 2;
	mov.u32 	%r28, data_mmv;
	add.s32 	%r4, %r28, %r27;
	@%p1 bra 	$L__BB73_2;

	mov.u32 	%r29, 0;
	st.shared.u32 	[%r4], %r29;

$L__BB73_2:
	bar.sync 	0;
	mov.f32 	%f114, 0f00000000;
	st.local.v2.f32 	[%rd3], {%f114, %f114};
	setp.ge.s32 	%p2, %r3, %r15;
	mov.f32 	%f115, %f114;
	@%p2 bra 	$L__BB73_11;

	not.b32 	%r30, %r3;
	add.s32 	%r5, %r30, %r15;
	shr.u32 	%r31, %r5, 6;
	add.s32 	%r32, %r31, 1;
	and.b32  	%r140, %r32, 3;
	setp.eq.s32 	%p3, %r140, 0;
	mov.f32 	%f114, 0f00000000;
	mov.u32 	%r141, %r3;
	@%p3 bra 	$L__BB73_7;

	mul.wide.s32 	%rd30, %r16, 2;
	mul.wide.s32 	%rd31, %r3, 2;
	add.s64 	%rd32, %rd30, %rd31;
	add.s64 	%rd33, %rd32, %rd5;
	shl.b64 	%rd34, %rd33, 1;
	add.s64 	%rd60, %rd1, %rd34;
	add.s64 	%rd35, %rd31, %rd5;
	shl.b64 	%rd36, %rd35, 1;
	add.s64 	%rd59, %rd1, %rd36;
	add.s64 	%rd37, %rd31, %rd4;
	shl.b64 	%rd38, %rd37, 1;
	add.s64 	%rd58, %rd2, %rd38;
	mov.f32 	%f114, 0f00000000;
	mov.f32 	%f115, %f114;
	mov.u32 	%r141, %r3;

$L__BB73_5:
	.pragma "nounroll";
	ld.global.nc.u32 	%r33, [%rd58];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r33;
  cvt.f32.f16 %f19, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r33;
  cvt.f32.f16 %f20, high;}

	// end inline asm
	ld.global.nc.u32 	%r35, [%rd59];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r35;
  cvt.f32.f16 %f21, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r35;
  cvt.f32.f16 %f22, high;}

	// end inline asm
	fma.rn.f32 	%f25, %f19, %f21, %f115;
	fma.rn.f32 	%f115, %f20, %f22, %f25;
	ld.global.nc.u32 	%r37, [%rd60];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r37;
  cvt.f32.f16 %f23, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r37;
  cvt.f32.f16 %f24, high;}

	// end inline asm
	fma.rn.f32 	%f26, %f19, %f23, %f114;
	fma.rn.f32 	%f114, %f20, %f24, %f26;
	add.s32 	%r141, %r141, 64;
	add.s64 	%rd60, %rd60, 256;
	add.s64 	%rd59, %rd59, 256;
	add.s64 	%rd58, %rd58, 256;
	add.s32 	%r140, %r140, -1;
	setp.ne.s32 	%p4, %r140, 0;
	@%p4 bra 	$L__BB73_5;

	st.local.v2.f32 	[%rd3], {%f115, %f114};

$L__BB73_7:
	setp.lt.u32 	%p5, %r5, 192;
	@%p5 bra 	$L__BB73_11;

	mul.wide.s32 	%rd39, %r141, 2;
	add.s64 	%rd40, %rd39, %rd4;
	shl.b64 	%rd41, %rd40, 1;
	add.s64 	%rd42, %rd2, %rd41;
	add.s64 	%rd63, %rd42, 512;
	add.s64 	%rd43, %rd39, %rd5;
	shl.b64 	%rd44, %rd43, 1;
	add.s64 	%rd45, %rd1, %rd44;
	add.s64 	%rd62, %rd45, 768;
	mul.wide.s32 	%rd46, %r16, 2;
	add.s64 	%rd47, %rd43, %rd46;
	shl.b64 	%rd48, %rd47, 1;
	add.s64 	%rd49, %rd1, %rd48;
	add.s64 	%rd61, %rd49, 512;

$L__BB73_9:
	ld.global.nc.u32 	%r39, [%rd63+-512];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r39;
  cvt.f32.f16 %f27, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r39;
  cvt.f32.f16 %f28, high;}

	// end inline asm
	ld.global.nc.u32 	%r41, [%rd62+-768];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r41;
  cvt.f32.f16 %f29, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r41;
  cvt.f32.f16 %f30, high;}

	// end inline asm
	fma.rn.f32 	%f51, %f27, %f29, %f115;
	fma.rn.f32 	%f52, %f28, %f30, %f51;
	ld.global.nc.u32 	%r43, [%rd61+-512];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r43;
  cvt.f32.f16 %f31, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r43;
  cvt.f32.f16 %f32, high;}

	// end inline asm
	fma.rn.f32 	%f53, %f27, %f31, %f114;
	fma.rn.f32 	%f54, %f28, %f32, %f53;
	ld.global.nc.u32 	%r45, [%rd63+-256];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r45;
  cvt.f32.f16 %f33, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r45;
  cvt.f32.f16 %f34, high;}

	// end inline asm
	ld.global.nc.u32 	%r47, [%rd62+-512];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r47;
  cvt.f32.f16 %f35, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r47;
  cvt.f32.f16 %f36, high;}

	// end inline asm
	fma.rn.f32 	%f55, %f33, %f35, %f52;
	fma.rn.f32 	%f56, %f34, %f36, %f55;
	ld.global.nc.u32 	%r49, [%rd61+-256];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r49;
  cvt.f32.f16 %f37, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r49;
  cvt.f32.f16 %f38, high;}

	// end inline asm
	fma.rn.f32 	%f57, %f33, %f37, %f54;
	fma.rn.f32 	%f58, %f34, %f38, %f57;
	ld.global.nc.u32 	%r51, [%rd63];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r51;
  cvt.f32.f16 %f39, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r51;
  cvt.f32.f16 %f40, high;}

	// end inline asm
	ld.global.nc.u32 	%r53, [%rd62+-256];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r53;
  cvt.f32.f16 %f41, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r53;
  cvt.f32.f16 %f42, high;}

	// end inline asm
	fma.rn.f32 	%f59, %f39, %f41, %f56;
	fma.rn.f32 	%f60, %f40, %f42, %f59;
	ld.global.nc.u32 	%r55, [%rd61];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r55;
  cvt.f32.f16 %f43, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r55;
  cvt.f32.f16 %f44, high;}

	// end inline asm
	fma.rn.f32 	%f61, %f39, %f43, %f58;
	fma.rn.f32 	%f62, %f40, %f44, %f61;
	ld.global.nc.u32 	%r57, [%rd63+256];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r57;
  cvt.f32.f16 %f45, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r57;
  cvt.f32.f16 %f46, high;}

	// end inline asm
	ld.global.nc.u32 	%r59, [%rd62];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r59;
  cvt.f32.f16 %f47, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r59;
  cvt.f32.f16 %f48, high;}

	// end inline asm
	fma.rn.f32 	%f63, %f45, %f47, %f60;
	fma.rn.f32 	%f115, %f46, %f48, %f63;
	ld.global.nc.u32 	%r61, [%rd61+256];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r61;
  cvt.f32.f16 %f49, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r61;
  cvt.f32.f16 %f50, high;}

	// end inline asm
	fma.rn.f32 	%f64, %f45, %f49, %f62;
	fma.rn.f32 	%f114, %f46, %f50, %f64;
	add.s64 	%rd63, %rd63, 1024;
	add.s64 	%rd62, %rd62, 1024;
	add.s64 	%rd61, %rd61, 1024;
	add.s32 	%r141, %r141, 256;
	setp.lt.s32 	%p6, %r141, %r15;
	@%p6 bra 	$L__BB73_9;

	st.local.v2.f32 	[%rd3], {%f115, %f114};

$L__BB73_11:
	shr.s32 	%r63, %r3, 31;
	shr.u32 	%r64, %r63, 27;
	add.s32 	%r65, %r3, %r64;
	shr.s32 	%r66, %r65, 5;
	shl.b32 	%r67, %r66, 2;
	add.s32 	%r14, %r28, %r67;
	mov.u32 	%r69, 2;
	mov.b32 	%r70, %f115;
	mov.u32 	%r71, 31;
	mov.u32 	%r72, 16;
	mov.u32 	%r73, -1;
	shfl.sync.bfly.b32 	%r74|%p7, %r70, %r72, %r71, %r73;
	mov.b32 	%f65, %r74;
	add.f32 	%f66, %f115, %f65;
	mov.b32 	%r75, %f66;
	mov.u32 	%r76, 8;
	shfl.sync.bfly.b32 	%r77|%p8, %r75, %r76, %r71, %r73;
	mov.b32 	%f67, %r77;
	add.f32 	%f68, %f66, %f67;
	mov.b32 	%r78, %f68;
	mov.u32 	%r79, 4;
	shfl.sync.bfly.b32 	%r80|%p9, %r78, %r79, %r71, %r73;
	mov.b32 	%f69, %r80;
	add.f32 	%f70, %f68, %f69;
	mov.b32 	%r81, %f70;
	shfl.sync.bfly.b32 	%r82|%p10, %r81, %r69, %r71, %r73;
	mov.b32 	%f71, %r82;
	add.f32 	%f72, %f70, %f71;
	mov.b32 	%r83, %f72;
	mov.u32 	%r84, 1;
	shfl.sync.bfly.b32 	%r85|%p11, %r83, %r84, %r71, %r73;
	mov.b32 	%f73, %r85;
	add.f32 	%f74, %f72, %f73;
	st.local.f32 	[%rd3], %f74;
	st.shared.f32 	[%r14], %f74;
	bar.sync 	0;
	@%p1 bra 	$L__BB73_13;

	ld.shared.f32 	%f75, [%r4];
	mov.b32 	%r86, %f75;
	shfl.sync.bfly.b32 	%r90|%p13, %r86, %r72, %r71, %r73;
	mov.b32 	%f76, %r90;
	add.f32 	%f77, %f75, %f76;
	mov.b32 	%r91, %f77;
	shfl.sync.bfly.b32 	%r93|%p14, %r91, %r76, %r71, %r73;
	mov.b32 	%f78, %r93;
	add.f32 	%f79, %f77, %f78;
	mov.b32 	%r94, %f79;
	shfl.sync.bfly.b32 	%r96|%p15, %r94, %r79, %r71, %r73;
	mov.b32 	%f80, %r96;
	add.f32 	%f81, %f79, %f80;
	mov.b32 	%r97, %f81;
	shfl.sync.bfly.b32 	%r99|%p16, %r97, %r69, %r71, %r73;
	mov.b32 	%f82, %r99;
	add.f32 	%f83, %f81, %f82;
	mov.b32 	%r100, %f83;
	shfl.sync.bfly.b32 	%r102|%p17, %r100, %r84, %r71, %r73;
	mov.b32 	%f84, %r102;
	add.f32 	%f85, %f83, %f84;
	st.local.f32 	[%rd3], %f85;

$L__BB73_13:
	bar.sync 	0;
	mov.b32 	%r103, %f114;
	shfl.sync.bfly.b32 	%r107|%p19, %r103, %r72, %r71, %r73;
	mov.b32 	%f86, %r107;
	add.f32 	%f87, %f114, %f86;
	mov.b32 	%r108, %f87;
	shfl.sync.bfly.b32 	%r110|%p20, %r108, %r76, %r71, %r73;
	mov.b32 	%f88, %r110;
	add.f32 	%f89, %f87, %f88;
	mov.b32 	%r111, %f89;
	shfl.sync.bfly.b32 	%r113|%p21, %r111, %r79, %r71, %r73;
	mov.b32 	%f90, %r113;
	add.f32 	%f91, %f89, %f90;
	mov.b32 	%r114, %f91;
	shfl.sync.bfly.b32 	%r116|%p22, %r114, %r69, %r71, %r73;
	mov.b32 	%f92, %r116;
	add.f32 	%f93, %f91, %f92;
	mov.b32 	%r117, %f93;
	shfl.sync.bfly.b32 	%r119|%p23, %r117, %r84, %r71, %r73;
	mov.b32 	%f94, %r119;
	add.f32 	%f95, %f93, %f94;
	st.local.f32 	[%rd3+4], %f95;
	st.shared.f32 	[%r14], %f95;
	bar.sync 	0;
	@%p1 bra 	$L__BB73_15;

	ld.shared.f32 	%f96, [%r4];
	mov.b32 	%r120, %f96;
	mov.u32 	%r121, 31;
	mov.u32 	%r122, 16;
	mov.u32 	%r123, -1;
	shfl.sync.bfly.b32 	%r124|%p24, %r120, %r122, %r121, %r123;
	mov.b32 	%f97, %r124;
	add.f32 	%f98, %f96, %f97;
	mov.b32 	%r125, %f98;
	mov.u32 	%r126, 8;
	shfl.sync.bfly.b32 	%r127|%p25, %r125, %r126, %r121, %r123;
	mov.b32 	%f99, %r127;
	add.f32 	%f100, %f98, %f99;
	mov.b32 	%r128, %f100;
	mov.u32 	%r129, 4;
	shfl.sync.bfly.b32 	%r130|%p26, %r128, %r129, %r121, %r123;
	mov.b32 	%f101, %r130;
	add.f32 	%f102, %f100, %f101;
	mov.b32 	%r131, %f102;
	mov.u32 	%r132, 2;
	shfl.sync.bfly.b32 	%r133|%p27, %r131, %r132, %r121, %r123;
	mov.b32 	%f103, %r133;
	add.f32 	%f104, %f102, %f103;
	mov.b32 	%r134, %f104;
	mov.u32 	%r135, 1;
	shfl.sync.bfly.b32 	%r136|%p28, %r134, %r135, %r121, %r123;
	mov.b32 	%f105, %r136;
	add.f32 	%f106, %f104, %f105;
	st.local.f32 	[%rd3+4], %f106;

$L__BB73_15:
	bar.sync 	0;
	setp.gt.s32 	%p29, %r3, 1;
	@%p29 bra 	$L__BB73_17;

	mad.lo.s32 	%r137, %r3, %r17, %r2;
	cvt.s64.s32 	%rd50, %r137;
	mul.lo.s32 	%r138, %r1, %r18;
	cvt.s64.s32 	%rd51, %r138;
	add.s64 	%rd52, %rd51, %rd50;
	mul.wide.s32 	%rd53, %r3, 4;
	add.s64 	%rd54, %rd3, %rd53;
	ld.local.f32 	%f107, [%rd54];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f107;}

	// end inline asm
	cvta.to.global.u64 	%rd55, %rd26;
	shl.b64 	%rd56, %rd52, 1;
	add.s64 	%rd57, %rd55, %rd56;
	st.global.u16 	[%rd57], %rs1;

$L__BB73_17:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_3_bs_64
.visible .entry ggml_matvec_f16_ncols_3_bs_64(
	.param .u64 ggml_matvec_f16_ncols_3_bs_64_param_0,
	.param .u64 ggml_matvec_f16_ncols_3_bs_64_param_1,
	.param .u64 ggml_matvec_f16_ncols_3_bs_64_param_2,
	.param .u32 ggml_matvec_f16_ncols_3_bs_64_param_3,
	.param .u32 ggml_matvec_f16_ncols_3_bs_64_param_4,
	.param .u32 ggml_matvec_f16_ncols_3_bs_64_param_5,
	.param .u32 ggml_matvec_f16_ncols_3_bs_64_param_6,
	.param .u32 ggml_matvec_f16_ncols_3_bs_64_param_7,
	.param .u32 ggml_matvec_f16_ncols_3_bs_64_param_8,
	.param .u32 ggml_matvec_f16_ncols_3_bs_64_param_9,
	.param .u32 ggml_matvec_f16_ncols_3_bs_64_param_10,
	.param .u32 ggml_matvec_f16_ncols_3_bs_64_param_11
)
{
	.local .align 4 .b8 	__local_depot74[12];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<41>;
	.reg .b16 	%rs<2>;
	.reg .f32 	%f<168>;
	.reg .b32 	%r<194>;
	.reg .b64 	%rd<72>;


	mov.u64 	%SPL, __local_depot74;
	ld.param.u64 	%rd29, [ggml_matvec_f16_ncols_3_bs_64_param_0];
	ld.param.u64 	%rd30, [ggml_matvec_f16_ncols_3_bs_64_param_1];
	ld.param.u64 	%rd28, [ggml_matvec_f16_ncols_3_bs_64_param_2];
	ld.param.u32 	%r15, [ggml_matvec_f16_ncols_3_bs_64_param_3];
	ld.param.u32 	%r19, [ggml_matvec_f16_ncols_3_bs_64_param_5];
	ld.param.u32 	%r16, [ggml_matvec_f16_ncols_3_bs_64_param_6];
	ld.param.u32 	%r17, [ggml_matvec_f16_ncols_3_bs_64_param_7];
	ld.param.u32 	%r20, [ggml_matvec_f16_ncols_3_bs_64_param_8];
	ld.param.u32 	%r21, [ggml_matvec_f16_ncols_3_bs_64_param_9];
	ld.param.u32 	%r22, [ggml_matvec_f16_ncols_3_bs_64_param_10];
	ld.param.u32 	%r18, [ggml_matvec_f16_ncols_3_bs_64_param_11];
	cvta.to.global.u64 	%rd71, %rd30;
	cvta.to.global.u64 	%rd2, %rd29;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r23, %r1, %r20;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r24, %r2, %r19;
	mad.lo.s32 	%r25, %r23, %r21, %r24;
	cvt.s64.s32 	%rd4, %r25;
	mul.lo.s32 	%r26, %r1, %r22;
	cvt.s64.s32 	%rd5, %r26;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r27, %r3, 2;
	mov.u32 	%r28, data_mmv;
	add.s32 	%r4, %r28, %r27;
	@%p1 bra 	$L__BB74_2;

	mov.u32 	%r29, 0;
	st.shared.u32 	[%r4], %r29;

$L__BB74_2:
	bar.sync 	0;
	mov.f32 	%f165, 0f00000000;
	mov.u32 	%r30, 0;
	st.local.u32 	[%rd3], %r30;
	st.local.u32 	[%rd3+4], %r30;
	st.local.u32 	[%rd3+8], %r30;
	setp.ge.s32 	%p2, %r3, %r15;
	mov.f32 	%f166, %f165;
	mov.f32 	%f167, %f165;
	@%p2 bra 	$L__BB74_11;

	not.b32 	%r31, %r3;
	add.s32 	%r5, %r31, %r15;
	shr.u32 	%r32, %r5, 6;
	add.s32 	%r33, %r32, 1;
	and.b32  	%r191, %r33, 3;
	setp.eq.s32 	%p3, %r191, 0;
	mov.f32 	%f165, 0f00000000;
	mov.u32 	%r192, %r3;
	@%p3 bra 	$L__BB74_7;

	shl.b32 	%r34, %r16, 1;
	add.s32 	%r35, %r3, %r34;
	mul.wide.s32 	%rd32, %r35, 2;
	add.s64 	%rd33, %rd32, %rd5;
	shl.b64 	%rd34, %rd33, 1;
	add.s64 	%rd69, %rd71, %rd34;
	mul.wide.s32 	%rd35, %r16, 2;
	mul.wide.s32 	%rd36, %r3, 2;
	add.s64 	%rd37, %rd35, %rd36;
	add.s64 	%rd38, %rd37, %rd5;
	shl.b64 	%rd39, %rd38, 1;
	add.s64 	%rd68, %rd71, %rd39;
	add.s64 	%rd40, %rd36, %rd5;
	shl.b64 	%rd41, %rd40, 1;
	add.s64 	%rd67, %rd71, %rd41;
	add.s64 	%rd42, %rd36, %rd4;
	shl.b64 	%rd43, %rd42, 1;
	add.s64 	%rd66, %rd2, %rd43;
	mov.f32 	%f165, 0f00000000;
	mov.f32 	%f166, %f165;
	mov.f32 	%f167, %f165;
	mov.u32 	%r192, %r3;

$L__BB74_5:
	.pragma "nounroll";
	ld.global.nc.u32 	%r36, [%rd66];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r36;
  cvt.f32.f16 %f28, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r36;
  cvt.f32.f16 %f29, high;}

	// end inline asm
	ld.global.nc.u32 	%r38, [%rd67];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r38;
  cvt.f32.f16 %f30, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r38;
  cvt.f32.f16 %f31, high;}

	// end inline asm
	fma.rn.f32 	%f36, %f28, %f30, %f167;
	fma.rn.f32 	%f167, %f29, %f31, %f36;
	ld.global.nc.u32 	%r40, [%rd68];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r40;
  cvt.f32.f16 %f32, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r40;
  cvt.f32.f16 %f33, high;}

	// end inline asm
	fma.rn.f32 	%f37, %f28, %f32, %f166;
	fma.rn.f32 	%f166, %f29, %f33, %f37;
	ld.global.nc.u32 	%r42, [%rd69];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r42;
  cvt.f32.f16 %f34, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r42;
  cvt.f32.f16 %f35, high;}

	// end inline asm
	fma.rn.f32 	%f38, %f28, %f34, %f165;
	fma.rn.f32 	%f165, %f29, %f35, %f38;
	add.s32 	%r192, %r192, 64;
	add.s64 	%rd69, %rd69, 256;
	add.s64 	%rd68, %rd68, 256;
	add.s64 	%rd67, %rd67, 256;
	add.s64 	%rd66, %rd66, 256;
	add.s32 	%r191, %r191, -1;
	setp.ne.s32 	%p4, %r191, 0;
	@%p4 bra 	$L__BB74_5;

	st.local.f32 	[%rd3], %f167;
	st.local.f32 	[%rd3+4], %f166;
	st.local.f32 	[%rd3+8], %f165;

$L__BB74_7:
	setp.lt.u32 	%p5, %r5, 192;
	@%p5 bra 	$L__BB74_11;

	add.s32 	%r44, %r192, %r16;
	shl.b32 	%r45, %r16, 1;
	add.s32 	%r46, %r192, %r45;
	add.s32 	%r47, %r44, 64;
	mul.wide.s32 	%rd44, %r47, 4;
	shl.b64 	%rd45, %rd5, 1;
	add.s64 	%rd19, %rd44, %rd45;
	mul.wide.s32 	%rd46, %r46, 4;
	add.s64 	%rd20, %rd46, %rd45;
	mul.wide.s32 	%rd47, %r192, 2;
	add.s64 	%rd48, %rd47, %rd4;
	shl.b64 	%rd49, %rd48, 1;
	add.s64 	%rd50, %rd2, %rd49;
	add.s64 	%rd70, %rd50, 512;
	mul.wide.s32 	%rd51, %r192, 4;
	add.s64 	%rd22, %rd51, %rd45;
	mul.wide.s32 	%rd52, %r16, 4;
	add.s64 	%rd53, %rd51, %rd52;
	add.s64 	%rd23, %rd53, %rd45;

$L__BB74_9:
	ld.global.nc.u32 	%r48, [%rd70+-512];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r48;
  cvt.f32.f16 %f39, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r48;
  cvt.f32.f16 %f40, high;}

	// end inline asm
	add.s64 	%rd54, %rd71, %rd22;
	ld.global.nc.u32 	%r50, [%rd54];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r50;
  cvt.f32.f16 %f41, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r50;
  cvt.f32.f16 %f42, high;}

	// end inline asm
	fma.rn.f32 	%f71, %f39, %f41, %f167;
	fma.rn.f32 	%f72, %f40, %f42, %f71;
	add.s64 	%rd55, %rd71, %rd23;
	ld.global.nc.u32 	%r52, [%rd55];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r52;
  cvt.f32.f16 %f43, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r52;
  cvt.f32.f16 %f44, high;}

	// end inline asm
	fma.rn.f32 	%f73, %f39, %f43, %f166;
	fma.rn.f32 	%f74, %f40, %f44, %f73;
	add.s64 	%rd56, %rd71, %rd20;
	ld.global.nc.u32 	%r54, [%rd56];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r54;
  cvt.f32.f16 %f45, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r54;
  cvt.f32.f16 %f46, high;}

	// end inline asm
	fma.rn.f32 	%f75, %f39, %f45, %f165;
	fma.rn.f32 	%f76, %f40, %f46, %f75;
	ld.global.nc.u32 	%r56, [%rd70+-256];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r56;
  cvt.f32.f16 %f47, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r56;
  cvt.f32.f16 %f48, high;}

	// end inline asm
	ld.global.nc.u32 	%r58, [%rd54+256];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r58;
  cvt.f32.f16 %f49, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r58;
  cvt.f32.f16 %f50, high;}

	// end inline asm
	fma.rn.f32 	%f77, %f47, %f49, %f72;
	fma.rn.f32 	%f78, %f48, %f50, %f77;
	add.s64 	%rd57, %rd71, %rd19;
	ld.global.nc.u32 	%r60, [%rd57];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r60;
  cvt.f32.f16 %f51, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r60;
  cvt.f32.f16 %f52, high;}

	// end inline asm
	fma.rn.f32 	%f79, %f47, %f51, %f74;
	fma.rn.f32 	%f80, %f48, %f52, %f79;
	ld.global.nc.u32 	%r62, [%rd56+256];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r62;
  cvt.f32.f16 %f53, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r62;
  cvt.f32.f16 %f54, high;}

	// end inline asm
	fma.rn.f32 	%f81, %f47, %f53, %f76;
	fma.rn.f32 	%f82, %f48, %f54, %f81;
	ld.global.nc.u32 	%r64, [%rd70];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r64;
  cvt.f32.f16 %f55, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r64;
  cvt.f32.f16 %f56, high;}

	// end inline asm
	ld.global.nc.u32 	%r66, [%rd54+512];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r66;
  cvt.f32.f16 %f57, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r66;
  cvt.f32.f16 %f58, high;}

	// end inline asm
	fma.rn.f32 	%f83, %f55, %f57, %f78;
	fma.rn.f32 	%f84, %f56, %f58, %f83;
	ld.global.nc.u32 	%r68, [%rd57+256];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r68;
  cvt.f32.f16 %f59, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r68;
  cvt.f32.f16 %f60, high;}

	// end inline asm
	fma.rn.f32 	%f85, %f55, %f59, %f80;
	fma.rn.f32 	%f86, %f56, %f60, %f85;
	ld.global.nc.u32 	%r70, [%rd56+512];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r70;
  cvt.f32.f16 %f61, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r70;
  cvt.f32.f16 %f62, high;}

	// end inline asm
	fma.rn.f32 	%f87, %f55, %f61, %f82;
	fma.rn.f32 	%f88, %f56, %f62, %f87;
	ld.global.nc.u32 	%r72, [%rd70+256];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r72;
  cvt.f32.f16 %f63, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r72;
  cvt.f32.f16 %f64, high;}

	// end inline asm
	ld.global.nc.u32 	%r74, [%rd54+768];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r74;
  cvt.f32.f16 %f65, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r74;
  cvt.f32.f16 %f66, high;}

	// end inline asm
	fma.rn.f32 	%f89, %f63, %f65, %f84;
	fma.rn.f32 	%f167, %f64, %f66, %f89;
	ld.global.nc.u32 	%r76, [%rd57+512];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r76;
  cvt.f32.f16 %f67, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r76;
  cvt.f32.f16 %f68, high;}

	// end inline asm
	fma.rn.f32 	%f90, %f63, %f67, %f86;
	fma.rn.f32 	%f166, %f64, %f68, %f90;
	ld.global.nc.u32 	%r78, [%rd56+768];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r78;
  cvt.f32.f16 %f69, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r78;
  cvt.f32.f16 %f70, high;}

	// end inline asm
	fma.rn.f32 	%f91, %f63, %f69, %f88;
	fma.rn.f32 	%f165, %f64, %f70, %f91;
	add.s64 	%rd71, %rd71, 1024;
	add.s64 	%rd70, %rd70, 1024;
	add.s32 	%r192, %r192, 256;
	setp.lt.s32 	%p6, %r192, %r15;
	@%p6 bra 	$L__BB74_9;

	st.local.f32 	[%rd3], %f167;
	st.local.f32 	[%rd3+4], %f166;
	st.local.f32 	[%rd3+8], %f165;

$L__BB74_11:
	shr.s32 	%r80, %r3, 31;
	shr.u32 	%r81, %r80, 27;
	add.s32 	%r82, %r3, %r81;
	shr.s32 	%r83, %r82, 5;
	shl.b32 	%r84, %r83, 2;
	add.s32 	%r14, %r28, %r84;
	mov.u32 	%r86, 2;
	mov.b32 	%r87, %f167;
	mov.u32 	%r88, 31;
	mov.u32 	%r89, 16;
	mov.u32 	%r90, -1;
	shfl.sync.bfly.b32 	%r91|%p7, %r87, %r89, %r88, %r90;
	mov.b32 	%f92, %r91;
	add.f32 	%f93, %f167, %f92;
	mov.b32 	%r92, %f93;
	mov.u32 	%r93, 8;
	shfl.sync.bfly.b32 	%r94|%p8, %r92, %r93, %r88, %r90;
	mov.b32 	%f94, %r94;
	add.f32 	%f95, %f93, %f94;
	mov.b32 	%r95, %f95;
	mov.u32 	%r96, 4;
	shfl.sync.bfly.b32 	%r97|%p9, %r95, %r96, %r88, %r90;
	mov.b32 	%f96, %r97;
	add.f32 	%f97, %f95, %f96;
	mov.b32 	%r98, %f97;
	shfl.sync.bfly.b32 	%r99|%p10, %r98, %r86, %r88, %r90;
	mov.b32 	%f98, %r99;
	add.f32 	%f99, %f97, %f98;
	mov.b32 	%r100, %f99;
	mov.u32 	%r101, 1;
	shfl.sync.bfly.b32 	%r102|%p11, %r100, %r101, %r88, %r90;
	mov.b32 	%f100, %r102;
	add.f32 	%f101, %f99, %f100;
	st.local.f32 	[%rd3], %f101;
	st.shared.f32 	[%r14], %f101;
	bar.sync 	0;
	@%p1 bra 	$L__BB74_13;

	ld.shared.f32 	%f102, [%r4];
	mov.b32 	%r103, %f102;
	shfl.sync.bfly.b32 	%r107|%p13, %r103, %r89, %r88, %r90;
	mov.b32 	%f103, %r107;
	add.f32 	%f104, %f102, %f103;
	mov.b32 	%r108, %f104;
	shfl.sync.bfly.b32 	%r110|%p14, %r108, %r93, %r88, %r90;
	mov.b32 	%f105, %r110;
	add.f32 	%f106, %f104, %f105;
	mov.b32 	%r111, %f106;
	shfl.sync.bfly.b32 	%r113|%p15, %r111, %r96, %r88, %r90;
	mov.b32 	%f107, %r113;
	add.f32 	%f108, %f106, %f107;
	mov.b32 	%r114, %f108;
	shfl.sync.bfly.b32 	%r116|%p16, %r114, %r86, %r88, %r90;
	mov.b32 	%f109, %r116;
	add.f32 	%f110, %f108, %f109;
	mov.b32 	%r117, %f110;
	shfl.sync.bfly.b32 	%r119|%p17, %r117, %r101, %r88, %r90;
	mov.b32 	%f111, %r119;
	add.f32 	%f112, %f110, %f111;
	st.local.f32 	[%rd3], %f112;

$L__BB74_13:
	bar.sync 	0;
	mov.b32 	%r120, %f166;
	shfl.sync.bfly.b32 	%r124|%p19, %r120, %r89, %r88, %r90;
	mov.b32 	%f113, %r124;
	add.f32 	%f114, %f166, %f113;
	mov.b32 	%r125, %f114;
	shfl.sync.bfly.b32 	%r127|%p20, %r125, %r93, %r88, %r90;
	mov.b32 	%f115, %r127;
	add.f32 	%f116, %f114, %f115;
	mov.b32 	%r128, %f116;
	shfl.sync.bfly.b32 	%r130|%p21, %r128, %r96, %r88, %r90;
	mov.b32 	%f117, %r130;
	add.f32 	%f118, %f116, %f117;
	mov.b32 	%r131, %f118;
	shfl.sync.bfly.b32 	%r133|%p22, %r131, %r86, %r88, %r90;
	mov.b32 	%f119, %r133;
	add.f32 	%f120, %f118, %f119;
	mov.b32 	%r134, %f120;
	shfl.sync.bfly.b32 	%r136|%p23, %r134, %r101, %r88, %r90;
	mov.b32 	%f121, %r136;
	add.f32 	%f122, %f120, %f121;
	st.local.f32 	[%rd3+4], %f122;
	st.shared.f32 	[%r14], %f122;
	bar.sync 	0;
	@%p1 bra 	$L__BB74_15;

	ld.shared.f32 	%f123, [%r4];
	mov.b32 	%r137, %f123;
	mov.u32 	%r138, 31;
	mov.u32 	%r139, 16;
	mov.u32 	%r140, -1;
	shfl.sync.bfly.b32 	%r141|%p24, %r137, %r139, %r138, %r140;
	mov.b32 	%f124, %r141;
	add.f32 	%f125, %f123, %f124;
	mov.b32 	%r142, %f125;
	mov.u32 	%r143, 8;
	shfl.sync.bfly.b32 	%r144|%p25, %r142, %r143, %r138, %r140;
	mov.b32 	%f126, %r144;
	add.f32 	%f127, %f125, %f126;
	mov.b32 	%r145, %f127;
	mov.u32 	%r146, 4;
	shfl.sync.bfly.b32 	%r147|%p26, %r145, %r146, %r138, %r140;
	mov.b32 	%f128, %r147;
	add.f32 	%f129, %f127, %f128;
	mov.b32 	%r148, %f129;
	mov.u32 	%r149, 2;
	shfl.sync.bfly.b32 	%r150|%p27, %r148, %r149, %r138, %r140;
	mov.b32 	%f130, %r150;
	add.f32 	%f131, %f129, %f130;
	mov.b32 	%r151, %f131;
	mov.u32 	%r152, 1;
	shfl.sync.bfly.b32 	%r153|%p28, %r151, %r152, %r138, %r140;
	mov.b32 	%f132, %r153;
	add.f32 	%f133, %f131, %f132;
	st.local.f32 	[%rd3+4], %f133;

$L__BB74_15:
	bar.sync 	0;
	mov.b32 	%r154, %f165;
	mov.u32 	%r155, 31;
	mov.u32 	%r156, 16;
	mov.u32 	%r157, -1;
	shfl.sync.bfly.b32 	%r158|%p30, %r154, %r156, %r155, %r157;
	mov.b32 	%f134, %r158;
	add.f32 	%f135, %f165, %f134;
	mov.b32 	%r159, %f135;
	mov.u32 	%r160, 8;
	shfl.sync.bfly.b32 	%r161|%p31, %r159, %r160, %r155, %r157;
	mov.b32 	%f136, %r161;
	add.f32 	%f137, %f135, %f136;
	mov.b32 	%r162, %f137;
	mov.u32 	%r163, 4;
	shfl.sync.bfly.b32 	%r164|%p32, %r162, %r163, %r155, %r157;
	mov.b32 	%f138, %r164;
	add.f32 	%f139, %f137, %f138;
	mov.b32 	%r165, %f139;
	mov.u32 	%r166, 2;
	shfl.sync.bfly.b32 	%r167|%p33, %r165, %r166, %r155, %r157;
	mov.b32 	%f140, %r167;
	add.f32 	%f141, %f139, %f140;
	mov.b32 	%r168, %f141;
	mov.u32 	%r169, 1;
	shfl.sync.bfly.b32 	%r170|%p34, %r168, %r169, %r155, %r157;
	mov.b32 	%f142, %r170;
	add.f32 	%f143, %f141, %f142;
	st.local.f32 	[%rd3+8], %f143;
	st.shared.f32 	[%r14], %f143;
	bar.sync 	0;
	@%p1 bra 	$L__BB74_17;

	ld.shared.f32 	%f144, [%r4];
	mov.b32 	%r171, %f144;
	shfl.sync.bfly.b32 	%r175|%p35, %r171, %r156, %r155, %r157;
	mov.b32 	%f145, %r175;
	add.f32 	%f146, %f144, %f145;
	mov.b32 	%r176, %f146;
	shfl.sync.bfly.b32 	%r178|%p36, %r176, %r160, %r155, %r157;
	mov.b32 	%f147, %r178;
	add.f32 	%f148, %f146, %f147;
	mov.b32 	%r179, %f148;
	shfl.sync.bfly.b32 	%r181|%p37, %r179, %r163, %r155, %r157;
	mov.b32 	%f149, %r181;
	add.f32 	%f150, %f148, %f149;
	mov.b32 	%r182, %f150;
	shfl.sync.bfly.b32 	%r184|%p38, %r182, %r166, %r155, %r157;
	mov.b32 	%f151, %r184;
	add.f32 	%f152, %f150, %f151;
	mov.b32 	%r185, %f152;
	shfl.sync.bfly.b32 	%r187|%p39, %r185, %r169, %r155, %r157;
	mov.b32 	%f153, %r187;
	add.f32 	%f154, %f152, %f153;
	st.local.f32 	[%rd3+8], %f154;

$L__BB74_17:
	bar.sync 	0;
	setp.gt.s32 	%p40, %r3, 2;
	@%p40 bra 	$L__BB74_19;

	mad.lo.s32 	%r188, %r3, %r17, %r2;
	cvt.s64.s32 	%rd58, %r188;
	mul.lo.s32 	%r189, %r1, %r18;
	cvt.s64.s32 	%rd59, %r189;
	add.s64 	%rd60, %rd59, %rd58;
	mul.wide.s32 	%rd61, %r3, 4;
	add.s64 	%rd62, %rd3, %rd61;
	ld.local.f32 	%f155, [%rd62];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f155;}

	// end inline asm
	cvta.to.global.u64 	%rd63, %rd28;
	shl.b64 	%rd64, %rd60, 1;
	add.s64 	%rd65, %rd63, %rd64;
	st.global.u16 	[%rd65], %rs1;

$L__BB74_19:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_4_bs_64
.visible .entry ggml_matvec_f16_ncols_4_bs_64(
	.param .u64 ggml_matvec_f16_ncols_4_bs_64_param_0,
	.param .u64 ggml_matvec_f16_ncols_4_bs_64_param_1,
	.param .u64 ggml_matvec_f16_ncols_4_bs_64_param_2,
	.param .u32 ggml_matvec_f16_ncols_4_bs_64_param_3,
	.param .u32 ggml_matvec_f16_ncols_4_bs_64_param_4,
	.param .u32 ggml_matvec_f16_ncols_4_bs_64_param_5,
	.param .u32 ggml_matvec_f16_ncols_4_bs_64_param_6,
	.param .u32 ggml_matvec_f16_ncols_4_bs_64_param_7,
	.param .u32 ggml_matvec_f16_ncols_4_bs_64_param_8,
	.param .u32 ggml_matvec_f16_ncols_4_bs_64_param_9,
	.param .u32 ggml_matvec_f16_ncols_4_bs_64_param_10,
	.param .u32 ggml_matvec_f16_ncols_4_bs_64_param_11
)
{
	.local .align 16 .b8 	__local_depot75[16];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<51>;
	.reg .b16 	%rs<2>;
	.reg .f32 	%f<172>;
	.reg .b32 	%r<213>;
	.reg .b64 	%rd<61>;


	mov.u64 	%SPL, __local_depot75;
	ld.param.u64 	%rd19, [ggml_matvec_f16_ncols_4_bs_64_param_0];
	ld.param.u64 	%rd20, [ggml_matvec_f16_ncols_4_bs_64_param_1];
	ld.param.u64 	%rd18, [ggml_matvec_f16_ncols_4_bs_64_param_2];
	ld.param.u32 	%r11, [ggml_matvec_f16_ncols_4_bs_64_param_3];
	ld.param.u32 	%r15, [ggml_matvec_f16_ncols_4_bs_64_param_5];
	ld.param.u32 	%r12, [ggml_matvec_f16_ncols_4_bs_64_param_6];
	ld.param.u32 	%r13, [ggml_matvec_f16_ncols_4_bs_64_param_7];
	ld.param.u32 	%r16, [ggml_matvec_f16_ncols_4_bs_64_param_8];
	ld.param.u32 	%r17, [ggml_matvec_f16_ncols_4_bs_64_param_9];
	ld.param.u32 	%r18, [ggml_matvec_f16_ncols_4_bs_64_param_10];
	ld.param.u32 	%r14, [ggml_matvec_f16_ncols_4_bs_64_param_11];
	cvta.to.global.u64 	%rd60, %rd20;
	add.u64 	%rd2, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r19, %r1, %r16;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r20, %r2, %r15;
	mad.lo.s32 	%r21, %r19, %r17, %r20;
	cvt.s64.s32 	%rd3, %r21;
	cvta.to.global.u64 	%rd4, %rd19;
	mul.lo.s32 	%r22, %r1, %r18;
	cvt.s64.s32 	%rd5, %r22;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r23, %r3, 2;
	mov.u32 	%r24, data_mmv;
	add.s32 	%r4, %r24, %r23;
	@%p1 bra 	$L__BB75_2;

	mov.u32 	%r25, 0;
	st.shared.u32 	[%r4], %r25;

$L__BB75_2:
	bar.sync 	0;
	mov.f32 	%f168, 0f00000000;
	st.local.v4.f32 	[%rd2], {%f168, %f168, %f168, %f168};
	setp.ge.s32 	%p2, %r3, %r11;
	mov.f32 	%f169, %f168;
	mov.f32 	%f170, %f168;
	mov.f32 	%f171, %f168;
	@%p2 bra 	$L__BB75_9;

	not.b32 	%r26, %r3;
	add.s32 	%r5, %r26, %r11;
	and.b32  	%r27, %r5, 64;
	setp.ne.s32 	%p3, %r27, 0;
	mov.f32 	%f168, 0f00000000;
	mov.u32 	%r212, %r3;
	@%p3 bra 	$L__BB75_5;

	shl.b64 	%rd22, %rd5, 1;
	add.s64 	%rd23, %rd60, %rd22;
	shl.b64 	%rd24, %rd3, 1;
	add.s64 	%rd25, %rd4, %rd24;
	mul.wide.s32 	%rd26, %r3, 4;
	add.s64 	%rd27, %rd25, %rd26;
	ld.global.nc.u32 	%r28, [%rd27];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r28;
  cvt.f32.f16 %f29, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r28;
  cvt.f32.f16 %f30, high;}

	// end inline asm
	add.s64 	%rd28, %rd23, %rd26;
	ld.global.nc.u32 	%r30, [%rd28];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r30;
  cvt.f32.f16 %f31, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r30;
  cvt.f32.f16 %f32, high;}

	// end inline asm
	fma.rn.f32 	%f39, %f29, %f31, 0f00000000;
	fma.rn.f32 	%f171, %f30, %f32, %f39;
	st.local.f32 	[%rd2], %f171;
	mul.wide.s32 	%rd29, %r12, 4;
	add.s64 	%rd30, %rd28, %rd29;
	ld.global.nc.u32 	%r32, [%rd30];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r32;
  cvt.f32.f16 %f33, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r32;
  cvt.f32.f16 %f34, high;}

	// end inline asm
	fma.rn.f32 	%f40, %f29, %f33, 0f00000000;
	fma.rn.f32 	%f170, %f30, %f34, %f40;
	st.local.f32 	[%rd2+4], %f170;
	add.s32 	%r38, %r3, %r12;
	add.s32 	%r39, %r38, %r12;
	mul.wide.s32 	%rd31, %r39, 4;
	add.s64 	%rd32, %rd23, %rd31;
	ld.global.nc.u32 	%r34, [%rd32];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r34;
  cvt.f32.f16 %f35, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r34;
  cvt.f32.f16 %f36, high;}

	// end inline asm
	fma.rn.f32 	%f41, %f29, %f35, 0f00000000;
	fma.rn.f32 	%f169, %f30, %f36, %f41;
	st.local.f32 	[%rd2+8], %f169;
	add.s32 	%r40, %r39, %r12;
	mul.wide.s32 	%rd33, %r40, 4;
	add.s64 	%rd34, %rd23, %rd33;
	ld.global.nc.u32 	%r36, [%rd34];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r36;
  cvt.f32.f16 %f37, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r36;
  cvt.f32.f16 %f38, high;}

	// end inline asm
	fma.rn.f32 	%f42, %f29, %f37, 0f00000000;
	fma.rn.f32 	%f168, %f30, %f38, %f42;
	st.local.f32 	[%rd2+12], %f168;
	add.s32 	%r212, %r3, 64;

$L__BB75_5:
	and.b32  	%r41, %r5, -64;
	setp.eq.s32 	%p4, %r41, 0;
	@%p4 bra 	$L__BB75_9;

	add.s32 	%r42, %r212, %r12;
	add.s32 	%r43, %r42, 64;
	mul.wide.s32 	%rd35, %r43, 4;
	shl.b64 	%rd36, %rd5, 1;
	add.s64 	%rd8, %rd35, %rd36;
	shl.b32 	%r44, %r12, 1;
	add.s32 	%r45, %r212, %r44;
	mad.lo.s32 	%r46, %r12, 3, %r212;
	mul.wide.s32 	%rd37, %r45, 4;
	add.s64 	%rd9, %rd37, %rd36;
	mul.wide.s32 	%rd38, %r46, 4;
	add.s64 	%rd10, %rd38, %rd36;
	mul.wide.s32 	%rd39, %r212, 2;
	add.s64 	%rd40, %rd39, %rd3;
	shl.b64 	%rd41, %rd40, 1;
	add.s64 	%rd42, %rd4, %rd41;
	add.s64 	%rd59, %rd42, 256;
	mul.wide.s32 	%rd43, %r212, 4;
	mul.wide.s32 	%rd44, %r12, 4;
	add.s64 	%rd45, %rd43, %rd44;
	add.s64 	%rd12, %rd45, %rd36;
	add.s64 	%rd13, %rd43, %rd36;

$L__BB75_7:
	ld.global.nc.u32 	%r47, [%rd59+-256];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r47;
  cvt.f32.f16 %f43, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r47;
  cvt.f32.f16 %f44, high;}

	// end inline asm
	add.s64 	%rd46, %rd60, %rd13;
	ld.global.nc.u32 	%r49, [%rd46];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r49;
  cvt.f32.f16 %f45, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r49;
  cvt.f32.f16 %f46, high;}

	// end inline asm
	fma.rn.f32 	%f63, %f43, %f45, %f171;
	fma.rn.f32 	%f64, %f44, %f46, %f63;
	add.s64 	%rd47, %rd60, %rd12;
	ld.global.nc.u32 	%r51, [%rd47];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r51;
  cvt.f32.f16 %f47, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r51;
  cvt.f32.f16 %f48, high;}

	// end inline asm
	fma.rn.f32 	%f65, %f43, %f47, %f170;
	fma.rn.f32 	%f66, %f44, %f48, %f65;
	add.s64 	%rd48, %rd60, %rd9;
	ld.global.nc.u32 	%r53, [%rd48];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r53;
  cvt.f32.f16 %f49, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r53;
  cvt.f32.f16 %f50, high;}

	// end inline asm
	fma.rn.f32 	%f67, %f43, %f49, %f169;
	fma.rn.f32 	%f68, %f44, %f50, %f67;
	add.s64 	%rd49, %rd60, %rd10;
	ld.global.nc.u32 	%r55, [%rd49];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r55;
  cvt.f32.f16 %f51, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r55;
  cvt.f32.f16 %f52, high;}

	// end inline asm
	fma.rn.f32 	%f69, %f43, %f51, %f168;
	fma.rn.f32 	%f70, %f44, %f52, %f69;
	ld.global.nc.u32 	%r57, [%rd59];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r57;
  cvt.f32.f16 %f53, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r57;
  cvt.f32.f16 %f54, high;}

	// end inline asm
	ld.global.nc.u32 	%r59, [%rd46+256];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r59;
  cvt.f32.f16 %f55, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r59;
  cvt.f32.f16 %f56, high;}

	// end inline asm
	fma.rn.f32 	%f71, %f53, %f55, %f64;
	fma.rn.f32 	%f171, %f54, %f56, %f71;
	add.s64 	%rd50, %rd60, %rd8;
	ld.global.nc.u32 	%r61, [%rd50];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r61;
  cvt.f32.f16 %f57, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r61;
  cvt.f32.f16 %f58, high;}

	// end inline asm
	fma.rn.f32 	%f72, %f53, %f57, %f66;
	fma.rn.f32 	%f170, %f54, %f58, %f72;
	ld.global.nc.u32 	%r63, [%rd48+256];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r63;
  cvt.f32.f16 %f59, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r63;
  cvt.f32.f16 %f60, high;}

	// end inline asm
	fma.rn.f32 	%f73, %f53, %f59, %f68;
	fma.rn.f32 	%f169, %f54, %f60, %f73;
	ld.global.nc.u32 	%r65, [%rd49+256];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r65;
  cvt.f32.f16 %f61, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r65;
  cvt.f32.f16 %f62, high;}

	// end inline asm
	fma.rn.f32 	%f74, %f53, %f61, %f70;
	fma.rn.f32 	%f168, %f54, %f62, %f74;
	add.s64 	%rd60, %rd60, 512;
	add.s64 	%rd59, %rd59, 512;
	add.s32 	%r212, %r212, 128;
	setp.lt.s32 	%p5, %r212, %r11;
	@%p5 bra 	$L__BB75_7;

	st.local.v4.f32 	[%rd2], {%f171, %f170, %f169, %f168};

$L__BB75_9:
	shr.s32 	%r67, %r3, 31;
	shr.u32 	%r68, %r67, 27;
	add.s32 	%r69, %r3, %r68;
	shr.s32 	%r70, %r69, 5;
	shl.b32 	%r71, %r70, 2;
	add.s32 	%r10, %r24, %r71;
	mov.u32 	%r73, 2;
	mov.b32 	%r74, %f171;
	mov.u32 	%r75, 31;
	mov.u32 	%r76, 16;
	mov.u32 	%r77, -1;
	shfl.sync.bfly.b32 	%r78|%p6, %r74, %r76, %r75, %r77;
	mov.b32 	%f75, %r78;
	add.f32 	%f76, %f171, %f75;
	mov.b32 	%r79, %f76;
	mov.u32 	%r80, 8;
	shfl.sync.bfly.b32 	%r81|%p7, %r79, %r80, %r75, %r77;
	mov.b32 	%f77, %r81;
	add.f32 	%f78, %f76, %f77;
	mov.b32 	%r82, %f78;
	mov.u32 	%r83, 4;
	shfl.sync.bfly.b32 	%r84|%p8, %r82, %r83, %r75, %r77;
	mov.b32 	%f79, %r84;
	add.f32 	%f80, %f78, %f79;
	mov.b32 	%r85, %f80;
	shfl.sync.bfly.b32 	%r86|%p9, %r85, %r73, %r75, %r77;
	mov.b32 	%f81, %r86;
	add.f32 	%f82, %f80, %f81;
	mov.b32 	%r87, %f82;
	mov.u32 	%r88, 1;
	shfl.sync.bfly.b32 	%r89|%p10, %r87, %r88, %r75, %r77;
	mov.b32 	%f83, %r89;
	add.f32 	%f84, %f82, %f83;
	st.local.f32 	[%rd2], %f84;
	st.shared.f32 	[%r10], %f84;
	bar.sync 	0;
	@%p1 bra 	$L__BB75_11;

	ld.shared.f32 	%f85, [%r4];
	mov.b32 	%r90, %f85;
	shfl.sync.bfly.b32 	%r94|%p12, %r90, %r76, %r75, %r77;
	mov.b32 	%f86, %r94;
	add.f32 	%f87, %f85, %f86;
	mov.b32 	%r95, %f87;
	shfl.sync.bfly.b32 	%r97|%p13, %r95, %r80, %r75, %r77;
	mov.b32 	%f88, %r97;
	add.f32 	%f89, %f87, %f88;
	mov.b32 	%r98, %f89;
	shfl.sync.bfly.b32 	%r100|%p14, %r98, %r83, %r75, %r77;
	mov.b32 	%f90, %r100;
	add.f32 	%f91, %f89, %f90;
	mov.b32 	%r101, %f91;
	shfl.sync.bfly.b32 	%r103|%p15, %r101, %r73, %r75, %r77;
	mov.b32 	%f92, %r103;
	add.f32 	%f93, %f91, %f92;
	mov.b32 	%r104, %f93;
	shfl.sync.bfly.b32 	%r106|%p16, %r104, %r88, %r75, %r77;
	mov.b32 	%f94, %r106;
	add.f32 	%f95, %f93, %f94;
	st.local.f32 	[%rd2], %f95;

$L__BB75_11:
	bar.sync 	0;
	mov.b32 	%r107, %f170;
	shfl.sync.bfly.b32 	%r111|%p18, %r107, %r76, %r75, %r77;
	mov.b32 	%f96, %r111;
	add.f32 	%f97, %f170, %f96;
	mov.b32 	%r112, %f97;
	shfl.sync.bfly.b32 	%r114|%p19, %r112, %r80, %r75, %r77;
	mov.b32 	%f98, %r114;
	add.f32 	%f99, %f97, %f98;
	mov.b32 	%r115, %f99;
	shfl.sync.bfly.b32 	%r117|%p20, %r115, %r83, %r75, %r77;
	mov.b32 	%f100, %r117;
	add.f32 	%f101, %f99, %f100;
	mov.b32 	%r118, %f101;
	shfl.sync.bfly.b32 	%r120|%p21, %r118, %r73, %r75, %r77;
	mov.b32 	%f102, %r120;
	add.f32 	%f103, %f101, %f102;
	mov.b32 	%r121, %f103;
	shfl.sync.bfly.b32 	%r123|%p22, %r121, %r88, %r75, %r77;
	mov.b32 	%f104, %r123;
	add.f32 	%f105, %f103, %f104;
	st.local.f32 	[%rd2+4], %f105;
	st.shared.f32 	[%r10], %f105;
	bar.sync 	0;
	@%p1 bra 	$L__BB75_13;

	ld.shared.f32 	%f106, [%r4];
	mov.b32 	%r124, %f106;
	mov.u32 	%r125, 31;
	mov.u32 	%r126, 16;
	mov.u32 	%r127, -1;
	shfl.sync.bfly.b32 	%r128|%p23, %r124, %r126, %r125, %r127;
	mov.b32 	%f107, %r128;
	add.f32 	%f108, %f106, %f107;
	mov.b32 	%r129, %f108;
	mov.u32 	%r130, 8;
	shfl.sync.bfly.b32 	%r131|%p24, %r129, %r130, %r125, %r127;
	mov.b32 	%f109, %r131;
	add.f32 	%f110, %f108, %f109;
	mov.b32 	%r132, %f110;
	mov.u32 	%r133, 4;
	shfl.sync.bfly.b32 	%r134|%p25, %r132, %r133, %r125, %r127;
	mov.b32 	%f111, %r134;
	add.f32 	%f112, %f110, %f111;
	mov.b32 	%r135, %f112;
	mov.u32 	%r136, 2;
	shfl.sync.bfly.b32 	%r137|%p26, %r135, %r136, %r125, %r127;
	mov.b32 	%f113, %r137;
	add.f32 	%f114, %f112, %f113;
	mov.b32 	%r138, %f114;
	mov.u32 	%r139, 1;
	shfl.sync.bfly.b32 	%r140|%p27, %r138, %r139, %r125, %r127;
	mov.b32 	%f115, %r140;
	add.f32 	%f116, %f114, %f115;
	st.local.f32 	[%rd2+4], %f116;

$L__BB75_13:
	bar.sync 	0;
	mov.b32 	%r141, %f169;
	mov.u32 	%r142, 31;
	mov.u32 	%r143, 16;
	mov.u32 	%r144, -1;
	shfl.sync.bfly.b32 	%r145|%p29, %r141, %r143, %r142, %r144;
	mov.b32 	%f117, %r145;
	add.f32 	%f118, %f169, %f117;
	mov.b32 	%r146, %f118;
	mov.u32 	%r147, 8;
	shfl.sync.bfly.b32 	%r148|%p30, %r146, %r147, %r142, %r144;
	mov.b32 	%f119, %r148;
	add.f32 	%f120, %f118, %f119;
	mov.b32 	%r149, %f120;
	mov.u32 	%r150, 4;
	shfl.sync.bfly.b32 	%r151|%p31, %r149, %r150, %r142, %r144;
	mov.b32 	%f121, %r151;
	add.f32 	%f122, %f120, %f121;
	mov.b32 	%r152, %f122;
	mov.u32 	%r153, 2;
	shfl.sync.bfly.b32 	%r154|%p32, %r152, %r153, %r142, %r144;
	mov.b32 	%f123, %r154;
	add.f32 	%f124, %f122, %f123;
	mov.b32 	%r155, %f124;
	mov.u32 	%r156, 1;
	shfl.sync.bfly.b32 	%r157|%p33, %r155, %r156, %r142, %r144;
	mov.b32 	%f125, %r157;
	add.f32 	%f126, %f124, %f125;
	st.local.f32 	[%rd2+8], %f126;
	st.shared.f32 	[%r10], %f126;
	bar.sync 	0;
	@%p1 bra 	$L__BB75_15;

	ld.shared.f32 	%f127, [%r4];
	mov.b32 	%r158, %f127;
	shfl.sync.bfly.b32 	%r162|%p34, %r158, %r143, %r142, %r144;
	mov.b32 	%f128, %r162;
	add.f32 	%f129, %f127, %f128;
	mov.b32 	%r163, %f129;
	shfl.sync.bfly.b32 	%r165|%p35, %r163, %r147, %r142, %r144;
	mov.b32 	%f130, %r165;
	add.f32 	%f131, %f129, %f130;
	mov.b32 	%r166, %f131;
	shfl.sync.bfly.b32 	%r168|%p36, %r166, %r150, %r142, %r144;
	mov.b32 	%f132, %r168;
	add.f32 	%f133, %f131, %f132;
	mov.b32 	%r169, %f133;
	shfl.sync.bfly.b32 	%r171|%p37, %r169, %r153, %r142, %r144;
	mov.b32 	%f134, %r171;
	add.f32 	%f135, %f133, %f134;
	mov.b32 	%r172, %f135;
	shfl.sync.bfly.b32 	%r174|%p38, %r172, %r156, %r142, %r144;
	mov.b32 	%f136, %r174;
	add.f32 	%f137, %f135, %f136;
	st.local.f32 	[%rd2+8], %f137;

$L__BB75_15:
	bar.sync 	0;
	mov.b32 	%r175, %f168;
	shfl.sync.bfly.b32 	%r179|%p40, %r175, %r143, %r142, %r144;
	mov.b32 	%f138, %r179;
	add.f32 	%f139, %f168, %f138;
	mov.b32 	%r180, %f139;
	shfl.sync.bfly.b32 	%r182|%p41, %r180, %r147, %r142, %r144;
	mov.b32 	%f140, %r182;
	add.f32 	%f141, %f139, %f140;
	mov.b32 	%r183, %f141;
	shfl.sync.bfly.b32 	%r185|%p42, %r183, %r150, %r142, %r144;
	mov.b32 	%f142, %r185;
	add.f32 	%f143, %f141, %f142;
	mov.b32 	%r186, %f143;
	shfl.sync.bfly.b32 	%r188|%p43, %r186, %r153, %r142, %r144;
	mov.b32 	%f144, %r188;
	add.f32 	%f145, %f143, %f144;
	mov.b32 	%r189, %f145;
	shfl.sync.bfly.b32 	%r191|%p44, %r189, %r156, %r142, %r144;
	mov.b32 	%f146, %r191;
	add.f32 	%f147, %f145, %f146;
	st.local.f32 	[%rd2+12], %f147;
	st.shared.f32 	[%r10], %f147;
	bar.sync 	0;
	@%p1 bra 	$L__BB75_17;

	ld.shared.f32 	%f148, [%r4];
	mov.b32 	%r192, %f148;
	mov.u32 	%r193, 31;
	mov.u32 	%r194, 16;
	mov.u32 	%r195, -1;
	shfl.sync.bfly.b32 	%r196|%p45, %r192, %r194, %r193, %r195;
	mov.b32 	%f149, %r196;
	add.f32 	%f150, %f148, %f149;
	mov.b32 	%r197, %f150;
	mov.u32 	%r198, 8;
	shfl.sync.bfly.b32 	%r199|%p46, %r197, %r198, %r193, %r195;
	mov.b32 	%f151, %r199;
	add.f32 	%f152, %f150, %f151;
	mov.b32 	%r200, %f152;
	mov.u32 	%r201, 4;
	shfl.sync.bfly.b32 	%r202|%p47, %r200, %r201, %r193, %r195;
	mov.b32 	%f153, %r202;
	add.f32 	%f154, %f152, %f153;
	mov.b32 	%r203, %f154;
	mov.u32 	%r204, 2;
	shfl.sync.bfly.b32 	%r205|%p48, %r203, %r204, %r193, %r195;
	mov.b32 	%f155, %r205;
	add.f32 	%f156, %f154, %f155;
	mov.b32 	%r206, %f156;
	mov.u32 	%r207, 1;
	shfl.sync.bfly.b32 	%r208|%p49, %r206, %r207, %r193, %r195;
	mov.b32 	%f157, %r208;
	add.f32 	%f158, %f156, %f157;
	st.local.f32 	[%rd2+12], %f158;

$L__BB75_17:
	bar.sync 	0;
	setp.gt.s32 	%p50, %r3, 3;
	@%p50 bra 	$L__BB75_19;

	mad.lo.s32 	%r209, %r3, %r13, %r2;
	cvt.s64.s32 	%rd51, %r209;
	mul.lo.s32 	%r210, %r1, %r14;
	cvt.s64.s32 	%rd52, %r210;
	add.s64 	%rd53, %rd52, %rd51;
	mul.wide.s32 	%rd54, %r3, 4;
	add.s64 	%rd55, %rd2, %rd54;
	ld.local.f32 	%f159, [%rd55];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f159;}

	// end inline asm
	cvta.to.global.u64 	%rd56, %rd18;
	shl.b64 	%rd57, %rd53, 1;
	add.s64 	%rd58, %rd56, %rd57;
	st.global.u16 	[%rd58], %rs1;

$L__BB75_19:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_5_bs_64
.visible .entry ggml_matvec_f16_ncols_5_bs_64(
	.param .u64 ggml_matvec_f16_ncols_5_bs_64_param_0,
	.param .u64 ggml_matvec_f16_ncols_5_bs_64_param_1,
	.param .u64 ggml_matvec_f16_ncols_5_bs_64_param_2,
	.param .u32 ggml_matvec_f16_ncols_5_bs_64_param_3,
	.param .u32 ggml_matvec_f16_ncols_5_bs_64_param_4,
	.param .u32 ggml_matvec_f16_ncols_5_bs_64_param_5,
	.param .u32 ggml_matvec_f16_ncols_5_bs_64_param_6,
	.param .u32 ggml_matvec_f16_ncols_5_bs_64_param_7,
	.param .u32 ggml_matvec_f16_ncols_5_bs_64_param_8,
	.param .u32 ggml_matvec_f16_ncols_5_bs_64_param_9,
	.param .u32 ggml_matvec_f16_ncols_5_bs_64_param_10,
	.param .u32 ggml_matvec_f16_ncols_5_bs_64_param_11
)
{
	.local .align 4 .b8 	__local_depot76[20];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<62>;
	.reg .b16 	%rs<2>;
	.reg .f32 	%f<213>;
	.reg .b32 	%r<257>;
	.reg .b64 	%rd<64>;


	mov.u64 	%SPL, __local_depot76;
	ld.param.u64 	%rd19, [ggml_matvec_f16_ncols_5_bs_64_param_0];
	ld.param.u64 	%rd20, [ggml_matvec_f16_ncols_5_bs_64_param_1];
	ld.param.u64 	%rd18, [ggml_matvec_f16_ncols_5_bs_64_param_2];
	ld.param.u32 	%r11, [ggml_matvec_f16_ncols_5_bs_64_param_3];
	ld.param.u32 	%r15, [ggml_matvec_f16_ncols_5_bs_64_param_5];
	ld.param.u32 	%r12, [ggml_matvec_f16_ncols_5_bs_64_param_6];
	ld.param.u32 	%r13, [ggml_matvec_f16_ncols_5_bs_64_param_7];
	ld.param.u32 	%r16, [ggml_matvec_f16_ncols_5_bs_64_param_8];
	ld.param.u32 	%r17, [ggml_matvec_f16_ncols_5_bs_64_param_9];
	ld.param.u32 	%r18, [ggml_matvec_f16_ncols_5_bs_64_param_10];
	ld.param.u32 	%r14, [ggml_matvec_f16_ncols_5_bs_64_param_11];
	cvta.to.global.u64 	%rd63, %rd20;
	add.u64 	%rd2, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r19, %r1, %r16;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r20, %r2, %r15;
	mad.lo.s32 	%r21, %r19, %r17, %r20;
	cvt.s64.s32 	%rd3, %r21;
	cvta.to.global.u64 	%rd4, %rd19;
	mul.lo.s32 	%r22, %r1, %r18;
	cvt.s64.s32 	%rd5, %r22;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r23, %r3, 2;
	mov.u32 	%r24, data_mmv;
	add.s32 	%r4, %r24, %r23;
	@%p1 bra 	$L__BB76_2;

	mov.u32 	%r25, 0;
	st.shared.u32 	[%r4], %r25;

$L__BB76_2:
	bar.sync 	0;
	mov.f32 	%f208, 0f00000000;
	mov.u32 	%r26, 0;
	st.local.u32 	[%rd2], %r26;
	st.local.u32 	[%rd2+4], %r26;
	st.local.u32 	[%rd2+8], %r26;
	st.local.u32 	[%rd2+12], %r26;
	st.local.u32 	[%rd2+16], %r26;
	setp.ge.s32 	%p2, %r3, %r11;
	mov.f32 	%f209, %f208;
	mov.f32 	%f210, %f208;
	mov.f32 	%f211, %f208;
	mov.f32 	%f212, %f208;
	@%p2 bra 	$L__BB76_9;

	not.b32 	%r27, %r3;
	add.s32 	%r5, %r27, %r11;
	and.b32  	%r28, %r5, 64;
	setp.ne.s32 	%p3, %r28, 0;
	mov.f32 	%f208, 0f00000000;
	mov.u32 	%r256, %r3;
	@%p3 bra 	$L__BB76_5;

	shl.b64 	%rd22, %rd5, 1;
	add.s64 	%rd23, %rd63, %rd22;
	shl.b64 	%rd24, %rd3, 1;
	add.s64 	%rd25, %rd4, %rd24;
	mul.wide.s32 	%rd26, %r3, 4;
	add.s64 	%rd27, %rd25, %rd26;
	ld.global.nc.u32 	%r29, [%rd27];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r29;
  cvt.f32.f16 %f36, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r29;
  cvt.f32.f16 %f37, high;}

	// end inline asm
	add.s64 	%rd28, %rd23, %rd26;
	ld.global.nc.u32 	%r31, [%rd28];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r31;
  cvt.f32.f16 %f38, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r31;
  cvt.f32.f16 %f39, high;}

	// end inline asm
	fma.rn.f32 	%f48, %f36, %f38, 0f00000000;
	fma.rn.f32 	%f212, %f37, %f39, %f48;
	st.local.f32 	[%rd2], %f212;
	mul.wide.s32 	%rd29, %r12, 4;
	add.s64 	%rd30, %rd28, %rd29;
	ld.global.nc.u32 	%r33, [%rd30];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r33;
  cvt.f32.f16 %f40, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r33;
  cvt.f32.f16 %f41, high;}

	// end inline asm
	fma.rn.f32 	%f49, %f36, %f40, 0f00000000;
	fma.rn.f32 	%f211, %f37, %f41, %f49;
	st.local.f32 	[%rd2+4], %f211;
	add.s32 	%r41, %r3, %r12;
	add.s32 	%r42, %r41, %r12;
	shl.b32 	%r43, %r12, 1;
	mul.wide.s32 	%rd31, %r43, 4;
	add.s64 	%rd32, %rd28, %rd31;
	ld.global.nc.u32 	%r35, [%rd32];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r35;
  cvt.f32.f16 %f42, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r35;
  cvt.f32.f16 %f43, high;}

	// end inline asm
	fma.rn.f32 	%f50, %f36, %f42, 0f00000000;
	fma.rn.f32 	%f210, %f37, %f43, %f50;
	st.local.f32 	[%rd2+8], %f210;
	add.s32 	%r44, %r42, %r12;
	mul.wide.s32 	%rd33, %r44, 4;
	add.s64 	%rd34, %rd23, %rd33;
	ld.global.nc.u32 	%r37, [%rd34];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r37;
  cvt.f32.f16 %f44, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r37;
  cvt.f32.f16 %f45, high;}

	// end inline asm
	fma.rn.f32 	%f51, %f36, %f44, 0f00000000;
	fma.rn.f32 	%f209, %f37, %f45, %f51;
	st.local.f32 	[%rd2+12], %f209;
	add.s64 	%rd35, %rd32, %rd31;
	ld.global.nc.u32 	%r39, [%rd35];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r39;
  cvt.f32.f16 %f46, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r39;
  cvt.f32.f16 %f47, high;}

	// end inline asm
	fma.rn.f32 	%f52, %f36, %f46, 0f00000000;
	fma.rn.f32 	%f208, %f37, %f47, %f52;
	st.local.f32 	[%rd2+16], %f208;
	add.s32 	%r256, %r3, 64;

$L__BB76_5:
	and.b32  	%r45, %r5, -64;
	setp.eq.s32 	%p4, %r45, 0;
	@%p4 bra 	$L__BB76_9;

	add.s32 	%r46, %r256, %r12;
	add.s32 	%r47, %r46, 64;
	mul.wide.s32 	%rd36, %r47, 4;
	shl.b64 	%rd37, %rd5, 1;
	add.s64 	%rd7, %rd36, %rd37;
	shl.b32 	%r48, %r12, 1;
	add.s32 	%r49, %r256, %r48;
	mad.lo.s32 	%r50, %r12, 3, %r256;
	shl.b32 	%r51, %r12, 2;
	add.s32 	%r52, %r256, %r51;
	mul.wide.s32 	%rd38, %r49, 4;
	add.s64 	%rd8, %rd38, %rd37;
	mul.wide.s32 	%rd39, %r50, 4;
	add.s64 	%rd9, %rd39, %rd37;
	mul.wide.s32 	%rd40, %r52, 4;
	add.s64 	%rd10, %rd40, %rd37;
	mul.wide.s32 	%rd41, %r256, 2;
	add.s64 	%rd42, %rd41, %rd3;
	shl.b64 	%rd43, %rd42, 1;
	add.s64 	%rd44, %rd4, %rd43;
	add.s64 	%rd62, %rd44, 256;
	mul.wide.s32 	%rd45, %r256, 4;
	mul.wide.s32 	%rd46, %r12, 4;
	add.s64 	%rd47, %rd45, %rd46;
	add.s64 	%rd12, %rd47, %rd37;
	add.s64 	%rd13, %rd45, %rd37;

$L__BB76_7:
	ld.global.nc.u32 	%r53, [%rd62+-256];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r53;
  cvt.f32.f16 %f53, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r53;
  cvt.f32.f16 %f54, high;}

	// end inline asm
	add.s64 	%rd48, %rd63, %rd13;
	ld.global.nc.u32 	%r55, [%rd48];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r55;
  cvt.f32.f16 %f55, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r55;
  cvt.f32.f16 %f56, high;}

	// end inline asm
	fma.rn.f32 	%f77, %f53, %f55, %f212;
	fma.rn.f32 	%f78, %f54, %f56, %f77;
	add.s64 	%rd49, %rd63, %rd12;
	ld.global.nc.u32 	%r57, [%rd49];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r57;
  cvt.f32.f16 %f57, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r57;
  cvt.f32.f16 %f58, high;}

	// end inline asm
	fma.rn.f32 	%f79, %f53, %f57, %f211;
	fma.rn.f32 	%f80, %f54, %f58, %f79;
	add.s64 	%rd50, %rd63, %rd8;
	ld.global.nc.u32 	%r59, [%rd50];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r59;
  cvt.f32.f16 %f59, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r59;
  cvt.f32.f16 %f60, high;}

	// end inline asm
	fma.rn.f32 	%f81, %f53, %f59, %f210;
	fma.rn.f32 	%f82, %f54, %f60, %f81;
	add.s64 	%rd51, %rd63, %rd9;
	ld.global.nc.u32 	%r61, [%rd51];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r61;
  cvt.f32.f16 %f61, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r61;
  cvt.f32.f16 %f62, high;}

	// end inline asm
	fma.rn.f32 	%f83, %f53, %f61, %f209;
	fma.rn.f32 	%f84, %f54, %f62, %f83;
	add.s64 	%rd52, %rd63, %rd10;
	ld.global.nc.u32 	%r63, [%rd52];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r63;
  cvt.f32.f16 %f63, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r63;
  cvt.f32.f16 %f64, high;}

	// end inline asm
	fma.rn.f32 	%f85, %f53, %f63, %f208;
	fma.rn.f32 	%f86, %f54, %f64, %f85;
	ld.global.nc.u32 	%r65, [%rd62];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r65;
  cvt.f32.f16 %f65, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r65;
  cvt.f32.f16 %f66, high;}

	// end inline asm
	ld.global.nc.u32 	%r67, [%rd48+256];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r67;
  cvt.f32.f16 %f67, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r67;
  cvt.f32.f16 %f68, high;}

	// end inline asm
	fma.rn.f32 	%f87, %f65, %f67, %f78;
	fma.rn.f32 	%f212, %f66, %f68, %f87;
	add.s64 	%rd53, %rd63, %rd7;
	ld.global.nc.u32 	%r69, [%rd53];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r69;
  cvt.f32.f16 %f69, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r69;
  cvt.f32.f16 %f70, high;}

	// end inline asm
	fma.rn.f32 	%f88, %f65, %f69, %f80;
	fma.rn.f32 	%f211, %f66, %f70, %f88;
	ld.global.nc.u32 	%r71, [%rd50+256];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r71;
  cvt.f32.f16 %f71, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r71;
  cvt.f32.f16 %f72, high;}

	// end inline asm
	fma.rn.f32 	%f89, %f65, %f71, %f82;
	fma.rn.f32 	%f210, %f66, %f72, %f89;
	ld.global.nc.u32 	%r73, [%rd51+256];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r73;
  cvt.f32.f16 %f73, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r73;
  cvt.f32.f16 %f74, high;}

	// end inline asm
	fma.rn.f32 	%f90, %f65, %f73, %f84;
	fma.rn.f32 	%f209, %f66, %f74, %f90;
	ld.global.nc.u32 	%r75, [%rd52+256];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r75;
  cvt.f32.f16 %f75, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r75;
  cvt.f32.f16 %f76, high;}

	// end inline asm
	fma.rn.f32 	%f91, %f65, %f75, %f86;
	fma.rn.f32 	%f208, %f66, %f76, %f91;
	add.s64 	%rd63, %rd63, 512;
	add.s64 	%rd62, %rd62, 512;
	add.s32 	%r256, %r256, 128;
	setp.lt.s32 	%p5, %r256, %r11;
	@%p5 bra 	$L__BB76_7;

	st.local.f32 	[%rd2], %f212;
	st.local.f32 	[%rd2+4], %f211;
	st.local.f32 	[%rd2+8], %f210;
	st.local.f32 	[%rd2+12], %f209;
	st.local.f32 	[%rd2+16], %f208;

$L__BB76_9:
	shr.s32 	%r77, %r3, 31;
	shr.u32 	%r78, %r77, 27;
	add.s32 	%r79, %r3, %r78;
	shr.s32 	%r80, %r79, 5;
	shl.b32 	%r81, %r80, 2;
	add.s32 	%r10, %r24, %r81;
	mov.u32 	%r83, 2;
	mov.b32 	%r84, %f212;
	mov.u32 	%r85, 31;
	mov.u32 	%r86, 16;
	mov.u32 	%r87, -1;
	shfl.sync.bfly.b32 	%r88|%p6, %r84, %r86, %r85, %r87;
	mov.b32 	%f92, %r88;
	add.f32 	%f93, %f212, %f92;
	mov.b32 	%r89, %f93;
	mov.u32 	%r90, 8;
	shfl.sync.bfly.b32 	%r91|%p7, %r89, %r90, %r85, %r87;
	mov.b32 	%f94, %r91;
	add.f32 	%f95, %f93, %f94;
	mov.b32 	%r92, %f95;
	mov.u32 	%r93, 4;
	shfl.sync.bfly.b32 	%r94|%p8, %r92, %r93, %r85, %r87;
	mov.b32 	%f96, %r94;
	add.f32 	%f97, %f95, %f96;
	mov.b32 	%r95, %f97;
	shfl.sync.bfly.b32 	%r96|%p9, %r95, %r83, %r85, %r87;
	mov.b32 	%f98, %r96;
	add.f32 	%f99, %f97, %f98;
	mov.b32 	%r97, %f99;
	mov.u32 	%r98, 1;
	shfl.sync.bfly.b32 	%r99|%p10, %r97, %r98, %r85, %r87;
	mov.b32 	%f100, %r99;
	add.f32 	%f101, %f99, %f100;
	st.local.f32 	[%rd2], %f101;
	st.shared.f32 	[%r10], %f101;
	bar.sync 	0;
	@%p1 bra 	$L__BB76_11;

	ld.shared.f32 	%f102, [%r4];
	mov.b32 	%r100, %f102;
	shfl.sync.bfly.b32 	%r104|%p12, %r100, %r86, %r85, %r87;
	mov.b32 	%f103, %r104;
	add.f32 	%f104, %f102, %f103;
	mov.b32 	%r105, %f104;
	shfl.sync.bfly.b32 	%r107|%p13, %r105, %r90, %r85, %r87;
	mov.b32 	%f105, %r107;
	add.f32 	%f106, %f104, %f105;
	mov.b32 	%r108, %f106;
	shfl.sync.bfly.b32 	%r110|%p14, %r108, %r93, %r85, %r87;
	mov.b32 	%f107, %r110;
	add.f32 	%f108, %f106, %f107;
	mov.b32 	%r111, %f108;
	shfl.sync.bfly.b32 	%r113|%p15, %r111, %r83, %r85, %r87;
	mov.b32 	%f109, %r113;
	add.f32 	%f110, %f108, %f109;
	mov.b32 	%r114, %f110;
	shfl.sync.bfly.b32 	%r116|%p16, %r114, %r98, %r85, %r87;
	mov.b32 	%f111, %r116;
	add.f32 	%f112, %f110, %f111;
	st.local.f32 	[%rd2], %f112;

$L__BB76_11:
	bar.sync 	0;
	mov.b32 	%r117, %f211;
	shfl.sync.bfly.b32 	%r121|%p18, %r117, %r86, %r85, %r87;
	mov.b32 	%f113, %r121;
	add.f32 	%f114, %f211, %f113;
	mov.b32 	%r122, %f114;
	shfl.sync.bfly.b32 	%r124|%p19, %r122, %r90, %r85, %r87;
	mov.b32 	%f115, %r124;
	add.f32 	%f116, %f114, %f115;
	mov.b32 	%r125, %f116;
	shfl.sync.bfly.b32 	%r127|%p20, %r125, %r93, %r85, %r87;
	mov.b32 	%f117, %r127;
	add.f32 	%f118, %f116, %f117;
	mov.b32 	%r128, %f118;
	shfl.sync.bfly.b32 	%r130|%p21, %r128, %r83, %r85, %r87;
	mov.b32 	%f119, %r130;
	add.f32 	%f120, %f118, %f119;
	mov.b32 	%r131, %f120;
	shfl.sync.bfly.b32 	%r133|%p22, %r131, %r98, %r85, %r87;
	mov.b32 	%f121, %r133;
	add.f32 	%f122, %f120, %f121;
	st.local.f32 	[%rd2+4], %f122;
	st.shared.f32 	[%r10], %f122;
	bar.sync 	0;
	@%p1 bra 	$L__BB76_13;

	ld.shared.f32 	%f123, [%r4];
	mov.b32 	%r134, %f123;
	mov.u32 	%r135, 31;
	mov.u32 	%r136, 16;
	mov.u32 	%r137, -1;
	shfl.sync.bfly.b32 	%r138|%p23, %r134, %r136, %r135, %r137;
	mov.b32 	%f124, %r138;
	add.f32 	%f125, %f123, %f124;
	mov.b32 	%r139, %f125;
	mov.u32 	%r140, 8;
	shfl.sync.bfly.b32 	%r141|%p24, %r139, %r140, %r135, %r137;
	mov.b32 	%f126, %r141;
	add.f32 	%f127, %f125, %f126;
	mov.b32 	%r142, %f127;
	mov.u32 	%r143, 4;
	shfl.sync.bfly.b32 	%r144|%p25, %r142, %r143, %r135, %r137;
	mov.b32 	%f128, %r144;
	add.f32 	%f129, %f127, %f128;
	mov.b32 	%r145, %f129;
	mov.u32 	%r146, 2;
	shfl.sync.bfly.b32 	%r147|%p26, %r145, %r146, %r135, %r137;
	mov.b32 	%f130, %r147;
	add.f32 	%f131, %f129, %f130;
	mov.b32 	%r148, %f131;
	mov.u32 	%r149, 1;
	shfl.sync.bfly.b32 	%r150|%p27, %r148, %r149, %r135, %r137;
	mov.b32 	%f132, %r150;
	add.f32 	%f133, %f131, %f132;
	st.local.f32 	[%rd2+4], %f133;

$L__BB76_13:
	bar.sync 	0;
	mov.b32 	%r151, %f210;
	mov.u32 	%r152, 31;
	mov.u32 	%r153, 16;
	mov.u32 	%r154, -1;
	shfl.sync.bfly.b32 	%r155|%p29, %r151, %r153, %r152, %r154;
	mov.b32 	%f134, %r155;
	add.f32 	%f135, %f210, %f134;
	mov.b32 	%r156, %f135;
	mov.u32 	%r157, 8;
	shfl.sync.bfly.b32 	%r158|%p30, %r156, %r157, %r152, %r154;
	mov.b32 	%f136, %r158;
	add.f32 	%f137, %f135, %f136;
	mov.b32 	%r159, %f137;
	mov.u32 	%r160, 4;
	shfl.sync.bfly.b32 	%r161|%p31, %r159, %r160, %r152, %r154;
	mov.b32 	%f138, %r161;
	add.f32 	%f139, %f137, %f138;
	mov.b32 	%r162, %f139;
	mov.u32 	%r163, 2;
	shfl.sync.bfly.b32 	%r164|%p32, %r162, %r163, %r152, %r154;
	mov.b32 	%f140, %r164;
	add.f32 	%f141, %f139, %f140;
	mov.b32 	%r165, %f141;
	mov.u32 	%r166, 1;
	shfl.sync.bfly.b32 	%r167|%p33, %r165, %r166, %r152, %r154;
	mov.b32 	%f142, %r167;
	add.f32 	%f143, %f141, %f142;
	st.local.f32 	[%rd2+8], %f143;
	st.shared.f32 	[%r10], %f143;
	bar.sync 	0;
	@%p1 bra 	$L__BB76_15;

	ld.shared.f32 	%f144, [%r4];
	mov.b32 	%r168, %f144;
	shfl.sync.bfly.b32 	%r172|%p34, %r168, %r153, %r152, %r154;
	mov.b32 	%f145, %r172;
	add.f32 	%f146, %f144, %f145;
	mov.b32 	%r173, %f146;
	shfl.sync.bfly.b32 	%r175|%p35, %r173, %r157, %r152, %r154;
	mov.b32 	%f147, %r175;
	add.f32 	%f148, %f146, %f147;
	mov.b32 	%r176, %f148;
	shfl.sync.bfly.b32 	%r178|%p36, %r176, %r160, %r152, %r154;
	mov.b32 	%f149, %r178;
	add.f32 	%f150, %f148, %f149;
	mov.b32 	%r179, %f150;
	shfl.sync.bfly.b32 	%r181|%p37, %r179, %r163, %r152, %r154;
	mov.b32 	%f151, %r181;
	add.f32 	%f152, %f150, %f151;
	mov.b32 	%r182, %f152;
	shfl.sync.bfly.b32 	%r184|%p38, %r182, %r166, %r152, %r154;
	mov.b32 	%f153, %r184;
	add.f32 	%f154, %f152, %f153;
	st.local.f32 	[%rd2+8], %f154;

$L__BB76_15:
	bar.sync 	0;
	mov.b32 	%r185, %f209;
	shfl.sync.bfly.b32 	%r189|%p40, %r185, %r153, %r152, %r154;
	mov.b32 	%f155, %r189;
	add.f32 	%f156, %f209, %f155;
	mov.b32 	%r190, %f156;
	shfl.sync.bfly.b32 	%r192|%p41, %r190, %r157, %r152, %r154;
	mov.b32 	%f157, %r192;
	add.f32 	%f158, %f156, %f157;
	mov.b32 	%r193, %f158;
	shfl.sync.bfly.b32 	%r195|%p42, %r193, %r160, %r152, %r154;
	mov.b32 	%f159, %r195;
	add.f32 	%f160, %f158, %f159;
	mov.b32 	%r196, %f160;
	shfl.sync.bfly.b32 	%r198|%p43, %r196, %r163, %r152, %r154;
	mov.b32 	%f161, %r198;
	add.f32 	%f162, %f160, %f161;
	mov.b32 	%r199, %f162;
	shfl.sync.bfly.b32 	%r201|%p44, %r199, %r166, %r152, %r154;
	mov.b32 	%f163, %r201;
	add.f32 	%f164, %f162, %f163;
	st.local.f32 	[%rd2+12], %f164;
	st.shared.f32 	[%r10], %f164;
	bar.sync 	0;
	@%p1 bra 	$L__BB76_17;

	ld.shared.f32 	%f165, [%r4];
	mov.b32 	%r202, %f165;
	mov.u32 	%r203, 31;
	mov.u32 	%r204, 16;
	mov.u32 	%r205, -1;
	shfl.sync.bfly.b32 	%r206|%p45, %r202, %r204, %r203, %r205;
	mov.b32 	%f166, %r206;
	add.f32 	%f167, %f165, %f166;
	mov.b32 	%r207, %f167;
	mov.u32 	%r208, 8;
	shfl.sync.bfly.b32 	%r209|%p46, %r207, %r208, %r203, %r205;
	mov.b32 	%f168, %r209;
	add.f32 	%f169, %f167, %f168;
	mov.b32 	%r210, %f169;
	mov.u32 	%r211, 4;
	shfl.sync.bfly.b32 	%r212|%p47, %r210, %r211, %r203, %r205;
	mov.b32 	%f170, %r212;
	add.f32 	%f171, %f169, %f170;
	mov.b32 	%r213, %f171;
	mov.u32 	%r214, 2;
	shfl.sync.bfly.b32 	%r215|%p48, %r213, %r214, %r203, %r205;
	mov.b32 	%f172, %r215;
	add.f32 	%f173, %f171, %f172;
	mov.b32 	%r216, %f173;
	mov.u32 	%r217, 1;
	shfl.sync.bfly.b32 	%r218|%p49, %r216, %r217, %r203, %r205;
	mov.b32 	%f174, %r218;
	add.f32 	%f175, %f173, %f174;
	st.local.f32 	[%rd2+12], %f175;

$L__BB76_17:
	bar.sync 	0;
	mov.b32 	%r219, %f208;
	mov.u32 	%r220, 31;
	mov.u32 	%r221, 16;
	mov.u32 	%r222, -1;
	shfl.sync.bfly.b32 	%r223|%p51, %r219, %r221, %r220, %r222;
	mov.b32 	%f176, %r223;
	add.f32 	%f177, %f208, %f176;
	mov.b32 	%r224, %f177;
	mov.u32 	%r225, 8;
	shfl.sync.bfly.b32 	%r226|%p52, %r224, %r225, %r220, %r222;
	mov.b32 	%f178, %r226;
	add.f32 	%f179, %f177, %f178;
	mov.b32 	%r227, %f179;
	mov.u32 	%r228, 4;
	shfl.sync.bfly.b32 	%r229|%p53, %r227, %r228, %r220, %r222;
	mov.b32 	%f180, %r229;
	add.f32 	%f181, %f179, %f180;
	mov.b32 	%r230, %f181;
	mov.u32 	%r231, 2;
	shfl.sync.bfly.b32 	%r232|%p54, %r230, %r231, %r220, %r222;
	mov.b32 	%f182, %r232;
	add.f32 	%f183, %f181, %f182;
	mov.b32 	%r233, %f183;
	mov.u32 	%r234, 1;
	shfl.sync.bfly.b32 	%r235|%p55, %r233, %r234, %r220, %r222;
	mov.b32 	%f184, %r235;
	add.f32 	%f185, %f183, %f184;
	st.local.f32 	[%rd2+16], %f185;
	st.shared.f32 	[%r10], %f185;
	bar.sync 	0;
	@%p1 bra 	$L__BB76_19;

	ld.shared.f32 	%f186, [%r4];
	mov.b32 	%r236, %f186;
	shfl.sync.bfly.b32 	%r240|%p56, %r236, %r221, %r220, %r222;
	mov.b32 	%f187, %r240;
	add.f32 	%f188, %f186, %f187;
	mov.b32 	%r241, %f188;
	shfl.sync.bfly.b32 	%r243|%p57, %r241, %r225, %r220, %r222;
	mov.b32 	%f189, %r243;
	add.f32 	%f190, %f188, %f189;
	mov.b32 	%r244, %f190;
	shfl.sync.bfly.b32 	%r246|%p58, %r244, %r228, %r220, %r222;
	mov.b32 	%f191, %r246;
	add.f32 	%f192, %f190, %f191;
	mov.b32 	%r247, %f192;
	shfl.sync.bfly.b32 	%r249|%p59, %r247, %r231, %r220, %r222;
	mov.b32 	%f193, %r249;
	add.f32 	%f194, %f192, %f193;
	mov.b32 	%r250, %f194;
	shfl.sync.bfly.b32 	%r252|%p60, %r250, %r234, %r220, %r222;
	mov.b32 	%f195, %r252;
	add.f32 	%f196, %f194, %f195;
	st.local.f32 	[%rd2+16], %f196;

$L__BB76_19:
	bar.sync 	0;
	setp.gt.s32 	%p61, %r3, 4;
	@%p61 bra 	$L__BB76_21;

	mad.lo.s32 	%r253, %r3, %r13, %r2;
	cvt.s64.s32 	%rd54, %r253;
	mul.lo.s32 	%r254, %r1, %r14;
	cvt.s64.s32 	%rd55, %r254;
	add.s64 	%rd56, %rd55, %rd54;
	mul.wide.s32 	%rd57, %r3, 4;
	add.s64 	%rd58, %rd2, %rd57;
	ld.local.f32 	%f197, [%rd58];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f197;}

	// end inline asm
	cvta.to.global.u64 	%rd59, %rd18;
	shl.b64 	%rd60, %rd56, 1;
	add.s64 	%rd61, %rd59, %rd60;
	st.global.u16 	[%rd61], %rs1;

$L__BB76_21:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_6_bs_64
.visible .entry ggml_matvec_f16_ncols_6_bs_64(
	.param .u64 ggml_matvec_f16_ncols_6_bs_64_param_0,
	.param .u64 ggml_matvec_f16_ncols_6_bs_64_param_1,
	.param .u64 ggml_matvec_f16_ncols_6_bs_64_param_2,
	.param .u32 ggml_matvec_f16_ncols_6_bs_64_param_3,
	.param .u32 ggml_matvec_f16_ncols_6_bs_64_param_4,
	.param .u32 ggml_matvec_f16_ncols_6_bs_64_param_5,
	.param .u32 ggml_matvec_f16_ncols_6_bs_64_param_6,
	.param .u32 ggml_matvec_f16_ncols_6_bs_64_param_7,
	.param .u32 ggml_matvec_f16_ncols_6_bs_64_param_8,
	.param .u32 ggml_matvec_f16_ncols_6_bs_64_param_9,
	.param .u32 ggml_matvec_f16_ncols_6_bs_64_param_10,
	.param .u32 ggml_matvec_f16_ncols_6_bs_64_param_11
)
{
	.local .align 8 .b8 	__local_depot77[24];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<73>;
	.reg .b16 	%rs<2>;
	.reg .f32 	%f<254>;
	.reg .b32 	%r<295>;
	.reg .b64 	%rd<67>;


	mov.u64 	%SPL, __local_depot77;
	ld.param.u64 	%rd20, [ggml_matvec_f16_ncols_6_bs_64_param_0];
	ld.param.u64 	%rd21, [ggml_matvec_f16_ncols_6_bs_64_param_1];
	ld.param.u64 	%rd19, [ggml_matvec_f16_ncols_6_bs_64_param_2];
	ld.param.u32 	%r11, [ggml_matvec_f16_ncols_6_bs_64_param_3];
	ld.param.u32 	%r15, [ggml_matvec_f16_ncols_6_bs_64_param_5];
	ld.param.u32 	%r12, [ggml_matvec_f16_ncols_6_bs_64_param_6];
	ld.param.u32 	%r13, [ggml_matvec_f16_ncols_6_bs_64_param_7];
	ld.param.u32 	%r16, [ggml_matvec_f16_ncols_6_bs_64_param_8];
	ld.param.u32 	%r17, [ggml_matvec_f16_ncols_6_bs_64_param_9];
	ld.param.u32 	%r18, [ggml_matvec_f16_ncols_6_bs_64_param_10];
	ld.param.u32 	%r14, [ggml_matvec_f16_ncols_6_bs_64_param_11];
	cvta.to.global.u64 	%rd66, %rd21;
	add.u64 	%rd2, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r19, %r1, %r16;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r20, %r2, %r15;
	mad.lo.s32 	%r21, %r19, %r17, %r20;
	cvt.s64.s32 	%rd3, %r21;
	cvta.to.global.u64 	%rd4, %rd20;
	mul.lo.s32 	%r22, %r1, %r18;
	cvt.s64.s32 	%rd5, %r22;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r23, %r3, 2;
	mov.u32 	%r24, data_mmv;
	add.s32 	%r4, %r24, %r23;
	@%p1 bra 	$L__BB77_2;

	mov.u32 	%r25, 0;
	st.shared.u32 	[%r4], %r25;

$L__BB77_2:
	bar.sync 	0;
	mov.f32 	%f248, 0f00000000;
	st.local.v2.f32 	[%rd2], {%f248, %f248};
	st.local.v2.f32 	[%rd2+8], {%f248, %f248};
	st.local.v2.f32 	[%rd2+16], {%f248, %f248};
	setp.ge.s32 	%p2, %r3, %r11;
	mov.f32 	%f249, %f248;
	mov.f32 	%f250, %f248;
	mov.f32 	%f251, %f248;
	mov.f32 	%f252, %f248;
	mov.f32 	%f253, %f248;
	@%p2 bra 	$L__BB77_9;

	not.b32 	%r26, %r3;
	add.s32 	%r5, %r26, %r11;
	and.b32  	%r27, %r5, 64;
	setp.ne.s32 	%p3, %r27, 0;
	mov.f32 	%f248, 0f00000000;
	mov.u32 	%r294, %r3;
	@%p3 bra 	$L__BB77_5;

	shl.b64 	%rd23, %rd5, 1;
	add.s64 	%rd24, %rd66, %rd23;
	shl.b64 	%rd25, %rd3, 1;
	add.s64 	%rd26, %rd4, %rd25;
	mul.wide.s32 	%rd27, %r3, 4;
	add.s64 	%rd28, %rd26, %rd27;
	ld.global.nc.u32 	%r28, [%rd28];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r28;
  cvt.f32.f16 %f43, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r28;
  cvt.f32.f16 %f44, high;}

	// end inline asm
	add.s64 	%rd29, %rd24, %rd27;
	ld.global.nc.u32 	%r30, [%rd29];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r30;
  cvt.f32.f16 %f45, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r30;
  cvt.f32.f16 %f46, high;}

	// end inline asm
	fma.rn.f32 	%f57, %f43, %f45, 0f00000000;
	fma.rn.f32 	%f253, %f44, %f46, %f57;
	st.local.f32 	[%rd2], %f253;
	mul.wide.s32 	%rd30, %r12, 4;
	add.s64 	%rd31, %rd29, %rd30;
	ld.global.nc.u32 	%r32, [%rd31];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r32;
  cvt.f32.f16 %f47, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r32;
  cvt.f32.f16 %f48, high;}

	// end inline asm
	fma.rn.f32 	%f58, %f43, %f47, 0f00000000;
	fma.rn.f32 	%f252, %f44, %f48, %f58;
	st.local.f32 	[%rd2+4], %f252;
	add.s32 	%r42, %r3, %r12;
	add.s32 	%r43, %r42, %r12;
	mul.wide.s32 	%rd32, %r43, 4;
	add.s64 	%rd33, %rd24, %rd32;
	ld.global.nc.u32 	%r34, [%rd33];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r34;
  cvt.f32.f16 %f49, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r34;
  cvt.f32.f16 %f50, high;}

	// end inline asm
	fma.rn.f32 	%f59, %f43, %f49, 0f00000000;
	fma.rn.f32 	%f251, %f44, %f50, %f59;
	st.local.f32 	[%rd2+8], %f251;
	add.s64 	%rd34, %rd33, %rd30;
	ld.global.nc.u32 	%r36, [%rd34];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r36;
  cvt.f32.f16 %f51, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r36;
  cvt.f32.f16 %f52, high;}

	// end inline asm
	fma.rn.f32 	%f60, %f43, %f51, 0f00000000;
	fma.rn.f32 	%f250, %f44, %f52, %f60;
	st.local.f32 	[%rd2+12], %f250;
	add.s64 	%rd35, %rd34, %rd30;
	ld.global.nc.u32 	%r38, [%rd35];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r38;
  cvt.f32.f16 %f53, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r38;
  cvt.f32.f16 %f54, high;}

	// end inline asm
	fma.rn.f32 	%f61, %f43, %f53, 0f00000000;
	fma.rn.f32 	%f249, %f44, %f54, %f61;
	st.local.f32 	[%rd2+16], %f249;
	add.s64 	%rd36, %rd35, %rd30;
	ld.global.nc.u32 	%r40, [%rd36];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r40;
  cvt.f32.f16 %f55, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r40;
  cvt.f32.f16 %f56, high;}

	// end inline asm
	fma.rn.f32 	%f62, %f43, %f55, 0f00000000;
	fma.rn.f32 	%f248, %f44, %f56, %f62;
	st.local.f32 	[%rd2+20], %f248;
	add.s32 	%r294, %r3, 64;

$L__BB77_5:
	and.b32  	%r44, %r5, -64;
	setp.eq.s32 	%p4, %r44, 0;
	@%p4 bra 	$L__BB77_9;

	add.s32 	%r45, %r294, %r12;
	add.s32 	%r46, %r45, 64;
	mul.wide.s32 	%rd37, %r46, 4;
	shl.b64 	%rd38, %rd5, 1;
	add.s64 	%rd7, %rd37, %rd38;
	shl.b32 	%r47, %r12, 1;
	add.s32 	%r48, %r294, %r47;
	mad.lo.s32 	%r49, %r12, 3, %r294;
	shl.b32 	%r50, %r12, 2;
	add.s32 	%r51, %r294, %r50;
	mad.lo.s32 	%r52, %r12, 5, %r294;
	mul.wide.s32 	%rd39, %r48, 4;
	add.s64 	%rd8, %rd39, %rd38;
	mul.wide.s32 	%rd40, %r49, 4;
	add.s64 	%rd9, %rd40, %rd38;
	mul.wide.s32 	%rd41, %r51, 4;
	add.s64 	%rd10, %rd41, %rd38;
	mul.wide.s32 	%rd42, %r52, 4;
	add.s64 	%rd11, %rd42, %rd38;
	mul.wide.s32 	%rd43, %r294, 2;
	add.s64 	%rd44, %rd43, %rd3;
	shl.b64 	%rd45, %rd44, 1;
	add.s64 	%rd46, %rd4, %rd45;
	add.s64 	%rd65, %rd46, 256;
	mul.wide.s32 	%rd47, %r294, 4;
	mul.wide.s32 	%rd48, %r12, 4;
	add.s64 	%rd49, %rd47, %rd48;
	add.s64 	%rd13, %rd49, %rd38;
	add.s64 	%rd14, %rd47, %rd38;

$L__BB77_7:
	ld.global.nc.u32 	%r53, [%rd65+-256];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r53;
  cvt.f32.f16 %f63, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r53;
  cvt.f32.f16 %f64, high;}

	// end inline asm
	add.s64 	%rd50, %rd66, %rd14;
	ld.global.nc.u32 	%r55, [%rd50];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r55;
  cvt.f32.f16 %f65, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r55;
  cvt.f32.f16 %f66, high;}

	// end inline asm
	fma.rn.f32 	%f91, %f63, %f65, %f253;
	fma.rn.f32 	%f92, %f64, %f66, %f91;
	add.s64 	%rd51, %rd66, %rd13;
	ld.global.nc.u32 	%r57, [%rd51];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r57;
  cvt.f32.f16 %f67, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r57;
  cvt.f32.f16 %f68, high;}

	// end inline asm
	fma.rn.f32 	%f93, %f63, %f67, %f252;
	fma.rn.f32 	%f94, %f64, %f68, %f93;
	add.s64 	%rd52, %rd66, %rd8;
	ld.global.nc.u32 	%r59, [%rd52];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r59;
  cvt.f32.f16 %f69, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r59;
  cvt.f32.f16 %f70, high;}

	// end inline asm
	fma.rn.f32 	%f95, %f63, %f69, %f251;
	fma.rn.f32 	%f96, %f64, %f70, %f95;
	add.s64 	%rd53, %rd66, %rd9;
	ld.global.nc.u32 	%r61, [%rd53];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r61;
  cvt.f32.f16 %f71, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r61;
  cvt.f32.f16 %f72, high;}

	// end inline asm
	fma.rn.f32 	%f97, %f63, %f71, %f250;
	fma.rn.f32 	%f98, %f64, %f72, %f97;
	add.s64 	%rd54, %rd66, %rd10;
	ld.global.nc.u32 	%r63, [%rd54];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r63;
  cvt.f32.f16 %f73, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r63;
  cvt.f32.f16 %f74, high;}

	// end inline asm
	fma.rn.f32 	%f99, %f63, %f73, %f249;
	fma.rn.f32 	%f100, %f64, %f74, %f99;
	add.s64 	%rd55, %rd66, %rd11;
	ld.global.nc.u32 	%r65, [%rd55];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r65;
  cvt.f32.f16 %f75, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r65;
  cvt.f32.f16 %f76, high;}

	// end inline asm
	fma.rn.f32 	%f101, %f63, %f75, %f248;
	fma.rn.f32 	%f102, %f64, %f76, %f101;
	ld.global.nc.u32 	%r67, [%rd65];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r67;
  cvt.f32.f16 %f77, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r67;
  cvt.f32.f16 %f78, high;}

	// end inline asm
	ld.global.nc.u32 	%r69, [%rd50+256];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r69;
  cvt.f32.f16 %f79, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r69;
  cvt.f32.f16 %f80, high;}

	// end inline asm
	fma.rn.f32 	%f103, %f77, %f79, %f92;
	fma.rn.f32 	%f253, %f78, %f80, %f103;
	add.s64 	%rd56, %rd66, %rd7;
	ld.global.nc.u32 	%r71, [%rd56];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r71;
  cvt.f32.f16 %f81, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r71;
  cvt.f32.f16 %f82, high;}

	// end inline asm
	fma.rn.f32 	%f104, %f77, %f81, %f94;
	fma.rn.f32 	%f252, %f78, %f82, %f104;
	ld.global.nc.u32 	%r73, [%rd52+256];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r73;
  cvt.f32.f16 %f83, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r73;
  cvt.f32.f16 %f84, high;}

	// end inline asm
	fma.rn.f32 	%f105, %f77, %f83, %f96;
	fma.rn.f32 	%f251, %f78, %f84, %f105;
	ld.global.nc.u32 	%r75, [%rd53+256];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r75;
  cvt.f32.f16 %f85, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r75;
  cvt.f32.f16 %f86, high;}

	// end inline asm
	fma.rn.f32 	%f106, %f77, %f85, %f98;
	fma.rn.f32 	%f250, %f78, %f86, %f106;
	ld.global.nc.u32 	%r77, [%rd54+256];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r77;
  cvt.f32.f16 %f87, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r77;
  cvt.f32.f16 %f88, high;}

	// end inline asm
	fma.rn.f32 	%f107, %f77, %f87, %f100;
	fma.rn.f32 	%f249, %f78, %f88, %f107;
	ld.global.nc.u32 	%r79, [%rd55+256];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r79;
  cvt.f32.f16 %f89, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r79;
  cvt.f32.f16 %f90, high;}

	// end inline asm
	fma.rn.f32 	%f108, %f77, %f89, %f102;
	fma.rn.f32 	%f248, %f78, %f90, %f108;
	add.s64 	%rd66, %rd66, 512;
	add.s64 	%rd65, %rd65, 512;
	add.s32 	%r294, %r294, 128;
	setp.lt.s32 	%p5, %r294, %r11;
	@%p5 bra 	$L__BB77_7;

	st.local.v2.f32 	[%rd2], {%f253, %f252};
	st.local.v2.f32 	[%rd2+8], {%f251, %f250};
	st.local.v2.f32 	[%rd2+16], {%f249, %f248};

$L__BB77_9:
	shr.s32 	%r81, %r3, 31;
	shr.u32 	%r82, %r81, 27;
	add.s32 	%r83, %r3, %r82;
	shr.s32 	%r84, %r83, 5;
	shl.b32 	%r85, %r84, 2;
	add.s32 	%r10, %r24, %r85;
	mov.u32 	%r87, 2;
	mov.b32 	%r88, %f253;
	mov.u32 	%r89, 31;
	mov.u32 	%r90, 16;
	mov.u32 	%r91, -1;
	shfl.sync.bfly.b32 	%r92|%p6, %r88, %r90, %r89, %r91;
	mov.b32 	%f109, %r92;
	add.f32 	%f110, %f253, %f109;
	mov.b32 	%r93, %f110;
	mov.u32 	%r94, 8;
	shfl.sync.bfly.b32 	%r95|%p7, %r93, %r94, %r89, %r91;
	mov.b32 	%f111, %r95;
	add.f32 	%f112, %f110, %f111;
	mov.b32 	%r96, %f112;
	mov.u32 	%r97, 4;
	shfl.sync.bfly.b32 	%r98|%p8, %r96, %r97, %r89, %r91;
	mov.b32 	%f113, %r98;
	add.f32 	%f114, %f112, %f113;
	mov.b32 	%r99, %f114;
	shfl.sync.bfly.b32 	%r100|%p9, %r99, %r87, %r89, %r91;
	mov.b32 	%f115, %r100;
	add.f32 	%f116, %f114, %f115;
	mov.b32 	%r101, %f116;
	mov.u32 	%r102, 1;
	shfl.sync.bfly.b32 	%r103|%p10, %r101, %r102, %r89, %r91;
	mov.b32 	%f117, %r103;
	add.f32 	%f118, %f116, %f117;
	st.local.f32 	[%rd2], %f118;
	st.shared.f32 	[%r10], %f118;
	bar.sync 	0;
	@%p1 bra 	$L__BB77_11;

	ld.shared.f32 	%f119, [%r4];
	mov.b32 	%r104, %f119;
	shfl.sync.bfly.b32 	%r108|%p12, %r104, %r90, %r89, %r91;
	mov.b32 	%f120, %r108;
	add.f32 	%f121, %f119, %f120;
	mov.b32 	%r109, %f121;
	shfl.sync.bfly.b32 	%r111|%p13, %r109, %r94, %r89, %r91;
	mov.b32 	%f122, %r111;
	add.f32 	%f123, %f121, %f122;
	mov.b32 	%r112, %f123;
	shfl.sync.bfly.b32 	%r114|%p14, %r112, %r97, %r89, %r91;
	mov.b32 	%f124, %r114;
	add.f32 	%f125, %f123, %f124;
	mov.b32 	%r115, %f125;
	shfl.sync.bfly.b32 	%r117|%p15, %r115, %r87, %r89, %r91;
	mov.b32 	%f126, %r117;
	add.f32 	%f127, %f125, %f126;
	mov.b32 	%r118, %f127;
	shfl.sync.bfly.b32 	%r120|%p16, %r118, %r102, %r89, %r91;
	mov.b32 	%f128, %r120;
	add.f32 	%f129, %f127, %f128;
	st.local.f32 	[%rd2], %f129;

$L__BB77_11:
	bar.sync 	0;
	mov.b32 	%r121, %f252;
	shfl.sync.bfly.b32 	%r125|%p18, %r121, %r90, %r89, %r91;
	mov.b32 	%f130, %r125;
	add.f32 	%f131, %f252, %f130;
	mov.b32 	%r126, %f131;
	shfl.sync.bfly.b32 	%r128|%p19, %r126, %r94, %r89, %r91;
	mov.b32 	%f132, %r128;
	add.f32 	%f133, %f131, %f132;
	mov.b32 	%r129, %f133;
	shfl.sync.bfly.b32 	%r131|%p20, %r129, %r97, %r89, %r91;
	mov.b32 	%f134, %r131;
	add.f32 	%f135, %f133, %f134;
	mov.b32 	%r132, %f135;
	shfl.sync.bfly.b32 	%r134|%p21, %r132, %r87, %r89, %r91;
	mov.b32 	%f136, %r134;
	add.f32 	%f137, %f135, %f136;
	mov.b32 	%r135, %f137;
	shfl.sync.bfly.b32 	%r137|%p22, %r135, %r102, %r89, %r91;
	mov.b32 	%f138, %r137;
	add.f32 	%f139, %f137, %f138;
	st.local.f32 	[%rd2+4], %f139;
	st.shared.f32 	[%r10], %f139;
	bar.sync 	0;
	@%p1 bra 	$L__BB77_13;

	ld.shared.f32 	%f140, [%r4];
	mov.b32 	%r138, %f140;
	mov.u32 	%r139, 31;
	mov.u32 	%r140, 16;
	mov.u32 	%r141, -1;
	shfl.sync.bfly.b32 	%r142|%p23, %r138, %r140, %r139, %r141;
	mov.b32 	%f141, %r142;
	add.f32 	%f142, %f140, %f141;
	mov.b32 	%r143, %f142;
	mov.u32 	%r144, 8;
	shfl.sync.bfly.b32 	%r145|%p24, %r143, %r144, %r139, %r141;
	mov.b32 	%f143, %r145;
	add.f32 	%f144, %f142, %f143;
	mov.b32 	%r146, %f144;
	mov.u32 	%r147, 4;
	shfl.sync.bfly.b32 	%r148|%p25, %r146, %r147, %r139, %r141;
	mov.b32 	%f145, %r148;
	add.f32 	%f146, %f144, %f145;
	mov.b32 	%r149, %f146;
	mov.u32 	%r150, 2;
	shfl.sync.bfly.b32 	%r151|%p26, %r149, %r150, %r139, %r141;
	mov.b32 	%f147, %r151;
	add.f32 	%f148, %f146, %f147;
	mov.b32 	%r152, %f148;
	mov.u32 	%r153, 1;
	shfl.sync.bfly.b32 	%r154|%p27, %r152, %r153, %r139, %r141;
	mov.b32 	%f149, %r154;
	add.f32 	%f150, %f148, %f149;
	st.local.f32 	[%rd2+4], %f150;

$L__BB77_13:
	bar.sync 	0;
	mov.b32 	%r155, %f251;
	mov.u32 	%r156, 31;
	mov.u32 	%r157, 16;
	mov.u32 	%r158, -1;
	shfl.sync.bfly.b32 	%r159|%p29, %r155, %r157, %r156, %r158;
	mov.b32 	%f151, %r159;
	add.f32 	%f152, %f251, %f151;
	mov.b32 	%r160, %f152;
	mov.u32 	%r161, 8;
	shfl.sync.bfly.b32 	%r162|%p30, %r160, %r161, %r156, %r158;
	mov.b32 	%f153, %r162;
	add.f32 	%f154, %f152, %f153;
	mov.b32 	%r163, %f154;
	mov.u32 	%r164, 4;
	shfl.sync.bfly.b32 	%r165|%p31, %r163, %r164, %r156, %r158;
	mov.b32 	%f155, %r165;
	add.f32 	%f156, %f154, %f155;
	mov.b32 	%r166, %f156;
	mov.u32 	%r167, 2;
	shfl.sync.bfly.b32 	%r168|%p32, %r166, %r167, %r156, %r158;
	mov.b32 	%f157, %r168;
	add.f32 	%f158, %f156, %f157;
	mov.b32 	%r169, %f158;
	mov.u32 	%r170, 1;
	shfl.sync.bfly.b32 	%r171|%p33, %r169, %r170, %r156, %r158;
	mov.b32 	%f159, %r171;
	add.f32 	%f160, %f158, %f159;
	st.local.f32 	[%rd2+8], %f160;
	st.shared.f32 	[%r10], %f160;
	bar.sync 	0;
	@%p1 bra 	$L__BB77_15;

	ld.shared.f32 	%f161, [%r4];
	mov.b32 	%r172, %f161;
	shfl.sync.bfly.b32 	%r176|%p34, %r172, %r157, %r156, %r158;
	mov.b32 	%f162, %r176;
	add.f32 	%f163, %f161, %f162;
	mov.b32 	%r177, %f163;
	shfl.sync.bfly.b32 	%r179|%p35, %r177, %r161, %r156, %r158;
	mov.b32 	%f164, %r179;
	add.f32 	%f165, %f163, %f164;
	mov.b32 	%r180, %f165;
	shfl.sync.bfly.b32 	%r182|%p36, %r180, %r164, %r156, %r158;
	mov.b32 	%f166, %r182;
	add.f32 	%f167, %f165, %f166;
	mov.b32 	%r183, %f167;
	shfl.sync.bfly.b32 	%r185|%p37, %r183, %r167, %r156, %r158;
	mov.b32 	%f168, %r185;
	add.f32 	%f169, %f167, %f168;
	mov.b32 	%r186, %f169;
	shfl.sync.bfly.b32 	%r188|%p38, %r186, %r170, %r156, %r158;
	mov.b32 	%f170, %r188;
	add.f32 	%f171, %f169, %f170;
	st.local.f32 	[%rd2+8], %f171;

$L__BB77_15:
	bar.sync 	0;
	mov.b32 	%r189, %f250;
	shfl.sync.bfly.b32 	%r193|%p40, %r189, %r157, %r156, %r158;
	mov.b32 	%f172, %r193;
	add.f32 	%f173, %f250, %f172;
	mov.b32 	%r194, %f173;
	shfl.sync.bfly.b32 	%r196|%p41, %r194, %r161, %r156, %r158;
	mov.b32 	%f174, %r196;
	add.f32 	%f175, %f173, %f174;
	mov.b32 	%r197, %f175;
	shfl.sync.bfly.b32 	%r199|%p42, %r197, %r164, %r156, %r158;
	mov.b32 	%f176, %r199;
	add.f32 	%f177, %f175, %f176;
	mov.b32 	%r200, %f177;
	shfl.sync.bfly.b32 	%r202|%p43, %r200, %r167, %r156, %r158;
	mov.b32 	%f178, %r202;
	add.f32 	%f179, %f177, %f178;
	mov.b32 	%r203, %f179;
	shfl.sync.bfly.b32 	%r205|%p44, %r203, %r170, %r156, %r158;
	mov.b32 	%f180, %r205;
	add.f32 	%f181, %f179, %f180;
	st.local.f32 	[%rd2+12], %f181;
	st.shared.f32 	[%r10], %f181;
	bar.sync 	0;
	@%p1 bra 	$L__BB77_17;

	ld.shared.f32 	%f182, [%r4];
	mov.b32 	%r206, %f182;
	mov.u32 	%r207, 31;
	mov.u32 	%r208, 16;
	mov.u32 	%r209, -1;
	shfl.sync.bfly.b32 	%r210|%p45, %r206, %r208, %r207, %r209;
	mov.b32 	%f183, %r210;
	add.f32 	%f184, %f182, %f183;
	mov.b32 	%r211, %f184;
	mov.u32 	%r212, 8;
	shfl.sync.bfly.b32 	%r213|%p46, %r211, %r212, %r207, %r209;
	mov.b32 	%f185, %r213;
	add.f32 	%f186, %f184, %f185;
	mov.b32 	%r214, %f186;
	mov.u32 	%r215, 4;
	shfl.sync.bfly.b32 	%r216|%p47, %r214, %r215, %r207, %r209;
	mov.b32 	%f187, %r216;
	add.f32 	%f188, %f186, %f187;
	mov.b32 	%r217, %f188;
	mov.u32 	%r218, 2;
	shfl.sync.bfly.b32 	%r219|%p48, %r217, %r218, %r207, %r209;
	mov.b32 	%f189, %r219;
	add.f32 	%f190, %f188, %f189;
	mov.b32 	%r220, %f190;
	mov.u32 	%r221, 1;
	shfl.sync.bfly.b32 	%r222|%p49, %r220, %r221, %r207, %r209;
	mov.b32 	%f191, %r222;
	add.f32 	%f192, %f190, %f191;
	st.local.f32 	[%rd2+12], %f192;

$L__BB77_17:
	bar.sync 	0;
	mov.b32 	%r223, %f249;
	mov.u32 	%r224, 31;
	mov.u32 	%r225, 16;
	mov.u32 	%r226, -1;
	shfl.sync.bfly.b32 	%r227|%p51, %r223, %r225, %r224, %r226;
	mov.b32 	%f193, %r227;
	add.f32 	%f194, %f249, %f193;
	mov.b32 	%r228, %f194;
	mov.u32 	%r229, 8;
	shfl.sync.bfly.b32 	%r230|%p52, %r228, %r229, %r224, %r226;
	mov.b32 	%f195, %r230;
	add.f32 	%f196, %f194, %f195;
	mov.b32 	%r231, %f196;
	mov.u32 	%r232, 4;
	shfl.sync.bfly.b32 	%r233|%p53, %r231, %r232, %r224, %r226;
	mov.b32 	%f197, %r233;
	add.f32 	%f198, %f196, %f197;
	mov.b32 	%r234, %f198;
	mov.u32 	%r235, 2;
	shfl.sync.bfly.b32 	%r236|%p54, %r234, %r235, %r224, %r226;
	mov.b32 	%f199, %r236;
	add.f32 	%f200, %f198, %f199;
	mov.b32 	%r237, %f200;
	mov.u32 	%r238, 1;
	shfl.sync.bfly.b32 	%r239|%p55, %r237, %r238, %r224, %r226;
	mov.b32 	%f201, %r239;
	add.f32 	%f202, %f200, %f201;
	st.local.f32 	[%rd2+16], %f202;
	st.shared.f32 	[%r10], %f202;
	bar.sync 	0;
	@%p1 bra 	$L__BB77_19;

	ld.shared.f32 	%f203, [%r4];
	mov.b32 	%r240, %f203;
	shfl.sync.bfly.b32 	%r244|%p56, %r240, %r225, %r224, %r226;
	mov.b32 	%f204, %r244;
	add.f32 	%f205, %f203, %f204;
	mov.b32 	%r245, %f205;
	shfl.sync.bfly.b32 	%r247|%p57, %r245, %r229, %r224, %r226;
	mov.b32 	%f206, %r247;
	add.f32 	%f207, %f205, %f206;
	mov.b32 	%r248, %f207;
	shfl.sync.bfly.b32 	%r250|%p58, %r248, %r232, %r224, %r226;
	mov.b32 	%f208, %r250;
	add.f32 	%f209, %f207, %f208;
	mov.b32 	%r251, %f209;
	shfl.sync.bfly.b32 	%r253|%p59, %r251, %r235, %r224, %r226;
	mov.b32 	%f210, %r253;
	add.f32 	%f211, %f209, %f210;
	mov.b32 	%r254, %f211;
	shfl.sync.bfly.b32 	%r256|%p60, %r254, %r238, %r224, %r226;
	mov.b32 	%f212, %r256;
	add.f32 	%f213, %f211, %f212;
	st.local.f32 	[%rd2+16], %f213;

$L__BB77_19:
	bar.sync 	0;
	mov.b32 	%r257, %f248;
	shfl.sync.bfly.b32 	%r261|%p62, %r257, %r225, %r224, %r226;
	mov.b32 	%f214, %r261;
	add.f32 	%f215, %f248, %f214;
	mov.b32 	%r262, %f215;
	shfl.sync.bfly.b32 	%r264|%p63, %r262, %r229, %r224, %r226;
	mov.b32 	%f216, %r264;
	add.f32 	%f217, %f215, %f216;
	mov.b32 	%r265, %f217;
	shfl.sync.bfly.b32 	%r267|%p64, %r265, %r232, %r224, %r226;
	mov.b32 	%f218, %r267;
	add.f32 	%f219, %f217, %f218;
	mov.b32 	%r268, %f219;
	shfl.sync.bfly.b32 	%r270|%p65, %r268, %r235, %r224, %r226;
	mov.b32 	%f220, %r270;
	add.f32 	%f221, %f219, %f220;
	mov.b32 	%r271, %f221;
	shfl.sync.bfly.b32 	%r273|%p66, %r271, %r238, %r224, %r226;
	mov.b32 	%f222, %r273;
	add.f32 	%f223, %f221, %f222;
	st.local.f32 	[%rd2+20], %f223;
	st.shared.f32 	[%r10], %f223;
	bar.sync 	0;
	@%p1 bra 	$L__BB77_21;

	ld.shared.f32 	%f224, [%r4];
	mov.b32 	%r274, %f224;
	mov.u32 	%r275, 31;
	mov.u32 	%r276, 16;
	mov.u32 	%r277, -1;
	shfl.sync.bfly.b32 	%r278|%p67, %r274, %r276, %r275, %r277;
	mov.b32 	%f225, %r278;
	add.f32 	%f226, %f224, %f225;
	mov.b32 	%r279, %f226;
	mov.u32 	%r280, 8;
	shfl.sync.bfly.b32 	%r281|%p68, %r279, %r280, %r275, %r277;
	mov.b32 	%f227, %r281;
	add.f32 	%f228, %f226, %f227;
	mov.b32 	%r282, %f228;
	mov.u32 	%r283, 4;
	shfl.sync.bfly.b32 	%r284|%p69, %r282, %r283, %r275, %r277;
	mov.b32 	%f229, %r284;
	add.f32 	%f230, %f228, %f229;
	mov.b32 	%r285, %f230;
	mov.u32 	%r286, 2;
	shfl.sync.bfly.b32 	%r287|%p70, %r285, %r286, %r275, %r277;
	mov.b32 	%f231, %r287;
	add.f32 	%f232, %f230, %f231;
	mov.b32 	%r288, %f232;
	mov.u32 	%r289, 1;
	shfl.sync.bfly.b32 	%r290|%p71, %r288, %r289, %r275, %r277;
	mov.b32 	%f233, %r290;
	add.f32 	%f234, %f232, %f233;
	st.local.f32 	[%rd2+20], %f234;

$L__BB77_21:
	bar.sync 	0;
	setp.gt.s32 	%p72, %r3, 5;
	@%p72 bra 	$L__BB77_23;

	mad.lo.s32 	%r291, %r3, %r13, %r2;
	cvt.s64.s32 	%rd57, %r291;
	mul.lo.s32 	%r292, %r1, %r14;
	cvt.s64.s32 	%rd58, %r292;
	add.s64 	%rd59, %rd58, %rd57;
	mul.wide.s32 	%rd60, %r3, 4;
	add.s64 	%rd61, %rd2, %rd60;
	ld.local.f32 	%f235, [%rd61];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f235;}

	// end inline asm
	cvta.to.global.u64 	%rd62, %rd19;
	shl.b64 	%rd63, %rd59, 1;
	add.s64 	%rd64, %rd62, %rd63;
	st.global.u16 	[%rd64], %rs1;

$L__BB77_23:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_7_bs_64
.visible .entry ggml_matvec_f16_ncols_7_bs_64(
	.param .u64 ggml_matvec_f16_ncols_7_bs_64_param_0,
	.param .u64 ggml_matvec_f16_ncols_7_bs_64_param_1,
	.param .u64 ggml_matvec_f16_ncols_7_bs_64_param_2,
	.param .u32 ggml_matvec_f16_ncols_7_bs_64_param_3,
	.param .u32 ggml_matvec_f16_ncols_7_bs_64_param_4,
	.param .u32 ggml_matvec_f16_ncols_7_bs_64_param_5,
	.param .u32 ggml_matvec_f16_ncols_7_bs_64_param_6,
	.param .u32 ggml_matvec_f16_ncols_7_bs_64_param_7,
	.param .u32 ggml_matvec_f16_ncols_7_bs_64_param_8,
	.param .u32 ggml_matvec_f16_ncols_7_bs_64_param_9,
	.param .u32 ggml_matvec_f16_ncols_7_bs_64_param_10,
	.param .u32 ggml_matvec_f16_ncols_7_bs_64_param_11
)
{
	.local .align 4 .b8 	__local_depot78[28];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<82>;
	.reg .b16 	%rs<2>;
	.reg .f32 	%f<221>;
	.reg .b32 	%r<289>;
	.reg .b64 	%rd<43>;


	mov.u64 	%SPL, __local_depot78;
	ld.param.u64 	%rd13, [ggml_matvec_f16_ncols_7_bs_64_param_0];
	ld.param.u64 	%rd14, [ggml_matvec_f16_ncols_7_bs_64_param_1];
	ld.param.u64 	%rd15, [ggml_matvec_f16_ncols_7_bs_64_param_2];
	ld.param.u32 	%r8, [ggml_matvec_f16_ncols_7_bs_64_param_3];
	ld.param.u32 	%r9, [ggml_matvec_f16_ncols_7_bs_64_param_5];
	ld.param.u32 	%r10, [ggml_matvec_f16_ncols_7_bs_64_param_6];
	ld.param.u32 	%r11, [ggml_matvec_f16_ncols_7_bs_64_param_7];
	ld.param.u32 	%r12, [ggml_matvec_f16_ncols_7_bs_64_param_8];
	ld.param.u32 	%r13, [ggml_matvec_f16_ncols_7_bs_64_param_9];
	ld.param.u32 	%r14, [ggml_matvec_f16_ncols_7_bs_64_param_10];
	ld.param.u32 	%r15, [ggml_matvec_f16_ncols_7_bs_64_param_11];
	add.u64 	%rd1, %SPL, 0;
	mov.u32 	%r1, %ctaid.x;
	mov.u32 	%r2, %tid.x;
	setp.gt.s32 	%p1, %r2, 31;
	shl.b32 	%r16, %r2, 2;
	mov.u32 	%r17, data_mmv;
	add.s32 	%r3, %r17, %r16;
	@%p1 bra 	$L__BB78_2;

	mov.u32 	%r18, 0;
	st.shared.u32 	[%r3], %r18;

$L__BB78_2:
	mov.u32 	%r4, %ctaid.y;
	bar.sync 	0;
	mov.f32 	%f214, 0f00000000;
	mov.u32 	%r19, 0;
	st.local.u32 	[%rd1], %r19;
	st.local.u32 	[%rd1+4], %r19;
	st.local.u32 	[%rd1+8], %r19;
	st.local.u32 	[%rd1+12], %r19;
	st.local.u32 	[%rd1+16], %r19;
	st.local.u32 	[%rd1+20], %r19;
	st.local.u32 	[%rd1+24], %r19;
	setp.ge.s32 	%p2, %r2, %r8;
	mov.f32 	%f215, %f214;
	mov.f32 	%f216, %f214;
	mov.f32 	%f217, %f214;
	mov.f32 	%f218, %f214;
	mov.f32 	%f219, %f214;
	mov.f32 	%f220, %f214;
	@%p2 bra 	$L__BB78_6;

	shl.b32 	%r20, %r10, 1;
	add.s32 	%r21, %r2, %r20;
	mul.wide.s32 	%rd17, %r21, 4;
	mul.lo.s32 	%r22, %r4, %r14;
	mul.wide.s32 	%rd18, %r22, 2;
	add.s64 	%rd3, %rd17, %rd18;
	mul.wide.s32 	%rd19, %r2, 4;
	mul.wide.s32 	%rd4, %r10, 4;
	add.s64 	%rd20, %rd19, %rd4;
	add.s64 	%rd5, %rd20, %rd18;
	add.s64 	%rd6, %rd19, %rd18;
	mul.wide.s32 	%rd21, %r2, 2;
	div.s32 	%r23, %r4, %r12;
	mul.lo.s32 	%r24, %r1, %r9;
	mad.lo.s32 	%r25, %r23, %r13, %r24;
	cvt.s64.s32 	%rd22, %r25;
	add.s64 	%rd23, %rd21, %rd22;
	cvta.to.global.u64 	%rd24, %rd13;
	shl.b64 	%rd25, %rd23, 1;
	add.s64 	%rd41, %rd24, %rd25;
	cvta.to.global.u64 	%rd42, %rd14;
	mov.f32 	%f214, 0f00000000;
	mov.u32 	%r288, %r2;

$L__BB78_4:
	ld.global.nc.u32 	%r26, [%rd41];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r26;
  cvt.f32.f16 %f36, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r26;
  cvt.f32.f16 %f37, high;}

	// end inline asm
	add.s64 	%rd26, %rd42, %rd6;
	ld.global.nc.u32 	%r28, [%rd26];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r28;
  cvt.f32.f16 %f38, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r28;
  cvt.f32.f16 %f39, high;}

	// end inline asm
	fma.rn.f32 	%f52, %f36, %f38, %f220;
	fma.rn.f32 	%f220, %f37, %f39, %f52;
	add.s64 	%rd27, %rd42, %rd5;
	ld.global.nc.u32 	%r30, [%rd27];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r30;
  cvt.f32.f16 %f40, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r30;
  cvt.f32.f16 %f41, high;}

	// end inline asm
	fma.rn.f32 	%f53, %f36, %f40, %f219;
	fma.rn.f32 	%f219, %f37, %f41, %f53;
	add.s64 	%rd28, %rd42, %rd3;
	ld.global.nc.u32 	%r32, [%rd28];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r32;
  cvt.f32.f16 %f42, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r32;
  cvt.f32.f16 %f43, high;}

	// end inline asm
	fma.rn.f32 	%f54, %f36, %f42, %f218;
	fma.rn.f32 	%f218, %f37, %f43, %f54;
	add.s64 	%rd29, %rd28, %rd4;
	ld.global.nc.u32 	%r34, [%rd29];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r34;
  cvt.f32.f16 %f44, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r34;
  cvt.f32.f16 %f45, high;}

	// end inline asm
	fma.rn.f32 	%f55, %f36, %f44, %f217;
	fma.rn.f32 	%f217, %f37, %f45, %f55;
	add.s64 	%rd30, %rd29, %rd4;
	ld.global.nc.u32 	%r36, [%rd30];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r36;
  cvt.f32.f16 %f46, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r36;
  cvt.f32.f16 %f47, high;}

	// end inline asm
	fma.rn.f32 	%f56, %f36, %f46, %f216;
	fma.rn.f32 	%f216, %f37, %f47, %f56;
	add.s64 	%rd31, %rd30, %rd4;
	ld.global.nc.u32 	%r38, [%rd31];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r38;
  cvt.f32.f16 %f48, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r38;
  cvt.f32.f16 %f49, high;}

	// end inline asm
	fma.rn.f32 	%f57, %f36, %f48, %f215;
	fma.rn.f32 	%f215, %f37, %f49, %f57;
	add.s64 	%rd32, %rd31, %rd4;
	ld.global.nc.u32 	%r40, [%rd32];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r40;
  cvt.f32.f16 %f50, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r40;
  cvt.f32.f16 %f51, high;}

	// end inline asm
	fma.rn.f32 	%f58, %f36, %f50, %f214;
	fma.rn.f32 	%f214, %f37, %f51, %f58;
	add.s64 	%rd42, %rd42, 256;
	add.s64 	%rd41, %rd41, 256;
	add.s32 	%r288, %r288, 64;
	setp.lt.s32 	%p3, %r288, %r8;
	@%p3 bra 	$L__BB78_4;

	st.local.f32 	[%rd1], %f220;
	st.local.f32 	[%rd1+4], %f219;
	st.local.f32 	[%rd1+8], %f218;
	st.local.f32 	[%rd1+12], %f217;
	st.local.f32 	[%rd1+16], %f216;
	st.local.f32 	[%rd1+20], %f215;
	st.local.f32 	[%rd1+24], %f214;

$L__BB78_6:
	shr.s32 	%r42, %r2, 31;
	shr.u32 	%r43, %r42, 27;
	add.s32 	%r44, %r2, %r43;
	shr.s32 	%r45, %r44, 5;
	shl.b32 	%r46, %r45, 2;
	add.s32 	%r7, %r17, %r46;
	mov.u32 	%r48, 2;
	mov.b32 	%r49, %f220;
	mov.u32 	%r50, 31;
	mov.u32 	%r51, 16;
	mov.u32 	%r52, -1;
	shfl.sync.bfly.b32 	%r53|%p4, %r49, %r51, %r50, %r52;
	mov.b32 	%f59, %r53;
	add.f32 	%f60, %f220, %f59;
	mov.b32 	%r54, %f60;
	mov.u32 	%r55, 8;
	shfl.sync.bfly.b32 	%r56|%p5, %r54, %r55, %r50, %r52;
	mov.b32 	%f61, %r56;
	add.f32 	%f62, %f60, %f61;
	mov.b32 	%r57, %f62;
	mov.u32 	%r58, 4;
	shfl.sync.bfly.b32 	%r59|%p6, %r57, %r58, %r50, %r52;
	mov.b32 	%f63, %r59;
	add.f32 	%f64, %f62, %f63;
	mov.b32 	%r60, %f64;
	shfl.sync.bfly.b32 	%r61|%p7, %r60, %r48, %r50, %r52;
	mov.b32 	%f65, %r61;
	add.f32 	%f66, %f64, %f65;
	mov.b32 	%r62, %f66;
	mov.u32 	%r63, 1;
	shfl.sync.bfly.b32 	%r64|%p8, %r62, %r63, %r50, %r52;
	mov.b32 	%f67, %r64;
	add.f32 	%f68, %f66, %f67;
	st.local.f32 	[%rd1], %f68;
	st.shared.f32 	[%r7], %f68;
	bar.sync 	0;
	@%p1 bra 	$L__BB78_8;

	ld.shared.f32 	%f69, [%r3];
	mov.b32 	%r65, %f69;
	shfl.sync.bfly.b32 	%r69|%p10, %r65, %r51, %r50, %r52;
	mov.b32 	%f70, %r69;
	add.f32 	%f71, %f69, %f70;
	mov.b32 	%r70, %f71;
	shfl.sync.bfly.b32 	%r72|%p11, %r70, %r55, %r50, %r52;
	mov.b32 	%f72, %r72;
	add.f32 	%f73, %f71, %f72;
	mov.b32 	%r73, %f73;
	shfl.sync.bfly.b32 	%r75|%p12, %r73, %r58, %r50, %r52;
	mov.b32 	%f74, %r75;
	add.f32 	%f75, %f73, %f74;
	mov.b32 	%r76, %f75;
	shfl.sync.bfly.b32 	%r78|%p13, %r76, %r48, %r50, %r52;
	mov.b32 	%f76, %r78;
	add.f32 	%f77, %f75, %f76;
	mov.b32 	%r79, %f77;
	shfl.sync.bfly.b32 	%r81|%p14, %r79, %r63, %r50, %r52;
	mov.b32 	%f78, %r81;
	add.f32 	%f79, %f77, %f78;
	st.local.f32 	[%rd1], %f79;

$L__BB78_8:
	bar.sync 	0;
	mov.b32 	%r82, %f219;
	shfl.sync.bfly.b32 	%r86|%p16, %r82, %r51, %r50, %r52;
	mov.b32 	%f80, %r86;
	add.f32 	%f81, %f219, %f80;
	mov.b32 	%r87, %f81;
	shfl.sync.bfly.b32 	%r89|%p17, %r87, %r55, %r50, %r52;
	mov.b32 	%f82, %r89;
	add.f32 	%f83, %f81, %f82;
	mov.b32 	%r90, %f83;
	shfl.sync.bfly.b32 	%r92|%p18, %r90, %r58, %r50, %r52;
	mov.b32 	%f84, %r92;
	add.f32 	%f85, %f83, %f84;
	mov.b32 	%r93, %f85;
	shfl.sync.bfly.b32 	%r95|%p19, %r93, %r48, %r50, %r52;
	mov.b32 	%f86, %r95;
	add.f32 	%f87, %f85, %f86;
	mov.b32 	%r96, %f87;
	shfl.sync.bfly.b32 	%r98|%p20, %r96, %r63, %r50, %r52;
	mov.b32 	%f88, %r98;
	add.f32 	%f89, %f87, %f88;
	st.local.f32 	[%rd1+4], %f89;
	st.shared.f32 	[%r7], %f89;
	bar.sync 	0;
	@%p1 bra 	$L__BB78_10;

	ld.shared.f32 	%f90, [%r3];
	mov.b32 	%r99, %f90;
	mov.u32 	%r100, 31;
	mov.u32 	%r101, 16;
	mov.u32 	%r102, -1;
	shfl.sync.bfly.b32 	%r103|%p21, %r99, %r101, %r100, %r102;
	mov.b32 	%f91, %r103;
	add.f32 	%f92, %f90, %f91;
	mov.b32 	%r104, %f92;
	mov.u32 	%r105, 8;
	shfl.sync.bfly.b32 	%r106|%p22, %r104, %r105, %r100, %r102;
	mov.b32 	%f93, %r106;
	add.f32 	%f94, %f92, %f93;
	mov.b32 	%r107, %f94;
	mov.u32 	%r108, 4;
	shfl.sync.bfly.b32 	%r109|%p23, %r107, %r108, %r100, %r102;
	mov.b32 	%f95, %r109;
	add.f32 	%f96, %f94, %f95;
	mov.b32 	%r110, %f96;
	mov.u32 	%r111, 2;
	shfl.sync.bfly.b32 	%r112|%p24, %r110, %r111, %r100, %r102;
	mov.b32 	%f97, %r112;
	add.f32 	%f98, %f96, %f97;
	mov.b32 	%r113, %f98;
	mov.u32 	%r114, 1;
	shfl.sync.bfly.b32 	%r115|%p25, %r113, %r114, %r100, %r102;
	mov.b32 	%f99, %r115;
	add.f32 	%f100, %f98, %f99;
	st.local.f32 	[%rd1+4], %f100;

$L__BB78_10:
	bar.sync 	0;
	mov.b32 	%r116, %f218;
	mov.u32 	%r117, 31;
	mov.u32 	%r118, 16;
	mov.u32 	%r119, -1;
	shfl.sync.bfly.b32 	%r120|%p27, %r116, %r118, %r117, %r119;
	mov.b32 	%f101, %r120;
	add.f32 	%f102, %f218, %f101;
	mov.b32 	%r121, %f102;
	mov.u32 	%r122, 8;
	shfl.sync.bfly.b32 	%r123|%p28, %r121, %r122, %r117, %r119;
	mov.b32 	%f103, %r123;
	add.f32 	%f104, %f102, %f103;
	mov.b32 	%r124, %f104;
	mov.u32 	%r125, 4;
	shfl.sync.bfly.b32 	%r126|%p29, %r124, %r125, %r117, %r119;
	mov.b32 	%f105, %r126;
	add.f32 	%f106, %f104, %f105;
	mov.b32 	%r127, %f106;
	mov.u32 	%r128, 2;
	shfl.sync.bfly.b32 	%r129|%p30, %r127, %r128, %r117, %r119;
	mov.b32 	%f107, %r129;
	add.f32 	%f108, %f106, %f107;
	mov.b32 	%r130, %f108;
	mov.u32 	%r131, 1;
	shfl.sync.bfly.b32 	%r132|%p31, %r130, %r131, %r117, %r119;
	mov.b32 	%f109, %r132;
	add.f32 	%f110, %f108, %f109;
	st.local.f32 	[%rd1+8], %f110;
	st.shared.f32 	[%r7], %f110;
	bar.sync 	0;
	@%p1 bra 	$L__BB78_12;

	ld.shared.f32 	%f111, [%r3];
	mov.b32 	%r133, %f111;
	shfl.sync.bfly.b32 	%r137|%p32, %r133, %r118, %r117, %r119;
	mov.b32 	%f112, %r137;
	add.f32 	%f113, %f111, %f112;
	mov.b32 	%r138, %f113;
	shfl.sync.bfly.b32 	%r140|%p33, %r138, %r122, %r117, %r119;
	mov.b32 	%f114, %r140;
	add.f32 	%f115, %f113, %f114;
	mov.b32 	%r141, %f115;
	shfl.sync.bfly.b32 	%r143|%p34, %r141, %r125, %r117, %r119;
	mov.b32 	%f116, %r143;
	add.f32 	%f117, %f115, %f116;
	mov.b32 	%r144, %f117;
	shfl.sync.bfly.b32 	%r146|%p35, %r144, %r128, %r117, %r119;
	mov.b32 	%f118, %r146;
	add.f32 	%f119, %f117, %f118;
	mov.b32 	%r147, %f119;
	shfl.sync.bfly.b32 	%r149|%p36, %r147, %r131, %r117, %r119;
	mov.b32 	%f120, %r149;
	add.f32 	%f121, %f119, %f120;
	st.local.f32 	[%rd1+8], %f121;

$L__BB78_12:
	bar.sync 	0;
	mov.b32 	%r150, %f217;
	shfl.sync.bfly.b32 	%r154|%p38, %r150, %r118, %r117, %r119;
	mov.b32 	%f122, %r154;
	add.f32 	%f123, %f217, %f122;
	mov.b32 	%r155, %f123;
	shfl.sync.bfly.b32 	%r157|%p39, %r155, %r122, %r117, %r119;
	mov.b32 	%f124, %r157;
	add.f32 	%f125, %f123, %f124;
	mov.b32 	%r158, %f125;
	shfl.sync.bfly.b32 	%r160|%p40, %r158, %r125, %r117, %r119;
	mov.b32 	%f126, %r160;
	add.f32 	%f127, %f125, %f126;
	mov.b32 	%r161, %f127;
	shfl.sync.bfly.b32 	%r163|%p41, %r161, %r128, %r117, %r119;
	mov.b32 	%f128, %r163;
	add.f32 	%f129, %f127, %f128;
	mov.b32 	%r164, %f129;
	shfl.sync.bfly.b32 	%r166|%p42, %r164, %r131, %r117, %r119;
	mov.b32 	%f130, %r166;
	add.f32 	%f131, %f129, %f130;
	st.local.f32 	[%rd1+12], %f131;
	st.shared.f32 	[%r7], %f131;
	bar.sync 	0;
	@%p1 bra 	$L__BB78_14;

	ld.shared.f32 	%f132, [%r3];
	mov.b32 	%r167, %f132;
	mov.u32 	%r168, 31;
	mov.u32 	%r169, 16;
	mov.u32 	%r170, -1;
	shfl.sync.bfly.b32 	%r171|%p43, %r167, %r169, %r168, %r170;
	mov.b32 	%f133, %r171;
	add.f32 	%f134, %f132, %f133;
	mov.b32 	%r172, %f134;
	mov.u32 	%r173, 8;
	shfl.sync.bfly.b32 	%r174|%p44, %r172, %r173, %r168, %r170;
	mov.b32 	%f135, %r174;
	add.f32 	%f136, %f134, %f135;
	mov.b32 	%r175, %f136;
	mov.u32 	%r176, 4;
	shfl.sync.bfly.b32 	%r177|%p45, %r175, %r176, %r168, %r170;
	mov.b32 	%f137, %r177;
	add.f32 	%f138, %f136, %f137;
	mov.b32 	%r178, %f138;
	mov.u32 	%r179, 2;
	shfl.sync.bfly.b32 	%r180|%p46, %r178, %r179, %r168, %r170;
	mov.b32 	%f139, %r180;
	add.f32 	%f140, %f138, %f139;
	mov.b32 	%r181, %f140;
	mov.u32 	%r182, 1;
	shfl.sync.bfly.b32 	%r183|%p47, %r181, %r182, %r168, %r170;
	mov.b32 	%f141, %r183;
	add.f32 	%f142, %f140, %f141;
	st.local.f32 	[%rd1+12], %f142;

$L__BB78_14:
	bar.sync 	0;
	mov.b32 	%r184, %f216;
	mov.u32 	%r185, 31;
	mov.u32 	%r186, 16;
	mov.u32 	%r187, -1;
	shfl.sync.bfly.b32 	%r188|%p49, %r184, %r186, %r185, %r187;
	mov.b32 	%f143, %r188;
	add.f32 	%f144, %f216, %f143;
	mov.b32 	%r189, %f144;
	mov.u32 	%r190, 8;
	shfl.sync.bfly.b32 	%r191|%p50, %r189, %r190, %r185, %r187;
	mov.b32 	%f145, %r191;
	add.f32 	%f146, %f144, %f145;
	mov.b32 	%r192, %f146;
	mov.u32 	%r193, 4;
	shfl.sync.bfly.b32 	%r194|%p51, %r192, %r193, %r185, %r187;
	mov.b32 	%f147, %r194;
	add.f32 	%f148, %f146, %f147;
	mov.b32 	%r195, %f148;
	mov.u32 	%r196, 2;
	shfl.sync.bfly.b32 	%r197|%p52, %r195, %r196, %r185, %r187;
	mov.b32 	%f149, %r197;
	add.f32 	%f150, %f148, %f149;
	mov.b32 	%r198, %f150;
	mov.u32 	%r199, 1;
	shfl.sync.bfly.b32 	%r200|%p53, %r198, %r199, %r185, %r187;
	mov.b32 	%f151, %r200;
	add.f32 	%f152, %f150, %f151;
	st.local.f32 	[%rd1+16], %f152;
	st.shared.f32 	[%r7], %f152;
	bar.sync 	0;
	@%p1 bra 	$L__BB78_16;

	ld.shared.f32 	%f153, [%r3];
	mov.b32 	%r201, %f153;
	shfl.sync.bfly.b32 	%r205|%p54, %r201, %r186, %r185, %r187;
	mov.b32 	%f154, %r205;
	add.f32 	%f155, %f153, %f154;
	mov.b32 	%r206, %f155;
	shfl.sync.bfly.b32 	%r208|%p55, %r206, %r190, %r185, %r187;
	mov.b32 	%f156, %r208;
	add.f32 	%f157, %f155, %f156;
	mov.b32 	%r209, %f157;
	shfl.sync.bfly.b32 	%r211|%p56, %r209, %r193, %r185, %r187;
	mov.b32 	%f158, %r211;
	add.f32 	%f159, %f157, %f158;
	mov.b32 	%r212, %f159;
	shfl.sync.bfly.b32 	%r214|%p57, %r212, %r196, %r185, %r187;
	mov.b32 	%f160, %r214;
	add.f32 	%f161, %f159, %f160;
	mov.b32 	%r215, %f161;
	shfl.sync.bfly.b32 	%r217|%p58, %r215, %r199, %r185, %r187;
	mov.b32 	%f162, %r217;
	add.f32 	%f163, %f161, %f162;
	st.local.f32 	[%rd1+16], %f163;

$L__BB78_16:
	bar.sync 	0;
	mov.b32 	%r218, %f215;
	shfl.sync.bfly.b32 	%r222|%p60, %r218, %r186, %r185, %r187;
	mov.b32 	%f164, %r222;
	add.f32 	%f165, %f215, %f164;
	mov.b32 	%r223, %f165;
	shfl.sync.bfly.b32 	%r225|%p61, %r223, %r190, %r185, %r187;
	mov.b32 	%f166, %r225;
	add.f32 	%f167, %f165, %f166;
	mov.b32 	%r226, %f167;
	shfl.sync.bfly.b32 	%r228|%p62, %r226, %r193, %r185, %r187;
	mov.b32 	%f168, %r228;
	add.f32 	%f169, %f167, %f168;
	mov.b32 	%r229, %f169;
	shfl.sync.bfly.b32 	%r231|%p63, %r229, %r196, %r185, %r187;
	mov.b32 	%f170, %r231;
	add.f32 	%f171, %f169, %f170;
	mov.b32 	%r232, %f171;
	shfl.sync.bfly.b32 	%r234|%p64, %r232, %r199, %r185, %r187;
	mov.b32 	%f172, %r234;
	add.f32 	%f173, %f171, %f172;
	st.local.f32 	[%rd1+20], %f173;
	st.shared.f32 	[%r7], %f173;
	bar.sync 	0;
	@%p1 bra 	$L__BB78_18;

	ld.shared.f32 	%f174, [%r3];
	mov.b32 	%r235, %f174;
	mov.u32 	%r236, 31;
	mov.u32 	%r237, 16;
	mov.u32 	%r238, -1;
	shfl.sync.bfly.b32 	%r239|%p65, %r235, %r237, %r236, %r238;
	mov.b32 	%f175, %r239;
	add.f32 	%f176, %f174, %f175;
	mov.b32 	%r240, %f176;
	mov.u32 	%r241, 8;
	shfl.sync.bfly.b32 	%r242|%p66, %r240, %r241, %r236, %r238;
	mov.b32 	%f177, %r242;
	add.f32 	%f178, %f176, %f177;
	mov.b32 	%r243, %f178;
	mov.u32 	%r244, 4;
	shfl.sync.bfly.b32 	%r245|%p67, %r243, %r244, %r236, %r238;
	mov.b32 	%f179, %r245;
	add.f32 	%f180, %f178, %f179;
	mov.b32 	%r246, %f180;
	mov.u32 	%r247, 2;
	shfl.sync.bfly.b32 	%r248|%p68, %r246, %r247, %r236, %r238;
	mov.b32 	%f181, %r248;
	add.f32 	%f182, %f180, %f181;
	mov.b32 	%r249, %f182;
	mov.u32 	%r250, 1;
	shfl.sync.bfly.b32 	%r251|%p69, %r249, %r250, %r236, %r238;
	mov.b32 	%f183, %r251;
	add.f32 	%f184, %f182, %f183;
	st.local.f32 	[%rd1+20], %f184;

$L__BB78_18:
	bar.sync 	0;
	mov.b32 	%r252, %f214;
	mov.u32 	%r253, 31;
	mov.u32 	%r254, 16;
	mov.u32 	%r255, -1;
	shfl.sync.bfly.b32 	%r256|%p71, %r252, %r254, %r253, %r255;
	mov.b32 	%f185, %r256;
	add.f32 	%f186, %f214, %f185;
	mov.b32 	%r257, %f186;
	mov.u32 	%r258, 8;
	shfl.sync.bfly.b32 	%r259|%p72, %r257, %r258, %r253, %r255;
	mov.b32 	%f187, %r259;
	add.f32 	%f188, %f186, %f187;
	mov.b32 	%r260, %f188;
	mov.u32 	%r261, 4;
	shfl.sync.bfly.b32 	%r262|%p73, %r260, %r261, %r253, %r255;
	mov.b32 	%f189, %r262;
	add.f32 	%f190, %f188, %f189;
	mov.b32 	%r263, %f190;
	mov.u32 	%r264, 2;
	shfl.sync.bfly.b32 	%r265|%p74, %r263, %r264, %r253, %r255;
	mov.b32 	%f191, %r265;
	add.f32 	%f192, %f190, %f191;
	mov.b32 	%r266, %f192;
	mov.u32 	%r267, 1;
	shfl.sync.bfly.b32 	%r268|%p75, %r266, %r267, %r253, %r255;
	mov.b32 	%f193, %r268;
	add.f32 	%f194, %f192, %f193;
	st.local.f32 	[%rd1+24], %f194;
	st.shared.f32 	[%r7], %f194;
	bar.sync 	0;
	@%p1 bra 	$L__BB78_20;

	ld.shared.f32 	%f195, [%r3];
	mov.b32 	%r269, %f195;
	shfl.sync.bfly.b32 	%r273|%p76, %r269, %r254, %r253, %r255;
	mov.b32 	%f196, %r273;
	add.f32 	%f197, %f195, %f196;
	mov.b32 	%r274, %f197;
	shfl.sync.bfly.b32 	%r276|%p77, %r274, %r258, %r253, %r255;
	mov.b32 	%f198, %r276;
	add.f32 	%f199, %f197, %f198;
	mov.b32 	%r277, %f199;
	shfl.sync.bfly.b32 	%r279|%p78, %r277, %r261, %r253, %r255;
	mov.b32 	%f200, %r279;
	add.f32 	%f201, %f199, %f200;
	mov.b32 	%r280, %f201;
	shfl.sync.bfly.b32 	%r282|%p79, %r280, %r264, %r253, %r255;
	mov.b32 	%f202, %r282;
	add.f32 	%f203, %f201, %f202;
	mov.b32 	%r283, %f203;
	shfl.sync.bfly.b32 	%r285|%p80, %r283, %r267, %r253, %r255;
	mov.b32 	%f204, %r285;
	add.f32 	%f205, %f203, %f204;
	st.local.f32 	[%rd1+24], %f205;

$L__BB78_20:
	bar.sync 	0;
	setp.gt.s32 	%p81, %r2, 6;
	@%p81 bra 	$L__BB78_22;

	mad.lo.s32 	%r286, %r2, %r11, %r1;
	cvt.s64.s32 	%rd33, %r286;
	mul.lo.s32 	%r287, %r4, %r15;
	cvt.s64.s32 	%rd34, %r287;
	add.s64 	%rd35, %rd34, %rd33;
	mul.wide.s32 	%rd36, %r2, 4;
	add.s64 	%rd37, %rd1, %rd36;
	ld.local.f32 	%f206, [%rd37];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f206;}

	// end inline asm
	cvta.to.global.u64 	%rd38, %rd15;
	shl.b64 	%rd39, %rd35, 1;
	add.s64 	%rd40, %rd38, %rd39;
	st.global.u16 	[%rd40], %rs1;

$L__BB78_22:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_8_bs_64
.visible .entry ggml_matvec_f16_ncols_8_bs_64(
	.param .u64 ggml_matvec_f16_ncols_8_bs_64_param_0,
	.param .u64 ggml_matvec_f16_ncols_8_bs_64_param_1,
	.param .u64 ggml_matvec_f16_ncols_8_bs_64_param_2,
	.param .u32 ggml_matvec_f16_ncols_8_bs_64_param_3,
	.param .u32 ggml_matvec_f16_ncols_8_bs_64_param_4,
	.param .u32 ggml_matvec_f16_ncols_8_bs_64_param_5,
	.param .u32 ggml_matvec_f16_ncols_8_bs_64_param_6,
	.param .u32 ggml_matvec_f16_ncols_8_bs_64_param_7,
	.param .u32 ggml_matvec_f16_ncols_8_bs_64_param_8,
	.param .u32 ggml_matvec_f16_ncols_8_bs_64_param_9,
	.param .u32 ggml_matvec_f16_ncols_8_bs_64_param_10,
	.param .u32 ggml_matvec_f16_ncols_8_bs_64_param_11
)
{
	.local .align 16 .b8 	__local_depot79[32];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<93>;
	.reg .b16 	%rs<2>;
	.reg .f32 	%f<252>;
	.reg .b32 	%r<324>;
	.reg .b64 	%rd<45>;


	mov.u64 	%SPL, __local_depot79;
	ld.param.u64 	%rd14, [ggml_matvec_f16_ncols_8_bs_64_param_0];
	ld.param.u64 	%rd15, [ggml_matvec_f16_ncols_8_bs_64_param_1];
	ld.param.u64 	%rd16, [ggml_matvec_f16_ncols_8_bs_64_param_2];
	ld.param.u32 	%r8, [ggml_matvec_f16_ncols_8_bs_64_param_3];
	ld.param.u32 	%r9, [ggml_matvec_f16_ncols_8_bs_64_param_5];
	ld.param.u32 	%r10, [ggml_matvec_f16_ncols_8_bs_64_param_6];
	ld.param.u32 	%r11, [ggml_matvec_f16_ncols_8_bs_64_param_7];
	ld.param.u32 	%r12, [ggml_matvec_f16_ncols_8_bs_64_param_8];
	ld.param.u32 	%r13, [ggml_matvec_f16_ncols_8_bs_64_param_9];
	ld.param.u32 	%r14, [ggml_matvec_f16_ncols_8_bs_64_param_10];
	ld.param.u32 	%r15, [ggml_matvec_f16_ncols_8_bs_64_param_11];
	add.u64 	%rd1, %SPL, 0;
	mov.u32 	%r1, %ctaid.x;
	mov.u32 	%r2, %tid.x;
	setp.gt.s32 	%p1, %r2, 31;
	shl.b32 	%r16, %r2, 2;
	mov.u32 	%r17, data_mmv;
	add.s32 	%r3, %r17, %r16;
	@%p1 bra 	$L__BB79_2;

	mov.u32 	%r18, 0;
	st.shared.u32 	[%r3], %r18;

$L__BB79_2:
	mov.u32 	%r4, %ctaid.y;
	bar.sync 	0;
	mov.f32 	%f244, 0f00000000;
	st.local.v4.f32 	[%rd1], {%f244, %f244, %f244, %f244};
	st.local.v4.f32 	[%rd1+16], {%f244, %f244, %f244, %f244};
	setp.ge.s32 	%p2, %r2, %r8;
	mov.f32 	%f245, %f244;
	mov.f32 	%f246, %f244;
	mov.f32 	%f247, %f244;
	mov.f32 	%f248, %f244;
	mov.f32 	%f249, %f244;
	mov.f32 	%f250, %f244;
	mov.f32 	%f251, %f244;
	@%p2 bra 	$L__BB79_6;

	shl.b32 	%r19, %r10, 1;
	add.s32 	%r20, %r2, %r19;
	mul.wide.s32 	%rd18, %r20, 4;
	mul.lo.s32 	%r21, %r4, %r14;
	mul.wide.s32 	%rd19, %r21, 2;
	add.s64 	%rd4, %rd18, %rd19;
	mul.wide.s32 	%rd20, %r2, 4;
	mul.wide.s32 	%rd5, %r10, 4;
	add.s64 	%rd21, %rd20, %rd5;
	add.s64 	%rd6, %rd21, %rd19;
	add.s64 	%rd7, %rd20, %rd19;
	mul.wide.s32 	%rd22, %r2, 2;
	div.s32 	%r22, %r4, %r12;
	mul.lo.s32 	%r23, %r1, %r9;
	mad.lo.s32 	%r24, %r22, %r13, %r23;
	cvt.s64.s32 	%rd23, %r24;
	add.s64 	%rd24, %rd22, %rd23;
	cvta.to.global.u64 	%rd25, %rd14;
	shl.b64 	%rd26, %rd24, 1;
	add.s64 	%rd43, %rd25, %rd26;
	cvta.to.global.u64 	%rd44, %rd15;
	mov.f32 	%f244, 0f00000000;
	mov.u32 	%r323, %r2;

$L__BB79_4:
	ld.global.nc.u32 	%r25, [%rd43];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r25;
  cvt.f32.f16 %f41, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r25;
  cvt.f32.f16 %f42, high;}

	// end inline asm
	add.s64 	%rd27, %rd44, %rd7;
	ld.global.nc.u32 	%r27, [%rd27];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r27;
  cvt.f32.f16 %f43, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r27;
  cvt.f32.f16 %f44, high;}

	// end inline asm
	fma.rn.f32 	%f59, %f41, %f43, %f251;
	fma.rn.f32 	%f251, %f42, %f44, %f59;
	add.s64 	%rd28, %rd44, %rd6;
	ld.global.nc.u32 	%r29, [%rd28];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r29;
  cvt.f32.f16 %f45, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r29;
  cvt.f32.f16 %f46, high;}

	// end inline asm
	fma.rn.f32 	%f60, %f41, %f45, %f250;
	fma.rn.f32 	%f250, %f42, %f46, %f60;
	add.s64 	%rd29, %rd44, %rd4;
	ld.global.nc.u32 	%r31, [%rd29];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r31;
  cvt.f32.f16 %f47, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r31;
  cvt.f32.f16 %f48, high;}

	// end inline asm
	fma.rn.f32 	%f61, %f41, %f47, %f249;
	fma.rn.f32 	%f249, %f42, %f48, %f61;
	add.s64 	%rd30, %rd29, %rd5;
	ld.global.nc.u32 	%r33, [%rd30];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r33;
  cvt.f32.f16 %f49, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r33;
  cvt.f32.f16 %f50, high;}

	// end inline asm
	fma.rn.f32 	%f62, %f41, %f49, %f248;
	fma.rn.f32 	%f248, %f42, %f50, %f62;
	add.s64 	%rd31, %rd30, %rd5;
	ld.global.nc.u32 	%r35, [%rd31];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r35;
  cvt.f32.f16 %f51, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r35;
  cvt.f32.f16 %f52, high;}

	// end inline asm
	fma.rn.f32 	%f63, %f41, %f51, %f247;
	fma.rn.f32 	%f247, %f42, %f52, %f63;
	add.s64 	%rd32, %rd31, %rd5;
	ld.global.nc.u32 	%r37, [%rd32];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r37;
  cvt.f32.f16 %f53, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r37;
  cvt.f32.f16 %f54, high;}

	// end inline asm
	fma.rn.f32 	%f64, %f41, %f53, %f246;
	fma.rn.f32 	%f246, %f42, %f54, %f64;
	add.s64 	%rd33, %rd32, %rd5;
	ld.global.nc.u32 	%r39, [%rd33];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r39;
  cvt.f32.f16 %f55, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r39;
  cvt.f32.f16 %f56, high;}

	// end inline asm
	fma.rn.f32 	%f65, %f41, %f55, %f245;
	fma.rn.f32 	%f245, %f42, %f56, %f65;
	add.s64 	%rd34, %rd33, %rd5;
	ld.global.nc.u32 	%r41, [%rd34];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r41;
  cvt.f32.f16 %f57, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r41;
  cvt.f32.f16 %f58, high;}

	// end inline asm
	fma.rn.f32 	%f66, %f41, %f57, %f244;
	fma.rn.f32 	%f244, %f42, %f58, %f66;
	add.s64 	%rd44, %rd44, 256;
	add.s64 	%rd43, %rd43, 256;
	add.s32 	%r323, %r323, 64;
	setp.lt.s32 	%p3, %r323, %r8;
	@%p3 bra 	$L__BB79_4;

	st.local.v4.f32 	[%rd1], {%f251, %f250, %f249, %f248};
	st.local.v4.f32 	[%rd1+16], {%f247, %f246, %f245, %f244};

$L__BB79_6:
	shr.s32 	%r43, %r2, 31;
	shr.u32 	%r44, %r43, 27;
	add.s32 	%r45, %r2, %r44;
	shr.s32 	%r46, %r45, 5;
	shl.b32 	%r47, %r46, 2;
	add.s32 	%r7, %r17, %r47;
	mov.u32 	%r49, 2;
	mov.b32 	%r50, %f251;
	mov.u32 	%r51, 31;
	mov.u32 	%r52, 16;
	mov.u32 	%r53, -1;
	shfl.sync.bfly.b32 	%r54|%p4, %r50, %r52, %r51, %r53;
	mov.b32 	%f67, %r54;
	add.f32 	%f68, %f251, %f67;
	mov.b32 	%r55, %f68;
	mov.u32 	%r56, 8;
	shfl.sync.bfly.b32 	%r57|%p5, %r55, %r56, %r51, %r53;
	mov.b32 	%f69, %r57;
	add.f32 	%f70, %f68, %f69;
	mov.b32 	%r58, %f70;
	mov.u32 	%r59, 4;
	shfl.sync.bfly.b32 	%r60|%p6, %r58, %r59, %r51, %r53;
	mov.b32 	%f71, %r60;
	add.f32 	%f72, %f70, %f71;
	mov.b32 	%r61, %f72;
	shfl.sync.bfly.b32 	%r62|%p7, %r61, %r49, %r51, %r53;
	mov.b32 	%f73, %r62;
	add.f32 	%f74, %f72, %f73;
	mov.b32 	%r63, %f74;
	mov.u32 	%r64, 1;
	shfl.sync.bfly.b32 	%r65|%p8, %r63, %r64, %r51, %r53;
	mov.b32 	%f75, %r65;
	add.f32 	%f76, %f74, %f75;
	st.local.f32 	[%rd1], %f76;
	st.shared.f32 	[%r7], %f76;
	bar.sync 	0;
	@%p1 bra 	$L__BB79_8;

	ld.shared.f32 	%f77, [%r3];
	mov.b32 	%r66, %f77;
	shfl.sync.bfly.b32 	%r70|%p10, %r66, %r52, %r51, %r53;
	mov.b32 	%f78, %r70;
	add.f32 	%f79, %f77, %f78;
	mov.b32 	%r71, %f79;
	shfl.sync.bfly.b32 	%r73|%p11, %r71, %r56, %r51, %r53;
	mov.b32 	%f80, %r73;
	add.f32 	%f81, %f79, %f80;
	mov.b32 	%r74, %f81;
	shfl.sync.bfly.b32 	%r76|%p12, %r74, %r59, %r51, %r53;
	mov.b32 	%f82, %r76;
	add.f32 	%f83, %f81, %f82;
	mov.b32 	%r77, %f83;
	shfl.sync.bfly.b32 	%r79|%p13, %r77, %r49, %r51, %r53;
	mov.b32 	%f84, %r79;
	add.f32 	%f85, %f83, %f84;
	mov.b32 	%r80, %f85;
	shfl.sync.bfly.b32 	%r82|%p14, %r80, %r64, %r51, %r53;
	mov.b32 	%f86, %r82;
	add.f32 	%f87, %f85, %f86;
	st.local.f32 	[%rd1], %f87;

$L__BB79_8:
	bar.sync 	0;
	mov.b32 	%r83, %f250;
	shfl.sync.bfly.b32 	%r87|%p16, %r83, %r52, %r51, %r53;
	mov.b32 	%f88, %r87;
	add.f32 	%f89, %f250, %f88;
	mov.b32 	%r88, %f89;
	shfl.sync.bfly.b32 	%r90|%p17, %r88, %r56, %r51, %r53;
	mov.b32 	%f90, %r90;
	add.f32 	%f91, %f89, %f90;
	mov.b32 	%r91, %f91;
	shfl.sync.bfly.b32 	%r93|%p18, %r91, %r59, %r51, %r53;
	mov.b32 	%f92, %r93;
	add.f32 	%f93, %f91, %f92;
	mov.b32 	%r94, %f93;
	shfl.sync.bfly.b32 	%r96|%p19, %r94, %r49, %r51, %r53;
	mov.b32 	%f94, %r96;
	add.f32 	%f95, %f93, %f94;
	mov.b32 	%r97, %f95;
	shfl.sync.bfly.b32 	%r99|%p20, %r97, %r64, %r51, %r53;
	mov.b32 	%f96, %r99;
	add.f32 	%f97, %f95, %f96;
	st.local.f32 	[%rd1+4], %f97;
	st.shared.f32 	[%r7], %f97;
	bar.sync 	0;
	@%p1 bra 	$L__BB79_10;

	ld.shared.f32 	%f98, [%r3];
	mov.b32 	%r100, %f98;
	mov.u32 	%r101, 31;
	mov.u32 	%r102, 16;
	mov.u32 	%r103, -1;
	shfl.sync.bfly.b32 	%r104|%p21, %r100, %r102, %r101, %r103;
	mov.b32 	%f99, %r104;
	add.f32 	%f100, %f98, %f99;
	mov.b32 	%r105, %f100;
	mov.u32 	%r106, 8;
	shfl.sync.bfly.b32 	%r107|%p22, %r105, %r106, %r101, %r103;
	mov.b32 	%f101, %r107;
	add.f32 	%f102, %f100, %f101;
	mov.b32 	%r108, %f102;
	mov.u32 	%r109, 4;
	shfl.sync.bfly.b32 	%r110|%p23, %r108, %r109, %r101, %r103;
	mov.b32 	%f103, %r110;
	add.f32 	%f104, %f102, %f103;
	mov.b32 	%r111, %f104;
	mov.u32 	%r112, 2;
	shfl.sync.bfly.b32 	%r113|%p24, %r111, %r112, %r101, %r103;
	mov.b32 	%f105, %r113;
	add.f32 	%f106, %f104, %f105;
	mov.b32 	%r114, %f106;
	mov.u32 	%r115, 1;
	shfl.sync.bfly.b32 	%r116|%p25, %r114, %r115, %r101, %r103;
	mov.b32 	%f107, %r116;
	add.f32 	%f108, %f106, %f107;
	st.local.f32 	[%rd1+4], %f108;

$L__BB79_10:
	bar.sync 	0;
	mov.b32 	%r117, %f249;
	mov.u32 	%r118, 31;
	mov.u32 	%r119, 16;
	mov.u32 	%r120, -1;
	shfl.sync.bfly.b32 	%r121|%p27, %r117, %r119, %r118, %r120;
	mov.b32 	%f109, %r121;
	add.f32 	%f110, %f249, %f109;
	mov.b32 	%r122, %f110;
	mov.u32 	%r123, 8;
	shfl.sync.bfly.b32 	%r124|%p28, %r122, %r123, %r118, %r120;
	mov.b32 	%f111, %r124;
	add.f32 	%f112, %f110, %f111;
	mov.b32 	%r125, %f112;
	mov.u32 	%r126, 4;
	shfl.sync.bfly.b32 	%r127|%p29, %r125, %r126, %r118, %r120;
	mov.b32 	%f113, %r127;
	add.f32 	%f114, %f112, %f113;
	mov.b32 	%r128, %f114;
	mov.u32 	%r129, 2;
	shfl.sync.bfly.b32 	%r130|%p30, %r128, %r129, %r118, %r120;
	mov.b32 	%f115, %r130;
	add.f32 	%f116, %f114, %f115;
	mov.b32 	%r131, %f116;
	mov.u32 	%r132, 1;
	shfl.sync.bfly.b32 	%r133|%p31, %r131, %r132, %r118, %r120;
	mov.b32 	%f117, %r133;
	add.f32 	%f118, %f116, %f117;
	st.local.f32 	[%rd1+8], %f118;
	st.shared.f32 	[%r7], %f118;
	bar.sync 	0;
	@%p1 bra 	$L__BB79_12;

	ld.shared.f32 	%f119, [%r3];
	mov.b32 	%r134, %f119;
	shfl.sync.bfly.b32 	%r138|%p32, %r134, %r119, %r118, %r120;
	mov.b32 	%f120, %r138;
	add.f32 	%f121, %f119, %f120;
	mov.b32 	%r139, %f121;
	shfl.sync.bfly.b32 	%r141|%p33, %r139, %r123, %r118, %r120;
	mov.b32 	%f122, %r141;
	add.f32 	%f123, %f121, %f122;
	mov.b32 	%r142, %f123;
	shfl.sync.bfly.b32 	%r144|%p34, %r142, %r126, %r118, %r120;
	mov.b32 	%f124, %r144;
	add.f32 	%f125, %f123, %f124;
	mov.b32 	%r145, %f125;
	shfl.sync.bfly.b32 	%r147|%p35, %r145, %r129, %r118, %r120;
	mov.b32 	%f126, %r147;
	add.f32 	%f127, %f125, %f126;
	mov.b32 	%r148, %f127;
	shfl.sync.bfly.b32 	%r150|%p36, %r148, %r132, %r118, %r120;
	mov.b32 	%f128, %r150;
	add.f32 	%f129, %f127, %f128;
	st.local.f32 	[%rd1+8], %f129;

$L__BB79_12:
	bar.sync 	0;
	mov.b32 	%r151, %f248;
	shfl.sync.bfly.b32 	%r155|%p38, %r151, %r119, %r118, %r120;
	mov.b32 	%f130, %r155;
	add.f32 	%f131, %f248, %f130;
	mov.b32 	%r156, %f131;
	shfl.sync.bfly.b32 	%r158|%p39, %r156, %r123, %r118, %r120;
	mov.b32 	%f132, %r158;
	add.f32 	%f133, %f131, %f132;
	mov.b32 	%r159, %f133;
	shfl.sync.bfly.b32 	%r161|%p40, %r159, %r126, %r118, %r120;
	mov.b32 	%f134, %r161;
	add.f32 	%f135, %f133, %f134;
	mov.b32 	%r162, %f135;
	shfl.sync.bfly.b32 	%r164|%p41, %r162, %r129, %r118, %r120;
	mov.b32 	%f136, %r164;
	add.f32 	%f137, %f135, %f136;
	mov.b32 	%r165, %f137;
	shfl.sync.bfly.b32 	%r167|%p42, %r165, %r132, %r118, %r120;
	mov.b32 	%f138, %r167;
	add.f32 	%f139, %f137, %f138;
	st.local.f32 	[%rd1+12], %f139;
	st.shared.f32 	[%r7], %f139;
	bar.sync 	0;
	@%p1 bra 	$L__BB79_14;

	ld.shared.f32 	%f140, [%r3];
	mov.b32 	%r168, %f140;
	mov.u32 	%r169, 31;
	mov.u32 	%r170, 16;
	mov.u32 	%r171, -1;
	shfl.sync.bfly.b32 	%r172|%p43, %r168, %r170, %r169, %r171;
	mov.b32 	%f141, %r172;
	add.f32 	%f142, %f140, %f141;
	mov.b32 	%r173, %f142;
	mov.u32 	%r174, 8;
	shfl.sync.bfly.b32 	%r175|%p44, %r173, %r174, %r169, %r171;
	mov.b32 	%f143, %r175;
	add.f32 	%f144, %f142, %f143;
	mov.b32 	%r176, %f144;
	mov.u32 	%r177, 4;
	shfl.sync.bfly.b32 	%r178|%p45, %r176, %r177, %r169, %r171;
	mov.b32 	%f145, %r178;
	add.f32 	%f146, %f144, %f145;
	mov.b32 	%r179, %f146;
	mov.u32 	%r180, 2;
	shfl.sync.bfly.b32 	%r181|%p46, %r179, %r180, %r169, %r171;
	mov.b32 	%f147, %r181;
	add.f32 	%f148, %f146, %f147;
	mov.b32 	%r182, %f148;
	mov.u32 	%r183, 1;
	shfl.sync.bfly.b32 	%r184|%p47, %r182, %r183, %r169, %r171;
	mov.b32 	%f149, %r184;
	add.f32 	%f150, %f148, %f149;
	st.local.f32 	[%rd1+12], %f150;

$L__BB79_14:
	bar.sync 	0;
	mov.b32 	%r185, %f247;
	mov.u32 	%r186, 31;
	mov.u32 	%r187, 16;
	mov.u32 	%r188, -1;
	shfl.sync.bfly.b32 	%r189|%p49, %r185, %r187, %r186, %r188;
	mov.b32 	%f151, %r189;
	add.f32 	%f152, %f247, %f151;
	mov.b32 	%r190, %f152;
	mov.u32 	%r191, 8;
	shfl.sync.bfly.b32 	%r192|%p50, %r190, %r191, %r186, %r188;
	mov.b32 	%f153, %r192;
	add.f32 	%f154, %f152, %f153;
	mov.b32 	%r193, %f154;
	mov.u32 	%r194, 4;
	shfl.sync.bfly.b32 	%r195|%p51, %r193, %r194, %r186, %r188;
	mov.b32 	%f155, %r195;
	add.f32 	%f156, %f154, %f155;
	mov.b32 	%r196, %f156;
	mov.u32 	%r197, 2;
	shfl.sync.bfly.b32 	%r198|%p52, %r196, %r197, %r186, %r188;
	mov.b32 	%f157, %r198;
	add.f32 	%f158, %f156, %f157;
	mov.b32 	%r199, %f158;
	mov.u32 	%r200, 1;
	shfl.sync.bfly.b32 	%r201|%p53, %r199, %r200, %r186, %r188;
	mov.b32 	%f159, %r201;
	add.f32 	%f160, %f158, %f159;
	st.local.f32 	[%rd1+16], %f160;
	st.shared.f32 	[%r7], %f160;
	bar.sync 	0;
	@%p1 bra 	$L__BB79_16;

	ld.shared.f32 	%f161, [%r3];
	mov.b32 	%r202, %f161;
	shfl.sync.bfly.b32 	%r206|%p54, %r202, %r187, %r186, %r188;
	mov.b32 	%f162, %r206;
	add.f32 	%f163, %f161, %f162;
	mov.b32 	%r207, %f163;
	shfl.sync.bfly.b32 	%r209|%p55, %r207, %r191, %r186, %r188;
	mov.b32 	%f164, %r209;
	add.f32 	%f165, %f163, %f164;
	mov.b32 	%r210, %f165;
	shfl.sync.bfly.b32 	%r212|%p56, %r210, %r194, %r186, %r188;
	mov.b32 	%f166, %r212;
	add.f32 	%f167, %f165, %f166;
	mov.b32 	%r213, %f167;
	shfl.sync.bfly.b32 	%r215|%p57, %r213, %r197, %r186, %r188;
	mov.b32 	%f168, %r215;
	add.f32 	%f169, %f167, %f168;
	mov.b32 	%r216, %f169;
	shfl.sync.bfly.b32 	%r218|%p58, %r216, %r200, %r186, %r188;
	mov.b32 	%f170, %r218;
	add.f32 	%f171, %f169, %f170;
	st.local.f32 	[%rd1+16], %f171;

$L__BB79_16:
	bar.sync 	0;
	mov.b32 	%r219, %f246;
	shfl.sync.bfly.b32 	%r223|%p60, %r219, %r187, %r186, %r188;
	mov.b32 	%f172, %r223;
	add.f32 	%f173, %f246, %f172;
	mov.b32 	%r224, %f173;
	shfl.sync.bfly.b32 	%r226|%p61, %r224, %r191, %r186, %r188;
	mov.b32 	%f174, %r226;
	add.f32 	%f175, %f173, %f174;
	mov.b32 	%r227, %f175;
	shfl.sync.bfly.b32 	%r229|%p62, %r227, %r194, %r186, %r188;
	mov.b32 	%f176, %r229;
	add.f32 	%f177, %f175, %f176;
	mov.b32 	%r230, %f177;
	shfl.sync.bfly.b32 	%r232|%p63, %r230, %r197, %r186, %r188;
	mov.b32 	%f178, %r232;
	add.f32 	%f179, %f177, %f178;
	mov.b32 	%r233, %f179;
	shfl.sync.bfly.b32 	%r235|%p64, %r233, %r200, %r186, %r188;
	mov.b32 	%f180, %r235;
	add.f32 	%f181, %f179, %f180;
	st.local.f32 	[%rd1+20], %f181;
	st.shared.f32 	[%r7], %f181;
	bar.sync 	0;
	@%p1 bra 	$L__BB79_18;

	ld.shared.f32 	%f182, [%r3];
	mov.b32 	%r236, %f182;
	mov.u32 	%r237, 31;
	mov.u32 	%r238, 16;
	mov.u32 	%r239, -1;
	shfl.sync.bfly.b32 	%r240|%p65, %r236, %r238, %r237, %r239;
	mov.b32 	%f183, %r240;
	add.f32 	%f184, %f182, %f183;
	mov.b32 	%r241, %f184;
	mov.u32 	%r242, 8;
	shfl.sync.bfly.b32 	%r243|%p66, %r241, %r242, %r237, %r239;
	mov.b32 	%f185, %r243;
	add.f32 	%f186, %f184, %f185;
	mov.b32 	%r244, %f186;
	mov.u32 	%r245, 4;
	shfl.sync.bfly.b32 	%r246|%p67, %r244, %r245, %r237, %r239;
	mov.b32 	%f187, %r246;
	add.f32 	%f188, %f186, %f187;
	mov.b32 	%r247, %f188;
	mov.u32 	%r248, 2;
	shfl.sync.bfly.b32 	%r249|%p68, %r247, %r248, %r237, %r239;
	mov.b32 	%f189, %r249;
	add.f32 	%f190, %f188, %f189;
	mov.b32 	%r250, %f190;
	mov.u32 	%r251, 1;
	shfl.sync.bfly.b32 	%r252|%p69, %r250, %r251, %r237, %r239;
	mov.b32 	%f191, %r252;
	add.f32 	%f192, %f190, %f191;
	st.local.f32 	[%rd1+20], %f192;

$L__BB79_18:
	bar.sync 	0;
	mov.b32 	%r253, %f245;
	mov.u32 	%r254, 31;
	mov.u32 	%r255, 16;
	mov.u32 	%r256, -1;
	shfl.sync.bfly.b32 	%r257|%p71, %r253, %r255, %r254, %r256;
	mov.b32 	%f193, %r257;
	add.f32 	%f194, %f245, %f193;
	mov.b32 	%r258, %f194;
	mov.u32 	%r259, 8;
	shfl.sync.bfly.b32 	%r260|%p72, %r258, %r259, %r254, %r256;
	mov.b32 	%f195, %r260;
	add.f32 	%f196, %f194, %f195;
	mov.b32 	%r261, %f196;
	mov.u32 	%r262, 4;
	shfl.sync.bfly.b32 	%r263|%p73, %r261, %r262, %r254, %r256;
	mov.b32 	%f197, %r263;
	add.f32 	%f198, %f196, %f197;
	mov.b32 	%r264, %f198;
	mov.u32 	%r265, 2;
	shfl.sync.bfly.b32 	%r266|%p74, %r264, %r265, %r254, %r256;
	mov.b32 	%f199, %r266;
	add.f32 	%f200, %f198, %f199;
	mov.b32 	%r267, %f200;
	mov.u32 	%r268, 1;
	shfl.sync.bfly.b32 	%r269|%p75, %r267, %r268, %r254, %r256;
	mov.b32 	%f201, %r269;
	add.f32 	%f202, %f200, %f201;
	st.local.f32 	[%rd1+24], %f202;
	st.shared.f32 	[%r7], %f202;
	bar.sync 	0;
	@%p1 bra 	$L__BB79_20;

	ld.shared.f32 	%f203, [%r3];
	mov.b32 	%r270, %f203;
	shfl.sync.bfly.b32 	%r274|%p76, %r270, %r255, %r254, %r256;
	mov.b32 	%f204, %r274;
	add.f32 	%f205, %f203, %f204;
	mov.b32 	%r275, %f205;
	shfl.sync.bfly.b32 	%r277|%p77, %r275, %r259, %r254, %r256;
	mov.b32 	%f206, %r277;
	add.f32 	%f207, %f205, %f206;
	mov.b32 	%r278, %f207;
	shfl.sync.bfly.b32 	%r280|%p78, %r278, %r262, %r254, %r256;
	mov.b32 	%f208, %r280;
	add.f32 	%f209, %f207, %f208;
	mov.b32 	%r281, %f209;
	shfl.sync.bfly.b32 	%r283|%p79, %r281, %r265, %r254, %r256;
	mov.b32 	%f210, %r283;
	add.f32 	%f211, %f209, %f210;
	mov.b32 	%r284, %f211;
	shfl.sync.bfly.b32 	%r286|%p80, %r284, %r268, %r254, %r256;
	mov.b32 	%f212, %r286;
	add.f32 	%f213, %f211, %f212;
	st.local.f32 	[%rd1+24], %f213;

$L__BB79_20:
	bar.sync 	0;
	mov.b32 	%r287, %f244;
	shfl.sync.bfly.b32 	%r291|%p82, %r287, %r255, %r254, %r256;
	mov.b32 	%f214, %r291;
	add.f32 	%f215, %f244, %f214;
	mov.b32 	%r292, %f215;
	shfl.sync.bfly.b32 	%r294|%p83, %r292, %r259, %r254, %r256;
	mov.b32 	%f216, %r294;
	add.f32 	%f217, %f215, %f216;
	mov.b32 	%r295, %f217;
	shfl.sync.bfly.b32 	%r297|%p84, %r295, %r262, %r254, %r256;
	mov.b32 	%f218, %r297;
	add.f32 	%f219, %f217, %f218;
	mov.b32 	%r298, %f219;
	shfl.sync.bfly.b32 	%r300|%p85, %r298, %r265, %r254, %r256;
	mov.b32 	%f220, %r300;
	add.f32 	%f221, %f219, %f220;
	mov.b32 	%r301, %f221;
	shfl.sync.bfly.b32 	%r303|%p86, %r301, %r268, %r254, %r256;
	mov.b32 	%f222, %r303;
	add.f32 	%f223, %f221, %f222;
	st.local.f32 	[%rd1+28], %f223;
	st.shared.f32 	[%r7], %f223;
	bar.sync 	0;
	@%p1 bra 	$L__BB79_22;

	ld.shared.f32 	%f224, [%r3];
	mov.b32 	%r304, %f224;
	mov.u32 	%r305, 31;
	mov.u32 	%r306, 16;
	mov.u32 	%r307, -1;
	shfl.sync.bfly.b32 	%r308|%p87, %r304, %r306, %r305, %r307;
	mov.b32 	%f225, %r308;
	add.f32 	%f226, %f224, %f225;
	mov.b32 	%r309, %f226;
	mov.u32 	%r310, 8;
	shfl.sync.bfly.b32 	%r311|%p88, %r309, %r310, %r305, %r307;
	mov.b32 	%f227, %r311;
	add.f32 	%f228, %f226, %f227;
	mov.b32 	%r312, %f228;
	mov.u32 	%r313, 4;
	shfl.sync.bfly.b32 	%r314|%p89, %r312, %r313, %r305, %r307;
	mov.b32 	%f229, %r314;
	add.f32 	%f230, %f228, %f229;
	mov.b32 	%r315, %f230;
	mov.u32 	%r316, 2;
	shfl.sync.bfly.b32 	%r317|%p90, %r315, %r316, %r305, %r307;
	mov.b32 	%f231, %r317;
	add.f32 	%f232, %f230, %f231;
	mov.b32 	%r318, %f232;
	mov.u32 	%r319, 1;
	shfl.sync.bfly.b32 	%r320|%p91, %r318, %r319, %r305, %r307;
	mov.b32 	%f233, %r320;
	add.f32 	%f234, %f232, %f233;
	st.local.f32 	[%rd1+28], %f234;

$L__BB79_22:
	bar.sync 	0;
	setp.gt.s32 	%p92, %r2, 7;
	@%p92 bra 	$L__BB79_24;

	mad.lo.s32 	%r321, %r2, %r11, %r1;
	cvt.s64.s32 	%rd35, %r321;
	mul.lo.s32 	%r322, %r4, %r15;
	cvt.s64.s32 	%rd36, %r322;
	add.s64 	%rd37, %rd36, %rd35;
	mul.wide.s32 	%rd38, %r2, 4;
	add.s64 	%rd39, %rd1, %rd38;
	ld.local.f32 	%f235, [%rd39];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f235;}

	// end inline asm
	cvta.to.global.u64 	%rd40, %rd16;
	shl.b64 	%rd41, %rd37, 1;
	add.s64 	%rd42, %rd40, %rd41;
	st.global.u16 	[%rd42], %rs1;

$L__BB79_24:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_1_bs_96
.visible .entry ggml_matvec_f16_ncols_1_bs_96(
	.param .u64 ggml_matvec_f16_ncols_1_bs_96_param_0,
	.param .u64 ggml_matvec_f16_ncols_1_bs_96_param_1,
	.param .u64 ggml_matvec_f16_ncols_1_bs_96_param_2,
	.param .u32 ggml_matvec_f16_ncols_1_bs_96_param_3,
	.param .u32 ggml_matvec_f16_ncols_1_bs_96_param_4,
	.param .u32 ggml_matvec_f16_ncols_1_bs_96_param_5,
	.param .u32 ggml_matvec_f16_ncols_1_bs_96_param_6,
	.param .u32 ggml_matvec_f16_ncols_1_bs_96_param_7,
	.param .u32 ggml_matvec_f16_ncols_1_bs_96_param_8,
	.param .u32 ggml_matvec_f16_ncols_1_bs_96_param_9,
	.param .u32 ggml_matvec_f16_ncols_1_bs_96_param_10,
	.param .u32 ggml_matvec_f16_ncols_1_bs_96_param_11
)
{
	.reg .pred 	%p<19>;
	.reg .b16 	%rs<2>;
	.reg .f32 	%f<69>;
	.reg .b32 	%r<99>;
	.reg .b64 	%rd<44>;


	ld.param.u64 	%rd18, [ggml_matvec_f16_ncols_1_bs_96_param_0];
	ld.param.u64 	%rd19, [ggml_matvec_f16_ncols_1_bs_96_param_1];
	ld.param.u64 	%rd17, [ggml_matvec_f16_ncols_1_bs_96_param_2];
	ld.param.u32 	%r13, [ggml_matvec_f16_ncols_1_bs_96_param_3];
	ld.param.u32 	%r16, [ggml_matvec_f16_ncols_1_bs_96_param_5];
	ld.param.u32 	%r14, [ggml_matvec_f16_ncols_1_bs_96_param_7];
	ld.param.u32 	%r17, [ggml_matvec_f16_ncols_1_bs_96_param_8];
	ld.param.u32 	%r18, [ggml_matvec_f16_ncols_1_bs_96_param_9];
	ld.param.u32 	%r19, [ggml_matvec_f16_ncols_1_bs_96_param_10];
	ld.param.u32 	%r15, [ggml_matvec_f16_ncols_1_bs_96_param_11];
	cvta.to.global.u64 	%rd1, %rd19;
	cvta.to.global.u64 	%rd2, %rd18;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r20, %r1, %r17;
	mov.u32 	%r21, %ctaid.x;
	mul.lo.s32 	%r22, %r21, %r16;
	mad.lo.s32 	%r23, %r20, %r18, %r22;
	cvt.s64.s32 	%rd3, %r23;
	mul.lo.s32 	%r24, %r1, %r19;
	cvt.s64.s32 	%rd4, %r24;
	mov.u32 	%r2, %tid.x;
	setp.gt.s32 	%p1, %r2, 31;
	shl.b32 	%r25, %r2, 2;
	mov.u32 	%r26, data_mmv;
	add.s32 	%r3, %r26, %r25;
	@%p1 bra 	$L__BB80_2;

	mov.u32 	%r27, 0;
	st.shared.u32 	[%r3], %r27;

$L__BB80_2:
	bar.sync 	0;
	setp.ge.s32 	%p2, %r2, %r13;
	mov.f32 	%f67, 0f00000000;
	@%p2 bra 	$L__BB80_9;

	not.b32 	%r28, %r2;
	add.s32 	%r4, %r28, %r13;
	mul.wide.u32 	%rd20, %r4, -1431655765;
	shr.u64 	%rd21, %rd20, 38;
	cvt.u32.u64 	%r29, %rd21;
	add.s32 	%r30, %r29, 1;
	and.b32  	%r96, %r30, 3;
	setp.eq.s32 	%p3, %r96, 0;
	mov.f32 	%f67, 0f00000000;
	mov.u32 	%r97, %r2;
	@%p3 bra 	$L__BB80_6;

	mul.wide.s32 	%rd22, %r2, 2;
	add.s64 	%rd23, %rd22, %rd4;
	shl.b64 	%rd24, %rd23, 1;
	add.s64 	%rd41, %rd1, %rd24;
	add.s64 	%rd25, %rd22, %rd3;
	shl.b64 	%rd26, %rd25, 1;
	add.s64 	%rd40, %rd2, %rd26;
	mov.f32 	%f67, 0f00000000;
	mov.u32 	%r97, %r2;

$L__BB80_5:
	.pragma "nounroll";
	ld.global.nc.u32 	%r31, [%rd40];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r31;
  cvt.f32.f16 %f15, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r31;
  cvt.f32.f16 %f16, high;}

	// end inline asm
	ld.global.nc.u32 	%r33, [%rd41];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r33;
  cvt.f32.f16 %f17, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r33;
  cvt.f32.f16 %f18, high;}

	// end inline asm
	fma.rn.f32 	%f19, %f15, %f17, %f67;
	fma.rn.f32 	%f67, %f16, %f18, %f19;
	add.s32 	%r97, %r97, 96;
	add.s64 	%rd41, %rd41, 384;
	add.s64 	%rd40, %rd40, 384;
	add.s32 	%r96, %r96, -1;
	setp.ne.s32 	%p4, %r96, 0;
	@%p4 bra 	$L__BB80_5;

$L__BB80_6:
	setp.lt.u32 	%p5, %r4, 288;
	@%p5 bra 	$L__BB80_9;

	mul.wide.s32 	%rd27, %r97, 2;
	add.s64 	%rd28, %rd27, %rd3;
	shl.b64 	%rd29, %rd28, 1;
	add.s64 	%rd30, %rd2, %rd29;
	add.s64 	%rd43, %rd30, 768;
	add.s64 	%rd31, %rd27, %rd4;
	shl.b64 	%rd32, %rd31, 1;
	add.s64 	%rd33, %rd1, %rd32;
	add.s64 	%rd42, %rd33, 768;

$L__BB80_8:
	ld.global.nc.u32 	%r35, [%rd43+-768];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r35;
  cvt.f32.f16 %f20, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r35;
  cvt.f32.f16 %f21, high;}

	// end inline asm
	ld.global.nc.u32 	%r37, [%rd42+-768];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r37;
  cvt.f32.f16 %f22, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r37;
  cvt.f32.f16 %f23, high;}

	// end inline asm
	fma.rn.f32 	%f36, %f20, %f22, %f67;
	fma.rn.f32 	%f37, %f21, %f23, %f36;
	ld.global.nc.u32 	%r39, [%rd43+-384];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r39;
  cvt.f32.f16 %f24, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r39;
  cvt.f32.f16 %f25, high;}

	// end inline asm
	ld.global.nc.u32 	%r41, [%rd42+-384];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r41;
  cvt.f32.f16 %f26, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r41;
  cvt.f32.f16 %f27, high;}

	// end inline asm
	fma.rn.f32 	%f38, %f24, %f26, %f37;
	fma.rn.f32 	%f39, %f25, %f27, %f38;
	ld.global.nc.u32 	%r43, [%rd43];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r43;
  cvt.f32.f16 %f28, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r43;
  cvt.f32.f16 %f29, high;}

	// end inline asm
	ld.global.nc.u32 	%r45, [%rd42];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r45;
  cvt.f32.f16 %f30, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r45;
  cvt.f32.f16 %f31, high;}

	// end inline asm
	fma.rn.f32 	%f40, %f28, %f30, %f39;
	fma.rn.f32 	%f41, %f29, %f31, %f40;
	ld.global.nc.u32 	%r47, [%rd43+384];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r47;
  cvt.f32.f16 %f32, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r47;
  cvt.f32.f16 %f33, high;}

	// end inline asm
	ld.global.nc.u32 	%r49, [%rd42+384];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r49;
  cvt.f32.f16 %f34, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r49;
  cvt.f32.f16 %f35, high;}

	// end inline asm
	fma.rn.f32 	%f42, %f32, %f34, %f41;
	fma.rn.f32 	%f67, %f33, %f35, %f42;
	add.s64 	%rd43, %rd43, 1536;
	add.s64 	%rd42, %rd42, 1536;
	add.s32 	%r97, %r97, 384;
	setp.lt.s32 	%p6, %r97, %r13;
	@%p6 bra 	$L__BB80_8;

$L__BB80_9:
	mov.b32 	%r51, %f67;
	mov.u32 	%r52, 31;
	mov.u32 	%r53, 16;
	mov.u32 	%r54, -1;
	shfl.sync.bfly.b32 	%r55|%p7, %r51, %r53, %r52, %r54;
	mov.b32 	%f43, %r55;
	add.f32 	%f44, %f67, %f43;
	mov.b32 	%r56, %f44;
	mov.u32 	%r57, 8;
	shfl.sync.bfly.b32 	%r58|%p8, %r56, %r57, %r52, %r54;
	mov.b32 	%f45, %r58;
	add.f32 	%f46, %f44, %f45;
	mov.b32 	%r59, %f46;
	mov.u32 	%r60, 4;
	shfl.sync.bfly.b32 	%r61|%p9, %r59, %r60, %r52, %r54;
	mov.b32 	%f47, %r61;
	add.f32 	%f48, %f46, %f47;
	mov.b32 	%r62, %f48;
	mov.u32 	%r63, 2;
	shfl.sync.bfly.b32 	%r64|%p10, %r62, %r63, %r52, %r54;
	mov.b32 	%f49, %r64;
	add.f32 	%f50, %f48, %f49;
	mov.b32 	%r65, %f50;
	mov.u32 	%r66, 1;
	shfl.sync.bfly.b32 	%r67|%p11, %r65, %r66, %r52, %r54;
	mov.b32 	%f51, %r67;
	add.f32 	%f68, %f50, %f51;
	shr.s32 	%r68, %r2, 31;
	shr.u32 	%r69, %r68, 27;
	add.s32 	%r70, %r2, %r69;
	shr.s32 	%r71, %r70, 5;
	shl.b32 	%r72, %r71, 2;
	add.s32 	%r74, %r26, %r72;
	st.shared.f32 	[%r74], %f68;
	bar.sync 	0;
	@%p1 bra 	$L__BB80_11;

	ld.shared.f32 	%f52, [%r3];
	mov.b32 	%r75, %f52;
	shfl.sync.bfly.b32 	%r79|%p13, %r75, %r53, %r52, %r54;
	mov.b32 	%f53, %r79;
	add.f32 	%f54, %f52, %f53;
	mov.b32 	%r80, %f54;
	shfl.sync.bfly.b32 	%r82|%p14, %r80, %r57, %r52, %r54;
	mov.b32 	%f55, %r82;
	add.f32 	%f56, %f54, %f55;
	mov.b32 	%r83, %f56;
	shfl.sync.bfly.b32 	%r85|%p15, %r83, %r60, %r52, %r54;
	mov.b32 	%f57, %r85;
	add.f32 	%f58, %f56, %f57;
	mov.b32 	%r86, %f58;
	shfl.sync.bfly.b32 	%r88|%p16, %r86, %r63, %r52, %r54;
	mov.b32 	%f59, %r88;
	add.f32 	%f60, %f58, %f59;
	mov.b32 	%r89, %f60;
	shfl.sync.bfly.b32 	%r91|%p17, %r89, %r66, %r52, %r54;
	mov.b32 	%f61, %r91;
	add.f32 	%f68, %f60, %f61;

$L__BB80_11:
	bar.sync 	0;
	setp.gt.s32 	%p18, %r2, 0;
	@%p18 bra 	$L__BB80_13;

	mad.lo.s32 	%r93, %r2, %r14, %r21;
	cvt.s64.s32 	%rd34, %r93;
	mul.lo.s32 	%r94, %r1, %r15;
	cvt.s64.s32 	%rd35, %r94;
	add.s64 	%rd36, %rd35, %rd34;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f68;}

	// end inline asm
	cvta.to.global.u64 	%rd37, %rd17;
	shl.b64 	%rd38, %rd36, 1;
	add.s64 	%rd39, %rd37, %rd38;
	st.global.u16 	[%rd39], %rs1;

$L__BB80_13:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_2_bs_96
.visible .entry ggml_matvec_f16_ncols_2_bs_96(
	.param .u64 ggml_matvec_f16_ncols_2_bs_96_param_0,
	.param .u64 ggml_matvec_f16_ncols_2_bs_96_param_1,
	.param .u64 ggml_matvec_f16_ncols_2_bs_96_param_2,
	.param .u32 ggml_matvec_f16_ncols_2_bs_96_param_3,
	.param .u32 ggml_matvec_f16_ncols_2_bs_96_param_4,
	.param .u32 ggml_matvec_f16_ncols_2_bs_96_param_5,
	.param .u32 ggml_matvec_f16_ncols_2_bs_96_param_6,
	.param .u32 ggml_matvec_f16_ncols_2_bs_96_param_7,
	.param .u32 ggml_matvec_f16_ncols_2_bs_96_param_8,
	.param .u32 ggml_matvec_f16_ncols_2_bs_96_param_9,
	.param .u32 ggml_matvec_f16_ncols_2_bs_96_param_10,
	.param .u32 ggml_matvec_f16_ncols_2_bs_96_param_11
)
{
	.local .align 8 .b8 	__local_depot81[8];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<30>;
	.reg .b16 	%rs<2>;
	.reg .f32 	%f<116>;
	.reg .b32 	%r<143>;
	.reg .b64 	%rd<66>;


	mov.u64 	%SPL, __local_depot81;
	ld.param.u64 	%rd27, [ggml_matvec_f16_ncols_2_bs_96_param_0];
	ld.param.u64 	%rd28, [ggml_matvec_f16_ncols_2_bs_96_param_1];
	ld.param.u64 	%rd26, [ggml_matvec_f16_ncols_2_bs_96_param_2];
	ld.param.u32 	%r15, [ggml_matvec_f16_ncols_2_bs_96_param_3];
	ld.param.u32 	%r19, [ggml_matvec_f16_ncols_2_bs_96_param_5];
	ld.param.u32 	%r16, [ggml_matvec_f16_ncols_2_bs_96_param_6];
	ld.param.u32 	%r17, [ggml_matvec_f16_ncols_2_bs_96_param_7];
	ld.param.u32 	%r20, [ggml_matvec_f16_ncols_2_bs_96_param_8];
	ld.param.u32 	%r21, [ggml_matvec_f16_ncols_2_bs_96_param_9];
	ld.param.u32 	%r22, [ggml_matvec_f16_ncols_2_bs_96_param_10];
	ld.param.u32 	%r18, [ggml_matvec_f16_ncols_2_bs_96_param_11];
	cvta.to.global.u64 	%rd1, %rd28;
	cvta.to.global.u64 	%rd2, %rd27;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r23, %r1, %r20;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r24, %r2, %r19;
	mad.lo.s32 	%r25, %r23, %r21, %r24;
	cvt.s64.s32 	%rd4, %r25;
	mul.lo.s32 	%r26, %r1, %r22;
	cvt.s64.s32 	%rd5, %r26;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r27, %r3, 2;
	mov.u32 	%r28, data_mmv;
	add.s32 	%r4, %r28, %r27;
	@%p1 bra 	$L__BB81_2;

	mov.u32 	%r29, 0;
	st.shared.u32 	[%r4], %r29;

$L__BB81_2:
	bar.sync 	0;
	mov.f32 	%f114, 0f00000000;
	st.local.v2.f32 	[%rd3], {%f114, %f114};
	setp.ge.s32 	%p2, %r3, %r15;
	mov.f32 	%f115, %f114;
	@%p2 bra 	$L__BB81_11;

	not.b32 	%r30, %r3;
	add.s32 	%r5, %r30, %r15;
	mul.wide.u32 	%rd30, %r5, -1431655765;
	shr.u64 	%rd31, %rd30, 38;
	cvt.u32.u64 	%r31, %rd31;
	add.s32 	%r32, %r31, 1;
	and.b32  	%r140, %r32, 3;
	setp.eq.s32 	%p3, %r140, 0;
	mov.f32 	%f114, 0f00000000;
	mov.u32 	%r141, %r3;
	@%p3 bra 	$L__BB81_7;

	mul.wide.s32 	%rd32, %r16, 2;
	mul.wide.s32 	%rd33, %r3, 2;
	add.s64 	%rd34, %rd32, %rd33;
	add.s64 	%rd35, %rd34, %rd5;
	shl.b64 	%rd36, %rd35, 1;
	add.s64 	%rd62, %rd1, %rd36;
	add.s64 	%rd37, %rd33, %rd5;
	shl.b64 	%rd38, %rd37, 1;
	add.s64 	%rd61, %rd1, %rd38;
	add.s64 	%rd39, %rd33, %rd4;
	shl.b64 	%rd40, %rd39, 1;
	add.s64 	%rd60, %rd2, %rd40;
	mov.f32 	%f114, 0f00000000;
	mov.f32 	%f115, %f114;
	mov.u32 	%r141, %r3;

$L__BB81_5:
	.pragma "nounroll";
	ld.global.nc.u32 	%r33, [%rd60];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r33;
  cvt.f32.f16 %f19, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r33;
  cvt.f32.f16 %f20, high;}

	// end inline asm
	ld.global.nc.u32 	%r35, [%rd61];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r35;
  cvt.f32.f16 %f21, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r35;
  cvt.f32.f16 %f22, high;}

	// end inline asm
	fma.rn.f32 	%f25, %f19, %f21, %f115;
	fma.rn.f32 	%f115, %f20, %f22, %f25;
	ld.global.nc.u32 	%r37, [%rd62];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r37;
  cvt.f32.f16 %f23, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r37;
  cvt.f32.f16 %f24, high;}

	// end inline asm
	fma.rn.f32 	%f26, %f19, %f23, %f114;
	fma.rn.f32 	%f114, %f20, %f24, %f26;
	add.s32 	%r141, %r141, 96;
	add.s64 	%rd62, %rd62, 384;
	add.s64 	%rd61, %rd61, 384;
	add.s64 	%rd60, %rd60, 384;
	add.s32 	%r140, %r140, -1;
	setp.ne.s32 	%p4, %r140, 0;
	@%p4 bra 	$L__BB81_5;

	st.local.v2.f32 	[%rd3], {%f115, %f114};

$L__BB81_7:
	setp.lt.u32 	%p5, %r5, 288;
	@%p5 bra 	$L__BB81_11;

	mul.wide.s32 	%rd41, %r141, 2;
	add.s64 	%rd42, %rd41, %rd4;
	shl.b64 	%rd43, %rd42, 1;
	add.s64 	%rd44, %rd2, %rd43;
	add.s64 	%rd65, %rd44, 768;
	add.s64 	%rd45, %rd41, %rd5;
	shl.b64 	%rd46, %rd45, 1;
	add.s64 	%rd47, %rd1, %rd46;
	add.s64 	%rd64, %rd47, 1152;
	mul.wide.s32 	%rd48, %r16, 2;
	add.s64 	%rd49, %rd45, %rd48;
	shl.b64 	%rd50, %rd49, 1;
	add.s64 	%rd51, %rd1, %rd50;
	add.s64 	%rd63, %rd51, 768;

$L__BB81_9:
	ld.global.nc.u32 	%r39, [%rd65+-768];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r39;
  cvt.f32.f16 %f27, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r39;
  cvt.f32.f16 %f28, high;}

	// end inline asm
	ld.global.nc.u32 	%r41, [%rd64+-1152];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r41;
  cvt.f32.f16 %f29, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r41;
  cvt.f32.f16 %f30, high;}

	// end inline asm
	fma.rn.f32 	%f51, %f27, %f29, %f115;
	fma.rn.f32 	%f52, %f28, %f30, %f51;
	ld.global.nc.u32 	%r43, [%rd63+-768];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r43;
  cvt.f32.f16 %f31, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r43;
  cvt.f32.f16 %f32, high;}

	// end inline asm
	fma.rn.f32 	%f53, %f27, %f31, %f114;
	fma.rn.f32 	%f54, %f28, %f32, %f53;
	ld.global.nc.u32 	%r45, [%rd65+-384];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r45;
  cvt.f32.f16 %f33, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r45;
  cvt.f32.f16 %f34, high;}

	// end inline asm
	ld.global.nc.u32 	%r47, [%rd64+-768];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r47;
  cvt.f32.f16 %f35, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r47;
  cvt.f32.f16 %f36, high;}

	// end inline asm
	fma.rn.f32 	%f55, %f33, %f35, %f52;
	fma.rn.f32 	%f56, %f34, %f36, %f55;
	ld.global.nc.u32 	%r49, [%rd63+-384];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r49;
  cvt.f32.f16 %f37, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r49;
  cvt.f32.f16 %f38, high;}

	// end inline asm
	fma.rn.f32 	%f57, %f33, %f37, %f54;
	fma.rn.f32 	%f58, %f34, %f38, %f57;
	ld.global.nc.u32 	%r51, [%rd65];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r51;
  cvt.f32.f16 %f39, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r51;
  cvt.f32.f16 %f40, high;}

	// end inline asm
	ld.global.nc.u32 	%r53, [%rd64+-384];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r53;
  cvt.f32.f16 %f41, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r53;
  cvt.f32.f16 %f42, high;}

	// end inline asm
	fma.rn.f32 	%f59, %f39, %f41, %f56;
	fma.rn.f32 	%f60, %f40, %f42, %f59;
	ld.global.nc.u32 	%r55, [%rd63];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r55;
  cvt.f32.f16 %f43, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r55;
  cvt.f32.f16 %f44, high;}

	// end inline asm
	fma.rn.f32 	%f61, %f39, %f43, %f58;
	fma.rn.f32 	%f62, %f40, %f44, %f61;
	ld.global.nc.u32 	%r57, [%rd65+384];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r57;
  cvt.f32.f16 %f45, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r57;
  cvt.f32.f16 %f46, high;}

	// end inline asm
	ld.global.nc.u32 	%r59, [%rd64];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r59;
  cvt.f32.f16 %f47, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r59;
  cvt.f32.f16 %f48, high;}

	// end inline asm
	fma.rn.f32 	%f63, %f45, %f47, %f60;
	fma.rn.f32 	%f115, %f46, %f48, %f63;
	ld.global.nc.u32 	%r61, [%rd63+384];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r61;
  cvt.f32.f16 %f49, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r61;
  cvt.f32.f16 %f50, high;}

	// end inline asm
	fma.rn.f32 	%f64, %f45, %f49, %f62;
	fma.rn.f32 	%f114, %f46, %f50, %f64;
	add.s64 	%rd65, %rd65, 1536;
	add.s64 	%rd64, %rd64, 1536;
	add.s64 	%rd63, %rd63, 1536;
	add.s32 	%r141, %r141, 384;
	setp.lt.s32 	%p6, %r141, %r15;
	@%p6 bra 	$L__BB81_9;

	st.local.v2.f32 	[%rd3], {%f115, %f114};

$L__BB81_11:
	shr.s32 	%r63, %r3, 31;
	shr.u32 	%r64, %r63, 27;
	add.s32 	%r65, %r3, %r64;
	shr.s32 	%r66, %r65, 5;
	shl.b32 	%r67, %r66, 2;
	add.s32 	%r14, %r28, %r67;
	mov.u32 	%r69, 2;
	mov.b32 	%r70, %f115;
	mov.u32 	%r71, 31;
	mov.u32 	%r72, 16;
	mov.u32 	%r73, -1;
	shfl.sync.bfly.b32 	%r74|%p7, %r70, %r72, %r71, %r73;
	mov.b32 	%f65, %r74;
	add.f32 	%f66, %f115, %f65;
	mov.b32 	%r75, %f66;
	mov.u32 	%r76, 8;
	shfl.sync.bfly.b32 	%r77|%p8, %r75, %r76, %r71, %r73;
	mov.b32 	%f67, %r77;
	add.f32 	%f68, %f66, %f67;
	mov.b32 	%r78, %f68;
	mov.u32 	%r79, 4;
	shfl.sync.bfly.b32 	%r80|%p9, %r78, %r79, %r71, %r73;
	mov.b32 	%f69, %r80;
	add.f32 	%f70, %f68, %f69;
	mov.b32 	%r81, %f70;
	shfl.sync.bfly.b32 	%r82|%p10, %r81, %r69, %r71, %r73;
	mov.b32 	%f71, %r82;
	add.f32 	%f72, %f70, %f71;
	mov.b32 	%r83, %f72;
	mov.u32 	%r84, 1;
	shfl.sync.bfly.b32 	%r85|%p11, %r83, %r84, %r71, %r73;
	mov.b32 	%f73, %r85;
	add.f32 	%f74, %f72, %f73;
	st.local.f32 	[%rd3], %f74;
	st.shared.f32 	[%r14], %f74;
	bar.sync 	0;
	@%p1 bra 	$L__BB81_13;

	ld.shared.f32 	%f75, [%r4];
	mov.b32 	%r86, %f75;
	shfl.sync.bfly.b32 	%r90|%p13, %r86, %r72, %r71, %r73;
	mov.b32 	%f76, %r90;
	add.f32 	%f77, %f75, %f76;
	mov.b32 	%r91, %f77;
	shfl.sync.bfly.b32 	%r93|%p14, %r91, %r76, %r71, %r73;
	mov.b32 	%f78, %r93;
	add.f32 	%f79, %f77, %f78;
	mov.b32 	%r94, %f79;
	shfl.sync.bfly.b32 	%r96|%p15, %r94, %r79, %r71, %r73;
	mov.b32 	%f80, %r96;
	add.f32 	%f81, %f79, %f80;
	mov.b32 	%r97, %f81;
	shfl.sync.bfly.b32 	%r99|%p16, %r97, %r69, %r71, %r73;
	mov.b32 	%f82, %r99;
	add.f32 	%f83, %f81, %f82;
	mov.b32 	%r100, %f83;
	shfl.sync.bfly.b32 	%r102|%p17, %r100, %r84, %r71, %r73;
	mov.b32 	%f84, %r102;
	add.f32 	%f85, %f83, %f84;
	st.local.f32 	[%rd3], %f85;

$L__BB81_13:
	bar.sync 	0;
	mov.b32 	%r103, %f114;
	shfl.sync.bfly.b32 	%r107|%p19, %r103, %r72, %r71, %r73;
	mov.b32 	%f86, %r107;
	add.f32 	%f87, %f114, %f86;
	mov.b32 	%r108, %f87;
	shfl.sync.bfly.b32 	%r110|%p20, %r108, %r76, %r71, %r73;
	mov.b32 	%f88, %r110;
	add.f32 	%f89, %f87, %f88;
	mov.b32 	%r111, %f89;
	shfl.sync.bfly.b32 	%r113|%p21, %r111, %r79, %r71, %r73;
	mov.b32 	%f90, %r113;
	add.f32 	%f91, %f89, %f90;
	mov.b32 	%r114, %f91;
	shfl.sync.bfly.b32 	%r116|%p22, %r114, %r69, %r71, %r73;
	mov.b32 	%f92, %r116;
	add.f32 	%f93, %f91, %f92;
	mov.b32 	%r117, %f93;
	shfl.sync.bfly.b32 	%r119|%p23, %r117, %r84, %r71, %r73;
	mov.b32 	%f94, %r119;
	add.f32 	%f95, %f93, %f94;
	st.local.f32 	[%rd3+4], %f95;
	st.shared.f32 	[%r14], %f95;
	bar.sync 	0;
	@%p1 bra 	$L__BB81_15;

	ld.shared.f32 	%f96, [%r4];
	mov.b32 	%r120, %f96;
	mov.u32 	%r121, 31;
	mov.u32 	%r122, 16;
	mov.u32 	%r123, -1;
	shfl.sync.bfly.b32 	%r124|%p24, %r120, %r122, %r121, %r123;
	mov.b32 	%f97, %r124;
	add.f32 	%f98, %f96, %f97;
	mov.b32 	%r125, %f98;
	mov.u32 	%r126, 8;
	shfl.sync.bfly.b32 	%r127|%p25, %r125, %r126, %r121, %r123;
	mov.b32 	%f99, %r127;
	add.f32 	%f100, %f98, %f99;
	mov.b32 	%r128, %f100;
	mov.u32 	%r129, 4;
	shfl.sync.bfly.b32 	%r130|%p26, %r128, %r129, %r121, %r123;
	mov.b32 	%f101, %r130;
	add.f32 	%f102, %f100, %f101;
	mov.b32 	%r131, %f102;
	mov.u32 	%r132, 2;
	shfl.sync.bfly.b32 	%r133|%p27, %r131, %r132, %r121, %r123;
	mov.b32 	%f103, %r133;
	add.f32 	%f104, %f102, %f103;
	mov.b32 	%r134, %f104;
	mov.u32 	%r135, 1;
	shfl.sync.bfly.b32 	%r136|%p28, %r134, %r135, %r121, %r123;
	mov.b32 	%f105, %r136;
	add.f32 	%f106, %f104, %f105;
	st.local.f32 	[%rd3+4], %f106;

$L__BB81_15:
	bar.sync 	0;
	setp.gt.s32 	%p29, %r3, 1;
	@%p29 bra 	$L__BB81_17;

	mad.lo.s32 	%r137, %r3, %r17, %r2;
	cvt.s64.s32 	%rd52, %r137;
	mul.lo.s32 	%r138, %r1, %r18;
	cvt.s64.s32 	%rd53, %r138;
	add.s64 	%rd54, %rd53, %rd52;
	mul.wide.s32 	%rd55, %r3, 4;
	add.s64 	%rd56, %rd3, %rd55;
	ld.local.f32 	%f107, [%rd56];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f107;}

	// end inline asm
	cvta.to.global.u64 	%rd57, %rd26;
	shl.b64 	%rd58, %rd54, 1;
	add.s64 	%rd59, %rd57, %rd58;
	st.global.u16 	[%rd59], %rs1;

$L__BB81_17:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_3_bs_96
.visible .entry ggml_matvec_f16_ncols_3_bs_96(
	.param .u64 ggml_matvec_f16_ncols_3_bs_96_param_0,
	.param .u64 ggml_matvec_f16_ncols_3_bs_96_param_1,
	.param .u64 ggml_matvec_f16_ncols_3_bs_96_param_2,
	.param .u32 ggml_matvec_f16_ncols_3_bs_96_param_3,
	.param .u32 ggml_matvec_f16_ncols_3_bs_96_param_4,
	.param .u32 ggml_matvec_f16_ncols_3_bs_96_param_5,
	.param .u32 ggml_matvec_f16_ncols_3_bs_96_param_6,
	.param .u32 ggml_matvec_f16_ncols_3_bs_96_param_7,
	.param .u32 ggml_matvec_f16_ncols_3_bs_96_param_8,
	.param .u32 ggml_matvec_f16_ncols_3_bs_96_param_9,
	.param .u32 ggml_matvec_f16_ncols_3_bs_96_param_10,
	.param .u32 ggml_matvec_f16_ncols_3_bs_96_param_11
)
{
	.local .align 4 .b8 	__local_depot82[12];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<41>;
	.reg .b16 	%rs<2>;
	.reg .f32 	%f<168>;
	.reg .b32 	%r<194>;
	.reg .b64 	%rd<74>;


	mov.u64 	%SPL, __local_depot82;
	ld.param.u64 	%rd29, [ggml_matvec_f16_ncols_3_bs_96_param_0];
	ld.param.u64 	%rd30, [ggml_matvec_f16_ncols_3_bs_96_param_1];
	ld.param.u64 	%rd28, [ggml_matvec_f16_ncols_3_bs_96_param_2];
	ld.param.u32 	%r15, [ggml_matvec_f16_ncols_3_bs_96_param_3];
	ld.param.u32 	%r19, [ggml_matvec_f16_ncols_3_bs_96_param_5];
	ld.param.u32 	%r16, [ggml_matvec_f16_ncols_3_bs_96_param_6];
	ld.param.u32 	%r17, [ggml_matvec_f16_ncols_3_bs_96_param_7];
	ld.param.u32 	%r20, [ggml_matvec_f16_ncols_3_bs_96_param_8];
	ld.param.u32 	%r21, [ggml_matvec_f16_ncols_3_bs_96_param_9];
	ld.param.u32 	%r22, [ggml_matvec_f16_ncols_3_bs_96_param_10];
	ld.param.u32 	%r18, [ggml_matvec_f16_ncols_3_bs_96_param_11];
	cvta.to.global.u64 	%rd73, %rd30;
	cvta.to.global.u64 	%rd2, %rd29;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r23, %r1, %r20;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r24, %r2, %r19;
	mad.lo.s32 	%r25, %r23, %r21, %r24;
	cvt.s64.s32 	%rd4, %r25;
	mul.lo.s32 	%r26, %r1, %r22;
	cvt.s64.s32 	%rd5, %r26;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r27, %r3, 2;
	mov.u32 	%r28, data_mmv;
	add.s32 	%r4, %r28, %r27;
	@%p1 bra 	$L__BB82_2;

	mov.u32 	%r29, 0;
	st.shared.u32 	[%r4], %r29;

$L__BB82_2:
	bar.sync 	0;
	mov.f32 	%f165, 0f00000000;
	mov.u32 	%r30, 0;
	st.local.u32 	[%rd3], %r30;
	st.local.u32 	[%rd3+4], %r30;
	st.local.u32 	[%rd3+8], %r30;
	setp.ge.s32 	%p2, %r3, %r15;
	mov.f32 	%f166, %f165;
	mov.f32 	%f167, %f165;
	@%p2 bra 	$L__BB82_11;

	not.b32 	%r31, %r3;
	add.s32 	%r5, %r31, %r15;
	mul.wide.u32 	%rd32, %r5, -1431655765;
	shr.u64 	%rd33, %rd32, 38;
	cvt.u32.u64 	%r32, %rd33;
	add.s32 	%r33, %r32, 1;
	and.b32  	%r191, %r33, 3;
	setp.eq.s32 	%p3, %r191, 0;
	mov.f32 	%f165, 0f00000000;
	mov.u32 	%r192, %r3;
	@%p3 bra 	$L__BB82_7;

	shl.b32 	%r34, %r16, 1;
	add.s32 	%r35, %r3, %r34;
	mul.wide.s32 	%rd34, %r35, 2;
	add.s64 	%rd35, %rd34, %rd5;
	shl.b64 	%rd36, %rd35, 1;
	add.s64 	%rd71, %rd73, %rd36;
	mul.wide.s32 	%rd37, %r16, 2;
	mul.wide.s32 	%rd38, %r3, 2;
	add.s64 	%rd39, %rd37, %rd38;
	add.s64 	%rd40, %rd39, %rd5;
	shl.b64 	%rd41, %rd40, 1;
	add.s64 	%rd70, %rd73, %rd41;
	add.s64 	%rd42, %rd38, %rd5;
	shl.b64 	%rd43, %rd42, 1;
	add.s64 	%rd69, %rd73, %rd43;
	add.s64 	%rd44, %rd38, %rd4;
	shl.b64 	%rd45, %rd44, 1;
	add.s64 	%rd68, %rd2, %rd45;
	mov.f32 	%f165, 0f00000000;
	mov.f32 	%f166, %f165;
	mov.f32 	%f167, %f165;
	mov.u32 	%r192, %r3;

$L__BB82_5:
	.pragma "nounroll";
	ld.global.nc.u32 	%r36, [%rd68];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r36;
  cvt.f32.f16 %f28, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r36;
  cvt.f32.f16 %f29, high;}

	// end inline asm
	ld.global.nc.u32 	%r38, [%rd69];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r38;
  cvt.f32.f16 %f30, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r38;
  cvt.f32.f16 %f31, high;}

	// end inline asm
	fma.rn.f32 	%f36, %f28, %f30, %f167;
	fma.rn.f32 	%f167, %f29, %f31, %f36;
	ld.global.nc.u32 	%r40, [%rd70];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r40;
  cvt.f32.f16 %f32, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r40;
  cvt.f32.f16 %f33, high;}

	// end inline asm
	fma.rn.f32 	%f37, %f28, %f32, %f166;
	fma.rn.f32 	%f166, %f29, %f33, %f37;
	ld.global.nc.u32 	%r42, [%rd71];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r42;
  cvt.f32.f16 %f34, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r42;
  cvt.f32.f16 %f35, high;}

	// end inline asm
	fma.rn.f32 	%f38, %f28, %f34, %f165;
	fma.rn.f32 	%f165, %f29, %f35, %f38;
	add.s32 	%r192, %r192, 96;
	add.s64 	%rd71, %rd71, 384;
	add.s64 	%rd70, %rd70, 384;
	add.s64 	%rd69, %rd69, 384;
	add.s64 	%rd68, %rd68, 384;
	add.s32 	%r191, %r191, -1;
	setp.ne.s32 	%p4, %r191, 0;
	@%p4 bra 	$L__BB82_5;

	st.local.f32 	[%rd3], %f167;
	st.local.f32 	[%rd3+4], %f166;
	st.local.f32 	[%rd3+8], %f165;

$L__BB82_7:
	setp.lt.u32 	%p5, %r5, 288;
	@%p5 bra 	$L__BB82_11;

	add.s32 	%r44, %r192, %r16;
	shl.b32 	%r45, %r16, 1;
	add.s32 	%r46, %r192, %r45;
	add.s32 	%r47, %r44, 96;
	mul.wide.s32 	%rd46, %r47, 4;
	shl.b64 	%rd47, %rd5, 1;
	add.s64 	%rd19, %rd46, %rd47;
	mul.wide.s32 	%rd48, %r46, 4;
	add.s64 	%rd20, %rd48, %rd47;
	mul.wide.s32 	%rd49, %r192, 2;
	add.s64 	%rd50, %rd49, %rd4;
	shl.b64 	%rd51, %rd50, 1;
	add.s64 	%rd52, %rd2, %rd51;
	add.s64 	%rd72, %rd52, 768;
	mul.wide.s32 	%rd53, %r192, 4;
	add.s64 	%rd22, %rd53, %rd47;
	mul.wide.s32 	%rd54, %r16, 4;
	add.s64 	%rd55, %rd53, %rd54;
	add.s64 	%rd23, %rd55, %rd47;

$L__BB82_9:
	ld.global.nc.u32 	%r48, [%rd72+-768];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r48;
  cvt.f32.f16 %f39, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r48;
  cvt.f32.f16 %f40, high;}

	// end inline asm
	add.s64 	%rd56, %rd73, %rd22;
	ld.global.nc.u32 	%r50, [%rd56];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r50;
  cvt.f32.f16 %f41, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r50;
  cvt.f32.f16 %f42, high;}

	// end inline asm
	fma.rn.f32 	%f71, %f39, %f41, %f167;
	fma.rn.f32 	%f72, %f40, %f42, %f71;
	add.s64 	%rd57, %rd73, %rd23;
	ld.global.nc.u32 	%r52, [%rd57];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r52;
  cvt.f32.f16 %f43, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r52;
  cvt.f32.f16 %f44, high;}

	// end inline asm
	fma.rn.f32 	%f73, %f39, %f43, %f166;
	fma.rn.f32 	%f74, %f40, %f44, %f73;
	add.s64 	%rd58, %rd73, %rd20;
	ld.global.nc.u32 	%r54, [%rd58];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r54;
  cvt.f32.f16 %f45, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r54;
  cvt.f32.f16 %f46, high;}

	// end inline asm
	fma.rn.f32 	%f75, %f39, %f45, %f165;
	fma.rn.f32 	%f76, %f40, %f46, %f75;
	ld.global.nc.u32 	%r56, [%rd72+-384];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r56;
  cvt.f32.f16 %f47, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r56;
  cvt.f32.f16 %f48, high;}

	// end inline asm
	ld.global.nc.u32 	%r58, [%rd56+384];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r58;
  cvt.f32.f16 %f49, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r58;
  cvt.f32.f16 %f50, high;}

	// end inline asm
	fma.rn.f32 	%f77, %f47, %f49, %f72;
	fma.rn.f32 	%f78, %f48, %f50, %f77;
	add.s64 	%rd59, %rd73, %rd19;
	ld.global.nc.u32 	%r60, [%rd59];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r60;
  cvt.f32.f16 %f51, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r60;
  cvt.f32.f16 %f52, high;}

	// end inline asm
	fma.rn.f32 	%f79, %f47, %f51, %f74;
	fma.rn.f32 	%f80, %f48, %f52, %f79;
	ld.global.nc.u32 	%r62, [%rd58+384];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r62;
  cvt.f32.f16 %f53, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r62;
  cvt.f32.f16 %f54, high;}

	// end inline asm
	fma.rn.f32 	%f81, %f47, %f53, %f76;
	fma.rn.f32 	%f82, %f48, %f54, %f81;
	ld.global.nc.u32 	%r64, [%rd72];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r64;
  cvt.f32.f16 %f55, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r64;
  cvt.f32.f16 %f56, high;}

	// end inline asm
	ld.global.nc.u32 	%r66, [%rd56+768];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r66;
  cvt.f32.f16 %f57, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r66;
  cvt.f32.f16 %f58, high;}

	// end inline asm
	fma.rn.f32 	%f83, %f55, %f57, %f78;
	fma.rn.f32 	%f84, %f56, %f58, %f83;
	ld.global.nc.u32 	%r68, [%rd59+384];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r68;
  cvt.f32.f16 %f59, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r68;
  cvt.f32.f16 %f60, high;}

	// end inline asm
	fma.rn.f32 	%f85, %f55, %f59, %f80;
	fma.rn.f32 	%f86, %f56, %f60, %f85;
	ld.global.nc.u32 	%r70, [%rd58+768];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r70;
  cvt.f32.f16 %f61, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r70;
  cvt.f32.f16 %f62, high;}

	// end inline asm
	fma.rn.f32 	%f87, %f55, %f61, %f82;
	fma.rn.f32 	%f88, %f56, %f62, %f87;
	ld.global.nc.u32 	%r72, [%rd72+384];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r72;
  cvt.f32.f16 %f63, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r72;
  cvt.f32.f16 %f64, high;}

	// end inline asm
	ld.global.nc.u32 	%r74, [%rd56+1152];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r74;
  cvt.f32.f16 %f65, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r74;
  cvt.f32.f16 %f66, high;}

	// end inline asm
	fma.rn.f32 	%f89, %f63, %f65, %f84;
	fma.rn.f32 	%f167, %f64, %f66, %f89;
	ld.global.nc.u32 	%r76, [%rd59+768];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r76;
  cvt.f32.f16 %f67, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r76;
  cvt.f32.f16 %f68, high;}

	// end inline asm
	fma.rn.f32 	%f90, %f63, %f67, %f86;
	fma.rn.f32 	%f166, %f64, %f68, %f90;
	ld.global.nc.u32 	%r78, [%rd58+1152];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r78;
  cvt.f32.f16 %f69, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r78;
  cvt.f32.f16 %f70, high;}

	// end inline asm
	fma.rn.f32 	%f91, %f63, %f69, %f88;
	fma.rn.f32 	%f165, %f64, %f70, %f91;
	add.s64 	%rd73, %rd73, 1536;
	add.s64 	%rd72, %rd72, 1536;
	add.s32 	%r192, %r192, 384;
	setp.lt.s32 	%p6, %r192, %r15;
	@%p6 bra 	$L__BB82_9;

	st.local.f32 	[%rd3], %f167;
	st.local.f32 	[%rd3+4], %f166;
	st.local.f32 	[%rd3+8], %f165;

$L__BB82_11:
	shr.s32 	%r80, %r3, 31;
	shr.u32 	%r81, %r80, 27;
	add.s32 	%r82, %r3, %r81;
	shr.s32 	%r83, %r82, 5;
	shl.b32 	%r84, %r83, 2;
	add.s32 	%r14, %r28, %r84;
	mov.u32 	%r86, 2;
	mov.b32 	%r87, %f167;
	mov.u32 	%r88, 31;
	mov.u32 	%r89, 16;
	mov.u32 	%r90, -1;
	shfl.sync.bfly.b32 	%r91|%p7, %r87, %r89, %r88, %r90;
	mov.b32 	%f92, %r91;
	add.f32 	%f93, %f167, %f92;
	mov.b32 	%r92, %f93;
	mov.u32 	%r93, 8;
	shfl.sync.bfly.b32 	%r94|%p8, %r92, %r93, %r88, %r90;
	mov.b32 	%f94, %r94;
	add.f32 	%f95, %f93, %f94;
	mov.b32 	%r95, %f95;
	mov.u32 	%r96, 4;
	shfl.sync.bfly.b32 	%r97|%p9, %r95, %r96, %r88, %r90;
	mov.b32 	%f96, %r97;
	add.f32 	%f97, %f95, %f96;
	mov.b32 	%r98, %f97;
	shfl.sync.bfly.b32 	%r99|%p10, %r98, %r86, %r88, %r90;
	mov.b32 	%f98, %r99;
	add.f32 	%f99, %f97, %f98;
	mov.b32 	%r100, %f99;
	mov.u32 	%r101, 1;
	shfl.sync.bfly.b32 	%r102|%p11, %r100, %r101, %r88, %r90;
	mov.b32 	%f100, %r102;
	add.f32 	%f101, %f99, %f100;
	st.local.f32 	[%rd3], %f101;
	st.shared.f32 	[%r14], %f101;
	bar.sync 	0;
	@%p1 bra 	$L__BB82_13;

	ld.shared.f32 	%f102, [%r4];
	mov.b32 	%r103, %f102;
	shfl.sync.bfly.b32 	%r107|%p13, %r103, %r89, %r88, %r90;
	mov.b32 	%f103, %r107;
	add.f32 	%f104, %f102, %f103;
	mov.b32 	%r108, %f104;
	shfl.sync.bfly.b32 	%r110|%p14, %r108, %r93, %r88, %r90;
	mov.b32 	%f105, %r110;
	add.f32 	%f106, %f104, %f105;
	mov.b32 	%r111, %f106;
	shfl.sync.bfly.b32 	%r113|%p15, %r111, %r96, %r88, %r90;
	mov.b32 	%f107, %r113;
	add.f32 	%f108, %f106, %f107;
	mov.b32 	%r114, %f108;
	shfl.sync.bfly.b32 	%r116|%p16, %r114, %r86, %r88, %r90;
	mov.b32 	%f109, %r116;
	add.f32 	%f110, %f108, %f109;
	mov.b32 	%r117, %f110;
	shfl.sync.bfly.b32 	%r119|%p17, %r117, %r101, %r88, %r90;
	mov.b32 	%f111, %r119;
	add.f32 	%f112, %f110, %f111;
	st.local.f32 	[%rd3], %f112;

$L__BB82_13:
	bar.sync 	0;
	mov.b32 	%r120, %f166;
	shfl.sync.bfly.b32 	%r124|%p19, %r120, %r89, %r88, %r90;
	mov.b32 	%f113, %r124;
	add.f32 	%f114, %f166, %f113;
	mov.b32 	%r125, %f114;
	shfl.sync.bfly.b32 	%r127|%p20, %r125, %r93, %r88, %r90;
	mov.b32 	%f115, %r127;
	add.f32 	%f116, %f114, %f115;
	mov.b32 	%r128, %f116;
	shfl.sync.bfly.b32 	%r130|%p21, %r128, %r96, %r88, %r90;
	mov.b32 	%f117, %r130;
	add.f32 	%f118, %f116, %f117;
	mov.b32 	%r131, %f118;
	shfl.sync.bfly.b32 	%r133|%p22, %r131, %r86, %r88, %r90;
	mov.b32 	%f119, %r133;
	add.f32 	%f120, %f118, %f119;
	mov.b32 	%r134, %f120;
	shfl.sync.bfly.b32 	%r136|%p23, %r134, %r101, %r88, %r90;
	mov.b32 	%f121, %r136;
	add.f32 	%f122, %f120, %f121;
	st.local.f32 	[%rd3+4], %f122;
	st.shared.f32 	[%r14], %f122;
	bar.sync 	0;
	@%p1 bra 	$L__BB82_15;

	ld.shared.f32 	%f123, [%r4];
	mov.b32 	%r137, %f123;
	mov.u32 	%r138, 31;
	mov.u32 	%r139, 16;
	mov.u32 	%r140, -1;
	shfl.sync.bfly.b32 	%r141|%p24, %r137, %r139, %r138, %r140;
	mov.b32 	%f124, %r141;
	add.f32 	%f125, %f123, %f124;
	mov.b32 	%r142, %f125;
	mov.u32 	%r143, 8;
	shfl.sync.bfly.b32 	%r144|%p25, %r142, %r143, %r138, %r140;
	mov.b32 	%f126, %r144;
	add.f32 	%f127, %f125, %f126;
	mov.b32 	%r145, %f127;
	mov.u32 	%r146, 4;
	shfl.sync.bfly.b32 	%r147|%p26, %r145, %r146, %r138, %r140;
	mov.b32 	%f128, %r147;
	add.f32 	%f129, %f127, %f128;
	mov.b32 	%r148, %f129;
	mov.u32 	%r149, 2;
	shfl.sync.bfly.b32 	%r150|%p27, %r148, %r149, %r138, %r140;
	mov.b32 	%f130, %r150;
	add.f32 	%f131, %f129, %f130;
	mov.b32 	%r151, %f131;
	mov.u32 	%r152, 1;
	shfl.sync.bfly.b32 	%r153|%p28, %r151, %r152, %r138, %r140;
	mov.b32 	%f132, %r153;
	add.f32 	%f133, %f131, %f132;
	st.local.f32 	[%rd3+4], %f133;

$L__BB82_15:
	bar.sync 	0;
	mov.b32 	%r154, %f165;
	mov.u32 	%r155, 31;
	mov.u32 	%r156, 16;
	mov.u32 	%r157, -1;
	shfl.sync.bfly.b32 	%r158|%p30, %r154, %r156, %r155, %r157;
	mov.b32 	%f134, %r158;
	add.f32 	%f135, %f165, %f134;
	mov.b32 	%r159, %f135;
	mov.u32 	%r160, 8;
	shfl.sync.bfly.b32 	%r161|%p31, %r159, %r160, %r155, %r157;
	mov.b32 	%f136, %r161;
	add.f32 	%f137, %f135, %f136;
	mov.b32 	%r162, %f137;
	mov.u32 	%r163, 4;
	shfl.sync.bfly.b32 	%r164|%p32, %r162, %r163, %r155, %r157;
	mov.b32 	%f138, %r164;
	add.f32 	%f139, %f137, %f138;
	mov.b32 	%r165, %f139;
	mov.u32 	%r166, 2;
	shfl.sync.bfly.b32 	%r167|%p33, %r165, %r166, %r155, %r157;
	mov.b32 	%f140, %r167;
	add.f32 	%f141, %f139, %f140;
	mov.b32 	%r168, %f141;
	mov.u32 	%r169, 1;
	shfl.sync.bfly.b32 	%r170|%p34, %r168, %r169, %r155, %r157;
	mov.b32 	%f142, %r170;
	add.f32 	%f143, %f141, %f142;
	st.local.f32 	[%rd3+8], %f143;
	st.shared.f32 	[%r14], %f143;
	bar.sync 	0;
	@%p1 bra 	$L__BB82_17;

	ld.shared.f32 	%f144, [%r4];
	mov.b32 	%r171, %f144;
	shfl.sync.bfly.b32 	%r175|%p35, %r171, %r156, %r155, %r157;
	mov.b32 	%f145, %r175;
	add.f32 	%f146, %f144, %f145;
	mov.b32 	%r176, %f146;
	shfl.sync.bfly.b32 	%r178|%p36, %r176, %r160, %r155, %r157;
	mov.b32 	%f147, %r178;
	add.f32 	%f148, %f146, %f147;
	mov.b32 	%r179, %f148;
	shfl.sync.bfly.b32 	%r181|%p37, %r179, %r163, %r155, %r157;
	mov.b32 	%f149, %r181;
	add.f32 	%f150, %f148, %f149;
	mov.b32 	%r182, %f150;
	shfl.sync.bfly.b32 	%r184|%p38, %r182, %r166, %r155, %r157;
	mov.b32 	%f151, %r184;
	add.f32 	%f152, %f150, %f151;
	mov.b32 	%r185, %f152;
	shfl.sync.bfly.b32 	%r187|%p39, %r185, %r169, %r155, %r157;
	mov.b32 	%f153, %r187;
	add.f32 	%f154, %f152, %f153;
	st.local.f32 	[%rd3+8], %f154;

$L__BB82_17:
	bar.sync 	0;
	setp.gt.s32 	%p40, %r3, 2;
	@%p40 bra 	$L__BB82_19;

	mad.lo.s32 	%r188, %r3, %r17, %r2;
	cvt.s64.s32 	%rd60, %r188;
	mul.lo.s32 	%r189, %r1, %r18;
	cvt.s64.s32 	%rd61, %r189;
	add.s64 	%rd62, %rd61, %rd60;
	mul.wide.s32 	%rd63, %r3, 4;
	add.s64 	%rd64, %rd3, %rd63;
	ld.local.f32 	%f155, [%rd64];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f155;}

	// end inline asm
	cvta.to.global.u64 	%rd65, %rd28;
	shl.b64 	%rd66, %rd62, 1;
	add.s64 	%rd67, %rd65, %rd66;
	st.global.u16 	[%rd67], %rs1;

$L__BB82_19:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_4_bs_96
.visible .entry ggml_matvec_f16_ncols_4_bs_96(
	.param .u64 ggml_matvec_f16_ncols_4_bs_96_param_0,
	.param .u64 ggml_matvec_f16_ncols_4_bs_96_param_1,
	.param .u64 ggml_matvec_f16_ncols_4_bs_96_param_2,
	.param .u32 ggml_matvec_f16_ncols_4_bs_96_param_3,
	.param .u32 ggml_matvec_f16_ncols_4_bs_96_param_4,
	.param .u32 ggml_matvec_f16_ncols_4_bs_96_param_5,
	.param .u32 ggml_matvec_f16_ncols_4_bs_96_param_6,
	.param .u32 ggml_matvec_f16_ncols_4_bs_96_param_7,
	.param .u32 ggml_matvec_f16_ncols_4_bs_96_param_8,
	.param .u32 ggml_matvec_f16_ncols_4_bs_96_param_9,
	.param .u32 ggml_matvec_f16_ncols_4_bs_96_param_10,
	.param .u32 ggml_matvec_f16_ncols_4_bs_96_param_11
)
{
	.local .align 16 .b8 	__local_depot83[16];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<53>;
	.reg .b16 	%rs<2>;
	.reg .f32 	%f<172>;
	.reg .b32 	%r<211>;
	.reg .b64 	%rd<64>;


	mov.u64 	%SPL, __local_depot83;
	ld.param.u64 	%rd19, [ggml_matvec_f16_ncols_4_bs_96_param_0];
	ld.param.u64 	%rd20, [ggml_matvec_f16_ncols_4_bs_96_param_1];
	ld.param.u64 	%rd18, [ggml_matvec_f16_ncols_4_bs_96_param_2];
	ld.param.u32 	%r11, [ggml_matvec_f16_ncols_4_bs_96_param_3];
	ld.param.u32 	%r15, [ggml_matvec_f16_ncols_4_bs_96_param_5];
	ld.param.u32 	%r12, [ggml_matvec_f16_ncols_4_bs_96_param_6];
	ld.param.u32 	%r13, [ggml_matvec_f16_ncols_4_bs_96_param_7];
	ld.param.u32 	%r16, [ggml_matvec_f16_ncols_4_bs_96_param_8];
	ld.param.u32 	%r17, [ggml_matvec_f16_ncols_4_bs_96_param_9];
	ld.param.u32 	%r18, [ggml_matvec_f16_ncols_4_bs_96_param_10];
	ld.param.u32 	%r14, [ggml_matvec_f16_ncols_4_bs_96_param_11];
	cvta.to.global.u64 	%rd63, %rd20;
	add.u64 	%rd2, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r19, %r1, %r16;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r20, %r2, %r15;
	mad.lo.s32 	%r21, %r19, %r17, %r20;
	cvt.s64.s32 	%rd3, %r21;
	cvta.to.global.u64 	%rd4, %rd19;
	mul.lo.s32 	%r22, %r1, %r18;
	cvt.s64.s32 	%rd5, %r22;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r23, %r3, 2;
	mov.u32 	%r24, data_mmv;
	add.s32 	%r4, %r24, %r23;
	@%p1 bra 	$L__BB83_2;

	mov.u32 	%r25, 0;
	st.shared.u32 	[%r4], %r25;

$L__BB83_2:
	bar.sync 	0;
	mov.f32 	%f168, 0f00000000;
	st.local.v4.f32 	[%rd2], {%f168, %f168, %f168, %f168};
	setp.ge.s32 	%p2, %r3, %r11;
	mov.f32 	%f169, %f168;
	mov.f32 	%f170, %f168;
	mov.f32 	%f171, %f168;
	@%p2 bra 	$L__BB83_9;

	not.b32 	%r26, %r3;
	add.s32 	%r5, %r26, %r11;
	mul.wide.u32 	%rd22, %r5, -1431655765;
	shr.u64 	%rd23, %rd22, 38;
	and.b64  	%rd24, %rd23, 1;
	setp.eq.b64 	%p3, %rd24, 1;
	mov.pred 	%p4, 0;
	xor.pred  	%p5, %p3, %p4;
	mov.f32 	%f168, 0f00000000;
	mov.u32 	%r210, %r3;
	@%p5 bra 	$L__BB83_5;

	shl.b64 	%rd25, %rd5, 1;
	add.s64 	%rd26, %rd63, %rd25;
	shl.b64 	%rd27, %rd3, 1;
	add.s64 	%rd28, %rd4, %rd27;
	mul.wide.s32 	%rd29, %r3, 4;
	add.s64 	%rd30, %rd28, %rd29;
	ld.global.nc.u32 	%r27, [%rd30];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r27;
  cvt.f32.f16 %f29, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r27;
  cvt.f32.f16 %f30, high;}

	// end inline asm
	add.s64 	%rd31, %rd26, %rd29;
	ld.global.nc.u32 	%r29, [%rd31];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r29;
  cvt.f32.f16 %f31, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r29;
  cvt.f32.f16 %f32, high;}

	// end inline asm
	fma.rn.f32 	%f39, %f29, %f31, 0f00000000;
	fma.rn.f32 	%f171, %f30, %f32, %f39;
	st.local.f32 	[%rd2], %f171;
	mul.wide.s32 	%rd32, %r12, 4;
	add.s64 	%rd33, %rd31, %rd32;
	ld.global.nc.u32 	%r31, [%rd33];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r31;
  cvt.f32.f16 %f33, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r31;
  cvt.f32.f16 %f34, high;}

	// end inline asm
	fma.rn.f32 	%f40, %f29, %f33, 0f00000000;
	fma.rn.f32 	%f170, %f30, %f34, %f40;
	st.local.f32 	[%rd2+4], %f170;
	add.s32 	%r37, %r3, %r12;
	add.s32 	%r38, %r37, %r12;
	mul.wide.s32 	%rd34, %r38, 4;
	add.s64 	%rd35, %rd26, %rd34;
	ld.global.nc.u32 	%r33, [%rd35];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r33;
  cvt.f32.f16 %f35, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r33;
  cvt.f32.f16 %f36, high;}

	// end inline asm
	fma.rn.f32 	%f41, %f29, %f35, 0f00000000;
	fma.rn.f32 	%f169, %f30, %f36, %f41;
	st.local.f32 	[%rd2+8], %f169;
	add.s32 	%r39, %r38, %r12;
	mul.wide.s32 	%rd36, %r39, 4;
	add.s64 	%rd37, %rd26, %rd36;
	ld.global.nc.u32 	%r35, [%rd37];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r35;
  cvt.f32.f16 %f37, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r35;
  cvt.f32.f16 %f38, high;}

	// end inline asm
	fma.rn.f32 	%f42, %f29, %f37, 0f00000000;
	fma.rn.f32 	%f168, %f30, %f38, %f42;
	st.local.f32 	[%rd2+12], %f168;
	add.s32 	%r210, %r3, 96;

$L__BB83_5:
	setp.lt.u32 	%p6, %r5, 96;
	@%p6 bra 	$L__BB83_9;

	add.s32 	%r40, %r210, %r12;
	add.s32 	%r41, %r40, 96;
	mul.wide.s32 	%rd38, %r41, 4;
	shl.b64 	%rd39, %rd5, 1;
	add.s64 	%rd8, %rd38, %rd39;
	shl.b32 	%r42, %r12, 1;
	add.s32 	%r43, %r210, %r42;
	mad.lo.s32 	%r44, %r12, 3, %r210;
	mul.wide.s32 	%rd40, %r43, 4;
	add.s64 	%rd9, %rd40, %rd39;
	mul.wide.s32 	%rd41, %r44, 4;
	add.s64 	%rd10, %rd41, %rd39;
	mul.wide.s32 	%rd42, %r210, 2;
	add.s64 	%rd43, %rd42, %rd3;
	shl.b64 	%rd44, %rd43, 1;
	add.s64 	%rd45, %rd4, %rd44;
	add.s64 	%rd62, %rd45, 384;
	mul.wide.s32 	%rd46, %r210, 4;
	mul.wide.s32 	%rd47, %r12, 4;
	add.s64 	%rd48, %rd46, %rd47;
	add.s64 	%rd12, %rd48, %rd39;
	add.s64 	%rd13, %rd46, %rd39;

$L__BB83_7:
	ld.global.nc.u32 	%r45, [%rd62+-384];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r45;
  cvt.f32.f16 %f43, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r45;
  cvt.f32.f16 %f44, high;}

	// end inline asm
	add.s64 	%rd49, %rd63, %rd13;
	ld.global.nc.u32 	%r47, [%rd49];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r47;
  cvt.f32.f16 %f45, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r47;
  cvt.f32.f16 %f46, high;}

	// end inline asm
	fma.rn.f32 	%f63, %f43, %f45, %f171;
	fma.rn.f32 	%f64, %f44, %f46, %f63;
	add.s64 	%rd50, %rd63, %rd12;
	ld.global.nc.u32 	%r49, [%rd50];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r49;
  cvt.f32.f16 %f47, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r49;
  cvt.f32.f16 %f48, high;}

	// end inline asm
	fma.rn.f32 	%f65, %f43, %f47, %f170;
	fma.rn.f32 	%f66, %f44, %f48, %f65;
	add.s64 	%rd51, %rd63, %rd9;
	ld.global.nc.u32 	%r51, [%rd51];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r51;
  cvt.f32.f16 %f49, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r51;
  cvt.f32.f16 %f50, high;}

	// end inline asm
	fma.rn.f32 	%f67, %f43, %f49, %f169;
	fma.rn.f32 	%f68, %f44, %f50, %f67;
	add.s64 	%rd52, %rd63, %rd10;
	ld.global.nc.u32 	%r53, [%rd52];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r53;
  cvt.f32.f16 %f51, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r53;
  cvt.f32.f16 %f52, high;}

	// end inline asm
	fma.rn.f32 	%f69, %f43, %f51, %f168;
	fma.rn.f32 	%f70, %f44, %f52, %f69;
	ld.global.nc.u32 	%r55, [%rd62];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r55;
  cvt.f32.f16 %f53, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r55;
  cvt.f32.f16 %f54, high;}

	// end inline asm
	ld.global.nc.u32 	%r57, [%rd49+384];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r57;
  cvt.f32.f16 %f55, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r57;
  cvt.f32.f16 %f56, high;}

	// end inline asm
	fma.rn.f32 	%f71, %f53, %f55, %f64;
	fma.rn.f32 	%f171, %f54, %f56, %f71;
	add.s64 	%rd53, %rd63, %rd8;
	ld.global.nc.u32 	%r59, [%rd53];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r59;
  cvt.f32.f16 %f57, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r59;
  cvt.f32.f16 %f58, high;}

	// end inline asm
	fma.rn.f32 	%f72, %f53, %f57, %f66;
	fma.rn.f32 	%f170, %f54, %f58, %f72;
	ld.global.nc.u32 	%r61, [%rd51+384];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r61;
  cvt.f32.f16 %f59, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r61;
  cvt.f32.f16 %f60, high;}

	// end inline asm
	fma.rn.f32 	%f73, %f53, %f59, %f68;
	fma.rn.f32 	%f169, %f54, %f60, %f73;
	ld.global.nc.u32 	%r63, [%rd52+384];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r63;
  cvt.f32.f16 %f61, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r63;
  cvt.f32.f16 %f62, high;}

	// end inline asm
	fma.rn.f32 	%f74, %f53, %f61, %f70;
	fma.rn.f32 	%f168, %f54, %f62, %f74;
	add.s64 	%rd63, %rd63, 768;
	add.s64 	%rd62, %rd62, 768;
	add.s32 	%r210, %r210, 192;
	setp.lt.s32 	%p7, %r210, %r11;
	@%p7 bra 	$L__BB83_7;

	st.local.v4.f32 	[%rd2], {%f171, %f170, %f169, %f168};

$L__BB83_9:
	shr.s32 	%r65, %r3, 31;
	shr.u32 	%r66, %r65, 27;
	add.s32 	%r67, %r3, %r66;
	shr.s32 	%r68, %r67, 5;
	shl.b32 	%r69, %r68, 2;
	add.s32 	%r10, %r24, %r69;
	mov.u32 	%r71, 2;
	mov.b32 	%r72, %f171;
	mov.u32 	%r73, 31;
	mov.u32 	%r74, 16;
	mov.u32 	%r75, -1;
	shfl.sync.bfly.b32 	%r76|%p8, %r72, %r74, %r73, %r75;
	mov.b32 	%f75, %r76;
	add.f32 	%f76, %f171, %f75;
	mov.b32 	%r77, %f76;
	mov.u32 	%r78, 8;
	shfl.sync.bfly.b32 	%r79|%p9, %r77, %r78, %r73, %r75;
	mov.b32 	%f77, %r79;
	add.f32 	%f78, %f76, %f77;
	mov.b32 	%r80, %f78;
	mov.u32 	%r81, 4;
	shfl.sync.bfly.b32 	%r82|%p10, %r80, %r81, %r73, %r75;
	mov.b32 	%f79, %r82;
	add.f32 	%f80, %f78, %f79;
	mov.b32 	%r83, %f80;
	shfl.sync.bfly.b32 	%r84|%p11, %r83, %r71, %r73, %r75;
	mov.b32 	%f81, %r84;
	add.f32 	%f82, %f80, %f81;
	mov.b32 	%r85, %f82;
	mov.u32 	%r86, 1;
	shfl.sync.bfly.b32 	%r87|%p12, %r85, %r86, %r73, %r75;
	mov.b32 	%f83, %r87;
	add.f32 	%f84, %f82, %f83;
	st.local.f32 	[%rd2], %f84;
	st.shared.f32 	[%r10], %f84;
	bar.sync 	0;
	@%p1 bra 	$L__BB83_11;

	ld.shared.f32 	%f85, [%r4];
	mov.b32 	%r88, %f85;
	shfl.sync.bfly.b32 	%r92|%p14, %r88, %r74, %r73, %r75;
	mov.b32 	%f86, %r92;
	add.f32 	%f87, %f85, %f86;
	mov.b32 	%r93, %f87;
	shfl.sync.bfly.b32 	%r95|%p15, %r93, %r78, %r73, %r75;
	mov.b32 	%f88, %r95;
	add.f32 	%f89, %f87, %f88;
	mov.b32 	%r96, %f89;
	shfl.sync.bfly.b32 	%r98|%p16, %r96, %r81, %r73, %r75;
	mov.b32 	%f90, %r98;
	add.f32 	%f91, %f89, %f90;
	mov.b32 	%r99, %f91;
	shfl.sync.bfly.b32 	%r101|%p17, %r99, %r71, %r73, %r75;
	mov.b32 	%f92, %r101;
	add.f32 	%f93, %f91, %f92;
	mov.b32 	%r102, %f93;
	shfl.sync.bfly.b32 	%r104|%p18, %r102, %r86, %r73, %r75;
	mov.b32 	%f94, %r104;
	add.f32 	%f95, %f93, %f94;
	st.local.f32 	[%rd2], %f95;

$L__BB83_11:
	bar.sync 	0;
	mov.b32 	%r105, %f170;
	shfl.sync.bfly.b32 	%r109|%p20, %r105, %r74, %r73, %r75;
	mov.b32 	%f96, %r109;
	add.f32 	%f97, %f170, %f96;
	mov.b32 	%r110, %f97;
	shfl.sync.bfly.b32 	%r112|%p21, %r110, %r78, %r73, %r75;
	mov.b32 	%f98, %r112;
	add.f32 	%f99, %f97, %f98;
	mov.b32 	%r113, %f99;
	shfl.sync.bfly.b32 	%r115|%p22, %r113, %r81, %r73, %r75;
	mov.b32 	%f100, %r115;
	add.f32 	%f101, %f99, %f100;
	mov.b32 	%r116, %f101;
	shfl.sync.bfly.b32 	%r118|%p23, %r116, %r71, %r73, %r75;
	mov.b32 	%f102, %r118;
	add.f32 	%f103, %f101, %f102;
	mov.b32 	%r119, %f103;
	shfl.sync.bfly.b32 	%r121|%p24, %r119, %r86, %r73, %r75;
	mov.b32 	%f104, %r121;
	add.f32 	%f105, %f103, %f104;
	st.local.f32 	[%rd2+4], %f105;
	st.shared.f32 	[%r10], %f105;
	bar.sync 	0;
	@%p1 bra 	$L__BB83_13;

	ld.shared.f32 	%f106, [%r4];
	mov.b32 	%r122, %f106;
	mov.u32 	%r123, 31;
	mov.u32 	%r124, 16;
	mov.u32 	%r125, -1;
	shfl.sync.bfly.b32 	%r126|%p25, %r122, %r124, %r123, %r125;
	mov.b32 	%f107, %r126;
	add.f32 	%f108, %f106, %f107;
	mov.b32 	%r127, %f108;
	mov.u32 	%r128, 8;
	shfl.sync.bfly.b32 	%r129|%p26, %r127, %r128, %r123, %r125;
	mov.b32 	%f109, %r129;
	add.f32 	%f110, %f108, %f109;
	mov.b32 	%r130, %f110;
	mov.u32 	%r131, 4;
	shfl.sync.bfly.b32 	%r132|%p27, %r130, %r131, %r123, %r125;
	mov.b32 	%f111, %r132;
	add.f32 	%f112, %f110, %f111;
	mov.b32 	%r133, %f112;
	mov.u32 	%r134, 2;
	shfl.sync.bfly.b32 	%r135|%p28, %r133, %r134, %r123, %r125;
	mov.b32 	%f113, %r135;
	add.f32 	%f114, %f112, %f113;
	mov.b32 	%r136, %f114;
	mov.u32 	%r137, 1;
	shfl.sync.bfly.b32 	%r138|%p29, %r136, %r137, %r123, %r125;
	mov.b32 	%f115, %r138;
	add.f32 	%f116, %f114, %f115;
	st.local.f32 	[%rd2+4], %f116;

$L__BB83_13:
	bar.sync 	0;
	mov.b32 	%r139, %f169;
	mov.u32 	%r140, 31;
	mov.u32 	%r141, 16;
	mov.u32 	%r142, -1;
	shfl.sync.bfly.b32 	%r143|%p31, %r139, %r141, %r140, %r142;
	mov.b32 	%f117, %r143;
	add.f32 	%f118, %f169, %f117;
	mov.b32 	%r144, %f118;
	mov.u32 	%r145, 8;
	shfl.sync.bfly.b32 	%r146|%p32, %r144, %r145, %r140, %r142;
	mov.b32 	%f119, %r146;
	add.f32 	%f120, %f118, %f119;
	mov.b32 	%r147, %f120;
	mov.u32 	%r148, 4;
	shfl.sync.bfly.b32 	%r149|%p33, %r147, %r148, %r140, %r142;
	mov.b32 	%f121, %r149;
	add.f32 	%f122, %f120, %f121;
	mov.b32 	%r150, %f122;
	mov.u32 	%r151, 2;
	shfl.sync.bfly.b32 	%r152|%p34, %r150, %r151, %r140, %r142;
	mov.b32 	%f123, %r152;
	add.f32 	%f124, %f122, %f123;
	mov.b32 	%r153, %f124;
	mov.u32 	%r154, 1;
	shfl.sync.bfly.b32 	%r155|%p35, %r153, %r154, %r140, %r142;
	mov.b32 	%f125, %r155;
	add.f32 	%f126, %f124, %f125;
	st.local.f32 	[%rd2+8], %f126;
	st.shared.f32 	[%r10], %f126;
	bar.sync 	0;
	@%p1 bra 	$L__BB83_15;

	ld.shared.f32 	%f127, [%r4];
	mov.b32 	%r156, %f127;
	shfl.sync.bfly.b32 	%r160|%p36, %r156, %r141, %r140, %r142;
	mov.b32 	%f128, %r160;
	add.f32 	%f129, %f127, %f128;
	mov.b32 	%r161, %f129;
	shfl.sync.bfly.b32 	%r163|%p37, %r161, %r145, %r140, %r142;
	mov.b32 	%f130, %r163;
	add.f32 	%f131, %f129, %f130;
	mov.b32 	%r164, %f131;
	shfl.sync.bfly.b32 	%r166|%p38, %r164, %r148, %r140, %r142;
	mov.b32 	%f132, %r166;
	add.f32 	%f133, %f131, %f132;
	mov.b32 	%r167, %f133;
	shfl.sync.bfly.b32 	%r169|%p39, %r167, %r151, %r140, %r142;
	mov.b32 	%f134, %r169;
	add.f32 	%f135, %f133, %f134;
	mov.b32 	%r170, %f135;
	shfl.sync.bfly.b32 	%r172|%p40, %r170, %r154, %r140, %r142;
	mov.b32 	%f136, %r172;
	add.f32 	%f137, %f135, %f136;
	st.local.f32 	[%rd2+8], %f137;

$L__BB83_15:
	bar.sync 	0;
	mov.b32 	%r173, %f168;
	shfl.sync.bfly.b32 	%r177|%p42, %r173, %r141, %r140, %r142;
	mov.b32 	%f138, %r177;
	add.f32 	%f139, %f168, %f138;
	mov.b32 	%r178, %f139;
	shfl.sync.bfly.b32 	%r180|%p43, %r178, %r145, %r140, %r142;
	mov.b32 	%f140, %r180;
	add.f32 	%f141, %f139, %f140;
	mov.b32 	%r181, %f141;
	shfl.sync.bfly.b32 	%r183|%p44, %r181, %r148, %r140, %r142;
	mov.b32 	%f142, %r183;
	add.f32 	%f143, %f141, %f142;
	mov.b32 	%r184, %f143;
	shfl.sync.bfly.b32 	%r186|%p45, %r184, %r151, %r140, %r142;
	mov.b32 	%f144, %r186;
	add.f32 	%f145, %f143, %f144;
	mov.b32 	%r187, %f145;
	shfl.sync.bfly.b32 	%r189|%p46, %r187, %r154, %r140, %r142;
	mov.b32 	%f146, %r189;
	add.f32 	%f147, %f145, %f146;
	st.local.f32 	[%rd2+12], %f147;
	st.shared.f32 	[%r10], %f147;
	bar.sync 	0;
	@%p1 bra 	$L__BB83_17;

	ld.shared.f32 	%f148, [%r4];
	mov.b32 	%r190, %f148;
	mov.u32 	%r191, 31;
	mov.u32 	%r192, 16;
	mov.u32 	%r193, -1;
	shfl.sync.bfly.b32 	%r194|%p47, %r190, %r192, %r191, %r193;
	mov.b32 	%f149, %r194;
	add.f32 	%f150, %f148, %f149;
	mov.b32 	%r195, %f150;
	mov.u32 	%r196, 8;
	shfl.sync.bfly.b32 	%r197|%p48, %r195, %r196, %r191, %r193;
	mov.b32 	%f151, %r197;
	add.f32 	%f152, %f150, %f151;
	mov.b32 	%r198, %f152;
	mov.u32 	%r199, 4;
	shfl.sync.bfly.b32 	%r200|%p49, %r198, %r199, %r191, %r193;
	mov.b32 	%f153, %r200;
	add.f32 	%f154, %f152, %f153;
	mov.b32 	%r201, %f154;
	mov.u32 	%r202, 2;
	shfl.sync.bfly.b32 	%r203|%p50, %r201, %r202, %r191, %r193;
	mov.b32 	%f155, %r203;
	add.f32 	%f156, %f154, %f155;
	mov.b32 	%r204, %f156;
	mov.u32 	%r205, 1;
	shfl.sync.bfly.b32 	%r206|%p51, %r204, %r205, %r191, %r193;
	mov.b32 	%f157, %r206;
	add.f32 	%f158, %f156, %f157;
	st.local.f32 	[%rd2+12], %f158;

$L__BB83_17:
	bar.sync 	0;
	setp.gt.s32 	%p52, %r3, 3;
	@%p52 bra 	$L__BB83_19;

	mad.lo.s32 	%r207, %r3, %r13, %r2;
	cvt.s64.s32 	%rd54, %r207;
	mul.lo.s32 	%r208, %r1, %r14;
	cvt.s64.s32 	%rd55, %r208;
	add.s64 	%rd56, %rd55, %rd54;
	mul.wide.s32 	%rd57, %r3, 4;
	add.s64 	%rd58, %rd2, %rd57;
	ld.local.f32 	%f159, [%rd58];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f159;}

	// end inline asm
	cvta.to.global.u64 	%rd59, %rd18;
	shl.b64 	%rd60, %rd56, 1;
	add.s64 	%rd61, %rd59, %rd60;
	st.global.u16 	[%rd61], %rs1;

$L__BB83_19:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_5_bs_96
.visible .entry ggml_matvec_f16_ncols_5_bs_96(
	.param .u64 ggml_matvec_f16_ncols_5_bs_96_param_0,
	.param .u64 ggml_matvec_f16_ncols_5_bs_96_param_1,
	.param .u64 ggml_matvec_f16_ncols_5_bs_96_param_2,
	.param .u32 ggml_matvec_f16_ncols_5_bs_96_param_3,
	.param .u32 ggml_matvec_f16_ncols_5_bs_96_param_4,
	.param .u32 ggml_matvec_f16_ncols_5_bs_96_param_5,
	.param .u32 ggml_matvec_f16_ncols_5_bs_96_param_6,
	.param .u32 ggml_matvec_f16_ncols_5_bs_96_param_7,
	.param .u32 ggml_matvec_f16_ncols_5_bs_96_param_8,
	.param .u32 ggml_matvec_f16_ncols_5_bs_96_param_9,
	.param .u32 ggml_matvec_f16_ncols_5_bs_96_param_10,
	.param .u32 ggml_matvec_f16_ncols_5_bs_96_param_11
)
{
	.local .align 4 .b8 	__local_depot84[20];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<64>;
	.reg .b16 	%rs<2>;
	.reg .f32 	%f<213>;
	.reg .b32 	%r<255>;
	.reg .b64 	%rd<67>;


	mov.u64 	%SPL, __local_depot84;
	ld.param.u64 	%rd19, [ggml_matvec_f16_ncols_5_bs_96_param_0];
	ld.param.u64 	%rd20, [ggml_matvec_f16_ncols_5_bs_96_param_1];
	ld.param.u64 	%rd18, [ggml_matvec_f16_ncols_5_bs_96_param_2];
	ld.param.u32 	%r11, [ggml_matvec_f16_ncols_5_bs_96_param_3];
	ld.param.u32 	%r15, [ggml_matvec_f16_ncols_5_bs_96_param_5];
	ld.param.u32 	%r12, [ggml_matvec_f16_ncols_5_bs_96_param_6];
	ld.param.u32 	%r13, [ggml_matvec_f16_ncols_5_bs_96_param_7];
	ld.param.u32 	%r16, [ggml_matvec_f16_ncols_5_bs_96_param_8];
	ld.param.u32 	%r17, [ggml_matvec_f16_ncols_5_bs_96_param_9];
	ld.param.u32 	%r18, [ggml_matvec_f16_ncols_5_bs_96_param_10];
	ld.param.u32 	%r14, [ggml_matvec_f16_ncols_5_bs_96_param_11];
	cvta.to.global.u64 	%rd66, %rd20;
	add.u64 	%rd2, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r19, %r1, %r16;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r20, %r2, %r15;
	mad.lo.s32 	%r21, %r19, %r17, %r20;
	cvt.s64.s32 	%rd3, %r21;
	cvta.to.global.u64 	%rd4, %rd19;
	mul.lo.s32 	%r22, %r1, %r18;
	cvt.s64.s32 	%rd5, %r22;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r23, %r3, 2;
	mov.u32 	%r24, data_mmv;
	add.s32 	%r4, %r24, %r23;
	@%p1 bra 	$L__BB84_2;

	mov.u32 	%r25, 0;
	st.shared.u32 	[%r4], %r25;

$L__BB84_2:
	bar.sync 	0;
	mov.f32 	%f208, 0f00000000;
	mov.u32 	%r26, 0;
	st.local.u32 	[%rd2], %r26;
	st.local.u32 	[%rd2+4], %r26;
	st.local.u32 	[%rd2+8], %r26;
	st.local.u32 	[%rd2+12], %r26;
	st.local.u32 	[%rd2+16], %r26;
	setp.ge.s32 	%p2, %r3, %r11;
	mov.f32 	%f209, %f208;
	mov.f32 	%f210, %f208;
	mov.f32 	%f211, %f208;
	mov.f32 	%f212, %f208;
	@%p2 bra 	$L__BB84_9;

	not.b32 	%r27, %r3;
	add.s32 	%r5, %r27, %r11;
	mul.wide.u32 	%rd22, %r5, -1431655765;
	shr.u64 	%rd23, %rd22, 38;
	and.b64  	%rd24, %rd23, 1;
	setp.eq.b64 	%p3, %rd24, 1;
	mov.pred 	%p4, 0;
	xor.pred  	%p5, %p3, %p4;
	mov.f32 	%f208, 0f00000000;
	mov.u32 	%r254, %r3;
	@%p5 bra 	$L__BB84_5;

	shl.b64 	%rd25, %rd5, 1;
	add.s64 	%rd26, %rd66, %rd25;
	shl.b64 	%rd27, %rd3, 1;
	add.s64 	%rd28, %rd4, %rd27;
	mul.wide.s32 	%rd29, %r3, 4;
	add.s64 	%rd30, %rd28, %rd29;
	ld.global.nc.u32 	%r28, [%rd30];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r28;
  cvt.f32.f16 %f36, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r28;
  cvt.f32.f16 %f37, high;}

	// end inline asm
	add.s64 	%rd31, %rd26, %rd29;
	ld.global.nc.u32 	%r30, [%rd31];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r30;
  cvt.f32.f16 %f38, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r30;
  cvt.f32.f16 %f39, high;}

	// end inline asm
	fma.rn.f32 	%f48, %f36, %f38, 0f00000000;
	fma.rn.f32 	%f212, %f37, %f39, %f48;
	st.local.f32 	[%rd2], %f212;
	mul.wide.s32 	%rd32, %r12, 4;
	add.s64 	%rd33, %rd31, %rd32;
	ld.global.nc.u32 	%r32, [%rd33];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r32;
  cvt.f32.f16 %f40, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r32;
  cvt.f32.f16 %f41, high;}

	// end inline asm
	fma.rn.f32 	%f49, %f36, %f40, 0f00000000;
	fma.rn.f32 	%f211, %f37, %f41, %f49;
	st.local.f32 	[%rd2+4], %f211;
	add.s32 	%r40, %r3, %r12;
	add.s32 	%r41, %r40, %r12;
	shl.b32 	%r42, %r12, 1;
	mul.wide.s32 	%rd34, %r42, 4;
	add.s64 	%rd35, %rd31, %rd34;
	ld.global.nc.u32 	%r34, [%rd35];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r34;
  cvt.f32.f16 %f42, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r34;
  cvt.f32.f16 %f43, high;}

	// end inline asm
	fma.rn.f32 	%f50, %f36, %f42, 0f00000000;
	fma.rn.f32 	%f210, %f37, %f43, %f50;
	st.local.f32 	[%rd2+8], %f210;
	add.s32 	%r43, %r41, %r12;
	mul.wide.s32 	%rd36, %r43, 4;
	add.s64 	%rd37, %rd26, %rd36;
	ld.global.nc.u32 	%r36, [%rd37];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r36;
  cvt.f32.f16 %f44, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r36;
  cvt.f32.f16 %f45, high;}

	// end inline asm
	fma.rn.f32 	%f51, %f36, %f44, 0f00000000;
	fma.rn.f32 	%f209, %f37, %f45, %f51;
	st.local.f32 	[%rd2+12], %f209;
	add.s64 	%rd38, %rd35, %rd34;
	ld.global.nc.u32 	%r38, [%rd38];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r38;
  cvt.f32.f16 %f46, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r38;
  cvt.f32.f16 %f47, high;}

	// end inline asm
	fma.rn.f32 	%f52, %f36, %f46, 0f00000000;
	fma.rn.f32 	%f208, %f37, %f47, %f52;
	st.local.f32 	[%rd2+16], %f208;
	add.s32 	%r254, %r3, 96;

$L__BB84_5:
	setp.lt.u32 	%p6, %r5, 96;
	@%p6 bra 	$L__BB84_9;

	add.s32 	%r44, %r254, %r12;
	add.s32 	%r45, %r44, 96;
	mul.wide.s32 	%rd39, %r45, 4;
	shl.b64 	%rd40, %rd5, 1;
	add.s64 	%rd7, %rd39, %rd40;
	shl.b32 	%r46, %r12, 1;
	add.s32 	%r47, %r254, %r46;
	mad.lo.s32 	%r48, %r12, 3, %r254;
	shl.b32 	%r49, %r12, 2;
	add.s32 	%r50, %r254, %r49;
	mul.wide.s32 	%rd41, %r47, 4;
	add.s64 	%rd8, %rd41, %rd40;
	mul.wide.s32 	%rd42, %r48, 4;
	add.s64 	%rd9, %rd42, %rd40;
	mul.wide.s32 	%rd43, %r50, 4;
	add.s64 	%rd10, %rd43, %rd40;
	mul.wide.s32 	%rd44, %r254, 2;
	add.s64 	%rd45, %rd44, %rd3;
	shl.b64 	%rd46, %rd45, 1;
	add.s64 	%rd47, %rd4, %rd46;
	add.s64 	%rd65, %rd47, 384;
	mul.wide.s32 	%rd48, %r254, 4;
	mul.wide.s32 	%rd49, %r12, 4;
	add.s64 	%rd50, %rd48, %rd49;
	add.s64 	%rd12, %rd50, %rd40;
	add.s64 	%rd13, %rd48, %rd40;

$L__BB84_7:
	ld.global.nc.u32 	%r51, [%rd65+-384];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r51;
  cvt.f32.f16 %f53, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r51;
  cvt.f32.f16 %f54, high;}

	// end inline asm
	add.s64 	%rd51, %rd66, %rd13;
	ld.global.nc.u32 	%r53, [%rd51];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r53;
  cvt.f32.f16 %f55, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r53;
  cvt.f32.f16 %f56, high;}

	// end inline asm
	fma.rn.f32 	%f77, %f53, %f55, %f212;
	fma.rn.f32 	%f78, %f54, %f56, %f77;
	add.s64 	%rd52, %rd66, %rd12;
	ld.global.nc.u32 	%r55, [%rd52];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r55;
  cvt.f32.f16 %f57, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r55;
  cvt.f32.f16 %f58, high;}

	// end inline asm
	fma.rn.f32 	%f79, %f53, %f57, %f211;
	fma.rn.f32 	%f80, %f54, %f58, %f79;
	add.s64 	%rd53, %rd66, %rd8;
	ld.global.nc.u32 	%r57, [%rd53];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r57;
  cvt.f32.f16 %f59, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r57;
  cvt.f32.f16 %f60, high;}

	// end inline asm
	fma.rn.f32 	%f81, %f53, %f59, %f210;
	fma.rn.f32 	%f82, %f54, %f60, %f81;
	add.s64 	%rd54, %rd66, %rd9;
	ld.global.nc.u32 	%r59, [%rd54];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r59;
  cvt.f32.f16 %f61, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r59;
  cvt.f32.f16 %f62, high;}

	// end inline asm
	fma.rn.f32 	%f83, %f53, %f61, %f209;
	fma.rn.f32 	%f84, %f54, %f62, %f83;
	add.s64 	%rd55, %rd66, %rd10;
	ld.global.nc.u32 	%r61, [%rd55];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r61;
  cvt.f32.f16 %f63, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r61;
  cvt.f32.f16 %f64, high;}

	// end inline asm
	fma.rn.f32 	%f85, %f53, %f63, %f208;
	fma.rn.f32 	%f86, %f54, %f64, %f85;
	ld.global.nc.u32 	%r63, [%rd65];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r63;
  cvt.f32.f16 %f65, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r63;
  cvt.f32.f16 %f66, high;}

	// end inline asm
	ld.global.nc.u32 	%r65, [%rd51+384];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r65;
  cvt.f32.f16 %f67, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r65;
  cvt.f32.f16 %f68, high;}

	// end inline asm
	fma.rn.f32 	%f87, %f65, %f67, %f78;
	fma.rn.f32 	%f212, %f66, %f68, %f87;
	add.s64 	%rd56, %rd66, %rd7;
	ld.global.nc.u32 	%r67, [%rd56];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r67;
  cvt.f32.f16 %f69, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r67;
  cvt.f32.f16 %f70, high;}

	// end inline asm
	fma.rn.f32 	%f88, %f65, %f69, %f80;
	fma.rn.f32 	%f211, %f66, %f70, %f88;
	ld.global.nc.u32 	%r69, [%rd53+384];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r69;
  cvt.f32.f16 %f71, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r69;
  cvt.f32.f16 %f72, high;}

	// end inline asm
	fma.rn.f32 	%f89, %f65, %f71, %f82;
	fma.rn.f32 	%f210, %f66, %f72, %f89;
	ld.global.nc.u32 	%r71, [%rd54+384];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r71;
  cvt.f32.f16 %f73, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r71;
  cvt.f32.f16 %f74, high;}

	// end inline asm
	fma.rn.f32 	%f90, %f65, %f73, %f84;
	fma.rn.f32 	%f209, %f66, %f74, %f90;
	ld.global.nc.u32 	%r73, [%rd55+384];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r73;
  cvt.f32.f16 %f75, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r73;
  cvt.f32.f16 %f76, high;}

	// end inline asm
	fma.rn.f32 	%f91, %f65, %f75, %f86;
	fma.rn.f32 	%f208, %f66, %f76, %f91;
	add.s64 	%rd66, %rd66, 768;
	add.s64 	%rd65, %rd65, 768;
	add.s32 	%r254, %r254, 192;
	setp.lt.s32 	%p7, %r254, %r11;
	@%p7 bra 	$L__BB84_7;

	st.local.f32 	[%rd2], %f212;
	st.local.f32 	[%rd2+4], %f211;
	st.local.f32 	[%rd2+8], %f210;
	st.local.f32 	[%rd2+12], %f209;
	st.local.f32 	[%rd2+16], %f208;

$L__BB84_9:
	shr.s32 	%r75, %r3, 31;
	shr.u32 	%r76, %r75, 27;
	add.s32 	%r77, %r3, %r76;
	shr.s32 	%r78, %r77, 5;
	shl.b32 	%r79, %r78, 2;
	add.s32 	%r10, %r24, %r79;
	mov.u32 	%r81, 2;
	mov.b32 	%r82, %f212;
	mov.u32 	%r83, 31;
	mov.u32 	%r84, 16;
	mov.u32 	%r85, -1;
	shfl.sync.bfly.b32 	%r86|%p8, %r82, %r84, %r83, %r85;
	mov.b32 	%f92, %r86;
	add.f32 	%f93, %f212, %f92;
	mov.b32 	%r87, %f93;
	mov.u32 	%r88, 8;
	shfl.sync.bfly.b32 	%r89|%p9, %r87, %r88, %r83, %r85;
	mov.b32 	%f94, %r89;
	add.f32 	%f95, %f93, %f94;
	mov.b32 	%r90, %f95;
	mov.u32 	%r91, 4;
	shfl.sync.bfly.b32 	%r92|%p10, %r90, %r91, %r83, %r85;
	mov.b32 	%f96, %r92;
	add.f32 	%f97, %f95, %f96;
	mov.b32 	%r93, %f97;
	shfl.sync.bfly.b32 	%r94|%p11, %r93, %r81, %r83, %r85;
	mov.b32 	%f98, %r94;
	add.f32 	%f99, %f97, %f98;
	mov.b32 	%r95, %f99;
	mov.u32 	%r96, 1;
	shfl.sync.bfly.b32 	%r97|%p12, %r95, %r96, %r83, %r85;
	mov.b32 	%f100, %r97;
	add.f32 	%f101, %f99, %f100;
	st.local.f32 	[%rd2], %f101;
	st.shared.f32 	[%r10], %f101;
	bar.sync 	0;
	@%p1 bra 	$L__BB84_11;

	ld.shared.f32 	%f102, [%r4];
	mov.b32 	%r98, %f102;
	shfl.sync.bfly.b32 	%r102|%p14, %r98, %r84, %r83, %r85;
	mov.b32 	%f103, %r102;
	add.f32 	%f104, %f102, %f103;
	mov.b32 	%r103, %f104;
	shfl.sync.bfly.b32 	%r105|%p15, %r103, %r88, %r83, %r85;
	mov.b32 	%f105, %r105;
	add.f32 	%f106, %f104, %f105;
	mov.b32 	%r106, %f106;
	shfl.sync.bfly.b32 	%r108|%p16, %r106, %r91, %r83, %r85;
	mov.b32 	%f107, %r108;
	add.f32 	%f108, %f106, %f107;
	mov.b32 	%r109, %f108;
	shfl.sync.bfly.b32 	%r111|%p17, %r109, %r81, %r83, %r85;
	mov.b32 	%f109, %r111;
	add.f32 	%f110, %f108, %f109;
	mov.b32 	%r112, %f110;
	shfl.sync.bfly.b32 	%r114|%p18, %r112, %r96, %r83, %r85;
	mov.b32 	%f111, %r114;
	add.f32 	%f112, %f110, %f111;
	st.local.f32 	[%rd2], %f112;

$L__BB84_11:
	bar.sync 	0;
	mov.b32 	%r115, %f211;
	shfl.sync.bfly.b32 	%r119|%p20, %r115, %r84, %r83, %r85;
	mov.b32 	%f113, %r119;
	add.f32 	%f114, %f211, %f113;
	mov.b32 	%r120, %f114;
	shfl.sync.bfly.b32 	%r122|%p21, %r120, %r88, %r83, %r85;
	mov.b32 	%f115, %r122;
	add.f32 	%f116, %f114, %f115;
	mov.b32 	%r123, %f116;
	shfl.sync.bfly.b32 	%r125|%p22, %r123, %r91, %r83, %r85;
	mov.b32 	%f117, %r125;
	add.f32 	%f118, %f116, %f117;
	mov.b32 	%r126, %f118;
	shfl.sync.bfly.b32 	%r128|%p23, %r126, %r81, %r83, %r85;
	mov.b32 	%f119, %r128;
	add.f32 	%f120, %f118, %f119;
	mov.b32 	%r129, %f120;
	shfl.sync.bfly.b32 	%r131|%p24, %r129, %r96, %r83, %r85;
	mov.b32 	%f121, %r131;
	add.f32 	%f122, %f120, %f121;
	st.local.f32 	[%rd2+4], %f122;
	st.shared.f32 	[%r10], %f122;
	bar.sync 	0;
	@%p1 bra 	$L__BB84_13;

	ld.shared.f32 	%f123, [%r4];
	mov.b32 	%r132, %f123;
	mov.u32 	%r133, 31;
	mov.u32 	%r134, 16;
	mov.u32 	%r135, -1;
	shfl.sync.bfly.b32 	%r136|%p25, %r132, %r134, %r133, %r135;
	mov.b32 	%f124, %r136;
	add.f32 	%f125, %f123, %f124;
	mov.b32 	%r137, %f125;
	mov.u32 	%r138, 8;
	shfl.sync.bfly.b32 	%r139|%p26, %r137, %r138, %r133, %r135;
	mov.b32 	%f126, %r139;
	add.f32 	%f127, %f125, %f126;
	mov.b32 	%r140, %f127;
	mov.u32 	%r141, 4;
	shfl.sync.bfly.b32 	%r142|%p27, %r140, %r141, %r133, %r135;
	mov.b32 	%f128, %r142;
	add.f32 	%f129, %f127, %f128;
	mov.b32 	%r143, %f129;
	mov.u32 	%r144, 2;
	shfl.sync.bfly.b32 	%r145|%p28, %r143, %r144, %r133, %r135;
	mov.b32 	%f130, %r145;
	add.f32 	%f131, %f129, %f130;
	mov.b32 	%r146, %f131;
	mov.u32 	%r147, 1;
	shfl.sync.bfly.b32 	%r148|%p29, %r146, %r147, %r133, %r135;
	mov.b32 	%f132, %r148;
	add.f32 	%f133, %f131, %f132;
	st.local.f32 	[%rd2+4], %f133;

$L__BB84_13:
	bar.sync 	0;
	mov.b32 	%r149, %f210;
	mov.u32 	%r150, 31;
	mov.u32 	%r151, 16;
	mov.u32 	%r152, -1;
	shfl.sync.bfly.b32 	%r153|%p31, %r149, %r151, %r150, %r152;
	mov.b32 	%f134, %r153;
	add.f32 	%f135, %f210, %f134;
	mov.b32 	%r154, %f135;
	mov.u32 	%r155, 8;
	shfl.sync.bfly.b32 	%r156|%p32, %r154, %r155, %r150, %r152;
	mov.b32 	%f136, %r156;
	add.f32 	%f137, %f135, %f136;
	mov.b32 	%r157, %f137;
	mov.u32 	%r158, 4;
	shfl.sync.bfly.b32 	%r159|%p33, %r157, %r158, %r150, %r152;
	mov.b32 	%f138, %r159;
	add.f32 	%f139, %f137, %f138;
	mov.b32 	%r160, %f139;
	mov.u32 	%r161, 2;
	shfl.sync.bfly.b32 	%r162|%p34, %r160, %r161, %r150, %r152;
	mov.b32 	%f140, %r162;
	add.f32 	%f141, %f139, %f140;
	mov.b32 	%r163, %f141;
	mov.u32 	%r164, 1;
	shfl.sync.bfly.b32 	%r165|%p35, %r163, %r164, %r150, %r152;
	mov.b32 	%f142, %r165;
	add.f32 	%f143, %f141, %f142;
	st.local.f32 	[%rd2+8], %f143;
	st.shared.f32 	[%r10], %f143;
	bar.sync 	0;
	@%p1 bra 	$L__BB84_15;

	ld.shared.f32 	%f144, [%r4];
	mov.b32 	%r166, %f144;
	shfl.sync.bfly.b32 	%r170|%p36, %r166, %r151, %r150, %r152;
	mov.b32 	%f145, %r170;
	add.f32 	%f146, %f144, %f145;
	mov.b32 	%r171, %f146;
	shfl.sync.bfly.b32 	%r173|%p37, %r171, %r155, %r150, %r152;
	mov.b32 	%f147, %r173;
	add.f32 	%f148, %f146, %f147;
	mov.b32 	%r174, %f148;
	shfl.sync.bfly.b32 	%r176|%p38, %r174, %r158, %r150, %r152;
	mov.b32 	%f149, %r176;
	add.f32 	%f150, %f148, %f149;
	mov.b32 	%r177, %f150;
	shfl.sync.bfly.b32 	%r179|%p39, %r177, %r161, %r150, %r152;
	mov.b32 	%f151, %r179;
	add.f32 	%f152, %f150, %f151;
	mov.b32 	%r180, %f152;
	shfl.sync.bfly.b32 	%r182|%p40, %r180, %r164, %r150, %r152;
	mov.b32 	%f153, %r182;
	add.f32 	%f154, %f152, %f153;
	st.local.f32 	[%rd2+8], %f154;

$L__BB84_15:
	bar.sync 	0;
	mov.b32 	%r183, %f209;
	shfl.sync.bfly.b32 	%r187|%p42, %r183, %r151, %r150, %r152;
	mov.b32 	%f155, %r187;
	add.f32 	%f156, %f209, %f155;
	mov.b32 	%r188, %f156;
	shfl.sync.bfly.b32 	%r190|%p43, %r188, %r155, %r150, %r152;
	mov.b32 	%f157, %r190;
	add.f32 	%f158, %f156, %f157;
	mov.b32 	%r191, %f158;
	shfl.sync.bfly.b32 	%r193|%p44, %r191, %r158, %r150, %r152;
	mov.b32 	%f159, %r193;
	add.f32 	%f160, %f158, %f159;
	mov.b32 	%r194, %f160;
	shfl.sync.bfly.b32 	%r196|%p45, %r194, %r161, %r150, %r152;
	mov.b32 	%f161, %r196;
	add.f32 	%f162, %f160, %f161;
	mov.b32 	%r197, %f162;
	shfl.sync.bfly.b32 	%r199|%p46, %r197, %r164, %r150, %r152;
	mov.b32 	%f163, %r199;
	add.f32 	%f164, %f162, %f163;
	st.local.f32 	[%rd2+12], %f164;
	st.shared.f32 	[%r10], %f164;
	bar.sync 	0;
	@%p1 bra 	$L__BB84_17;

	ld.shared.f32 	%f165, [%r4];
	mov.b32 	%r200, %f165;
	mov.u32 	%r201, 31;
	mov.u32 	%r202, 16;
	mov.u32 	%r203, -1;
	shfl.sync.bfly.b32 	%r204|%p47, %r200, %r202, %r201, %r203;
	mov.b32 	%f166, %r204;
	add.f32 	%f167, %f165, %f166;
	mov.b32 	%r205, %f167;
	mov.u32 	%r206, 8;
	shfl.sync.bfly.b32 	%r207|%p48, %r205, %r206, %r201, %r203;
	mov.b32 	%f168, %r207;
	add.f32 	%f169, %f167, %f168;
	mov.b32 	%r208, %f169;
	mov.u32 	%r209, 4;
	shfl.sync.bfly.b32 	%r210|%p49, %r208, %r209, %r201, %r203;
	mov.b32 	%f170, %r210;
	add.f32 	%f171, %f169, %f170;
	mov.b32 	%r211, %f171;
	mov.u32 	%r212, 2;
	shfl.sync.bfly.b32 	%r213|%p50, %r211, %r212, %r201, %r203;
	mov.b32 	%f172, %r213;
	add.f32 	%f173, %f171, %f172;
	mov.b32 	%r214, %f173;
	mov.u32 	%r215, 1;
	shfl.sync.bfly.b32 	%r216|%p51, %r214, %r215, %r201, %r203;
	mov.b32 	%f174, %r216;
	add.f32 	%f175, %f173, %f174;
	st.local.f32 	[%rd2+12], %f175;

$L__BB84_17:
	bar.sync 	0;
	mov.b32 	%r217, %f208;
	mov.u32 	%r218, 31;
	mov.u32 	%r219, 16;
	mov.u32 	%r220, -1;
	shfl.sync.bfly.b32 	%r221|%p53, %r217, %r219, %r218, %r220;
	mov.b32 	%f176, %r221;
	add.f32 	%f177, %f208, %f176;
	mov.b32 	%r222, %f177;
	mov.u32 	%r223, 8;
	shfl.sync.bfly.b32 	%r224|%p54, %r222, %r223, %r218, %r220;
	mov.b32 	%f178, %r224;
	add.f32 	%f179, %f177, %f178;
	mov.b32 	%r225, %f179;
	mov.u32 	%r226, 4;
	shfl.sync.bfly.b32 	%r227|%p55, %r225, %r226, %r218, %r220;
	mov.b32 	%f180, %r227;
	add.f32 	%f181, %f179, %f180;
	mov.b32 	%r228, %f181;
	mov.u32 	%r229, 2;
	shfl.sync.bfly.b32 	%r230|%p56, %r228, %r229, %r218, %r220;
	mov.b32 	%f182, %r230;
	add.f32 	%f183, %f181, %f182;
	mov.b32 	%r231, %f183;
	mov.u32 	%r232, 1;
	shfl.sync.bfly.b32 	%r233|%p57, %r231, %r232, %r218, %r220;
	mov.b32 	%f184, %r233;
	add.f32 	%f185, %f183, %f184;
	st.local.f32 	[%rd2+16], %f185;
	st.shared.f32 	[%r10], %f185;
	bar.sync 	0;
	@%p1 bra 	$L__BB84_19;

	ld.shared.f32 	%f186, [%r4];
	mov.b32 	%r234, %f186;
	shfl.sync.bfly.b32 	%r238|%p58, %r234, %r219, %r218, %r220;
	mov.b32 	%f187, %r238;
	add.f32 	%f188, %f186, %f187;
	mov.b32 	%r239, %f188;
	shfl.sync.bfly.b32 	%r241|%p59, %r239, %r223, %r218, %r220;
	mov.b32 	%f189, %r241;
	add.f32 	%f190, %f188, %f189;
	mov.b32 	%r242, %f190;
	shfl.sync.bfly.b32 	%r244|%p60, %r242, %r226, %r218, %r220;
	mov.b32 	%f191, %r244;
	add.f32 	%f192, %f190, %f191;
	mov.b32 	%r245, %f192;
	shfl.sync.bfly.b32 	%r247|%p61, %r245, %r229, %r218, %r220;
	mov.b32 	%f193, %r247;
	add.f32 	%f194, %f192, %f193;
	mov.b32 	%r248, %f194;
	shfl.sync.bfly.b32 	%r250|%p62, %r248, %r232, %r218, %r220;
	mov.b32 	%f195, %r250;
	add.f32 	%f196, %f194, %f195;
	st.local.f32 	[%rd2+16], %f196;

$L__BB84_19:
	bar.sync 	0;
	setp.gt.s32 	%p63, %r3, 4;
	@%p63 bra 	$L__BB84_21;

	mad.lo.s32 	%r251, %r3, %r13, %r2;
	cvt.s64.s32 	%rd57, %r251;
	mul.lo.s32 	%r252, %r1, %r14;
	cvt.s64.s32 	%rd58, %r252;
	add.s64 	%rd59, %rd58, %rd57;
	mul.wide.s32 	%rd60, %r3, 4;
	add.s64 	%rd61, %rd2, %rd60;
	ld.local.f32 	%f197, [%rd61];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f197;}

	// end inline asm
	cvta.to.global.u64 	%rd62, %rd18;
	shl.b64 	%rd63, %rd59, 1;
	add.s64 	%rd64, %rd62, %rd63;
	st.global.u16 	[%rd64], %rs1;

$L__BB84_21:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_6_bs_96
.visible .entry ggml_matvec_f16_ncols_6_bs_96(
	.param .u64 ggml_matvec_f16_ncols_6_bs_96_param_0,
	.param .u64 ggml_matvec_f16_ncols_6_bs_96_param_1,
	.param .u64 ggml_matvec_f16_ncols_6_bs_96_param_2,
	.param .u32 ggml_matvec_f16_ncols_6_bs_96_param_3,
	.param .u32 ggml_matvec_f16_ncols_6_bs_96_param_4,
	.param .u32 ggml_matvec_f16_ncols_6_bs_96_param_5,
	.param .u32 ggml_matvec_f16_ncols_6_bs_96_param_6,
	.param .u32 ggml_matvec_f16_ncols_6_bs_96_param_7,
	.param .u32 ggml_matvec_f16_ncols_6_bs_96_param_8,
	.param .u32 ggml_matvec_f16_ncols_6_bs_96_param_9,
	.param .u32 ggml_matvec_f16_ncols_6_bs_96_param_10,
	.param .u32 ggml_matvec_f16_ncols_6_bs_96_param_11
)
{
	.local .align 8 .b8 	__local_depot85[24];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<75>;
	.reg .b16 	%rs<2>;
	.reg .f32 	%f<254>;
	.reg .b32 	%r<293>;
	.reg .b64 	%rd<70>;


	mov.u64 	%SPL, __local_depot85;
	ld.param.u64 	%rd20, [ggml_matvec_f16_ncols_6_bs_96_param_0];
	ld.param.u64 	%rd21, [ggml_matvec_f16_ncols_6_bs_96_param_1];
	ld.param.u64 	%rd19, [ggml_matvec_f16_ncols_6_bs_96_param_2];
	ld.param.u32 	%r11, [ggml_matvec_f16_ncols_6_bs_96_param_3];
	ld.param.u32 	%r15, [ggml_matvec_f16_ncols_6_bs_96_param_5];
	ld.param.u32 	%r12, [ggml_matvec_f16_ncols_6_bs_96_param_6];
	ld.param.u32 	%r13, [ggml_matvec_f16_ncols_6_bs_96_param_7];
	ld.param.u32 	%r16, [ggml_matvec_f16_ncols_6_bs_96_param_8];
	ld.param.u32 	%r17, [ggml_matvec_f16_ncols_6_bs_96_param_9];
	ld.param.u32 	%r18, [ggml_matvec_f16_ncols_6_bs_96_param_10];
	ld.param.u32 	%r14, [ggml_matvec_f16_ncols_6_bs_96_param_11];
	cvta.to.global.u64 	%rd69, %rd21;
	add.u64 	%rd2, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r19, %r1, %r16;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r20, %r2, %r15;
	mad.lo.s32 	%r21, %r19, %r17, %r20;
	cvt.s64.s32 	%rd3, %r21;
	cvta.to.global.u64 	%rd4, %rd20;
	mul.lo.s32 	%r22, %r1, %r18;
	cvt.s64.s32 	%rd5, %r22;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r23, %r3, 2;
	mov.u32 	%r24, data_mmv;
	add.s32 	%r4, %r24, %r23;
	@%p1 bra 	$L__BB85_2;

	mov.u32 	%r25, 0;
	st.shared.u32 	[%r4], %r25;

$L__BB85_2:
	bar.sync 	0;
	mov.f32 	%f248, 0f00000000;
	st.local.v2.f32 	[%rd2], {%f248, %f248};
	st.local.v2.f32 	[%rd2+8], {%f248, %f248};
	st.local.v2.f32 	[%rd2+16], {%f248, %f248};
	setp.ge.s32 	%p2, %r3, %r11;
	mov.f32 	%f249, %f248;
	mov.f32 	%f250, %f248;
	mov.f32 	%f251, %f248;
	mov.f32 	%f252, %f248;
	mov.f32 	%f253, %f248;
	@%p2 bra 	$L__BB85_9;

	not.b32 	%r26, %r3;
	add.s32 	%r5, %r26, %r11;
	mul.wide.u32 	%rd23, %r5, -1431655765;
	shr.u64 	%rd24, %rd23, 38;
	and.b64  	%rd25, %rd24, 1;
	setp.eq.b64 	%p3, %rd25, 1;
	mov.pred 	%p4, 0;
	xor.pred  	%p5, %p3, %p4;
	mov.f32 	%f248, 0f00000000;
	mov.u32 	%r292, %r3;
	@%p5 bra 	$L__BB85_5;

	shl.b64 	%rd26, %rd5, 1;
	add.s64 	%rd27, %rd69, %rd26;
	shl.b64 	%rd28, %rd3, 1;
	add.s64 	%rd29, %rd4, %rd28;
	mul.wide.s32 	%rd30, %r3, 4;
	add.s64 	%rd31, %rd29, %rd30;
	ld.global.nc.u32 	%r27, [%rd31];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r27;
  cvt.f32.f16 %f43, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r27;
  cvt.f32.f16 %f44, high;}

	// end inline asm
	add.s64 	%rd32, %rd27, %rd30;
	ld.global.nc.u32 	%r29, [%rd32];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r29;
  cvt.f32.f16 %f45, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r29;
  cvt.f32.f16 %f46, high;}

	// end inline asm
	fma.rn.f32 	%f57, %f43, %f45, 0f00000000;
	fma.rn.f32 	%f253, %f44, %f46, %f57;
	st.local.f32 	[%rd2], %f253;
	mul.wide.s32 	%rd33, %r12, 4;
	add.s64 	%rd34, %rd32, %rd33;
	ld.global.nc.u32 	%r31, [%rd34];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r31;
  cvt.f32.f16 %f47, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r31;
  cvt.f32.f16 %f48, high;}

	// end inline asm
	fma.rn.f32 	%f58, %f43, %f47, 0f00000000;
	fma.rn.f32 	%f252, %f44, %f48, %f58;
	st.local.f32 	[%rd2+4], %f252;
	add.s32 	%r41, %r3, %r12;
	add.s32 	%r42, %r41, %r12;
	mul.wide.s32 	%rd35, %r42, 4;
	add.s64 	%rd36, %rd27, %rd35;
	ld.global.nc.u32 	%r33, [%rd36];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r33;
  cvt.f32.f16 %f49, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r33;
  cvt.f32.f16 %f50, high;}

	// end inline asm
	fma.rn.f32 	%f59, %f43, %f49, 0f00000000;
	fma.rn.f32 	%f251, %f44, %f50, %f59;
	st.local.f32 	[%rd2+8], %f251;
	add.s64 	%rd37, %rd36, %rd33;
	ld.global.nc.u32 	%r35, [%rd37];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r35;
  cvt.f32.f16 %f51, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r35;
  cvt.f32.f16 %f52, high;}

	// end inline asm
	fma.rn.f32 	%f60, %f43, %f51, 0f00000000;
	fma.rn.f32 	%f250, %f44, %f52, %f60;
	st.local.f32 	[%rd2+12], %f250;
	add.s64 	%rd38, %rd37, %rd33;
	ld.global.nc.u32 	%r37, [%rd38];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r37;
  cvt.f32.f16 %f53, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r37;
  cvt.f32.f16 %f54, high;}

	// end inline asm
	fma.rn.f32 	%f61, %f43, %f53, 0f00000000;
	fma.rn.f32 	%f249, %f44, %f54, %f61;
	st.local.f32 	[%rd2+16], %f249;
	add.s64 	%rd39, %rd38, %rd33;
	ld.global.nc.u32 	%r39, [%rd39];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r39;
  cvt.f32.f16 %f55, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r39;
  cvt.f32.f16 %f56, high;}

	// end inline asm
	fma.rn.f32 	%f62, %f43, %f55, 0f00000000;
	fma.rn.f32 	%f248, %f44, %f56, %f62;
	st.local.f32 	[%rd2+20], %f248;
	add.s32 	%r292, %r3, 96;

$L__BB85_5:
	setp.lt.u32 	%p6, %r5, 96;
	@%p6 bra 	$L__BB85_9;

	add.s32 	%r43, %r292, %r12;
	add.s32 	%r44, %r43, 96;
	mul.wide.s32 	%rd40, %r44, 4;
	shl.b64 	%rd41, %rd5, 1;
	add.s64 	%rd7, %rd40, %rd41;
	shl.b32 	%r45, %r12, 1;
	add.s32 	%r46, %r292, %r45;
	mad.lo.s32 	%r47, %r12, 3, %r292;
	shl.b32 	%r48, %r12, 2;
	add.s32 	%r49, %r292, %r48;
	mad.lo.s32 	%r50, %r12, 5, %r292;
	mul.wide.s32 	%rd42, %r46, 4;
	add.s64 	%rd8, %rd42, %rd41;
	mul.wide.s32 	%rd43, %r47, 4;
	add.s64 	%rd9, %rd43, %rd41;
	mul.wide.s32 	%rd44, %r49, 4;
	add.s64 	%rd10, %rd44, %rd41;
	mul.wide.s32 	%rd45, %r50, 4;
	add.s64 	%rd11, %rd45, %rd41;
	mul.wide.s32 	%rd46, %r292, 2;
	add.s64 	%rd47, %rd46, %rd3;
	shl.b64 	%rd48, %rd47, 1;
	add.s64 	%rd49, %rd4, %rd48;
	add.s64 	%rd68, %rd49, 384;
	mul.wide.s32 	%rd50, %r292, 4;
	mul.wide.s32 	%rd51, %r12, 4;
	add.s64 	%rd52, %rd50, %rd51;
	add.s64 	%rd13, %rd52, %rd41;
	add.s64 	%rd14, %rd50, %rd41;

$L__BB85_7:
	ld.global.nc.u32 	%r51, [%rd68+-384];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r51;
  cvt.f32.f16 %f63, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r51;
  cvt.f32.f16 %f64, high;}

	// end inline asm
	add.s64 	%rd53, %rd69, %rd14;
	ld.global.nc.u32 	%r53, [%rd53];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r53;
  cvt.f32.f16 %f65, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r53;
  cvt.f32.f16 %f66, high;}

	// end inline asm
	fma.rn.f32 	%f91, %f63, %f65, %f253;
	fma.rn.f32 	%f92, %f64, %f66, %f91;
	add.s64 	%rd54, %rd69, %rd13;
	ld.global.nc.u32 	%r55, [%rd54];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r55;
  cvt.f32.f16 %f67, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r55;
  cvt.f32.f16 %f68, high;}

	// end inline asm
	fma.rn.f32 	%f93, %f63, %f67, %f252;
	fma.rn.f32 	%f94, %f64, %f68, %f93;
	add.s64 	%rd55, %rd69, %rd8;
	ld.global.nc.u32 	%r57, [%rd55];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r57;
  cvt.f32.f16 %f69, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r57;
  cvt.f32.f16 %f70, high;}

	// end inline asm
	fma.rn.f32 	%f95, %f63, %f69, %f251;
	fma.rn.f32 	%f96, %f64, %f70, %f95;
	add.s64 	%rd56, %rd69, %rd9;
	ld.global.nc.u32 	%r59, [%rd56];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r59;
  cvt.f32.f16 %f71, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r59;
  cvt.f32.f16 %f72, high;}

	// end inline asm
	fma.rn.f32 	%f97, %f63, %f71, %f250;
	fma.rn.f32 	%f98, %f64, %f72, %f97;
	add.s64 	%rd57, %rd69, %rd10;
	ld.global.nc.u32 	%r61, [%rd57];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r61;
  cvt.f32.f16 %f73, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r61;
  cvt.f32.f16 %f74, high;}

	// end inline asm
	fma.rn.f32 	%f99, %f63, %f73, %f249;
	fma.rn.f32 	%f100, %f64, %f74, %f99;
	add.s64 	%rd58, %rd69, %rd11;
	ld.global.nc.u32 	%r63, [%rd58];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r63;
  cvt.f32.f16 %f75, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r63;
  cvt.f32.f16 %f76, high;}

	// end inline asm
	fma.rn.f32 	%f101, %f63, %f75, %f248;
	fma.rn.f32 	%f102, %f64, %f76, %f101;
	ld.global.nc.u32 	%r65, [%rd68];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r65;
  cvt.f32.f16 %f77, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r65;
  cvt.f32.f16 %f78, high;}

	// end inline asm
	ld.global.nc.u32 	%r67, [%rd53+384];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r67;
  cvt.f32.f16 %f79, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r67;
  cvt.f32.f16 %f80, high;}

	// end inline asm
	fma.rn.f32 	%f103, %f77, %f79, %f92;
	fma.rn.f32 	%f253, %f78, %f80, %f103;
	add.s64 	%rd59, %rd69, %rd7;
	ld.global.nc.u32 	%r69, [%rd59];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r69;
  cvt.f32.f16 %f81, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r69;
  cvt.f32.f16 %f82, high;}

	// end inline asm
	fma.rn.f32 	%f104, %f77, %f81, %f94;
	fma.rn.f32 	%f252, %f78, %f82, %f104;
	ld.global.nc.u32 	%r71, [%rd55+384];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r71;
  cvt.f32.f16 %f83, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r71;
  cvt.f32.f16 %f84, high;}

	// end inline asm
	fma.rn.f32 	%f105, %f77, %f83, %f96;
	fma.rn.f32 	%f251, %f78, %f84, %f105;
	ld.global.nc.u32 	%r73, [%rd56+384];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r73;
  cvt.f32.f16 %f85, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r73;
  cvt.f32.f16 %f86, high;}

	// end inline asm
	fma.rn.f32 	%f106, %f77, %f85, %f98;
	fma.rn.f32 	%f250, %f78, %f86, %f106;
	ld.global.nc.u32 	%r75, [%rd57+384];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r75;
  cvt.f32.f16 %f87, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r75;
  cvt.f32.f16 %f88, high;}

	// end inline asm
	fma.rn.f32 	%f107, %f77, %f87, %f100;
	fma.rn.f32 	%f249, %f78, %f88, %f107;
	ld.global.nc.u32 	%r77, [%rd58+384];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r77;
  cvt.f32.f16 %f89, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r77;
  cvt.f32.f16 %f90, high;}

	// end inline asm
	fma.rn.f32 	%f108, %f77, %f89, %f102;
	fma.rn.f32 	%f248, %f78, %f90, %f108;
	add.s64 	%rd69, %rd69, 768;
	add.s64 	%rd68, %rd68, 768;
	add.s32 	%r292, %r292, 192;
	setp.lt.s32 	%p7, %r292, %r11;
	@%p7 bra 	$L__BB85_7;

	st.local.v2.f32 	[%rd2], {%f253, %f252};
	st.local.v2.f32 	[%rd2+8], {%f251, %f250};
	st.local.v2.f32 	[%rd2+16], {%f249, %f248};

$L__BB85_9:
	shr.s32 	%r79, %r3, 31;
	shr.u32 	%r80, %r79, 27;
	add.s32 	%r81, %r3, %r80;
	shr.s32 	%r82, %r81, 5;
	shl.b32 	%r83, %r82, 2;
	add.s32 	%r10, %r24, %r83;
	mov.u32 	%r85, 2;
	mov.b32 	%r86, %f253;
	mov.u32 	%r87, 31;
	mov.u32 	%r88, 16;
	mov.u32 	%r89, -1;
	shfl.sync.bfly.b32 	%r90|%p8, %r86, %r88, %r87, %r89;
	mov.b32 	%f109, %r90;
	add.f32 	%f110, %f253, %f109;
	mov.b32 	%r91, %f110;
	mov.u32 	%r92, 8;
	shfl.sync.bfly.b32 	%r93|%p9, %r91, %r92, %r87, %r89;
	mov.b32 	%f111, %r93;
	add.f32 	%f112, %f110, %f111;
	mov.b32 	%r94, %f112;
	mov.u32 	%r95, 4;
	shfl.sync.bfly.b32 	%r96|%p10, %r94, %r95, %r87, %r89;
	mov.b32 	%f113, %r96;
	add.f32 	%f114, %f112, %f113;
	mov.b32 	%r97, %f114;
	shfl.sync.bfly.b32 	%r98|%p11, %r97, %r85, %r87, %r89;
	mov.b32 	%f115, %r98;
	add.f32 	%f116, %f114, %f115;
	mov.b32 	%r99, %f116;
	mov.u32 	%r100, 1;
	shfl.sync.bfly.b32 	%r101|%p12, %r99, %r100, %r87, %r89;
	mov.b32 	%f117, %r101;
	add.f32 	%f118, %f116, %f117;
	st.local.f32 	[%rd2], %f118;
	st.shared.f32 	[%r10], %f118;
	bar.sync 	0;
	@%p1 bra 	$L__BB85_11;

	ld.shared.f32 	%f119, [%r4];
	mov.b32 	%r102, %f119;
	shfl.sync.bfly.b32 	%r106|%p14, %r102, %r88, %r87, %r89;
	mov.b32 	%f120, %r106;
	add.f32 	%f121, %f119, %f120;
	mov.b32 	%r107, %f121;
	shfl.sync.bfly.b32 	%r109|%p15, %r107, %r92, %r87, %r89;
	mov.b32 	%f122, %r109;
	add.f32 	%f123, %f121, %f122;
	mov.b32 	%r110, %f123;
	shfl.sync.bfly.b32 	%r112|%p16, %r110, %r95, %r87, %r89;
	mov.b32 	%f124, %r112;
	add.f32 	%f125, %f123, %f124;
	mov.b32 	%r113, %f125;
	shfl.sync.bfly.b32 	%r115|%p17, %r113, %r85, %r87, %r89;
	mov.b32 	%f126, %r115;
	add.f32 	%f127, %f125, %f126;
	mov.b32 	%r116, %f127;
	shfl.sync.bfly.b32 	%r118|%p18, %r116, %r100, %r87, %r89;
	mov.b32 	%f128, %r118;
	add.f32 	%f129, %f127, %f128;
	st.local.f32 	[%rd2], %f129;

$L__BB85_11:
	bar.sync 	0;
	mov.b32 	%r119, %f252;
	shfl.sync.bfly.b32 	%r123|%p20, %r119, %r88, %r87, %r89;
	mov.b32 	%f130, %r123;
	add.f32 	%f131, %f252, %f130;
	mov.b32 	%r124, %f131;
	shfl.sync.bfly.b32 	%r126|%p21, %r124, %r92, %r87, %r89;
	mov.b32 	%f132, %r126;
	add.f32 	%f133, %f131, %f132;
	mov.b32 	%r127, %f133;
	shfl.sync.bfly.b32 	%r129|%p22, %r127, %r95, %r87, %r89;
	mov.b32 	%f134, %r129;
	add.f32 	%f135, %f133, %f134;
	mov.b32 	%r130, %f135;
	shfl.sync.bfly.b32 	%r132|%p23, %r130, %r85, %r87, %r89;
	mov.b32 	%f136, %r132;
	add.f32 	%f137, %f135, %f136;
	mov.b32 	%r133, %f137;
	shfl.sync.bfly.b32 	%r135|%p24, %r133, %r100, %r87, %r89;
	mov.b32 	%f138, %r135;
	add.f32 	%f139, %f137, %f138;
	st.local.f32 	[%rd2+4], %f139;
	st.shared.f32 	[%r10], %f139;
	bar.sync 	0;
	@%p1 bra 	$L__BB85_13;

	ld.shared.f32 	%f140, [%r4];
	mov.b32 	%r136, %f140;
	mov.u32 	%r137, 31;
	mov.u32 	%r138, 16;
	mov.u32 	%r139, -1;
	shfl.sync.bfly.b32 	%r140|%p25, %r136, %r138, %r137, %r139;
	mov.b32 	%f141, %r140;
	add.f32 	%f142, %f140, %f141;
	mov.b32 	%r141, %f142;
	mov.u32 	%r142, 8;
	shfl.sync.bfly.b32 	%r143|%p26, %r141, %r142, %r137, %r139;
	mov.b32 	%f143, %r143;
	add.f32 	%f144, %f142, %f143;
	mov.b32 	%r144, %f144;
	mov.u32 	%r145, 4;
	shfl.sync.bfly.b32 	%r146|%p27, %r144, %r145, %r137, %r139;
	mov.b32 	%f145, %r146;
	add.f32 	%f146, %f144, %f145;
	mov.b32 	%r147, %f146;
	mov.u32 	%r148, 2;
	shfl.sync.bfly.b32 	%r149|%p28, %r147, %r148, %r137, %r139;
	mov.b32 	%f147, %r149;
	add.f32 	%f148, %f146, %f147;
	mov.b32 	%r150, %f148;
	mov.u32 	%r151, 1;
	shfl.sync.bfly.b32 	%r152|%p29, %r150, %r151, %r137, %r139;
	mov.b32 	%f149, %r152;
	add.f32 	%f150, %f148, %f149;
	st.local.f32 	[%rd2+4], %f150;

$L__BB85_13:
	bar.sync 	0;
	mov.b32 	%r153, %f251;
	mov.u32 	%r154, 31;
	mov.u32 	%r155, 16;
	mov.u32 	%r156, -1;
	shfl.sync.bfly.b32 	%r157|%p31, %r153, %r155, %r154, %r156;
	mov.b32 	%f151, %r157;
	add.f32 	%f152, %f251, %f151;
	mov.b32 	%r158, %f152;
	mov.u32 	%r159, 8;
	shfl.sync.bfly.b32 	%r160|%p32, %r158, %r159, %r154, %r156;
	mov.b32 	%f153, %r160;
	add.f32 	%f154, %f152, %f153;
	mov.b32 	%r161, %f154;
	mov.u32 	%r162, 4;
	shfl.sync.bfly.b32 	%r163|%p33, %r161, %r162, %r154, %r156;
	mov.b32 	%f155, %r163;
	add.f32 	%f156, %f154, %f155;
	mov.b32 	%r164, %f156;
	mov.u32 	%r165, 2;
	shfl.sync.bfly.b32 	%r166|%p34, %r164, %r165, %r154, %r156;
	mov.b32 	%f157, %r166;
	add.f32 	%f158, %f156, %f157;
	mov.b32 	%r167, %f158;
	mov.u32 	%r168, 1;
	shfl.sync.bfly.b32 	%r169|%p35, %r167, %r168, %r154, %r156;
	mov.b32 	%f159, %r169;
	add.f32 	%f160, %f158, %f159;
	st.local.f32 	[%rd2+8], %f160;
	st.shared.f32 	[%r10], %f160;
	bar.sync 	0;
	@%p1 bra 	$L__BB85_15;

	ld.shared.f32 	%f161, [%r4];
	mov.b32 	%r170, %f161;
	shfl.sync.bfly.b32 	%r174|%p36, %r170, %r155, %r154, %r156;
	mov.b32 	%f162, %r174;
	add.f32 	%f163, %f161, %f162;
	mov.b32 	%r175, %f163;
	shfl.sync.bfly.b32 	%r177|%p37, %r175, %r159, %r154, %r156;
	mov.b32 	%f164, %r177;
	add.f32 	%f165, %f163, %f164;
	mov.b32 	%r178, %f165;
	shfl.sync.bfly.b32 	%r180|%p38, %r178, %r162, %r154, %r156;
	mov.b32 	%f166, %r180;
	add.f32 	%f167, %f165, %f166;
	mov.b32 	%r181, %f167;
	shfl.sync.bfly.b32 	%r183|%p39, %r181, %r165, %r154, %r156;
	mov.b32 	%f168, %r183;
	add.f32 	%f169, %f167, %f168;
	mov.b32 	%r184, %f169;
	shfl.sync.bfly.b32 	%r186|%p40, %r184, %r168, %r154, %r156;
	mov.b32 	%f170, %r186;
	add.f32 	%f171, %f169, %f170;
	st.local.f32 	[%rd2+8], %f171;

$L__BB85_15:
	bar.sync 	0;
	mov.b32 	%r187, %f250;
	shfl.sync.bfly.b32 	%r191|%p42, %r187, %r155, %r154, %r156;
	mov.b32 	%f172, %r191;
	add.f32 	%f173, %f250, %f172;
	mov.b32 	%r192, %f173;
	shfl.sync.bfly.b32 	%r194|%p43, %r192, %r159, %r154, %r156;
	mov.b32 	%f174, %r194;
	add.f32 	%f175, %f173, %f174;
	mov.b32 	%r195, %f175;
	shfl.sync.bfly.b32 	%r197|%p44, %r195, %r162, %r154, %r156;
	mov.b32 	%f176, %r197;
	add.f32 	%f177, %f175, %f176;
	mov.b32 	%r198, %f177;
	shfl.sync.bfly.b32 	%r200|%p45, %r198, %r165, %r154, %r156;
	mov.b32 	%f178, %r200;
	add.f32 	%f179, %f177, %f178;
	mov.b32 	%r201, %f179;
	shfl.sync.bfly.b32 	%r203|%p46, %r201, %r168, %r154, %r156;
	mov.b32 	%f180, %r203;
	add.f32 	%f181, %f179, %f180;
	st.local.f32 	[%rd2+12], %f181;
	st.shared.f32 	[%r10], %f181;
	bar.sync 	0;
	@%p1 bra 	$L__BB85_17;

	ld.shared.f32 	%f182, [%r4];
	mov.b32 	%r204, %f182;
	mov.u32 	%r205, 31;
	mov.u32 	%r206, 16;
	mov.u32 	%r207, -1;
	shfl.sync.bfly.b32 	%r208|%p47, %r204, %r206, %r205, %r207;
	mov.b32 	%f183, %r208;
	add.f32 	%f184, %f182, %f183;
	mov.b32 	%r209, %f184;
	mov.u32 	%r210, 8;
	shfl.sync.bfly.b32 	%r211|%p48, %r209, %r210, %r205, %r207;
	mov.b32 	%f185, %r211;
	add.f32 	%f186, %f184, %f185;
	mov.b32 	%r212, %f186;
	mov.u32 	%r213, 4;
	shfl.sync.bfly.b32 	%r214|%p49, %r212, %r213, %r205, %r207;
	mov.b32 	%f187, %r214;
	add.f32 	%f188, %f186, %f187;
	mov.b32 	%r215, %f188;
	mov.u32 	%r216, 2;
	shfl.sync.bfly.b32 	%r217|%p50, %r215, %r216, %r205, %r207;
	mov.b32 	%f189, %r217;
	add.f32 	%f190, %f188, %f189;
	mov.b32 	%r218, %f190;
	mov.u32 	%r219, 1;
	shfl.sync.bfly.b32 	%r220|%p51, %r218, %r219, %r205, %r207;
	mov.b32 	%f191, %r220;
	add.f32 	%f192, %f190, %f191;
	st.local.f32 	[%rd2+12], %f192;

$L__BB85_17:
	bar.sync 	0;
	mov.b32 	%r221, %f249;
	mov.u32 	%r222, 31;
	mov.u32 	%r223, 16;
	mov.u32 	%r224, -1;
	shfl.sync.bfly.b32 	%r225|%p53, %r221, %r223, %r222, %r224;
	mov.b32 	%f193, %r225;
	add.f32 	%f194, %f249, %f193;
	mov.b32 	%r226, %f194;
	mov.u32 	%r227, 8;
	shfl.sync.bfly.b32 	%r228|%p54, %r226, %r227, %r222, %r224;
	mov.b32 	%f195, %r228;
	add.f32 	%f196, %f194, %f195;
	mov.b32 	%r229, %f196;
	mov.u32 	%r230, 4;
	shfl.sync.bfly.b32 	%r231|%p55, %r229, %r230, %r222, %r224;
	mov.b32 	%f197, %r231;
	add.f32 	%f198, %f196, %f197;
	mov.b32 	%r232, %f198;
	mov.u32 	%r233, 2;
	shfl.sync.bfly.b32 	%r234|%p56, %r232, %r233, %r222, %r224;
	mov.b32 	%f199, %r234;
	add.f32 	%f200, %f198, %f199;
	mov.b32 	%r235, %f200;
	mov.u32 	%r236, 1;
	shfl.sync.bfly.b32 	%r237|%p57, %r235, %r236, %r222, %r224;
	mov.b32 	%f201, %r237;
	add.f32 	%f202, %f200, %f201;
	st.local.f32 	[%rd2+16], %f202;
	st.shared.f32 	[%r10], %f202;
	bar.sync 	0;
	@%p1 bra 	$L__BB85_19;

	ld.shared.f32 	%f203, [%r4];
	mov.b32 	%r238, %f203;
	shfl.sync.bfly.b32 	%r242|%p58, %r238, %r223, %r222, %r224;
	mov.b32 	%f204, %r242;
	add.f32 	%f205, %f203, %f204;
	mov.b32 	%r243, %f205;
	shfl.sync.bfly.b32 	%r245|%p59, %r243, %r227, %r222, %r224;
	mov.b32 	%f206, %r245;
	add.f32 	%f207, %f205, %f206;
	mov.b32 	%r246, %f207;
	shfl.sync.bfly.b32 	%r248|%p60, %r246, %r230, %r222, %r224;
	mov.b32 	%f208, %r248;
	add.f32 	%f209, %f207, %f208;
	mov.b32 	%r249, %f209;
	shfl.sync.bfly.b32 	%r251|%p61, %r249, %r233, %r222, %r224;
	mov.b32 	%f210, %r251;
	add.f32 	%f211, %f209, %f210;
	mov.b32 	%r252, %f211;
	shfl.sync.bfly.b32 	%r254|%p62, %r252, %r236, %r222, %r224;
	mov.b32 	%f212, %r254;
	add.f32 	%f213, %f211, %f212;
	st.local.f32 	[%rd2+16], %f213;

$L__BB85_19:
	bar.sync 	0;
	mov.b32 	%r255, %f248;
	shfl.sync.bfly.b32 	%r259|%p64, %r255, %r223, %r222, %r224;
	mov.b32 	%f214, %r259;
	add.f32 	%f215, %f248, %f214;
	mov.b32 	%r260, %f215;
	shfl.sync.bfly.b32 	%r262|%p65, %r260, %r227, %r222, %r224;
	mov.b32 	%f216, %r262;
	add.f32 	%f217, %f215, %f216;
	mov.b32 	%r263, %f217;
	shfl.sync.bfly.b32 	%r265|%p66, %r263, %r230, %r222, %r224;
	mov.b32 	%f218, %r265;
	add.f32 	%f219, %f217, %f218;
	mov.b32 	%r266, %f219;
	shfl.sync.bfly.b32 	%r268|%p67, %r266, %r233, %r222, %r224;
	mov.b32 	%f220, %r268;
	add.f32 	%f221, %f219, %f220;
	mov.b32 	%r269, %f221;
	shfl.sync.bfly.b32 	%r271|%p68, %r269, %r236, %r222, %r224;
	mov.b32 	%f222, %r271;
	add.f32 	%f223, %f221, %f222;
	st.local.f32 	[%rd2+20], %f223;
	st.shared.f32 	[%r10], %f223;
	bar.sync 	0;
	@%p1 bra 	$L__BB85_21;

	ld.shared.f32 	%f224, [%r4];
	mov.b32 	%r272, %f224;
	mov.u32 	%r273, 31;
	mov.u32 	%r274, 16;
	mov.u32 	%r275, -1;
	shfl.sync.bfly.b32 	%r276|%p69, %r272, %r274, %r273, %r275;
	mov.b32 	%f225, %r276;
	add.f32 	%f226, %f224, %f225;
	mov.b32 	%r277, %f226;
	mov.u32 	%r278, 8;
	shfl.sync.bfly.b32 	%r279|%p70, %r277, %r278, %r273, %r275;
	mov.b32 	%f227, %r279;
	add.f32 	%f228, %f226, %f227;
	mov.b32 	%r280, %f228;
	mov.u32 	%r281, 4;
	shfl.sync.bfly.b32 	%r282|%p71, %r280, %r281, %r273, %r275;
	mov.b32 	%f229, %r282;
	add.f32 	%f230, %f228, %f229;
	mov.b32 	%r283, %f230;
	mov.u32 	%r284, 2;
	shfl.sync.bfly.b32 	%r285|%p72, %r283, %r284, %r273, %r275;
	mov.b32 	%f231, %r285;
	add.f32 	%f232, %f230, %f231;
	mov.b32 	%r286, %f232;
	mov.u32 	%r287, 1;
	shfl.sync.bfly.b32 	%r288|%p73, %r286, %r287, %r273, %r275;
	mov.b32 	%f233, %r288;
	add.f32 	%f234, %f232, %f233;
	st.local.f32 	[%rd2+20], %f234;

$L__BB85_21:
	bar.sync 	0;
	setp.gt.s32 	%p74, %r3, 5;
	@%p74 bra 	$L__BB85_23;

	mad.lo.s32 	%r289, %r3, %r13, %r2;
	cvt.s64.s32 	%rd60, %r289;
	mul.lo.s32 	%r290, %r1, %r14;
	cvt.s64.s32 	%rd61, %r290;
	add.s64 	%rd62, %rd61, %rd60;
	mul.wide.s32 	%rd63, %r3, 4;
	add.s64 	%rd64, %rd2, %rd63;
	ld.local.f32 	%f235, [%rd64];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f235;}

	// end inline asm
	cvta.to.global.u64 	%rd65, %rd19;
	shl.b64 	%rd66, %rd62, 1;
	add.s64 	%rd67, %rd65, %rd66;
	st.global.u16 	[%rd67], %rs1;

$L__BB85_23:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_7_bs_96
.visible .entry ggml_matvec_f16_ncols_7_bs_96(
	.param .u64 ggml_matvec_f16_ncols_7_bs_96_param_0,
	.param .u64 ggml_matvec_f16_ncols_7_bs_96_param_1,
	.param .u64 ggml_matvec_f16_ncols_7_bs_96_param_2,
	.param .u32 ggml_matvec_f16_ncols_7_bs_96_param_3,
	.param .u32 ggml_matvec_f16_ncols_7_bs_96_param_4,
	.param .u32 ggml_matvec_f16_ncols_7_bs_96_param_5,
	.param .u32 ggml_matvec_f16_ncols_7_bs_96_param_6,
	.param .u32 ggml_matvec_f16_ncols_7_bs_96_param_7,
	.param .u32 ggml_matvec_f16_ncols_7_bs_96_param_8,
	.param .u32 ggml_matvec_f16_ncols_7_bs_96_param_9,
	.param .u32 ggml_matvec_f16_ncols_7_bs_96_param_10,
	.param .u32 ggml_matvec_f16_ncols_7_bs_96_param_11
)
{
	.local .align 4 .b8 	__local_depot86[28];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<82>;
	.reg .b16 	%rs<2>;
	.reg .f32 	%f<221>;
	.reg .b32 	%r<289>;
	.reg .b64 	%rd<43>;


	mov.u64 	%SPL, __local_depot86;
	ld.param.u64 	%rd13, [ggml_matvec_f16_ncols_7_bs_96_param_0];
	ld.param.u64 	%rd14, [ggml_matvec_f16_ncols_7_bs_96_param_1];
	ld.param.u64 	%rd15, [ggml_matvec_f16_ncols_7_bs_96_param_2];
	ld.param.u32 	%r8, [ggml_matvec_f16_ncols_7_bs_96_param_3];
	ld.param.u32 	%r9, [ggml_matvec_f16_ncols_7_bs_96_param_5];
	ld.param.u32 	%r10, [ggml_matvec_f16_ncols_7_bs_96_param_6];
	ld.param.u32 	%r11, [ggml_matvec_f16_ncols_7_bs_96_param_7];
	ld.param.u32 	%r12, [ggml_matvec_f16_ncols_7_bs_96_param_8];
	ld.param.u32 	%r13, [ggml_matvec_f16_ncols_7_bs_96_param_9];
	ld.param.u32 	%r14, [ggml_matvec_f16_ncols_7_bs_96_param_10];
	ld.param.u32 	%r15, [ggml_matvec_f16_ncols_7_bs_96_param_11];
	add.u64 	%rd1, %SPL, 0;
	mov.u32 	%r1, %ctaid.x;
	mov.u32 	%r2, %tid.x;
	setp.gt.s32 	%p1, %r2, 31;
	shl.b32 	%r16, %r2, 2;
	mov.u32 	%r17, data_mmv;
	add.s32 	%r3, %r17, %r16;
	@%p1 bra 	$L__BB86_2;

	mov.u32 	%r18, 0;
	st.shared.u32 	[%r3], %r18;

$L__BB86_2:
	mov.u32 	%r4, %ctaid.y;
	bar.sync 	0;
	mov.f32 	%f214, 0f00000000;
	mov.u32 	%r19, 0;
	st.local.u32 	[%rd1], %r19;
	st.local.u32 	[%rd1+4], %r19;
	st.local.u32 	[%rd1+8], %r19;
	st.local.u32 	[%rd1+12], %r19;
	st.local.u32 	[%rd1+16], %r19;
	st.local.u32 	[%rd1+20], %r19;
	st.local.u32 	[%rd1+24], %r19;
	setp.ge.s32 	%p2, %r2, %r8;
	mov.f32 	%f215, %f214;
	mov.f32 	%f216, %f214;
	mov.f32 	%f217, %f214;
	mov.f32 	%f218, %f214;
	mov.f32 	%f219, %f214;
	mov.f32 	%f220, %f214;
	@%p2 bra 	$L__BB86_6;

	shl.b32 	%r20, %r10, 1;
	add.s32 	%r21, %r2, %r20;
	mul.wide.s32 	%rd17, %r21, 4;
	mul.lo.s32 	%r22, %r4, %r14;
	mul.wide.s32 	%rd18, %r22, 2;
	add.s64 	%rd3, %rd17, %rd18;
	mul.wide.s32 	%rd19, %r2, 4;
	mul.wide.s32 	%rd4, %r10, 4;
	add.s64 	%rd20, %rd19, %rd4;
	add.s64 	%rd5, %rd20, %rd18;
	add.s64 	%rd6, %rd19, %rd18;
	mul.wide.s32 	%rd21, %r2, 2;
	div.s32 	%r23, %r4, %r12;
	mul.lo.s32 	%r24, %r1, %r9;
	mad.lo.s32 	%r25, %r23, %r13, %r24;
	cvt.s64.s32 	%rd22, %r25;
	add.s64 	%rd23, %rd21, %rd22;
	cvta.to.global.u64 	%rd24, %rd13;
	shl.b64 	%rd25, %rd23, 1;
	add.s64 	%rd41, %rd24, %rd25;
	cvta.to.global.u64 	%rd42, %rd14;
	mov.f32 	%f214, 0f00000000;
	mov.u32 	%r288, %r2;

$L__BB86_4:
	ld.global.nc.u32 	%r26, [%rd41];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r26;
  cvt.f32.f16 %f36, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r26;
  cvt.f32.f16 %f37, high;}

	// end inline asm
	add.s64 	%rd26, %rd42, %rd6;
	ld.global.nc.u32 	%r28, [%rd26];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r28;
  cvt.f32.f16 %f38, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r28;
  cvt.f32.f16 %f39, high;}

	// end inline asm
	fma.rn.f32 	%f52, %f36, %f38, %f220;
	fma.rn.f32 	%f220, %f37, %f39, %f52;
	add.s64 	%rd27, %rd42, %rd5;
	ld.global.nc.u32 	%r30, [%rd27];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r30;
  cvt.f32.f16 %f40, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r30;
  cvt.f32.f16 %f41, high;}

	// end inline asm
	fma.rn.f32 	%f53, %f36, %f40, %f219;
	fma.rn.f32 	%f219, %f37, %f41, %f53;
	add.s64 	%rd28, %rd42, %rd3;
	ld.global.nc.u32 	%r32, [%rd28];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r32;
  cvt.f32.f16 %f42, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r32;
  cvt.f32.f16 %f43, high;}

	// end inline asm
	fma.rn.f32 	%f54, %f36, %f42, %f218;
	fma.rn.f32 	%f218, %f37, %f43, %f54;
	add.s64 	%rd29, %rd28, %rd4;
	ld.global.nc.u32 	%r34, [%rd29];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r34;
  cvt.f32.f16 %f44, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r34;
  cvt.f32.f16 %f45, high;}

	// end inline asm
	fma.rn.f32 	%f55, %f36, %f44, %f217;
	fma.rn.f32 	%f217, %f37, %f45, %f55;
	add.s64 	%rd30, %rd29, %rd4;
	ld.global.nc.u32 	%r36, [%rd30];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r36;
  cvt.f32.f16 %f46, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r36;
  cvt.f32.f16 %f47, high;}

	// end inline asm
	fma.rn.f32 	%f56, %f36, %f46, %f216;
	fma.rn.f32 	%f216, %f37, %f47, %f56;
	add.s64 	%rd31, %rd30, %rd4;
	ld.global.nc.u32 	%r38, [%rd31];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r38;
  cvt.f32.f16 %f48, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r38;
  cvt.f32.f16 %f49, high;}

	// end inline asm
	fma.rn.f32 	%f57, %f36, %f48, %f215;
	fma.rn.f32 	%f215, %f37, %f49, %f57;
	add.s64 	%rd32, %rd31, %rd4;
	ld.global.nc.u32 	%r40, [%rd32];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r40;
  cvt.f32.f16 %f50, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r40;
  cvt.f32.f16 %f51, high;}

	// end inline asm
	fma.rn.f32 	%f58, %f36, %f50, %f214;
	fma.rn.f32 	%f214, %f37, %f51, %f58;
	add.s64 	%rd42, %rd42, 384;
	add.s64 	%rd41, %rd41, 384;
	add.s32 	%r288, %r288, 96;
	setp.lt.s32 	%p3, %r288, %r8;
	@%p3 bra 	$L__BB86_4;

	st.local.f32 	[%rd1], %f220;
	st.local.f32 	[%rd1+4], %f219;
	st.local.f32 	[%rd1+8], %f218;
	st.local.f32 	[%rd1+12], %f217;
	st.local.f32 	[%rd1+16], %f216;
	st.local.f32 	[%rd1+20], %f215;
	st.local.f32 	[%rd1+24], %f214;

$L__BB86_6:
	shr.s32 	%r42, %r2, 31;
	shr.u32 	%r43, %r42, 27;
	add.s32 	%r44, %r2, %r43;
	shr.s32 	%r45, %r44, 5;
	shl.b32 	%r46, %r45, 2;
	add.s32 	%r7, %r17, %r46;
	mov.u32 	%r48, 2;
	mov.b32 	%r49, %f220;
	mov.u32 	%r50, 31;
	mov.u32 	%r51, 16;
	mov.u32 	%r52, -1;
	shfl.sync.bfly.b32 	%r53|%p4, %r49, %r51, %r50, %r52;
	mov.b32 	%f59, %r53;
	add.f32 	%f60, %f220, %f59;
	mov.b32 	%r54, %f60;
	mov.u32 	%r55, 8;
	shfl.sync.bfly.b32 	%r56|%p5, %r54, %r55, %r50, %r52;
	mov.b32 	%f61, %r56;
	add.f32 	%f62, %f60, %f61;
	mov.b32 	%r57, %f62;
	mov.u32 	%r58, 4;
	shfl.sync.bfly.b32 	%r59|%p6, %r57, %r58, %r50, %r52;
	mov.b32 	%f63, %r59;
	add.f32 	%f64, %f62, %f63;
	mov.b32 	%r60, %f64;
	shfl.sync.bfly.b32 	%r61|%p7, %r60, %r48, %r50, %r52;
	mov.b32 	%f65, %r61;
	add.f32 	%f66, %f64, %f65;
	mov.b32 	%r62, %f66;
	mov.u32 	%r63, 1;
	shfl.sync.bfly.b32 	%r64|%p8, %r62, %r63, %r50, %r52;
	mov.b32 	%f67, %r64;
	add.f32 	%f68, %f66, %f67;
	st.local.f32 	[%rd1], %f68;
	st.shared.f32 	[%r7], %f68;
	bar.sync 	0;
	@%p1 bra 	$L__BB86_8;

	ld.shared.f32 	%f69, [%r3];
	mov.b32 	%r65, %f69;
	shfl.sync.bfly.b32 	%r69|%p10, %r65, %r51, %r50, %r52;
	mov.b32 	%f70, %r69;
	add.f32 	%f71, %f69, %f70;
	mov.b32 	%r70, %f71;
	shfl.sync.bfly.b32 	%r72|%p11, %r70, %r55, %r50, %r52;
	mov.b32 	%f72, %r72;
	add.f32 	%f73, %f71, %f72;
	mov.b32 	%r73, %f73;
	shfl.sync.bfly.b32 	%r75|%p12, %r73, %r58, %r50, %r52;
	mov.b32 	%f74, %r75;
	add.f32 	%f75, %f73, %f74;
	mov.b32 	%r76, %f75;
	shfl.sync.bfly.b32 	%r78|%p13, %r76, %r48, %r50, %r52;
	mov.b32 	%f76, %r78;
	add.f32 	%f77, %f75, %f76;
	mov.b32 	%r79, %f77;
	shfl.sync.bfly.b32 	%r81|%p14, %r79, %r63, %r50, %r52;
	mov.b32 	%f78, %r81;
	add.f32 	%f79, %f77, %f78;
	st.local.f32 	[%rd1], %f79;

$L__BB86_8:
	bar.sync 	0;
	mov.b32 	%r82, %f219;
	shfl.sync.bfly.b32 	%r86|%p16, %r82, %r51, %r50, %r52;
	mov.b32 	%f80, %r86;
	add.f32 	%f81, %f219, %f80;
	mov.b32 	%r87, %f81;
	shfl.sync.bfly.b32 	%r89|%p17, %r87, %r55, %r50, %r52;
	mov.b32 	%f82, %r89;
	add.f32 	%f83, %f81, %f82;
	mov.b32 	%r90, %f83;
	shfl.sync.bfly.b32 	%r92|%p18, %r90, %r58, %r50, %r52;
	mov.b32 	%f84, %r92;
	add.f32 	%f85, %f83, %f84;
	mov.b32 	%r93, %f85;
	shfl.sync.bfly.b32 	%r95|%p19, %r93, %r48, %r50, %r52;
	mov.b32 	%f86, %r95;
	add.f32 	%f87, %f85, %f86;
	mov.b32 	%r96, %f87;
	shfl.sync.bfly.b32 	%r98|%p20, %r96, %r63, %r50, %r52;
	mov.b32 	%f88, %r98;
	add.f32 	%f89, %f87, %f88;
	st.local.f32 	[%rd1+4], %f89;
	st.shared.f32 	[%r7], %f89;
	bar.sync 	0;
	@%p1 bra 	$L__BB86_10;

	ld.shared.f32 	%f90, [%r3];
	mov.b32 	%r99, %f90;
	mov.u32 	%r100, 31;
	mov.u32 	%r101, 16;
	mov.u32 	%r102, -1;
	shfl.sync.bfly.b32 	%r103|%p21, %r99, %r101, %r100, %r102;
	mov.b32 	%f91, %r103;
	add.f32 	%f92, %f90, %f91;
	mov.b32 	%r104, %f92;
	mov.u32 	%r105, 8;
	shfl.sync.bfly.b32 	%r106|%p22, %r104, %r105, %r100, %r102;
	mov.b32 	%f93, %r106;
	add.f32 	%f94, %f92, %f93;
	mov.b32 	%r107, %f94;
	mov.u32 	%r108, 4;
	shfl.sync.bfly.b32 	%r109|%p23, %r107, %r108, %r100, %r102;
	mov.b32 	%f95, %r109;
	add.f32 	%f96, %f94, %f95;
	mov.b32 	%r110, %f96;
	mov.u32 	%r111, 2;
	shfl.sync.bfly.b32 	%r112|%p24, %r110, %r111, %r100, %r102;
	mov.b32 	%f97, %r112;
	add.f32 	%f98, %f96, %f97;
	mov.b32 	%r113, %f98;
	mov.u32 	%r114, 1;
	shfl.sync.bfly.b32 	%r115|%p25, %r113, %r114, %r100, %r102;
	mov.b32 	%f99, %r115;
	add.f32 	%f100, %f98, %f99;
	st.local.f32 	[%rd1+4], %f100;

$L__BB86_10:
	bar.sync 	0;
	mov.b32 	%r116, %f218;
	mov.u32 	%r117, 31;
	mov.u32 	%r118, 16;
	mov.u32 	%r119, -1;
	shfl.sync.bfly.b32 	%r120|%p27, %r116, %r118, %r117, %r119;
	mov.b32 	%f101, %r120;
	add.f32 	%f102, %f218, %f101;
	mov.b32 	%r121, %f102;
	mov.u32 	%r122, 8;
	shfl.sync.bfly.b32 	%r123|%p28, %r121, %r122, %r117, %r119;
	mov.b32 	%f103, %r123;
	add.f32 	%f104, %f102, %f103;
	mov.b32 	%r124, %f104;
	mov.u32 	%r125, 4;
	shfl.sync.bfly.b32 	%r126|%p29, %r124, %r125, %r117, %r119;
	mov.b32 	%f105, %r126;
	add.f32 	%f106, %f104, %f105;
	mov.b32 	%r127, %f106;
	mov.u32 	%r128, 2;
	shfl.sync.bfly.b32 	%r129|%p30, %r127, %r128, %r117, %r119;
	mov.b32 	%f107, %r129;
	add.f32 	%f108, %f106, %f107;
	mov.b32 	%r130, %f108;
	mov.u32 	%r131, 1;
	shfl.sync.bfly.b32 	%r132|%p31, %r130, %r131, %r117, %r119;
	mov.b32 	%f109, %r132;
	add.f32 	%f110, %f108, %f109;
	st.local.f32 	[%rd1+8], %f110;
	st.shared.f32 	[%r7], %f110;
	bar.sync 	0;
	@%p1 bra 	$L__BB86_12;

	ld.shared.f32 	%f111, [%r3];
	mov.b32 	%r133, %f111;
	shfl.sync.bfly.b32 	%r137|%p32, %r133, %r118, %r117, %r119;
	mov.b32 	%f112, %r137;
	add.f32 	%f113, %f111, %f112;
	mov.b32 	%r138, %f113;
	shfl.sync.bfly.b32 	%r140|%p33, %r138, %r122, %r117, %r119;
	mov.b32 	%f114, %r140;
	add.f32 	%f115, %f113, %f114;
	mov.b32 	%r141, %f115;
	shfl.sync.bfly.b32 	%r143|%p34, %r141, %r125, %r117, %r119;
	mov.b32 	%f116, %r143;
	add.f32 	%f117, %f115, %f116;
	mov.b32 	%r144, %f117;
	shfl.sync.bfly.b32 	%r146|%p35, %r144, %r128, %r117, %r119;
	mov.b32 	%f118, %r146;
	add.f32 	%f119, %f117, %f118;
	mov.b32 	%r147, %f119;
	shfl.sync.bfly.b32 	%r149|%p36, %r147, %r131, %r117, %r119;
	mov.b32 	%f120, %r149;
	add.f32 	%f121, %f119, %f120;
	st.local.f32 	[%rd1+8], %f121;

$L__BB86_12:
	bar.sync 	0;
	mov.b32 	%r150, %f217;
	shfl.sync.bfly.b32 	%r154|%p38, %r150, %r118, %r117, %r119;
	mov.b32 	%f122, %r154;
	add.f32 	%f123, %f217, %f122;
	mov.b32 	%r155, %f123;
	shfl.sync.bfly.b32 	%r157|%p39, %r155, %r122, %r117, %r119;
	mov.b32 	%f124, %r157;
	add.f32 	%f125, %f123, %f124;
	mov.b32 	%r158, %f125;
	shfl.sync.bfly.b32 	%r160|%p40, %r158, %r125, %r117, %r119;
	mov.b32 	%f126, %r160;
	add.f32 	%f127, %f125, %f126;
	mov.b32 	%r161, %f127;
	shfl.sync.bfly.b32 	%r163|%p41, %r161, %r128, %r117, %r119;
	mov.b32 	%f128, %r163;
	add.f32 	%f129, %f127, %f128;
	mov.b32 	%r164, %f129;
	shfl.sync.bfly.b32 	%r166|%p42, %r164, %r131, %r117, %r119;
	mov.b32 	%f130, %r166;
	add.f32 	%f131, %f129, %f130;
	st.local.f32 	[%rd1+12], %f131;
	st.shared.f32 	[%r7], %f131;
	bar.sync 	0;
	@%p1 bra 	$L__BB86_14;

	ld.shared.f32 	%f132, [%r3];
	mov.b32 	%r167, %f132;
	mov.u32 	%r168, 31;
	mov.u32 	%r169, 16;
	mov.u32 	%r170, -1;
	shfl.sync.bfly.b32 	%r171|%p43, %r167, %r169, %r168, %r170;
	mov.b32 	%f133, %r171;
	add.f32 	%f134, %f132, %f133;
	mov.b32 	%r172, %f134;
	mov.u32 	%r173, 8;
	shfl.sync.bfly.b32 	%r174|%p44, %r172, %r173, %r168, %r170;
	mov.b32 	%f135, %r174;
	add.f32 	%f136, %f134, %f135;
	mov.b32 	%r175, %f136;
	mov.u32 	%r176, 4;
	shfl.sync.bfly.b32 	%r177|%p45, %r175, %r176, %r168, %r170;
	mov.b32 	%f137, %r177;
	add.f32 	%f138, %f136, %f137;
	mov.b32 	%r178, %f138;
	mov.u32 	%r179, 2;
	shfl.sync.bfly.b32 	%r180|%p46, %r178, %r179, %r168, %r170;
	mov.b32 	%f139, %r180;
	add.f32 	%f140, %f138, %f139;
	mov.b32 	%r181, %f140;
	mov.u32 	%r182, 1;
	shfl.sync.bfly.b32 	%r183|%p47, %r181, %r182, %r168, %r170;
	mov.b32 	%f141, %r183;
	add.f32 	%f142, %f140, %f141;
	st.local.f32 	[%rd1+12], %f142;

$L__BB86_14:
	bar.sync 	0;
	mov.b32 	%r184, %f216;
	mov.u32 	%r185, 31;
	mov.u32 	%r186, 16;
	mov.u32 	%r187, -1;
	shfl.sync.bfly.b32 	%r188|%p49, %r184, %r186, %r185, %r187;
	mov.b32 	%f143, %r188;
	add.f32 	%f144, %f216, %f143;
	mov.b32 	%r189, %f144;
	mov.u32 	%r190, 8;
	shfl.sync.bfly.b32 	%r191|%p50, %r189, %r190, %r185, %r187;
	mov.b32 	%f145, %r191;
	add.f32 	%f146, %f144, %f145;
	mov.b32 	%r192, %f146;
	mov.u32 	%r193, 4;
	shfl.sync.bfly.b32 	%r194|%p51, %r192, %r193, %r185, %r187;
	mov.b32 	%f147, %r194;
	add.f32 	%f148, %f146, %f147;
	mov.b32 	%r195, %f148;
	mov.u32 	%r196, 2;
	shfl.sync.bfly.b32 	%r197|%p52, %r195, %r196, %r185, %r187;
	mov.b32 	%f149, %r197;
	add.f32 	%f150, %f148, %f149;
	mov.b32 	%r198, %f150;
	mov.u32 	%r199, 1;
	shfl.sync.bfly.b32 	%r200|%p53, %r198, %r199, %r185, %r187;
	mov.b32 	%f151, %r200;
	add.f32 	%f152, %f150, %f151;
	st.local.f32 	[%rd1+16], %f152;
	st.shared.f32 	[%r7], %f152;
	bar.sync 	0;
	@%p1 bra 	$L__BB86_16;

	ld.shared.f32 	%f153, [%r3];
	mov.b32 	%r201, %f153;
	shfl.sync.bfly.b32 	%r205|%p54, %r201, %r186, %r185, %r187;
	mov.b32 	%f154, %r205;
	add.f32 	%f155, %f153, %f154;
	mov.b32 	%r206, %f155;
	shfl.sync.bfly.b32 	%r208|%p55, %r206, %r190, %r185, %r187;
	mov.b32 	%f156, %r208;
	add.f32 	%f157, %f155, %f156;
	mov.b32 	%r209, %f157;
	shfl.sync.bfly.b32 	%r211|%p56, %r209, %r193, %r185, %r187;
	mov.b32 	%f158, %r211;
	add.f32 	%f159, %f157, %f158;
	mov.b32 	%r212, %f159;
	shfl.sync.bfly.b32 	%r214|%p57, %r212, %r196, %r185, %r187;
	mov.b32 	%f160, %r214;
	add.f32 	%f161, %f159, %f160;
	mov.b32 	%r215, %f161;
	shfl.sync.bfly.b32 	%r217|%p58, %r215, %r199, %r185, %r187;
	mov.b32 	%f162, %r217;
	add.f32 	%f163, %f161, %f162;
	st.local.f32 	[%rd1+16], %f163;

$L__BB86_16:
	bar.sync 	0;
	mov.b32 	%r218, %f215;
	shfl.sync.bfly.b32 	%r222|%p60, %r218, %r186, %r185, %r187;
	mov.b32 	%f164, %r222;
	add.f32 	%f165, %f215, %f164;
	mov.b32 	%r223, %f165;
	shfl.sync.bfly.b32 	%r225|%p61, %r223, %r190, %r185, %r187;
	mov.b32 	%f166, %r225;
	add.f32 	%f167, %f165, %f166;
	mov.b32 	%r226, %f167;
	shfl.sync.bfly.b32 	%r228|%p62, %r226, %r193, %r185, %r187;
	mov.b32 	%f168, %r228;
	add.f32 	%f169, %f167, %f168;
	mov.b32 	%r229, %f169;
	shfl.sync.bfly.b32 	%r231|%p63, %r229, %r196, %r185, %r187;
	mov.b32 	%f170, %r231;
	add.f32 	%f171, %f169, %f170;
	mov.b32 	%r232, %f171;
	shfl.sync.bfly.b32 	%r234|%p64, %r232, %r199, %r185, %r187;
	mov.b32 	%f172, %r234;
	add.f32 	%f173, %f171, %f172;
	st.local.f32 	[%rd1+20], %f173;
	st.shared.f32 	[%r7], %f173;
	bar.sync 	0;
	@%p1 bra 	$L__BB86_18;

	ld.shared.f32 	%f174, [%r3];
	mov.b32 	%r235, %f174;
	mov.u32 	%r236, 31;
	mov.u32 	%r237, 16;
	mov.u32 	%r238, -1;
	shfl.sync.bfly.b32 	%r239|%p65, %r235, %r237, %r236, %r238;
	mov.b32 	%f175, %r239;
	add.f32 	%f176, %f174, %f175;
	mov.b32 	%r240, %f176;
	mov.u32 	%r241, 8;
	shfl.sync.bfly.b32 	%r242|%p66, %r240, %r241, %r236, %r238;
	mov.b32 	%f177, %r242;
	add.f32 	%f178, %f176, %f177;
	mov.b32 	%r243, %f178;
	mov.u32 	%r244, 4;
	shfl.sync.bfly.b32 	%r245|%p67, %r243, %r244, %r236, %r238;
	mov.b32 	%f179, %r245;
	add.f32 	%f180, %f178, %f179;
	mov.b32 	%r246, %f180;
	mov.u32 	%r247, 2;
	shfl.sync.bfly.b32 	%r248|%p68, %r246, %r247, %r236, %r238;
	mov.b32 	%f181, %r248;
	add.f32 	%f182, %f180, %f181;
	mov.b32 	%r249, %f182;
	mov.u32 	%r250, 1;
	shfl.sync.bfly.b32 	%r251|%p69, %r249, %r250, %r236, %r238;
	mov.b32 	%f183, %r251;
	add.f32 	%f184, %f182, %f183;
	st.local.f32 	[%rd1+20], %f184;

$L__BB86_18:
	bar.sync 	0;
	mov.b32 	%r252, %f214;
	mov.u32 	%r253, 31;
	mov.u32 	%r254, 16;
	mov.u32 	%r255, -1;
	shfl.sync.bfly.b32 	%r256|%p71, %r252, %r254, %r253, %r255;
	mov.b32 	%f185, %r256;
	add.f32 	%f186, %f214, %f185;
	mov.b32 	%r257, %f186;
	mov.u32 	%r258, 8;
	shfl.sync.bfly.b32 	%r259|%p72, %r257, %r258, %r253, %r255;
	mov.b32 	%f187, %r259;
	add.f32 	%f188, %f186, %f187;
	mov.b32 	%r260, %f188;
	mov.u32 	%r261, 4;
	shfl.sync.bfly.b32 	%r262|%p73, %r260, %r261, %r253, %r255;
	mov.b32 	%f189, %r262;
	add.f32 	%f190, %f188, %f189;
	mov.b32 	%r263, %f190;
	mov.u32 	%r264, 2;
	shfl.sync.bfly.b32 	%r265|%p74, %r263, %r264, %r253, %r255;
	mov.b32 	%f191, %r265;
	add.f32 	%f192, %f190, %f191;
	mov.b32 	%r266, %f192;
	mov.u32 	%r267, 1;
	shfl.sync.bfly.b32 	%r268|%p75, %r266, %r267, %r253, %r255;
	mov.b32 	%f193, %r268;
	add.f32 	%f194, %f192, %f193;
	st.local.f32 	[%rd1+24], %f194;
	st.shared.f32 	[%r7], %f194;
	bar.sync 	0;
	@%p1 bra 	$L__BB86_20;

	ld.shared.f32 	%f195, [%r3];
	mov.b32 	%r269, %f195;
	shfl.sync.bfly.b32 	%r273|%p76, %r269, %r254, %r253, %r255;
	mov.b32 	%f196, %r273;
	add.f32 	%f197, %f195, %f196;
	mov.b32 	%r274, %f197;
	shfl.sync.bfly.b32 	%r276|%p77, %r274, %r258, %r253, %r255;
	mov.b32 	%f198, %r276;
	add.f32 	%f199, %f197, %f198;
	mov.b32 	%r277, %f199;
	shfl.sync.bfly.b32 	%r279|%p78, %r277, %r261, %r253, %r255;
	mov.b32 	%f200, %r279;
	add.f32 	%f201, %f199, %f200;
	mov.b32 	%r280, %f201;
	shfl.sync.bfly.b32 	%r282|%p79, %r280, %r264, %r253, %r255;
	mov.b32 	%f202, %r282;
	add.f32 	%f203, %f201, %f202;
	mov.b32 	%r283, %f203;
	shfl.sync.bfly.b32 	%r285|%p80, %r283, %r267, %r253, %r255;
	mov.b32 	%f204, %r285;
	add.f32 	%f205, %f203, %f204;
	st.local.f32 	[%rd1+24], %f205;

$L__BB86_20:
	bar.sync 	0;
	setp.gt.s32 	%p81, %r2, 6;
	@%p81 bra 	$L__BB86_22;

	mad.lo.s32 	%r286, %r2, %r11, %r1;
	cvt.s64.s32 	%rd33, %r286;
	mul.lo.s32 	%r287, %r4, %r15;
	cvt.s64.s32 	%rd34, %r287;
	add.s64 	%rd35, %rd34, %rd33;
	mul.wide.s32 	%rd36, %r2, 4;
	add.s64 	%rd37, %rd1, %rd36;
	ld.local.f32 	%f206, [%rd37];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f206;}

	// end inline asm
	cvta.to.global.u64 	%rd38, %rd15;
	shl.b64 	%rd39, %rd35, 1;
	add.s64 	%rd40, %rd38, %rd39;
	st.global.u16 	[%rd40], %rs1;

$L__BB86_22:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_8_bs_96
.visible .entry ggml_matvec_f16_ncols_8_bs_96(
	.param .u64 ggml_matvec_f16_ncols_8_bs_96_param_0,
	.param .u64 ggml_matvec_f16_ncols_8_bs_96_param_1,
	.param .u64 ggml_matvec_f16_ncols_8_bs_96_param_2,
	.param .u32 ggml_matvec_f16_ncols_8_bs_96_param_3,
	.param .u32 ggml_matvec_f16_ncols_8_bs_96_param_4,
	.param .u32 ggml_matvec_f16_ncols_8_bs_96_param_5,
	.param .u32 ggml_matvec_f16_ncols_8_bs_96_param_6,
	.param .u32 ggml_matvec_f16_ncols_8_bs_96_param_7,
	.param .u32 ggml_matvec_f16_ncols_8_bs_96_param_8,
	.param .u32 ggml_matvec_f16_ncols_8_bs_96_param_9,
	.param .u32 ggml_matvec_f16_ncols_8_bs_96_param_10,
	.param .u32 ggml_matvec_f16_ncols_8_bs_96_param_11
)
{
	.local .align 16 .b8 	__local_depot87[32];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<93>;
	.reg .b16 	%rs<2>;
	.reg .f32 	%f<252>;
	.reg .b32 	%r<324>;
	.reg .b64 	%rd<45>;


	mov.u64 	%SPL, __local_depot87;
	ld.param.u64 	%rd14, [ggml_matvec_f16_ncols_8_bs_96_param_0];
	ld.param.u64 	%rd15, [ggml_matvec_f16_ncols_8_bs_96_param_1];
	ld.param.u64 	%rd16, [ggml_matvec_f16_ncols_8_bs_96_param_2];
	ld.param.u32 	%r8, [ggml_matvec_f16_ncols_8_bs_96_param_3];
	ld.param.u32 	%r9, [ggml_matvec_f16_ncols_8_bs_96_param_5];
	ld.param.u32 	%r10, [ggml_matvec_f16_ncols_8_bs_96_param_6];
	ld.param.u32 	%r11, [ggml_matvec_f16_ncols_8_bs_96_param_7];
	ld.param.u32 	%r12, [ggml_matvec_f16_ncols_8_bs_96_param_8];
	ld.param.u32 	%r13, [ggml_matvec_f16_ncols_8_bs_96_param_9];
	ld.param.u32 	%r14, [ggml_matvec_f16_ncols_8_bs_96_param_10];
	ld.param.u32 	%r15, [ggml_matvec_f16_ncols_8_bs_96_param_11];
	add.u64 	%rd1, %SPL, 0;
	mov.u32 	%r1, %ctaid.x;
	mov.u32 	%r2, %tid.x;
	setp.gt.s32 	%p1, %r2, 31;
	shl.b32 	%r16, %r2, 2;
	mov.u32 	%r17, data_mmv;
	add.s32 	%r3, %r17, %r16;
	@%p1 bra 	$L__BB87_2;

	mov.u32 	%r18, 0;
	st.shared.u32 	[%r3], %r18;

$L__BB87_2:
	mov.u32 	%r4, %ctaid.y;
	bar.sync 	0;
	mov.f32 	%f244, 0f00000000;
	st.local.v4.f32 	[%rd1], {%f244, %f244, %f244, %f244};
	st.local.v4.f32 	[%rd1+16], {%f244, %f244, %f244, %f244};
	setp.ge.s32 	%p2, %r2, %r8;
	mov.f32 	%f245, %f244;
	mov.f32 	%f246, %f244;
	mov.f32 	%f247, %f244;
	mov.f32 	%f248, %f244;
	mov.f32 	%f249, %f244;
	mov.f32 	%f250, %f244;
	mov.f32 	%f251, %f244;
	@%p2 bra 	$L__BB87_6;

	shl.b32 	%r19, %r10, 1;
	add.s32 	%r20, %r2, %r19;
	mul.wide.s32 	%rd18, %r20, 4;
	mul.lo.s32 	%r21, %r4, %r14;
	mul.wide.s32 	%rd19, %r21, 2;
	add.s64 	%rd4, %rd18, %rd19;
	mul.wide.s32 	%rd20, %r2, 4;
	mul.wide.s32 	%rd5, %r10, 4;
	add.s64 	%rd21, %rd20, %rd5;
	add.s64 	%rd6, %rd21, %rd19;
	add.s64 	%rd7, %rd20, %rd19;
	mul.wide.s32 	%rd22, %r2, 2;
	div.s32 	%r22, %r4, %r12;
	mul.lo.s32 	%r23, %r1, %r9;
	mad.lo.s32 	%r24, %r22, %r13, %r23;
	cvt.s64.s32 	%rd23, %r24;
	add.s64 	%rd24, %rd22, %rd23;
	cvta.to.global.u64 	%rd25, %rd14;
	shl.b64 	%rd26, %rd24, 1;
	add.s64 	%rd43, %rd25, %rd26;
	cvta.to.global.u64 	%rd44, %rd15;
	mov.f32 	%f244, 0f00000000;
	mov.u32 	%r323, %r2;

$L__BB87_4:
	ld.global.nc.u32 	%r25, [%rd43];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r25;
  cvt.f32.f16 %f41, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r25;
  cvt.f32.f16 %f42, high;}

	// end inline asm
	add.s64 	%rd27, %rd44, %rd7;
	ld.global.nc.u32 	%r27, [%rd27];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r27;
  cvt.f32.f16 %f43, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r27;
  cvt.f32.f16 %f44, high;}

	// end inline asm
	fma.rn.f32 	%f59, %f41, %f43, %f251;
	fma.rn.f32 	%f251, %f42, %f44, %f59;
	add.s64 	%rd28, %rd44, %rd6;
	ld.global.nc.u32 	%r29, [%rd28];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r29;
  cvt.f32.f16 %f45, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r29;
  cvt.f32.f16 %f46, high;}

	// end inline asm
	fma.rn.f32 	%f60, %f41, %f45, %f250;
	fma.rn.f32 	%f250, %f42, %f46, %f60;
	add.s64 	%rd29, %rd44, %rd4;
	ld.global.nc.u32 	%r31, [%rd29];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r31;
  cvt.f32.f16 %f47, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r31;
  cvt.f32.f16 %f48, high;}

	// end inline asm
	fma.rn.f32 	%f61, %f41, %f47, %f249;
	fma.rn.f32 	%f249, %f42, %f48, %f61;
	add.s64 	%rd30, %rd29, %rd5;
	ld.global.nc.u32 	%r33, [%rd30];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r33;
  cvt.f32.f16 %f49, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r33;
  cvt.f32.f16 %f50, high;}

	// end inline asm
	fma.rn.f32 	%f62, %f41, %f49, %f248;
	fma.rn.f32 	%f248, %f42, %f50, %f62;
	add.s64 	%rd31, %rd30, %rd5;
	ld.global.nc.u32 	%r35, [%rd31];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r35;
  cvt.f32.f16 %f51, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r35;
  cvt.f32.f16 %f52, high;}

	// end inline asm
	fma.rn.f32 	%f63, %f41, %f51, %f247;
	fma.rn.f32 	%f247, %f42, %f52, %f63;
	add.s64 	%rd32, %rd31, %rd5;
	ld.global.nc.u32 	%r37, [%rd32];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r37;
  cvt.f32.f16 %f53, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r37;
  cvt.f32.f16 %f54, high;}

	// end inline asm
	fma.rn.f32 	%f64, %f41, %f53, %f246;
	fma.rn.f32 	%f246, %f42, %f54, %f64;
	add.s64 	%rd33, %rd32, %rd5;
	ld.global.nc.u32 	%r39, [%rd33];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r39;
  cvt.f32.f16 %f55, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r39;
  cvt.f32.f16 %f56, high;}

	// end inline asm
	fma.rn.f32 	%f65, %f41, %f55, %f245;
	fma.rn.f32 	%f245, %f42, %f56, %f65;
	add.s64 	%rd34, %rd33, %rd5;
	ld.global.nc.u32 	%r41, [%rd34];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r41;
  cvt.f32.f16 %f57, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r41;
  cvt.f32.f16 %f58, high;}

	// end inline asm
	fma.rn.f32 	%f66, %f41, %f57, %f244;
	fma.rn.f32 	%f244, %f42, %f58, %f66;
	add.s64 	%rd44, %rd44, 384;
	add.s64 	%rd43, %rd43, 384;
	add.s32 	%r323, %r323, 96;
	setp.lt.s32 	%p3, %r323, %r8;
	@%p3 bra 	$L__BB87_4;

	st.local.v4.f32 	[%rd1], {%f251, %f250, %f249, %f248};
	st.local.v4.f32 	[%rd1+16], {%f247, %f246, %f245, %f244};

$L__BB87_6:
	shr.s32 	%r43, %r2, 31;
	shr.u32 	%r44, %r43, 27;
	add.s32 	%r45, %r2, %r44;
	shr.s32 	%r46, %r45, 5;
	shl.b32 	%r47, %r46, 2;
	add.s32 	%r7, %r17, %r47;
	mov.u32 	%r49, 2;
	mov.b32 	%r50, %f251;
	mov.u32 	%r51, 31;
	mov.u32 	%r52, 16;
	mov.u32 	%r53, -1;
	shfl.sync.bfly.b32 	%r54|%p4, %r50, %r52, %r51, %r53;
	mov.b32 	%f67, %r54;
	add.f32 	%f68, %f251, %f67;
	mov.b32 	%r55, %f68;
	mov.u32 	%r56, 8;
	shfl.sync.bfly.b32 	%r57|%p5, %r55, %r56, %r51, %r53;
	mov.b32 	%f69, %r57;
	add.f32 	%f70, %f68, %f69;
	mov.b32 	%r58, %f70;
	mov.u32 	%r59, 4;
	shfl.sync.bfly.b32 	%r60|%p6, %r58, %r59, %r51, %r53;
	mov.b32 	%f71, %r60;
	add.f32 	%f72, %f70, %f71;
	mov.b32 	%r61, %f72;
	shfl.sync.bfly.b32 	%r62|%p7, %r61, %r49, %r51, %r53;
	mov.b32 	%f73, %r62;
	add.f32 	%f74, %f72, %f73;
	mov.b32 	%r63, %f74;
	mov.u32 	%r64, 1;
	shfl.sync.bfly.b32 	%r65|%p8, %r63, %r64, %r51, %r53;
	mov.b32 	%f75, %r65;
	add.f32 	%f76, %f74, %f75;
	st.local.f32 	[%rd1], %f76;
	st.shared.f32 	[%r7], %f76;
	bar.sync 	0;
	@%p1 bra 	$L__BB87_8;

	ld.shared.f32 	%f77, [%r3];
	mov.b32 	%r66, %f77;
	shfl.sync.bfly.b32 	%r70|%p10, %r66, %r52, %r51, %r53;
	mov.b32 	%f78, %r70;
	add.f32 	%f79, %f77, %f78;
	mov.b32 	%r71, %f79;
	shfl.sync.bfly.b32 	%r73|%p11, %r71, %r56, %r51, %r53;
	mov.b32 	%f80, %r73;
	add.f32 	%f81, %f79, %f80;
	mov.b32 	%r74, %f81;
	shfl.sync.bfly.b32 	%r76|%p12, %r74, %r59, %r51, %r53;
	mov.b32 	%f82, %r76;
	add.f32 	%f83, %f81, %f82;
	mov.b32 	%r77, %f83;
	shfl.sync.bfly.b32 	%r79|%p13, %r77, %r49, %r51, %r53;
	mov.b32 	%f84, %r79;
	add.f32 	%f85, %f83, %f84;
	mov.b32 	%r80, %f85;
	shfl.sync.bfly.b32 	%r82|%p14, %r80, %r64, %r51, %r53;
	mov.b32 	%f86, %r82;
	add.f32 	%f87, %f85, %f86;
	st.local.f32 	[%rd1], %f87;

$L__BB87_8:
	bar.sync 	0;
	mov.b32 	%r83, %f250;
	shfl.sync.bfly.b32 	%r87|%p16, %r83, %r52, %r51, %r53;
	mov.b32 	%f88, %r87;
	add.f32 	%f89, %f250, %f88;
	mov.b32 	%r88, %f89;
	shfl.sync.bfly.b32 	%r90|%p17, %r88, %r56, %r51, %r53;
	mov.b32 	%f90, %r90;
	add.f32 	%f91, %f89, %f90;
	mov.b32 	%r91, %f91;
	shfl.sync.bfly.b32 	%r93|%p18, %r91, %r59, %r51, %r53;
	mov.b32 	%f92, %r93;
	add.f32 	%f93, %f91, %f92;
	mov.b32 	%r94, %f93;
	shfl.sync.bfly.b32 	%r96|%p19, %r94, %r49, %r51, %r53;
	mov.b32 	%f94, %r96;
	add.f32 	%f95, %f93, %f94;
	mov.b32 	%r97, %f95;
	shfl.sync.bfly.b32 	%r99|%p20, %r97, %r64, %r51, %r53;
	mov.b32 	%f96, %r99;
	add.f32 	%f97, %f95, %f96;
	st.local.f32 	[%rd1+4], %f97;
	st.shared.f32 	[%r7], %f97;
	bar.sync 	0;
	@%p1 bra 	$L__BB87_10;

	ld.shared.f32 	%f98, [%r3];
	mov.b32 	%r100, %f98;
	mov.u32 	%r101, 31;
	mov.u32 	%r102, 16;
	mov.u32 	%r103, -1;
	shfl.sync.bfly.b32 	%r104|%p21, %r100, %r102, %r101, %r103;
	mov.b32 	%f99, %r104;
	add.f32 	%f100, %f98, %f99;
	mov.b32 	%r105, %f100;
	mov.u32 	%r106, 8;
	shfl.sync.bfly.b32 	%r107|%p22, %r105, %r106, %r101, %r103;
	mov.b32 	%f101, %r107;
	add.f32 	%f102, %f100, %f101;
	mov.b32 	%r108, %f102;
	mov.u32 	%r109, 4;
	shfl.sync.bfly.b32 	%r110|%p23, %r108, %r109, %r101, %r103;
	mov.b32 	%f103, %r110;
	add.f32 	%f104, %f102, %f103;
	mov.b32 	%r111, %f104;
	mov.u32 	%r112, 2;
	shfl.sync.bfly.b32 	%r113|%p24, %r111, %r112, %r101, %r103;
	mov.b32 	%f105, %r113;
	add.f32 	%f106, %f104, %f105;
	mov.b32 	%r114, %f106;
	mov.u32 	%r115, 1;
	shfl.sync.bfly.b32 	%r116|%p25, %r114, %r115, %r101, %r103;
	mov.b32 	%f107, %r116;
	add.f32 	%f108, %f106, %f107;
	st.local.f32 	[%rd1+4], %f108;

$L__BB87_10:
	bar.sync 	0;
	mov.b32 	%r117, %f249;
	mov.u32 	%r118, 31;
	mov.u32 	%r119, 16;
	mov.u32 	%r120, -1;
	shfl.sync.bfly.b32 	%r121|%p27, %r117, %r119, %r118, %r120;
	mov.b32 	%f109, %r121;
	add.f32 	%f110, %f249, %f109;
	mov.b32 	%r122, %f110;
	mov.u32 	%r123, 8;
	shfl.sync.bfly.b32 	%r124|%p28, %r122, %r123, %r118, %r120;
	mov.b32 	%f111, %r124;
	add.f32 	%f112, %f110, %f111;
	mov.b32 	%r125, %f112;
	mov.u32 	%r126, 4;
	shfl.sync.bfly.b32 	%r127|%p29, %r125, %r126, %r118, %r120;
	mov.b32 	%f113, %r127;
	add.f32 	%f114, %f112, %f113;
	mov.b32 	%r128, %f114;
	mov.u32 	%r129, 2;
	shfl.sync.bfly.b32 	%r130|%p30, %r128, %r129, %r118, %r120;
	mov.b32 	%f115, %r130;
	add.f32 	%f116, %f114, %f115;
	mov.b32 	%r131, %f116;
	mov.u32 	%r132, 1;
	shfl.sync.bfly.b32 	%r133|%p31, %r131, %r132, %r118, %r120;
	mov.b32 	%f117, %r133;
	add.f32 	%f118, %f116, %f117;
	st.local.f32 	[%rd1+8], %f118;
	st.shared.f32 	[%r7], %f118;
	bar.sync 	0;
	@%p1 bra 	$L__BB87_12;

	ld.shared.f32 	%f119, [%r3];
	mov.b32 	%r134, %f119;
	shfl.sync.bfly.b32 	%r138|%p32, %r134, %r119, %r118, %r120;
	mov.b32 	%f120, %r138;
	add.f32 	%f121, %f119, %f120;
	mov.b32 	%r139, %f121;
	shfl.sync.bfly.b32 	%r141|%p33, %r139, %r123, %r118, %r120;
	mov.b32 	%f122, %r141;
	add.f32 	%f123, %f121, %f122;
	mov.b32 	%r142, %f123;
	shfl.sync.bfly.b32 	%r144|%p34, %r142, %r126, %r118, %r120;
	mov.b32 	%f124, %r144;
	add.f32 	%f125, %f123, %f124;
	mov.b32 	%r145, %f125;
	shfl.sync.bfly.b32 	%r147|%p35, %r145, %r129, %r118, %r120;
	mov.b32 	%f126, %r147;
	add.f32 	%f127, %f125, %f126;
	mov.b32 	%r148, %f127;
	shfl.sync.bfly.b32 	%r150|%p36, %r148, %r132, %r118, %r120;
	mov.b32 	%f128, %r150;
	add.f32 	%f129, %f127, %f128;
	st.local.f32 	[%rd1+8], %f129;

$L__BB87_12:
	bar.sync 	0;
	mov.b32 	%r151, %f248;
	shfl.sync.bfly.b32 	%r155|%p38, %r151, %r119, %r118, %r120;
	mov.b32 	%f130, %r155;
	add.f32 	%f131, %f248, %f130;
	mov.b32 	%r156, %f131;
	shfl.sync.bfly.b32 	%r158|%p39, %r156, %r123, %r118, %r120;
	mov.b32 	%f132, %r158;
	add.f32 	%f133, %f131, %f132;
	mov.b32 	%r159, %f133;
	shfl.sync.bfly.b32 	%r161|%p40, %r159, %r126, %r118, %r120;
	mov.b32 	%f134, %r161;
	add.f32 	%f135, %f133, %f134;
	mov.b32 	%r162, %f135;
	shfl.sync.bfly.b32 	%r164|%p41, %r162, %r129, %r118, %r120;
	mov.b32 	%f136, %r164;
	add.f32 	%f137, %f135, %f136;
	mov.b32 	%r165, %f137;
	shfl.sync.bfly.b32 	%r167|%p42, %r165, %r132, %r118, %r120;
	mov.b32 	%f138, %r167;
	add.f32 	%f139, %f137, %f138;
	st.local.f32 	[%rd1+12], %f139;
	st.shared.f32 	[%r7], %f139;
	bar.sync 	0;
	@%p1 bra 	$L__BB87_14;

	ld.shared.f32 	%f140, [%r3];
	mov.b32 	%r168, %f140;
	mov.u32 	%r169, 31;
	mov.u32 	%r170, 16;
	mov.u32 	%r171, -1;
	shfl.sync.bfly.b32 	%r172|%p43, %r168, %r170, %r169, %r171;
	mov.b32 	%f141, %r172;
	add.f32 	%f142, %f140, %f141;
	mov.b32 	%r173, %f142;
	mov.u32 	%r174, 8;
	shfl.sync.bfly.b32 	%r175|%p44, %r173, %r174, %r169, %r171;
	mov.b32 	%f143, %r175;
	add.f32 	%f144, %f142, %f143;
	mov.b32 	%r176, %f144;
	mov.u32 	%r177, 4;
	shfl.sync.bfly.b32 	%r178|%p45, %r176, %r177, %r169, %r171;
	mov.b32 	%f145, %r178;
	add.f32 	%f146, %f144, %f145;
	mov.b32 	%r179, %f146;
	mov.u32 	%r180, 2;
	shfl.sync.bfly.b32 	%r181|%p46, %r179, %r180, %r169, %r171;
	mov.b32 	%f147, %r181;
	add.f32 	%f148, %f146, %f147;
	mov.b32 	%r182, %f148;
	mov.u32 	%r183, 1;
	shfl.sync.bfly.b32 	%r184|%p47, %r182, %r183, %r169, %r171;
	mov.b32 	%f149, %r184;
	add.f32 	%f150, %f148, %f149;
	st.local.f32 	[%rd1+12], %f150;

$L__BB87_14:
	bar.sync 	0;
	mov.b32 	%r185, %f247;
	mov.u32 	%r186, 31;
	mov.u32 	%r187, 16;
	mov.u32 	%r188, -1;
	shfl.sync.bfly.b32 	%r189|%p49, %r185, %r187, %r186, %r188;
	mov.b32 	%f151, %r189;
	add.f32 	%f152, %f247, %f151;
	mov.b32 	%r190, %f152;
	mov.u32 	%r191, 8;
	shfl.sync.bfly.b32 	%r192|%p50, %r190, %r191, %r186, %r188;
	mov.b32 	%f153, %r192;
	add.f32 	%f154, %f152, %f153;
	mov.b32 	%r193, %f154;
	mov.u32 	%r194, 4;
	shfl.sync.bfly.b32 	%r195|%p51, %r193, %r194, %r186, %r188;
	mov.b32 	%f155, %r195;
	add.f32 	%f156, %f154, %f155;
	mov.b32 	%r196, %f156;
	mov.u32 	%r197, 2;
	shfl.sync.bfly.b32 	%r198|%p52, %r196, %r197, %r186, %r188;
	mov.b32 	%f157, %r198;
	add.f32 	%f158, %f156, %f157;
	mov.b32 	%r199, %f158;
	mov.u32 	%r200, 1;
	shfl.sync.bfly.b32 	%r201|%p53, %r199, %r200, %r186, %r188;
	mov.b32 	%f159, %r201;
	add.f32 	%f160, %f158, %f159;
	st.local.f32 	[%rd1+16], %f160;
	st.shared.f32 	[%r7], %f160;
	bar.sync 	0;
	@%p1 bra 	$L__BB87_16;

	ld.shared.f32 	%f161, [%r3];
	mov.b32 	%r202, %f161;
	shfl.sync.bfly.b32 	%r206|%p54, %r202, %r187, %r186, %r188;
	mov.b32 	%f162, %r206;
	add.f32 	%f163, %f161, %f162;
	mov.b32 	%r207, %f163;
	shfl.sync.bfly.b32 	%r209|%p55, %r207, %r191, %r186, %r188;
	mov.b32 	%f164, %r209;
	add.f32 	%f165, %f163, %f164;
	mov.b32 	%r210, %f165;
	shfl.sync.bfly.b32 	%r212|%p56, %r210, %r194, %r186, %r188;
	mov.b32 	%f166, %r212;
	add.f32 	%f167, %f165, %f166;
	mov.b32 	%r213, %f167;
	shfl.sync.bfly.b32 	%r215|%p57, %r213, %r197, %r186, %r188;
	mov.b32 	%f168, %r215;
	add.f32 	%f169, %f167, %f168;
	mov.b32 	%r216, %f169;
	shfl.sync.bfly.b32 	%r218|%p58, %r216, %r200, %r186, %r188;
	mov.b32 	%f170, %r218;
	add.f32 	%f171, %f169, %f170;
	st.local.f32 	[%rd1+16], %f171;

$L__BB87_16:
	bar.sync 	0;
	mov.b32 	%r219, %f246;
	shfl.sync.bfly.b32 	%r223|%p60, %r219, %r187, %r186, %r188;
	mov.b32 	%f172, %r223;
	add.f32 	%f173, %f246, %f172;
	mov.b32 	%r224, %f173;
	shfl.sync.bfly.b32 	%r226|%p61, %r224, %r191, %r186, %r188;
	mov.b32 	%f174, %r226;
	add.f32 	%f175, %f173, %f174;
	mov.b32 	%r227, %f175;
	shfl.sync.bfly.b32 	%r229|%p62, %r227, %r194, %r186, %r188;
	mov.b32 	%f176, %r229;
	add.f32 	%f177, %f175, %f176;
	mov.b32 	%r230, %f177;
	shfl.sync.bfly.b32 	%r232|%p63, %r230, %r197, %r186, %r188;
	mov.b32 	%f178, %r232;
	add.f32 	%f179, %f177, %f178;
	mov.b32 	%r233, %f179;
	shfl.sync.bfly.b32 	%r235|%p64, %r233, %r200, %r186, %r188;
	mov.b32 	%f180, %r235;
	add.f32 	%f181, %f179, %f180;
	st.local.f32 	[%rd1+20], %f181;
	st.shared.f32 	[%r7], %f181;
	bar.sync 	0;
	@%p1 bra 	$L__BB87_18;

	ld.shared.f32 	%f182, [%r3];
	mov.b32 	%r236, %f182;
	mov.u32 	%r237, 31;
	mov.u32 	%r238, 16;
	mov.u32 	%r239, -1;
	shfl.sync.bfly.b32 	%r240|%p65, %r236, %r238, %r237, %r239;
	mov.b32 	%f183, %r240;
	add.f32 	%f184, %f182, %f183;
	mov.b32 	%r241, %f184;
	mov.u32 	%r242, 8;
	shfl.sync.bfly.b32 	%r243|%p66, %r241, %r242, %r237, %r239;
	mov.b32 	%f185, %r243;
	add.f32 	%f186, %f184, %f185;
	mov.b32 	%r244, %f186;
	mov.u32 	%r245, 4;
	shfl.sync.bfly.b32 	%r246|%p67, %r244, %r245, %r237, %r239;
	mov.b32 	%f187, %r246;
	add.f32 	%f188, %f186, %f187;
	mov.b32 	%r247, %f188;
	mov.u32 	%r248, 2;
	shfl.sync.bfly.b32 	%r249|%p68, %r247, %r248, %r237, %r239;
	mov.b32 	%f189, %r249;
	add.f32 	%f190, %f188, %f189;
	mov.b32 	%r250, %f190;
	mov.u32 	%r251, 1;
	shfl.sync.bfly.b32 	%r252|%p69, %r250, %r251, %r237, %r239;
	mov.b32 	%f191, %r252;
	add.f32 	%f192, %f190, %f191;
	st.local.f32 	[%rd1+20], %f192;

$L__BB87_18:
	bar.sync 	0;
	mov.b32 	%r253, %f245;
	mov.u32 	%r254, 31;
	mov.u32 	%r255, 16;
	mov.u32 	%r256, -1;
	shfl.sync.bfly.b32 	%r257|%p71, %r253, %r255, %r254, %r256;
	mov.b32 	%f193, %r257;
	add.f32 	%f194, %f245, %f193;
	mov.b32 	%r258, %f194;
	mov.u32 	%r259, 8;
	shfl.sync.bfly.b32 	%r260|%p72, %r258, %r259, %r254, %r256;
	mov.b32 	%f195, %r260;
	add.f32 	%f196, %f194, %f195;
	mov.b32 	%r261, %f196;
	mov.u32 	%r262, 4;
	shfl.sync.bfly.b32 	%r263|%p73, %r261, %r262, %r254, %r256;
	mov.b32 	%f197, %r263;
	add.f32 	%f198, %f196, %f197;
	mov.b32 	%r264, %f198;
	mov.u32 	%r265, 2;
	shfl.sync.bfly.b32 	%r266|%p74, %r264, %r265, %r254, %r256;
	mov.b32 	%f199, %r266;
	add.f32 	%f200, %f198, %f199;
	mov.b32 	%r267, %f200;
	mov.u32 	%r268, 1;
	shfl.sync.bfly.b32 	%r269|%p75, %r267, %r268, %r254, %r256;
	mov.b32 	%f201, %r269;
	add.f32 	%f202, %f200, %f201;
	st.local.f32 	[%rd1+24], %f202;
	st.shared.f32 	[%r7], %f202;
	bar.sync 	0;
	@%p1 bra 	$L__BB87_20;

	ld.shared.f32 	%f203, [%r3];
	mov.b32 	%r270, %f203;
	shfl.sync.bfly.b32 	%r274|%p76, %r270, %r255, %r254, %r256;
	mov.b32 	%f204, %r274;
	add.f32 	%f205, %f203, %f204;
	mov.b32 	%r275, %f205;
	shfl.sync.bfly.b32 	%r277|%p77, %r275, %r259, %r254, %r256;
	mov.b32 	%f206, %r277;
	add.f32 	%f207, %f205, %f206;
	mov.b32 	%r278, %f207;
	shfl.sync.bfly.b32 	%r280|%p78, %r278, %r262, %r254, %r256;
	mov.b32 	%f208, %r280;
	add.f32 	%f209, %f207, %f208;
	mov.b32 	%r281, %f209;
	shfl.sync.bfly.b32 	%r283|%p79, %r281, %r265, %r254, %r256;
	mov.b32 	%f210, %r283;
	add.f32 	%f211, %f209, %f210;
	mov.b32 	%r284, %f211;
	shfl.sync.bfly.b32 	%r286|%p80, %r284, %r268, %r254, %r256;
	mov.b32 	%f212, %r286;
	add.f32 	%f213, %f211, %f212;
	st.local.f32 	[%rd1+24], %f213;

$L__BB87_20:
	bar.sync 	0;
	mov.b32 	%r287, %f244;
	shfl.sync.bfly.b32 	%r291|%p82, %r287, %r255, %r254, %r256;
	mov.b32 	%f214, %r291;
	add.f32 	%f215, %f244, %f214;
	mov.b32 	%r292, %f215;
	shfl.sync.bfly.b32 	%r294|%p83, %r292, %r259, %r254, %r256;
	mov.b32 	%f216, %r294;
	add.f32 	%f217, %f215, %f216;
	mov.b32 	%r295, %f217;
	shfl.sync.bfly.b32 	%r297|%p84, %r295, %r262, %r254, %r256;
	mov.b32 	%f218, %r297;
	add.f32 	%f219, %f217, %f218;
	mov.b32 	%r298, %f219;
	shfl.sync.bfly.b32 	%r300|%p85, %r298, %r265, %r254, %r256;
	mov.b32 	%f220, %r300;
	add.f32 	%f221, %f219, %f220;
	mov.b32 	%r301, %f221;
	shfl.sync.bfly.b32 	%r303|%p86, %r301, %r268, %r254, %r256;
	mov.b32 	%f222, %r303;
	add.f32 	%f223, %f221, %f222;
	st.local.f32 	[%rd1+28], %f223;
	st.shared.f32 	[%r7], %f223;
	bar.sync 	0;
	@%p1 bra 	$L__BB87_22;

	ld.shared.f32 	%f224, [%r3];
	mov.b32 	%r304, %f224;
	mov.u32 	%r305, 31;
	mov.u32 	%r306, 16;
	mov.u32 	%r307, -1;
	shfl.sync.bfly.b32 	%r308|%p87, %r304, %r306, %r305, %r307;
	mov.b32 	%f225, %r308;
	add.f32 	%f226, %f224, %f225;
	mov.b32 	%r309, %f226;
	mov.u32 	%r310, 8;
	shfl.sync.bfly.b32 	%r311|%p88, %r309, %r310, %r305, %r307;
	mov.b32 	%f227, %r311;
	add.f32 	%f228, %f226, %f227;
	mov.b32 	%r312, %f228;
	mov.u32 	%r313, 4;
	shfl.sync.bfly.b32 	%r314|%p89, %r312, %r313, %r305, %r307;
	mov.b32 	%f229, %r314;
	add.f32 	%f230, %f228, %f229;
	mov.b32 	%r315, %f230;
	mov.u32 	%r316, 2;
	shfl.sync.bfly.b32 	%r317|%p90, %r315, %r316, %r305, %r307;
	mov.b32 	%f231, %r317;
	add.f32 	%f232, %f230, %f231;
	mov.b32 	%r318, %f232;
	mov.u32 	%r319, 1;
	shfl.sync.bfly.b32 	%r320|%p91, %r318, %r319, %r305, %r307;
	mov.b32 	%f233, %r320;
	add.f32 	%f234, %f232, %f233;
	st.local.f32 	[%rd1+28], %f234;

$L__BB87_22:
	bar.sync 	0;
	setp.gt.s32 	%p92, %r2, 7;
	@%p92 bra 	$L__BB87_24;

	mad.lo.s32 	%r321, %r2, %r11, %r1;
	cvt.s64.s32 	%rd35, %r321;
	mul.lo.s32 	%r322, %r4, %r15;
	cvt.s64.s32 	%rd36, %r322;
	add.s64 	%rd37, %rd36, %rd35;
	mul.wide.s32 	%rd38, %r2, 4;
	add.s64 	%rd39, %rd1, %rd38;
	ld.local.f32 	%f235, [%rd39];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f235;}

	// end inline asm
	cvta.to.global.u64 	%rd40, %rd16;
	shl.b64 	%rd41, %rd37, 1;
	add.s64 	%rd42, %rd40, %rd41;
	st.global.u16 	[%rd42], %rs1;

$L__BB87_24:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_1_bs_128
.visible .entry ggml_matvec_f16_ncols_1_bs_128(
	.param .u64 ggml_matvec_f16_ncols_1_bs_128_param_0,
	.param .u64 ggml_matvec_f16_ncols_1_bs_128_param_1,
	.param .u64 ggml_matvec_f16_ncols_1_bs_128_param_2,
	.param .u32 ggml_matvec_f16_ncols_1_bs_128_param_3,
	.param .u32 ggml_matvec_f16_ncols_1_bs_128_param_4,
	.param .u32 ggml_matvec_f16_ncols_1_bs_128_param_5,
	.param .u32 ggml_matvec_f16_ncols_1_bs_128_param_6,
	.param .u32 ggml_matvec_f16_ncols_1_bs_128_param_7,
	.param .u32 ggml_matvec_f16_ncols_1_bs_128_param_8,
	.param .u32 ggml_matvec_f16_ncols_1_bs_128_param_9,
	.param .u32 ggml_matvec_f16_ncols_1_bs_128_param_10,
	.param .u32 ggml_matvec_f16_ncols_1_bs_128_param_11
)
{
	.reg .pred 	%p<19>;
	.reg .b16 	%rs<2>;
	.reg .f32 	%f<69>;
	.reg .b32 	%r<99>;
	.reg .b64 	%rd<42>;


	ld.param.u64 	%rd18, [ggml_matvec_f16_ncols_1_bs_128_param_0];
	ld.param.u64 	%rd19, [ggml_matvec_f16_ncols_1_bs_128_param_1];
	ld.param.u64 	%rd17, [ggml_matvec_f16_ncols_1_bs_128_param_2];
	ld.param.u32 	%r13, [ggml_matvec_f16_ncols_1_bs_128_param_3];
	ld.param.u32 	%r16, [ggml_matvec_f16_ncols_1_bs_128_param_5];
	ld.param.u32 	%r14, [ggml_matvec_f16_ncols_1_bs_128_param_7];
	ld.param.u32 	%r17, [ggml_matvec_f16_ncols_1_bs_128_param_8];
	ld.param.u32 	%r18, [ggml_matvec_f16_ncols_1_bs_128_param_9];
	ld.param.u32 	%r19, [ggml_matvec_f16_ncols_1_bs_128_param_10];
	ld.param.u32 	%r15, [ggml_matvec_f16_ncols_1_bs_128_param_11];
	cvta.to.global.u64 	%rd1, %rd19;
	cvta.to.global.u64 	%rd2, %rd18;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r20, %r1, %r17;
	mov.u32 	%r21, %ctaid.x;
	mul.lo.s32 	%r22, %r21, %r16;
	mad.lo.s32 	%r23, %r20, %r18, %r22;
	cvt.s64.s32 	%rd3, %r23;
	mul.lo.s32 	%r24, %r1, %r19;
	cvt.s64.s32 	%rd4, %r24;
	mov.u32 	%r2, %tid.x;
	setp.gt.s32 	%p1, %r2, 31;
	shl.b32 	%r25, %r2, 2;
	mov.u32 	%r26, data_mmv;
	add.s32 	%r3, %r26, %r25;
	@%p1 bra 	$L__BB88_2;

	mov.u32 	%r27, 0;
	st.shared.u32 	[%r3], %r27;

$L__BB88_2:
	bar.sync 	0;
	setp.ge.s32 	%p2, %r2, %r13;
	mov.f32 	%f67, 0f00000000;
	@%p2 bra 	$L__BB88_9;

	not.b32 	%r28, %r2;
	add.s32 	%r4, %r28, %r13;
	shr.u32 	%r29, %r4, 7;
	add.s32 	%r30, %r29, 1;
	and.b32  	%r96, %r30, 3;
	setp.eq.s32 	%p3, %r96, 0;
	mov.f32 	%f67, 0f00000000;
	mov.u32 	%r97, %r2;
	@%p3 bra 	$L__BB88_6;

	mul.wide.s32 	%rd20, %r2, 2;
	add.s64 	%rd21, %rd20, %rd4;
	shl.b64 	%rd22, %rd21, 1;
	add.s64 	%rd39, %rd1, %rd22;
	add.s64 	%rd23, %rd20, %rd3;
	shl.b64 	%rd24, %rd23, 1;
	add.s64 	%rd38, %rd2, %rd24;
	mov.f32 	%f67, 0f00000000;
	mov.u32 	%r97, %r2;

$L__BB88_5:
	.pragma "nounroll";
	ld.global.nc.u32 	%r31, [%rd38];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r31;
  cvt.f32.f16 %f15, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r31;
  cvt.f32.f16 %f16, high;}

	// end inline asm
	ld.global.nc.u32 	%r33, [%rd39];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r33;
  cvt.f32.f16 %f17, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r33;
  cvt.f32.f16 %f18, high;}

	// end inline asm
	fma.rn.f32 	%f19, %f15, %f17, %f67;
	fma.rn.f32 	%f67, %f16, %f18, %f19;
	add.s32 	%r97, %r97, 128;
	add.s64 	%rd39, %rd39, 512;
	add.s64 	%rd38, %rd38, 512;
	add.s32 	%r96, %r96, -1;
	setp.ne.s32 	%p4, %r96, 0;
	@%p4 bra 	$L__BB88_5;

$L__BB88_6:
	setp.lt.u32 	%p5, %r4, 384;
	@%p5 bra 	$L__BB88_9;

	mul.wide.s32 	%rd25, %r97, 2;
	add.s64 	%rd26, %rd25, %rd3;
	shl.b64 	%rd27, %rd26, 1;
	add.s64 	%rd28, %rd2, %rd27;
	add.s64 	%rd41, %rd28, 1024;
	add.s64 	%rd29, %rd25, %rd4;
	shl.b64 	%rd30, %rd29, 1;
	add.s64 	%rd31, %rd1, %rd30;
	add.s64 	%rd40, %rd31, 1024;

$L__BB88_8:
	ld.global.nc.u32 	%r35, [%rd41+-1024];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r35;
  cvt.f32.f16 %f20, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r35;
  cvt.f32.f16 %f21, high;}

	// end inline asm
	ld.global.nc.u32 	%r37, [%rd40+-1024];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r37;
  cvt.f32.f16 %f22, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r37;
  cvt.f32.f16 %f23, high;}

	// end inline asm
	fma.rn.f32 	%f36, %f20, %f22, %f67;
	fma.rn.f32 	%f37, %f21, %f23, %f36;
	ld.global.nc.u32 	%r39, [%rd41+-512];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r39;
  cvt.f32.f16 %f24, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r39;
  cvt.f32.f16 %f25, high;}

	// end inline asm
	ld.global.nc.u32 	%r41, [%rd40+-512];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r41;
  cvt.f32.f16 %f26, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r41;
  cvt.f32.f16 %f27, high;}

	// end inline asm
	fma.rn.f32 	%f38, %f24, %f26, %f37;
	fma.rn.f32 	%f39, %f25, %f27, %f38;
	ld.global.nc.u32 	%r43, [%rd41];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r43;
  cvt.f32.f16 %f28, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r43;
  cvt.f32.f16 %f29, high;}

	// end inline asm
	ld.global.nc.u32 	%r45, [%rd40];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r45;
  cvt.f32.f16 %f30, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r45;
  cvt.f32.f16 %f31, high;}

	// end inline asm
	fma.rn.f32 	%f40, %f28, %f30, %f39;
	fma.rn.f32 	%f41, %f29, %f31, %f40;
	ld.global.nc.u32 	%r47, [%rd41+512];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r47;
  cvt.f32.f16 %f32, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r47;
  cvt.f32.f16 %f33, high;}

	// end inline asm
	ld.global.nc.u32 	%r49, [%rd40+512];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r49;
  cvt.f32.f16 %f34, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r49;
  cvt.f32.f16 %f35, high;}

	// end inline asm
	fma.rn.f32 	%f42, %f32, %f34, %f41;
	fma.rn.f32 	%f67, %f33, %f35, %f42;
	add.s64 	%rd41, %rd41, 2048;
	add.s64 	%rd40, %rd40, 2048;
	add.s32 	%r97, %r97, 512;
	setp.lt.s32 	%p6, %r97, %r13;
	@%p6 bra 	$L__BB88_8;

$L__BB88_9:
	mov.b32 	%r51, %f67;
	mov.u32 	%r52, 31;
	mov.u32 	%r53, 16;
	mov.u32 	%r54, -1;
	shfl.sync.bfly.b32 	%r55|%p7, %r51, %r53, %r52, %r54;
	mov.b32 	%f43, %r55;
	add.f32 	%f44, %f67, %f43;
	mov.b32 	%r56, %f44;
	mov.u32 	%r57, 8;
	shfl.sync.bfly.b32 	%r58|%p8, %r56, %r57, %r52, %r54;
	mov.b32 	%f45, %r58;
	add.f32 	%f46, %f44, %f45;
	mov.b32 	%r59, %f46;
	mov.u32 	%r60, 4;
	shfl.sync.bfly.b32 	%r61|%p9, %r59, %r60, %r52, %r54;
	mov.b32 	%f47, %r61;
	add.f32 	%f48, %f46, %f47;
	mov.b32 	%r62, %f48;
	mov.u32 	%r63, 2;
	shfl.sync.bfly.b32 	%r64|%p10, %r62, %r63, %r52, %r54;
	mov.b32 	%f49, %r64;
	add.f32 	%f50, %f48, %f49;
	mov.b32 	%r65, %f50;
	mov.u32 	%r66, 1;
	shfl.sync.bfly.b32 	%r67|%p11, %r65, %r66, %r52, %r54;
	mov.b32 	%f51, %r67;
	add.f32 	%f68, %f50, %f51;
	shr.s32 	%r68, %r2, 31;
	shr.u32 	%r69, %r68, 27;
	add.s32 	%r70, %r2, %r69;
	shr.s32 	%r71, %r70, 5;
	shl.b32 	%r72, %r71, 2;
	add.s32 	%r74, %r26, %r72;
	st.shared.f32 	[%r74], %f68;
	bar.sync 	0;
	@%p1 bra 	$L__BB88_11;

	ld.shared.f32 	%f52, [%r3];
	mov.b32 	%r75, %f52;
	shfl.sync.bfly.b32 	%r79|%p13, %r75, %r53, %r52, %r54;
	mov.b32 	%f53, %r79;
	add.f32 	%f54, %f52, %f53;
	mov.b32 	%r80, %f54;
	shfl.sync.bfly.b32 	%r82|%p14, %r80, %r57, %r52, %r54;
	mov.b32 	%f55, %r82;
	add.f32 	%f56, %f54, %f55;
	mov.b32 	%r83, %f56;
	shfl.sync.bfly.b32 	%r85|%p15, %r83, %r60, %r52, %r54;
	mov.b32 	%f57, %r85;
	add.f32 	%f58, %f56, %f57;
	mov.b32 	%r86, %f58;
	shfl.sync.bfly.b32 	%r88|%p16, %r86, %r63, %r52, %r54;
	mov.b32 	%f59, %r88;
	add.f32 	%f60, %f58, %f59;
	mov.b32 	%r89, %f60;
	shfl.sync.bfly.b32 	%r91|%p17, %r89, %r66, %r52, %r54;
	mov.b32 	%f61, %r91;
	add.f32 	%f68, %f60, %f61;

$L__BB88_11:
	bar.sync 	0;
	setp.gt.s32 	%p18, %r2, 0;
	@%p18 bra 	$L__BB88_13;

	mad.lo.s32 	%r93, %r2, %r14, %r21;
	cvt.s64.s32 	%rd32, %r93;
	mul.lo.s32 	%r94, %r1, %r15;
	cvt.s64.s32 	%rd33, %r94;
	add.s64 	%rd34, %rd33, %rd32;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f68;}

	// end inline asm
	cvta.to.global.u64 	%rd35, %rd17;
	shl.b64 	%rd36, %rd34, 1;
	add.s64 	%rd37, %rd35, %rd36;
	st.global.u16 	[%rd37], %rs1;

$L__BB88_13:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_2_bs_128
.visible .entry ggml_matvec_f16_ncols_2_bs_128(
	.param .u64 ggml_matvec_f16_ncols_2_bs_128_param_0,
	.param .u64 ggml_matvec_f16_ncols_2_bs_128_param_1,
	.param .u64 ggml_matvec_f16_ncols_2_bs_128_param_2,
	.param .u32 ggml_matvec_f16_ncols_2_bs_128_param_3,
	.param .u32 ggml_matvec_f16_ncols_2_bs_128_param_4,
	.param .u32 ggml_matvec_f16_ncols_2_bs_128_param_5,
	.param .u32 ggml_matvec_f16_ncols_2_bs_128_param_6,
	.param .u32 ggml_matvec_f16_ncols_2_bs_128_param_7,
	.param .u32 ggml_matvec_f16_ncols_2_bs_128_param_8,
	.param .u32 ggml_matvec_f16_ncols_2_bs_128_param_9,
	.param .u32 ggml_matvec_f16_ncols_2_bs_128_param_10,
	.param .u32 ggml_matvec_f16_ncols_2_bs_128_param_11
)
{
	.local .align 8 .b8 	__local_depot89[8];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<30>;
	.reg .b16 	%rs<2>;
	.reg .f32 	%f<116>;
	.reg .b32 	%r<143>;
	.reg .b64 	%rd<64>;


	mov.u64 	%SPL, __local_depot89;
	ld.param.u64 	%rd27, [ggml_matvec_f16_ncols_2_bs_128_param_0];
	ld.param.u64 	%rd28, [ggml_matvec_f16_ncols_2_bs_128_param_1];
	ld.param.u64 	%rd26, [ggml_matvec_f16_ncols_2_bs_128_param_2];
	ld.param.u32 	%r15, [ggml_matvec_f16_ncols_2_bs_128_param_3];
	ld.param.u32 	%r19, [ggml_matvec_f16_ncols_2_bs_128_param_5];
	ld.param.u32 	%r16, [ggml_matvec_f16_ncols_2_bs_128_param_6];
	ld.param.u32 	%r17, [ggml_matvec_f16_ncols_2_bs_128_param_7];
	ld.param.u32 	%r20, [ggml_matvec_f16_ncols_2_bs_128_param_8];
	ld.param.u32 	%r21, [ggml_matvec_f16_ncols_2_bs_128_param_9];
	ld.param.u32 	%r22, [ggml_matvec_f16_ncols_2_bs_128_param_10];
	ld.param.u32 	%r18, [ggml_matvec_f16_ncols_2_bs_128_param_11];
	cvta.to.global.u64 	%rd1, %rd28;
	cvta.to.global.u64 	%rd2, %rd27;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r23, %r1, %r20;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r24, %r2, %r19;
	mad.lo.s32 	%r25, %r23, %r21, %r24;
	cvt.s64.s32 	%rd4, %r25;
	mul.lo.s32 	%r26, %r1, %r22;
	cvt.s64.s32 	%rd5, %r26;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r27, %r3, 2;
	mov.u32 	%r28, data_mmv;
	add.s32 	%r4, %r28, %r27;
	@%p1 bra 	$L__BB89_2;

	mov.u32 	%r29, 0;
	st.shared.u32 	[%r4], %r29;

$L__BB89_2:
	bar.sync 	0;
	mov.f32 	%f114, 0f00000000;
	st.local.v2.f32 	[%rd3], {%f114, %f114};
	setp.ge.s32 	%p2, %r3, %r15;
	mov.f32 	%f115, %f114;
	@%p2 bra 	$L__BB89_11;

	not.b32 	%r30, %r3;
	add.s32 	%r5, %r30, %r15;
	shr.u32 	%r31, %r5, 7;
	add.s32 	%r32, %r31, 1;
	and.b32  	%r140, %r32, 3;
	setp.eq.s32 	%p3, %r140, 0;
	mov.f32 	%f114, 0f00000000;
	mov.u32 	%r141, %r3;
	@%p3 bra 	$L__BB89_7;

	mul.wide.s32 	%rd30, %r16, 2;
	mul.wide.s32 	%rd31, %r3, 2;
	add.s64 	%rd32, %rd30, %rd31;
	add.s64 	%rd33, %rd32, %rd5;
	shl.b64 	%rd34, %rd33, 1;
	add.s64 	%rd60, %rd1, %rd34;
	add.s64 	%rd35, %rd31, %rd5;
	shl.b64 	%rd36, %rd35, 1;
	add.s64 	%rd59, %rd1, %rd36;
	add.s64 	%rd37, %rd31, %rd4;
	shl.b64 	%rd38, %rd37, 1;
	add.s64 	%rd58, %rd2, %rd38;
	mov.f32 	%f114, 0f00000000;
	mov.f32 	%f115, %f114;
	mov.u32 	%r141, %r3;

$L__BB89_5:
	.pragma "nounroll";
	ld.global.nc.u32 	%r33, [%rd58];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r33;
  cvt.f32.f16 %f19, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r33;
  cvt.f32.f16 %f20, high;}

	// end inline asm
	ld.global.nc.u32 	%r35, [%rd59];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r35;
  cvt.f32.f16 %f21, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r35;
  cvt.f32.f16 %f22, high;}

	// end inline asm
	fma.rn.f32 	%f25, %f19, %f21, %f115;
	fma.rn.f32 	%f115, %f20, %f22, %f25;
	ld.global.nc.u32 	%r37, [%rd60];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r37;
  cvt.f32.f16 %f23, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r37;
  cvt.f32.f16 %f24, high;}

	// end inline asm
	fma.rn.f32 	%f26, %f19, %f23, %f114;
	fma.rn.f32 	%f114, %f20, %f24, %f26;
	add.s32 	%r141, %r141, 128;
	add.s64 	%rd60, %rd60, 512;
	add.s64 	%rd59, %rd59, 512;
	add.s64 	%rd58, %rd58, 512;
	add.s32 	%r140, %r140, -1;
	setp.ne.s32 	%p4, %r140, 0;
	@%p4 bra 	$L__BB89_5;

	st.local.v2.f32 	[%rd3], {%f115, %f114};

$L__BB89_7:
	setp.lt.u32 	%p5, %r5, 384;
	@%p5 bra 	$L__BB89_11;

	mul.wide.s32 	%rd39, %r141, 2;
	add.s64 	%rd40, %rd39, %rd4;
	shl.b64 	%rd41, %rd40, 1;
	add.s64 	%rd42, %rd2, %rd41;
	add.s64 	%rd63, %rd42, 1024;
	add.s64 	%rd43, %rd39, %rd5;
	shl.b64 	%rd44, %rd43, 1;
	add.s64 	%rd45, %rd1, %rd44;
	add.s64 	%rd62, %rd45, 1536;
	mul.wide.s32 	%rd46, %r16, 2;
	add.s64 	%rd47, %rd43, %rd46;
	shl.b64 	%rd48, %rd47, 1;
	add.s64 	%rd49, %rd1, %rd48;
	add.s64 	%rd61, %rd49, 1024;

$L__BB89_9:
	ld.global.nc.u32 	%r39, [%rd63+-1024];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r39;
  cvt.f32.f16 %f27, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r39;
  cvt.f32.f16 %f28, high;}

	// end inline asm
	ld.global.nc.u32 	%r41, [%rd62+-1536];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r41;
  cvt.f32.f16 %f29, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r41;
  cvt.f32.f16 %f30, high;}

	// end inline asm
	fma.rn.f32 	%f51, %f27, %f29, %f115;
	fma.rn.f32 	%f52, %f28, %f30, %f51;
	ld.global.nc.u32 	%r43, [%rd61+-1024];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r43;
  cvt.f32.f16 %f31, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r43;
  cvt.f32.f16 %f32, high;}

	// end inline asm
	fma.rn.f32 	%f53, %f27, %f31, %f114;
	fma.rn.f32 	%f54, %f28, %f32, %f53;
	ld.global.nc.u32 	%r45, [%rd63+-512];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r45;
  cvt.f32.f16 %f33, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r45;
  cvt.f32.f16 %f34, high;}

	// end inline asm
	ld.global.nc.u32 	%r47, [%rd62+-1024];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r47;
  cvt.f32.f16 %f35, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r47;
  cvt.f32.f16 %f36, high;}

	// end inline asm
	fma.rn.f32 	%f55, %f33, %f35, %f52;
	fma.rn.f32 	%f56, %f34, %f36, %f55;
	ld.global.nc.u32 	%r49, [%rd61+-512];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r49;
  cvt.f32.f16 %f37, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r49;
  cvt.f32.f16 %f38, high;}

	// end inline asm
	fma.rn.f32 	%f57, %f33, %f37, %f54;
	fma.rn.f32 	%f58, %f34, %f38, %f57;
	ld.global.nc.u32 	%r51, [%rd63];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r51;
  cvt.f32.f16 %f39, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r51;
  cvt.f32.f16 %f40, high;}

	// end inline asm
	ld.global.nc.u32 	%r53, [%rd62+-512];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r53;
  cvt.f32.f16 %f41, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r53;
  cvt.f32.f16 %f42, high;}

	// end inline asm
	fma.rn.f32 	%f59, %f39, %f41, %f56;
	fma.rn.f32 	%f60, %f40, %f42, %f59;
	ld.global.nc.u32 	%r55, [%rd61];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r55;
  cvt.f32.f16 %f43, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r55;
  cvt.f32.f16 %f44, high;}

	// end inline asm
	fma.rn.f32 	%f61, %f39, %f43, %f58;
	fma.rn.f32 	%f62, %f40, %f44, %f61;
	ld.global.nc.u32 	%r57, [%rd63+512];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r57;
  cvt.f32.f16 %f45, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r57;
  cvt.f32.f16 %f46, high;}

	// end inline asm
	ld.global.nc.u32 	%r59, [%rd62];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r59;
  cvt.f32.f16 %f47, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r59;
  cvt.f32.f16 %f48, high;}

	// end inline asm
	fma.rn.f32 	%f63, %f45, %f47, %f60;
	fma.rn.f32 	%f115, %f46, %f48, %f63;
	ld.global.nc.u32 	%r61, [%rd61+512];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r61;
  cvt.f32.f16 %f49, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r61;
  cvt.f32.f16 %f50, high;}

	// end inline asm
	fma.rn.f32 	%f64, %f45, %f49, %f62;
	fma.rn.f32 	%f114, %f46, %f50, %f64;
	add.s64 	%rd63, %rd63, 2048;
	add.s64 	%rd62, %rd62, 2048;
	add.s64 	%rd61, %rd61, 2048;
	add.s32 	%r141, %r141, 512;
	setp.lt.s32 	%p6, %r141, %r15;
	@%p6 bra 	$L__BB89_9;

	st.local.v2.f32 	[%rd3], {%f115, %f114};

$L__BB89_11:
	shr.s32 	%r63, %r3, 31;
	shr.u32 	%r64, %r63, 27;
	add.s32 	%r65, %r3, %r64;
	shr.s32 	%r66, %r65, 5;
	shl.b32 	%r67, %r66, 2;
	add.s32 	%r14, %r28, %r67;
	mov.u32 	%r69, 2;
	mov.b32 	%r70, %f115;
	mov.u32 	%r71, 31;
	mov.u32 	%r72, 16;
	mov.u32 	%r73, -1;
	shfl.sync.bfly.b32 	%r74|%p7, %r70, %r72, %r71, %r73;
	mov.b32 	%f65, %r74;
	add.f32 	%f66, %f115, %f65;
	mov.b32 	%r75, %f66;
	mov.u32 	%r76, 8;
	shfl.sync.bfly.b32 	%r77|%p8, %r75, %r76, %r71, %r73;
	mov.b32 	%f67, %r77;
	add.f32 	%f68, %f66, %f67;
	mov.b32 	%r78, %f68;
	mov.u32 	%r79, 4;
	shfl.sync.bfly.b32 	%r80|%p9, %r78, %r79, %r71, %r73;
	mov.b32 	%f69, %r80;
	add.f32 	%f70, %f68, %f69;
	mov.b32 	%r81, %f70;
	shfl.sync.bfly.b32 	%r82|%p10, %r81, %r69, %r71, %r73;
	mov.b32 	%f71, %r82;
	add.f32 	%f72, %f70, %f71;
	mov.b32 	%r83, %f72;
	mov.u32 	%r84, 1;
	shfl.sync.bfly.b32 	%r85|%p11, %r83, %r84, %r71, %r73;
	mov.b32 	%f73, %r85;
	add.f32 	%f74, %f72, %f73;
	st.local.f32 	[%rd3], %f74;
	st.shared.f32 	[%r14], %f74;
	bar.sync 	0;
	@%p1 bra 	$L__BB89_13;

	ld.shared.f32 	%f75, [%r4];
	mov.b32 	%r86, %f75;
	shfl.sync.bfly.b32 	%r90|%p13, %r86, %r72, %r71, %r73;
	mov.b32 	%f76, %r90;
	add.f32 	%f77, %f75, %f76;
	mov.b32 	%r91, %f77;
	shfl.sync.bfly.b32 	%r93|%p14, %r91, %r76, %r71, %r73;
	mov.b32 	%f78, %r93;
	add.f32 	%f79, %f77, %f78;
	mov.b32 	%r94, %f79;
	shfl.sync.bfly.b32 	%r96|%p15, %r94, %r79, %r71, %r73;
	mov.b32 	%f80, %r96;
	add.f32 	%f81, %f79, %f80;
	mov.b32 	%r97, %f81;
	shfl.sync.bfly.b32 	%r99|%p16, %r97, %r69, %r71, %r73;
	mov.b32 	%f82, %r99;
	add.f32 	%f83, %f81, %f82;
	mov.b32 	%r100, %f83;
	shfl.sync.bfly.b32 	%r102|%p17, %r100, %r84, %r71, %r73;
	mov.b32 	%f84, %r102;
	add.f32 	%f85, %f83, %f84;
	st.local.f32 	[%rd3], %f85;

$L__BB89_13:
	bar.sync 	0;
	mov.b32 	%r103, %f114;
	shfl.sync.bfly.b32 	%r107|%p19, %r103, %r72, %r71, %r73;
	mov.b32 	%f86, %r107;
	add.f32 	%f87, %f114, %f86;
	mov.b32 	%r108, %f87;
	shfl.sync.bfly.b32 	%r110|%p20, %r108, %r76, %r71, %r73;
	mov.b32 	%f88, %r110;
	add.f32 	%f89, %f87, %f88;
	mov.b32 	%r111, %f89;
	shfl.sync.bfly.b32 	%r113|%p21, %r111, %r79, %r71, %r73;
	mov.b32 	%f90, %r113;
	add.f32 	%f91, %f89, %f90;
	mov.b32 	%r114, %f91;
	shfl.sync.bfly.b32 	%r116|%p22, %r114, %r69, %r71, %r73;
	mov.b32 	%f92, %r116;
	add.f32 	%f93, %f91, %f92;
	mov.b32 	%r117, %f93;
	shfl.sync.bfly.b32 	%r119|%p23, %r117, %r84, %r71, %r73;
	mov.b32 	%f94, %r119;
	add.f32 	%f95, %f93, %f94;
	st.local.f32 	[%rd3+4], %f95;
	st.shared.f32 	[%r14], %f95;
	bar.sync 	0;
	@%p1 bra 	$L__BB89_15;

	ld.shared.f32 	%f96, [%r4];
	mov.b32 	%r120, %f96;
	mov.u32 	%r121, 31;
	mov.u32 	%r122, 16;
	mov.u32 	%r123, -1;
	shfl.sync.bfly.b32 	%r124|%p24, %r120, %r122, %r121, %r123;
	mov.b32 	%f97, %r124;
	add.f32 	%f98, %f96, %f97;
	mov.b32 	%r125, %f98;
	mov.u32 	%r126, 8;
	shfl.sync.bfly.b32 	%r127|%p25, %r125, %r126, %r121, %r123;
	mov.b32 	%f99, %r127;
	add.f32 	%f100, %f98, %f99;
	mov.b32 	%r128, %f100;
	mov.u32 	%r129, 4;
	shfl.sync.bfly.b32 	%r130|%p26, %r128, %r129, %r121, %r123;
	mov.b32 	%f101, %r130;
	add.f32 	%f102, %f100, %f101;
	mov.b32 	%r131, %f102;
	mov.u32 	%r132, 2;
	shfl.sync.bfly.b32 	%r133|%p27, %r131, %r132, %r121, %r123;
	mov.b32 	%f103, %r133;
	add.f32 	%f104, %f102, %f103;
	mov.b32 	%r134, %f104;
	mov.u32 	%r135, 1;
	shfl.sync.bfly.b32 	%r136|%p28, %r134, %r135, %r121, %r123;
	mov.b32 	%f105, %r136;
	add.f32 	%f106, %f104, %f105;
	st.local.f32 	[%rd3+4], %f106;

$L__BB89_15:
	bar.sync 	0;
	setp.gt.s32 	%p29, %r3, 1;
	@%p29 bra 	$L__BB89_17;

	mad.lo.s32 	%r137, %r3, %r17, %r2;
	cvt.s64.s32 	%rd50, %r137;
	mul.lo.s32 	%r138, %r1, %r18;
	cvt.s64.s32 	%rd51, %r138;
	add.s64 	%rd52, %rd51, %rd50;
	mul.wide.s32 	%rd53, %r3, 4;
	add.s64 	%rd54, %rd3, %rd53;
	ld.local.f32 	%f107, [%rd54];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f107;}

	// end inline asm
	cvta.to.global.u64 	%rd55, %rd26;
	shl.b64 	%rd56, %rd52, 1;
	add.s64 	%rd57, %rd55, %rd56;
	st.global.u16 	[%rd57], %rs1;

$L__BB89_17:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_3_bs_128
.visible .entry ggml_matvec_f16_ncols_3_bs_128(
	.param .u64 ggml_matvec_f16_ncols_3_bs_128_param_0,
	.param .u64 ggml_matvec_f16_ncols_3_bs_128_param_1,
	.param .u64 ggml_matvec_f16_ncols_3_bs_128_param_2,
	.param .u32 ggml_matvec_f16_ncols_3_bs_128_param_3,
	.param .u32 ggml_matvec_f16_ncols_3_bs_128_param_4,
	.param .u32 ggml_matvec_f16_ncols_3_bs_128_param_5,
	.param .u32 ggml_matvec_f16_ncols_3_bs_128_param_6,
	.param .u32 ggml_matvec_f16_ncols_3_bs_128_param_7,
	.param .u32 ggml_matvec_f16_ncols_3_bs_128_param_8,
	.param .u32 ggml_matvec_f16_ncols_3_bs_128_param_9,
	.param .u32 ggml_matvec_f16_ncols_3_bs_128_param_10,
	.param .u32 ggml_matvec_f16_ncols_3_bs_128_param_11
)
{
	.local .align 4 .b8 	__local_depot90[12];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<41>;
	.reg .b16 	%rs<2>;
	.reg .f32 	%f<168>;
	.reg .b32 	%r<194>;
	.reg .b64 	%rd<72>;


	mov.u64 	%SPL, __local_depot90;
	ld.param.u64 	%rd29, [ggml_matvec_f16_ncols_3_bs_128_param_0];
	ld.param.u64 	%rd30, [ggml_matvec_f16_ncols_3_bs_128_param_1];
	ld.param.u64 	%rd28, [ggml_matvec_f16_ncols_3_bs_128_param_2];
	ld.param.u32 	%r15, [ggml_matvec_f16_ncols_3_bs_128_param_3];
	ld.param.u32 	%r19, [ggml_matvec_f16_ncols_3_bs_128_param_5];
	ld.param.u32 	%r16, [ggml_matvec_f16_ncols_3_bs_128_param_6];
	ld.param.u32 	%r17, [ggml_matvec_f16_ncols_3_bs_128_param_7];
	ld.param.u32 	%r20, [ggml_matvec_f16_ncols_3_bs_128_param_8];
	ld.param.u32 	%r21, [ggml_matvec_f16_ncols_3_bs_128_param_9];
	ld.param.u32 	%r22, [ggml_matvec_f16_ncols_3_bs_128_param_10];
	ld.param.u32 	%r18, [ggml_matvec_f16_ncols_3_bs_128_param_11];
	cvta.to.global.u64 	%rd71, %rd30;
	cvta.to.global.u64 	%rd2, %rd29;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r23, %r1, %r20;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r24, %r2, %r19;
	mad.lo.s32 	%r25, %r23, %r21, %r24;
	cvt.s64.s32 	%rd4, %r25;
	mul.lo.s32 	%r26, %r1, %r22;
	cvt.s64.s32 	%rd5, %r26;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r27, %r3, 2;
	mov.u32 	%r28, data_mmv;
	add.s32 	%r4, %r28, %r27;
	@%p1 bra 	$L__BB90_2;

	mov.u32 	%r29, 0;
	st.shared.u32 	[%r4], %r29;

$L__BB90_2:
	bar.sync 	0;
	mov.f32 	%f165, 0f00000000;
	mov.u32 	%r30, 0;
	st.local.u32 	[%rd3], %r30;
	st.local.u32 	[%rd3+4], %r30;
	st.local.u32 	[%rd3+8], %r30;
	setp.ge.s32 	%p2, %r3, %r15;
	mov.f32 	%f166, %f165;
	mov.f32 	%f167, %f165;
	@%p2 bra 	$L__BB90_11;

	not.b32 	%r31, %r3;
	add.s32 	%r5, %r31, %r15;
	shr.u32 	%r32, %r5, 7;
	add.s32 	%r33, %r32, 1;
	and.b32  	%r191, %r33, 3;
	setp.eq.s32 	%p3, %r191, 0;
	mov.f32 	%f165, 0f00000000;
	mov.u32 	%r192, %r3;
	@%p3 bra 	$L__BB90_7;

	shl.b32 	%r34, %r16, 1;
	add.s32 	%r35, %r3, %r34;
	mul.wide.s32 	%rd32, %r35, 2;
	add.s64 	%rd33, %rd32, %rd5;
	shl.b64 	%rd34, %rd33, 1;
	add.s64 	%rd69, %rd71, %rd34;
	mul.wide.s32 	%rd35, %r16, 2;
	mul.wide.s32 	%rd36, %r3, 2;
	add.s64 	%rd37, %rd35, %rd36;
	add.s64 	%rd38, %rd37, %rd5;
	shl.b64 	%rd39, %rd38, 1;
	add.s64 	%rd68, %rd71, %rd39;
	add.s64 	%rd40, %rd36, %rd5;
	shl.b64 	%rd41, %rd40, 1;
	add.s64 	%rd67, %rd71, %rd41;
	add.s64 	%rd42, %rd36, %rd4;
	shl.b64 	%rd43, %rd42, 1;
	add.s64 	%rd66, %rd2, %rd43;
	mov.f32 	%f165, 0f00000000;
	mov.f32 	%f166, %f165;
	mov.f32 	%f167, %f165;
	mov.u32 	%r192, %r3;

$L__BB90_5:
	.pragma "nounroll";
	ld.global.nc.u32 	%r36, [%rd66];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r36;
  cvt.f32.f16 %f28, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r36;
  cvt.f32.f16 %f29, high;}

	// end inline asm
	ld.global.nc.u32 	%r38, [%rd67];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r38;
  cvt.f32.f16 %f30, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r38;
  cvt.f32.f16 %f31, high;}

	// end inline asm
	fma.rn.f32 	%f36, %f28, %f30, %f167;
	fma.rn.f32 	%f167, %f29, %f31, %f36;
	ld.global.nc.u32 	%r40, [%rd68];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r40;
  cvt.f32.f16 %f32, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r40;
  cvt.f32.f16 %f33, high;}

	// end inline asm
	fma.rn.f32 	%f37, %f28, %f32, %f166;
	fma.rn.f32 	%f166, %f29, %f33, %f37;
	ld.global.nc.u32 	%r42, [%rd69];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r42;
  cvt.f32.f16 %f34, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r42;
  cvt.f32.f16 %f35, high;}

	// end inline asm
	fma.rn.f32 	%f38, %f28, %f34, %f165;
	fma.rn.f32 	%f165, %f29, %f35, %f38;
	add.s32 	%r192, %r192, 128;
	add.s64 	%rd69, %rd69, 512;
	add.s64 	%rd68, %rd68, 512;
	add.s64 	%rd67, %rd67, 512;
	add.s64 	%rd66, %rd66, 512;
	add.s32 	%r191, %r191, -1;
	setp.ne.s32 	%p4, %r191, 0;
	@%p4 bra 	$L__BB90_5;

	st.local.f32 	[%rd3], %f167;
	st.local.f32 	[%rd3+4], %f166;
	st.local.f32 	[%rd3+8], %f165;

$L__BB90_7:
	setp.lt.u32 	%p5, %r5, 384;
	@%p5 bra 	$L__BB90_11;

	add.s32 	%r44, %r192, %r16;
	shl.b32 	%r45, %r16, 1;
	add.s32 	%r46, %r192, %r45;
	add.s32 	%r47, %r44, 128;
	mul.wide.s32 	%rd44, %r47, 4;
	shl.b64 	%rd45, %rd5, 1;
	add.s64 	%rd19, %rd44, %rd45;
	mul.wide.s32 	%rd46, %r46, 4;
	add.s64 	%rd20, %rd46, %rd45;
	mul.wide.s32 	%rd47, %r192, 2;
	add.s64 	%rd48, %rd47, %rd4;
	shl.b64 	%rd49, %rd48, 1;
	add.s64 	%rd50, %rd2, %rd49;
	add.s64 	%rd70, %rd50, 1024;
	mul.wide.s32 	%rd51, %r192, 4;
	add.s64 	%rd22, %rd51, %rd45;
	mul.wide.s32 	%rd52, %r16, 4;
	add.s64 	%rd53, %rd51, %rd52;
	add.s64 	%rd23, %rd53, %rd45;

$L__BB90_9:
	ld.global.nc.u32 	%r48, [%rd70+-1024];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r48;
  cvt.f32.f16 %f39, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r48;
  cvt.f32.f16 %f40, high;}

	// end inline asm
	add.s64 	%rd54, %rd71, %rd22;
	ld.global.nc.u32 	%r50, [%rd54];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r50;
  cvt.f32.f16 %f41, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r50;
  cvt.f32.f16 %f42, high;}

	// end inline asm
	fma.rn.f32 	%f71, %f39, %f41, %f167;
	fma.rn.f32 	%f72, %f40, %f42, %f71;
	add.s64 	%rd55, %rd71, %rd23;
	ld.global.nc.u32 	%r52, [%rd55];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r52;
  cvt.f32.f16 %f43, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r52;
  cvt.f32.f16 %f44, high;}

	// end inline asm
	fma.rn.f32 	%f73, %f39, %f43, %f166;
	fma.rn.f32 	%f74, %f40, %f44, %f73;
	add.s64 	%rd56, %rd71, %rd20;
	ld.global.nc.u32 	%r54, [%rd56];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r54;
  cvt.f32.f16 %f45, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r54;
  cvt.f32.f16 %f46, high;}

	// end inline asm
	fma.rn.f32 	%f75, %f39, %f45, %f165;
	fma.rn.f32 	%f76, %f40, %f46, %f75;
	ld.global.nc.u32 	%r56, [%rd70+-512];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r56;
  cvt.f32.f16 %f47, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r56;
  cvt.f32.f16 %f48, high;}

	// end inline asm
	ld.global.nc.u32 	%r58, [%rd54+512];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r58;
  cvt.f32.f16 %f49, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r58;
  cvt.f32.f16 %f50, high;}

	// end inline asm
	fma.rn.f32 	%f77, %f47, %f49, %f72;
	fma.rn.f32 	%f78, %f48, %f50, %f77;
	add.s64 	%rd57, %rd71, %rd19;
	ld.global.nc.u32 	%r60, [%rd57];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r60;
  cvt.f32.f16 %f51, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r60;
  cvt.f32.f16 %f52, high;}

	// end inline asm
	fma.rn.f32 	%f79, %f47, %f51, %f74;
	fma.rn.f32 	%f80, %f48, %f52, %f79;
	ld.global.nc.u32 	%r62, [%rd56+512];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r62;
  cvt.f32.f16 %f53, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r62;
  cvt.f32.f16 %f54, high;}

	// end inline asm
	fma.rn.f32 	%f81, %f47, %f53, %f76;
	fma.rn.f32 	%f82, %f48, %f54, %f81;
	ld.global.nc.u32 	%r64, [%rd70];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r64;
  cvt.f32.f16 %f55, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r64;
  cvt.f32.f16 %f56, high;}

	// end inline asm
	ld.global.nc.u32 	%r66, [%rd54+1024];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r66;
  cvt.f32.f16 %f57, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r66;
  cvt.f32.f16 %f58, high;}

	// end inline asm
	fma.rn.f32 	%f83, %f55, %f57, %f78;
	fma.rn.f32 	%f84, %f56, %f58, %f83;
	ld.global.nc.u32 	%r68, [%rd57+512];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r68;
  cvt.f32.f16 %f59, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r68;
  cvt.f32.f16 %f60, high;}

	// end inline asm
	fma.rn.f32 	%f85, %f55, %f59, %f80;
	fma.rn.f32 	%f86, %f56, %f60, %f85;
	ld.global.nc.u32 	%r70, [%rd56+1024];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r70;
  cvt.f32.f16 %f61, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r70;
  cvt.f32.f16 %f62, high;}

	// end inline asm
	fma.rn.f32 	%f87, %f55, %f61, %f82;
	fma.rn.f32 	%f88, %f56, %f62, %f87;
	ld.global.nc.u32 	%r72, [%rd70+512];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r72;
  cvt.f32.f16 %f63, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r72;
  cvt.f32.f16 %f64, high;}

	// end inline asm
	ld.global.nc.u32 	%r74, [%rd54+1536];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r74;
  cvt.f32.f16 %f65, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r74;
  cvt.f32.f16 %f66, high;}

	// end inline asm
	fma.rn.f32 	%f89, %f63, %f65, %f84;
	fma.rn.f32 	%f167, %f64, %f66, %f89;
	ld.global.nc.u32 	%r76, [%rd57+1024];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r76;
  cvt.f32.f16 %f67, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r76;
  cvt.f32.f16 %f68, high;}

	// end inline asm
	fma.rn.f32 	%f90, %f63, %f67, %f86;
	fma.rn.f32 	%f166, %f64, %f68, %f90;
	ld.global.nc.u32 	%r78, [%rd56+1536];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r78;
  cvt.f32.f16 %f69, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r78;
  cvt.f32.f16 %f70, high;}

	// end inline asm
	fma.rn.f32 	%f91, %f63, %f69, %f88;
	fma.rn.f32 	%f165, %f64, %f70, %f91;
	add.s64 	%rd71, %rd71, 2048;
	add.s64 	%rd70, %rd70, 2048;
	add.s32 	%r192, %r192, 512;
	setp.lt.s32 	%p6, %r192, %r15;
	@%p6 bra 	$L__BB90_9;

	st.local.f32 	[%rd3], %f167;
	st.local.f32 	[%rd3+4], %f166;
	st.local.f32 	[%rd3+8], %f165;

$L__BB90_11:
	shr.s32 	%r80, %r3, 31;
	shr.u32 	%r81, %r80, 27;
	add.s32 	%r82, %r3, %r81;
	shr.s32 	%r83, %r82, 5;
	shl.b32 	%r84, %r83, 2;
	add.s32 	%r14, %r28, %r84;
	mov.u32 	%r86, 2;
	mov.b32 	%r87, %f167;
	mov.u32 	%r88, 31;
	mov.u32 	%r89, 16;
	mov.u32 	%r90, -1;
	shfl.sync.bfly.b32 	%r91|%p7, %r87, %r89, %r88, %r90;
	mov.b32 	%f92, %r91;
	add.f32 	%f93, %f167, %f92;
	mov.b32 	%r92, %f93;
	mov.u32 	%r93, 8;
	shfl.sync.bfly.b32 	%r94|%p8, %r92, %r93, %r88, %r90;
	mov.b32 	%f94, %r94;
	add.f32 	%f95, %f93, %f94;
	mov.b32 	%r95, %f95;
	mov.u32 	%r96, 4;
	shfl.sync.bfly.b32 	%r97|%p9, %r95, %r96, %r88, %r90;
	mov.b32 	%f96, %r97;
	add.f32 	%f97, %f95, %f96;
	mov.b32 	%r98, %f97;
	shfl.sync.bfly.b32 	%r99|%p10, %r98, %r86, %r88, %r90;
	mov.b32 	%f98, %r99;
	add.f32 	%f99, %f97, %f98;
	mov.b32 	%r100, %f99;
	mov.u32 	%r101, 1;
	shfl.sync.bfly.b32 	%r102|%p11, %r100, %r101, %r88, %r90;
	mov.b32 	%f100, %r102;
	add.f32 	%f101, %f99, %f100;
	st.local.f32 	[%rd3], %f101;
	st.shared.f32 	[%r14], %f101;
	bar.sync 	0;
	@%p1 bra 	$L__BB90_13;

	ld.shared.f32 	%f102, [%r4];
	mov.b32 	%r103, %f102;
	shfl.sync.bfly.b32 	%r107|%p13, %r103, %r89, %r88, %r90;
	mov.b32 	%f103, %r107;
	add.f32 	%f104, %f102, %f103;
	mov.b32 	%r108, %f104;
	shfl.sync.bfly.b32 	%r110|%p14, %r108, %r93, %r88, %r90;
	mov.b32 	%f105, %r110;
	add.f32 	%f106, %f104, %f105;
	mov.b32 	%r111, %f106;
	shfl.sync.bfly.b32 	%r113|%p15, %r111, %r96, %r88, %r90;
	mov.b32 	%f107, %r113;
	add.f32 	%f108, %f106, %f107;
	mov.b32 	%r114, %f108;
	shfl.sync.bfly.b32 	%r116|%p16, %r114, %r86, %r88, %r90;
	mov.b32 	%f109, %r116;
	add.f32 	%f110, %f108, %f109;
	mov.b32 	%r117, %f110;
	shfl.sync.bfly.b32 	%r119|%p17, %r117, %r101, %r88, %r90;
	mov.b32 	%f111, %r119;
	add.f32 	%f112, %f110, %f111;
	st.local.f32 	[%rd3], %f112;

$L__BB90_13:
	bar.sync 	0;
	mov.b32 	%r120, %f166;
	shfl.sync.bfly.b32 	%r124|%p19, %r120, %r89, %r88, %r90;
	mov.b32 	%f113, %r124;
	add.f32 	%f114, %f166, %f113;
	mov.b32 	%r125, %f114;
	shfl.sync.bfly.b32 	%r127|%p20, %r125, %r93, %r88, %r90;
	mov.b32 	%f115, %r127;
	add.f32 	%f116, %f114, %f115;
	mov.b32 	%r128, %f116;
	shfl.sync.bfly.b32 	%r130|%p21, %r128, %r96, %r88, %r90;
	mov.b32 	%f117, %r130;
	add.f32 	%f118, %f116, %f117;
	mov.b32 	%r131, %f118;
	shfl.sync.bfly.b32 	%r133|%p22, %r131, %r86, %r88, %r90;
	mov.b32 	%f119, %r133;
	add.f32 	%f120, %f118, %f119;
	mov.b32 	%r134, %f120;
	shfl.sync.bfly.b32 	%r136|%p23, %r134, %r101, %r88, %r90;
	mov.b32 	%f121, %r136;
	add.f32 	%f122, %f120, %f121;
	st.local.f32 	[%rd3+4], %f122;
	st.shared.f32 	[%r14], %f122;
	bar.sync 	0;
	@%p1 bra 	$L__BB90_15;

	ld.shared.f32 	%f123, [%r4];
	mov.b32 	%r137, %f123;
	mov.u32 	%r138, 31;
	mov.u32 	%r139, 16;
	mov.u32 	%r140, -1;
	shfl.sync.bfly.b32 	%r141|%p24, %r137, %r139, %r138, %r140;
	mov.b32 	%f124, %r141;
	add.f32 	%f125, %f123, %f124;
	mov.b32 	%r142, %f125;
	mov.u32 	%r143, 8;
	shfl.sync.bfly.b32 	%r144|%p25, %r142, %r143, %r138, %r140;
	mov.b32 	%f126, %r144;
	add.f32 	%f127, %f125, %f126;
	mov.b32 	%r145, %f127;
	mov.u32 	%r146, 4;
	shfl.sync.bfly.b32 	%r147|%p26, %r145, %r146, %r138, %r140;
	mov.b32 	%f128, %r147;
	add.f32 	%f129, %f127, %f128;
	mov.b32 	%r148, %f129;
	mov.u32 	%r149, 2;
	shfl.sync.bfly.b32 	%r150|%p27, %r148, %r149, %r138, %r140;
	mov.b32 	%f130, %r150;
	add.f32 	%f131, %f129, %f130;
	mov.b32 	%r151, %f131;
	mov.u32 	%r152, 1;
	shfl.sync.bfly.b32 	%r153|%p28, %r151, %r152, %r138, %r140;
	mov.b32 	%f132, %r153;
	add.f32 	%f133, %f131, %f132;
	st.local.f32 	[%rd3+4], %f133;

$L__BB90_15:
	bar.sync 	0;
	mov.b32 	%r154, %f165;
	mov.u32 	%r155, 31;
	mov.u32 	%r156, 16;
	mov.u32 	%r157, -1;
	shfl.sync.bfly.b32 	%r158|%p30, %r154, %r156, %r155, %r157;
	mov.b32 	%f134, %r158;
	add.f32 	%f135, %f165, %f134;
	mov.b32 	%r159, %f135;
	mov.u32 	%r160, 8;
	shfl.sync.bfly.b32 	%r161|%p31, %r159, %r160, %r155, %r157;
	mov.b32 	%f136, %r161;
	add.f32 	%f137, %f135, %f136;
	mov.b32 	%r162, %f137;
	mov.u32 	%r163, 4;
	shfl.sync.bfly.b32 	%r164|%p32, %r162, %r163, %r155, %r157;
	mov.b32 	%f138, %r164;
	add.f32 	%f139, %f137, %f138;
	mov.b32 	%r165, %f139;
	mov.u32 	%r166, 2;
	shfl.sync.bfly.b32 	%r167|%p33, %r165, %r166, %r155, %r157;
	mov.b32 	%f140, %r167;
	add.f32 	%f141, %f139, %f140;
	mov.b32 	%r168, %f141;
	mov.u32 	%r169, 1;
	shfl.sync.bfly.b32 	%r170|%p34, %r168, %r169, %r155, %r157;
	mov.b32 	%f142, %r170;
	add.f32 	%f143, %f141, %f142;
	st.local.f32 	[%rd3+8], %f143;
	st.shared.f32 	[%r14], %f143;
	bar.sync 	0;
	@%p1 bra 	$L__BB90_17;

	ld.shared.f32 	%f144, [%r4];
	mov.b32 	%r171, %f144;
	shfl.sync.bfly.b32 	%r175|%p35, %r171, %r156, %r155, %r157;
	mov.b32 	%f145, %r175;
	add.f32 	%f146, %f144, %f145;
	mov.b32 	%r176, %f146;
	shfl.sync.bfly.b32 	%r178|%p36, %r176, %r160, %r155, %r157;
	mov.b32 	%f147, %r178;
	add.f32 	%f148, %f146, %f147;
	mov.b32 	%r179, %f148;
	shfl.sync.bfly.b32 	%r181|%p37, %r179, %r163, %r155, %r157;
	mov.b32 	%f149, %r181;
	add.f32 	%f150, %f148, %f149;
	mov.b32 	%r182, %f150;
	shfl.sync.bfly.b32 	%r184|%p38, %r182, %r166, %r155, %r157;
	mov.b32 	%f151, %r184;
	add.f32 	%f152, %f150, %f151;
	mov.b32 	%r185, %f152;
	shfl.sync.bfly.b32 	%r187|%p39, %r185, %r169, %r155, %r157;
	mov.b32 	%f153, %r187;
	add.f32 	%f154, %f152, %f153;
	st.local.f32 	[%rd3+8], %f154;

$L__BB90_17:
	bar.sync 	0;
	setp.gt.s32 	%p40, %r3, 2;
	@%p40 bra 	$L__BB90_19;

	mad.lo.s32 	%r188, %r3, %r17, %r2;
	cvt.s64.s32 	%rd58, %r188;
	mul.lo.s32 	%r189, %r1, %r18;
	cvt.s64.s32 	%rd59, %r189;
	add.s64 	%rd60, %rd59, %rd58;
	mul.wide.s32 	%rd61, %r3, 4;
	add.s64 	%rd62, %rd3, %rd61;
	ld.local.f32 	%f155, [%rd62];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f155;}

	// end inline asm
	cvta.to.global.u64 	%rd63, %rd28;
	shl.b64 	%rd64, %rd60, 1;
	add.s64 	%rd65, %rd63, %rd64;
	st.global.u16 	[%rd65], %rs1;

$L__BB90_19:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_4_bs_128
.visible .entry ggml_matvec_f16_ncols_4_bs_128(
	.param .u64 ggml_matvec_f16_ncols_4_bs_128_param_0,
	.param .u64 ggml_matvec_f16_ncols_4_bs_128_param_1,
	.param .u64 ggml_matvec_f16_ncols_4_bs_128_param_2,
	.param .u32 ggml_matvec_f16_ncols_4_bs_128_param_3,
	.param .u32 ggml_matvec_f16_ncols_4_bs_128_param_4,
	.param .u32 ggml_matvec_f16_ncols_4_bs_128_param_5,
	.param .u32 ggml_matvec_f16_ncols_4_bs_128_param_6,
	.param .u32 ggml_matvec_f16_ncols_4_bs_128_param_7,
	.param .u32 ggml_matvec_f16_ncols_4_bs_128_param_8,
	.param .u32 ggml_matvec_f16_ncols_4_bs_128_param_9,
	.param .u32 ggml_matvec_f16_ncols_4_bs_128_param_10,
	.param .u32 ggml_matvec_f16_ncols_4_bs_128_param_11
)
{
	.local .align 16 .b8 	__local_depot91[16];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<51>;
	.reg .b16 	%rs<2>;
	.reg .f32 	%f<172>;
	.reg .b32 	%r<213>;
	.reg .b64 	%rd<61>;


	mov.u64 	%SPL, __local_depot91;
	ld.param.u64 	%rd19, [ggml_matvec_f16_ncols_4_bs_128_param_0];
	ld.param.u64 	%rd20, [ggml_matvec_f16_ncols_4_bs_128_param_1];
	ld.param.u64 	%rd18, [ggml_matvec_f16_ncols_4_bs_128_param_2];
	ld.param.u32 	%r11, [ggml_matvec_f16_ncols_4_bs_128_param_3];
	ld.param.u32 	%r15, [ggml_matvec_f16_ncols_4_bs_128_param_5];
	ld.param.u32 	%r12, [ggml_matvec_f16_ncols_4_bs_128_param_6];
	ld.param.u32 	%r13, [ggml_matvec_f16_ncols_4_bs_128_param_7];
	ld.param.u32 	%r16, [ggml_matvec_f16_ncols_4_bs_128_param_8];
	ld.param.u32 	%r17, [ggml_matvec_f16_ncols_4_bs_128_param_9];
	ld.param.u32 	%r18, [ggml_matvec_f16_ncols_4_bs_128_param_10];
	ld.param.u32 	%r14, [ggml_matvec_f16_ncols_4_bs_128_param_11];
	cvta.to.global.u64 	%rd60, %rd20;
	add.u64 	%rd2, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r19, %r1, %r16;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r20, %r2, %r15;
	mad.lo.s32 	%r21, %r19, %r17, %r20;
	cvt.s64.s32 	%rd3, %r21;
	cvta.to.global.u64 	%rd4, %rd19;
	mul.lo.s32 	%r22, %r1, %r18;
	cvt.s64.s32 	%rd5, %r22;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r23, %r3, 2;
	mov.u32 	%r24, data_mmv;
	add.s32 	%r4, %r24, %r23;
	@%p1 bra 	$L__BB91_2;

	mov.u32 	%r25, 0;
	st.shared.u32 	[%r4], %r25;

$L__BB91_2:
	bar.sync 	0;
	mov.f32 	%f168, 0f00000000;
	st.local.v4.f32 	[%rd2], {%f168, %f168, %f168, %f168};
	setp.ge.s32 	%p2, %r3, %r11;
	mov.f32 	%f169, %f168;
	mov.f32 	%f170, %f168;
	mov.f32 	%f171, %f168;
	@%p2 bra 	$L__BB91_9;

	not.b32 	%r26, %r3;
	add.s32 	%r5, %r26, %r11;
	and.b32  	%r27, %r5, 128;
	setp.ne.s32 	%p3, %r27, 0;
	mov.f32 	%f168, 0f00000000;
	mov.u32 	%r212, %r3;
	@%p3 bra 	$L__BB91_5;

	shl.b64 	%rd22, %rd5, 1;
	add.s64 	%rd23, %rd60, %rd22;
	shl.b64 	%rd24, %rd3, 1;
	add.s64 	%rd25, %rd4, %rd24;
	mul.wide.s32 	%rd26, %r3, 4;
	add.s64 	%rd27, %rd25, %rd26;
	ld.global.nc.u32 	%r28, [%rd27];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r28;
  cvt.f32.f16 %f29, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r28;
  cvt.f32.f16 %f30, high;}

	// end inline asm
	add.s64 	%rd28, %rd23, %rd26;
	ld.global.nc.u32 	%r30, [%rd28];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r30;
  cvt.f32.f16 %f31, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r30;
  cvt.f32.f16 %f32, high;}

	// end inline asm
	fma.rn.f32 	%f39, %f29, %f31, 0f00000000;
	fma.rn.f32 	%f171, %f30, %f32, %f39;
	st.local.f32 	[%rd2], %f171;
	mul.wide.s32 	%rd29, %r12, 4;
	add.s64 	%rd30, %rd28, %rd29;
	ld.global.nc.u32 	%r32, [%rd30];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r32;
  cvt.f32.f16 %f33, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r32;
  cvt.f32.f16 %f34, high;}

	// end inline asm
	fma.rn.f32 	%f40, %f29, %f33, 0f00000000;
	fma.rn.f32 	%f170, %f30, %f34, %f40;
	st.local.f32 	[%rd2+4], %f170;
	add.s32 	%r38, %r3, %r12;
	add.s32 	%r39, %r38, %r12;
	mul.wide.s32 	%rd31, %r39, 4;
	add.s64 	%rd32, %rd23, %rd31;
	ld.global.nc.u32 	%r34, [%rd32];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r34;
  cvt.f32.f16 %f35, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r34;
  cvt.f32.f16 %f36, high;}

	// end inline asm
	fma.rn.f32 	%f41, %f29, %f35, 0f00000000;
	fma.rn.f32 	%f169, %f30, %f36, %f41;
	st.local.f32 	[%rd2+8], %f169;
	add.s32 	%r40, %r39, %r12;
	mul.wide.s32 	%rd33, %r40, 4;
	add.s64 	%rd34, %rd23, %rd33;
	ld.global.nc.u32 	%r36, [%rd34];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r36;
  cvt.f32.f16 %f37, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r36;
  cvt.f32.f16 %f38, high;}

	// end inline asm
	fma.rn.f32 	%f42, %f29, %f37, 0f00000000;
	fma.rn.f32 	%f168, %f30, %f38, %f42;
	st.local.f32 	[%rd2+12], %f168;
	add.s32 	%r212, %r3, 128;

$L__BB91_5:
	and.b32  	%r41, %r5, -128;
	setp.eq.s32 	%p4, %r41, 0;
	@%p4 bra 	$L__BB91_9;

	add.s32 	%r42, %r212, %r12;
	add.s32 	%r43, %r42, 128;
	mul.wide.s32 	%rd35, %r43, 4;
	shl.b64 	%rd36, %rd5, 1;
	add.s64 	%rd8, %rd35, %rd36;
	shl.b32 	%r44, %r12, 1;
	add.s32 	%r45, %r212, %r44;
	mad.lo.s32 	%r46, %r12, 3, %r212;
	mul.wide.s32 	%rd37, %r45, 4;
	add.s64 	%rd9, %rd37, %rd36;
	mul.wide.s32 	%rd38, %r46, 4;
	add.s64 	%rd10, %rd38, %rd36;
	mul.wide.s32 	%rd39, %r212, 2;
	add.s64 	%rd40, %rd39, %rd3;
	shl.b64 	%rd41, %rd40, 1;
	add.s64 	%rd42, %rd4, %rd41;
	add.s64 	%rd59, %rd42, 512;
	mul.wide.s32 	%rd43, %r212, 4;
	mul.wide.s32 	%rd44, %r12, 4;
	add.s64 	%rd45, %rd43, %rd44;
	add.s64 	%rd12, %rd45, %rd36;
	add.s64 	%rd13, %rd43, %rd36;

$L__BB91_7:
	ld.global.nc.u32 	%r47, [%rd59+-512];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r47;
  cvt.f32.f16 %f43, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r47;
  cvt.f32.f16 %f44, high;}

	// end inline asm
	add.s64 	%rd46, %rd60, %rd13;
	ld.global.nc.u32 	%r49, [%rd46];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r49;
  cvt.f32.f16 %f45, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r49;
  cvt.f32.f16 %f46, high;}

	// end inline asm
	fma.rn.f32 	%f63, %f43, %f45, %f171;
	fma.rn.f32 	%f64, %f44, %f46, %f63;
	add.s64 	%rd47, %rd60, %rd12;
	ld.global.nc.u32 	%r51, [%rd47];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r51;
  cvt.f32.f16 %f47, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r51;
  cvt.f32.f16 %f48, high;}

	// end inline asm
	fma.rn.f32 	%f65, %f43, %f47, %f170;
	fma.rn.f32 	%f66, %f44, %f48, %f65;
	add.s64 	%rd48, %rd60, %rd9;
	ld.global.nc.u32 	%r53, [%rd48];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r53;
  cvt.f32.f16 %f49, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r53;
  cvt.f32.f16 %f50, high;}

	// end inline asm
	fma.rn.f32 	%f67, %f43, %f49, %f169;
	fma.rn.f32 	%f68, %f44, %f50, %f67;
	add.s64 	%rd49, %rd60, %rd10;
	ld.global.nc.u32 	%r55, [%rd49];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r55;
  cvt.f32.f16 %f51, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r55;
  cvt.f32.f16 %f52, high;}

	// end inline asm
	fma.rn.f32 	%f69, %f43, %f51, %f168;
	fma.rn.f32 	%f70, %f44, %f52, %f69;
	ld.global.nc.u32 	%r57, [%rd59];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r57;
  cvt.f32.f16 %f53, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r57;
  cvt.f32.f16 %f54, high;}

	// end inline asm
	ld.global.nc.u32 	%r59, [%rd46+512];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r59;
  cvt.f32.f16 %f55, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r59;
  cvt.f32.f16 %f56, high;}

	// end inline asm
	fma.rn.f32 	%f71, %f53, %f55, %f64;
	fma.rn.f32 	%f171, %f54, %f56, %f71;
	add.s64 	%rd50, %rd60, %rd8;
	ld.global.nc.u32 	%r61, [%rd50];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r61;
  cvt.f32.f16 %f57, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r61;
  cvt.f32.f16 %f58, high;}

	// end inline asm
	fma.rn.f32 	%f72, %f53, %f57, %f66;
	fma.rn.f32 	%f170, %f54, %f58, %f72;
	ld.global.nc.u32 	%r63, [%rd48+512];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r63;
  cvt.f32.f16 %f59, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r63;
  cvt.f32.f16 %f60, high;}

	// end inline asm
	fma.rn.f32 	%f73, %f53, %f59, %f68;
	fma.rn.f32 	%f169, %f54, %f60, %f73;
	ld.global.nc.u32 	%r65, [%rd49+512];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r65;
  cvt.f32.f16 %f61, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r65;
  cvt.f32.f16 %f62, high;}

	// end inline asm
	fma.rn.f32 	%f74, %f53, %f61, %f70;
	fma.rn.f32 	%f168, %f54, %f62, %f74;
	add.s64 	%rd60, %rd60, 1024;
	add.s64 	%rd59, %rd59, 1024;
	add.s32 	%r212, %r212, 256;
	setp.lt.s32 	%p5, %r212, %r11;
	@%p5 bra 	$L__BB91_7;

	st.local.v4.f32 	[%rd2], {%f171, %f170, %f169, %f168};

$L__BB91_9:
	shr.s32 	%r67, %r3, 31;
	shr.u32 	%r68, %r67, 27;
	add.s32 	%r69, %r3, %r68;
	shr.s32 	%r70, %r69, 5;
	shl.b32 	%r71, %r70, 2;
	add.s32 	%r10, %r24, %r71;
	mov.u32 	%r73, 2;
	mov.b32 	%r74, %f171;
	mov.u32 	%r75, 31;
	mov.u32 	%r76, 16;
	mov.u32 	%r77, -1;
	shfl.sync.bfly.b32 	%r78|%p6, %r74, %r76, %r75, %r77;
	mov.b32 	%f75, %r78;
	add.f32 	%f76, %f171, %f75;
	mov.b32 	%r79, %f76;
	mov.u32 	%r80, 8;
	shfl.sync.bfly.b32 	%r81|%p7, %r79, %r80, %r75, %r77;
	mov.b32 	%f77, %r81;
	add.f32 	%f78, %f76, %f77;
	mov.b32 	%r82, %f78;
	mov.u32 	%r83, 4;
	shfl.sync.bfly.b32 	%r84|%p8, %r82, %r83, %r75, %r77;
	mov.b32 	%f79, %r84;
	add.f32 	%f80, %f78, %f79;
	mov.b32 	%r85, %f80;
	shfl.sync.bfly.b32 	%r86|%p9, %r85, %r73, %r75, %r77;
	mov.b32 	%f81, %r86;
	add.f32 	%f82, %f80, %f81;
	mov.b32 	%r87, %f82;
	mov.u32 	%r88, 1;
	shfl.sync.bfly.b32 	%r89|%p10, %r87, %r88, %r75, %r77;
	mov.b32 	%f83, %r89;
	add.f32 	%f84, %f82, %f83;
	st.local.f32 	[%rd2], %f84;
	st.shared.f32 	[%r10], %f84;
	bar.sync 	0;
	@%p1 bra 	$L__BB91_11;

	ld.shared.f32 	%f85, [%r4];
	mov.b32 	%r90, %f85;
	shfl.sync.bfly.b32 	%r94|%p12, %r90, %r76, %r75, %r77;
	mov.b32 	%f86, %r94;
	add.f32 	%f87, %f85, %f86;
	mov.b32 	%r95, %f87;
	shfl.sync.bfly.b32 	%r97|%p13, %r95, %r80, %r75, %r77;
	mov.b32 	%f88, %r97;
	add.f32 	%f89, %f87, %f88;
	mov.b32 	%r98, %f89;
	shfl.sync.bfly.b32 	%r100|%p14, %r98, %r83, %r75, %r77;
	mov.b32 	%f90, %r100;
	add.f32 	%f91, %f89, %f90;
	mov.b32 	%r101, %f91;
	shfl.sync.bfly.b32 	%r103|%p15, %r101, %r73, %r75, %r77;
	mov.b32 	%f92, %r103;
	add.f32 	%f93, %f91, %f92;
	mov.b32 	%r104, %f93;
	shfl.sync.bfly.b32 	%r106|%p16, %r104, %r88, %r75, %r77;
	mov.b32 	%f94, %r106;
	add.f32 	%f95, %f93, %f94;
	st.local.f32 	[%rd2], %f95;

$L__BB91_11:
	bar.sync 	0;
	mov.b32 	%r107, %f170;
	shfl.sync.bfly.b32 	%r111|%p18, %r107, %r76, %r75, %r77;
	mov.b32 	%f96, %r111;
	add.f32 	%f97, %f170, %f96;
	mov.b32 	%r112, %f97;
	shfl.sync.bfly.b32 	%r114|%p19, %r112, %r80, %r75, %r77;
	mov.b32 	%f98, %r114;
	add.f32 	%f99, %f97, %f98;
	mov.b32 	%r115, %f99;
	shfl.sync.bfly.b32 	%r117|%p20, %r115, %r83, %r75, %r77;
	mov.b32 	%f100, %r117;
	add.f32 	%f101, %f99, %f100;
	mov.b32 	%r118, %f101;
	shfl.sync.bfly.b32 	%r120|%p21, %r118, %r73, %r75, %r77;
	mov.b32 	%f102, %r120;
	add.f32 	%f103, %f101, %f102;
	mov.b32 	%r121, %f103;
	shfl.sync.bfly.b32 	%r123|%p22, %r121, %r88, %r75, %r77;
	mov.b32 	%f104, %r123;
	add.f32 	%f105, %f103, %f104;
	st.local.f32 	[%rd2+4], %f105;
	st.shared.f32 	[%r10], %f105;
	bar.sync 	0;
	@%p1 bra 	$L__BB91_13;

	ld.shared.f32 	%f106, [%r4];
	mov.b32 	%r124, %f106;
	mov.u32 	%r125, 31;
	mov.u32 	%r126, 16;
	mov.u32 	%r127, -1;
	shfl.sync.bfly.b32 	%r128|%p23, %r124, %r126, %r125, %r127;
	mov.b32 	%f107, %r128;
	add.f32 	%f108, %f106, %f107;
	mov.b32 	%r129, %f108;
	mov.u32 	%r130, 8;
	shfl.sync.bfly.b32 	%r131|%p24, %r129, %r130, %r125, %r127;
	mov.b32 	%f109, %r131;
	add.f32 	%f110, %f108, %f109;
	mov.b32 	%r132, %f110;
	mov.u32 	%r133, 4;
	shfl.sync.bfly.b32 	%r134|%p25, %r132, %r133, %r125, %r127;
	mov.b32 	%f111, %r134;
	add.f32 	%f112, %f110, %f111;
	mov.b32 	%r135, %f112;
	mov.u32 	%r136, 2;
	shfl.sync.bfly.b32 	%r137|%p26, %r135, %r136, %r125, %r127;
	mov.b32 	%f113, %r137;
	add.f32 	%f114, %f112, %f113;
	mov.b32 	%r138, %f114;
	mov.u32 	%r139, 1;
	shfl.sync.bfly.b32 	%r140|%p27, %r138, %r139, %r125, %r127;
	mov.b32 	%f115, %r140;
	add.f32 	%f116, %f114, %f115;
	st.local.f32 	[%rd2+4], %f116;

$L__BB91_13:
	bar.sync 	0;
	mov.b32 	%r141, %f169;
	mov.u32 	%r142, 31;
	mov.u32 	%r143, 16;
	mov.u32 	%r144, -1;
	shfl.sync.bfly.b32 	%r145|%p29, %r141, %r143, %r142, %r144;
	mov.b32 	%f117, %r145;
	add.f32 	%f118, %f169, %f117;
	mov.b32 	%r146, %f118;
	mov.u32 	%r147, 8;
	shfl.sync.bfly.b32 	%r148|%p30, %r146, %r147, %r142, %r144;
	mov.b32 	%f119, %r148;
	add.f32 	%f120, %f118, %f119;
	mov.b32 	%r149, %f120;
	mov.u32 	%r150, 4;
	shfl.sync.bfly.b32 	%r151|%p31, %r149, %r150, %r142, %r144;
	mov.b32 	%f121, %r151;
	add.f32 	%f122, %f120, %f121;
	mov.b32 	%r152, %f122;
	mov.u32 	%r153, 2;
	shfl.sync.bfly.b32 	%r154|%p32, %r152, %r153, %r142, %r144;
	mov.b32 	%f123, %r154;
	add.f32 	%f124, %f122, %f123;
	mov.b32 	%r155, %f124;
	mov.u32 	%r156, 1;
	shfl.sync.bfly.b32 	%r157|%p33, %r155, %r156, %r142, %r144;
	mov.b32 	%f125, %r157;
	add.f32 	%f126, %f124, %f125;
	st.local.f32 	[%rd2+8], %f126;
	st.shared.f32 	[%r10], %f126;
	bar.sync 	0;
	@%p1 bra 	$L__BB91_15;

	ld.shared.f32 	%f127, [%r4];
	mov.b32 	%r158, %f127;
	shfl.sync.bfly.b32 	%r162|%p34, %r158, %r143, %r142, %r144;
	mov.b32 	%f128, %r162;
	add.f32 	%f129, %f127, %f128;
	mov.b32 	%r163, %f129;
	shfl.sync.bfly.b32 	%r165|%p35, %r163, %r147, %r142, %r144;
	mov.b32 	%f130, %r165;
	add.f32 	%f131, %f129, %f130;
	mov.b32 	%r166, %f131;
	shfl.sync.bfly.b32 	%r168|%p36, %r166, %r150, %r142, %r144;
	mov.b32 	%f132, %r168;
	add.f32 	%f133, %f131, %f132;
	mov.b32 	%r169, %f133;
	shfl.sync.bfly.b32 	%r171|%p37, %r169, %r153, %r142, %r144;
	mov.b32 	%f134, %r171;
	add.f32 	%f135, %f133, %f134;
	mov.b32 	%r172, %f135;
	shfl.sync.bfly.b32 	%r174|%p38, %r172, %r156, %r142, %r144;
	mov.b32 	%f136, %r174;
	add.f32 	%f137, %f135, %f136;
	st.local.f32 	[%rd2+8], %f137;

$L__BB91_15:
	bar.sync 	0;
	mov.b32 	%r175, %f168;
	shfl.sync.bfly.b32 	%r179|%p40, %r175, %r143, %r142, %r144;
	mov.b32 	%f138, %r179;
	add.f32 	%f139, %f168, %f138;
	mov.b32 	%r180, %f139;
	shfl.sync.bfly.b32 	%r182|%p41, %r180, %r147, %r142, %r144;
	mov.b32 	%f140, %r182;
	add.f32 	%f141, %f139, %f140;
	mov.b32 	%r183, %f141;
	shfl.sync.bfly.b32 	%r185|%p42, %r183, %r150, %r142, %r144;
	mov.b32 	%f142, %r185;
	add.f32 	%f143, %f141, %f142;
	mov.b32 	%r186, %f143;
	shfl.sync.bfly.b32 	%r188|%p43, %r186, %r153, %r142, %r144;
	mov.b32 	%f144, %r188;
	add.f32 	%f145, %f143, %f144;
	mov.b32 	%r189, %f145;
	shfl.sync.bfly.b32 	%r191|%p44, %r189, %r156, %r142, %r144;
	mov.b32 	%f146, %r191;
	add.f32 	%f147, %f145, %f146;
	st.local.f32 	[%rd2+12], %f147;
	st.shared.f32 	[%r10], %f147;
	bar.sync 	0;
	@%p1 bra 	$L__BB91_17;

	ld.shared.f32 	%f148, [%r4];
	mov.b32 	%r192, %f148;
	mov.u32 	%r193, 31;
	mov.u32 	%r194, 16;
	mov.u32 	%r195, -1;
	shfl.sync.bfly.b32 	%r196|%p45, %r192, %r194, %r193, %r195;
	mov.b32 	%f149, %r196;
	add.f32 	%f150, %f148, %f149;
	mov.b32 	%r197, %f150;
	mov.u32 	%r198, 8;
	shfl.sync.bfly.b32 	%r199|%p46, %r197, %r198, %r193, %r195;
	mov.b32 	%f151, %r199;
	add.f32 	%f152, %f150, %f151;
	mov.b32 	%r200, %f152;
	mov.u32 	%r201, 4;
	shfl.sync.bfly.b32 	%r202|%p47, %r200, %r201, %r193, %r195;
	mov.b32 	%f153, %r202;
	add.f32 	%f154, %f152, %f153;
	mov.b32 	%r203, %f154;
	mov.u32 	%r204, 2;
	shfl.sync.bfly.b32 	%r205|%p48, %r203, %r204, %r193, %r195;
	mov.b32 	%f155, %r205;
	add.f32 	%f156, %f154, %f155;
	mov.b32 	%r206, %f156;
	mov.u32 	%r207, 1;
	shfl.sync.bfly.b32 	%r208|%p49, %r206, %r207, %r193, %r195;
	mov.b32 	%f157, %r208;
	add.f32 	%f158, %f156, %f157;
	st.local.f32 	[%rd2+12], %f158;

$L__BB91_17:
	bar.sync 	0;
	setp.gt.s32 	%p50, %r3, 3;
	@%p50 bra 	$L__BB91_19;

	mad.lo.s32 	%r209, %r3, %r13, %r2;
	cvt.s64.s32 	%rd51, %r209;
	mul.lo.s32 	%r210, %r1, %r14;
	cvt.s64.s32 	%rd52, %r210;
	add.s64 	%rd53, %rd52, %rd51;
	mul.wide.s32 	%rd54, %r3, 4;
	add.s64 	%rd55, %rd2, %rd54;
	ld.local.f32 	%f159, [%rd55];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f159;}

	// end inline asm
	cvta.to.global.u64 	%rd56, %rd18;
	shl.b64 	%rd57, %rd53, 1;
	add.s64 	%rd58, %rd56, %rd57;
	st.global.u16 	[%rd58], %rs1;

$L__BB91_19:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_5_bs_128
.visible .entry ggml_matvec_f16_ncols_5_bs_128(
	.param .u64 ggml_matvec_f16_ncols_5_bs_128_param_0,
	.param .u64 ggml_matvec_f16_ncols_5_bs_128_param_1,
	.param .u64 ggml_matvec_f16_ncols_5_bs_128_param_2,
	.param .u32 ggml_matvec_f16_ncols_5_bs_128_param_3,
	.param .u32 ggml_matvec_f16_ncols_5_bs_128_param_4,
	.param .u32 ggml_matvec_f16_ncols_5_bs_128_param_5,
	.param .u32 ggml_matvec_f16_ncols_5_bs_128_param_6,
	.param .u32 ggml_matvec_f16_ncols_5_bs_128_param_7,
	.param .u32 ggml_matvec_f16_ncols_5_bs_128_param_8,
	.param .u32 ggml_matvec_f16_ncols_5_bs_128_param_9,
	.param .u32 ggml_matvec_f16_ncols_5_bs_128_param_10,
	.param .u32 ggml_matvec_f16_ncols_5_bs_128_param_11
)
{
	.local .align 4 .b8 	__local_depot92[20];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<62>;
	.reg .b16 	%rs<2>;
	.reg .f32 	%f<213>;
	.reg .b32 	%r<257>;
	.reg .b64 	%rd<64>;


	mov.u64 	%SPL, __local_depot92;
	ld.param.u64 	%rd19, [ggml_matvec_f16_ncols_5_bs_128_param_0];
	ld.param.u64 	%rd20, [ggml_matvec_f16_ncols_5_bs_128_param_1];
	ld.param.u64 	%rd18, [ggml_matvec_f16_ncols_5_bs_128_param_2];
	ld.param.u32 	%r11, [ggml_matvec_f16_ncols_5_bs_128_param_3];
	ld.param.u32 	%r15, [ggml_matvec_f16_ncols_5_bs_128_param_5];
	ld.param.u32 	%r12, [ggml_matvec_f16_ncols_5_bs_128_param_6];
	ld.param.u32 	%r13, [ggml_matvec_f16_ncols_5_bs_128_param_7];
	ld.param.u32 	%r16, [ggml_matvec_f16_ncols_5_bs_128_param_8];
	ld.param.u32 	%r17, [ggml_matvec_f16_ncols_5_bs_128_param_9];
	ld.param.u32 	%r18, [ggml_matvec_f16_ncols_5_bs_128_param_10];
	ld.param.u32 	%r14, [ggml_matvec_f16_ncols_5_bs_128_param_11];
	cvta.to.global.u64 	%rd63, %rd20;
	add.u64 	%rd2, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r19, %r1, %r16;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r20, %r2, %r15;
	mad.lo.s32 	%r21, %r19, %r17, %r20;
	cvt.s64.s32 	%rd3, %r21;
	cvta.to.global.u64 	%rd4, %rd19;
	mul.lo.s32 	%r22, %r1, %r18;
	cvt.s64.s32 	%rd5, %r22;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r23, %r3, 2;
	mov.u32 	%r24, data_mmv;
	add.s32 	%r4, %r24, %r23;
	@%p1 bra 	$L__BB92_2;

	mov.u32 	%r25, 0;
	st.shared.u32 	[%r4], %r25;

$L__BB92_2:
	bar.sync 	0;
	mov.f32 	%f208, 0f00000000;
	mov.u32 	%r26, 0;
	st.local.u32 	[%rd2], %r26;
	st.local.u32 	[%rd2+4], %r26;
	st.local.u32 	[%rd2+8], %r26;
	st.local.u32 	[%rd2+12], %r26;
	st.local.u32 	[%rd2+16], %r26;
	setp.ge.s32 	%p2, %r3, %r11;
	mov.f32 	%f209, %f208;
	mov.f32 	%f210, %f208;
	mov.f32 	%f211, %f208;
	mov.f32 	%f212, %f208;
	@%p2 bra 	$L__BB92_9;

	not.b32 	%r27, %r3;
	add.s32 	%r5, %r27, %r11;
	and.b32  	%r28, %r5, 128;
	setp.ne.s32 	%p3, %r28, 0;
	mov.f32 	%f208, 0f00000000;
	mov.u32 	%r256, %r3;
	@%p3 bra 	$L__BB92_5;

	shl.b64 	%rd22, %rd5, 1;
	add.s64 	%rd23, %rd63, %rd22;
	shl.b64 	%rd24, %rd3, 1;
	add.s64 	%rd25, %rd4, %rd24;
	mul.wide.s32 	%rd26, %r3, 4;
	add.s64 	%rd27, %rd25, %rd26;
	ld.global.nc.u32 	%r29, [%rd27];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r29;
  cvt.f32.f16 %f36, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r29;
  cvt.f32.f16 %f37, high;}

	// end inline asm
	add.s64 	%rd28, %rd23, %rd26;
	ld.global.nc.u32 	%r31, [%rd28];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r31;
  cvt.f32.f16 %f38, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r31;
  cvt.f32.f16 %f39, high;}

	// end inline asm
	fma.rn.f32 	%f48, %f36, %f38, 0f00000000;
	fma.rn.f32 	%f212, %f37, %f39, %f48;
	st.local.f32 	[%rd2], %f212;
	mul.wide.s32 	%rd29, %r12, 4;
	add.s64 	%rd30, %rd28, %rd29;
	ld.global.nc.u32 	%r33, [%rd30];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r33;
  cvt.f32.f16 %f40, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r33;
  cvt.f32.f16 %f41, high;}

	// end inline asm
	fma.rn.f32 	%f49, %f36, %f40, 0f00000000;
	fma.rn.f32 	%f211, %f37, %f41, %f49;
	st.local.f32 	[%rd2+4], %f211;
	add.s32 	%r41, %r3, %r12;
	add.s32 	%r42, %r41, %r12;
	shl.b32 	%r43, %r12, 1;
	mul.wide.s32 	%rd31, %r43, 4;
	add.s64 	%rd32, %rd28, %rd31;
	ld.global.nc.u32 	%r35, [%rd32];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r35;
  cvt.f32.f16 %f42, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r35;
  cvt.f32.f16 %f43, high;}

	// end inline asm
	fma.rn.f32 	%f50, %f36, %f42, 0f00000000;
	fma.rn.f32 	%f210, %f37, %f43, %f50;
	st.local.f32 	[%rd2+8], %f210;
	add.s32 	%r44, %r42, %r12;
	mul.wide.s32 	%rd33, %r44, 4;
	add.s64 	%rd34, %rd23, %rd33;
	ld.global.nc.u32 	%r37, [%rd34];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r37;
  cvt.f32.f16 %f44, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r37;
  cvt.f32.f16 %f45, high;}

	// end inline asm
	fma.rn.f32 	%f51, %f36, %f44, 0f00000000;
	fma.rn.f32 	%f209, %f37, %f45, %f51;
	st.local.f32 	[%rd2+12], %f209;
	add.s64 	%rd35, %rd32, %rd31;
	ld.global.nc.u32 	%r39, [%rd35];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r39;
  cvt.f32.f16 %f46, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r39;
  cvt.f32.f16 %f47, high;}

	// end inline asm
	fma.rn.f32 	%f52, %f36, %f46, 0f00000000;
	fma.rn.f32 	%f208, %f37, %f47, %f52;
	st.local.f32 	[%rd2+16], %f208;
	add.s32 	%r256, %r3, 128;

$L__BB92_5:
	and.b32  	%r45, %r5, -128;
	setp.eq.s32 	%p4, %r45, 0;
	@%p4 bra 	$L__BB92_9;

	add.s32 	%r46, %r256, %r12;
	add.s32 	%r47, %r46, 128;
	mul.wide.s32 	%rd36, %r47, 4;
	shl.b64 	%rd37, %rd5, 1;
	add.s64 	%rd7, %rd36, %rd37;
	shl.b32 	%r48, %r12, 1;
	add.s32 	%r49, %r256, %r48;
	mad.lo.s32 	%r50, %r12, 3, %r256;
	shl.b32 	%r51, %r12, 2;
	add.s32 	%r52, %r256, %r51;
	mul.wide.s32 	%rd38, %r49, 4;
	add.s64 	%rd8, %rd38, %rd37;
	mul.wide.s32 	%rd39, %r50, 4;
	add.s64 	%rd9, %rd39, %rd37;
	mul.wide.s32 	%rd40, %r52, 4;
	add.s64 	%rd10, %rd40, %rd37;
	mul.wide.s32 	%rd41, %r256, 2;
	add.s64 	%rd42, %rd41, %rd3;
	shl.b64 	%rd43, %rd42, 1;
	add.s64 	%rd44, %rd4, %rd43;
	add.s64 	%rd62, %rd44, 512;
	mul.wide.s32 	%rd45, %r256, 4;
	mul.wide.s32 	%rd46, %r12, 4;
	add.s64 	%rd47, %rd45, %rd46;
	add.s64 	%rd12, %rd47, %rd37;
	add.s64 	%rd13, %rd45, %rd37;

$L__BB92_7:
	ld.global.nc.u32 	%r53, [%rd62+-512];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r53;
  cvt.f32.f16 %f53, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r53;
  cvt.f32.f16 %f54, high;}

	// end inline asm
	add.s64 	%rd48, %rd63, %rd13;
	ld.global.nc.u32 	%r55, [%rd48];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r55;
  cvt.f32.f16 %f55, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r55;
  cvt.f32.f16 %f56, high;}

	// end inline asm
	fma.rn.f32 	%f77, %f53, %f55, %f212;
	fma.rn.f32 	%f78, %f54, %f56, %f77;
	add.s64 	%rd49, %rd63, %rd12;
	ld.global.nc.u32 	%r57, [%rd49];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r57;
  cvt.f32.f16 %f57, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r57;
  cvt.f32.f16 %f58, high;}

	// end inline asm
	fma.rn.f32 	%f79, %f53, %f57, %f211;
	fma.rn.f32 	%f80, %f54, %f58, %f79;
	add.s64 	%rd50, %rd63, %rd8;
	ld.global.nc.u32 	%r59, [%rd50];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r59;
  cvt.f32.f16 %f59, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r59;
  cvt.f32.f16 %f60, high;}

	// end inline asm
	fma.rn.f32 	%f81, %f53, %f59, %f210;
	fma.rn.f32 	%f82, %f54, %f60, %f81;
	add.s64 	%rd51, %rd63, %rd9;
	ld.global.nc.u32 	%r61, [%rd51];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r61;
  cvt.f32.f16 %f61, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r61;
  cvt.f32.f16 %f62, high;}

	// end inline asm
	fma.rn.f32 	%f83, %f53, %f61, %f209;
	fma.rn.f32 	%f84, %f54, %f62, %f83;
	add.s64 	%rd52, %rd63, %rd10;
	ld.global.nc.u32 	%r63, [%rd52];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r63;
  cvt.f32.f16 %f63, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r63;
  cvt.f32.f16 %f64, high;}

	// end inline asm
	fma.rn.f32 	%f85, %f53, %f63, %f208;
	fma.rn.f32 	%f86, %f54, %f64, %f85;
	ld.global.nc.u32 	%r65, [%rd62];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r65;
  cvt.f32.f16 %f65, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r65;
  cvt.f32.f16 %f66, high;}

	// end inline asm
	ld.global.nc.u32 	%r67, [%rd48+512];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r67;
  cvt.f32.f16 %f67, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r67;
  cvt.f32.f16 %f68, high;}

	// end inline asm
	fma.rn.f32 	%f87, %f65, %f67, %f78;
	fma.rn.f32 	%f212, %f66, %f68, %f87;
	add.s64 	%rd53, %rd63, %rd7;
	ld.global.nc.u32 	%r69, [%rd53];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r69;
  cvt.f32.f16 %f69, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r69;
  cvt.f32.f16 %f70, high;}

	// end inline asm
	fma.rn.f32 	%f88, %f65, %f69, %f80;
	fma.rn.f32 	%f211, %f66, %f70, %f88;
	ld.global.nc.u32 	%r71, [%rd50+512];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r71;
  cvt.f32.f16 %f71, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r71;
  cvt.f32.f16 %f72, high;}

	// end inline asm
	fma.rn.f32 	%f89, %f65, %f71, %f82;
	fma.rn.f32 	%f210, %f66, %f72, %f89;
	ld.global.nc.u32 	%r73, [%rd51+512];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r73;
  cvt.f32.f16 %f73, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r73;
  cvt.f32.f16 %f74, high;}

	// end inline asm
	fma.rn.f32 	%f90, %f65, %f73, %f84;
	fma.rn.f32 	%f209, %f66, %f74, %f90;
	ld.global.nc.u32 	%r75, [%rd52+512];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r75;
  cvt.f32.f16 %f75, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r75;
  cvt.f32.f16 %f76, high;}

	// end inline asm
	fma.rn.f32 	%f91, %f65, %f75, %f86;
	fma.rn.f32 	%f208, %f66, %f76, %f91;
	add.s64 	%rd63, %rd63, 1024;
	add.s64 	%rd62, %rd62, 1024;
	add.s32 	%r256, %r256, 256;
	setp.lt.s32 	%p5, %r256, %r11;
	@%p5 bra 	$L__BB92_7;

	st.local.f32 	[%rd2], %f212;
	st.local.f32 	[%rd2+4], %f211;
	st.local.f32 	[%rd2+8], %f210;
	st.local.f32 	[%rd2+12], %f209;
	st.local.f32 	[%rd2+16], %f208;

$L__BB92_9:
	shr.s32 	%r77, %r3, 31;
	shr.u32 	%r78, %r77, 27;
	add.s32 	%r79, %r3, %r78;
	shr.s32 	%r80, %r79, 5;
	shl.b32 	%r81, %r80, 2;
	add.s32 	%r10, %r24, %r81;
	mov.u32 	%r83, 2;
	mov.b32 	%r84, %f212;
	mov.u32 	%r85, 31;
	mov.u32 	%r86, 16;
	mov.u32 	%r87, -1;
	shfl.sync.bfly.b32 	%r88|%p6, %r84, %r86, %r85, %r87;
	mov.b32 	%f92, %r88;
	add.f32 	%f93, %f212, %f92;
	mov.b32 	%r89, %f93;
	mov.u32 	%r90, 8;
	shfl.sync.bfly.b32 	%r91|%p7, %r89, %r90, %r85, %r87;
	mov.b32 	%f94, %r91;
	add.f32 	%f95, %f93, %f94;
	mov.b32 	%r92, %f95;
	mov.u32 	%r93, 4;
	shfl.sync.bfly.b32 	%r94|%p8, %r92, %r93, %r85, %r87;
	mov.b32 	%f96, %r94;
	add.f32 	%f97, %f95, %f96;
	mov.b32 	%r95, %f97;
	shfl.sync.bfly.b32 	%r96|%p9, %r95, %r83, %r85, %r87;
	mov.b32 	%f98, %r96;
	add.f32 	%f99, %f97, %f98;
	mov.b32 	%r97, %f99;
	mov.u32 	%r98, 1;
	shfl.sync.bfly.b32 	%r99|%p10, %r97, %r98, %r85, %r87;
	mov.b32 	%f100, %r99;
	add.f32 	%f101, %f99, %f100;
	st.local.f32 	[%rd2], %f101;
	st.shared.f32 	[%r10], %f101;
	bar.sync 	0;
	@%p1 bra 	$L__BB92_11;

	ld.shared.f32 	%f102, [%r4];
	mov.b32 	%r100, %f102;
	shfl.sync.bfly.b32 	%r104|%p12, %r100, %r86, %r85, %r87;
	mov.b32 	%f103, %r104;
	add.f32 	%f104, %f102, %f103;
	mov.b32 	%r105, %f104;
	shfl.sync.bfly.b32 	%r107|%p13, %r105, %r90, %r85, %r87;
	mov.b32 	%f105, %r107;
	add.f32 	%f106, %f104, %f105;
	mov.b32 	%r108, %f106;
	shfl.sync.bfly.b32 	%r110|%p14, %r108, %r93, %r85, %r87;
	mov.b32 	%f107, %r110;
	add.f32 	%f108, %f106, %f107;
	mov.b32 	%r111, %f108;
	shfl.sync.bfly.b32 	%r113|%p15, %r111, %r83, %r85, %r87;
	mov.b32 	%f109, %r113;
	add.f32 	%f110, %f108, %f109;
	mov.b32 	%r114, %f110;
	shfl.sync.bfly.b32 	%r116|%p16, %r114, %r98, %r85, %r87;
	mov.b32 	%f111, %r116;
	add.f32 	%f112, %f110, %f111;
	st.local.f32 	[%rd2], %f112;

$L__BB92_11:
	bar.sync 	0;
	mov.b32 	%r117, %f211;
	shfl.sync.bfly.b32 	%r121|%p18, %r117, %r86, %r85, %r87;
	mov.b32 	%f113, %r121;
	add.f32 	%f114, %f211, %f113;
	mov.b32 	%r122, %f114;
	shfl.sync.bfly.b32 	%r124|%p19, %r122, %r90, %r85, %r87;
	mov.b32 	%f115, %r124;
	add.f32 	%f116, %f114, %f115;
	mov.b32 	%r125, %f116;
	shfl.sync.bfly.b32 	%r127|%p20, %r125, %r93, %r85, %r87;
	mov.b32 	%f117, %r127;
	add.f32 	%f118, %f116, %f117;
	mov.b32 	%r128, %f118;
	shfl.sync.bfly.b32 	%r130|%p21, %r128, %r83, %r85, %r87;
	mov.b32 	%f119, %r130;
	add.f32 	%f120, %f118, %f119;
	mov.b32 	%r131, %f120;
	shfl.sync.bfly.b32 	%r133|%p22, %r131, %r98, %r85, %r87;
	mov.b32 	%f121, %r133;
	add.f32 	%f122, %f120, %f121;
	st.local.f32 	[%rd2+4], %f122;
	st.shared.f32 	[%r10], %f122;
	bar.sync 	0;
	@%p1 bra 	$L__BB92_13;

	ld.shared.f32 	%f123, [%r4];
	mov.b32 	%r134, %f123;
	mov.u32 	%r135, 31;
	mov.u32 	%r136, 16;
	mov.u32 	%r137, -1;
	shfl.sync.bfly.b32 	%r138|%p23, %r134, %r136, %r135, %r137;
	mov.b32 	%f124, %r138;
	add.f32 	%f125, %f123, %f124;
	mov.b32 	%r139, %f125;
	mov.u32 	%r140, 8;
	shfl.sync.bfly.b32 	%r141|%p24, %r139, %r140, %r135, %r137;
	mov.b32 	%f126, %r141;
	add.f32 	%f127, %f125, %f126;
	mov.b32 	%r142, %f127;
	mov.u32 	%r143, 4;
	shfl.sync.bfly.b32 	%r144|%p25, %r142, %r143, %r135, %r137;
	mov.b32 	%f128, %r144;
	add.f32 	%f129, %f127, %f128;
	mov.b32 	%r145, %f129;
	mov.u32 	%r146, 2;
	shfl.sync.bfly.b32 	%r147|%p26, %r145, %r146, %r135, %r137;
	mov.b32 	%f130, %r147;
	add.f32 	%f131, %f129, %f130;
	mov.b32 	%r148, %f131;
	mov.u32 	%r149, 1;
	shfl.sync.bfly.b32 	%r150|%p27, %r148, %r149, %r135, %r137;
	mov.b32 	%f132, %r150;
	add.f32 	%f133, %f131, %f132;
	st.local.f32 	[%rd2+4], %f133;

$L__BB92_13:
	bar.sync 	0;
	mov.b32 	%r151, %f210;
	mov.u32 	%r152, 31;
	mov.u32 	%r153, 16;
	mov.u32 	%r154, -1;
	shfl.sync.bfly.b32 	%r155|%p29, %r151, %r153, %r152, %r154;
	mov.b32 	%f134, %r155;
	add.f32 	%f135, %f210, %f134;
	mov.b32 	%r156, %f135;
	mov.u32 	%r157, 8;
	shfl.sync.bfly.b32 	%r158|%p30, %r156, %r157, %r152, %r154;
	mov.b32 	%f136, %r158;
	add.f32 	%f137, %f135, %f136;
	mov.b32 	%r159, %f137;
	mov.u32 	%r160, 4;
	shfl.sync.bfly.b32 	%r161|%p31, %r159, %r160, %r152, %r154;
	mov.b32 	%f138, %r161;
	add.f32 	%f139, %f137, %f138;
	mov.b32 	%r162, %f139;
	mov.u32 	%r163, 2;
	shfl.sync.bfly.b32 	%r164|%p32, %r162, %r163, %r152, %r154;
	mov.b32 	%f140, %r164;
	add.f32 	%f141, %f139, %f140;
	mov.b32 	%r165, %f141;
	mov.u32 	%r166, 1;
	shfl.sync.bfly.b32 	%r167|%p33, %r165, %r166, %r152, %r154;
	mov.b32 	%f142, %r167;
	add.f32 	%f143, %f141, %f142;
	st.local.f32 	[%rd2+8], %f143;
	st.shared.f32 	[%r10], %f143;
	bar.sync 	0;
	@%p1 bra 	$L__BB92_15;

	ld.shared.f32 	%f144, [%r4];
	mov.b32 	%r168, %f144;
	shfl.sync.bfly.b32 	%r172|%p34, %r168, %r153, %r152, %r154;
	mov.b32 	%f145, %r172;
	add.f32 	%f146, %f144, %f145;
	mov.b32 	%r173, %f146;
	shfl.sync.bfly.b32 	%r175|%p35, %r173, %r157, %r152, %r154;
	mov.b32 	%f147, %r175;
	add.f32 	%f148, %f146, %f147;
	mov.b32 	%r176, %f148;
	shfl.sync.bfly.b32 	%r178|%p36, %r176, %r160, %r152, %r154;
	mov.b32 	%f149, %r178;
	add.f32 	%f150, %f148, %f149;
	mov.b32 	%r179, %f150;
	shfl.sync.bfly.b32 	%r181|%p37, %r179, %r163, %r152, %r154;
	mov.b32 	%f151, %r181;
	add.f32 	%f152, %f150, %f151;
	mov.b32 	%r182, %f152;
	shfl.sync.bfly.b32 	%r184|%p38, %r182, %r166, %r152, %r154;
	mov.b32 	%f153, %r184;
	add.f32 	%f154, %f152, %f153;
	st.local.f32 	[%rd2+8], %f154;

$L__BB92_15:
	bar.sync 	0;
	mov.b32 	%r185, %f209;
	shfl.sync.bfly.b32 	%r189|%p40, %r185, %r153, %r152, %r154;
	mov.b32 	%f155, %r189;
	add.f32 	%f156, %f209, %f155;
	mov.b32 	%r190, %f156;
	shfl.sync.bfly.b32 	%r192|%p41, %r190, %r157, %r152, %r154;
	mov.b32 	%f157, %r192;
	add.f32 	%f158, %f156, %f157;
	mov.b32 	%r193, %f158;
	shfl.sync.bfly.b32 	%r195|%p42, %r193, %r160, %r152, %r154;
	mov.b32 	%f159, %r195;
	add.f32 	%f160, %f158, %f159;
	mov.b32 	%r196, %f160;
	shfl.sync.bfly.b32 	%r198|%p43, %r196, %r163, %r152, %r154;
	mov.b32 	%f161, %r198;
	add.f32 	%f162, %f160, %f161;
	mov.b32 	%r199, %f162;
	shfl.sync.bfly.b32 	%r201|%p44, %r199, %r166, %r152, %r154;
	mov.b32 	%f163, %r201;
	add.f32 	%f164, %f162, %f163;
	st.local.f32 	[%rd2+12], %f164;
	st.shared.f32 	[%r10], %f164;
	bar.sync 	0;
	@%p1 bra 	$L__BB92_17;

	ld.shared.f32 	%f165, [%r4];
	mov.b32 	%r202, %f165;
	mov.u32 	%r203, 31;
	mov.u32 	%r204, 16;
	mov.u32 	%r205, -1;
	shfl.sync.bfly.b32 	%r206|%p45, %r202, %r204, %r203, %r205;
	mov.b32 	%f166, %r206;
	add.f32 	%f167, %f165, %f166;
	mov.b32 	%r207, %f167;
	mov.u32 	%r208, 8;
	shfl.sync.bfly.b32 	%r209|%p46, %r207, %r208, %r203, %r205;
	mov.b32 	%f168, %r209;
	add.f32 	%f169, %f167, %f168;
	mov.b32 	%r210, %f169;
	mov.u32 	%r211, 4;
	shfl.sync.bfly.b32 	%r212|%p47, %r210, %r211, %r203, %r205;
	mov.b32 	%f170, %r212;
	add.f32 	%f171, %f169, %f170;
	mov.b32 	%r213, %f171;
	mov.u32 	%r214, 2;
	shfl.sync.bfly.b32 	%r215|%p48, %r213, %r214, %r203, %r205;
	mov.b32 	%f172, %r215;
	add.f32 	%f173, %f171, %f172;
	mov.b32 	%r216, %f173;
	mov.u32 	%r217, 1;
	shfl.sync.bfly.b32 	%r218|%p49, %r216, %r217, %r203, %r205;
	mov.b32 	%f174, %r218;
	add.f32 	%f175, %f173, %f174;
	st.local.f32 	[%rd2+12], %f175;

$L__BB92_17:
	bar.sync 	0;
	mov.b32 	%r219, %f208;
	mov.u32 	%r220, 31;
	mov.u32 	%r221, 16;
	mov.u32 	%r222, -1;
	shfl.sync.bfly.b32 	%r223|%p51, %r219, %r221, %r220, %r222;
	mov.b32 	%f176, %r223;
	add.f32 	%f177, %f208, %f176;
	mov.b32 	%r224, %f177;
	mov.u32 	%r225, 8;
	shfl.sync.bfly.b32 	%r226|%p52, %r224, %r225, %r220, %r222;
	mov.b32 	%f178, %r226;
	add.f32 	%f179, %f177, %f178;
	mov.b32 	%r227, %f179;
	mov.u32 	%r228, 4;
	shfl.sync.bfly.b32 	%r229|%p53, %r227, %r228, %r220, %r222;
	mov.b32 	%f180, %r229;
	add.f32 	%f181, %f179, %f180;
	mov.b32 	%r230, %f181;
	mov.u32 	%r231, 2;
	shfl.sync.bfly.b32 	%r232|%p54, %r230, %r231, %r220, %r222;
	mov.b32 	%f182, %r232;
	add.f32 	%f183, %f181, %f182;
	mov.b32 	%r233, %f183;
	mov.u32 	%r234, 1;
	shfl.sync.bfly.b32 	%r235|%p55, %r233, %r234, %r220, %r222;
	mov.b32 	%f184, %r235;
	add.f32 	%f185, %f183, %f184;
	st.local.f32 	[%rd2+16], %f185;
	st.shared.f32 	[%r10], %f185;
	bar.sync 	0;
	@%p1 bra 	$L__BB92_19;

	ld.shared.f32 	%f186, [%r4];
	mov.b32 	%r236, %f186;
	shfl.sync.bfly.b32 	%r240|%p56, %r236, %r221, %r220, %r222;
	mov.b32 	%f187, %r240;
	add.f32 	%f188, %f186, %f187;
	mov.b32 	%r241, %f188;
	shfl.sync.bfly.b32 	%r243|%p57, %r241, %r225, %r220, %r222;
	mov.b32 	%f189, %r243;
	add.f32 	%f190, %f188, %f189;
	mov.b32 	%r244, %f190;
	shfl.sync.bfly.b32 	%r246|%p58, %r244, %r228, %r220, %r222;
	mov.b32 	%f191, %r246;
	add.f32 	%f192, %f190, %f191;
	mov.b32 	%r247, %f192;
	shfl.sync.bfly.b32 	%r249|%p59, %r247, %r231, %r220, %r222;
	mov.b32 	%f193, %r249;
	add.f32 	%f194, %f192, %f193;
	mov.b32 	%r250, %f194;
	shfl.sync.bfly.b32 	%r252|%p60, %r250, %r234, %r220, %r222;
	mov.b32 	%f195, %r252;
	add.f32 	%f196, %f194, %f195;
	st.local.f32 	[%rd2+16], %f196;

$L__BB92_19:
	bar.sync 	0;
	setp.gt.s32 	%p61, %r3, 4;
	@%p61 bra 	$L__BB92_21;

	mad.lo.s32 	%r253, %r3, %r13, %r2;
	cvt.s64.s32 	%rd54, %r253;
	mul.lo.s32 	%r254, %r1, %r14;
	cvt.s64.s32 	%rd55, %r254;
	add.s64 	%rd56, %rd55, %rd54;
	mul.wide.s32 	%rd57, %r3, 4;
	add.s64 	%rd58, %rd2, %rd57;
	ld.local.f32 	%f197, [%rd58];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f197;}

	// end inline asm
	cvta.to.global.u64 	%rd59, %rd18;
	shl.b64 	%rd60, %rd56, 1;
	add.s64 	%rd61, %rd59, %rd60;
	st.global.u16 	[%rd61], %rs1;

$L__BB92_21:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_6_bs_128
.visible .entry ggml_matvec_f16_ncols_6_bs_128(
	.param .u64 ggml_matvec_f16_ncols_6_bs_128_param_0,
	.param .u64 ggml_matvec_f16_ncols_6_bs_128_param_1,
	.param .u64 ggml_matvec_f16_ncols_6_bs_128_param_2,
	.param .u32 ggml_matvec_f16_ncols_6_bs_128_param_3,
	.param .u32 ggml_matvec_f16_ncols_6_bs_128_param_4,
	.param .u32 ggml_matvec_f16_ncols_6_bs_128_param_5,
	.param .u32 ggml_matvec_f16_ncols_6_bs_128_param_6,
	.param .u32 ggml_matvec_f16_ncols_6_bs_128_param_7,
	.param .u32 ggml_matvec_f16_ncols_6_bs_128_param_8,
	.param .u32 ggml_matvec_f16_ncols_6_bs_128_param_9,
	.param .u32 ggml_matvec_f16_ncols_6_bs_128_param_10,
	.param .u32 ggml_matvec_f16_ncols_6_bs_128_param_11
)
{
	.local .align 8 .b8 	__local_depot93[24];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<73>;
	.reg .b16 	%rs<2>;
	.reg .f32 	%f<254>;
	.reg .b32 	%r<295>;
	.reg .b64 	%rd<67>;


	mov.u64 	%SPL, __local_depot93;
	ld.param.u64 	%rd20, [ggml_matvec_f16_ncols_6_bs_128_param_0];
	ld.param.u64 	%rd21, [ggml_matvec_f16_ncols_6_bs_128_param_1];
	ld.param.u64 	%rd19, [ggml_matvec_f16_ncols_6_bs_128_param_2];
	ld.param.u32 	%r11, [ggml_matvec_f16_ncols_6_bs_128_param_3];
	ld.param.u32 	%r15, [ggml_matvec_f16_ncols_6_bs_128_param_5];
	ld.param.u32 	%r12, [ggml_matvec_f16_ncols_6_bs_128_param_6];
	ld.param.u32 	%r13, [ggml_matvec_f16_ncols_6_bs_128_param_7];
	ld.param.u32 	%r16, [ggml_matvec_f16_ncols_6_bs_128_param_8];
	ld.param.u32 	%r17, [ggml_matvec_f16_ncols_6_bs_128_param_9];
	ld.param.u32 	%r18, [ggml_matvec_f16_ncols_6_bs_128_param_10];
	ld.param.u32 	%r14, [ggml_matvec_f16_ncols_6_bs_128_param_11];
	cvta.to.global.u64 	%rd66, %rd21;
	add.u64 	%rd2, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r19, %r1, %r16;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r20, %r2, %r15;
	mad.lo.s32 	%r21, %r19, %r17, %r20;
	cvt.s64.s32 	%rd3, %r21;
	cvta.to.global.u64 	%rd4, %rd20;
	mul.lo.s32 	%r22, %r1, %r18;
	cvt.s64.s32 	%rd5, %r22;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r23, %r3, 2;
	mov.u32 	%r24, data_mmv;
	add.s32 	%r4, %r24, %r23;
	@%p1 bra 	$L__BB93_2;

	mov.u32 	%r25, 0;
	st.shared.u32 	[%r4], %r25;

$L__BB93_2:
	bar.sync 	0;
	mov.f32 	%f248, 0f00000000;
	st.local.v2.f32 	[%rd2], {%f248, %f248};
	st.local.v2.f32 	[%rd2+8], {%f248, %f248};
	st.local.v2.f32 	[%rd2+16], {%f248, %f248};
	setp.ge.s32 	%p2, %r3, %r11;
	mov.f32 	%f249, %f248;
	mov.f32 	%f250, %f248;
	mov.f32 	%f251, %f248;
	mov.f32 	%f252, %f248;
	mov.f32 	%f253, %f248;
	@%p2 bra 	$L__BB93_9;

	not.b32 	%r26, %r3;
	add.s32 	%r5, %r26, %r11;
	and.b32  	%r27, %r5, 128;
	setp.ne.s32 	%p3, %r27, 0;
	mov.f32 	%f248, 0f00000000;
	mov.u32 	%r294, %r3;
	@%p3 bra 	$L__BB93_5;

	shl.b64 	%rd23, %rd5, 1;
	add.s64 	%rd24, %rd66, %rd23;
	shl.b64 	%rd25, %rd3, 1;
	add.s64 	%rd26, %rd4, %rd25;
	mul.wide.s32 	%rd27, %r3, 4;
	add.s64 	%rd28, %rd26, %rd27;
	ld.global.nc.u32 	%r28, [%rd28];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r28;
  cvt.f32.f16 %f43, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r28;
  cvt.f32.f16 %f44, high;}

	// end inline asm
	add.s64 	%rd29, %rd24, %rd27;
	ld.global.nc.u32 	%r30, [%rd29];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r30;
  cvt.f32.f16 %f45, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r30;
  cvt.f32.f16 %f46, high;}

	// end inline asm
	fma.rn.f32 	%f57, %f43, %f45, 0f00000000;
	fma.rn.f32 	%f253, %f44, %f46, %f57;
	st.local.f32 	[%rd2], %f253;
	mul.wide.s32 	%rd30, %r12, 4;
	add.s64 	%rd31, %rd29, %rd30;
	ld.global.nc.u32 	%r32, [%rd31];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r32;
  cvt.f32.f16 %f47, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r32;
  cvt.f32.f16 %f48, high;}

	// end inline asm
	fma.rn.f32 	%f58, %f43, %f47, 0f00000000;
	fma.rn.f32 	%f252, %f44, %f48, %f58;
	st.local.f32 	[%rd2+4], %f252;
	add.s32 	%r42, %r3, %r12;
	add.s32 	%r43, %r42, %r12;
	mul.wide.s32 	%rd32, %r43, 4;
	add.s64 	%rd33, %rd24, %rd32;
	ld.global.nc.u32 	%r34, [%rd33];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r34;
  cvt.f32.f16 %f49, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r34;
  cvt.f32.f16 %f50, high;}

	// end inline asm
	fma.rn.f32 	%f59, %f43, %f49, 0f00000000;
	fma.rn.f32 	%f251, %f44, %f50, %f59;
	st.local.f32 	[%rd2+8], %f251;
	add.s64 	%rd34, %rd33, %rd30;
	ld.global.nc.u32 	%r36, [%rd34];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r36;
  cvt.f32.f16 %f51, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r36;
  cvt.f32.f16 %f52, high;}

	// end inline asm
	fma.rn.f32 	%f60, %f43, %f51, 0f00000000;
	fma.rn.f32 	%f250, %f44, %f52, %f60;
	st.local.f32 	[%rd2+12], %f250;
	add.s64 	%rd35, %rd34, %rd30;
	ld.global.nc.u32 	%r38, [%rd35];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r38;
  cvt.f32.f16 %f53, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r38;
  cvt.f32.f16 %f54, high;}

	// end inline asm
	fma.rn.f32 	%f61, %f43, %f53, 0f00000000;
	fma.rn.f32 	%f249, %f44, %f54, %f61;
	st.local.f32 	[%rd2+16], %f249;
	add.s64 	%rd36, %rd35, %rd30;
	ld.global.nc.u32 	%r40, [%rd36];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r40;
  cvt.f32.f16 %f55, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r40;
  cvt.f32.f16 %f56, high;}

	// end inline asm
	fma.rn.f32 	%f62, %f43, %f55, 0f00000000;
	fma.rn.f32 	%f248, %f44, %f56, %f62;
	st.local.f32 	[%rd2+20], %f248;
	add.s32 	%r294, %r3, 128;

$L__BB93_5:
	and.b32  	%r44, %r5, -128;
	setp.eq.s32 	%p4, %r44, 0;
	@%p4 bra 	$L__BB93_9;

	add.s32 	%r45, %r294, %r12;
	add.s32 	%r46, %r45, 128;
	mul.wide.s32 	%rd37, %r46, 4;
	shl.b64 	%rd38, %rd5, 1;
	add.s64 	%rd7, %rd37, %rd38;
	shl.b32 	%r47, %r12, 1;
	add.s32 	%r48, %r294, %r47;
	mad.lo.s32 	%r49, %r12, 3, %r294;
	shl.b32 	%r50, %r12, 2;
	add.s32 	%r51, %r294, %r50;
	mad.lo.s32 	%r52, %r12, 5, %r294;
	mul.wide.s32 	%rd39, %r48, 4;
	add.s64 	%rd8, %rd39, %rd38;
	mul.wide.s32 	%rd40, %r49, 4;
	add.s64 	%rd9, %rd40, %rd38;
	mul.wide.s32 	%rd41, %r51, 4;
	add.s64 	%rd10, %rd41, %rd38;
	mul.wide.s32 	%rd42, %r52, 4;
	add.s64 	%rd11, %rd42, %rd38;
	mul.wide.s32 	%rd43, %r294, 2;
	add.s64 	%rd44, %rd43, %rd3;
	shl.b64 	%rd45, %rd44, 1;
	add.s64 	%rd46, %rd4, %rd45;
	add.s64 	%rd65, %rd46, 512;
	mul.wide.s32 	%rd47, %r294, 4;
	mul.wide.s32 	%rd48, %r12, 4;
	add.s64 	%rd49, %rd47, %rd48;
	add.s64 	%rd13, %rd49, %rd38;
	add.s64 	%rd14, %rd47, %rd38;

$L__BB93_7:
	ld.global.nc.u32 	%r53, [%rd65+-512];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r53;
  cvt.f32.f16 %f63, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r53;
  cvt.f32.f16 %f64, high;}

	// end inline asm
	add.s64 	%rd50, %rd66, %rd14;
	ld.global.nc.u32 	%r55, [%rd50];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r55;
  cvt.f32.f16 %f65, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r55;
  cvt.f32.f16 %f66, high;}

	// end inline asm
	fma.rn.f32 	%f91, %f63, %f65, %f253;
	fma.rn.f32 	%f92, %f64, %f66, %f91;
	add.s64 	%rd51, %rd66, %rd13;
	ld.global.nc.u32 	%r57, [%rd51];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r57;
  cvt.f32.f16 %f67, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r57;
  cvt.f32.f16 %f68, high;}

	// end inline asm
	fma.rn.f32 	%f93, %f63, %f67, %f252;
	fma.rn.f32 	%f94, %f64, %f68, %f93;
	add.s64 	%rd52, %rd66, %rd8;
	ld.global.nc.u32 	%r59, [%rd52];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r59;
  cvt.f32.f16 %f69, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r59;
  cvt.f32.f16 %f70, high;}

	// end inline asm
	fma.rn.f32 	%f95, %f63, %f69, %f251;
	fma.rn.f32 	%f96, %f64, %f70, %f95;
	add.s64 	%rd53, %rd66, %rd9;
	ld.global.nc.u32 	%r61, [%rd53];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r61;
  cvt.f32.f16 %f71, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r61;
  cvt.f32.f16 %f72, high;}

	// end inline asm
	fma.rn.f32 	%f97, %f63, %f71, %f250;
	fma.rn.f32 	%f98, %f64, %f72, %f97;
	add.s64 	%rd54, %rd66, %rd10;
	ld.global.nc.u32 	%r63, [%rd54];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r63;
  cvt.f32.f16 %f73, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r63;
  cvt.f32.f16 %f74, high;}

	// end inline asm
	fma.rn.f32 	%f99, %f63, %f73, %f249;
	fma.rn.f32 	%f100, %f64, %f74, %f99;
	add.s64 	%rd55, %rd66, %rd11;
	ld.global.nc.u32 	%r65, [%rd55];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r65;
  cvt.f32.f16 %f75, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r65;
  cvt.f32.f16 %f76, high;}

	// end inline asm
	fma.rn.f32 	%f101, %f63, %f75, %f248;
	fma.rn.f32 	%f102, %f64, %f76, %f101;
	ld.global.nc.u32 	%r67, [%rd65];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r67;
  cvt.f32.f16 %f77, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r67;
  cvt.f32.f16 %f78, high;}

	// end inline asm
	ld.global.nc.u32 	%r69, [%rd50+512];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r69;
  cvt.f32.f16 %f79, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r69;
  cvt.f32.f16 %f80, high;}

	// end inline asm
	fma.rn.f32 	%f103, %f77, %f79, %f92;
	fma.rn.f32 	%f253, %f78, %f80, %f103;
	add.s64 	%rd56, %rd66, %rd7;
	ld.global.nc.u32 	%r71, [%rd56];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r71;
  cvt.f32.f16 %f81, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r71;
  cvt.f32.f16 %f82, high;}

	// end inline asm
	fma.rn.f32 	%f104, %f77, %f81, %f94;
	fma.rn.f32 	%f252, %f78, %f82, %f104;
	ld.global.nc.u32 	%r73, [%rd52+512];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r73;
  cvt.f32.f16 %f83, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r73;
  cvt.f32.f16 %f84, high;}

	// end inline asm
	fma.rn.f32 	%f105, %f77, %f83, %f96;
	fma.rn.f32 	%f251, %f78, %f84, %f105;
	ld.global.nc.u32 	%r75, [%rd53+512];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r75;
  cvt.f32.f16 %f85, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r75;
  cvt.f32.f16 %f86, high;}

	// end inline asm
	fma.rn.f32 	%f106, %f77, %f85, %f98;
	fma.rn.f32 	%f250, %f78, %f86, %f106;
	ld.global.nc.u32 	%r77, [%rd54+512];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r77;
  cvt.f32.f16 %f87, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r77;
  cvt.f32.f16 %f88, high;}

	// end inline asm
	fma.rn.f32 	%f107, %f77, %f87, %f100;
	fma.rn.f32 	%f249, %f78, %f88, %f107;
	ld.global.nc.u32 	%r79, [%rd55+512];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r79;
  cvt.f32.f16 %f89, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r79;
  cvt.f32.f16 %f90, high;}

	// end inline asm
	fma.rn.f32 	%f108, %f77, %f89, %f102;
	fma.rn.f32 	%f248, %f78, %f90, %f108;
	add.s64 	%rd66, %rd66, 1024;
	add.s64 	%rd65, %rd65, 1024;
	add.s32 	%r294, %r294, 256;
	setp.lt.s32 	%p5, %r294, %r11;
	@%p5 bra 	$L__BB93_7;

	st.local.v2.f32 	[%rd2], {%f253, %f252};
	st.local.v2.f32 	[%rd2+8], {%f251, %f250};
	st.local.v2.f32 	[%rd2+16], {%f249, %f248};

$L__BB93_9:
	shr.s32 	%r81, %r3, 31;
	shr.u32 	%r82, %r81, 27;
	add.s32 	%r83, %r3, %r82;
	shr.s32 	%r84, %r83, 5;
	shl.b32 	%r85, %r84, 2;
	add.s32 	%r10, %r24, %r85;
	mov.u32 	%r87, 2;
	mov.b32 	%r88, %f253;
	mov.u32 	%r89, 31;
	mov.u32 	%r90, 16;
	mov.u32 	%r91, -1;
	shfl.sync.bfly.b32 	%r92|%p6, %r88, %r90, %r89, %r91;
	mov.b32 	%f109, %r92;
	add.f32 	%f110, %f253, %f109;
	mov.b32 	%r93, %f110;
	mov.u32 	%r94, 8;
	shfl.sync.bfly.b32 	%r95|%p7, %r93, %r94, %r89, %r91;
	mov.b32 	%f111, %r95;
	add.f32 	%f112, %f110, %f111;
	mov.b32 	%r96, %f112;
	mov.u32 	%r97, 4;
	shfl.sync.bfly.b32 	%r98|%p8, %r96, %r97, %r89, %r91;
	mov.b32 	%f113, %r98;
	add.f32 	%f114, %f112, %f113;
	mov.b32 	%r99, %f114;
	shfl.sync.bfly.b32 	%r100|%p9, %r99, %r87, %r89, %r91;
	mov.b32 	%f115, %r100;
	add.f32 	%f116, %f114, %f115;
	mov.b32 	%r101, %f116;
	mov.u32 	%r102, 1;
	shfl.sync.bfly.b32 	%r103|%p10, %r101, %r102, %r89, %r91;
	mov.b32 	%f117, %r103;
	add.f32 	%f118, %f116, %f117;
	st.local.f32 	[%rd2], %f118;
	st.shared.f32 	[%r10], %f118;
	bar.sync 	0;
	@%p1 bra 	$L__BB93_11;

	ld.shared.f32 	%f119, [%r4];
	mov.b32 	%r104, %f119;
	shfl.sync.bfly.b32 	%r108|%p12, %r104, %r90, %r89, %r91;
	mov.b32 	%f120, %r108;
	add.f32 	%f121, %f119, %f120;
	mov.b32 	%r109, %f121;
	shfl.sync.bfly.b32 	%r111|%p13, %r109, %r94, %r89, %r91;
	mov.b32 	%f122, %r111;
	add.f32 	%f123, %f121, %f122;
	mov.b32 	%r112, %f123;
	shfl.sync.bfly.b32 	%r114|%p14, %r112, %r97, %r89, %r91;
	mov.b32 	%f124, %r114;
	add.f32 	%f125, %f123, %f124;
	mov.b32 	%r115, %f125;
	shfl.sync.bfly.b32 	%r117|%p15, %r115, %r87, %r89, %r91;
	mov.b32 	%f126, %r117;
	add.f32 	%f127, %f125, %f126;
	mov.b32 	%r118, %f127;
	shfl.sync.bfly.b32 	%r120|%p16, %r118, %r102, %r89, %r91;
	mov.b32 	%f128, %r120;
	add.f32 	%f129, %f127, %f128;
	st.local.f32 	[%rd2], %f129;

$L__BB93_11:
	bar.sync 	0;
	mov.b32 	%r121, %f252;
	shfl.sync.bfly.b32 	%r125|%p18, %r121, %r90, %r89, %r91;
	mov.b32 	%f130, %r125;
	add.f32 	%f131, %f252, %f130;
	mov.b32 	%r126, %f131;
	shfl.sync.bfly.b32 	%r128|%p19, %r126, %r94, %r89, %r91;
	mov.b32 	%f132, %r128;
	add.f32 	%f133, %f131, %f132;
	mov.b32 	%r129, %f133;
	shfl.sync.bfly.b32 	%r131|%p20, %r129, %r97, %r89, %r91;
	mov.b32 	%f134, %r131;
	add.f32 	%f135, %f133, %f134;
	mov.b32 	%r132, %f135;
	shfl.sync.bfly.b32 	%r134|%p21, %r132, %r87, %r89, %r91;
	mov.b32 	%f136, %r134;
	add.f32 	%f137, %f135, %f136;
	mov.b32 	%r135, %f137;
	shfl.sync.bfly.b32 	%r137|%p22, %r135, %r102, %r89, %r91;
	mov.b32 	%f138, %r137;
	add.f32 	%f139, %f137, %f138;
	st.local.f32 	[%rd2+4], %f139;
	st.shared.f32 	[%r10], %f139;
	bar.sync 	0;
	@%p1 bra 	$L__BB93_13;

	ld.shared.f32 	%f140, [%r4];
	mov.b32 	%r138, %f140;
	mov.u32 	%r139, 31;
	mov.u32 	%r140, 16;
	mov.u32 	%r141, -1;
	shfl.sync.bfly.b32 	%r142|%p23, %r138, %r140, %r139, %r141;
	mov.b32 	%f141, %r142;
	add.f32 	%f142, %f140, %f141;
	mov.b32 	%r143, %f142;
	mov.u32 	%r144, 8;
	shfl.sync.bfly.b32 	%r145|%p24, %r143, %r144, %r139, %r141;
	mov.b32 	%f143, %r145;
	add.f32 	%f144, %f142, %f143;
	mov.b32 	%r146, %f144;
	mov.u32 	%r147, 4;
	shfl.sync.bfly.b32 	%r148|%p25, %r146, %r147, %r139, %r141;
	mov.b32 	%f145, %r148;
	add.f32 	%f146, %f144, %f145;
	mov.b32 	%r149, %f146;
	mov.u32 	%r150, 2;
	shfl.sync.bfly.b32 	%r151|%p26, %r149, %r150, %r139, %r141;
	mov.b32 	%f147, %r151;
	add.f32 	%f148, %f146, %f147;
	mov.b32 	%r152, %f148;
	mov.u32 	%r153, 1;
	shfl.sync.bfly.b32 	%r154|%p27, %r152, %r153, %r139, %r141;
	mov.b32 	%f149, %r154;
	add.f32 	%f150, %f148, %f149;
	st.local.f32 	[%rd2+4], %f150;

$L__BB93_13:
	bar.sync 	0;
	mov.b32 	%r155, %f251;
	mov.u32 	%r156, 31;
	mov.u32 	%r157, 16;
	mov.u32 	%r158, -1;
	shfl.sync.bfly.b32 	%r159|%p29, %r155, %r157, %r156, %r158;
	mov.b32 	%f151, %r159;
	add.f32 	%f152, %f251, %f151;
	mov.b32 	%r160, %f152;
	mov.u32 	%r161, 8;
	shfl.sync.bfly.b32 	%r162|%p30, %r160, %r161, %r156, %r158;
	mov.b32 	%f153, %r162;
	add.f32 	%f154, %f152, %f153;
	mov.b32 	%r163, %f154;
	mov.u32 	%r164, 4;
	shfl.sync.bfly.b32 	%r165|%p31, %r163, %r164, %r156, %r158;
	mov.b32 	%f155, %r165;
	add.f32 	%f156, %f154, %f155;
	mov.b32 	%r166, %f156;
	mov.u32 	%r167, 2;
	shfl.sync.bfly.b32 	%r168|%p32, %r166, %r167, %r156, %r158;
	mov.b32 	%f157, %r168;
	add.f32 	%f158, %f156, %f157;
	mov.b32 	%r169, %f158;
	mov.u32 	%r170, 1;
	shfl.sync.bfly.b32 	%r171|%p33, %r169, %r170, %r156, %r158;
	mov.b32 	%f159, %r171;
	add.f32 	%f160, %f158, %f159;
	st.local.f32 	[%rd2+8], %f160;
	st.shared.f32 	[%r10], %f160;
	bar.sync 	0;
	@%p1 bra 	$L__BB93_15;

	ld.shared.f32 	%f161, [%r4];
	mov.b32 	%r172, %f161;
	shfl.sync.bfly.b32 	%r176|%p34, %r172, %r157, %r156, %r158;
	mov.b32 	%f162, %r176;
	add.f32 	%f163, %f161, %f162;
	mov.b32 	%r177, %f163;
	shfl.sync.bfly.b32 	%r179|%p35, %r177, %r161, %r156, %r158;
	mov.b32 	%f164, %r179;
	add.f32 	%f165, %f163, %f164;
	mov.b32 	%r180, %f165;
	shfl.sync.bfly.b32 	%r182|%p36, %r180, %r164, %r156, %r158;
	mov.b32 	%f166, %r182;
	add.f32 	%f167, %f165, %f166;
	mov.b32 	%r183, %f167;
	shfl.sync.bfly.b32 	%r185|%p37, %r183, %r167, %r156, %r158;
	mov.b32 	%f168, %r185;
	add.f32 	%f169, %f167, %f168;
	mov.b32 	%r186, %f169;
	shfl.sync.bfly.b32 	%r188|%p38, %r186, %r170, %r156, %r158;
	mov.b32 	%f170, %r188;
	add.f32 	%f171, %f169, %f170;
	st.local.f32 	[%rd2+8], %f171;

$L__BB93_15:
	bar.sync 	0;
	mov.b32 	%r189, %f250;
	shfl.sync.bfly.b32 	%r193|%p40, %r189, %r157, %r156, %r158;
	mov.b32 	%f172, %r193;
	add.f32 	%f173, %f250, %f172;
	mov.b32 	%r194, %f173;
	shfl.sync.bfly.b32 	%r196|%p41, %r194, %r161, %r156, %r158;
	mov.b32 	%f174, %r196;
	add.f32 	%f175, %f173, %f174;
	mov.b32 	%r197, %f175;
	shfl.sync.bfly.b32 	%r199|%p42, %r197, %r164, %r156, %r158;
	mov.b32 	%f176, %r199;
	add.f32 	%f177, %f175, %f176;
	mov.b32 	%r200, %f177;
	shfl.sync.bfly.b32 	%r202|%p43, %r200, %r167, %r156, %r158;
	mov.b32 	%f178, %r202;
	add.f32 	%f179, %f177, %f178;
	mov.b32 	%r203, %f179;
	shfl.sync.bfly.b32 	%r205|%p44, %r203, %r170, %r156, %r158;
	mov.b32 	%f180, %r205;
	add.f32 	%f181, %f179, %f180;
	st.local.f32 	[%rd2+12], %f181;
	st.shared.f32 	[%r10], %f181;
	bar.sync 	0;
	@%p1 bra 	$L__BB93_17;

	ld.shared.f32 	%f182, [%r4];
	mov.b32 	%r206, %f182;
	mov.u32 	%r207, 31;
	mov.u32 	%r208, 16;
	mov.u32 	%r209, -1;
	shfl.sync.bfly.b32 	%r210|%p45, %r206, %r208, %r207, %r209;
	mov.b32 	%f183, %r210;
	add.f32 	%f184, %f182, %f183;
	mov.b32 	%r211, %f184;
	mov.u32 	%r212, 8;
	shfl.sync.bfly.b32 	%r213|%p46, %r211, %r212, %r207, %r209;
	mov.b32 	%f185, %r213;
	add.f32 	%f186, %f184, %f185;
	mov.b32 	%r214, %f186;
	mov.u32 	%r215, 4;
	shfl.sync.bfly.b32 	%r216|%p47, %r214, %r215, %r207, %r209;
	mov.b32 	%f187, %r216;
	add.f32 	%f188, %f186, %f187;
	mov.b32 	%r217, %f188;
	mov.u32 	%r218, 2;
	shfl.sync.bfly.b32 	%r219|%p48, %r217, %r218, %r207, %r209;
	mov.b32 	%f189, %r219;
	add.f32 	%f190, %f188, %f189;
	mov.b32 	%r220, %f190;
	mov.u32 	%r221, 1;
	shfl.sync.bfly.b32 	%r222|%p49, %r220, %r221, %r207, %r209;
	mov.b32 	%f191, %r222;
	add.f32 	%f192, %f190, %f191;
	st.local.f32 	[%rd2+12], %f192;

$L__BB93_17:
	bar.sync 	0;
	mov.b32 	%r223, %f249;
	mov.u32 	%r224, 31;
	mov.u32 	%r225, 16;
	mov.u32 	%r226, -1;
	shfl.sync.bfly.b32 	%r227|%p51, %r223, %r225, %r224, %r226;
	mov.b32 	%f193, %r227;
	add.f32 	%f194, %f249, %f193;
	mov.b32 	%r228, %f194;
	mov.u32 	%r229, 8;
	shfl.sync.bfly.b32 	%r230|%p52, %r228, %r229, %r224, %r226;
	mov.b32 	%f195, %r230;
	add.f32 	%f196, %f194, %f195;
	mov.b32 	%r231, %f196;
	mov.u32 	%r232, 4;
	shfl.sync.bfly.b32 	%r233|%p53, %r231, %r232, %r224, %r226;
	mov.b32 	%f197, %r233;
	add.f32 	%f198, %f196, %f197;
	mov.b32 	%r234, %f198;
	mov.u32 	%r235, 2;
	shfl.sync.bfly.b32 	%r236|%p54, %r234, %r235, %r224, %r226;
	mov.b32 	%f199, %r236;
	add.f32 	%f200, %f198, %f199;
	mov.b32 	%r237, %f200;
	mov.u32 	%r238, 1;
	shfl.sync.bfly.b32 	%r239|%p55, %r237, %r238, %r224, %r226;
	mov.b32 	%f201, %r239;
	add.f32 	%f202, %f200, %f201;
	st.local.f32 	[%rd2+16], %f202;
	st.shared.f32 	[%r10], %f202;
	bar.sync 	0;
	@%p1 bra 	$L__BB93_19;

	ld.shared.f32 	%f203, [%r4];
	mov.b32 	%r240, %f203;
	shfl.sync.bfly.b32 	%r244|%p56, %r240, %r225, %r224, %r226;
	mov.b32 	%f204, %r244;
	add.f32 	%f205, %f203, %f204;
	mov.b32 	%r245, %f205;
	shfl.sync.bfly.b32 	%r247|%p57, %r245, %r229, %r224, %r226;
	mov.b32 	%f206, %r247;
	add.f32 	%f207, %f205, %f206;
	mov.b32 	%r248, %f207;
	shfl.sync.bfly.b32 	%r250|%p58, %r248, %r232, %r224, %r226;
	mov.b32 	%f208, %r250;
	add.f32 	%f209, %f207, %f208;
	mov.b32 	%r251, %f209;
	shfl.sync.bfly.b32 	%r253|%p59, %r251, %r235, %r224, %r226;
	mov.b32 	%f210, %r253;
	add.f32 	%f211, %f209, %f210;
	mov.b32 	%r254, %f211;
	shfl.sync.bfly.b32 	%r256|%p60, %r254, %r238, %r224, %r226;
	mov.b32 	%f212, %r256;
	add.f32 	%f213, %f211, %f212;
	st.local.f32 	[%rd2+16], %f213;

$L__BB93_19:
	bar.sync 	0;
	mov.b32 	%r257, %f248;
	shfl.sync.bfly.b32 	%r261|%p62, %r257, %r225, %r224, %r226;
	mov.b32 	%f214, %r261;
	add.f32 	%f215, %f248, %f214;
	mov.b32 	%r262, %f215;
	shfl.sync.bfly.b32 	%r264|%p63, %r262, %r229, %r224, %r226;
	mov.b32 	%f216, %r264;
	add.f32 	%f217, %f215, %f216;
	mov.b32 	%r265, %f217;
	shfl.sync.bfly.b32 	%r267|%p64, %r265, %r232, %r224, %r226;
	mov.b32 	%f218, %r267;
	add.f32 	%f219, %f217, %f218;
	mov.b32 	%r268, %f219;
	shfl.sync.bfly.b32 	%r270|%p65, %r268, %r235, %r224, %r226;
	mov.b32 	%f220, %r270;
	add.f32 	%f221, %f219, %f220;
	mov.b32 	%r271, %f221;
	shfl.sync.bfly.b32 	%r273|%p66, %r271, %r238, %r224, %r226;
	mov.b32 	%f222, %r273;
	add.f32 	%f223, %f221, %f222;
	st.local.f32 	[%rd2+20], %f223;
	st.shared.f32 	[%r10], %f223;
	bar.sync 	0;
	@%p1 bra 	$L__BB93_21;

	ld.shared.f32 	%f224, [%r4];
	mov.b32 	%r274, %f224;
	mov.u32 	%r275, 31;
	mov.u32 	%r276, 16;
	mov.u32 	%r277, -1;
	shfl.sync.bfly.b32 	%r278|%p67, %r274, %r276, %r275, %r277;
	mov.b32 	%f225, %r278;
	add.f32 	%f226, %f224, %f225;
	mov.b32 	%r279, %f226;
	mov.u32 	%r280, 8;
	shfl.sync.bfly.b32 	%r281|%p68, %r279, %r280, %r275, %r277;
	mov.b32 	%f227, %r281;
	add.f32 	%f228, %f226, %f227;
	mov.b32 	%r282, %f228;
	mov.u32 	%r283, 4;
	shfl.sync.bfly.b32 	%r284|%p69, %r282, %r283, %r275, %r277;
	mov.b32 	%f229, %r284;
	add.f32 	%f230, %f228, %f229;
	mov.b32 	%r285, %f230;
	mov.u32 	%r286, 2;
	shfl.sync.bfly.b32 	%r287|%p70, %r285, %r286, %r275, %r277;
	mov.b32 	%f231, %r287;
	add.f32 	%f232, %f230, %f231;
	mov.b32 	%r288, %f232;
	mov.u32 	%r289, 1;
	shfl.sync.bfly.b32 	%r290|%p71, %r288, %r289, %r275, %r277;
	mov.b32 	%f233, %r290;
	add.f32 	%f234, %f232, %f233;
	st.local.f32 	[%rd2+20], %f234;

$L__BB93_21:
	bar.sync 	0;
	setp.gt.s32 	%p72, %r3, 5;
	@%p72 bra 	$L__BB93_23;

	mad.lo.s32 	%r291, %r3, %r13, %r2;
	cvt.s64.s32 	%rd57, %r291;
	mul.lo.s32 	%r292, %r1, %r14;
	cvt.s64.s32 	%rd58, %r292;
	add.s64 	%rd59, %rd58, %rd57;
	mul.wide.s32 	%rd60, %r3, 4;
	add.s64 	%rd61, %rd2, %rd60;
	ld.local.f32 	%f235, [%rd61];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f235;}

	// end inline asm
	cvta.to.global.u64 	%rd62, %rd19;
	shl.b64 	%rd63, %rd59, 1;
	add.s64 	%rd64, %rd62, %rd63;
	st.global.u16 	[%rd64], %rs1;

$L__BB93_23:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_7_bs_128
.visible .entry ggml_matvec_f16_ncols_7_bs_128(
	.param .u64 ggml_matvec_f16_ncols_7_bs_128_param_0,
	.param .u64 ggml_matvec_f16_ncols_7_bs_128_param_1,
	.param .u64 ggml_matvec_f16_ncols_7_bs_128_param_2,
	.param .u32 ggml_matvec_f16_ncols_7_bs_128_param_3,
	.param .u32 ggml_matvec_f16_ncols_7_bs_128_param_4,
	.param .u32 ggml_matvec_f16_ncols_7_bs_128_param_5,
	.param .u32 ggml_matvec_f16_ncols_7_bs_128_param_6,
	.param .u32 ggml_matvec_f16_ncols_7_bs_128_param_7,
	.param .u32 ggml_matvec_f16_ncols_7_bs_128_param_8,
	.param .u32 ggml_matvec_f16_ncols_7_bs_128_param_9,
	.param .u32 ggml_matvec_f16_ncols_7_bs_128_param_10,
	.param .u32 ggml_matvec_f16_ncols_7_bs_128_param_11
)
{
	.local .align 4 .b8 	__local_depot94[28];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<82>;
	.reg .b16 	%rs<2>;
	.reg .f32 	%f<221>;
	.reg .b32 	%r<289>;
	.reg .b64 	%rd<43>;


	mov.u64 	%SPL, __local_depot94;
	ld.param.u64 	%rd13, [ggml_matvec_f16_ncols_7_bs_128_param_0];
	ld.param.u64 	%rd14, [ggml_matvec_f16_ncols_7_bs_128_param_1];
	ld.param.u64 	%rd15, [ggml_matvec_f16_ncols_7_bs_128_param_2];
	ld.param.u32 	%r8, [ggml_matvec_f16_ncols_7_bs_128_param_3];
	ld.param.u32 	%r9, [ggml_matvec_f16_ncols_7_bs_128_param_5];
	ld.param.u32 	%r10, [ggml_matvec_f16_ncols_7_bs_128_param_6];
	ld.param.u32 	%r11, [ggml_matvec_f16_ncols_7_bs_128_param_7];
	ld.param.u32 	%r12, [ggml_matvec_f16_ncols_7_bs_128_param_8];
	ld.param.u32 	%r13, [ggml_matvec_f16_ncols_7_bs_128_param_9];
	ld.param.u32 	%r14, [ggml_matvec_f16_ncols_7_bs_128_param_10];
	ld.param.u32 	%r15, [ggml_matvec_f16_ncols_7_bs_128_param_11];
	add.u64 	%rd1, %SPL, 0;
	mov.u32 	%r1, %ctaid.x;
	mov.u32 	%r2, %tid.x;
	setp.gt.s32 	%p1, %r2, 31;
	shl.b32 	%r16, %r2, 2;
	mov.u32 	%r17, data_mmv;
	add.s32 	%r3, %r17, %r16;
	@%p1 bra 	$L__BB94_2;

	mov.u32 	%r18, 0;
	st.shared.u32 	[%r3], %r18;

$L__BB94_2:
	mov.u32 	%r4, %ctaid.y;
	bar.sync 	0;
	mov.f32 	%f214, 0f00000000;
	mov.u32 	%r19, 0;
	st.local.u32 	[%rd1], %r19;
	st.local.u32 	[%rd1+4], %r19;
	st.local.u32 	[%rd1+8], %r19;
	st.local.u32 	[%rd1+12], %r19;
	st.local.u32 	[%rd1+16], %r19;
	st.local.u32 	[%rd1+20], %r19;
	st.local.u32 	[%rd1+24], %r19;
	setp.ge.s32 	%p2, %r2, %r8;
	mov.f32 	%f215, %f214;
	mov.f32 	%f216, %f214;
	mov.f32 	%f217, %f214;
	mov.f32 	%f218, %f214;
	mov.f32 	%f219, %f214;
	mov.f32 	%f220, %f214;
	@%p2 bra 	$L__BB94_6;

	shl.b32 	%r20, %r10, 1;
	add.s32 	%r21, %r2, %r20;
	mul.wide.s32 	%rd17, %r21, 4;
	mul.lo.s32 	%r22, %r4, %r14;
	mul.wide.s32 	%rd18, %r22, 2;
	add.s64 	%rd3, %rd17, %rd18;
	mul.wide.s32 	%rd19, %r2, 4;
	mul.wide.s32 	%rd4, %r10, 4;
	add.s64 	%rd20, %rd19, %rd4;
	add.s64 	%rd5, %rd20, %rd18;
	add.s64 	%rd6, %rd19, %rd18;
	mul.wide.s32 	%rd21, %r2, 2;
	div.s32 	%r23, %r4, %r12;
	mul.lo.s32 	%r24, %r1, %r9;
	mad.lo.s32 	%r25, %r23, %r13, %r24;
	cvt.s64.s32 	%rd22, %r25;
	add.s64 	%rd23, %rd21, %rd22;
	cvta.to.global.u64 	%rd24, %rd13;
	shl.b64 	%rd25, %rd23, 1;
	add.s64 	%rd41, %rd24, %rd25;
	cvta.to.global.u64 	%rd42, %rd14;
	mov.f32 	%f214, 0f00000000;
	mov.u32 	%r288, %r2;

$L__BB94_4:
	ld.global.nc.u32 	%r26, [%rd41];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r26;
  cvt.f32.f16 %f36, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r26;
  cvt.f32.f16 %f37, high;}

	// end inline asm
	add.s64 	%rd26, %rd42, %rd6;
	ld.global.nc.u32 	%r28, [%rd26];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r28;
  cvt.f32.f16 %f38, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r28;
  cvt.f32.f16 %f39, high;}

	// end inline asm
	fma.rn.f32 	%f52, %f36, %f38, %f220;
	fma.rn.f32 	%f220, %f37, %f39, %f52;
	add.s64 	%rd27, %rd42, %rd5;
	ld.global.nc.u32 	%r30, [%rd27];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r30;
  cvt.f32.f16 %f40, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r30;
  cvt.f32.f16 %f41, high;}

	// end inline asm
	fma.rn.f32 	%f53, %f36, %f40, %f219;
	fma.rn.f32 	%f219, %f37, %f41, %f53;
	add.s64 	%rd28, %rd42, %rd3;
	ld.global.nc.u32 	%r32, [%rd28];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r32;
  cvt.f32.f16 %f42, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r32;
  cvt.f32.f16 %f43, high;}

	// end inline asm
	fma.rn.f32 	%f54, %f36, %f42, %f218;
	fma.rn.f32 	%f218, %f37, %f43, %f54;
	add.s64 	%rd29, %rd28, %rd4;
	ld.global.nc.u32 	%r34, [%rd29];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r34;
  cvt.f32.f16 %f44, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r34;
  cvt.f32.f16 %f45, high;}

	// end inline asm
	fma.rn.f32 	%f55, %f36, %f44, %f217;
	fma.rn.f32 	%f217, %f37, %f45, %f55;
	add.s64 	%rd30, %rd29, %rd4;
	ld.global.nc.u32 	%r36, [%rd30];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r36;
  cvt.f32.f16 %f46, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r36;
  cvt.f32.f16 %f47, high;}

	// end inline asm
	fma.rn.f32 	%f56, %f36, %f46, %f216;
	fma.rn.f32 	%f216, %f37, %f47, %f56;
	add.s64 	%rd31, %rd30, %rd4;
	ld.global.nc.u32 	%r38, [%rd31];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r38;
  cvt.f32.f16 %f48, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r38;
  cvt.f32.f16 %f49, high;}

	// end inline asm
	fma.rn.f32 	%f57, %f36, %f48, %f215;
	fma.rn.f32 	%f215, %f37, %f49, %f57;
	add.s64 	%rd32, %rd31, %rd4;
	ld.global.nc.u32 	%r40, [%rd32];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r40;
  cvt.f32.f16 %f50, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r40;
  cvt.f32.f16 %f51, high;}

	// end inline asm
	fma.rn.f32 	%f58, %f36, %f50, %f214;
	fma.rn.f32 	%f214, %f37, %f51, %f58;
	add.s64 	%rd42, %rd42, 512;
	add.s64 	%rd41, %rd41, 512;
	add.s32 	%r288, %r288, 128;
	setp.lt.s32 	%p3, %r288, %r8;
	@%p3 bra 	$L__BB94_4;

	st.local.f32 	[%rd1], %f220;
	st.local.f32 	[%rd1+4], %f219;
	st.local.f32 	[%rd1+8], %f218;
	st.local.f32 	[%rd1+12], %f217;
	st.local.f32 	[%rd1+16], %f216;
	st.local.f32 	[%rd1+20], %f215;
	st.local.f32 	[%rd1+24], %f214;

$L__BB94_6:
	shr.s32 	%r42, %r2, 31;
	shr.u32 	%r43, %r42, 27;
	add.s32 	%r44, %r2, %r43;
	shr.s32 	%r45, %r44, 5;
	shl.b32 	%r46, %r45, 2;
	add.s32 	%r7, %r17, %r46;
	mov.u32 	%r48, 2;
	mov.b32 	%r49, %f220;
	mov.u32 	%r50, 31;
	mov.u32 	%r51, 16;
	mov.u32 	%r52, -1;
	shfl.sync.bfly.b32 	%r53|%p4, %r49, %r51, %r50, %r52;
	mov.b32 	%f59, %r53;
	add.f32 	%f60, %f220, %f59;
	mov.b32 	%r54, %f60;
	mov.u32 	%r55, 8;
	shfl.sync.bfly.b32 	%r56|%p5, %r54, %r55, %r50, %r52;
	mov.b32 	%f61, %r56;
	add.f32 	%f62, %f60, %f61;
	mov.b32 	%r57, %f62;
	mov.u32 	%r58, 4;
	shfl.sync.bfly.b32 	%r59|%p6, %r57, %r58, %r50, %r52;
	mov.b32 	%f63, %r59;
	add.f32 	%f64, %f62, %f63;
	mov.b32 	%r60, %f64;
	shfl.sync.bfly.b32 	%r61|%p7, %r60, %r48, %r50, %r52;
	mov.b32 	%f65, %r61;
	add.f32 	%f66, %f64, %f65;
	mov.b32 	%r62, %f66;
	mov.u32 	%r63, 1;
	shfl.sync.bfly.b32 	%r64|%p8, %r62, %r63, %r50, %r52;
	mov.b32 	%f67, %r64;
	add.f32 	%f68, %f66, %f67;
	st.local.f32 	[%rd1], %f68;
	st.shared.f32 	[%r7], %f68;
	bar.sync 	0;
	@%p1 bra 	$L__BB94_8;

	ld.shared.f32 	%f69, [%r3];
	mov.b32 	%r65, %f69;
	shfl.sync.bfly.b32 	%r69|%p10, %r65, %r51, %r50, %r52;
	mov.b32 	%f70, %r69;
	add.f32 	%f71, %f69, %f70;
	mov.b32 	%r70, %f71;
	shfl.sync.bfly.b32 	%r72|%p11, %r70, %r55, %r50, %r52;
	mov.b32 	%f72, %r72;
	add.f32 	%f73, %f71, %f72;
	mov.b32 	%r73, %f73;
	shfl.sync.bfly.b32 	%r75|%p12, %r73, %r58, %r50, %r52;
	mov.b32 	%f74, %r75;
	add.f32 	%f75, %f73, %f74;
	mov.b32 	%r76, %f75;
	shfl.sync.bfly.b32 	%r78|%p13, %r76, %r48, %r50, %r52;
	mov.b32 	%f76, %r78;
	add.f32 	%f77, %f75, %f76;
	mov.b32 	%r79, %f77;
	shfl.sync.bfly.b32 	%r81|%p14, %r79, %r63, %r50, %r52;
	mov.b32 	%f78, %r81;
	add.f32 	%f79, %f77, %f78;
	st.local.f32 	[%rd1], %f79;

$L__BB94_8:
	bar.sync 	0;
	mov.b32 	%r82, %f219;
	shfl.sync.bfly.b32 	%r86|%p16, %r82, %r51, %r50, %r52;
	mov.b32 	%f80, %r86;
	add.f32 	%f81, %f219, %f80;
	mov.b32 	%r87, %f81;
	shfl.sync.bfly.b32 	%r89|%p17, %r87, %r55, %r50, %r52;
	mov.b32 	%f82, %r89;
	add.f32 	%f83, %f81, %f82;
	mov.b32 	%r90, %f83;
	shfl.sync.bfly.b32 	%r92|%p18, %r90, %r58, %r50, %r52;
	mov.b32 	%f84, %r92;
	add.f32 	%f85, %f83, %f84;
	mov.b32 	%r93, %f85;
	shfl.sync.bfly.b32 	%r95|%p19, %r93, %r48, %r50, %r52;
	mov.b32 	%f86, %r95;
	add.f32 	%f87, %f85, %f86;
	mov.b32 	%r96, %f87;
	shfl.sync.bfly.b32 	%r98|%p20, %r96, %r63, %r50, %r52;
	mov.b32 	%f88, %r98;
	add.f32 	%f89, %f87, %f88;
	st.local.f32 	[%rd1+4], %f89;
	st.shared.f32 	[%r7], %f89;
	bar.sync 	0;
	@%p1 bra 	$L__BB94_10;

	ld.shared.f32 	%f90, [%r3];
	mov.b32 	%r99, %f90;
	mov.u32 	%r100, 31;
	mov.u32 	%r101, 16;
	mov.u32 	%r102, -1;
	shfl.sync.bfly.b32 	%r103|%p21, %r99, %r101, %r100, %r102;
	mov.b32 	%f91, %r103;
	add.f32 	%f92, %f90, %f91;
	mov.b32 	%r104, %f92;
	mov.u32 	%r105, 8;
	shfl.sync.bfly.b32 	%r106|%p22, %r104, %r105, %r100, %r102;
	mov.b32 	%f93, %r106;
	add.f32 	%f94, %f92, %f93;
	mov.b32 	%r107, %f94;
	mov.u32 	%r108, 4;
	shfl.sync.bfly.b32 	%r109|%p23, %r107, %r108, %r100, %r102;
	mov.b32 	%f95, %r109;
	add.f32 	%f96, %f94, %f95;
	mov.b32 	%r110, %f96;
	mov.u32 	%r111, 2;
	shfl.sync.bfly.b32 	%r112|%p24, %r110, %r111, %r100, %r102;
	mov.b32 	%f97, %r112;
	add.f32 	%f98, %f96, %f97;
	mov.b32 	%r113, %f98;
	mov.u32 	%r114, 1;
	shfl.sync.bfly.b32 	%r115|%p25, %r113, %r114, %r100, %r102;
	mov.b32 	%f99, %r115;
	add.f32 	%f100, %f98, %f99;
	st.local.f32 	[%rd1+4], %f100;

$L__BB94_10:
	bar.sync 	0;
	mov.b32 	%r116, %f218;
	mov.u32 	%r117, 31;
	mov.u32 	%r118, 16;
	mov.u32 	%r119, -1;
	shfl.sync.bfly.b32 	%r120|%p27, %r116, %r118, %r117, %r119;
	mov.b32 	%f101, %r120;
	add.f32 	%f102, %f218, %f101;
	mov.b32 	%r121, %f102;
	mov.u32 	%r122, 8;
	shfl.sync.bfly.b32 	%r123|%p28, %r121, %r122, %r117, %r119;
	mov.b32 	%f103, %r123;
	add.f32 	%f104, %f102, %f103;
	mov.b32 	%r124, %f104;
	mov.u32 	%r125, 4;
	shfl.sync.bfly.b32 	%r126|%p29, %r124, %r125, %r117, %r119;
	mov.b32 	%f105, %r126;
	add.f32 	%f106, %f104, %f105;
	mov.b32 	%r127, %f106;
	mov.u32 	%r128, 2;
	shfl.sync.bfly.b32 	%r129|%p30, %r127, %r128, %r117, %r119;
	mov.b32 	%f107, %r129;
	add.f32 	%f108, %f106, %f107;
	mov.b32 	%r130, %f108;
	mov.u32 	%r131, 1;
	shfl.sync.bfly.b32 	%r132|%p31, %r130, %r131, %r117, %r119;
	mov.b32 	%f109, %r132;
	add.f32 	%f110, %f108, %f109;
	st.local.f32 	[%rd1+8], %f110;
	st.shared.f32 	[%r7], %f110;
	bar.sync 	0;
	@%p1 bra 	$L__BB94_12;

	ld.shared.f32 	%f111, [%r3];
	mov.b32 	%r133, %f111;
	shfl.sync.bfly.b32 	%r137|%p32, %r133, %r118, %r117, %r119;
	mov.b32 	%f112, %r137;
	add.f32 	%f113, %f111, %f112;
	mov.b32 	%r138, %f113;
	shfl.sync.bfly.b32 	%r140|%p33, %r138, %r122, %r117, %r119;
	mov.b32 	%f114, %r140;
	add.f32 	%f115, %f113, %f114;
	mov.b32 	%r141, %f115;
	shfl.sync.bfly.b32 	%r143|%p34, %r141, %r125, %r117, %r119;
	mov.b32 	%f116, %r143;
	add.f32 	%f117, %f115, %f116;
	mov.b32 	%r144, %f117;
	shfl.sync.bfly.b32 	%r146|%p35, %r144, %r128, %r117, %r119;
	mov.b32 	%f118, %r146;
	add.f32 	%f119, %f117, %f118;
	mov.b32 	%r147, %f119;
	shfl.sync.bfly.b32 	%r149|%p36, %r147, %r131, %r117, %r119;
	mov.b32 	%f120, %r149;
	add.f32 	%f121, %f119, %f120;
	st.local.f32 	[%rd1+8], %f121;

$L__BB94_12:
	bar.sync 	0;
	mov.b32 	%r150, %f217;
	shfl.sync.bfly.b32 	%r154|%p38, %r150, %r118, %r117, %r119;
	mov.b32 	%f122, %r154;
	add.f32 	%f123, %f217, %f122;
	mov.b32 	%r155, %f123;
	shfl.sync.bfly.b32 	%r157|%p39, %r155, %r122, %r117, %r119;
	mov.b32 	%f124, %r157;
	add.f32 	%f125, %f123, %f124;
	mov.b32 	%r158, %f125;
	shfl.sync.bfly.b32 	%r160|%p40, %r158, %r125, %r117, %r119;
	mov.b32 	%f126, %r160;
	add.f32 	%f127, %f125, %f126;
	mov.b32 	%r161, %f127;
	shfl.sync.bfly.b32 	%r163|%p41, %r161, %r128, %r117, %r119;
	mov.b32 	%f128, %r163;
	add.f32 	%f129, %f127, %f128;
	mov.b32 	%r164, %f129;
	shfl.sync.bfly.b32 	%r166|%p42, %r164, %r131, %r117, %r119;
	mov.b32 	%f130, %r166;
	add.f32 	%f131, %f129, %f130;
	st.local.f32 	[%rd1+12], %f131;
	st.shared.f32 	[%r7], %f131;
	bar.sync 	0;
	@%p1 bra 	$L__BB94_14;

	ld.shared.f32 	%f132, [%r3];
	mov.b32 	%r167, %f132;
	mov.u32 	%r168, 31;
	mov.u32 	%r169, 16;
	mov.u32 	%r170, -1;
	shfl.sync.bfly.b32 	%r171|%p43, %r167, %r169, %r168, %r170;
	mov.b32 	%f133, %r171;
	add.f32 	%f134, %f132, %f133;
	mov.b32 	%r172, %f134;
	mov.u32 	%r173, 8;
	shfl.sync.bfly.b32 	%r174|%p44, %r172, %r173, %r168, %r170;
	mov.b32 	%f135, %r174;
	add.f32 	%f136, %f134, %f135;
	mov.b32 	%r175, %f136;
	mov.u32 	%r176, 4;
	shfl.sync.bfly.b32 	%r177|%p45, %r175, %r176, %r168, %r170;
	mov.b32 	%f137, %r177;
	add.f32 	%f138, %f136, %f137;
	mov.b32 	%r178, %f138;
	mov.u32 	%r179, 2;
	shfl.sync.bfly.b32 	%r180|%p46, %r178, %r179, %r168, %r170;
	mov.b32 	%f139, %r180;
	add.f32 	%f140, %f138, %f139;
	mov.b32 	%r181, %f140;
	mov.u32 	%r182, 1;
	shfl.sync.bfly.b32 	%r183|%p47, %r181, %r182, %r168, %r170;
	mov.b32 	%f141, %r183;
	add.f32 	%f142, %f140, %f141;
	st.local.f32 	[%rd1+12], %f142;

$L__BB94_14:
	bar.sync 	0;
	mov.b32 	%r184, %f216;
	mov.u32 	%r185, 31;
	mov.u32 	%r186, 16;
	mov.u32 	%r187, -1;
	shfl.sync.bfly.b32 	%r188|%p49, %r184, %r186, %r185, %r187;
	mov.b32 	%f143, %r188;
	add.f32 	%f144, %f216, %f143;
	mov.b32 	%r189, %f144;
	mov.u32 	%r190, 8;
	shfl.sync.bfly.b32 	%r191|%p50, %r189, %r190, %r185, %r187;
	mov.b32 	%f145, %r191;
	add.f32 	%f146, %f144, %f145;
	mov.b32 	%r192, %f146;
	mov.u32 	%r193, 4;
	shfl.sync.bfly.b32 	%r194|%p51, %r192, %r193, %r185, %r187;
	mov.b32 	%f147, %r194;
	add.f32 	%f148, %f146, %f147;
	mov.b32 	%r195, %f148;
	mov.u32 	%r196, 2;
	shfl.sync.bfly.b32 	%r197|%p52, %r195, %r196, %r185, %r187;
	mov.b32 	%f149, %r197;
	add.f32 	%f150, %f148, %f149;
	mov.b32 	%r198, %f150;
	mov.u32 	%r199, 1;
	shfl.sync.bfly.b32 	%r200|%p53, %r198, %r199, %r185, %r187;
	mov.b32 	%f151, %r200;
	add.f32 	%f152, %f150, %f151;
	st.local.f32 	[%rd1+16], %f152;
	st.shared.f32 	[%r7], %f152;
	bar.sync 	0;
	@%p1 bra 	$L__BB94_16;

	ld.shared.f32 	%f153, [%r3];
	mov.b32 	%r201, %f153;
	shfl.sync.bfly.b32 	%r205|%p54, %r201, %r186, %r185, %r187;
	mov.b32 	%f154, %r205;
	add.f32 	%f155, %f153, %f154;
	mov.b32 	%r206, %f155;
	shfl.sync.bfly.b32 	%r208|%p55, %r206, %r190, %r185, %r187;
	mov.b32 	%f156, %r208;
	add.f32 	%f157, %f155, %f156;
	mov.b32 	%r209, %f157;
	shfl.sync.bfly.b32 	%r211|%p56, %r209, %r193, %r185, %r187;
	mov.b32 	%f158, %r211;
	add.f32 	%f159, %f157, %f158;
	mov.b32 	%r212, %f159;
	shfl.sync.bfly.b32 	%r214|%p57, %r212, %r196, %r185, %r187;
	mov.b32 	%f160, %r214;
	add.f32 	%f161, %f159, %f160;
	mov.b32 	%r215, %f161;
	shfl.sync.bfly.b32 	%r217|%p58, %r215, %r199, %r185, %r187;
	mov.b32 	%f162, %r217;
	add.f32 	%f163, %f161, %f162;
	st.local.f32 	[%rd1+16], %f163;

$L__BB94_16:
	bar.sync 	0;
	mov.b32 	%r218, %f215;
	shfl.sync.bfly.b32 	%r222|%p60, %r218, %r186, %r185, %r187;
	mov.b32 	%f164, %r222;
	add.f32 	%f165, %f215, %f164;
	mov.b32 	%r223, %f165;
	shfl.sync.bfly.b32 	%r225|%p61, %r223, %r190, %r185, %r187;
	mov.b32 	%f166, %r225;
	add.f32 	%f167, %f165, %f166;
	mov.b32 	%r226, %f167;
	shfl.sync.bfly.b32 	%r228|%p62, %r226, %r193, %r185, %r187;
	mov.b32 	%f168, %r228;
	add.f32 	%f169, %f167, %f168;
	mov.b32 	%r229, %f169;
	shfl.sync.bfly.b32 	%r231|%p63, %r229, %r196, %r185, %r187;
	mov.b32 	%f170, %r231;
	add.f32 	%f171, %f169, %f170;
	mov.b32 	%r232, %f171;
	shfl.sync.bfly.b32 	%r234|%p64, %r232, %r199, %r185, %r187;
	mov.b32 	%f172, %r234;
	add.f32 	%f173, %f171, %f172;
	st.local.f32 	[%rd1+20], %f173;
	st.shared.f32 	[%r7], %f173;
	bar.sync 	0;
	@%p1 bra 	$L__BB94_18;

	ld.shared.f32 	%f174, [%r3];
	mov.b32 	%r235, %f174;
	mov.u32 	%r236, 31;
	mov.u32 	%r237, 16;
	mov.u32 	%r238, -1;
	shfl.sync.bfly.b32 	%r239|%p65, %r235, %r237, %r236, %r238;
	mov.b32 	%f175, %r239;
	add.f32 	%f176, %f174, %f175;
	mov.b32 	%r240, %f176;
	mov.u32 	%r241, 8;
	shfl.sync.bfly.b32 	%r242|%p66, %r240, %r241, %r236, %r238;
	mov.b32 	%f177, %r242;
	add.f32 	%f178, %f176, %f177;
	mov.b32 	%r243, %f178;
	mov.u32 	%r244, 4;
	shfl.sync.bfly.b32 	%r245|%p67, %r243, %r244, %r236, %r238;
	mov.b32 	%f179, %r245;
	add.f32 	%f180, %f178, %f179;
	mov.b32 	%r246, %f180;
	mov.u32 	%r247, 2;
	shfl.sync.bfly.b32 	%r248|%p68, %r246, %r247, %r236, %r238;
	mov.b32 	%f181, %r248;
	add.f32 	%f182, %f180, %f181;
	mov.b32 	%r249, %f182;
	mov.u32 	%r250, 1;
	shfl.sync.bfly.b32 	%r251|%p69, %r249, %r250, %r236, %r238;
	mov.b32 	%f183, %r251;
	add.f32 	%f184, %f182, %f183;
	st.local.f32 	[%rd1+20], %f184;

$L__BB94_18:
	bar.sync 	0;
	mov.b32 	%r252, %f214;
	mov.u32 	%r253, 31;
	mov.u32 	%r254, 16;
	mov.u32 	%r255, -1;
	shfl.sync.bfly.b32 	%r256|%p71, %r252, %r254, %r253, %r255;
	mov.b32 	%f185, %r256;
	add.f32 	%f186, %f214, %f185;
	mov.b32 	%r257, %f186;
	mov.u32 	%r258, 8;
	shfl.sync.bfly.b32 	%r259|%p72, %r257, %r258, %r253, %r255;
	mov.b32 	%f187, %r259;
	add.f32 	%f188, %f186, %f187;
	mov.b32 	%r260, %f188;
	mov.u32 	%r261, 4;
	shfl.sync.bfly.b32 	%r262|%p73, %r260, %r261, %r253, %r255;
	mov.b32 	%f189, %r262;
	add.f32 	%f190, %f188, %f189;
	mov.b32 	%r263, %f190;
	mov.u32 	%r264, 2;
	shfl.sync.bfly.b32 	%r265|%p74, %r263, %r264, %r253, %r255;
	mov.b32 	%f191, %r265;
	add.f32 	%f192, %f190, %f191;
	mov.b32 	%r266, %f192;
	mov.u32 	%r267, 1;
	shfl.sync.bfly.b32 	%r268|%p75, %r266, %r267, %r253, %r255;
	mov.b32 	%f193, %r268;
	add.f32 	%f194, %f192, %f193;
	st.local.f32 	[%rd1+24], %f194;
	st.shared.f32 	[%r7], %f194;
	bar.sync 	0;
	@%p1 bra 	$L__BB94_20;

	ld.shared.f32 	%f195, [%r3];
	mov.b32 	%r269, %f195;
	shfl.sync.bfly.b32 	%r273|%p76, %r269, %r254, %r253, %r255;
	mov.b32 	%f196, %r273;
	add.f32 	%f197, %f195, %f196;
	mov.b32 	%r274, %f197;
	shfl.sync.bfly.b32 	%r276|%p77, %r274, %r258, %r253, %r255;
	mov.b32 	%f198, %r276;
	add.f32 	%f199, %f197, %f198;
	mov.b32 	%r277, %f199;
	shfl.sync.bfly.b32 	%r279|%p78, %r277, %r261, %r253, %r255;
	mov.b32 	%f200, %r279;
	add.f32 	%f201, %f199, %f200;
	mov.b32 	%r280, %f201;
	shfl.sync.bfly.b32 	%r282|%p79, %r280, %r264, %r253, %r255;
	mov.b32 	%f202, %r282;
	add.f32 	%f203, %f201, %f202;
	mov.b32 	%r283, %f203;
	shfl.sync.bfly.b32 	%r285|%p80, %r283, %r267, %r253, %r255;
	mov.b32 	%f204, %r285;
	add.f32 	%f205, %f203, %f204;
	st.local.f32 	[%rd1+24], %f205;

$L__BB94_20:
	bar.sync 	0;
	setp.gt.s32 	%p81, %r2, 6;
	@%p81 bra 	$L__BB94_22;

	mad.lo.s32 	%r286, %r2, %r11, %r1;
	cvt.s64.s32 	%rd33, %r286;
	mul.lo.s32 	%r287, %r4, %r15;
	cvt.s64.s32 	%rd34, %r287;
	add.s64 	%rd35, %rd34, %rd33;
	mul.wide.s32 	%rd36, %r2, 4;
	add.s64 	%rd37, %rd1, %rd36;
	ld.local.f32 	%f206, [%rd37];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f206;}

	// end inline asm
	cvta.to.global.u64 	%rd38, %rd15;
	shl.b64 	%rd39, %rd35, 1;
	add.s64 	%rd40, %rd38, %rd39;
	st.global.u16 	[%rd40], %rs1;

$L__BB94_22:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_8_bs_128
.visible .entry ggml_matvec_f16_ncols_8_bs_128(
	.param .u64 ggml_matvec_f16_ncols_8_bs_128_param_0,
	.param .u64 ggml_matvec_f16_ncols_8_bs_128_param_1,
	.param .u64 ggml_matvec_f16_ncols_8_bs_128_param_2,
	.param .u32 ggml_matvec_f16_ncols_8_bs_128_param_3,
	.param .u32 ggml_matvec_f16_ncols_8_bs_128_param_4,
	.param .u32 ggml_matvec_f16_ncols_8_bs_128_param_5,
	.param .u32 ggml_matvec_f16_ncols_8_bs_128_param_6,
	.param .u32 ggml_matvec_f16_ncols_8_bs_128_param_7,
	.param .u32 ggml_matvec_f16_ncols_8_bs_128_param_8,
	.param .u32 ggml_matvec_f16_ncols_8_bs_128_param_9,
	.param .u32 ggml_matvec_f16_ncols_8_bs_128_param_10,
	.param .u32 ggml_matvec_f16_ncols_8_bs_128_param_11
)
{
	.local .align 16 .b8 	__local_depot95[32];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<93>;
	.reg .b16 	%rs<2>;
	.reg .f32 	%f<252>;
	.reg .b32 	%r<324>;
	.reg .b64 	%rd<45>;


	mov.u64 	%SPL, __local_depot95;
	ld.param.u64 	%rd14, [ggml_matvec_f16_ncols_8_bs_128_param_0];
	ld.param.u64 	%rd15, [ggml_matvec_f16_ncols_8_bs_128_param_1];
	ld.param.u64 	%rd16, [ggml_matvec_f16_ncols_8_bs_128_param_2];
	ld.param.u32 	%r8, [ggml_matvec_f16_ncols_8_bs_128_param_3];
	ld.param.u32 	%r9, [ggml_matvec_f16_ncols_8_bs_128_param_5];
	ld.param.u32 	%r10, [ggml_matvec_f16_ncols_8_bs_128_param_6];
	ld.param.u32 	%r11, [ggml_matvec_f16_ncols_8_bs_128_param_7];
	ld.param.u32 	%r12, [ggml_matvec_f16_ncols_8_bs_128_param_8];
	ld.param.u32 	%r13, [ggml_matvec_f16_ncols_8_bs_128_param_9];
	ld.param.u32 	%r14, [ggml_matvec_f16_ncols_8_bs_128_param_10];
	ld.param.u32 	%r15, [ggml_matvec_f16_ncols_8_bs_128_param_11];
	add.u64 	%rd1, %SPL, 0;
	mov.u32 	%r1, %ctaid.x;
	mov.u32 	%r2, %tid.x;
	setp.gt.s32 	%p1, %r2, 31;
	shl.b32 	%r16, %r2, 2;
	mov.u32 	%r17, data_mmv;
	add.s32 	%r3, %r17, %r16;
	@%p1 bra 	$L__BB95_2;

	mov.u32 	%r18, 0;
	st.shared.u32 	[%r3], %r18;

$L__BB95_2:
	mov.u32 	%r4, %ctaid.y;
	bar.sync 	0;
	mov.f32 	%f244, 0f00000000;
	st.local.v4.f32 	[%rd1], {%f244, %f244, %f244, %f244};
	st.local.v4.f32 	[%rd1+16], {%f244, %f244, %f244, %f244};
	setp.ge.s32 	%p2, %r2, %r8;
	mov.f32 	%f245, %f244;
	mov.f32 	%f246, %f244;
	mov.f32 	%f247, %f244;
	mov.f32 	%f248, %f244;
	mov.f32 	%f249, %f244;
	mov.f32 	%f250, %f244;
	mov.f32 	%f251, %f244;
	@%p2 bra 	$L__BB95_6;

	shl.b32 	%r19, %r10, 1;
	add.s32 	%r20, %r2, %r19;
	mul.wide.s32 	%rd18, %r20, 4;
	mul.lo.s32 	%r21, %r4, %r14;
	mul.wide.s32 	%rd19, %r21, 2;
	add.s64 	%rd4, %rd18, %rd19;
	mul.wide.s32 	%rd20, %r2, 4;
	mul.wide.s32 	%rd5, %r10, 4;
	add.s64 	%rd21, %rd20, %rd5;
	add.s64 	%rd6, %rd21, %rd19;
	add.s64 	%rd7, %rd20, %rd19;
	mul.wide.s32 	%rd22, %r2, 2;
	div.s32 	%r22, %r4, %r12;
	mul.lo.s32 	%r23, %r1, %r9;
	mad.lo.s32 	%r24, %r22, %r13, %r23;
	cvt.s64.s32 	%rd23, %r24;
	add.s64 	%rd24, %rd22, %rd23;
	cvta.to.global.u64 	%rd25, %rd14;
	shl.b64 	%rd26, %rd24, 1;
	add.s64 	%rd43, %rd25, %rd26;
	cvta.to.global.u64 	%rd44, %rd15;
	mov.f32 	%f244, 0f00000000;
	mov.u32 	%r323, %r2;

$L__BB95_4:
	ld.global.nc.u32 	%r25, [%rd43];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r25;
  cvt.f32.f16 %f41, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r25;
  cvt.f32.f16 %f42, high;}

	// end inline asm
	add.s64 	%rd27, %rd44, %rd7;
	ld.global.nc.u32 	%r27, [%rd27];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r27;
  cvt.f32.f16 %f43, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r27;
  cvt.f32.f16 %f44, high;}

	// end inline asm
	fma.rn.f32 	%f59, %f41, %f43, %f251;
	fma.rn.f32 	%f251, %f42, %f44, %f59;
	add.s64 	%rd28, %rd44, %rd6;
	ld.global.nc.u32 	%r29, [%rd28];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r29;
  cvt.f32.f16 %f45, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r29;
  cvt.f32.f16 %f46, high;}

	// end inline asm
	fma.rn.f32 	%f60, %f41, %f45, %f250;
	fma.rn.f32 	%f250, %f42, %f46, %f60;
	add.s64 	%rd29, %rd44, %rd4;
	ld.global.nc.u32 	%r31, [%rd29];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r31;
  cvt.f32.f16 %f47, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r31;
  cvt.f32.f16 %f48, high;}

	// end inline asm
	fma.rn.f32 	%f61, %f41, %f47, %f249;
	fma.rn.f32 	%f249, %f42, %f48, %f61;
	add.s64 	%rd30, %rd29, %rd5;
	ld.global.nc.u32 	%r33, [%rd30];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r33;
  cvt.f32.f16 %f49, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r33;
  cvt.f32.f16 %f50, high;}

	// end inline asm
	fma.rn.f32 	%f62, %f41, %f49, %f248;
	fma.rn.f32 	%f248, %f42, %f50, %f62;
	add.s64 	%rd31, %rd30, %rd5;
	ld.global.nc.u32 	%r35, [%rd31];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r35;
  cvt.f32.f16 %f51, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r35;
  cvt.f32.f16 %f52, high;}

	// end inline asm
	fma.rn.f32 	%f63, %f41, %f51, %f247;
	fma.rn.f32 	%f247, %f42, %f52, %f63;
	add.s64 	%rd32, %rd31, %rd5;
	ld.global.nc.u32 	%r37, [%rd32];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r37;
  cvt.f32.f16 %f53, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r37;
  cvt.f32.f16 %f54, high;}

	// end inline asm
	fma.rn.f32 	%f64, %f41, %f53, %f246;
	fma.rn.f32 	%f246, %f42, %f54, %f64;
	add.s64 	%rd33, %rd32, %rd5;
	ld.global.nc.u32 	%r39, [%rd33];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r39;
  cvt.f32.f16 %f55, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r39;
  cvt.f32.f16 %f56, high;}

	// end inline asm
	fma.rn.f32 	%f65, %f41, %f55, %f245;
	fma.rn.f32 	%f245, %f42, %f56, %f65;
	add.s64 	%rd34, %rd33, %rd5;
	ld.global.nc.u32 	%r41, [%rd34];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r41;
  cvt.f32.f16 %f57, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r41;
  cvt.f32.f16 %f58, high;}

	// end inline asm
	fma.rn.f32 	%f66, %f41, %f57, %f244;
	fma.rn.f32 	%f244, %f42, %f58, %f66;
	add.s64 	%rd44, %rd44, 512;
	add.s64 	%rd43, %rd43, 512;
	add.s32 	%r323, %r323, 128;
	setp.lt.s32 	%p3, %r323, %r8;
	@%p3 bra 	$L__BB95_4;

	st.local.v4.f32 	[%rd1], {%f251, %f250, %f249, %f248};
	st.local.v4.f32 	[%rd1+16], {%f247, %f246, %f245, %f244};

$L__BB95_6:
	shr.s32 	%r43, %r2, 31;
	shr.u32 	%r44, %r43, 27;
	add.s32 	%r45, %r2, %r44;
	shr.s32 	%r46, %r45, 5;
	shl.b32 	%r47, %r46, 2;
	add.s32 	%r7, %r17, %r47;
	mov.u32 	%r49, 2;
	mov.b32 	%r50, %f251;
	mov.u32 	%r51, 31;
	mov.u32 	%r52, 16;
	mov.u32 	%r53, -1;
	shfl.sync.bfly.b32 	%r54|%p4, %r50, %r52, %r51, %r53;
	mov.b32 	%f67, %r54;
	add.f32 	%f68, %f251, %f67;
	mov.b32 	%r55, %f68;
	mov.u32 	%r56, 8;
	shfl.sync.bfly.b32 	%r57|%p5, %r55, %r56, %r51, %r53;
	mov.b32 	%f69, %r57;
	add.f32 	%f70, %f68, %f69;
	mov.b32 	%r58, %f70;
	mov.u32 	%r59, 4;
	shfl.sync.bfly.b32 	%r60|%p6, %r58, %r59, %r51, %r53;
	mov.b32 	%f71, %r60;
	add.f32 	%f72, %f70, %f71;
	mov.b32 	%r61, %f72;
	shfl.sync.bfly.b32 	%r62|%p7, %r61, %r49, %r51, %r53;
	mov.b32 	%f73, %r62;
	add.f32 	%f74, %f72, %f73;
	mov.b32 	%r63, %f74;
	mov.u32 	%r64, 1;
	shfl.sync.bfly.b32 	%r65|%p8, %r63, %r64, %r51, %r53;
	mov.b32 	%f75, %r65;
	add.f32 	%f76, %f74, %f75;
	st.local.f32 	[%rd1], %f76;
	st.shared.f32 	[%r7], %f76;
	bar.sync 	0;
	@%p1 bra 	$L__BB95_8;

	ld.shared.f32 	%f77, [%r3];
	mov.b32 	%r66, %f77;
	shfl.sync.bfly.b32 	%r70|%p10, %r66, %r52, %r51, %r53;
	mov.b32 	%f78, %r70;
	add.f32 	%f79, %f77, %f78;
	mov.b32 	%r71, %f79;
	shfl.sync.bfly.b32 	%r73|%p11, %r71, %r56, %r51, %r53;
	mov.b32 	%f80, %r73;
	add.f32 	%f81, %f79, %f80;
	mov.b32 	%r74, %f81;
	shfl.sync.bfly.b32 	%r76|%p12, %r74, %r59, %r51, %r53;
	mov.b32 	%f82, %r76;
	add.f32 	%f83, %f81, %f82;
	mov.b32 	%r77, %f83;
	shfl.sync.bfly.b32 	%r79|%p13, %r77, %r49, %r51, %r53;
	mov.b32 	%f84, %r79;
	add.f32 	%f85, %f83, %f84;
	mov.b32 	%r80, %f85;
	shfl.sync.bfly.b32 	%r82|%p14, %r80, %r64, %r51, %r53;
	mov.b32 	%f86, %r82;
	add.f32 	%f87, %f85, %f86;
	st.local.f32 	[%rd1], %f87;

$L__BB95_8:
	bar.sync 	0;
	mov.b32 	%r83, %f250;
	shfl.sync.bfly.b32 	%r87|%p16, %r83, %r52, %r51, %r53;
	mov.b32 	%f88, %r87;
	add.f32 	%f89, %f250, %f88;
	mov.b32 	%r88, %f89;
	shfl.sync.bfly.b32 	%r90|%p17, %r88, %r56, %r51, %r53;
	mov.b32 	%f90, %r90;
	add.f32 	%f91, %f89, %f90;
	mov.b32 	%r91, %f91;
	shfl.sync.bfly.b32 	%r93|%p18, %r91, %r59, %r51, %r53;
	mov.b32 	%f92, %r93;
	add.f32 	%f93, %f91, %f92;
	mov.b32 	%r94, %f93;
	shfl.sync.bfly.b32 	%r96|%p19, %r94, %r49, %r51, %r53;
	mov.b32 	%f94, %r96;
	add.f32 	%f95, %f93, %f94;
	mov.b32 	%r97, %f95;
	shfl.sync.bfly.b32 	%r99|%p20, %r97, %r64, %r51, %r53;
	mov.b32 	%f96, %r99;
	add.f32 	%f97, %f95, %f96;
	st.local.f32 	[%rd1+4], %f97;
	st.shared.f32 	[%r7], %f97;
	bar.sync 	0;
	@%p1 bra 	$L__BB95_10;

	ld.shared.f32 	%f98, [%r3];
	mov.b32 	%r100, %f98;
	mov.u32 	%r101, 31;
	mov.u32 	%r102, 16;
	mov.u32 	%r103, -1;
	shfl.sync.bfly.b32 	%r104|%p21, %r100, %r102, %r101, %r103;
	mov.b32 	%f99, %r104;
	add.f32 	%f100, %f98, %f99;
	mov.b32 	%r105, %f100;
	mov.u32 	%r106, 8;
	shfl.sync.bfly.b32 	%r107|%p22, %r105, %r106, %r101, %r103;
	mov.b32 	%f101, %r107;
	add.f32 	%f102, %f100, %f101;
	mov.b32 	%r108, %f102;
	mov.u32 	%r109, 4;
	shfl.sync.bfly.b32 	%r110|%p23, %r108, %r109, %r101, %r103;
	mov.b32 	%f103, %r110;
	add.f32 	%f104, %f102, %f103;
	mov.b32 	%r111, %f104;
	mov.u32 	%r112, 2;
	shfl.sync.bfly.b32 	%r113|%p24, %r111, %r112, %r101, %r103;
	mov.b32 	%f105, %r113;
	add.f32 	%f106, %f104, %f105;
	mov.b32 	%r114, %f106;
	mov.u32 	%r115, 1;
	shfl.sync.bfly.b32 	%r116|%p25, %r114, %r115, %r101, %r103;
	mov.b32 	%f107, %r116;
	add.f32 	%f108, %f106, %f107;
	st.local.f32 	[%rd1+4], %f108;

$L__BB95_10:
	bar.sync 	0;
	mov.b32 	%r117, %f249;
	mov.u32 	%r118, 31;
	mov.u32 	%r119, 16;
	mov.u32 	%r120, -1;
	shfl.sync.bfly.b32 	%r121|%p27, %r117, %r119, %r118, %r120;
	mov.b32 	%f109, %r121;
	add.f32 	%f110, %f249, %f109;
	mov.b32 	%r122, %f110;
	mov.u32 	%r123, 8;
	shfl.sync.bfly.b32 	%r124|%p28, %r122, %r123, %r118, %r120;
	mov.b32 	%f111, %r124;
	add.f32 	%f112, %f110, %f111;
	mov.b32 	%r125, %f112;
	mov.u32 	%r126, 4;
	shfl.sync.bfly.b32 	%r127|%p29, %r125, %r126, %r118, %r120;
	mov.b32 	%f113, %r127;
	add.f32 	%f114, %f112, %f113;
	mov.b32 	%r128, %f114;
	mov.u32 	%r129, 2;
	shfl.sync.bfly.b32 	%r130|%p30, %r128, %r129, %r118, %r120;
	mov.b32 	%f115, %r130;
	add.f32 	%f116, %f114, %f115;
	mov.b32 	%r131, %f116;
	mov.u32 	%r132, 1;
	shfl.sync.bfly.b32 	%r133|%p31, %r131, %r132, %r118, %r120;
	mov.b32 	%f117, %r133;
	add.f32 	%f118, %f116, %f117;
	st.local.f32 	[%rd1+8], %f118;
	st.shared.f32 	[%r7], %f118;
	bar.sync 	0;
	@%p1 bra 	$L__BB95_12;

	ld.shared.f32 	%f119, [%r3];
	mov.b32 	%r134, %f119;
	shfl.sync.bfly.b32 	%r138|%p32, %r134, %r119, %r118, %r120;
	mov.b32 	%f120, %r138;
	add.f32 	%f121, %f119, %f120;
	mov.b32 	%r139, %f121;
	shfl.sync.bfly.b32 	%r141|%p33, %r139, %r123, %r118, %r120;
	mov.b32 	%f122, %r141;
	add.f32 	%f123, %f121, %f122;
	mov.b32 	%r142, %f123;
	shfl.sync.bfly.b32 	%r144|%p34, %r142, %r126, %r118, %r120;
	mov.b32 	%f124, %r144;
	add.f32 	%f125, %f123, %f124;
	mov.b32 	%r145, %f125;
	shfl.sync.bfly.b32 	%r147|%p35, %r145, %r129, %r118, %r120;
	mov.b32 	%f126, %r147;
	add.f32 	%f127, %f125, %f126;
	mov.b32 	%r148, %f127;
	shfl.sync.bfly.b32 	%r150|%p36, %r148, %r132, %r118, %r120;
	mov.b32 	%f128, %r150;
	add.f32 	%f129, %f127, %f128;
	st.local.f32 	[%rd1+8], %f129;

$L__BB95_12:
	bar.sync 	0;
	mov.b32 	%r151, %f248;
	shfl.sync.bfly.b32 	%r155|%p38, %r151, %r119, %r118, %r120;
	mov.b32 	%f130, %r155;
	add.f32 	%f131, %f248, %f130;
	mov.b32 	%r156, %f131;
	shfl.sync.bfly.b32 	%r158|%p39, %r156, %r123, %r118, %r120;
	mov.b32 	%f132, %r158;
	add.f32 	%f133, %f131, %f132;
	mov.b32 	%r159, %f133;
	shfl.sync.bfly.b32 	%r161|%p40, %r159, %r126, %r118, %r120;
	mov.b32 	%f134, %r161;
	add.f32 	%f135, %f133, %f134;
	mov.b32 	%r162, %f135;
	shfl.sync.bfly.b32 	%r164|%p41, %r162, %r129, %r118, %r120;
	mov.b32 	%f136, %r164;
	add.f32 	%f137, %f135, %f136;
	mov.b32 	%r165, %f137;
	shfl.sync.bfly.b32 	%r167|%p42, %r165, %r132, %r118, %r120;
	mov.b32 	%f138, %r167;
	add.f32 	%f139, %f137, %f138;
	st.local.f32 	[%rd1+12], %f139;
	st.shared.f32 	[%r7], %f139;
	bar.sync 	0;
	@%p1 bra 	$L__BB95_14;

	ld.shared.f32 	%f140, [%r3];
	mov.b32 	%r168, %f140;
	mov.u32 	%r169, 31;
	mov.u32 	%r170, 16;
	mov.u32 	%r171, -1;
	shfl.sync.bfly.b32 	%r172|%p43, %r168, %r170, %r169, %r171;
	mov.b32 	%f141, %r172;
	add.f32 	%f142, %f140, %f141;
	mov.b32 	%r173, %f142;
	mov.u32 	%r174, 8;
	shfl.sync.bfly.b32 	%r175|%p44, %r173, %r174, %r169, %r171;
	mov.b32 	%f143, %r175;
	add.f32 	%f144, %f142, %f143;
	mov.b32 	%r176, %f144;
	mov.u32 	%r177, 4;
	shfl.sync.bfly.b32 	%r178|%p45, %r176, %r177, %r169, %r171;
	mov.b32 	%f145, %r178;
	add.f32 	%f146, %f144, %f145;
	mov.b32 	%r179, %f146;
	mov.u32 	%r180, 2;
	shfl.sync.bfly.b32 	%r181|%p46, %r179, %r180, %r169, %r171;
	mov.b32 	%f147, %r181;
	add.f32 	%f148, %f146, %f147;
	mov.b32 	%r182, %f148;
	mov.u32 	%r183, 1;
	shfl.sync.bfly.b32 	%r184|%p47, %r182, %r183, %r169, %r171;
	mov.b32 	%f149, %r184;
	add.f32 	%f150, %f148, %f149;
	st.local.f32 	[%rd1+12], %f150;

$L__BB95_14:
	bar.sync 	0;
	mov.b32 	%r185, %f247;
	mov.u32 	%r186, 31;
	mov.u32 	%r187, 16;
	mov.u32 	%r188, -1;
	shfl.sync.bfly.b32 	%r189|%p49, %r185, %r187, %r186, %r188;
	mov.b32 	%f151, %r189;
	add.f32 	%f152, %f247, %f151;
	mov.b32 	%r190, %f152;
	mov.u32 	%r191, 8;
	shfl.sync.bfly.b32 	%r192|%p50, %r190, %r191, %r186, %r188;
	mov.b32 	%f153, %r192;
	add.f32 	%f154, %f152, %f153;
	mov.b32 	%r193, %f154;
	mov.u32 	%r194, 4;
	shfl.sync.bfly.b32 	%r195|%p51, %r193, %r194, %r186, %r188;
	mov.b32 	%f155, %r195;
	add.f32 	%f156, %f154, %f155;
	mov.b32 	%r196, %f156;
	mov.u32 	%r197, 2;
	shfl.sync.bfly.b32 	%r198|%p52, %r196, %r197, %r186, %r188;
	mov.b32 	%f157, %r198;
	add.f32 	%f158, %f156, %f157;
	mov.b32 	%r199, %f158;
	mov.u32 	%r200, 1;
	shfl.sync.bfly.b32 	%r201|%p53, %r199, %r200, %r186, %r188;
	mov.b32 	%f159, %r201;
	add.f32 	%f160, %f158, %f159;
	st.local.f32 	[%rd1+16], %f160;
	st.shared.f32 	[%r7], %f160;
	bar.sync 	0;
	@%p1 bra 	$L__BB95_16;

	ld.shared.f32 	%f161, [%r3];
	mov.b32 	%r202, %f161;
	shfl.sync.bfly.b32 	%r206|%p54, %r202, %r187, %r186, %r188;
	mov.b32 	%f162, %r206;
	add.f32 	%f163, %f161, %f162;
	mov.b32 	%r207, %f163;
	shfl.sync.bfly.b32 	%r209|%p55, %r207, %r191, %r186, %r188;
	mov.b32 	%f164, %r209;
	add.f32 	%f165, %f163, %f164;
	mov.b32 	%r210, %f165;
	shfl.sync.bfly.b32 	%r212|%p56, %r210, %r194, %r186, %r188;
	mov.b32 	%f166, %r212;
	add.f32 	%f167, %f165, %f166;
	mov.b32 	%r213, %f167;
	shfl.sync.bfly.b32 	%r215|%p57, %r213, %r197, %r186, %r188;
	mov.b32 	%f168, %r215;
	add.f32 	%f169, %f167, %f168;
	mov.b32 	%r216, %f169;
	shfl.sync.bfly.b32 	%r218|%p58, %r216, %r200, %r186, %r188;
	mov.b32 	%f170, %r218;
	add.f32 	%f171, %f169, %f170;
	st.local.f32 	[%rd1+16], %f171;

$L__BB95_16:
	bar.sync 	0;
	mov.b32 	%r219, %f246;
	shfl.sync.bfly.b32 	%r223|%p60, %r219, %r187, %r186, %r188;
	mov.b32 	%f172, %r223;
	add.f32 	%f173, %f246, %f172;
	mov.b32 	%r224, %f173;
	shfl.sync.bfly.b32 	%r226|%p61, %r224, %r191, %r186, %r188;
	mov.b32 	%f174, %r226;
	add.f32 	%f175, %f173, %f174;
	mov.b32 	%r227, %f175;
	shfl.sync.bfly.b32 	%r229|%p62, %r227, %r194, %r186, %r188;
	mov.b32 	%f176, %r229;
	add.f32 	%f177, %f175, %f176;
	mov.b32 	%r230, %f177;
	shfl.sync.bfly.b32 	%r232|%p63, %r230, %r197, %r186, %r188;
	mov.b32 	%f178, %r232;
	add.f32 	%f179, %f177, %f178;
	mov.b32 	%r233, %f179;
	shfl.sync.bfly.b32 	%r235|%p64, %r233, %r200, %r186, %r188;
	mov.b32 	%f180, %r235;
	add.f32 	%f181, %f179, %f180;
	st.local.f32 	[%rd1+20], %f181;
	st.shared.f32 	[%r7], %f181;
	bar.sync 	0;
	@%p1 bra 	$L__BB95_18;

	ld.shared.f32 	%f182, [%r3];
	mov.b32 	%r236, %f182;
	mov.u32 	%r237, 31;
	mov.u32 	%r238, 16;
	mov.u32 	%r239, -1;
	shfl.sync.bfly.b32 	%r240|%p65, %r236, %r238, %r237, %r239;
	mov.b32 	%f183, %r240;
	add.f32 	%f184, %f182, %f183;
	mov.b32 	%r241, %f184;
	mov.u32 	%r242, 8;
	shfl.sync.bfly.b32 	%r243|%p66, %r241, %r242, %r237, %r239;
	mov.b32 	%f185, %r243;
	add.f32 	%f186, %f184, %f185;
	mov.b32 	%r244, %f186;
	mov.u32 	%r245, 4;
	shfl.sync.bfly.b32 	%r246|%p67, %r244, %r245, %r237, %r239;
	mov.b32 	%f187, %r246;
	add.f32 	%f188, %f186, %f187;
	mov.b32 	%r247, %f188;
	mov.u32 	%r248, 2;
	shfl.sync.bfly.b32 	%r249|%p68, %r247, %r248, %r237, %r239;
	mov.b32 	%f189, %r249;
	add.f32 	%f190, %f188, %f189;
	mov.b32 	%r250, %f190;
	mov.u32 	%r251, 1;
	shfl.sync.bfly.b32 	%r252|%p69, %r250, %r251, %r237, %r239;
	mov.b32 	%f191, %r252;
	add.f32 	%f192, %f190, %f191;
	st.local.f32 	[%rd1+20], %f192;

$L__BB95_18:
	bar.sync 	0;
	mov.b32 	%r253, %f245;
	mov.u32 	%r254, 31;
	mov.u32 	%r255, 16;
	mov.u32 	%r256, -1;
	shfl.sync.bfly.b32 	%r257|%p71, %r253, %r255, %r254, %r256;
	mov.b32 	%f193, %r257;
	add.f32 	%f194, %f245, %f193;
	mov.b32 	%r258, %f194;
	mov.u32 	%r259, 8;
	shfl.sync.bfly.b32 	%r260|%p72, %r258, %r259, %r254, %r256;
	mov.b32 	%f195, %r260;
	add.f32 	%f196, %f194, %f195;
	mov.b32 	%r261, %f196;
	mov.u32 	%r262, 4;
	shfl.sync.bfly.b32 	%r263|%p73, %r261, %r262, %r254, %r256;
	mov.b32 	%f197, %r263;
	add.f32 	%f198, %f196, %f197;
	mov.b32 	%r264, %f198;
	mov.u32 	%r265, 2;
	shfl.sync.bfly.b32 	%r266|%p74, %r264, %r265, %r254, %r256;
	mov.b32 	%f199, %r266;
	add.f32 	%f200, %f198, %f199;
	mov.b32 	%r267, %f200;
	mov.u32 	%r268, 1;
	shfl.sync.bfly.b32 	%r269|%p75, %r267, %r268, %r254, %r256;
	mov.b32 	%f201, %r269;
	add.f32 	%f202, %f200, %f201;
	st.local.f32 	[%rd1+24], %f202;
	st.shared.f32 	[%r7], %f202;
	bar.sync 	0;
	@%p1 bra 	$L__BB95_20;

	ld.shared.f32 	%f203, [%r3];
	mov.b32 	%r270, %f203;
	shfl.sync.bfly.b32 	%r274|%p76, %r270, %r255, %r254, %r256;
	mov.b32 	%f204, %r274;
	add.f32 	%f205, %f203, %f204;
	mov.b32 	%r275, %f205;
	shfl.sync.bfly.b32 	%r277|%p77, %r275, %r259, %r254, %r256;
	mov.b32 	%f206, %r277;
	add.f32 	%f207, %f205, %f206;
	mov.b32 	%r278, %f207;
	shfl.sync.bfly.b32 	%r280|%p78, %r278, %r262, %r254, %r256;
	mov.b32 	%f208, %r280;
	add.f32 	%f209, %f207, %f208;
	mov.b32 	%r281, %f209;
	shfl.sync.bfly.b32 	%r283|%p79, %r281, %r265, %r254, %r256;
	mov.b32 	%f210, %r283;
	add.f32 	%f211, %f209, %f210;
	mov.b32 	%r284, %f211;
	shfl.sync.bfly.b32 	%r286|%p80, %r284, %r268, %r254, %r256;
	mov.b32 	%f212, %r286;
	add.f32 	%f213, %f211, %f212;
	st.local.f32 	[%rd1+24], %f213;

$L__BB95_20:
	bar.sync 	0;
	mov.b32 	%r287, %f244;
	shfl.sync.bfly.b32 	%r291|%p82, %r287, %r255, %r254, %r256;
	mov.b32 	%f214, %r291;
	add.f32 	%f215, %f244, %f214;
	mov.b32 	%r292, %f215;
	shfl.sync.bfly.b32 	%r294|%p83, %r292, %r259, %r254, %r256;
	mov.b32 	%f216, %r294;
	add.f32 	%f217, %f215, %f216;
	mov.b32 	%r295, %f217;
	shfl.sync.bfly.b32 	%r297|%p84, %r295, %r262, %r254, %r256;
	mov.b32 	%f218, %r297;
	add.f32 	%f219, %f217, %f218;
	mov.b32 	%r298, %f219;
	shfl.sync.bfly.b32 	%r300|%p85, %r298, %r265, %r254, %r256;
	mov.b32 	%f220, %r300;
	add.f32 	%f221, %f219, %f220;
	mov.b32 	%r301, %f221;
	shfl.sync.bfly.b32 	%r303|%p86, %r301, %r268, %r254, %r256;
	mov.b32 	%f222, %r303;
	add.f32 	%f223, %f221, %f222;
	st.local.f32 	[%rd1+28], %f223;
	st.shared.f32 	[%r7], %f223;
	bar.sync 	0;
	@%p1 bra 	$L__BB95_22;

	ld.shared.f32 	%f224, [%r3];
	mov.b32 	%r304, %f224;
	mov.u32 	%r305, 31;
	mov.u32 	%r306, 16;
	mov.u32 	%r307, -1;
	shfl.sync.bfly.b32 	%r308|%p87, %r304, %r306, %r305, %r307;
	mov.b32 	%f225, %r308;
	add.f32 	%f226, %f224, %f225;
	mov.b32 	%r309, %f226;
	mov.u32 	%r310, 8;
	shfl.sync.bfly.b32 	%r311|%p88, %r309, %r310, %r305, %r307;
	mov.b32 	%f227, %r311;
	add.f32 	%f228, %f226, %f227;
	mov.b32 	%r312, %f228;
	mov.u32 	%r313, 4;
	shfl.sync.bfly.b32 	%r314|%p89, %r312, %r313, %r305, %r307;
	mov.b32 	%f229, %r314;
	add.f32 	%f230, %f228, %f229;
	mov.b32 	%r315, %f230;
	mov.u32 	%r316, 2;
	shfl.sync.bfly.b32 	%r317|%p90, %r315, %r316, %r305, %r307;
	mov.b32 	%f231, %r317;
	add.f32 	%f232, %f230, %f231;
	mov.b32 	%r318, %f232;
	mov.u32 	%r319, 1;
	shfl.sync.bfly.b32 	%r320|%p91, %r318, %r319, %r305, %r307;
	mov.b32 	%f233, %r320;
	add.f32 	%f234, %f232, %f233;
	st.local.f32 	[%rd1+28], %f234;

$L__BB95_22:
	bar.sync 	0;
	setp.gt.s32 	%p92, %r2, 7;
	@%p92 bra 	$L__BB95_24;

	mad.lo.s32 	%r321, %r2, %r11, %r1;
	cvt.s64.s32 	%rd35, %r321;
	mul.lo.s32 	%r322, %r4, %r15;
	cvt.s64.s32 	%rd36, %r322;
	add.s64 	%rd37, %rd36, %rd35;
	mul.wide.s32 	%rd38, %r2, 4;
	add.s64 	%rd39, %rd1, %rd38;
	ld.local.f32 	%f235, [%rd39];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f235;}

	// end inline asm
	cvta.to.global.u64 	%rd40, %rd16;
	shl.b64 	%rd41, %rd37, 1;
	add.s64 	%rd42, %rd40, %rd41;
	st.global.u16 	[%rd42], %rs1;

$L__BB95_24:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_1_bs_160
.visible .entry ggml_matvec_f16_ncols_1_bs_160(
	.param .u64 ggml_matvec_f16_ncols_1_bs_160_param_0,
	.param .u64 ggml_matvec_f16_ncols_1_bs_160_param_1,
	.param .u64 ggml_matvec_f16_ncols_1_bs_160_param_2,
	.param .u32 ggml_matvec_f16_ncols_1_bs_160_param_3,
	.param .u32 ggml_matvec_f16_ncols_1_bs_160_param_4,
	.param .u32 ggml_matvec_f16_ncols_1_bs_160_param_5,
	.param .u32 ggml_matvec_f16_ncols_1_bs_160_param_6,
	.param .u32 ggml_matvec_f16_ncols_1_bs_160_param_7,
	.param .u32 ggml_matvec_f16_ncols_1_bs_160_param_8,
	.param .u32 ggml_matvec_f16_ncols_1_bs_160_param_9,
	.param .u32 ggml_matvec_f16_ncols_1_bs_160_param_10,
	.param .u32 ggml_matvec_f16_ncols_1_bs_160_param_11
)
{
	.reg .pred 	%p<19>;
	.reg .b16 	%rs<2>;
	.reg .f32 	%f<69>;
	.reg .b32 	%r<99>;
	.reg .b64 	%rd<44>;


	ld.param.u64 	%rd18, [ggml_matvec_f16_ncols_1_bs_160_param_0];
	ld.param.u64 	%rd19, [ggml_matvec_f16_ncols_1_bs_160_param_1];
	ld.param.u64 	%rd17, [ggml_matvec_f16_ncols_1_bs_160_param_2];
	ld.param.u32 	%r13, [ggml_matvec_f16_ncols_1_bs_160_param_3];
	ld.param.u32 	%r16, [ggml_matvec_f16_ncols_1_bs_160_param_5];
	ld.param.u32 	%r14, [ggml_matvec_f16_ncols_1_bs_160_param_7];
	ld.param.u32 	%r17, [ggml_matvec_f16_ncols_1_bs_160_param_8];
	ld.param.u32 	%r18, [ggml_matvec_f16_ncols_1_bs_160_param_9];
	ld.param.u32 	%r19, [ggml_matvec_f16_ncols_1_bs_160_param_10];
	ld.param.u32 	%r15, [ggml_matvec_f16_ncols_1_bs_160_param_11];
	cvta.to.global.u64 	%rd1, %rd19;
	cvta.to.global.u64 	%rd2, %rd18;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r20, %r1, %r17;
	mov.u32 	%r21, %ctaid.x;
	mul.lo.s32 	%r22, %r21, %r16;
	mad.lo.s32 	%r23, %r20, %r18, %r22;
	cvt.s64.s32 	%rd3, %r23;
	mul.lo.s32 	%r24, %r1, %r19;
	cvt.s64.s32 	%rd4, %r24;
	mov.u32 	%r2, %tid.x;
	setp.gt.s32 	%p1, %r2, 31;
	shl.b32 	%r25, %r2, 2;
	mov.u32 	%r26, data_mmv;
	add.s32 	%r3, %r26, %r25;
	@%p1 bra 	$L__BB96_2;

	mov.u32 	%r27, 0;
	st.shared.u32 	[%r3], %r27;

$L__BB96_2:
	bar.sync 	0;
	setp.ge.s32 	%p2, %r2, %r13;
	mov.f32 	%f67, 0f00000000;
	@%p2 bra 	$L__BB96_9;

	not.b32 	%r28, %r2;
	add.s32 	%r4, %r28, %r13;
	mul.wide.u32 	%rd20, %r4, -858993459;
	shr.u64 	%rd21, %rd20, 39;
	cvt.u32.u64 	%r29, %rd21;
	add.s32 	%r30, %r29, 1;
	and.b32  	%r96, %r30, 3;
	setp.eq.s32 	%p3, %r96, 0;
	mov.f32 	%f67, 0f00000000;
	mov.u32 	%r97, %r2;
	@%p3 bra 	$L__BB96_6;

	mul.wide.s32 	%rd22, %r2, 2;
	add.s64 	%rd23, %rd22, %rd4;
	shl.b64 	%rd24, %rd23, 1;
	add.s64 	%rd41, %rd1, %rd24;
	add.s64 	%rd25, %rd22, %rd3;
	shl.b64 	%rd26, %rd25, 1;
	add.s64 	%rd40, %rd2, %rd26;
	mov.f32 	%f67, 0f00000000;
	mov.u32 	%r97, %r2;

$L__BB96_5:
	.pragma "nounroll";
	ld.global.nc.u32 	%r31, [%rd40];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r31;
  cvt.f32.f16 %f15, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r31;
  cvt.f32.f16 %f16, high;}

	// end inline asm
	ld.global.nc.u32 	%r33, [%rd41];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r33;
  cvt.f32.f16 %f17, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r33;
  cvt.f32.f16 %f18, high;}

	// end inline asm
	fma.rn.f32 	%f19, %f15, %f17, %f67;
	fma.rn.f32 	%f67, %f16, %f18, %f19;
	add.s32 	%r97, %r97, 160;
	add.s64 	%rd41, %rd41, 640;
	add.s64 	%rd40, %rd40, 640;
	add.s32 	%r96, %r96, -1;
	setp.ne.s32 	%p4, %r96, 0;
	@%p4 bra 	$L__BB96_5;

$L__BB96_6:
	setp.lt.u32 	%p5, %r4, 480;
	@%p5 bra 	$L__BB96_9;

	mul.wide.s32 	%rd27, %r97, 2;
	add.s64 	%rd28, %rd27, %rd3;
	shl.b64 	%rd29, %rd28, 1;
	add.s64 	%rd30, %rd2, %rd29;
	add.s64 	%rd43, %rd30, 1280;
	add.s64 	%rd31, %rd27, %rd4;
	shl.b64 	%rd32, %rd31, 1;
	add.s64 	%rd33, %rd1, %rd32;
	add.s64 	%rd42, %rd33, 1280;

$L__BB96_8:
	ld.global.nc.u32 	%r35, [%rd43+-1280];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r35;
  cvt.f32.f16 %f20, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r35;
  cvt.f32.f16 %f21, high;}

	// end inline asm
	ld.global.nc.u32 	%r37, [%rd42+-1280];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r37;
  cvt.f32.f16 %f22, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r37;
  cvt.f32.f16 %f23, high;}

	// end inline asm
	fma.rn.f32 	%f36, %f20, %f22, %f67;
	fma.rn.f32 	%f37, %f21, %f23, %f36;
	ld.global.nc.u32 	%r39, [%rd43+-640];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r39;
  cvt.f32.f16 %f24, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r39;
  cvt.f32.f16 %f25, high;}

	// end inline asm
	ld.global.nc.u32 	%r41, [%rd42+-640];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r41;
  cvt.f32.f16 %f26, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r41;
  cvt.f32.f16 %f27, high;}

	// end inline asm
	fma.rn.f32 	%f38, %f24, %f26, %f37;
	fma.rn.f32 	%f39, %f25, %f27, %f38;
	ld.global.nc.u32 	%r43, [%rd43];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r43;
  cvt.f32.f16 %f28, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r43;
  cvt.f32.f16 %f29, high;}

	// end inline asm
	ld.global.nc.u32 	%r45, [%rd42];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r45;
  cvt.f32.f16 %f30, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r45;
  cvt.f32.f16 %f31, high;}

	// end inline asm
	fma.rn.f32 	%f40, %f28, %f30, %f39;
	fma.rn.f32 	%f41, %f29, %f31, %f40;
	ld.global.nc.u32 	%r47, [%rd43+640];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r47;
  cvt.f32.f16 %f32, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r47;
  cvt.f32.f16 %f33, high;}

	// end inline asm
	ld.global.nc.u32 	%r49, [%rd42+640];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r49;
  cvt.f32.f16 %f34, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r49;
  cvt.f32.f16 %f35, high;}

	// end inline asm
	fma.rn.f32 	%f42, %f32, %f34, %f41;
	fma.rn.f32 	%f67, %f33, %f35, %f42;
	add.s64 	%rd43, %rd43, 2560;
	add.s64 	%rd42, %rd42, 2560;
	add.s32 	%r97, %r97, 640;
	setp.lt.s32 	%p6, %r97, %r13;
	@%p6 bra 	$L__BB96_8;

$L__BB96_9:
	mov.b32 	%r51, %f67;
	mov.u32 	%r52, 31;
	mov.u32 	%r53, 16;
	mov.u32 	%r54, -1;
	shfl.sync.bfly.b32 	%r55|%p7, %r51, %r53, %r52, %r54;
	mov.b32 	%f43, %r55;
	add.f32 	%f44, %f67, %f43;
	mov.b32 	%r56, %f44;
	mov.u32 	%r57, 8;
	shfl.sync.bfly.b32 	%r58|%p8, %r56, %r57, %r52, %r54;
	mov.b32 	%f45, %r58;
	add.f32 	%f46, %f44, %f45;
	mov.b32 	%r59, %f46;
	mov.u32 	%r60, 4;
	shfl.sync.bfly.b32 	%r61|%p9, %r59, %r60, %r52, %r54;
	mov.b32 	%f47, %r61;
	add.f32 	%f48, %f46, %f47;
	mov.b32 	%r62, %f48;
	mov.u32 	%r63, 2;
	shfl.sync.bfly.b32 	%r64|%p10, %r62, %r63, %r52, %r54;
	mov.b32 	%f49, %r64;
	add.f32 	%f50, %f48, %f49;
	mov.b32 	%r65, %f50;
	mov.u32 	%r66, 1;
	shfl.sync.bfly.b32 	%r67|%p11, %r65, %r66, %r52, %r54;
	mov.b32 	%f51, %r67;
	add.f32 	%f68, %f50, %f51;
	shr.s32 	%r68, %r2, 31;
	shr.u32 	%r69, %r68, 27;
	add.s32 	%r70, %r2, %r69;
	shr.s32 	%r71, %r70, 5;
	shl.b32 	%r72, %r71, 2;
	add.s32 	%r74, %r26, %r72;
	st.shared.f32 	[%r74], %f68;
	bar.sync 	0;
	@%p1 bra 	$L__BB96_11;

	ld.shared.f32 	%f52, [%r3];
	mov.b32 	%r75, %f52;
	shfl.sync.bfly.b32 	%r79|%p13, %r75, %r53, %r52, %r54;
	mov.b32 	%f53, %r79;
	add.f32 	%f54, %f52, %f53;
	mov.b32 	%r80, %f54;
	shfl.sync.bfly.b32 	%r82|%p14, %r80, %r57, %r52, %r54;
	mov.b32 	%f55, %r82;
	add.f32 	%f56, %f54, %f55;
	mov.b32 	%r83, %f56;
	shfl.sync.bfly.b32 	%r85|%p15, %r83, %r60, %r52, %r54;
	mov.b32 	%f57, %r85;
	add.f32 	%f58, %f56, %f57;
	mov.b32 	%r86, %f58;
	shfl.sync.bfly.b32 	%r88|%p16, %r86, %r63, %r52, %r54;
	mov.b32 	%f59, %r88;
	add.f32 	%f60, %f58, %f59;
	mov.b32 	%r89, %f60;
	shfl.sync.bfly.b32 	%r91|%p17, %r89, %r66, %r52, %r54;
	mov.b32 	%f61, %r91;
	add.f32 	%f68, %f60, %f61;

$L__BB96_11:
	bar.sync 	0;
	setp.gt.s32 	%p18, %r2, 0;
	@%p18 bra 	$L__BB96_13;

	mad.lo.s32 	%r93, %r2, %r14, %r21;
	cvt.s64.s32 	%rd34, %r93;
	mul.lo.s32 	%r94, %r1, %r15;
	cvt.s64.s32 	%rd35, %r94;
	add.s64 	%rd36, %rd35, %rd34;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f68;}

	// end inline asm
	cvta.to.global.u64 	%rd37, %rd17;
	shl.b64 	%rd38, %rd36, 1;
	add.s64 	%rd39, %rd37, %rd38;
	st.global.u16 	[%rd39], %rs1;

$L__BB96_13:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_2_bs_160
.visible .entry ggml_matvec_f16_ncols_2_bs_160(
	.param .u64 ggml_matvec_f16_ncols_2_bs_160_param_0,
	.param .u64 ggml_matvec_f16_ncols_2_bs_160_param_1,
	.param .u64 ggml_matvec_f16_ncols_2_bs_160_param_2,
	.param .u32 ggml_matvec_f16_ncols_2_bs_160_param_3,
	.param .u32 ggml_matvec_f16_ncols_2_bs_160_param_4,
	.param .u32 ggml_matvec_f16_ncols_2_bs_160_param_5,
	.param .u32 ggml_matvec_f16_ncols_2_bs_160_param_6,
	.param .u32 ggml_matvec_f16_ncols_2_bs_160_param_7,
	.param .u32 ggml_matvec_f16_ncols_2_bs_160_param_8,
	.param .u32 ggml_matvec_f16_ncols_2_bs_160_param_9,
	.param .u32 ggml_matvec_f16_ncols_2_bs_160_param_10,
	.param .u32 ggml_matvec_f16_ncols_2_bs_160_param_11
)
{
	.local .align 8 .b8 	__local_depot97[8];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<30>;
	.reg .b16 	%rs<2>;
	.reg .f32 	%f<116>;
	.reg .b32 	%r<143>;
	.reg .b64 	%rd<66>;


	mov.u64 	%SPL, __local_depot97;
	ld.param.u64 	%rd27, [ggml_matvec_f16_ncols_2_bs_160_param_0];
	ld.param.u64 	%rd28, [ggml_matvec_f16_ncols_2_bs_160_param_1];
	ld.param.u64 	%rd26, [ggml_matvec_f16_ncols_2_bs_160_param_2];
	ld.param.u32 	%r15, [ggml_matvec_f16_ncols_2_bs_160_param_3];
	ld.param.u32 	%r19, [ggml_matvec_f16_ncols_2_bs_160_param_5];
	ld.param.u32 	%r16, [ggml_matvec_f16_ncols_2_bs_160_param_6];
	ld.param.u32 	%r17, [ggml_matvec_f16_ncols_2_bs_160_param_7];
	ld.param.u32 	%r20, [ggml_matvec_f16_ncols_2_bs_160_param_8];
	ld.param.u32 	%r21, [ggml_matvec_f16_ncols_2_bs_160_param_9];
	ld.param.u32 	%r22, [ggml_matvec_f16_ncols_2_bs_160_param_10];
	ld.param.u32 	%r18, [ggml_matvec_f16_ncols_2_bs_160_param_11];
	cvta.to.global.u64 	%rd1, %rd28;
	cvta.to.global.u64 	%rd2, %rd27;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r23, %r1, %r20;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r24, %r2, %r19;
	mad.lo.s32 	%r25, %r23, %r21, %r24;
	cvt.s64.s32 	%rd4, %r25;
	mul.lo.s32 	%r26, %r1, %r22;
	cvt.s64.s32 	%rd5, %r26;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r27, %r3, 2;
	mov.u32 	%r28, data_mmv;
	add.s32 	%r4, %r28, %r27;
	@%p1 bra 	$L__BB97_2;

	mov.u32 	%r29, 0;
	st.shared.u32 	[%r4], %r29;

$L__BB97_2:
	bar.sync 	0;
	mov.f32 	%f114, 0f00000000;
	st.local.v2.f32 	[%rd3], {%f114, %f114};
	setp.ge.s32 	%p2, %r3, %r15;
	mov.f32 	%f115, %f114;
	@%p2 bra 	$L__BB97_11;

	not.b32 	%r30, %r3;
	add.s32 	%r5, %r30, %r15;
	mul.wide.u32 	%rd30, %r5, -858993459;
	shr.u64 	%rd31, %rd30, 39;
	cvt.u32.u64 	%r31, %rd31;
	add.s32 	%r32, %r31, 1;
	and.b32  	%r140, %r32, 3;
	setp.eq.s32 	%p3, %r140, 0;
	mov.f32 	%f114, 0f00000000;
	mov.u32 	%r141, %r3;
	@%p3 bra 	$L__BB97_7;

	mul.wide.s32 	%rd32, %r16, 2;
	mul.wide.s32 	%rd33, %r3, 2;
	add.s64 	%rd34, %rd32, %rd33;
	add.s64 	%rd35, %rd34, %rd5;
	shl.b64 	%rd36, %rd35, 1;
	add.s64 	%rd62, %rd1, %rd36;
	add.s64 	%rd37, %rd33, %rd5;
	shl.b64 	%rd38, %rd37, 1;
	add.s64 	%rd61, %rd1, %rd38;
	add.s64 	%rd39, %rd33, %rd4;
	shl.b64 	%rd40, %rd39, 1;
	add.s64 	%rd60, %rd2, %rd40;
	mov.f32 	%f114, 0f00000000;
	mov.f32 	%f115, %f114;
	mov.u32 	%r141, %r3;

$L__BB97_5:
	.pragma "nounroll";
	ld.global.nc.u32 	%r33, [%rd60];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r33;
  cvt.f32.f16 %f19, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r33;
  cvt.f32.f16 %f20, high;}

	// end inline asm
	ld.global.nc.u32 	%r35, [%rd61];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r35;
  cvt.f32.f16 %f21, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r35;
  cvt.f32.f16 %f22, high;}

	// end inline asm
	fma.rn.f32 	%f25, %f19, %f21, %f115;
	fma.rn.f32 	%f115, %f20, %f22, %f25;
	ld.global.nc.u32 	%r37, [%rd62];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r37;
  cvt.f32.f16 %f23, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r37;
  cvt.f32.f16 %f24, high;}

	// end inline asm
	fma.rn.f32 	%f26, %f19, %f23, %f114;
	fma.rn.f32 	%f114, %f20, %f24, %f26;
	add.s32 	%r141, %r141, 160;
	add.s64 	%rd62, %rd62, 640;
	add.s64 	%rd61, %rd61, 640;
	add.s64 	%rd60, %rd60, 640;
	add.s32 	%r140, %r140, -1;
	setp.ne.s32 	%p4, %r140, 0;
	@%p4 bra 	$L__BB97_5;

	st.local.v2.f32 	[%rd3], {%f115, %f114};

$L__BB97_7:
	setp.lt.u32 	%p5, %r5, 480;
	@%p5 bra 	$L__BB97_11;

	mul.wide.s32 	%rd41, %r141, 2;
	add.s64 	%rd42, %rd41, %rd4;
	shl.b64 	%rd43, %rd42, 1;
	add.s64 	%rd44, %rd2, %rd43;
	add.s64 	%rd65, %rd44, 1280;
	add.s64 	%rd45, %rd41, %rd5;
	shl.b64 	%rd46, %rd45, 1;
	add.s64 	%rd47, %rd1, %rd46;
	add.s64 	%rd64, %rd47, 1920;
	mul.wide.s32 	%rd48, %r16, 2;
	add.s64 	%rd49, %rd45, %rd48;
	shl.b64 	%rd50, %rd49, 1;
	add.s64 	%rd51, %rd1, %rd50;
	add.s64 	%rd63, %rd51, 1280;

$L__BB97_9:
	ld.global.nc.u32 	%r39, [%rd65+-1280];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r39;
  cvt.f32.f16 %f27, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r39;
  cvt.f32.f16 %f28, high;}

	// end inline asm
	ld.global.nc.u32 	%r41, [%rd64+-1920];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r41;
  cvt.f32.f16 %f29, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r41;
  cvt.f32.f16 %f30, high;}

	// end inline asm
	fma.rn.f32 	%f51, %f27, %f29, %f115;
	fma.rn.f32 	%f52, %f28, %f30, %f51;
	ld.global.nc.u32 	%r43, [%rd63+-1280];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r43;
  cvt.f32.f16 %f31, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r43;
  cvt.f32.f16 %f32, high;}

	// end inline asm
	fma.rn.f32 	%f53, %f27, %f31, %f114;
	fma.rn.f32 	%f54, %f28, %f32, %f53;
	ld.global.nc.u32 	%r45, [%rd65+-640];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r45;
  cvt.f32.f16 %f33, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r45;
  cvt.f32.f16 %f34, high;}

	// end inline asm
	ld.global.nc.u32 	%r47, [%rd64+-1280];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r47;
  cvt.f32.f16 %f35, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r47;
  cvt.f32.f16 %f36, high;}

	// end inline asm
	fma.rn.f32 	%f55, %f33, %f35, %f52;
	fma.rn.f32 	%f56, %f34, %f36, %f55;
	ld.global.nc.u32 	%r49, [%rd63+-640];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r49;
  cvt.f32.f16 %f37, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r49;
  cvt.f32.f16 %f38, high;}

	// end inline asm
	fma.rn.f32 	%f57, %f33, %f37, %f54;
	fma.rn.f32 	%f58, %f34, %f38, %f57;
	ld.global.nc.u32 	%r51, [%rd65];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r51;
  cvt.f32.f16 %f39, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r51;
  cvt.f32.f16 %f40, high;}

	// end inline asm
	ld.global.nc.u32 	%r53, [%rd64+-640];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r53;
  cvt.f32.f16 %f41, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r53;
  cvt.f32.f16 %f42, high;}

	// end inline asm
	fma.rn.f32 	%f59, %f39, %f41, %f56;
	fma.rn.f32 	%f60, %f40, %f42, %f59;
	ld.global.nc.u32 	%r55, [%rd63];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r55;
  cvt.f32.f16 %f43, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r55;
  cvt.f32.f16 %f44, high;}

	// end inline asm
	fma.rn.f32 	%f61, %f39, %f43, %f58;
	fma.rn.f32 	%f62, %f40, %f44, %f61;
	ld.global.nc.u32 	%r57, [%rd65+640];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r57;
  cvt.f32.f16 %f45, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r57;
  cvt.f32.f16 %f46, high;}

	// end inline asm
	ld.global.nc.u32 	%r59, [%rd64];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r59;
  cvt.f32.f16 %f47, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r59;
  cvt.f32.f16 %f48, high;}

	// end inline asm
	fma.rn.f32 	%f63, %f45, %f47, %f60;
	fma.rn.f32 	%f115, %f46, %f48, %f63;
	ld.global.nc.u32 	%r61, [%rd63+640];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r61;
  cvt.f32.f16 %f49, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r61;
  cvt.f32.f16 %f50, high;}

	// end inline asm
	fma.rn.f32 	%f64, %f45, %f49, %f62;
	fma.rn.f32 	%f114, %f46, %f50, %f64;
	add.s64 	%rd65, %rd65, 2560;
	add.s64 	%rd64, %rd64, 2560;
	add.s64 	%rd63, %rd63, 2560;
	add.s32 	%r141, %r141, 640;
	setp.lt.s32 	%p6, %r141, %r15;
	@%p6 bra 	$L__BB97_9;

	st.local.v2.f32 	[%rd3], {%f115, %f114};

$L__BB97_11:
	shr.s32 	%r63, %r3, 31;
	shr.u32 	%r64, %r63, 27;
	add.s32 	%r65, %r3, %r64;
	shr.s32 	%r66, %r65, 5;
	shl.b32 	%r67, %r66, 2;
	add.s32 	%r14, %r28, %r67;
	mov.u32 	%r69, 2;
	mov.b32 	%r70, %f115;
	mov.u32 	%r71, 31;
	mov.u32 	%r72, 16;
	mov.u32 	%r73, -1;
	shfl.sync.bfly.b32 	%r74|%p7, %r70, %r72, %r71, %r73;
	mov.b32 	%f65, %r74;
	add.f32 	%f66, %f115, %f65;
	mov.b32 	%r75, %f66;
	mov.u32 	%r76, 8;
	shfl.sync.bfly.b32 	%r77|%p8, %r75, %r76, %r71, %r73;
	mov.b32 	%f67, %r77;
	add.f32 	%f68, %f66, %f67;
	mov.b32 	%r78, %f68;
	mov.u32 	%r79, 4;
	shfl.sync.bfly.b32 	%r80|%p9, %r78, %r79, %r71, %r73;
	mov.b32 	%f69, %r80;
	add.f32 	%f70, %f68, %f69;
	mov.b32 	%r81, %f70;
	shfl.sync.bfly.b32 	%r82|%p10, %r81, %r69, %r71, %r73;
	mov.b32 	%f71, %r82;
	add.f32 	%f72, %f70, %f71;
	mov.b32 	%r83, %f72;
	mov.u32 	%r84, 1;
	shfl.sync.bfly.b32 	%r85|%p11, %r83, %r84, %r71, %r73;
	mov.b32 	%f73, %r85;
	add.f32 	%f74, %f72, %f73;
	st.local.f32 	[%rd3], %f74;
	st.shared.f32 	[%r14], %f74;
	bar.sync 	0;
	@%p1 bra 	$L__BB97_13;

	ld.shared.f32 	%f75, [%r4];
	mov.b32 	%r86, %f75;
	shfl.sync.bfly.b32 	%r90|%p13, %r86, %r72, %r71, %r73;
	mov.b32 	%f76, %r90;
	add.f32 	%f77, %f75, %f76;
	mov.b32 	%r91, %f77;
	shfl.sync.bfly.b32 	%r93|%p14, %r91, %r76, %r71, %r73;
	mov.b32 	%f78, %r93;
	add.f32 	%f79, %f77, %f78;
	mov.b32 	%r94, %f79;
	shfl.sync.bfly.b32 	%r96|%p15, %r94, %r79, %r71, %r73;
	mov.b32 	%f80, %r96;
	add.f32 	%f81, %f79, %f80;
	mov.b32 	%r97, %f81;
	shfl.sync.bfly.b32 	%r99|%p16, %r97, %r69, %r71, %r73;
	mov.b32 	%f82, %r99;
	add.f32 	%f83, %f81, %f82;
	mov.b32 	%r100, %f83;
	shfl.sync.bfly.b32 	%r102|%p17, %r100, %r84, %r71, %r73;
	mov.b32 	%f84, %r102;
	add.f32 	%f85, %f83, %f84;
	st.local.f32 	[%rd3], %f85;

$L__BB97_13:
	bar.sync 	0;
	mov.b32 	%r103, %f114;
	shfl.sync.bfly.b32 	%r107|%p19, %r103, %r72, %r71, %r73;
	mov.b32 	%f86, %r107;
	add.f32 	%f87, %f114, %f86;
	mov.b32 	%r108, %f87;
	shfl.sync.bfly.b32 	%r110|%p20, %r108, %r76, %r71, %r73;
	mov.b32 	%f88, %r110;
	add.f32 	%f89, %f87, %f88;
	mov.b32 	%r111, %f89;
	shfl.sync.bfly.b32 	%r113|%p21, %r111, %r79, %r71, %r73;
	mov.b32 	%f90, %r113;
	add.f32 	%f91, %f89, %f90;
	mov.b32 	%r114, %f91;
	shfl.sync.bfly.b32 	%r116|%p22, %r114, %r69, %r71, %r73;
	mov.b32 	%f92, %r116;
	add.f32 	%f93, %f91, %f92;
	mov.b32 	%r117, %f93;
	shfl.sync.bfly.b32 	%r119|%p23, %r117, %r84, %r71, %r73;
	mov.b32 	%f94, %r119;
	add.f32 	%f95, %f93, %f94;
	st.local.f32 	[%rd3+4], %f95;
	st.shared.f32 	[%r14], %f95;
	bar.sync 	0;
	@%p1 bra 	$L__BB97_15;

	ld.shared.f32 	%f96, [%r4];
	mov.b32 	%r120, %f96;
	mov.u32 	%r121, 31;
	mov.u32 	%r122, 16;
	mov.u32 	%r123, -1;
	shfl.sync.bfly.b32 	%r124|%p24, %r120, %r122, %r121, %r123;
	mov.b32 	%f97, %r124;
	add.f32 	%f98, %f96, %f97;
	mov.b32 	%r125, %f98;
	mov.u32 	%r126, 8;
	shfl.sync.bfly.b32 	%r127|%p25, %r125, %r126, %r121, %r123;
	mov.b32 	%f99, %r127;
	add.f32 	%f100, %f98, %f99;
	mov.b32 	%r128, %f100;
	mov.u32 	%r129, 4;
	shfl.sync.bfly.b32 	%r130|%p26, %r128, %r129, %r121, %r123;
	mov.b32 	%f101, %r130;
	add.f32 	%f102, %f100, %f101;
	mov.b32 	%r131, %f102;
	mov.u32 	%r132, 2;
	shfl.sync.bfly.b32 	%r133|%p27, %r131, %r132, %r121, %r123;
	mov.b32 	%f103, %r133;
	add.f32 	%f104, %f102, %f103;
	mov.b32 	%r134, %f104;
	mov.u32 	%r135, 1;
	shfl.sync.bfly.b32 	%r136|%p28, %r134, %r135, %r121, %r123;
	mov.b32 	%f105, %r136;
	add.f32 	%f106, %f104, %f105;
	st.local.f32 	[%rd3+4], %f106;

$L__BB97_15:
	bar.sync 	0;
	setp.gt.s32 	%p29, %r3, 1;
	@%p29 bra 	$L__BB97_17;

	mad.lo.s32 	%r137, %r3, %r17, %r2;
	cvt.s64.s32 	%rd52, %r137;
	mul.lo.s32 	%r138, %r1, %r18;
	cvt.s64.s32 	%rd53, %r138;
	add.s64 	%rd54, %rd53, %rd52;
	mul.wide.s32 	%rd55, %r3, 4;
	add.s64 	%rd56, %rd3, %rd55;
	ld.local.f32 	%f107, [%rd56];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f107;}

	// end inline asm
	cvta.to.global.u64 	%rd57, %rd26;
	shl.b64 	%rd58, %rd54, 1;
	add.s64 	%rd59, %rd57, %rd58;
	st.global.u16 	[%rd59], %rs1;

$L__BB97_17:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_3_bs_160
.visible .entry ggml_matvec_f16_ncols_3_bs_160(
	.param .u64 ggml_matvec_f16_ncols_3_bs_160_param_0,
	.param .u64 ggml_matvec_f16_ncols_3_bs_160_param_1,
	.param .u64 ggml_matvec_f16_ncols_3_bs_160_param_2,
	.param .u32 ggml_matvec_f16_ncols_3_bs_160_param_3,
	.param .u32 ggml_matvec_f16_ncols_3_bs_160_param_4,
	.param .u32 ggml_matvec_f16_ncols_3_bs_160_param_5,
	.param .u32 ggml_matvec_f16_ncols_3_bs_160_param_6,
	.param .u32 ggml_matvec_f16_ncols_3_bs_160_param_7,
	.param .u32 ggml_matvec_f16_ncols_3_bs_160_param_8,
	.param .u32 ggml_matvec_f16_ncols_3_bs_160_param_9,
	.param .u32 ggml_matvec_f16_ncols_3_bs_160_param_10,
	.param .u32 ggml_matvec_f16_ncols_3_bs_160_param_11
)
{
	.local .align 4 .b8 	__local_depot98[12];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<41>;
	.reg .b16 	%rs<2>;
	.reg .f32 	%f<168>;
	.reg .b32 	%r<194>;
	.reg .b64 	%rd<74>;


	mov.u64 	%SPL, __local_depot98;
	ld.param.u64 	%rd29, [ggml_matvec_f16_ncols_3_bs_160_param_0];
	ld.param.u64 	%rd30, [ggml_matvec_f16_ncols_3_bs_160_param_1];
	ld.param.u64 	%rd28, [ggml_matvec_f16_ncols_3_bs_160_param_2];
	ld.param.u32 	%r15, [ggml_matvec_f16_ncols_3_bs_160_param_3];
	ld.param.u32 	%r19, [ggml_matvec_f16_ncols_3_bs_160_param_5];
	ld.param.u32 	%r16, [ggml_matvec_f16_ncols_3_bs_160_param_6];
	ld.param.u32 	%r17, [ggml_matvec_f16_ncols_3_bs_160_param_7];
	ld.param.u32 	%r20, [ggml_matvec_f16_ncols_3_bs_160_param_8];
	ld.param.u32 	%r21, [ggml_matvec_f16_ncols_3_bs_160_param_9];
	ld.param.u32 	%r22, [ggml_matvec_f16_ncols_3_bs_160_param_10];
	ld.param.u32 	%r18, [ggml_matvec_f16_ncols_3_bs_160_param_11];
	cvta.to.global.u64 	%rd73, %rd30;
	cvta.to.global.u64 	%rd2, %rd29;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r23, %r1, %r20;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r24, %r2, %r19;
	mad.lo.s32 	%r25, %r23, %r21, %r24;
	cvt.s64.s32 	%rd4, %r25;
	mul.lo.s32 	%r26, %r1, %r22;
	cvt.s64.s32 	%rd5, %r26;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r27, %r3, 2;
	mov.u32 	%r28, data_mmv;
	add.s32 	%r4, %r28, %r27;
	@%p1 bra 	$L__BB98_2;

	mov.u32 	%r29, 0;
	st.shared.u32 	[%r4], %r29;

$L__BB98_2:
	bar.sync 	0;
	mov.f32 	%f165, 0f00000000;
	mov.u32 	%r30, 0;
	st.local.u32 	[%rd3], %r30;
	st.local.u32 	[%rd3+4], %r30;
	st.local.u32 	[%rd3+8], %r30;
	setp.ge.s32 	%p2, %r3, %r15;
	mov.f32 	%f166, %f165;
	mov.f32 	%f167, %f165;
	@%p2 bra 	$L__BB98_11;

	not.b32 	%r31, %r3;
	add.s32 	%r5, %r31, %r15;
	mul.wide.u32 	%rd32, %r5, -858993459;
	shr.u64 	%rd33, %rd32, 39;
	cvt.u32.u64 	%r32, %rd33;
	add.s32 	%r33, %r32, 1;
	and.b32  	%r191, %r33, 3;
	setp.eq.s32 	%p3, %r191, 0;
	mov.f32 	%f165, 0f00000000;
	mov.u32 	%r192, %r3;
	@%p3 bra 	$L__BB98_7;

	shl.b32 	%r34, %r16, 1;
	add.s32 	%r35, %r3, %r34;
	mul.wide.s32 	%rd34, %r35, 2;
	add.s64 	%rd35, %rd34, %rd5;
	shl.b64 	%rd36, %rd35, 1;
	add.s64 	%rd71, %rd73, %rd36;
	mul.wide.s32 	%rd37, %r16, 2;
	mul.wide.s32 	%rd38, %r3, 2;
	add.s64 	%rd39, %rd37, %rd38;
	add.s64 	%rd40, %rd39, %rd5;
	shl.b64 	%rd41, %rd40, 1;
	add.s64 	%rd70, %rd73, %rd41;
	add.s64 	%rd42, %rd38, %rd5;
	shl.b64 	%rd43, %rd42, 1;
	add.s64 	%rd69, %rd73, %rd43;
	add.s64 	%rd44, %rd38, %rd4;
	shl.b64 	%rd45, %rd44, 1;
	add.s64 	%rd68, %rd2, %rd45;
	mov.f32 	%f165, 0f00000000;
	mov.f32 	%f166, %f165;
	mov.f32 	%f167, %f165;
	mov.u32 	%r192, %r3;

$L__BB98_5:
	.pragma "nounroll";
	ld.global.nc.u32 	%r36, [%rd68];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r36;
  cvt.f32.f16 %f28, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r36;
  cvt.f32.f16 %f29, high;}

	// end inline asm
	ld.global.nc.u32 	%r38, [%rd69];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r38;
  cvt.f32.f16 %f30, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r38;
  cvt.f32.f16 %f31, high;}

	// end inline asm
	fma.rn.f32 	%f36, %f28, %f30, %f167;
	fma.rn.f32 	%f167, %f29, %f31, %f36;
	ld.global.nc.u32 	%r40, [%rd70];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r40;
  cvt.f32.f16 %f32, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r40;
  cvt.f32.f16 %f33, high;}

	// end inline asm
	fma.rn.f32 	%f37, %f28, %f32, %f166;
	fma.rn.f32 	%f166, %f29, %f33, %f37;
	ld.global.nc.u32 	%r42, [%rd71];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r42;
  cvt.f32.f16 %f34, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r42;
  cvt.f32.f16 %f35, high;}

	// end inline asm
	fma.rn.f32 	%f38, %f28, %f34, %f165;
	fma.rn.f32 	%f165, %f29, %f35, %f38;
	add.s32 	%r192, %r192, 160;
	add.s64 	%rd71, %rd71, 640;
	add.s64 	%rd70, %rd70, 640;
	add.s64 	%rd69, %rd69, 640;
	add.s64 	%rd68, %rd68, 640;
	add.s32 	%r191, %r191, -1;
	setp.ne.s32 	%p4, %r191, 0;
	@%p4 bra 	$L__BB98_5;

	st.local.f32 	[%rd3], %f167;
	st.local.f32 	[%rd3+4], %f166;
	st.local.f32 	[%rd3+8], %f165;

$L__BB98_7:
	setp.lt.u32 	%p5, %r5, 480;
	@%p5 bra 	$L__BB98_11;

	add.s32 	%r44, %r192, %r16;
	shl.b32 	%r45, %r16, 1;
	add.s32 	%r46, %r192, %r45;
	add.s32 	%r47, %r44, 160;
	mul.wide.s32 	%rd46, %r47, 4;
	shl.b64 	%rd47, %rd5, 1;
	add.s64 	%rd19, %rd46, %rd47;
	mul.wide.s32 	%rd48, %r46, 4;
	add.s64 	%rd20, %rd48, %rd47;
	mul.wide.s32 	%rd49, %r192, 2;
	add.s64 	%rd50, %rd49, %rd4;
	shl.b64 	%rd51, %rd50, 1;
	add.s64 	%rd52, %rd2, %rd51;
	add.s64 	%rd72, %rd52, 1280;
	mul.wide.s32 	%rd53, %r192, 4;
	add.s64 	%rd22, %rd53, %rd47;
	mul.wide.s32 	%rd54, %r16, 4;
	add.s64 	%rd55, %rd53, %rd54;
	add.s64 	%rd23, %rd55, %rd47;

$L__BB98_9:
	ld.global.nc.u32 	%r48, [%rd72+-1280];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r48;
  cvt.f32.f16 %f39, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r48;
  cvt.f32.f16 %f40, high;}

	// end inline asm
	add.s64 	%rd56, %rd73, %rd22;
	ld.global.nc.u32 	%r50, [%rd56];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r50;
  cvt.f32.f16 %f41, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r50;
  cvt.f32.f16 %f42, high;}

	// end inline asm
	fma.rn.f32 	%f71, %f39, %f41, %f167;
	fma.rn.f32 	%f72, %f40, %f42, %f71;
	add.s64 	%rd57, %rd73, %rd23;
	ld.global.nc.u32 	%r52, [%rd57];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r52;
  cvt.f32.f16 %f43, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r52;
  cvt.f32.f16 %f44, high;}

	// end inline asm
	fma.rn.f32 	%f73, %f39, %f43, %f166;
	fma.rn.f32 	%f74, %f40, %f44, %f73;
	add.s64 	%rd58, %rd73, %rd20;
	ld.global.nc.u32 	%r54, [%rd58];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r54;
  cvt.f32.f16 %f45, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r54;
  cvt.f32.f16 %f46, high;}

	// end inline asm
	fma.rn.f32 	%f75, %f39, %f45, %f165;
	fma.rn.f32 	%f76, %f40, %f46, %f75;
	ld.global.nc.u32 	%r56, [%rd72+-640];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r56;
  cvt.f32.f16 %f47, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r56;
  cvt.f32.f16 %f48, high;}

	// end inline asm
	ld.global.nc.u32 	%r58, [%rd56+640];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r58;
  cvt.f32.f16 %f49, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r58;
  cvt.f32.f16 %f50, high;}

	// end inline asm
	fma.rn.f32 	%f77, %f47, %f49, %f72;
	fma.rn.f32 	%f78, %f48, %f50, %f77;
	add.s64 	%rd59, %rd73, %rd19;
	ld.global.nc.u32 	%r60, [%rd59];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r60;
  cvt.f32.f16 %f51, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r60;
  cvt.f32.f16 %f52, high;}

	// end inline asm
	fma.rn.f32 	%f79, %f47, %f51, %f74;
	fma.rn.f32 	%f80, %f48, %f52, %f79;
	ld.global.nc.u32 	%r62, [%rd58+640];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r62;
  cvt.f32.f16 %f53, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r62;
  cvt.f32.f16 %f54, high;}

	// end inline asm
	fma.rn.f32 	%f81, %f47, %f53, %f76;
	fma.rn.f32 	%f82, %f48, %f54, %f81;
	ld.global.nc.u32 	%r64, [%rd72];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r64;
  cvt.f32.f16 %f55, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r64;
  cvt.f32.f16 %f56, high;}

	// end inline asm
	ld.global.nc.u32 	%r66, [%rd56+1280];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r66;
  cvt.f32.f16 %f57, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r66;
  cvt.f32.f16 %f58, high;}

	// end inline asm
	fma.rn.f32 	%f83, %f55, %f57, %f78;
	fma.rn.f32 	%f84, %f56, %f58, %f83;
	ld.global.nc.u32 	%r68, [%rd59+640];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r68;
  cvt.f32.f16 %f59, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r68;
  cvt.f32.f16 %f60, high;}

	// end inline asm
	fma.rn.f32 	%f85, %f55, %f59, %f80;
	fma.rn.f32 	%f86, %f56, %f60, %f85;
	ld.global.nc.u32 	%r70, [%rd58+1280];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r70;
  cvt.f32.f16 %f61, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r70;
  cvt.f32.f16 %f62, high;}

	// end inline asm
	fma.rn.f32 	%f87, %f55, %f61, %f82;
	fma.rn.f32 	%f88, %f56, %f62, %f87;
	ld.global.nc.u32 	%r72, [%rd72+640];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r72;
  cvt.f32.f16 %f63, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r72;
  cvt.f32.f16 %f64, high;}

	// end inline asm
	ld.global.nc.u32 	%r74, [%rd56+1920];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r74;
  cvt.f32.f16 %f65, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r74;
  cvt.f32.f16 %f66, high;}

	// end inline asm
	fma.rn.f32 	%f89, %f63, %f65, %f84;
	fma.rn.f32 	%f167, %f64, %f66, %f89;
	ld.global.nc.u32 	%r76, [%rd59+1280];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r76;
  cvt.f32.f16 %f67, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r76;
  cvt.f32.f16 %f68, high;}

	// end inline asm
	fma.rn.f32 	%f90, %f63, %f67, %f86;
	fma.rn.f32 	%f166, %f64, %f68, %f90;
	ld.global.nc.u32 	%r78, [%rd58+1920];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r78;
  cvt.f32.f16 %f69, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r78;
  cvt.f32.f16 %f70, high;}

	// end inline asm
	fma.rn.f32 	%f91, %f63, %f69, %f88;
	fma.rn.f32 	%f165, %f64, %f70, %f91;
	add.s64 	%rd73, %rd73, 2560;
	add.s64 	%rd72, %rd72, 2560;
	add.s32 	%r192, %r192, 640;
	setp.lt.s32 	%p6, %r192, %r15;
	@%p6 bra 	$L__BB98_9;

	st.local.f32 	[%rd3], %f167;
	st.local.f32 	[%rd3+4], %f166;
	st.local.f32 	[%rd3+8], %f165;

$L__BB98_11:
	shr.s32 	%r80, %r3, 31;
	shr.u32 	%r81, %r80, 27;
	add.s32 	%r82, %r3, %r81;
	shr.s32 	%r83, %r82, 5;
	shl.b32 	%r84, %r83, 2;
	add.s32 	%r14, %r28, %r84;
	mov.u32 	%r86, 2;
	mov.b32 	%r87, %f167;
	mov.u32 	%r88, 31;
	mov.u32 	%r89, 16;
	mov.u32 	%r90, -1;
	shfl.sync.bfly.b32 	%r91|%p7, %r87, %r89, %r88, %r90;
	mov.b32 	%f92, %r91;
	add.f32 	%f93, %f167, %f92;
	mov.b32 	%r92, %f93;
	mov.u32 	%r93, 8;
	shfl.sync.bfly.b32 	%r94|%p8, %r92, %r93, %r88, %r90;
	mov.b32 	%f94, %r94;
	add.f32 	%f95, %f93, %f94;
	mov.b32 	%r95, %f95;
	mov.u32 	%r96, 4;
	shfl.sync.bfly.b32 	%r97|%p9, %r95, %r96, %r88, %r90;
	mov.b32 	%f96, %r97;
	add.f32 	%f97, %f95, %f96;
	mov.b32 	%r98, %f97;
	shfl.sync.bfly.b32 	%r99|%p10, %r98, %r86, %r88, %r90;
	mov.b32 	%f98, %r99;
	add.f32 	%f99, %f97, %f98;
	mov.b32 	%r100, %f99;
	mov.u32 	%r101, 1;
	shfl.sync.bfly.b32 	%r102|%p11, %r100, %r101, %r88, %r90;
	mov.b32 	%f100, %r102;
	add.f32 	%f101, %f99, %f100;
	st.local.f32 	[%rd3], %f101;
	st.shared.f32 	[%r14], %f101;
	bar.sync 	0;
	@%p1 bra 	$L__BB98_13;

	ld.shared.f32 	%f102, [%r4];
	mov.b32 	%r103, %f102;
	shfl.sync.bfly.b32 	%r107|%p13, %r103, %r89, %r88, %r90;
	mov.b32 	%f103, %r107;
	add.f32 	%f104, %f102, %f103;
	mov.b32 	%r108, %f104;
	shfl.sync.bfly.b32 	%r110|%p14, %r108, %r93, %r88, %r90;
	mov.b32 	%f105, %r110;
	add.f32 	%f106, %f104, %f105;
	mov.b32 	%r111, %f106;
	shfl.sync.bfly.b32 	%r113|%p15, %r111, %r96, %r88, %r90;
	mov.b32 	%f107, %r113;
	add.f32 	%f108, %f106, %f107;
	mov.b32 	%r114, %f108;
	shfl.sync.bfly.b32 	%r116|%p16, %r114, %r86, %r88, %r90;
	mov.b32 	%f109, %r116;
	add.f32 	%f110, %f108, %f109;
	mov.b32 	%r117, %f110;
	shfl.sync.bfly.b32 	%r119|%p17, %r117, %r101, %r88, %r90;
	mov.b32 	%f111, %r119;
	add.f32 	%f112, %f110, %f111;
	st.local.f32 	[%rd3], %f112;

$L__BB98_13:
	bar.sync 	0;
	mov.b32 	%r120, %f166;
	shfl.sync.bfly.b32 	%r124|%p19, %r120, %r89, %r88, %r90;
	mov.b32 	%f113, %r124;
	add.f32 	%f114, %f166, %f113;
	mov.b32 	%r125, %f114;
	shfl.sync.bfly.b32 	%r127|%p20, %r125, %r93, %r88, %r90;
	mov.b32 	%f115, %r127;
	add.f32 	%f116, %f114, %f115;
	mov.b32 	%r128, %f116;
	shfl.sync.bfly.b32 	%r130|%p21, %r128, %r96, %r88, %r90;
	mov.b32 	%f117, %r130;
	add.f32 	%f118, %f116, %f117;
	mov.b32 	%r131, %f118;
	shfl.sync.bfly.b32 	%r133|%p22, %r131, %r86, %r88, %r90;
	mov.b32 	%f119, %r133;
	add.f32 	%f120, %f118, %f119;
	mov.b32 	%r134, %f120;
	shfl.sync.bfly.b32 	%r136|%p23, %r134, %r101, %r88, %r90;
	mov.b32 	%f121, %r136;
	add.f32 	%f122, %f120, %f121;
	st.local.f32 	[%rd3+4], %f122;
	st.shared.f32 	[%r14], %f122;
	bar.sync 	0;
	@%p1 bra 	$L__BB98_15;

	ld.shared.f32 	%f123, [%r4];
	mov.b32 	%r137, %f123;
	mov.u32 	%r138, 31;
	mov.u32 	%r139, 16;
	mov.u32 	%r140, -1;
	shfl.sync.bfly.b32 	%r141|%p24, %r137, %r139, %r138, %r140;
	mov.b32 	%f124, %r141;
	add.f32 	%f125, %f123, %f124;
	mov.b32 	%r142, %f125;
	mov.u32 	%r143, 8;
	shfl.sync.bfly.b32 	%r144|%p25, %r142, %r143, %r138, %r140;
	mov.b32 	%f126, %r144;
	add.f32 	%f127, %f125, %f126;
	mov.b32 	%r145, %f127;
	mov.u32 	%r146, 4;
	shfl.sync.bfly.b32 	%r147|%p26, %r145, %r146, %r138, %r140;
	mov.b32 	%f128, %r147;
	add.f32 	%f129, %f127, %f128;
	mov.b32 	%r148, %f129;
	mov.u32 	%r149, 2;
	shfl.sync.bfly.b32 	%r150|%p27, %r148, %r149, %r138, %r140;
	mov.b32 	%f130, %r150;
	add.f32 	%f131, %f129, %f130;
	mov.b32 	%r151, %f131;
	mov.u32 	%r152, 1;
	shfl.sync.bfly.b32 	%r153|%p28, %r151, %r152, %r138, %r140;
	mov.b32 	%f132, %r153;
	add.f32 	%f133, %f131, %f132;
	st.local.f32 	[%rd3+4], %f133;

$L__BB98_15:
	bar.sync 	0;
	mov.b32 	%r154, %f165;
	mov.u32 	%r155, 31;
	mov.u32 	%r156, 16;
	mov.u32 	%r157, -1;
	shfl.sync.bfly.b32 	%r158|%p30, %r154, %r156, %r155, %r157;
	mov.b32 	%f134, %r158;
	add.f32 	%f135, %f165, %f134;
	mov.b32 	%r159, %f135;
	mov.u32 	%r160, 8;
	shfl.sync.bfly.b32 	%r161|%p31, %r159, %r160, %r155, %r157;
	mov.b32 	%f136, %r161;
	add.f32 	%f137, %f135, %f136;
	mov.b32 	%r162, %f137;
	mov.u32 	%r163, 4;
	shfl.sync.bfly.b32 	%r164|%p32, %r162, %r163, %r155, %r157;
	mov.b32 	%f138, %r164;
	add.f32 	%f139, %f137, %f138;
	mov.b32 	%r165, %f139;
	mov.u32 	%r166, 2;
	shfl.sync.bfly.b32 	%r167|%p33, %r165, %r166, %r155, %r157;
	mov.b32 	%f140, %r167;
	add.f32 	%f141, %f139, %f140;
	mov.b32 	%r168, %f141;
	mov.u32 	%r169, 1;
	shfl.sync.bfly.b32 	%r170|%p34, %r168, %r169, %r155, %r157;
	mov.b32 	%f142, %r170;
	add.f32 	%f143, %f141, %f142;
	st.local.f32 	[%rd3+8], %f143;
	st.shared.f32 	[%r14], %f143;
	bar.sync 	0;
	@%p1 bra 	$L__BB98_17;

	ld.shared.f32 	%f144, [%r4];
	mov.b32 	%r171, %f144;
	shfl.sync.bfly.b32 	%r175|%p35, %r171, %r156, %r155, %r157;
	mov.b32 	%f145, %r175;
	add.f32 	%f146, %f144, %f145;
	mov.b32 	%r176, %f146;
	shfl.sync.bfly.b32 	%r178|%p36, %r176, %r160, %r155, %r157;
	mov.b32 	%f147, %r178;
	add.f32 	%f148, %f146, %f147;
	mov.b32 	%r179, %f148;
	shfl.sync.bfly.b32 	%r181|%p37, %r179, %r163, %r155, %r157;
	mov.b32 	%f149, %r181;
	add.f32 	%f150, %f148, %f149;
	mov.b32 	%r182, %f150;
	shfl.sync.bfly.b32 	%r184|%p38, %r182, %r166, %r155, %r157;
	mov.b32 	%f151, %r184;
	add.f32 	%f152, %f150, %f151;
	mov.b32 	%r185, %f152;
	shfl.sync.bfly.b32 	%r187|%p39, %r185, %r169, %r155, %r157;
	mov.b32 	%f153, %r187;
	add.f32 	%f154, %f152, %f153;
	st.local.f32 	[%rd3+8], %f154;

$L__BB98_17:
	bar.sync 	0;
	setp.gt.s32 	%p40, %r3, 2;
	@%p40 bra 	$L__BB98_19;

	mad.lo.s32 	%r188, %r3, %r17, %r2;
	cvt.s64.s32 	%rd60, %r188;
	mul.lo.s32 	%r189, %r1, %r18;
	cvt.s64.s32 	%rd61, %r189;
	add.s64 	%rd62, %rd61, %rd60;
	mul.wide.s32 	%rd63, %r3, 4;
	add.s64 	%rd64, %rd3, %rd63;
	ld.local.f32 	%f155, [%rd64];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f155;}

	// end inline asm
	cvta.to.global.u64 	%rd65, %rd28;
	shl.b64 	%rd66, %rd62, 1;
	add.s64 	%rd67, %rd65, %rd66;
	st.global.u16 	[%rd67], %rs1;

$L__BB98_19:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_4_bs_160
.visible .entry ggml_matvec_f16_ncols_4_bs_160(
	.param .u64 ggml_matvec_f16_ncols_4_bs_160_param_0,
	.param .u64 ggml_matvec_f16_ncols_4_bs_160_param_1,
	.param .u64 ggml_matvec_f16_ncols_4_bs_160_param_2,
	.param .u32 ggml_matvec_f16_ncols_4_bs_160_param_3,
	.param .u32 ggml_matvec_f16_ncols_4_bs_160_param_4,
	.param .u32 ggml_matvec_f16_ncols_4_bs_160_param_5,
	.param .u32 ggml_matvec_f16_ncols_4_bs_160_param_6,
	.param .u32 ggml_matvec_f16_ncols_4_bs_160_param_7,
	.param .u32 ggml_matvec_f16_ncols_4_bs_160_param_8,
	.param .u32 ggml_matvec_f16_ncols_4_bs_160_param_9,
	.param .u32 ggml_matvec_f16_ncols_4_bs_160_param_10,
	.param .u32 ggml_matvec_f16_ncols_4_bs_160_param_11
)
{
	.local .align 16 .b8 	__local_depot99[16];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<53>;
	.reg .b16 	%rs<2>;
	.reg .f32 	%f<172>;
	.reg .b32 	%r<211>;
	.reg .b64 	%rd<64>;


	mov.u64 	%SPL, __local_depot99;
	ld.param.u64 	%rd19, [ggml_matvec_f16_ncols_4_bs_160_param_0];
	ld.param.u64 	%rd20, [ggml_matvec_f16_ncols_4_bs_160_param_1];
	ld.param.u64 	%rd18, [ggml_matvec_f16_ncols_4_bs_160_param_2];
	ld.param.u32 	%r11, [ggml_matvec_f16_ncols_4_bs_160_param_3];
	ld.param.u32 	%r15, [ggml_matvec_f16_ncols_4_bs_160_param_5];
	ld.param.u32 	%r12, [ggml_matvec_f16_ncols_4_bs_160_param_6];
	ld.param.u32 	%r13, [ggml_matvec_f16_ncols_4_bs_160_param_7];
	ld.param.u32 	%r16, [ggml_matvec_f16_ncols_4_bs_160_param_8];
	ld.param.u32 	%r17, [ggml_matvec_f16_ncols_4_bs_160_param_9];
	ld.param.u32 	%r18, [ggml_matvec_f16_ncols_4_bs_160_param_10];
	ld.param.u32 	%r14, [ggml_matvec_f16_ncols_4_bs_160_param_11];
	cvta.to.global.u64 	%rd63, %rd20;
	add.u64 	%rd2, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r19, %r1, %r16;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r20, %r2, %r15;
	mad.lo.s32 	%r21, %r19, %r17, %r20;
	cvt.s64.s32 	%rd3, %r21;
	cvta.to.global.u64 	%rd4, %rd19;
	mul.lo.s32 	%r22, %r1, %r18;
	cvt.s64.s32 	%rd5, %r22;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r23, %r3, 2;
	mov.u32 	%r24, data_mmv;
	add.s32 	%r4, %r24, %r23;
	@%p1 bra 	$L__BB99_2;

	mov.u32 	%r25, 0;
	st.shared.u32 	[%r4], %r25;

$L__BB99_2:
	bar.sync 	0;
	mov.f32 	%f168, 0f00000000;
	st.local.v4.f32 	[%rd2], {%f168, %f168, %f168, %f168};
	setp.ge.s32 	%p2, %r3, %r11;
	mov.f32 	%f169, %f168;
	mov.f32 	%f170, %f168;
	mov.f32 	%f171, %f168;
	@%p2 bra 	$L__BB99_9;

	not.b32 	%r26, %r3;
	add.s32 	%r5, %r26, %r11;
	mul.wide.u32 	%rd22, %r5, -858993459;
	shr.u64 	%rd23, %rd22, 39;
	and.b64  	%rd24, %rd23, 1;
	setp.eq.b64 	%p3, %rd24, 1;
	mov.pred 	%p4, 0;
	xor.pred  	%p5, %p3, %p4;
	mov.f32 	%f168, 0f00000000;
	mov.u32 	%r210, %r3;
	@%p5 bra 	$L__BB99_5;

	shl.b64 	%rd25, %rd5, 1;
	add.s64 	%rd26, %rd63, %rd25;
	shl.b64 	%rd27, %rd3, 1;
	add.s64 	%rd28, %rd4, %rd27;
	mul.wide.s32 	%rd29, %r3, 4;
	add.s64 	%rd30, %rd28, %rd29;
	ld.global.nc.u32 	%r27, [%rd30];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r27;
  cvt.f32.f16 %f29, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r27;
  cvt.f32.f16 %f30, high;}

	// end inline asm
	add.s64 	%rd31, %rd26, %rd29;
	ld.global.nc.u32 	%r29, [%rd31];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r29;
  cvt.f32.f16 %f31, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r29;
  cvt.f32.f16 %f32, high;}

	// end inline asm
	fma.rn.f32 	%f39, %f29, %f31, 0f00000000;
	fma.rn.f32 	%f171, %f30, %f32, %f39;
	st.local.f32 	[%rd2], %f171;
	mul.wide.s32 	%rd32, %r12, 4;
	add.s64 	%rd33, %rd31, %rd32;
	ld.global.nc.u32 	%r31, [%rd33];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r31;
  cvt.f32.f16 %f33, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r31;
  cvt.f32.f16 %f34, high;}

	// end inline asm
	fma.rn.f32 	%f40, %f29, %f33, 0f00000000;
	fma.rn.f32 	%f170, %f30, %f34, %f40;
	st.local.f32 	[%rd2+4], %f170;
	add.s32 	%r37, %r3, %r12;
	add.s32 	%r38, %r37, %r12;
	mul.wide.s32 	%rd34, %r38, 4;
	add.s64 	%rd35, %rd26, %rd34;
	ld.global.nc.u32 	%r33, [%rd35];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r33;
  cvt.f32.f16 %f35, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r33;
  cvt.f32.f16 %f36, high;}

	// end inline asm
	fma.rn.f32 	%f41, %f29, %f35, 0f00000000;
	fma.rn.f32 	%f169, %f30, %f36, %f41;
	st.local.f32 	[%rd2+8], %f169;
	add.s32 	%r39, %r38, %r12;
	mul.wide.s32 	%rd36, %r39, 4;
	add.s64 	%rd37, %rd26, %rd36;
	ld.global.nc.u32 	%r35, [%rd37];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r35;
  cvt.f32.f16 %f37, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r35;
  cvt.f32.f16 %f38, high;}

	// end inline asm
	fma.rn.f32 	%f42, %f29, %f37, 0f00000000;
	fma.rn.f32 	%f168, %f30, %f38, %f42;
	st.local.f32 	[%rd2+12], %f168;
	add.s32 	%r210, %r3, 160;

$L__BB99_5:
	setp.lt.u32 	%p6, %r5, 160;
	@%p6 bra 	$L__BB99_9;

	add.s32 	%r40, %r210, %r12;
	add.s32 	%r41, %r40, 160;
	mul.wide.s32 	%rd38, %r41, 4;
	shl.b64 	%rd39, %rd5, 1;
	add.s64 	%rd8, %rd38, %rd39;
	shl.b32 	%r42, %r12, 1;
	add.s32 	%r43, %r210, %r42;
	mad.lo.s32 	%r44, %r12, 3, %r210;
	mul.wide.s32 	%rd40, %r43, 4;
	add.s64 	%rd9, %rd40, %rd39;
	mul.wide.s32 	%rd41, %r44, 4;
	add.s64 	%rd10, %rd41, %rd39;
	mul.wide.s32 	%rd42, %r210, 2;
	add.s64 	%rd43, %rd42, %rd3;
	shl.b64 	%rd44, %rd43, 1;
	add.s64 	%rd45, %rd4, %rd44;
	add.s64 	%rd62, %rd45, 640;
	mul.wide.s32 	%rd46, %r210, 4;
	mul.wide.s32 	%rd47, %r12, 4;
	add.s64 	%rd48, %rd46, %rd47;
	add.s64 	%rd12, %rd48, %rd39;
	add.s64 	%rd13, %rd46, %rd39;

$L__BB99_7:
	ld.global.nc.u32 	%r45, [%rd62+-640];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r45;
  cvt.f32.f16 %f43, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r45;
  cvt.f32.f16 %f44, high;}

	// end inline asm
	add.s64 	%rd49, %rd63, %rd13;
	ld.global.nc.u32 	%r47, [%rd49];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r47;
  cvt.f32.f16 %f45, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r47;
  cvt.f32.f16 %f46, high;}

	// end inline asm
	fma.rn.f32 	%f63, %f43, %f45, %f171;
	fma.rn.f32 	%f64, %f44, %f46, %f63;
	add.s64 	%rd50, %rd63, %rd12;
	ld.global.nc.u32 	%r49, [%rd50];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r49;
  cvt.f32.f16 %f47, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r49;
  cvt.f32.f16 %f48, high;}

	// end inline asm
	fma.rn.f32 	%f65, %f43, %f47, %f170;
	fma.rn.f32 	%f66, %f44, %f48, %f65;
	add.s64 	%rd51, %rd63, %rd9;
	ld.global.nc.u32 	%r51, [%rd51];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r51;
  cvt.f32.f16 %f49, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r51;
  cvt.f32.f16 %f50, high;}

	// end inline asm
	fma.rn.f32 	%f67, %f43, %f49, %f169;
	fma.rn.f32 	%f68, %f44, %f50, %f67;
	add.s64 	%rd52, %rd63, %rd10;
	ld.global.nc.u32 	%r53, [%rd52];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r53;
  cvt.f32.f16 %f51, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r53;
  cvt.f32.f16 %f52, high;}

	// end inline asm
	fma.rn.f32 	%f69, %f43, %f51, %f168;
	fma.rn.f32 	%f70, %f44, %f52, %f69;
	ld.global.nc.u32 	%r55, [%rd62];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r55;
  cvt.f32.f16 %f53, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r55;
  cvt.f32.f16 %f54, high;}

	// end inline asm
	ld.global.nc.u32 	%r57, [%rd49+640];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r57;
  cvt.f32.f16 %f55, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r57;
  cvt.f32.f16 %f56, high;}

	// end inline asm
	fma.rn.f32 	%f71, %f53, %f55, %f64;
	fma.rn.f32 	%f171, %f54, %f56, %f71;
	add.s64 	%rd53, %rd63, %rd8;
	ld.global.nc.u32 	%r59, [%rd53];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r59;
  cvt.f32.f16 %f57, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r59;
  cvt.f32.f16 %f58, high;}

	// end inline asm
	fma.rn.f32 	%f72, %f53, %f57, %f66;
	fma.rn.f32 	%f170, %f54, %f58, %f72;
	ld.global.nc.u32 	%r61, [%rd51+640];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r61;
  cvt.f32.f16 %f59, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r61;
  cvt.f32.f16 %f60, high;}

	// end inline asm
	fma.rn.f32 	%f73, %f53, %f59, %f68;
	fma.rn.f32 	%f169, %f54, %f60, %f73;
	ld.global.nc.u32 	%r63, [%rd52+640];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r63;
  cvt.f32.f16 %f61, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r63;
  cvt.f32.f16 %f62, high;}

	// end inline asm
	fma.rn.f32 	%f74, %f53, %f61, %f70;
	fma.rn.f32 	%f168, %f54, %f62, %f74;
	add.s64 	%rd63, %rd63, 1280;
	add.s64 	%rd62, %rd62, 1280;
	add.s32 	%r210, %r210, 320;
	setp.lt.s32 	%p7, %r210, %r11;
	@%p7 bra 	$L__BB99_7;

	st.local.v4.f32 	[%rd2], {%f171, %f170, %f169, %f168};

$L__BB99_9:
	shr.s32 	%r65, %r3, 31;
	shr.u32 	%r66, %r65, 27;
	add.s32 	%r67, %r3, %r66;
	shr.s32 	%r68, %r67, 5;
	shl.b32 	%r69, %r68, 2;
	add.s32 	%r10, %r24, %r69;
	mov.u32 	%r71, 2;
	mov.b32 	%r72, %f171;
	mov.u32 	%r73, 31;
	mov.u32 	%r74, 16;
	mov.u32 	%r75, -1;
	shfl.sync.bfly.b32 	%r76|%p8, %r72, %r74, %r73, %r75;
	mov.b32 	%f75, %r76;
	add.f32 	%f76, %f171, %f75;
	mov.b32 	%r77, %f76;
	mov.u32 	%r78, 8;
	shfl.sync.bfly.b32 	%r79|%p9, %r77, %r78, %r73, %r75;
	mov.b32 	%f77, %r79;
	add.f32 	%f78, %f76, %f77;
	mov.b32 	%r80, %f78;
	mov.u32 	%r81, 4;
	shfl.sync.bfly.b32 	%r82|%p10, %r80, %r81, %r73, %r75;
	mov.b32 	%f79, %r82;
	add.f32 	%f80, %f78, %f79;
	mov.b32 	%r83, %f80;
	shfl.sync.bfly.b32 	%r84|%p11, %r83, %r71, %r73, %r75;
	mov.b32 	%f81, %r84;
	add.f32 	%f82, %f80, %f81;
	mov.b32 	%r85, %f82;
	mov.u32 	%r86, 1;
	shfl.sync.bfly.b32 	%r87|%p12, %r85, %r86, %r73, %r75;
	mov.b32 	%f83, %r87;
	add.f32 	%f84, %f82, %f83;
	st.local.f32 	[%rd2], %f84;
	st.shared.f32 	[%r10], %f84;
	bar.sync 	0;
	@%p1 bra 	$L__BB99_11;

	ld.shared.f32 	%f85, [%r4];
	mov.b32 	%r88, %f85;
	shfl.sync.bfly.b32 	%r92|%p14, %r88, %r74, %r73, %r75;
	mov.b32 	%f86, %r92;
	add.f32 	%f87, %f85, %f86;
	mov.b32 	%r93, %f87;
	shfl.sync.bfly.b32 	%r95|%p15, %r93, %r78, %r73, %r75;
	mov.b32 	%f88, %r95;
	add.f32 	%f89, %f87, %f88;
	mov.b32 	%r96, %f89;
	shfl.sync.bfly.b32 	%r98|%p16, %r96, %r81, %r73, %r75;
	mov.b32 	%f90, %r98;
	add.f32 	%f91, %f89, %f90;
	mov.b32 	%r99, %f91;
	shfl.sync.bfly.b32 	%r101|%p17, %r99, %r71, %r73, %r75;
	mov.b32 	%f92, %r101;
	add.f32 	%f93, %f91, %f92;
	mov.b32 	%r102, %f93;
	shfl.sync.bfly.b32 	%r104|%p18, %r102, %r86, %r73, %r75;
	mov.b32 	%f94, %r104;
	add.f32 	%f95, %f93, %f94;
	st.local.f32 	[%rd2], %f95;

$L__BB99_11:
	bar.sync 	0;
	mov.b32 	%r105, %f170;
	shfl.sync.bfly.b32 	%r109|%p20, %r105, %r74, %r73, %r75;
	mov.b32 	%f96, %r109;
	add.f32 	%f97, %f170, %f96;
	mov.b32 	%r110, %f97;
	shfl.sync.bfly.b32 	%r112|%p21, %r110, %r78, %r73, %r75;
	mov.b32 	%f98, %r112;
	add.f32 	%f99, %f97, %f98;
	mov.b32 	%r113, %f99;
	shfl.sync.bfly.b32 	%r115|%p22, %r113, %r81, %r73, %r75;
	mov.b32 	%f100, %r115;
	add.f32 	%f101, %f99, %f100;
	mov.b32 	%r116, %f101;
	shfl.sync.bfly.b32 	%r118|%p23, %r116, %r71, %r73, %r75;
	mov.b32 	%f102, %r118;
	add.f32 	%f103, %f101, %f102;
	mov.b32 	%r119, %f103;
	shfl.sync.bfly.b32 	%r121|%p24, %r119, %r86, %r73, %r75;
	mov.b32 	%f104, %r121;
	add.f32 	%f105, %f103, %f104;
	st.local.f32 	[%rd2+4], %f105;
	st.shared.f32 	[%r10], %f105;
	bar.sync 	0;
	@%p1 bra 	$L__BB99_13;

	ld.shared.f32 	%f106, [%r4];
	mov.b32 	%r122, %f106;
	mov.u32 	%r123, 31;
	mov.u32 	%r124, 16;
	mov.u32 	%r125, -1;
	shfl.sync.bfly.b32 	%r126|%p25, %r122, %r124, %r123, %r125;
	mov.b32 	%f107, %r126;
	add.f32 	%f108, %f106, %f107;
	mov.b32 	%r127, %f108;
	mov.u32 	%r128, 8;
	shfl.sync.bfly.b32 	%r129|%p26, %r127, %r128, %r123, %r125;
	mov.b32 	%f109, %r129;
	add.f32 	%f110, %f108, %f109;
	mov.b32 	%r130, %f110;
	mov.u32 	%r131, 4;
	shfl.sync.bfly.b32 	%r132|%p27, %r130, %r131, %r123, %r125;
	mov.b32 	%f111, %r132;
	add.f32 	%f112, %f110, %f111;
	mov.b32 	%r133, %f112;
	mov.u32 	%r134, 2;
	shfl.sync.bfly.b32 	%r135|%p28, %r133, %r134, %r123, %r125;
	mov.b32 	%f113, %r135;
	add.f32 	%f114, %f112, %f113;
	mov.b32 	%r136, %f114;
	mov.u32 	%r137, 1;
	shfl.sync.bfly.b32 	%r138|%p29, %r136, %r137, %r123, %r125;
	mov.b32 	%f115, %r138;
	add.f32 	%f116, %f114, %f115;
	st.local.f32 	[%rd2+4], %f116;

$L__BB99_13:
	bar.sync 	0;
	mov.b32 	%r139, %f169;
	mov.u32 	%r140, 31;
	mov.u32 	%r141, 16;
	mov.u32 	%r142, -1;
	shfl.sync.bfly.b32 	%r143|%p31, %r139, %r141, %r140, %r142;
	mov.b32 	%f117, %r143;
	add.f32 	%f118, %f169, %f117;
	mov.b32 	%r144, %f118;
	mov.u32 	%r145, 8;
	shfl.sync.bfly.b32 	%r146|%p32, %r144, %r145, %r140, %r142;
	mov.b32 	%f119, %r146;
	add.f32 	%f120, %f118, %f119;
	mov.b32 	%r147, %f120;
	mov.u32 	%r148, 4;
	shfl.sync.bfly.b32 	%r149|%p33, %r147, %r148, %r140, %r142;
	mov.b32 	%f121, %r149;
	add.f32 	%f122, %f120, %f121;
	mov.b32 	%r150, %f122;
	mov.u32 	%r151, 2;
	shfl.sync.bfly.b32 	%r152|%p34, %r150, %r151, %r140, %r142;
	mov.b32 	%f123, %r152;
	add.f32 	%f124, %f122, %f123;
	mov.b32 	%r153, %f124;
	mov.u32 	%r154, 1;
	shfl.sync.bfly.b32 	%r155|%p35, %r153, %r154, %r140, %r142;
	mov.b32 	%f125, %r155;
	add.f32 	%f126, %f124, %f125;
	st.local.f32 	[%rd2+8], %f126;
	st.shared.f32 	[%r10], %f126;
	bar.sync 	0;
	@%p1 bra 	$L__BB99_15;

	ld.shared.f32 	%f127, [%r4];
	mov.b32 	%r156, %f127;
	shfl.sync.bfly.b32 	%r160|%p36, %r156, %r141, %r140, %r142;
	mov.b32 	%f128, %r160;
	add.f32 	%f129, %f127, %f128;
	mov.b32 	%r161, %f129;
	shfl.sync.bfly.b32 	%r163|%p37, %r161, %r145, %r140, %r142;
	mov.b32 	%f130, %r163;
	add.f32 	%f131, %f129, %f130;
	mov.b32 	%r164, %f131;
	shfl.sync.bfly.b32 	%r166|%p38, %r164, %r148, %r140, %r142;
	mov.b32 	%f132, %r166;
	add.f32 	%f133, %f131, %f132;
	mov.b32 	%r167, %f133;
	shfl.sync.bfly.b32 	%r169|%p39, %r167, %r151, %r140, %r142;
	mov.b32 	%f134, %r169;
	add.f32 	%f135, %f133, %f134;
	mov.b32 	%r170, %f135;
	shfl.sync.bfly.b32 	%r172|%p40, %r170, %r154, %r140, %r142;
	mov.b32 	%f136, %r172;
	add.f32 	%f137, %f135, %f136;
	st.local.f32 	[%rd2+8], %f137;

$L__BB99_15:
	bar.sync 	0;
	mov.b32 	%r173, %f168;
	shfl.sync.bfly.b32 	%r177|%p42, %r173, %r141, %r140, %r142;
	mov.b32 	%f138, %r177;
	add.f32 	%f139, %f168, %f138;
	mov.b32 	%r178, %f139;
	shfl.sync.bfly.b32 	%r180|%p43, %r178, %r145, %r140, %r142;
	mov.b32 	%f140, %r180;
	add.f32 	%f141, %f139, %f140;
	mov.b32 	%r181, %f141;
	shfl.sync.bfly.b32 	%r183|%p44, %r181, %r148, %r140, %r142;
	mov.b32 	%f142, %r183;
	add.f32 	%f143, %f141, %f142;
	mov.b32 	%r184, %f143;
	shfl.sync.bfly.b32 	%r186|%p45, %r184, %r151, %r140, %r142;
	mov.b32 	%f144, %r186;
	add.f32 	%f145, %f143, %f144;
	mov.b32 	%r187, %f145;
	shfl.sync.bfly.b32 	%r189|%p46, %r187, %r154, %r140, %r142;
	mov.b32 	%f146, %r189;
	add.f32 	%f147, %f145, %f146;
	st.local.f32 	[%rd2+12], %f147;
	st.shared.f32 	[%r10], %f147;
	bar.sync 	0;
	@%p1 bra 	$L__BB99_17;

	ld.shared.f32 	%f148, [%r4];
	mov.b32 	%r190, %f148;
	mov.u32 	%r191, 31;
	mov.u32 	%r192, 16;
	mov.u32 	%r193, -1;
	shfl.sync.bfly.b32 	%r194|%p47, %r190, %r192, %r191, %r193;
	mov.b32 	%f149, %r194;
	add.f32 	%f150, %f148, %f149;
	mov.b32 	%r195, %f150;
	mov.u32 	%r196, 8;
	shfl.sync.bfly.b32 	%r197|%p48, %r195, %r196, %r191, %r193;
	mov.b32 	%f151, %r197;
	add.f32 	%f152, %f150, %f151;
	mov.b32 	%r198, %f152;
	mov.u32 	%r199, 4;
	shfl.sync.bfly.b32 	%r200|%p49, %r198, %r199, %r191, %r193;
	mov.b32 	%f153, %r200;
	add.f32 	%f154, %f152, %f153;
	mov.b32 	%r201, %f154;
	mov.u32 	%r202, 2;
	shfl.sync.bfly.b32 	%r203|%p50, %r201, %r202, %r191, %r193;
	mov.b32 	%f155, %r203;
	add.f32 	%f156, %f154, %f155;
	mov.b32 	%r204, %f156;
	mov.u32 	%r205, 1;
	shfl.sync.bfly.b32 	%r206|%p51, %r204, %r205, %r191, %r193;
	mov.b32 	%f157, %r206;
	add.f32 	%f158, %f156, %f157;
	st.local.f32 	[%rd2+12], %f158;

$L__BB99_17:
	bar.sync 	0;
	setp.gt.s32 	%p52, %r3, 3;
	@%p52 bra 	$L__BB99_19;

	mad.lo.s32 	%r207, %r3, %r13, %r2;
	cvt.s64.s32 	%rd54, %r207;
	mul.lo.s32 	%r208, %r1, %r14;
	cvt.s64.s32 	%rd55, %r208;
	add.s64 	%rd56, %rd55, %rd54;
	mul.wide.s32 	%rd57, %r3, 4;
	add.s64 	%rd58, %rd2, %rd57;
	ld.local.f32 	%f159, [%rd58];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f159;}

	// end inline asm
	cvta.to.global.u64 	%rd59, %rd18;
	shl.b64 	%rd60, %rd56, 1;
	add.s64 	%rd61, %rd59, %rd60;
	st.global.u16 	[%rd61], %rs1;

$L__BB99_19:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_5_bs_160
.visible .entry ggml_matvec_f16_ncols_5_bs_160(
	.param .u64 ggml_matvec_f16_ncols_5_bs_160_param_0,
	.param .u64 ggml_matvec_f16_ncols_5_bs_160_param_1,
	.param .u64 ggml_matvec_f16_ncols_5_bs_160_param_2,
	.param .u32 ggml_matvec_f16_ncols_5_bs_160_param_3,
	.param .u32 ggml_matvec_f16_ncols_5_bs_160_param_4,
	.param .u32 ggml_matvec_f16_ncols_5_bs_160_param_5,
	.param .u32 ggml_matvec_f16_ncols_5_bs_160_param_6,
	.param .u32 ggml_matvec_f16_ncols_5_bs_160_param_7,
	.param .u32 ggml_matvec_f16_ncols_5_bs_160_param_8,
	.param .u32 ggml_matvec_f16_ncols_5_bs_160_param_9,
	.param .u32 ggml_matvec_f16_ncols_5_bs_160_param_10,
	.param .u32 ggml_matvec_f16_ncols_5_bs_160_param_11
)
{
	.local .align 4 .b8 	__local_depot100[20];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<64>;
	.reg .b16 	%rs<2>;
	.reg .f32 	%f<213>;
	.reg .b32 	%r<255>;
	.reg .b64 	%rd<67>;


	mov.u64 	%SPL, __local_depot100;
	ld.param.u64 	%rd19, [ggml_matvec_f16_ncols_5_bs_160_param_0];
	ld.param.u64 	%rd20, [ggml_matvec_f16_ncols_5_bs_160_param_1];
	ld.param.u64 	%rd18, [ggml_matvec_f16_ncols_5_bs_160_param_2];
	ld.param.u32 	%r11, [ggml_matvec_f16_ncols_5_bs_160_param_3];
	ld.param.u32 	%r15, [ggml_matvec_f16_ncols_5_bs_160_param_5];
	ld.param.u32 	%r12, [ggml_matvec_f16_ncols_5_bs_160_param_6];
	ld.param.u32 	%r13, [ggml_matvec_f16_ncols_5_bs_160_param_7];
	ld.param.u32 	%r16, [ggml_matvec_f16_ncols_5_bs_160_param_8];
	ld.param.u32 	%r17, [ggml_matvec_f16_ncols_5_bs_160_param_9];
	ld.param.u32 	%r18, [ggml_matvec_f16_ncols_5_bs_160_param_10];
	ld.param.u32 	%r14, [ggml_matvec_f16_ncols_5_bs_160_param_11];
	cvta.to.global.u64 	%rd66, %rd20;
	add.u64 	%rd2, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r19, %r1, %r16;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r20, %r2, %r15;
	mad.lo.s32 	%r21, %r19, %r17, %r20;
	cvt.s64.s32 	%rd3, %r21;
	cvta.to.global.u64 	%rd4, %rd19;
	mul.lo.s32 	%r22, %r1, %r18;
	cvt.s64.s32 	%rd5, %r22;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r23, %r3, 2;
	mov.u32 	%r24, data_mmv;
	add.s32 	%r4, %r24, %r23;
	@%p1 bra 	$L__BB100_2;

	mov.u32 	%r25, 0;
	st.shared.u32 	[%r4], %r25;

$L__BB100_2:
	bar.sync 	0;
	mov.f32 	%f208, 0f00000000;
	mov.u32 	%r26, 0;
	st.local.u32 	[%rd2], %r26;
	st.local.u32 	[%rd2+4], %r26;
	st.local.u32 	[%rd2+8], %r26;
	st.local.u32 	[%rd2+12], %r26;
	st.local.u32 	[%rd2+16], %r26;
	setp.ge.s32 	%p2, %r3, %r11;
	mov.f32 	%f209, %f208;
	mov.f32 	%f210, %f208;
	mov.f32 	%f211, %f208;
	mov.f32 	%f212, %f208;
	@%p2 bra 	$L__BB100_9;

	not.b32 	%r27, %r3;
	add.s32 	%r5, %r27, %r11;
	mul.wide.u32 	%rd22, %r5, -858993459;
	shr.u64 	%rd23, %rd22, 39;
	and.b64  	%rd24, %rd23, 1;
	setp.eq.b64 	%p3, %rd24, 1;
	mov.pred 	%p4, 0;
	xor.pred  	%p5, %p3, %p4;
	mov.f32 	%f208, 0f00000000;
	mov.u32 	%r254, %r3;
	@%p5 bra 	$L__BB100_5;

	shl.b64 	%rd25, %rd5, 1;
	add.s64 	%rd26, %rd66, %rd25;
	shl.b64 	%rd27, %rd3, 1;
	add.s64 	%rd28, %rd4, %rd27;
	mul.wide.s32 	%rd29, %r3, 4;
	add.s64 	%rd30, %rd28, %rd29;
	ld.global.nc.u32 	%r28, [%rd30];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r28;
  cvt.f32.f16 %f36, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r28;
  cvt.f32.f16 %f37, high;}

	// end inline asm
	add.s64 	%rd31, %rd26, %rd29;
	ld.global.nc.u32 	%r30, [%rd31];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r30;
  cvt.f32.f16 %f38, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r30;
  cvt.f32.f16 %f39, high;}

	// end inline asm
	fma.rn.f32 	%f48, %f36, %f38, 0f00000000;
	fma.rn.f32 	%f212, %f37, %f39, %f48;
	st.local.f32 	[%rd2], %f212;
	mul.wide.s32 	%rd32, %r12, 4;
	add.s64 	%rd33, %rd31, %rd32;
	ld.global.nc.u32 	%r32, [%rd33];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r32;
  cvt.f32.f16 %f40, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r32;
  cvt.f32.f16 %f41, high;}

	// end inline asm
	fma.rn.f32 	%f49, %f36, %f40, 0f00000000;
	fma.rn.f32 	%f211, %f37, %f41, %f49;
	st.local.f32 	[%rd2+4], %f211;
	add.s32 	%r40, %r3, %r12;
	add.s32 	%r41, %r40, %r12;
	shl.b32 	%r42, %r12, 1;
	mul.wide.s32 	%rd34, %r42, 4;
	add.s64 	%rd35, %rd31, %rd34;
	ld.global.nc.u32 	%r34, [%rd35];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r34;
  cvt.f32.f16 %f42, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r34;
  cvt.f32.f16 %f43, high;}

	// end inline asm
	fma.rn.f32 	%f50, %f36, %f42, 0f00000000;
	fma.rn.f32 	%f210, %f37, %f43, %f50;
	st.local.f32 	[%rd2+8], %f210;
	add.s32 	%r43, %r41, %r12;
	mul.wide.s32 	%rd36, %r43, 4;
	add.s64 	%rd37, %rd26, %rd36;
	ld.global.nc.u32 	%r36, [%rd37];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r36;
  cvt.f32.f16 %f44, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r36;
  cvt.f32.f16 %f45, high;}

	// end inline asm
	fma.rn.f32 	%f51, %f36, %f44, 0f00000000;
	fma.rn.f32 	%f209, %f37, %f45, %f51;
	st.local.f32 	[%rd2+12], %f209;
	add.s64 	%rd38, %rd35, %rd34;
	ld.global.nc.u32 	%r38, [%rd38];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r38;
  cvt.f32.f16 %f46, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r38;
  cvt.f32.f16 %f47, high;}

	// end inline asm
	fma.rn.f32 	%f52, %f36, %f46, 0f00000000;
	fma.rn.f32 	%f208, %f37, %f47, %f52;
	st.local.f32 	[%rd2+16], %f208;
	add.s32 	%r254, %r3, 160;

$L__BB100_5:
	setp.lt.u32 	%p6, %r5, 160;
	@%p6 bra 	$L__BB100_9;

	add.s32 	%r44, %r254, %r12;
	add.s32 	%r45, %r44, 160;
	mul.wide.s32 	%rd39, %r45, 4;
	shl.b64 	%rd40, %rd5, 1;
	add.s64 	%rd7, %rd39, %rd40;
	shl.b32 	%r46, %r12, 1;
	add.s32 	%r47, %r254, %r46;
	mad.lo.s32 	%r48, %r12, 3, %r254;
	shl.b32 	%r49, %r12, 2;
	add.s32 	%r50, %r254, %r49;
	mul.wide.s32 	%rd41, %r47, 4;
	add.s64 	%rd8, %rd41, %rd40;
	mul.wide.s32 	%rd42, %r48, 4;
	add.s64 	%rd9, %rd42, %rd40;
	mul.wide.s32 	%rd43, %r50, 4;
	add.s64 	%rd10, %rd43, %rd40;
	mul.wide.s32 	%rd44, %r254, 2;
	add.s64 	%rd45, %rd44, %rd3;
	shl.b64 	%rd46, %rd45, 1;
	add.s64 	%rd47, %rd4, %rd46;
	add.s64 	%rd65, %rd47, 640;
	mul.wide.s32 	%rd48, %r254, 4;
	mul.wide.s32 	%rd49, %r12, 4;
	add.s64 	%rd50, %rd48, %rd49;
	add.s64 	%rd12, %rd50, %rd40;
	add.s64 	%rd13, %rd48, %rd40;

$L__BB100_7:
	ld.global.nc.u32 	%r51, [%rd65+-640];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r51;
  cvt.f32.f16 %f53, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r51;
  cvt.f32.f16 %f54, high;}

	// end inline asm
	add.s64 	%rd51, %rd66, %rd13;
	ld.global.nc.u32 	%r53, [%rd51];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r53;
  cvt.f32.f16 %f55, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r53;
  cvt.f32.f16 %f56, high;}

	// end inline asm
	fma.rn.f32 	%f77, %f53, %f55, %f212;
	fma.rn.f32 	%f78, %f54, %f56, %f77;
	add.s64 	%rd52, %rd66, %rd12;
	ld.global.nc.u32 	%r55, [%rd52];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r55;
  cvt.f32.f16 %f57, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r55;
  cvt.f32.f16 %f58, high;}

	// end inline asm
	fma.rn.f32 	%f79, %f53, %f57, %f211;
	fma.rn.f32 	%f80, %f54, %f58, %f79;
	add.s64 	%rd53, %rd66, %rd8;
	ld.global.nc.u32 	%r57, [%rd53];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r57;
  cvt.f32.f16 %f59, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r57;
  cvt.f32.f16 %f60, high;}

	// end inline asm
	fma.rn.f32 	%f81, %f53, %f59, %f210;
	fma.rn.f32 	%f82, %f54, %f60, %f81;
	add.s64 	%rd54, %rd66, %rd9;
	ld.global.nc.u32 	%r59, [%rd54];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r59;
  cvt.f32.f16 %f61, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r59;
  cvt.f32.f16 %f62, high;}

	// end inline asm
	fma.rn.f32 	%f83, %f53, %f61, %f209;
	fma.rn.f32 	%f84, %f54, %f62, %f83;
	add.s64 	%rd55, %rd66, %rd10;
	ld.global.nc.u32 	%r61, [%rd55];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r61;
  cvt.f32.f16 %f63, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r61;
  cvt.f32.f16 %f64, high;}

	// end inline asm
	fma.rn.f32 	%f85, %f53, %f63, %f208;
	fma.rn.f32 	%f86, %f54, %f64, %f85;
	ld.global.nc.u32 	%r63, [%rd65];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r63;
  cvt.f32.f16 %f65, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r63;
  cvt.f32.f16 %f66, high;}

	// end inline asm
	ld.global.nc.u32 	%r65, [%rd51+640];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r65;
  cvt.f32.f16 %f67, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r65;
  cvt.f32.f16 %f68, high;}

	// end inline asm
	fma.rn.f32 	%f87, %f65, %f67, %f78;
	fma.rn.f32 	%f212, %f66, %f68, %f87;
	add.s64 	%rd56, %rd66, %rd7;
	ld.global.nc.u32 	%r67, [%rd56];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r67;
  cvt.f32.f16 %f69, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r67;
  cvt.f32.f16 %f70, high;}

	// end inline asm
	fma.rn.f32 	%f88, %f65, %f69, %f80;
	fma.rn.f32 	%f211, %f66, %f70, %f88;
	ld.global.nc.u32 	%r69, [%rd53+640];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r69;
  cvt.f32.f16 %f71, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r69;
  cvt.f32.f16 %f72, high;}

	// end inline asm
	fma.rn.f32 	%f89, %f65, %f71, %f82;
	fma.rn.f32 	%f210, %f66, %f72, %f89;
	ld.global.nc.u32 	%r71, [%rd54+640];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r71;
  cvt.f32.f16 %f73, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r71;
  cvt.f32.f16 %f74, high;}

	// end inline asm
	fma.rn.f32 	%f90, %f65, %f73, %f84;
	fma.rn.f32 	%f209, %f66, %f74, %f90;
	ld.global.nc.u32 	%r73, [%rd55+640];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r73;
  cvt.f32.f16 %f75, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r73;
  cvt.f32.f16 %f76, high;}

	// end inline asm
	fma.rn.f32 	%f91, %f65, %f75, %f86;
	fma.rn.f32 	%f208, %f66, %f76, %f91;
	add.s64 	%rd66, %rd66, 1280;
	add.s64 	%rd65, %rd65, 1280;
	add.s32 	%r254, %r254, 320;
	setp.lt.s32 	%p7, %r254, %r11;
	@%p7 bra 	$L__BB100_7;

	st.local.f32 	[%rd2], %f212;
	st.local.f32 	[%rd2+4], %f211;
	st.local.f32 	[%rd2+8], %f210;
	st.local.f32 	[%rd2+12], %f209;
	st.local.f32 	[%rd2+16], %f208;

$L__BB100_9:
	shr.s32 	%r75, %r3, 31;
	shr.u32 	%r76, %r75, 27;
	add.s32 	%r77, %r3, %r76;
	shr.s32 	%r78, %r77, 5;
	shl.b32 	%r79, %r78, 2;
	add.s32 	%r10, %r24, %r79;
	mov.u32 	%r81, 2;
	mov.b32 	%r82, %f212;
	mov.u32 	%r83, 31;
	mov.u32 	%r84, 16;
	mov.u32 	%r85, -1;
	shfl.sync.bfly.b32 	%r86|%p8, %r82, %r84, %r83, %r85;
	mov.b32 	%f92, %r86;
	add.f32 	%f93, %f212, %f92;
	mov.b32 	%r87, %f93;
	mov.u32 	%r88, 8;
	shfl.sync.bfly.b32 	%r89|%p9, %r87, %r88, %r83, %r85;
	mov.b32 	%f94, %r89;
	add.f32 	%f95, %f93, %f94;
	mov.b32 	%r90, %f95;
	mov.u32 	%r91, 4;
	shfl.sync.bfly.b32 	%r92|%p10, %r90, %r91, %r83, %r85;
	mov.b32 	%f96, %r92;
	add.f32 	%f97, %f95, %f96;
	mov.b32 	%r93, %f97;
	shfl.sync.bfly.b32 	%r94|%p11, %r93, %r81, %r83, %r85;
	mov.b32 	%f98, %r94;
	add.f32 	%f99, %f97, %f98;
	mov.b32 	%r95, %f99;
	mov.u32 	%r96, 1;
	shfl.sync.bfly.b32 	%r97|%p12, %r95, %r96, %r83, %r85;
	mov.b32 	%f100, %r97;
	add.f32 	%f101, %f99, %f100;
	st.local.f32 	[%rd2], %f101;
	st.shared.f32 	[%r10], %f101;
	bar.sync 	0;
	@%p1 bra 	$L__BB100_11;

	ld.shared.f32 	%f102, [%r4];
	mov.b32 	%r98, %f102;
	shfl.sync.bfly.b32 	%r102|%p14, %r98, %r84, %r83, %r85;
	mov.b32 	%f103, %r102;
	add.f32 	%f104, %f102, %f103;
	mov.b32 	%r103, %f104;
	shfl.sync.bfly.b32 	%r105|%p15, %r103, %r88, %r83, %r85;
	mov.b32 	%f105, %r105;
	add.f32 	%f106, %f104, %f105;
	mov.b32 	%r106, %f106;
	shfl.sync.bfly.b32 	%r108|%p16, %r106, %r91, %r83, %r85;
	mov.b32 	%f107, %r108;
	add.f32 	%f108, %f106, %f107;
	mov.b32 	%r109, %f108;
	shfl.sync.bfly.b32 	%r111|%p17, %r109, %r81, %r83, %r85;
	mov.b32 	%f109, %r111;
	add.f32 	%f110, %f108, %f109;
	mov.b32 	%r112, %f110;
	shfl.sync.bfly.b32 	%r114|%p18, %r112, %r96, %r83, %r85;
	mov.b32 	%f111, %r114;
	add.f32 	%f112, %f110, %f111;
	st.local.f32 	[%rd2], %f112;

$L__BB100_11:
	bar.sync 	0;
	mov.b32 	%r115, %f211;
	shfl.sync.bfly.b32 	%r119|%p20, %r115, %r84, %r83, %r85;
	mov.b32 	%f113, %r119;
	add.f32 	%f114, %f211, %f113;
	mov.b32 	%r120, %f114;
	shfl.sync.bfly.b32 	%r122|%p21, %r120, %r88, %r83, %r85;
	mov.b32 	%f115, %r122;
	add.f32 	%f116, %f114, %f115;
	mov.b32 	%r123, %f116;
	shfl.sync.bfly.b32 	%r125|%p22, %r123, %r91, %r83, %r85;
	mov.b32 	%f117, %r125;
	add.f32 	%f118, %f116, %f117;
	mov.b32 	%r126, %f118;
	shfl.sync.bfly.b32 	%r128|%p23, %r126, %r81, %r83, %r85;
	mov.b32 	%f119, %r128;
	add.f32 	%f120, %f118, %f119;
	mov.b32 	%r129, %f120;
	shfl.sync.bfly.b32 	%r131|%p24, %r129, %r96, %r83, %r85;
	mov.b32 	%f121, %r131;
	add.f32 	%f122, %f120, %f121;
	st.local.f32 	[%rd2+4], %f122;
	st.shared.f32 	[%r10], %f122;
	bar.sync 	0;
	@%p1 bra 	$L__BB100_13;

	ld.shared.f32 	%f123, [%r4];
	mov.b32 	%r132, %f123;
	mov.u32 	%r133, 31;
	mov.u32 	%r134, 16;
	mov.u32 	%r135, -1;
	shfl.sync.bfly.b32 	%r136|%p25, %r132, %r134, %r133, %r135;
	mov.b32 	%f124, %r136;
	add.f32 	%f125, %f123, %f124;
	mov.b32 	%r137, %f125;
	mov.u32 	%r138, 8;
	shfl.sync.bfly.b32 	%r139|%p26, %r137, %r138, %r133, %r135;
	mov.b32 	%f126, %r139;
	add.f32 	%f127, %f125, %f126;
	mov.b32 	%r140, %f127;
	mov.u32 	%r141, 4;
	shfl.sync.bfly.b32 	%r142|%p27, %r140, %r141, %r133, %r135;
	mov.b32 	%f128, %r142;
	add.f32 	%f129, %f127, %f128;
	mov.b32 	%r143, %f129;
	mov.u32 	%r144, 2;
	shfl.sync.bfly.b32 	%r145|%p28, %r143, %r144, %r133, %r135;
	mov.b32 	%f130, %r145;
	add.f32 	%f131, %f129, %f130;
	mov.b32 	%r146, %f131;
	mov.u32 	%r147, 1;
	shfl.sync.bfly.b32 	%r148|%p29, %r146, %r147, %r133, %r135;
	mov.b32 	%f132, %r148;
	add.f32 	%f133, %f131, %f132;
	st.local.f32 	[%rd2+4], %f133;

$L__BB100_13:
	bar.sync 	0;
	mov.b32 	%r149, %f210;
	mov.u32 	%r150, 31;
	mov.u32 	%r151, 16;
	mov.u32 	%r152, -1;
	shfl.sync.bfly.b32 	%r153|%p31, %r149, %r151, %r150, %r152;
	mov.b32 	%f134, %r153;
	add.f32 	%f135, %f210, %f134;
	mov.b32 	%r154, %f135;
	mov.u32 	%r155, 8;
	shfl.sync.bfly.b32 	%r156|%p32, %r154, %r155, %r150, %r152;
	mov.b32 	%f136, %r156;
	add.f32 	%f137, %f135, %f136;
	mov.b32 	%r157, %f137;
	mov.u32 	%r158, 4;
	shfl.sync.bfly.b32 	%r159|%p33, %r157, %r158, %r150, %r152;
	mov.b32 	%f138, %r159;
	add.f32 	%f139, %f137, %f138;
	mov.b32 	%r160, %f139;
	mov.u32 	%r161, 2;
	shfl.sync.bfly.b32 	%r162|%p34, %r160, %r161, %r150, %r152;
	mov.b32 	%f140, %r162;
	add.f32 	%f141, %f139, %f140;
	mov.b32 	%r163, %f141;
	mov.u32 	%r164, 1;
	shfl.sync.bfly.b32 	%r165|%p35, %r163, %r164, %r150, %r152;
	mov.b32 	%f142, %r165;
	add.f32 	%f143, %f141, %f142;
	st.local.f32 	[%rd2+8], %f143;
	st.shared.f32 	[%r10], %f143;
	bar.sync 	0;
	@%p1 bra 	$L__BB100_15;

	ld.shared.f32 	%f144, [%r4];
	mov.b32 	%r166, %f144;
	shfl.sync.bfly.b32 	%r170|%p36, %r166, %r151, %r150, %r152;
	mov.b32 	%f145, %r170;
	add.f32 	%f146, %f144, %f145;
	mov.b32 	%r171, %f146;
	shfl.sync.bfly.b32 	%r173|%p37, %r171, %r155, %r150, %r152;
	mov.b32 	%f147, %r173;
	add.f32 	%f148, %f146, %f147;
	mov.b32 	%r174, %f148;
	shfl.sync.bfly.b32 	%r176|%p38, %r174, %r158, %r150, %r152;
	mov.b32 	%f149, %r176;
	add.f32 	%f150, %f148, %f149;
	mov.b32 	%r177, %f150;
	shfl.sync.bfly.b32 	%r179|%p39, %r177, %r161, %r150, %r152;
	mov.b32 	%f151, %r179;
	add.f32 	%f152, %f150, %f151;
	mov.b32 	%r180, %f152;
	shfl.sync.bfly.b32 	%r182|%p40, %r180, %r164, %r150, %r152;
	mov.b32 	%f153, %r182;
	add.f32 	%f154, %f152, %f153;
	st.local.f32 	[%rd2+8], %f154;

$L__BB100_15:
	bar.sync 	0;
	mov.b32 	%r183, %f209;
	shfl.sync.bfly.b32 	%r187|%p42, %r183, %r151, %r150, %r152;
	mov.b32 	%f155, %r187;
	add.f32 	%f156, %f209, %f155;
	mov.b32 	%r188, %f156;
	shfl.sync.bfly.b32 	%r190|%p43, %r188, %r155, %r150, %r152;
	mov.b32 	%f157, %r190;
	add.f32 	%f158, %f156, %f157;
	mov.b32 	%r191, %f158;
	shfl.sync.bfly.b32 	%r193|%p44, %r191, %r158, %r150, %r152;
	mov.b32 	%f159, %r193;
	add.f32 	%f160, %f158, %f159;
	mov.b32 	%r194, %f160;
	shfl.sync.bfly.b32 	%r196|%p45, %r194, %r161, %r150, %r152;
	mov.b32 	%f161, %r196;
	add.f32 	%f162, %f160, %f161;
	mov.b32 	%r197, %f162;
	shfl.sync.bfly.b32 	%r199|%p46, %r197, %r164, %r150, %r152;
	mov.b32 	%f163, %r199;
	add.f32 	%f164, %f162, %f163;
	st.local.f32 	[%rd2+12], %f164;
	st.shared.f32 	[%r10], %f164;
	bar.sync 	0;
	@%p1 bra 	$L__BB100_17;

	ld.shared.f32 	%f165, [%r4];
	mov.b32 	%r200, %f165;
	mov.u32 	%r201, 31;
	mov.u32 	%r202, 16;
	mov.u32 	%r203, -1;
	shfl.sync.bfly.b32 	%r204|%p47, %r200, %r202, %r201, %r203;
	mov.b32 	%f166, %r204;
	add.f32 	%f167, %f165, %f166;
	mov.b32 	%r205, %f167;
	mov.u32 	%r206, 8;
	shfl.sync.bfly.b32 	%r207|%p48, %r205, %r206, %r201, %r203;
	mov.b32 	%f168, %r207;
	add.f32 	%f169, %f167, %f168;
	mov.b32 	%r208, %f169;
	mov.u32 	%r209, 4;
	shfl.sync.bfly.b32 	%r210|%p49, %r208, %r209, %r201, %r203;
	mov.b32 	%f170, %r210;
	add.f32 	%f171, %f169, %f170;
	mov.b32 	%r211, %f171;
	mov.u32 	%r212, 2;
	shfl.sync.bfly.b32 	%r213|%p50, %r211, %r212, %r201, %r203;
	mov.b32 	%f172, %r213;
	add.f32 	%f173, %f171, %f172;
	mov.b32 	%r214, %f173;
	mov.u32 	%r215, 1;
	shfl.sync.bfly.b32 	%r216|%p51, %r214, %r215, %r201, %r203;
	mov.b32 	%f174, %r216;
	add.f32 	%f175, %f173, %f174;
	st.local.f32 	[%rd2+12], %f175;

$L__BB100_17:
	bar.sync 	0;
	mov.b32 	%r217, %f208;
	mov.u32 	%r218, 31;
	mov.u32 	%r219, 16;
	mov.u32 	%r220, -1;
	shfl.sync.bfly.b32 	%r221|%p53, %r217, %r219, %r218, %r220;
	mov.b32 	%f176, %r221;
	add.f32 	%f177, %f208, %f176;
	mov.b32 	%r222, %f177;
	mov.u32 	%r223, 8;
	shfl.sync.bfly.b32 	%r224|%p54, %r222, %r223, %r218, %r220;
	mov.b32 	%f178, %r224;
	add.f32 	%f179, %f177, %f178;
	mov.b32 	%r225, %f179;
	mov.u32 	%r226, 4;
	shfl.sync.bfly.b32 	%r227|%p55, %r225, %r226, %r218, %r220;
	mov.b32 	%f180, %r227;
	add.f32 	%f181, %f179, %f180;
	mov.b32 	%r228, %f181;
	mov.u32 	%r229, 2;
	shfl.sync.bfly.b32 	%r230|%p56, %r228, %r229, %r218, %r220;
	mov.b32 	%f182, %r230;
	add.f32 	%f183, %f181, %f182;
	mov.b32 	%r231, %f183;
	mov.u32 	%r232, 1;
	shfl.sync.bfly.b32 	%r233|%p57, %r231, %r232, %r218, %r220;
	mov.b32 	%f184, %r233;
	add.f32 	%f185, %f183, %f184;
	st.local.f32 	[%rd2+16], %f185;
	st.shared.f32 	[%r10], %f185;
	bar.sync 	0;
	@%p1 bra 	$L__BB100_19;

	ld.shared.f32 	%f186, [%r4];
	mov.b32 	%r234, %f186;
	shfl.sync.bfly.b32 	%r238|%p58, %r234, %r219, %r218, %r220;
	mov.b32 	%f187, %r238;
	add.f32 	%f188, %f186, %f187;
	mov.b32 	%r239, %f188;
	shfl.sync.bfly.b32 	%r241|%p59, %r239, %r223, %r218, %r220;
	mov.b32 	%f189, %r241;
	add.f32 	%f190, %f188, %f189;
	mov.b32 	%r242, %f190;
	shfl.sync.bfly.b32 	%r244|%p60, %r242, %r226, %r218, %r220;
	mov.b32 	%f191, %r244;
	add.f32 	%f192, %f190, %f191;
	mov.b32 	%r245, %f192;
	shfl.sync.bfly.b32 	%r247|%p61, %r245, %r229, %r218, %r220;
	mov.b32 	%f193, %r247;
	add.f32 	%f194, %f192, %f193;
	mov.b32 	%r248, %f194;
	shfl.sync.bfly.b32 	%r250|%p62, %r248, %r232, %r218, %r220;
	mov.b32 	%f195, %r250;
	add.f32 	%f196, %f194, %f195;
	st.local.f32 	[%rd2+16], %f196;

$L__BB100_19:
	bar.sync 	0;
	setp.gt.s32 	%p63, %r3, 4;
	@%p63 bra 	$L__BB100_21;

	mad.lo.s32 	%r251, %r3, %r13, %r2;
	cvt.s64.s32 	%rd57, %r251;
	mul.lo.s32 	%r252, %r1, %r14;
	cvt.s64.s32 	%rd58, %r252;
	add.s64 	%rd59, %rd58, %rd57;
	mul.wide.s32 	%rd60, %r3, 4;
	add.s64 	%rd61, %rd2, %rd60;
	ld.local.f32 	%f197, [%rd61];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f197;}

	// end inline asm
	cvta.to.global.u64 	%rd62, %rd18;
	shl.b64 	%rd63, %rd59, 1;
	add.s64 	%rd64, %rd62, %rd63;
	st.global.u16 	[%rd64], %rs1;

$L__BB100_21:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_6_bs_160
.visible .entry ggml_matvec_f16_ncols_6_bs_160(
	.param .u64 ggml_matvec_f16_ncols_6_bs_160_param_0,
	.param .u64 ggml_matvec_f16_ncols_6_bs_160_param_1,
	.param .u64 ggml_matvec_f16_ncols_6_bs_160_param_2,
	.param .u32 ggml_matvec_f16_ncols_6_bs_160_param_3,
	.param .u32 ggml_matvec_f16_ncols_6_bs_160_param_4,
	.param .u32 ggml_matvec_f16_ncols_6_bs_160_param_5,
	.param .u32 ggml_matvec_f16_ncols_6_bs_160_param_6,
	.param .u32 ggml_matvec_f16_ncols_6_bs_160_param_7,
	.param .u32 ggml_matvec_f16_ncols_6_bs_160_param_8,
	.param .u32 ggml_matvec_f16_ncols_6_bs_160_param_9,
	.param .u32 ggml_matvec_f16_ncols_6_bs_160_param_10,
	.param .u32 ggml_matvec_f16_ncols_6_bs_160_param_11
)
{
	.local .align 8 .b8 	__local_depot101[24];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<75>;
	.reg .b16 	%rs<2>;
	.reg .f32 	%f<254>;
	.reg .b32 	%r<293>;
	.reg .b64 	%rd<70>;


	mov.u64 	%SPL, __local_depot101;
	ld.param.u64 	%rd20, [ggml_matvec_f16_ncols_6_bs_160_param_0];
	ld.param.u64 	%rd21, [ggml_matvec_f16_ncols_6_bs_160_param_1];
	ld.param.u64 	%rd19, [ggml_matvec_f16_ncols_6_bs_160_param_2];
	ld.param.u32 	%r11, [ggml_matvec_f16_ncols_6_bs_160_param_3];
	ld.param.u32 	%r15, [ggml_matvec_f16_ncols_6_bs_160_param_5];
	ld.param.u32 	%r12, [ggml_matvec_f16_ncols_6_bs_160_param_6];
	ld.param.u32 	%r13, [ggml_matvec_f16_ncols_6_bs_160_param_7];
	ld.param.u32 	%r16, [ggml_matvec_f16_ncols_6_bs_160_param_8];
	ld.param.u32 	%r17, [ggml_matvec_f16_ncols_6_bs_160_param_9];
	ld.param.u32 	%r18, [ggml_matvec_f16_ncols_6_bs_160_param_10];
	ld.param.u32 	%r14, [ggml_matvec_f16_ncols_6_bs_160_param_11];
	cvta.to.global.u64 	%rd69, %rd21;
	add.u64 	%rd2, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r19, %r1, %r16;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r20, %r2, %r15;
	mad.lo.s32 	%r21, %r19, %r17, %r20;
	cvt.s64.s32 	%rd3, %r21;
	cvta.to.global.u64 	%rd4, %rd20;
	mul.lo.s32 	%r22, %r1, %r18;
	cvt.s64.s32 	%rd5, %r22;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r23, %r3, 2;
	mov.u32 	%r24, data_mmv;
	add.s32 	%r4, %r24, %r23;
	@%p1 bra 	$L__BB101_2;

	mov.u32 	%r25, 0;
	st.shared.u32 	[%r4], %r25;

$L__BB101_2:
	bar.sync 	0;
	mov.f32 	%f248, 0f00000000;
	st.local.v2.f32 	[%rd2], {%f248, %f248};
	st.local.v2.f32 	[%rd2+8], {%f248, %f248};
	st.local.v2.f32 	[%rd2+16], {%f248, %f248};
	setp.ge.s32 	%p2, %r3, %r11;
	mov.f32 	%f249, %f248;
	mov.f32 	%f250, %f248;
	mov.f32 	%f251, %f248;
	mov.f32 	%f252, %f248;
	mov.f32 	%f253, %f248;
	@%p2 bra 	$L__BB101_9;

	not.b32 	%r26, %r3;
	add.s32 	%r5, %r26, %r11;
	mul.wide.u32 	%rd23, %r5, -858993459;
	shr.u64 	%rd24, %rd23, 39;
	and.b64  	%rd25, %rd24, 1;
	setp.eq.b64 	%p3, %rd25, 1;
	mov.pred 	%p4, 0;
	xor.pred  	%p5, %p3, %p4;
	mov.f32 	%f248, 0f00000000;
	mov.u32 	%r292, %r3;
	@%p5 bra 	$L__BB101_5;

	shl.b64 	%rd26, %rd5, 1;
	add.s64 	%rd27, %rd69, %rd26;
	shl.b64 	%rd28, %rd3, 1;
	add.s64 	%rd29, %rd4, %rd28;
	mul.wide.s32 	%rd30, %r3, 4;
	add.s64 	%rd31, %rd29, %rd30;
	ld.global.nc.u32 	%r27, [%rd31];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r27;
  cvt.f32.f16 %f43, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r27;
  cvt.f32.f16 %f44, high;}

	// end inline asm
	add.s64 	%rd32, %rd27, %rd30;
	ld.global.nc.u32 	%r29, [%rd32];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r29;
  cvt.f32.f16 %f45, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r29;
  cvt.f32.f16 %f46, high;}

	// end inline asm
	fma.rn.f32 	%f57, %f43, %f45, 0f00000000;
	fma.rn.f32 	%f253, %f44, %f46, %f57;
	st.local.f32 	[%rd2], %f253;
	mul.wide.s32 	%rd33, %r12, 4;
	add.s64 	%rd34, %rd32, %rd33;
	ld.global.nc.u32 	%r31, [%rd34];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r31;
  cvt.f32.f16 %f47, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r31;
  cvt.f32.f16 %f48, high;}

	// end inline asm
	fma.rn.f32 	%f58, %f43, %f47, 0f00000000;
	fma.rn.f32 	%f252, %f44, %f48, %f58;
	st.local.f32 	[%rd2+4], %f252;
	add.s32 	%r41, %r3, %r12;
	add.s32 	%r42, %r41, %r12;
	mul.wide.s32 	%rd35, %r42, 4;
	add.s64 	%rd36, %rd27, %rd35;
	ld.global.nc.u32 	%r33, [%rd36];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r33;
  cvt.f32.f16 %f49, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r33;
  cvt.f32.f16 %f50, high;}

	// end inline asm
	fma.rn.f32 	%f59, %f43, %f49, 0f00000000;
	fma.rn.f32 	%f251, %f44, %f50, %f59;
	st.local.f32 	[%rd2+8], %f251;
	add.s64 	%rd37, %rd36, %rd33;
	ld.global.nc.u32 	%r35, [%rd37];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r35;
  cvt.f32.f16 %f51, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r35;
  cvt.f32.f16 %f52, high;}

	// end inline asm
	fma.rn.f32 	%f60, %f43, %f51, 0f00000000;
	fma.rn.f32 	%f250, %f44, %f52, %f60;
	st.local.f32 	[%rd2+12], %f250;
	add.s64 	%rd38, %rd37, %rd33;
	ld.global.nc.u32 	%r37, [%rd38];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r37;
  cvt.f32.f16 %f53, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r37;
  cvt.f32.f16 %f54, high;}

	// end inline asm
	fma.rn.f32 	%f61, %f43, %f53, 0f00000000;
	fma.rn.f32 	%f249, %f44, %f54, %f61;
	st.local.f32 	[%rd2+16], %f249;
	add.s64 	%rd39, %rd38, %rd33;
	ld.global.nc.u32 	%r39, [%rd39];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r39;
  cvt.f32.f16 %f55, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r39;
  cvt.f32.f16 %f56, high;}

	// end inline asm
	fma.rn.f32 	%f62, %f43, %f55, 0f00000000;
	fma.rn.f32 	%f248, %f44, %f56, %f62;
	st.local.f32 	[%rd2+20], %f248;
	add.s32 	%r292, %r3, 160;

$L__BB101_5:
	setp.lt.u32 	%p6, %r5, 160;
	@%p6 bra 	$L__BB101_9;

	add.s32 	%r43, %r292, %r12;
	add.s32 	%r44, %r43, 160;
	mul.wide.s32 	%rd40, %r44, 4;
	shl.b64 	%rd41, %rd5, 1;
	add.s64 	%rd7, %rd40, %rd41;
	shl.b32 	%r45, %r12, 1;
	add.s32 	%r46, %r292, %r45;
	mad.lo.s32 	%r47, %r12, 3, %r292;
	shl.b32 	%r48, %r12, 2;
	add.s32 	%r49, %r292, %r48;
	mad.lo.s32 	%r50, %r12, 5, %r292;
	mul.wide.s32 	%rd42, %r46, 4;
	add.s64 	%rd8, %rd42, %rd41;
	mul.wide.s32 	%rd43, %r47, 4;
	add.s64 	%rd9, %rd43, %rd41;
	mul.wide.s32 	%rd44, %r49, 4;
	add.s64 	%rd10, %rd44, %rd41;
	mul.wide.s32 	%rd45, %r50, 4;
	add.s64 	%rd11, %rd45, %rd41;
	mul.wide.s32 	%rd46, %r292, 2;
	add.s64 	%rd47, %rd46, %rd3;
	shl.b64 	%rd48, %rd47, 1;
	add.s64 	%rd49, %rd4, %rd48;
	add.s64 	%rd68, %rd49, 640;
	mul.wide.s32 	%rd50, %r292, 4;
	mul.wide.s32 	%rd51, %r12, 4;
	add.s64 	%rd52, %rd50, %rd51;
	add.s64 	%rd13, %rd52, %rd41;
	add.s64 	%rd14, %rd50, %rd41;

$L__BB101_7:
	ld.global.nc.u32 	%r51, [%rd68+-640];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r51;
  cvt.f32.f16 %f63, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r51;
  cvt.f32.f16 %f64, high;}

	// end inline asm
	add.s64 	%rd53, %rd69, %rd14;
	ld.global.nc.u32 	%r53, [%rd53];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r53;
  cvt.f32.f16 %f65, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r53;
  cvt.f32.f16 %f66, high;}

	// end inline asm
	fma.rn.f32 	%f91, %f63, %f65, %f253;
	fma.rn.f32 	%f92, %f64, %f66, %f91;
	add.s64 	%rd54, %rd69, %rd13;
	ld.global.nc.u32 	%r55, [%rd54];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r55;
  cvt.f32.f16 %f67, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r55;
  cvt.f32.f16 %f68, high;}

	// end inline asm
	fma.rn.f32 	%f93, %f63, %f67, %f252;
	fma.rn.f32 	%f94, %f64, %f68, %f93;
	add.s64 	%rd55, %rd69, %rd8;
	ld.global.nc.u32 	%r57, [%rd55];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r57;
  cvt.f32.f16 %f69, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r57;
  cvt.f32.f16 %f70, high;}

	// end inline asm
	fma.rn.f32 	%f95, %f63, %f69, %f251;
	fma.rn.f32 	%f96, %f64, %f70, %f95;
	add.s64 	%rd56, %rd69, %rd9;
	ld.global.nc.u32 	%r59, [%rd56];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r59;
  cvt.f32.f16 %f71, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r59;
  cvt.f32.f16 %f72, high;}

	// end inline asm
	fma.rn.f32 	%f97, %f63, %f71, %f250;
	fma.rn.f32 	%f98, %f64, %f72, %f97;
	add.s64 	%rd57, %rd69, %rd10;
	ld.global.nc.u32 	%r61, [%rd57];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r61;
  cvt.f32.f16 %f73, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r61;
  cvt.f32.f16 %f74, high;}

	// end inline asm
	fma.rn.f32 	%f99, %f63, %f73, %f249;
	fma.rn.f32 	%f100, %f64, %f74, %f99;
	add.s64 	%rd58, %rd69, %rd11;
	ld.global.nc.u32 	%r63, [%rd58];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r63;
  cvt.f32.f16 %f75, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r63;
  cvt.f32.f16 %f76, high;}

	// end inline asm
	fma.rn.f32 	%f101, %f63, %f75, %f248;
	fma.rn.f32 	%f102, %f64, %f76, %f101;
	ld.global.nc.u32 	%r65, [%rd68];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r65;
  cvt.f32.f16 %f77, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r65;
  cvt.f32.f16 %f78, high;}

	// end inline asm
	ld.global.nc.u32 	%r67, [%rd53+640];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r67;
  cvt.f32.f16 %f79, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r67;
  cvt.f32.f16 %f80, high;}

	// end inline asm
	fma.rn.f32 	%f103, %f77, %f79, %f92;
	fma.rn.f32 	%f253, %f78, %f80, %f103;
	add.s64 	%rd59, %rd69, %rd7;
	ld.global.nc.u32 	%r69, [%rd59];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r69;
  cvt.f32.f16 %f81, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r69;
  cvt.f32.f16 %f82, high;}

	// end inline asm
	fma.rn.f32 	%f104, %f77, %f81, %f94;
	fma.rn.f32 	%f252, %f78, %f82, %f104;
	ld.global.nc.u32 	%r71, [%rd55+640];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r71;
  cvt.f32.f16 %f83, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r71;
  cvt.f32.f16 %f84, high;}

	// end inline asm
	fma.rn.f32 	%f105, %f77, %f83, %f96;
	fma.rn.f32 	%f251, %f78, %f84, %f105;
	ld.global.nc.u32 	%r73, [%rd56+640];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r73;
  cvt.f32.f16 %f85, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r73;
  cvt.f32.f16 %f86, high;}

	// end inline asm
	fma.rn.f32 	%f106, %f77, %f85, %f98;
	fma.rn.f32 	%f250, %f78, %f86, %f106;
	ld.global.nc.u32 	%r75, [%rd57+640];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r75;
  cvt.f32.f16 %f87, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r75;
  cvt.f32.f16 %f88, high;}

	// end inline asm
	fma.rn.f32 	%f107, %f77, %f87, %f100;
	fma.rn.f32 	%f249, %f78, %f88, %f107;
	ld.global.nc.u32 	%r77, [%rd58+640];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r77;
  cvt.f32.f16 %f89, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r77;
  cvt.f32.f16 %f90, high;}

	// end inline asm
	fma.rn.f32 	%f108, %f77, %f89, %f102;
	fma.rn.f32 	%f248, %f78, %f90, %f108;
	add.s64 	%rd69, %rd69, 1280;
	add.s64 	%rd68, %rd68, 1280;
	add.s32 	%r292, %r292, 320;
	setp.lt.s32 	%p7, %r292, %r11;
	@%p7 bra 	$L__BB101_7;

	st.local.v2.f32 	[%rd2], {%f253, %f252};
	st.local.v2.f32 	[%rd2+8], {%f251, %f250};
	st.local.v2.f32 	[%rd2+16], {%f249, %f248};

$L__BB101_9:
	shr.s32 	%r79, %r3, 31;
	shr.u32 	%r80, %r79, 27;
	add.s32 	%r81, %r3, %r80;
	shr.s32 	%r82, %r81, 5;
	shl.b32 	%r83, %r82, 2;
	add.s32 	%r10, %r24, %r83;
	mov.u32 	%r85, 2;
	mov.b32 	%r86, %f253;
	mov.u32 	%r87, 31;
	mov.u32 	%r88, 16;
	mov.u32 	%r89, -1;
	shfl.sync.bfly.b32 	%r90|%p8, %r86, %r88, %r87, %r89;
	mov.b32 	%f109, %r90;
	add.f32 	%f110, %f253, %f109;
	mov.b32 	%r91, %f110;
	mov.u32 	%r92, 8;
	shfl.sync.bfly.b32 	%r93|%p9, %r91, %r92, %r87, %r89;
	mov.b32 	%f111, %r93;
	add.f32 	%f112, %f110, %f111;
	mov.b32 	%r94, %f112;
	mov.u32 	%r95, 4;
	shfl.sync.bfly.b32 	%r96|%p10, %r94, %r95, %r87, %r89;
	mov.b32 	%f113, %r96;
	add.f32 	%f114, %f112, %f113;
	mov.b32 	%r97, %f114;
	shfl.sync.bfly.b32 	%r98|%p11, %r97, %r85, %r87, %r89;
	mov.b32 	%f115, %r98;
	add.f32 	%f116, %f114, %f115;
	mov.b32 	%r99, %f116;
	mov.u32 	%r100, 1;
	shfl.sync.bfly.b32 	%r101|%p12, %r99, %r100, %r87, %r89;
	mov.b32 	%f117, %r101;
	add.f32 	%f118, %f116, %f117;
	st.local.f32 	[%rd2], %f118;
	st.shared.f32 	[%r10], %f118;
	bar.sync 	0;
	@%p1 bra 	$L__BB101_11;

	ld.shared.f32 	%f119, [%r4];
	mov.b32 	%r102, %f119;
	shfl.sync.bfly.b32 	%r106|%p14, %r102, %r88, %r87, %r89;
	mov.b32 	%f120, %r106;
	add.f32 	%f121, %f119, %f120;
	mov.b32 	%r107, %f121;
	shfl.sync.bfly.b32 	%r109|%p15, %r107, %r92, %r87, %r89;
	mov.b32 	%f122, %r109;
	add.f32 	%f123, %f121, %f122;
	mov.b32 	%r110, %f123;
	shfl.sync.bfly.b32 	%r112|%p16, %r110, %r95, %r87, %r89;
	mov.b32 	%f124, %r112;
	add.f32 	%f125, %f123, %f124;
	mov.b32 	%r113, %f125;
	shfl.sync.bfly.b32 	%r115|%p17, %r113, %r85, %r87, %r89;
	mov.b32 	%f126, %r115;
	add.f32 	%f127, %f125, %f126;
	mov.b32 	%r116, %f127;
	shfl.sync.bfly.b32 	%r118|%p18, %r116, %r100, %r87, %r89;
	mov.b32 	%f128, %r118;
	add.f32 	%f129, %f127, %f128;
	st.local.f32 	[%rd2], %f129;

$L__BB101_11:
	bar.sync 	0;
	mov.b32 	%r119, %f252;
	shfl.sync.bfly.b32 	%r123|%p20, %r119, %r88, %r87, %r89;
	mov.b32 	%f130, %r123;
	add.f32 	%f131, %f252, %f130;
	mov.b32 	%r124, %f131;
	shfl.sync.bfly.b32 	%r126|%p21, %r124, %r92, %r87, %r89;
	mov.b32 	%f132, %r126;
	add.f32 	%f133, %f131, %f132;
	mov.b32 	%r127, %f133;
	shfl.sync.bfly.b32 	%r129|%p22, %r127, %r95, %r87, %r89;
	mov.b32 	%f134, %r129;
	add.f32 	%f135, %f133, %f134;
	mov.b32 	%r130, %f135;
	shfl.sync.bfly.b32 	%r132|%p23, %r130, %r85, %r87, %r89;
	mov.b32 	%f136, %r132;
	add.f32 	%f137, %f135, %f136;
	mov.b32 	%r133, %f137;
	shfl.sync.bfly.b32 	%r135|%p24, %r133, %r100, %r87, %r89;
	mov.b32 	%f138, %r135;
	add.f32 	%f139, %f137, %f138;
	st.local.f32 	[%rd2+4], %f139;
	st.shared.f32 	[%r10], %f139;
	bar.sync 	0;
	@%p1 bra 	$L__BB101_13;

	ld.shared.f32 	%f140, [%r4];
	mov.b32 	%r136, %f140;
	mov.u32 	%r137, 31;
	mov.u32 	%r138, 16;
	mov.u32 	%r139, -1;
	shfl.sync.bfly.b32 	%r140|%p25, %r136, %r138, %r137, %r139;
	mov.b32 	%f141, %r140;
	add.f32 	%f142, %f140, %f141;
	mov.b32 	%r141, %f142;
	mov.u32 	%r142, 8;
	shfl.sync.bfly.b32 	%r143|%p26, %r141, %r142, %r137, %r139;
	mov.b32 	%f143, %r143;
	add.f32 	%f144, %f142, %f143;
	mov.b32 	%r144, %f144;
	mov.u32 	%r145, 4;
	shfl.sync.bfly.b32 	%r146|%p27, %r144, %r145, %r137, %r139;
	mov.b32 	%f145, %r146;
	add.f32 	%f146, %f144, %f145;
	mov.b32 	%r147, %f146;
	mov.u32 	%r148, 2;
	shfl.sync.bfly.b32 	%r149|%p28, %r147, %r148, %r137, %r139;
	mov.b32 	%f147, %r149;
	add.f32 	%f148, %f146, %f147;
	mov.b32 	%r150, %f148;
	mov.u32 	%r151, 1;
	shfl.sync.bfly.b32 	%r152|%p29, %r150, %r151, %r137, %r139;
	mov.b32 	%f149, %r152;
	add.f32 	%f150, %f148, %f149;
	st.local.f32 	[%rd2+4], %f150;

$L__BB101_13:
	bar.sync 	0;
	mov.b32 	%r153, %f251;
	mov.u32 	%r154, 31;
	mov.u32 	%r155, 16;
	mov.u32 	%r156, -1;
	shfl.sync.bfly.b32 	%r157|%p31, %r153, %r155, %r154, %r156;
	mov.b32 	%f151, %r157;
	add.f32 	%f152, %f251, %f151;
	mov.b32 	%r158, %f152;
	mov.u32 	%r159, 8;
	shfl.sync.bfly.b32 	%r160|%p32, %r158, %r159, %r154, %r156;
	mov.b32 	%f153, %r160;
	add.f32 	%f154, %f152, %f153;
	mov.b32 	%r161, %f154;
	mov.u32 	%r162, 4;
	shfl.sync.bfly.b32 	%r163|%p33, %r161, %r162, %r154, %r156;
	mov.b32 	%f155, %r163;
	add.f32 	%f156, %f154, %f155;
	mov.b32 	%r164, %f156;
	mov.u32 	%r165, 2;
	shfl.sync.bfly.b32 	%r166|%p34, %r164, %r165, %r154, %r156;
	mov.b32 	%f157, %r166;
	add.f32 	%f158, %f156, %f157;
	mov.b32 	%r167, %f158;
	mov.u32 	%r168, 1;
	shfl.sync.bfly.b32 	%r169|%p35, %r167, %r168, %r154, %r156;
	mov.b32 	%f159, %r169;
	add.f32 	%f160, %f158, %f159;
	st.local.f32 	[%rd2+8], %f160;
	st.shared.f32 	[%r10], %f160;
	bar.sync 	0;
	@%p1 bra 	$L__BB101_15;

	ld.shared.f32 	%f161, [%r4];
	mov.b32 	%r170, %f161;
	shfl.sync.bfly.b32 	%r174|%p36, %r170, %r155, %r154, %r156;
	mov.b32 	%f162, %r174;
	add.f32 	%f163, %f161, %f162;
	mov.b32 	%r175, %f163;
	shfl.sync.bfly.b32 	%r177|%p37, %r175, %r159, %r154, %r156;
	mov.b32 	%f164, %r177;
	add.f32 	%f165, %f163, %f164;
	mov.b32 	%r178, %f165;
	shfl.sync.bfly.b32 	%r180|%p38, %r178, %r162, %r154, %r156;
	mov.b32 	%f166, %r180;
	add.f32 	%f167, %f165, %f166;
	mov.b32 	%r181, %f167;
	shfl.sync.bfly.b32 	%r183|%p39, %r181, %r165, %r154, %r156;
	mov.b32 	%f168, %r183;
	add.f32 	%f169, %f167, %f168;
	mov.b32 	%r184, %f169;
	shfl.sync.bfly.b32 	%r186|%p40, %r184, %r168, %r154, %r156;
	mov.b32 	%f170, %r186;
	add.f32 	%f171, %f169, %f170;
	st.local.f32 	[%rd2+8], %f171;

$L__BB101_15:
	bar.sync 	0;
	mov.b32 	%r187, %f250;
	shfl.sync.bfly.b32 	%r191|%p42, %r187, %r155, %r154, %r156;
	mov.b32 	%f172, %r191;
	add.f32 	%f173, %f250, %f172;
	mov.b32 	%r192, %f173;
	shfl.sync.bfly.b32 	%r194|%p43, %r192, %r159, %r154, %r156;
	mov.b32 	%f174, %r194;
	add.f32 	%f175, %f173, %f174;
	mov.b32 	%r195, %f175;
	shfl.sync.bfly.b32 	%r197|%p44, %r195, %r162, %r154, %r156;
	mov.b32 	%f176, %r197;
	add.f32 	%f177, %f175, %f176;
	mov.b32 	%r198, %f177;
	shfl.sync.bfly.b32 	%r200|%p45, %r198, %r165, %r154, %r156;
	mov.b32 	%f178, %r200;
	add.f32 	%f179, %f177, %f178;
	mov.b32 	%r201, %f179;
	shfl.sync.bfly.b32 	%r203|%p46, %r201, %r168, %r154, %r156;
	mov.b32 	%f180, %r203;
	add.f32 	%f181, %f179, %f180;
	st.local.f32 	[%rd2+12], %f181;
	st.shared.f32 	[%r10], %f181;
	bar.sync 	0;
	@%p1 bra 	$L__BB101_17;

	ld.shared.f32 	%f182, [%r4];
	mov.b32 	%r204, %f182;
	mov.u32 	%r205, 31;
	mov.u32 	%r206, 16;
	mov.u32 	%r207, -1;
	shfl.sync.bfly.b32 	%r208|%p47, %r204, %r206, %r205, %r207;
	mov.b32 	%f183, %r208;
	add.f32 	%f184, %f182, %f183;
	mov.b32 	%r209, %f184;
	mov.u32 	%r210, 8;
	shfl.sync.bfly.b32 	%r211|%p48, %r209, %r210, %r205, %r207;
	mov.b32 	%f185, %r211;
	add.f32 	%f186, %f184, %f185;
	mov.b32 	%r212, %f186;
	mov.u32 	%r213, 4;
	shfl.sync.bfly.b32 	%r214|%p49, %r212, %r213, %r205, %r207;
	mov.b32 	%f187, %r214;
	add.f32 	%f188, %f186, %f187;
	mov.b32 	%r215, %f188;
	mov.u32 	%r216, 2;
	shfl.sync.bfly.b32 	%r217|%p50, %r215, %r216, %r205, %r207;
	mov.b32 	%f189, %r217;
	add.f32 	%f190, %f188, %f189;
	mov.b32 	%r218, %f190;
	mov.u32 	%r219, 1;
	shfl.sync.bfly.b32 	%r220|%p51, %r218, %r219, %r205, %r207;
	mov.b32 	%f191, %r220;
	add.f32 	%f192, %f190, %f191;
	st.local.f32 	[%rd2+12], %f192;

$L__BB101_17:
	bar.sync 	0;
	mov.b32 	%r221, %f249;
	mov.u32 	%r222, 31;
	mov.u32 	%r223, 16;
	mov.u32 	%r224, -1;
	shfl.sync.bfly.b32 	%r225|%p53, %r221, %r223, %r222, %r224;
	mov.b32 	%f193, %r225;
	add.f32 	%f194, %f249, %f193;
	mov.b32 	%r226, %f194;
	mov.u32 	%r227, 8;
	shfl.sync.bfly.b32 	%r228|%p54, %r226, %r227, %r222, %r224;
	mov.b32 	%f195, %r228;
	add.f32 	%f196, %f194, %f195;
	mov.b32 	%r229, %f196;
	mov.u32 	%r230, 4;
	shfl.sync.bfly.b32 	%r231|%p55, %r229, %r230, %r222, %r224;
	mov.b32 	%f197, %r231;
	add.f32 	%f198, %f196, %f197;
	mov.b32 	%r232, %f198;
	mov.u32 	%r233, 2;
	shfl.sync.bfly.b32 	%r234|%p56, %r232, %r233, %r222, %r224;
	mov.b32 	%f199, %r234;
	add.f32 	%f200, %f198, %f199;
	mov.b32 	%r235, %f200;
	mov.u32 	%r236, 1;
	shfl.sync.bfly.b32 	%r237|%p57, %r235, %r236, %r222, %r224;
	mov.b32 	%f201, %r237;
	add.f32 	%f202, %f200, %f201;
	st.local.f32 	[%rd2+16], %f202;
	st.shared.f32 	[%r10], %f202;
	bar.sync 	0;
	@%p1 bra 	$L__BB101_19;

	ld.shared.f32 	%f203, [%r4];
	mov.b32 	%r238, %f203;
	shfl.sync.bfly.b32 	%r242|%p58, %r238, %r223, %r222, %r224;
	mov.b32 	%f204, %r242;
	add.f32 	%f205, %f203, %f204;
	mov.b32 	%r243, %f205;
	shfl.sync.bfly.b32 	%r245|%p59, %r243, %r227, %r222, %r224;
	mov.b32 	%f206, %r245;
	add.f32 	%f207, %f205, %f206;
	mov.b32 	%r246, %f207;
	shfl.sync.bfly.b32 	%r248|%p60, %r246, %r230, %r222, %r224;
	mov.b32 	%f208, %r248;
	add.f32 	%f209, %f207, %f208;
	mov.b32 	%r249, %f209;
	shfl.sync.bfly.b32 	%r251|%p61, %r249, %r233, %r222, %r224;
	mov.b32 	%f210, %r251;
	add.f32 	%f211, %f209, %f210;
	mov.b32 	%r252, %f211;
	shfl.sync.bfly.b32 	%r254|%p62, %r252, %r236, %r222, %r224;
	mov.b32 	%f212, %r254;
	add.f32 	%f213, %f211, %f212;
	st.local.f32 	[%rd2+16], %f213;

$L__BB101_19:
	bar.sync 	0;
	mov.b32 	%r255, %f248;
	shfl.sync.bfly.b32 	%r259|%p64, %r255, %r223, %r222, %r224;
	mov.b32 	%f214, %r259;
	add.f32 	%f215, %f248, %f214;
	mov.b32 	%r260, %f215;
	shfl.sync.bfly.b32 	%r262|%p65, %r260, %r227, %r222, %r224;
	mov.b32 	%f216, %r262;
	add.f32 	%f217, %f215, %f216;
	mov.b32 	%r263, %f217;
	shfl.sync.bfly.b32 	%r265|%p66, %r263, %r230, %r222, %r224;
	mov.b32 	%f218, %r265;
	add.f32 	%f219, %f217, %f218;
	mov.b32 	%r266, %f219;
	shfl.sync.bfly.b32 	%r268|%p67, %r266, %r233, %r222, %r224;
	mov.b32 	%f220, %r268;
	add.f32 	%f221, %f219, %f220;
	mov.b32 	%r269, %f221;
	shfl.sync.bfly.b32 	%r271|%p68, %r269, %r236, %r222, %r224;
	mov.b32 	%f222, %r271;
	add.f32 	%f223, %f221, %f222;
	st.local.f32 	[%rd2+20], %f223;
	st.shared.f32 	[%r10], %f223;
	bar.sync 	0;
	@%p1 bra 	$L__BB101_21;

	ld.shared.f32 	%f224, [%r4];
	mov.b32 	%r272, %f224;
	mov.u32 	%r273, 31;
	mov.u32 	%r274, 16;
	mov.u32 	%r275, -1;
	shfl.sync.bfly.b32 	%r276|%p69, %r272, %r274, %r273, %r275;
	mov.b32 	%f225, %r276;
	add.f32 	%f226, %f224, %f225;
	mov.b32 	%r277, %f226;
	mov.u32 	%r278, 8;
	shfl.sync.bfly.b32 	%r279|%p70, %r277, %r278, %r273, %r275;
	mov.b32 	%f227, %r279;
	add.f32 	%f228, %f226, %f227;
	mov.b32 	%r280, %f228;
	mov.u32 	%r281, 4;
	shfl.sync.bfly.b32 	%r282|%p71, %r280, %r281, %r273, %r275;
	mov.b32 	%f229, %r282;
	add.f32 	%f230, %f228, %f229;
	mov.b32 	%r283, %f230;
	mov.u32 	%r284, 2;
	shfl.sync.bfly.b32 	%r285|%p72, %r283, %r284, %r273, %r275;
	mov.b32 	%f231, %r285;
	add.f32 	%f232, %f230, %f231;
	mov.b32 	%r286, %f232;
	mov.u32 	%r287, 1;
	shfl.sync.bfly.b32 	%r288|%p73, %r286, %r287, %r273, %r275;
	mov.b32 	%f233, %r288;
	add.f32 	%f234, %f232, %f233;
	st.local.f32 	[%rd2+20], %f234;

$L__BB101_21:
	bar.sync 	0;
	setp.gt.s32 	%p74, %r3, 5;
	@%p74 bra 	$L__BB101_23;

	mad.lo.s32 	%r289, %r3, %r13, %r2;
	cvt.s64.s32 	%rd60, %r289;
	mul.lo.s32 	%r290, %r1, %r14;
	cvt.s64.s32 	%rd61, %r290;
	add.s64 	%rd62, %rd61, %rd60;
	mul.wide.s32 	%rd63, %r3, 4;
	add.s64 	%rd64, %rd2, %rd63;
	ld.local.f32 	%f235, [%rd64];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f235;}

	// end inline asm
	cvta.to.global.u64 	%rd65, %rd19;
	shl.b64 	%rd66, %rd62, 1;
	add.s64 	%rd67, %rd65, %rd66;
	st.global.u16 	[%rd67], %rs1;

$L__BB101_23:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_7_bs_160
.visible .entry ggml_matvec_f16_ncols_7_bs_160(
	.param .u64 ggml_matvec_f16_ncols_7_bs_160_param_0,
	.param .u64 ggml_matvec_f16_ncols_7_bs_160_param_1,
	.param .u64 ggml_matvec_f16_ncols_7_bs_160_param_2,
	.param .u32 ggml_matvec_f16_ncols_7_bs_160_param_3,
	.param .u32 ggml_matvec_f16_ncols_7_bs_160_param_4,
	.param .u32 ggml_matvec_f16_ncols_7_bs_160_param_5,
	.param .u32 ggml_matvec_f16_ncols_7_bs_160_param_6,
	.param .u32 ggml_matvec_f16_ncols_7_bs_160_param_7,
	.param .u32 ggml_matvec_f16_ncols_7_bs_160_param_8,
	.param .u32 ggml_matvec_f16_ncols_7_bs_160_param_9,
	.param .u32 ggml_matvec_f16_ncols_7_bs_160_param_10,
	.param .u32 ggml_matvec_f16_ncols_7_bs_160_param_11
)
{
	.local .align 4 .b8 	__local_depot102[28];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<82>;
	.reg .b16 	%rs<2>;
	.reg .f32 	%f<221>;
	.reg .b32 	%r<289>;
	.reg .b64 	%rd<43>;


	mov.u64 	%SPL, __local_depot102;
	ld.param.u64 	%rd13, [ggml_matvec_f16_ncols_7_bs_160_param_0];
	ld.param.u64 	%rd14, [ggml_matvec_f16_ncols_7_bs_160_param_1];
	ld.param.u64 	%rd15, [ggml_matvec_f16_ncols_7_bs_160_param_2];
	ld.param.u32 	%r8, [ggml_matvec_f16_ncols_7_bs_160_param_3];
	ld.param.u32 	%r9, [ggml_matvec_f16_ncols_7_bs_160_param_5];
	ld.param.u32 	%r10, [ggml_matvec_f16_ncols_7_bs_160_param_6];
	ld.param.u32 	%r11, [ggml_matvec_f16_ncols_7_bs_160_param_7];
	ld.param.u32 	%r12, [ggml_matvec_f16_ncols_7_bs_160_param_8];
	ld.param.u32 	%r13, [ggml_matvec_f16_ncols_7_bs_160_param_9];
	ld.param.u32 	%r14, [ggml_matvec_f16_ncols_7_bs_160_param_10];
	ld.param.u32 	%r15, [ggml_matvec_f16_ncols_7_bs_160_param_11];
	add.u64 	%rd1, %SPL, 0;
	mov.u32 	%r1, %ctaid.x;
	mov.u32 	%r2, %tid.x;
	setp.gt.s32 	%p1, %r2, 31;
	shl.b32 	%r16, %r2, 2;
	mov.u32 	%r17, data_mmv;
	add.s32 	%r3, %r17, %r16;
	@%p1 bra 	$L__BB102_2;

	mov.u32 	%r18, 0;
	st.shared.u32 	[%r3], %r18;

$L__BB102_2:
	mov.u32 	%r4, %ctaid.y;
	bar.sync 	0;
	mov.f32 	%f214, 0f00000000;
	mov.u32 	%r19, 0;
	st.local.u32 	[%rd1], %r19;
	st.local.u32 	[%rd1+4], %r19;
	st.local.u32 	[%rd1+8], %r19;
	st.local.u32 	[%rd1+12], %r19;
	st.local.u32 	[%rd1+16], %r19;
	st.local.u32 	[%rd1+20], %r19;
	st.local.u32 	[%rd1+24], %r19;
	setp.ge.s32 	%p2, %r2, %r8;
	mov.f32 	%f215, %f214;
	mov.f32 	%f216, %f214;
	mov.f32 	%f217, %f214;
	mov.f32 	%f218, %f214;
	mov.f32 	%f219, %f214;
	mov.f32 	%f220, %f214;
	@%p2 bra 	$L__BB102_6;

	shl.b32 	%r20, %r10, 1;
	add.s32 	%r21, %r2, %r20;
	mul.wide.s32 	%rd17, %r21, 4;
	mul.lo.s32 	%r22, %r4, %r14;
	mul.wide.s32 	%rd18, %r22, 2;
	add.s64 	%rd3, %rd17, %rd18;
	mul.wide.s32 	%rd19, %r2, 4;
	mul.wide.s32 	%rd4, %r10, 4;
	add.s64 	%rd20, %rd19, %rd4;
	add.s64 	%rd5, %rd20, %rd18;
	add.s64 	%rd6, %rd19, %rd18;
	mul.wide.s32 	%rd21, %r2, 2;
	div.s32 	%r23, %r4, %r12;
	mul.lo.s32 	%r24, %r1, %r9;
	mad.lo.s32 	%r25, %r23, %r13, %r24;
	cvt.s64.s32 	%rd22, %r25;
	add.s64 	%rd23, %rd21, %rd22;
	cvta.to.global.u64 	%rd24, %rd13;
	shl.b64 	%rd25, %rd23, 1;
	add.s64 	%rd41, %rd24, %rd25;
	cvta.to.global.u64 	%rd42, %rd14;
	mov.f32 	%f214, 0f00000000;
	mov.u32 	%r288, %r2;

$L__BB102_4:
	ld.global.nc.u32 	%r26, [%rd41];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r26;
  cvt.f32.f16 %f36, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r26;
  cvt.f32.f16 %f37, high;}

	// end inline asm
	add.s64 	%rd26, %rd42, %rd6;
	ld.global.nc.u32 	%r28, [%rd26];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r28;
  cvt.f32.f16 %f38, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r28;
  cvt.f32.f16 %f39, high;}

	// end inline asm
	fma.rn.f32 	%f52, %f36, %f38, %f220;
	fma.rn.f32 	%f220, %f37, %f39, %f52;
	add.s64 	%rd27, %rd42, %rd5;
	ld.global.nc.u32 	%r30, [%rd27];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r30;
  cvt.f32.f16 %f40, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r30;
  cvt.f32.f16 %f41, high;}

	// end inline asm
	fma.rn.f32 	%f53, %f36, %f40, %f219;
	fma.rn.f32 	%f219, %f37, %f41, %f53;
	add.s64 	%rd28, %rd42, %rd3;
	ld.global.nc.u32 	%r32, [%rd28];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r32;
  cvt.f32.f16 %f42, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r32;
  cvt.f32.f16 %f43, high;}

	// end inline asm
	fma.rn.f32 	%f54, %f36, %f42, %f218;
	fma.rn.f32 	%f218, %f37, %f43, %f54;
	add.s64 	%rd29, %rd28, %rd4;
	ld.global.nc.u32 	%r34, [%rd29];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r34;
  cvt.f32.f16 %f44, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r34;
  cvt.f32.f16 %f45, high;}

	// end inline asm
	fma.rn.f32 	%f55, %f36, %f44, %f217;
	fma.rn.f32 	%f217, %f37, %f45, %f55;
	add.s64 	%rd30, %rd29, %rd4;
	ld.global.nc.u32 	%r36, [%rd30];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r36;
  cvt.f32.f16 %f46, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r36;
  cvt.f32.f16 %f47, high;}

	// end inline asm
	fma.rn.f32 	%f56, %f36, %f46, %f216;
	fma.rn.f32 	%f216, %f37, %f47, %f56;
	add.s64 	%rd31, %rd30, %rd4;
	ld.global.nc.u32 	%r38, [%rd31];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r38;
  cvt.f32.f16 %f48, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r38;
  cvt.f32.f16 %f49, high;}

	// end inline asm
	fma.rn.f32 	%f57, %f36, %f48, %f215;
	fma.rn.f32 	%f215, %f37, %f49, %f57;
	add.s64 	%rd32, %rd31, %rd4;
	ld.global.nc.u32 	%r40, [%rd32];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r40;
  cvt.f32.f16 %f50, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r40;
  cvt.f32.f16 %f51, high;}

	// end inline asm
	fma.rn.f32 	%f58, %f36, %f50, %f214;
	fma.rn.f32 	%f214, %f37, %f51, %f58;
	add.s64 	%rd42, %rd42, 640;
	add.s64 	%rd41, %rd41, 640;
	add.s32 	%r288, %r288, 160;
	setp.lt.s32 	%p3, %r288, %r8;
	@%p3 bra 	$L__BB102_4;

	st.local.f32 	[%rd1], %f220;
	st.local.f32 	[%rd1+4], %f219;
	st.local.f32 	[%rd1+8], %f218;
	st.local.f32 	[%rd1+12], %f217;
	st.local.f32 	[%rd1+16], %f216;
	st.local.f32 	[%rd1+20], %f215;
	st.local.f32 	[%rd1+24], %f214;

$L__BB102_6:
	shr.s32 	%r42, %r2, 31;
	shr.u32 	%r43, %r42, 27;
	add.s32 	%r44, %r2, %r43;
	shr.s32 	%r45, %r44, 5;
	shl.b32 	%r46, %r45, 2;
	add.s32 	%r7, %r17, %r46;
	mov.u32 	%r48, 2;
	mov.b32 	%r49, %f220;
	mov.u32 	%r50, 31;
	mov.u32 	%r51, 16;
	mov.u32 	%r52, -1;
	shfl.sync.bfly.b32 	%r53|%p4, %r49, %r51, %r50, %r52;
	mov.b32 	%f59, %r53;
	add.f32 	%f60, %f220, %f59;
	mov.b32 	%r54, %f60;
	mov.u32 	%r55, 8;
	shfl.sync.bfly.b32 	%r56|%p5, %r54, %r55, %r50, %r52;
	mov.b32 	%f61, %r56;
	add.f32 	%f62, %f60, %f61;
	mov.b32 	%r57, %f62;
	mov.u32 	%r58, 4;
	shfl.sync.bfly.b32 	%r59|%p6, %r57, %r58, %r50, %r52;
	mov.b32 	%f63, %r59;
	add.f32 	%f64, %f62, %f63;
	mov.b32 	%r60, %f64;
	shfl.sync.bfly.b32 	%r61|%p7, %r60, %r48, %r50, %r52;
	mov.b32 	%f65, %r61;
	add.f32 	%f66, %f64, %f65;
	mov.b32 	%r62, %f66;
	mov.u32 	%r63, 1;
	shfl.sync.bfly.b32 	%r64|%p8, %r62, %r63, %r50, %r52;
	mov.b32 	%f67, %r64;
	add.f32 	%f68, %f66, %f67;
	st.local.f32 	[%rd1], %f68;
	st.shared.f32 	[%r7], %f68;
	bar.sync 	0;
	@%p1 bra 	$L__BB102_8;

	ld.shared.f32 	%f69, [%r3];
	mov.b32 	%r65, %f69;
	shfl.sync.bfly.b32 	%r69|%p10, %r65, %r51, %r50, %r52;
	mov.b32 	%f70, %r69;
	add.f32 	%f71, %f69, %f70;
	mov.b32 	%r70, %f71;
	shfl.sync.bfly.b32 	%r72|%p11, %r70, %r55, %r50, %r52;
	mov.b32 	%f72, %r72;
	add.f32 	%f73, %f71, %f72;
	mov.b32 	%r73, %f73;
	shfl.sync.bfly.b32 	%r75|%p12, %r73, %r58, %r50, %r52;
	mov.b32 	%f74, %r75;
	add.f32 	%f75, %f73, %f74;
	mov.b32 	%r76, %f75;
	shfl.sync.bfly.b32 	%r78|%p13, %r76, %r48, %r50, %r52;
	mov.b32 	%f76, %r78;
	add.f32 	%f77, %f75, %f76;
	mov.b32 	%r79, %f77;
	shfl.sync.bfly.b32 	%r81|%p14, %r79, %r63, %r50, %r52;
	mov.b32 	%f78, %r81;
	add.f32 	%f79, %f77, %f78;
	st.local.f32 	[%rd1], %f79;

$L__BB102_8:
	bar.sync 	0;
	mov.b32 	%r82, %f219;
	shfl.sync.bfly.b32 	%r86|%p16, %r82, %r51, %r50, %r52;
	mov.b32 	%f80, %r86;
	add.f32 	%f81, %f219, %f80;
	mov.b32 	%r87, %f81;
	shfl.sync.bfly.b32 	%r89|%p17, %r87, %r55, %r50, %r52;
	mov.b32 	%f82, %r89;
	add.f32 	%f83, %f81, %f82;
	mov.b32 	%r90, %f83;
	shfl.sync.bfly.b32 	%r92|%p18, %r90, %r58, %r50, %r52;
	mov.b32 	%f84, %r92;
	add.f32 	%f85, %f83, %f84;
	mov.b32 	%r93, %f85;
	shfl.sync.bfly.b32 	%r95|%p19, %r93, %r48, %r50, %r52;
	mov.b32 	%f86, %r95;
	add.f32 	%f87, %f85, %f86;
	mov.b32 	%r96, %f87;
	shfl.sync.bfly.b32 	%r98|%p20, %r96, %r63, %r50, %r52;
	mov.b32 	%f88, %r98;
	add.f32 	%f89, %f87, %f88;
	st.local.f32 	[%rd1+4], %f89;
	st.shared.f32 	[%r7], %f89;
	bar.sync 	0;
	@%p1 bra 	$L__BB102_10;

	ld.shared.f32 	%f90, [%r3];
	mov.b32 	%r99, %f90;
	mov.u32 	%r100, 31;
	mov.u32 	%r101, 16;
	mov.u32 	%r102, -1;
	shfl.sync.bfly.b32 	%r103|%p21, %r99, %r101, %r100, %r102;
	mov.b32 	%f91, %r103;
	add.f32 	%f92, %f90, %f91;
	mov.b32 	%r104, %f92;
	mov.u32 	%r105, 8;
	shfl.sync.bfly.b32 	%r106|%p22, %r104, %r105, %r100, %r102;
	mov.b32 	%f93, %r106;
	add.f32 	%f94, %f92, %f93;
	mov.b32 	%r107, %f94;
	mov.u32 	%r108, 4;
	shfl.sync.bfly.b32 	%r109|%p23, %r107, %r108, %r100, %r102;
	mov.b32 	%f95, %r109;
	add.f32 	%f96, %f94, %f95;
	mov.b32 	%r110, %f96;
	mov.u32 	%r111, 2;
	shfl.sync.bfly.b32 	%r112|%p24, %r110, %r111, %r100, %r102;
	mov.b32 	%f97, %r112;
	add.f32 	%f98, %f96, %f97;
	mov.b32 	%r113, %f98;
	mov.u32 	%r114, 1;
	shfl.sync.bfly.b32 	%r115|%p25, %r113, %r114, %r100, %r102;
	mov.b32 	%f99, %r115;
	add.f32 	%f100, %f98, %f99;
	st.local.f32 	[%rd1+4], %f100;

$L__BB102_10:
	bar.sync 	0;
	mov.b32 	%r116, %f218;
	mov.u32 	%r117, 31;
	mov.u32 	%r118, 16;
	mov.u32 	%r119, -1;
	shfl.sync.bfly.b32 	%r120|%p27, %r116, %r118, %r117, %r119;
	mov.b32 	%f101, %r120;
	add.f32 	%f102, %f218, %f101;
	mov.b32 	%r121, %f102;
	mov.u32 	%r122, 8;
	shfl.sync.bfly.b32 	%r123|%p28, %r121, %r122, %r117, %r119;
	mov.b32 	%f103, %r123;
	add.f32 	%f104, %f102, %f103;
	mov.b32 	%r124, %f104;
	mov.u32 	%r125, 4;
	shfl.sync.bfly.b32 	%r126|%p29, %r124, %r125, %r117, %r119;
	mov.b32 	%f105, %r126;
	add.f32 	%f106, %f104, %f105;
	mov.b32 	%r127, %f106;
	mov.u32 	%r128, 2;
	shfl.sync.bfly.b32 	%r129|%p30, %r127, %r128, %r117, %r119;
	mov.b32 	%f107, %r129;
	add.f32 	%f108, %f106, %f107;
	mov.b32 	%r130, %f108;
	mov.u32 	%r131, 1;
	shfl.sync.bfly.b32 	%r132|%p31, %r130, %r131, %r117, %r119;
	mov.b32 	%f109, %r132;
	add.f32 	%f110, %f108, %f109;
	st.local.f32 	[%rd1+8], %f110;
	st.shared.f32 	[%r7], %f110;
	bar.sync 	0;
	@%p1 bra 	$L__BB102_12;

	ld.shared.f32 	%f111, [%r3];
	mov.b32 	%r133, %f111;
	shfl.sync.bfly.b32 	%r137|%p32, %r133, %r118, %r117, %r119;
	mov.b32 	%f112, %r137;
	add.f32 	%f113, %f111, %f112;
	mov.b32 	%r138, %f113;
	shfl.sync.bfly.b32 	%r140|%p33, %r138, %r122, %r117, %r119;
	mov.b32 	%f114, %r140;
	add.f32 	%f115, %f113, %f114;
	mov.b32 	%r141, %f115;
	shfl.sync.bfly.b32 	%r143|%p34, %r141, %r125, %r117, %r119;
	mov.b32 	%f116, %r143;
	add.f32 	%f117, %f115, %f116;
	mov.b32 	%r144, %f117;
	shfl.sync.bfly.b32 	%r146|%p35, %r144, %r128, %r117, %r119;
	mov.b32 	%f118, %r146;
	add.f32 	%f119, %f117, %f118;
	mov.b32 	%r147, %f119;
	shfl.sync.bfly.b32 	%r149|%p36, %r147, %r131, %r117, %r119;
	mov.b32 	%f120, %r149;
	add.f32 	%f121, %f119, %f120;
	st.local.f32 	[%rd1+8], %f121;

$L__BB102_12:
	bar.sync 	0;
	mov.b32 	%r150, %f217;
	shfl.sync.bfly.b32 	%r154|%p38, %r150, %r118, %r117, %r119;
	mov.b32 	%f122, %r154;
	add.f32 	%f123, %f217, %f122;
	mov.b32 	%r155, %f123;
	shfl.sync.bfly.b32 	%r157|%p39, %r155, %r122, %r117, %r119;
	mov.b32 	%f124, %r157;
	add.f32 	%f125, %f123, %f124;
	mov.b32 	%r158, %f125;
	shfl.sync.bfly.b32 	%r160|%p40, %r158, %r125, %r117, %r119;
	mov.b32 	%f126, %r160;
	add.f32 	%f127, %f125, %f126;
	mov.b32 	%r161, %f127;
	shfl.sync.bfly.b32 	%r163|%p41, %r161, %r128, %r117, %r119;
	mov.b32 	%f128, %r163;
	add.f32 	%f129, %f127, %f128;
	mov.b32 	%r164, %f129;
	shfl.sync.bfly.b32 	%r166|%p42, %r164, %r131, %r117, %r119;
	mov.b32 	%f130, %r166;
	add.f32 	%f131, %f129, %f130;
	st.local.f32 	[%rd1+12], %f131;
	st.shared.f32 	[%r7], %f131;
	bar.sync 	0;
	@%p1 bra 	$L__BB102_14;

	ld.shared.f32 	%f132, [%r3];
	mov.b32 	%r167, %f132;
	mov.u32 	%r168, 31;
	mov.u32 	%r169, 16;
	mov.u32 	%r170, -1;
	shfl.sync.bfly.b32 	%r171|%p43, %r167, %r169, %r168, %r170;
	mov.b32 	%f133, %r171;
	add.f32 	%f134, %f132, %f133;
	mov.b32 	%r172, %f134;
	mov.u32 	%r173, 8;
	shfl.sync.bfly.b32 	%r174|%p44, %r172, %r173, %r168, %r170;
	mov.b32 	%f135, %r174;
	add.f32 	%f136, %f134, %f135;
	mov.b32 	%r175, %f136;
	mov.u32 	%r176, 4;
	shfl.sync.bfly.b32 	%r177|%p45, %r175, %r176, %r168, %r170;
	mov.b32 	%f137, %r177;
	add.f32 	%f138, %f136, %f137;
	mov.b32 	%r178, %f138;
	mov.u32 	%r179, 2;
	shfl.sync.bfly.b32 	%r180|%p46, %r178, %r179, %r168, %r170;
	mov.b32 	%f139, %r180;
	add.f32 	%f140, %f138, %f139;
	mov.b32 	%r181, %f140;
	mov.u32 	%r182, 1;
	shfl.sync.bfly.b32 	%r183|%p47, %r181, %r182, %r168, %r170;
	mov.b32 	%f141, %r183;
	add.f32 	%f142, %f140, %f141;
	st.local.f32 	[%rd1+12], %f142;

$L__BB102_14:
	bar.sync 	0;
	mov.b32 	%r184, %f216;
	mov.u32 	%r185, 31;
	mov.u32 	%r186, 16;
	mov.u32 	%r187, -1;
	shfl.sync.bfly.b32 	%r188|%p49, %r184, %r186, %r185, %r187;
	mov.b32 	%f143, %r188;
	add.f32 	%f144, %f216, %f143;
	mov.b32 	%r189, %f144;
	mov.u32 	%r190, 8;
	shfl.sync.bfly.b32 	%r191|%p50, %r189, %r190, %r185, %r187;
	mov.b32 	%f145, %r191;
	add.f32 	%f146, %f144, %f145;
	mov.b32 	%r192, %f146;
	mov.u32 	%r193, 4;
	shfl.sync.bfly.b32 	%r194|%p51, %r192, %r193, %r185, %r187;
	mov.b32 	%f147, %r194;
	add.f32 	%f148, %f146, %f147;
	mov.b32 	%r195, %f148;
	mov.u32 	%r196, 2;
	shfl.sync.bfly.b32 	%r197|%p52, %r195, %r196, %r185, %r187;
	mov.b32 	%f149, %r197;
	add.f32 	%f150, %f148, %f149;
	mov.b32 	%r198, %f150;
	mov.u32 	%r199, 1;
	shfl.sync.bfly.b32 	%r200|%p53, %r198, %r199, %r185, %r187;
	mov.b32 	%f151, %r200;
	add.f32 	%f152, %f150, %f151;
	st.local.f32 	[%rd1+16], %f152;
	st.shared.f32 	[%r7], %f152;
	bar.sync 	0;
	@%p1 bra 	$L__BB102_16;

	ld.shared.f32 	%f153, [%r3];
	mov.b32 	%r201, %f153;
	shfl.sync.bfly.b32 	%r205|%p54, %r201, %r186, %r185, %r187;
	mov.b32 	%f154, %r205;
	add.f32 	%f155, %f153, %f154;
	mov.b32 	%r206, %f155;
	shfl.sync.bfly.b32 	%r208|%p55, %r206, %r190, %r185, %r187;
	mov.b32 	%f156, %r208;
	add.f32 	%f157, %f155, %f156;
	mov.b32 	%r209, %f157;
	shfl.sync.bfly.b32 	%r211|%p56, %r209, %r193, %r185, %r187;
	mov.b32 	%f158, %r211;
	add.f32 	%f159, %f157, %f158;
	mov.b32 	%r212, %f159;
	shfl.sync.bfly.b32 	%r214|%p57, %r212, %r196, %r185, %r187;
	mov.b32 	%f160, %r214;
	add.f32 	%f161, %f159, %f160;
	mov.b32 	%r215, %f161;
	shfl.sync.bfly.b32 	%r217|%p58, %r215, %r199, %r185, %r187;
	mov.b32 	%f162, %r217;
	add.f32 	%f163, %f161, %f162;
	st.local.f32 	[%rd1+16], %f163;

$L__BB102_16:
	bar.sync 	0;
	mov.b32 	%r218, %f215;
	shfl.sync.bfly.b32 	%r222|%p60, %r218, %r186, %r185, %r187;
	mov.b32 	%f164, %r222;
	add.f32 	%f165, %f215, %f164;
	mov.b32 	%r223, %f165;
	shfl.sync.bfly.b32 	%r225|%p61, %r223, %r190, %r185, %r187;
	mov.b32 	%f166, %r225;
	add.f32 	%f167, %f165, %f166;
	mov.b32 	%r226, %f167;
	shfl.sync.bfly.b32 	%r228|%p62, %r226, %r193, %r185, %r187;
	mov.b32 	%f168, %r228;
	add.f32 	%f169, %f167, %f168;
	mov.b32 	%r229, %f169;
	shfl.sync.bfly.b32 	%r231|%p63, %r229, %r196, %r185, %r187;
	mov.b32 	%f170, %r231;
	add.f32 	%f171, %f169, %f170;
	mov.b32 	%r232, %f171;
	shfl.sync.bfly.b32 	%r234|%p64, %r232, %r199, %r185, %r187;
	mov.b32 	%f172, %r234;
	add.f32 	%f173, %f171, %f172;
	st.local.f32 	[%rd1+20], %f173;
	st.shared.f32 	[%r7], %f173;
	bar.sync 	0;
	@%p1 bra 	$L__BB102_18;

	ld.shared.f32 	%f174, [%r3];
	mov.b32 	%r235, %f174;
	mov.u32 	%r236, 31;
	mov.u32 	%r237, 16;
	mov.u32 	%r238, -1;
	shfl.sync.bfly.b32 	%r239|%p65, %r235, %r237, %r236, %r238;
	mov.b32 	%f175, %r239;
	add.f32 	%f176, %f174, %f175;
	mov.b32 	%r240, %f176;
	mov.u32 	%r241, 8;
	shfl.sync.bfly.b32 	%r242|%p66, %r240, %r241, %r236, %r238;
	mov.b32 	%f177, %r242;
	add.f32 	%f178, %f176, %f177;
	mov.b32 	%r243, %f178;
	mov.u32 	%r244, 4;
	shfl.sync.bfly.b32 	%r245|%p67, %r243, %r244, %r236, %r238;
	mov.b32 	%f179, %r245;
	add.f32 	%f180, %f178, %f179;
	mov.b32 	%r246, %f180;
	mov.u32 	%r247, 2;
	shfl.sync.bfly.b32 	%r248|%p68, %r246, %r247, %r236, %r238;
	mov.b32 	%f181, %r248;
	add.f32 	%f182, %f180, %f181;
	mov.b32 	%r249, %f182;
	mov.u32 	%r250, 1;
	shfl.sync.bfly.b32 	%r251|%p69, %r249, %r250, %r236, %r238;
	mov.b32 	%f183, %r251;
	add.f32 	%f184, %f182, %f183;
	st.local.f32 	[%rd1+20], %f184;

$L__BB102_18:
	bar.sync 	0;
	mov.b32 	%r252, %f214;
	mov.u32 	%r253, 31;
	mov.u32 	%r254, 16;
	mov.u32 	%r255, -1;
	shfl.sync.bfly.b32 	%r256|%p71, %r252, %r254, %r253, %r255;
	mov.b32 	%f185, %r256;
	add.f32 	%f186, %f214, %f185;
	mov.b32 	%r257, %f186;
	mov.u32 	%r258, 8;
	shfl.sync.bfly.b32 	%r259|%p72, %r257, %r258, %r253, %r255;
	mov.b32 	%f187, %r259;
	add.f32 	%f188, %f186, %f187;
	mov.b32 	%r260, %f188;
	mov.u32 	%r261, 4;
	shfl.sync.bfly.b32 	%r262|%p73, %r260, %r261, %r253, %r255;
	mov.b32 	%f189, %r262;
	add.f32 	%f190, %f188, %f189;
	mov.b32 	%r263, %f190;
	mov.u32 	%r264, 2;
	shfl.sync.bfly.b32 	%r265|%p74, %r263, %r264, %r253, %r255;
	mov.b32 	%f191, %r265;
	add.f32 	%f192, %f190, %f191;
	mov.b32 	%r266, %f192;
	mov.u32 	%r267, 1;
	shfl.sync.bfly.b32 	%r268|%p75, %r266, %r267, %r253, %r255;
	mov.b32 	%f193, %r268;
	add.f32 	%f194, %f192, %f193;
	st.local.f32 	[%rd1+24], %f194;
	st.shared.f32 	[%r7], %f194;
	bar.sync 	0;
	@%p1 bra 	$L__BB102_20;

	ld.shared.f32 	%f195, [%r3];
	mov.b32 	%r269, %f195;
	shfl.sync.bfly.b32 	%r273|%p76, %r269, %r254, %r253, %r255;
	mov.b32 	%f196, %r273;
	add.f32 	%f197, %f195, %f196;
	mov.b32 	%r274, %f197;
	shfl.sync.bfly.b32 	%r276|%p77, %r274, %r258, %r253, %r255;
	mov.b32 	%f198, %r276;
	add.f32 	%f199, %f197, %f198;
	mov.b32 	%r277, %f199;
	shfl.sync.bfly.b32 	%r279|%p78, %r277, %r261, %r253, %r255;
	mov.b32 	%f200, %r279;
	add.f32 	%f201, %f199, %f200;
	mov.b32 	%r280, %f201;
	shfl.sync.bfly.b32 	%r282|%p79, %r280, %r264, %r253, %r255;
	mov.b32 	%f202, %r282;
	add.f32 	%f203, %f201, %f202;
	mov.b32 	%r283, %f203;
	shfl.sync.bfly.b32 	%r285|%p80, %r283, %r267, %r253, %r255;
	mov.b32 	%f204, %r285;
	add.f32 	%f205, %f203, %f204;
	st.local.f32 	[%rd1+24], %f205;

$L__BB102_20:
	bar.sync 	0;
	setp.gt.s32 	%p81, %r2, 6;
	@%p81 bra 	$L__BB102_22;

	mad.lo.s32 	%r286, %r2, %r11, %r1;
	cvt.s64.s32 	%rd33, %r286;
	mul.lo.s32 	%r287, %r4, %r15;
	cvt.s64.s32 	%rd34, %r287;
	add.s64 	%rd35, %rd34, %rd33;
	mul.wide.s32 	%rd36, %r2, 4;
	add.s64 	%rd37, %rd1, %rd36;
	ld.local.f32 	%f206, [%rd37];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f206;}

	// end inline asm
	cvta.to.global.u64 	%rd38, %rd15;
	shl.b64 	%rd39, %rd35, 1;
	add.s64 	%rd40, %rd38, %rd39;
	st.global.u16 	[%rd40], %rs1;

$L__BB102_22:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_8_bs_160
.visible .entry ggml_matvec_f16_ncols_8_bs_160(
	.param .u64 ggml_matvec_f16_ncols_8_bs_160_param_0,
	.param .u64 ggml_matvec_f16_ncols_8_bs_160_param_1,
	.param .u64 ggml_matvec_f16_ncols_8_bs_160_param_2,
	.param .u32 ggml_matvec_f16_ncols_8_bs_160_param_3,
	.param .u32 ggml_matvec_f16_ncols_8_bs_160_param_4,
	.param .u32 ggml_matvec_f16_ncols_8_bs_160_param_5,
	.param .u32 ggml_matvec_f16_ncols_8_bs_160_param_6,
	.param .u32 ggml_matvec_f16_ncols_8_bs_160_param_7,
	.param .u32 ggml_matvec_f16_ncols_8_bs_160_param_8,
	.param .u32 ggml_matvec_f16_ncols_8_bs_160_param_9,
	.param .u32 ggml_matvec_f16_ncols_8_bs_160_param_10,
	.param .u32 ggml_matvec_f16_ncols_8_bs_160_param_11
)
{
	.local .align 16 .b8 	__local_depot103[32];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<93>;
	.reg .b16 	%rs<2>;
	.reg .f32 	%f<252>;
	.reg .b32 	%r<324>;
	.reg .b64 	%rd<45>;


	mov.u64 	%SPL, __local_depot103;
	ld.param.u64 	%rd14, [ggml_matvec_f16_ncols_8_bs_160_param_0];
	ld.param.u64 	%rd15, [ggml_matvec_f16_ncols_8_bs_160_param_1];
	ld.param.u64 	%rd16, [ggml_matvec_f16_ncols_8_bs_160_param_2];
	ld.param.u32 	%r8, [ggml_matvec_f16_ncols_8_bs_160_param_3];
	ld.param.u32 	%r9, [ggml_matvec_f16_ncols_8_bs_160_param_5];
	ld.param.u32 	%r10, [ggml_matvec_f16_ncols_8_bs_160_param_6];
	ld.param.u32 	%r11, [ggml_matvec_f16_ncols_8_bs_160_param_7];
	ld.param.u32 	%r12, [ggml_matvec_f16_ncols_8_bs_160_param_8];
	ld.param.u32 	%r13, [ggml_matvec_f16_ncols_8_bs_160_param_9];
	ld.param.u32 	%r14, [ggml_matvec_f16_ncols_8_bs_160_param_10];
	ld.param.u32 	%r15, [ggml_matvec_f16_ncols_8_bs_160_param_11];
	add.u64 	%rd1, %SPL, 0;
	mov.u32 	%r1, %ctaid.x;
	mov.u32 	%r2, %tid.x;
	setp.gt.s32 	%p1, %r2, 31;
	shl.b32 	%r16, %r2, 2;
	mov.u32 	%r17, data_mmv;
	add.s32 	%r3, %r17, %r16;
	@%p1 bra 	$L__BB103_2;

	mov.u32 	%r18, 0;
	st.shared.u32 	[%r3], %r18;

$L__BB103_2:
	mov.u32 	%r4, %ctaid.y;
	bar.sync 	0;
	mov.f32 	%f244, 0f00000000;
	st.local.v4.f32 	[%rd1], {%f244, %f244, %f244, %f244};
	st.local.v4.f32 	[%rd1+16], {%f244, %f244, %f244, %f244};
	setp.ge.s32 	%p2, %r2, %r8;
	mov.f32 	%f245, %f244;
	mov.f32 	%f246, %f244;
	mov.f32 	%f247, %f244;
	mov.f32 	%f248, %f244;
	mov.f32 	%f249, %f244;
	mov.f32 	%f250, %f244;
	mov.f32 	%f251, %f244;
	@%p2 bra 	$L__BB103_6;

	shl.b32 	%r19, %r10, 1;
	add.s32 	%r20, %r2, %r19;
	mul.wide.s32 	%rd18, %r20, 4;
	mul.lo.s32 	%r21, %r4, %r14;
	mul.wide.s32 	%rd19, %r21, 2;
	add.s64 	%rd4, %rd18, %rd19;
	mul.wide.s32 	%rd20, %r2, 4;
	mul.wide.s32 	%rd5, %r10, 4;
	add.s64 	%rd21, %rd20, %rd5;
	add.s64 	%rd6, %rd21, %rd19;
	add.s64 	%rd7, %rd20, %rd19;
	mul.wide.s32 	%rd22, %r2, 2;
	div.s32 	%r22, %r4, %r12;
	mul.lo.s32 	%r23, %r1, %r9;
	mad.lo.s32 	%r24, %r22, %r13, %r23;
	cvt.s64.s32 	%rd23, %r24;
	add.s64 	%rd24, %rd22, %rd23;
	cvta.to.global.u64 	%rd25, %rd14;
	shl.b64 	%rd26, %rd24, 1;
	add.s64 	%rd43, %rd25, %rd26;
	cvta.to.global.u64 	%rd44, %rd15;
	mov.f32 	%f244, 0f00000000;
	mov.u32 	%r323, %r2;

$L__BB103_4:
	ld.global.nc.u32 	%r25, [%rd43];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r25;
  cvt.f32.f16 %f41, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r25;
  cvt.f32.f16 %f42, high;}

	// end inline asm
	add.s64 	%rd27, %rd44, %rd7;
	ld.global.nc.u32 	%r27, [%rd27];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r27;
  cvt.f32.f16 %f43, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r27;
  cvt.f32.f16 %f44, high;}

	// end inline asm
	fma.rn.f32 	%f59, %f41, %f43, %f251;
	fma.rn.f32 	%f251, %f42, %f44, %f59;
	add.s64 	%rd28, %rd44, %rd6;
	ld.global.nc.u32 	%r29, [%rd28];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r29;
  cvt.f32.f16 %f45, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r29;
  cvt.f32.f16 %f46, high;}

	// end inline asm
	fma.rn.f32 	%f60, %f41, %f45, %f250;
	fma.rn.f32 	%f250, %f42, %f46, %f60;
	add.s64 	%rd29, %rd44, %rd4;
	ld.global.nc.u32 	%r31, [%rd29];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r31;
  cvt.f32.f16 %f47, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r31;
  cvt.f32.f16 %f48, high;}

	// end inline asm
	fma.rn.f32 	%f61, %f41, %f47, %f249;
	fma.rn.f32 	%f249, %f42, %f48, %f61;
	add.s64 	%rd30, %rd29, %rd5;
	ld.global.nc.u32 	%r33, [%rd30];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r33;
  cvt.f32.f16 %f49, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r33;
  cvt.f32.f16 %f50, high;}

	// end inline asm
	fma.rn.f32 	%f62, %f41, %f49, %f248;
	fma.rn.f32 	%f248, %f42, %f50, %f62;
	add.s64 	%rd31, %rd30, %rd5;
	ld.global.nc.u32 	%r35, [%rd31];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r35;
  cvt.f32.f16 %f51, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r35;
  cvt.f32.f16 %f52, high;}

	// end inline asm
	fma.rn.f32 	%f63, %f41, %f51, %f247;
	fma.rn.f32 	%f247, %f42, %f52, %f63;
	add.s64 	%rd32, %rd31, %rd5;
	ld.global.nc.u32 	%r37, [%rd32];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r37;
  cvt.f32.f16 %f53, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r37;
  cvt.f32.f16 %f54, high;}

	// end inline asm
	fma.rn.f32 	%f64, %f41, %f53, %f246;
	fma.rn.f32 	%f246, %f42, %f54, %f64;
	add.s64 	%rd33, %rd32, %rd5;
	ld.global.nc.u32 	%r39, [%rd33];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r39;
  cvt.f32.f16 %f55, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r39;
  cvt.f32.f16 %f56, high;}

	// end inline asm
	fma.rn.f32 	%f65, %f41, %f55, %f245;
	fma.rn.f32 	%f245, %f42, %f56, %f65;
	add.s64 	%rd34, %rd33, %rd5;
	ld.global.nc.u32 	%r41, [%rd34];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r41;
  cvt.f32.f16 %f57, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r41;
  cvt.f32.f16 %f58, high;}

	// end inline asm
	fma.rn.f32 	%f66, %f41, %f57, %f244;
	fma.rn.f32 	%f244, %f42, %f58, %f66;
	add.s64 	%rd44, %rd44, 640;
	add.s64 	%rd43, %rd43, 640;
	add.s32 	%r323, %r323, 160;
	setp.lt.s32 	%p3, %r323, %r8;
	@%p3 bra 	$L__BB103_4;

	st.local.v4.f32 	[%rd1], {%f251, %f250, %f249, %f248};
	st.local.v4.f32 	[%rd1+16], {%f247, %f246, %f245, %f244};

$L__BB103_6:
	shr.s32 	%r43, %r2, 31;
	shr.u32 	%r44, %r43, 27;
	add.s32 	%r45, %r2, %r44;
	shr.s32 	%r46, %r45, 5;
	shl.b32 	%r47, %r46, 2;
	add.s32 	%r7, %r17, %r47;
	mov.u32 	%r49, 2;
	mov.b32 	%r50, %f251;
	mov.u32 	%r51, 31;
	mov.u32 	%r52, 16;
	mov.u32 	%r53, -1;
	shfl.sync.bfly.b32 	%r54|%p4, %r50, %r52, %r51, %r53;
	mov.b32 	%f67, %r54;
	add.f32 	%f68, %f251, %f67;
	mov.b32 	%r55, %f68;
	mov.u32 	%r56, 8;
	shfl.sync.bfly.b32 	%r57|%p5, %r55, %r56, %r51, %r53;
	mov.b32 	%f69, %r57;
	add.f32 	%f70, %f68, %f69;
	mov.b32 	%r58, %f70;
	mov.u32 	%r59, 4;
	shfl.sync.bfly.b32 	%r60|%p6, %r58, %r59, %r51, %r53;
	mov.b32 	%f71, %r60;
	add.f32 	%f72, %f70, %f71;
	mov.b32 	%r61, %f72;
	shfl.sync.bfly.b32 	%r62|%p7, %r61, %r49, %r51, %r53;
	mov.b32 	%f73, %r62;
	add.f32 	%f74, %f72, %f73;
	mov.b32 	%r63, %f74;
	mov.u32 	%r64, 1;
	shfl.sync.bfly.b32 	%r65|%p8, %r63, %r64, %r51, %r53;
	mov.b32 	%f75, %r65;
	add.f32 	%f76, %f74, %f75;
	st.local.f32 	[%rd1], %f76;
	st.shared.f32 	[%r7], %f76;
	bar.sync 	0;
	@%p1 bra 	$L__BB103_8;

	ld.shared.f32 	%f77, [%r3];
	mov.b32 	%r66, %f77;
	shfl.sync.bfly.b32 	%r70|%p10, %r66, %r52, %r51, %r53;
	mov.b32 	%f78, %r70;
	add.f32 	%f79, %f77, %f78;
	mov.b32 	%r71, %f79;
	shfl.sync.bfly.b32 	%r73|%p11, %r71, %r56, %r51, %r53;
	mov.b32 	%f80, %r73;
	add.f32 	%f81, %f79, %f80;
	mov.b32 	%r74, %f81;
	shfl.sync.bfly.b32 	%r76|%p12, %r74, %r59, %r51, %r53;
	mov.b32 	%f82, %r76;
	add.f32 	%f83, %f81, %f82;
	mov.b32 	%r77, %f83;
	shfl.sync.bfly.b32 	%r79|%p13, %r77, %r49, %r51, %r53;
	mov.b32 	%f84, %r79;
	add.f32 	%f85, %f83, %f84;
	mov.b32 	%r80, %f85;
	shfl.sync.bfly.b32 	%r82|%p14, %r80, %r64, %r51, %r53;
	mov.b32 	%f86, %r82;
	add.f32 	%f87, %f85, %f86;
	st.local.f32 	[%rd1], %f87;

$L__BB103_8:
	bar.sync 	0;
	mov.b32 	%r83, %f250;
	shfl.sync.bfly.b32 	%r87|%p16, %r83, %r52, %r51, %r53;
	mov.b32 	%f88, %r87;
	add.f32 	%f89, %f250, %f88;
	mov.b32 	%r88, %f89;
	shfl.sync.bfly.b32 	%r90|%p17, %r88, %r56, %r51, %r53;
	mov.b32 	%f90, %r90;
	add.f32 	%f91, %f89, %f90;
	mov.b32 	%r91, %f91;
	shfl.sync.bfly.b32 	%r93|%p18, %r91, %r59, %r51, %r53;
	mov.b32 	%f92, %r93;
	add.f32 	%f93, %f91, %f92;
	mov.b32 	%r94, %f93;
	shfl.sync.bfly.b32 	%r96|%p19, %r94, %r49, %r51, %r53;
	mov.b32 	%f94, %r96;
	add.f32 	%f95, %f93, %f94;
	mov.b32 	%r97, %f95;
	shfl.sync.bfly.b32 	%r99|%p20, %r97, %r64, %r51, %r53;
	mov.b32 	%f96, %r99;
	add.f32 	%f97, %f95, %f96;
	st.local.f32 	[%rd1+4], %f97;
	st.shared.f32 	[%r7], %f97;
	bar.sync 	0;
	@%p1 bra 	$L__BB103_10;

	ld.shared.f32 	%f98, [%r3];
	mov.b32 	%r100, %f98;
	mov.u32 	%r101, 31;
	mov.u32 	%r102, 16;
	mov.u32 	%r103, -1;
	shfl.sync.bfly.b32 	%r104|%p21, %r100, %r102, %r101, %r103;
	mov.b32 	%f99, %r104;
	add.f32 	%f100, %f98, %f99;
	mov.b32 	%r105, %f100;
	mov.u32 	%r106, 8;
	shfl.sync.bfly.b32 	%r107|%p22, %r105, %r106, %r101, %r103;
	mov.b32 	%f101, %r107;
	add.f32 	%f102, %f100, %f101;
	mov.b32 	%r108, %f102;
	mov.u32 	%r109, 4;
	shfl.sync.bfly.b32 	%r110|%p23, %r108, %r109, %r101, %r103;
	mov.b32 	%f103, %r110;
	add.f32 	%f104, %f102, %f103;
	mov.b32 	%r111, %f104;
	mov.u32 	%r112, 2;
	shfl.sync.bfly.b32 	%r113|%p24, %r111, %r112, %r101, %r103;
	mov.b32 	%f105, %r113;
	add.f32 	%f106, %f104, %f105;
	mov.b32 	%r114, %f106;
	mov.u32 	%r115, 1;
	shfl.sync.bfly.b32 	%r116|%p25, %r114, %r115, %r101, %r103;
	mov.b32 	%f107, %r116;
	add.f32 	%f108, %f106, %f107;
	st.local.f32 	[%rd1+4], %f108;

$L__BB103_10:
	bar.sync 	0;
	mov.b32 	%r117, %f249;
	mov.u32 	%r118, 31;
	mov.u32 	%r119, 16;
	mov.u32 	%r120, -1;
	shfl.sync.bfly.b32 	%r121|%p27, %r117, %r119, %r118, %r120;
	mov.b32 	%f109, %r121;
	add.f32 	%f110, %f249, %f109;
	mov.b32 	%r122, %f110;
	mov.u32 	%r123, 8;
	shfl.sync.bfly.b32 	%r124|%p28, %r122, %r123, %r118, %r120;
	mov.b32 	%f111, %r124;
	add.f32 	%f112, %f110, %f111;
	mov.b32 	%r125, %f112;
	mov.u32 	%r126, 4;
	shfl.sync.bfly.b32 	%r127|%p29, %r125, %r126, %r118, %r120;
	mov.b32 	%f113, %r127;
	add.f32 	%f114, %f112, %f113;
	mov.b32 	%r128, %f114;
	mov.u32 	%r129, 2;
	shfl.sync.bfly.b32 	%r130|%p30, %r128, %r129, %r118, %r120;
	mov.b32 	%f115, %r130;
	add.f32 	%f116, %f114, %f115;
	mov.b32 	%r131, %f116;
	mov.u32 	%r132, 1;
	shfl.sync.bfly.b32 	%r133|%p31, %r131, %r132, %r118, %r120;
	mov.b32 	%f117, %r133;
	add.f32 	%f118, %f116, %f117;
	st.local.f32 	[%rd1+8], %f118;
	st.shared.f32 	[%r7], %f118;
	bar.sync 	0;
	@%p1 bra 	$L__BB103_12;

	ld.shared.f32 	%f119, [%r3];
	mov.b32 	%r134, %f119;
	shfl.sync.bfly.b32 	%r138|%p32, %r134, %r119, %r118, %r120;
	mov.b32 	%f120, %r138;
	add.f32 	%f121, %f119, %f120;
	mov.b32 	%r139, %f121;
	shfl.sync.bfly.b32 	%r141|%p33, %r139, %r123, %r118, %r120;
	mov.b32 	%f122, %r141;
	add.f32 	%f123, %f121, %f122;
	mov.b32 	%r142, %f123;
	shfl.sync.bfly.b32 	%r144|%p34, %r142, %r126, %r118, %r120;
	mov.b32 	%f124, %r144;
	add.f32 	%f125, %f123, %f124;
	mov.b32 	%r145, %f125;
	shfl.sync.bfly.b32 	%r147|%p35, %r145, %r129, %r118, %r120;
	mov.b32 	%f126, %r147;
	add.f32 	%f127, %f125, %f126;
	mov.b32 	%r148, %f127;
	shfl.sync.bfly.b32 	%r150|%p36, %r148, %r132, %r118, %r120;
	mov.b32 	%f128, %r150;
	add.f32 	%f129, %f127, %f128;
	st.local.f32 	[%rd1+8], %f129;

$L__BB103_12:
	bar.sync 	0;
	mov.b32 	%r151, %f248;
	shfl.sync.bfly.b32 	%r155|%p38, %r151, %r119, %r118, %r120;
	mov.b32 	%f130, %r155;
	add.f32 	%f131, %f248, %f130;
	mov.b32 	%r156, %f131;
	shfl.sync.bfly.b32 	%r158|%p39, %r156, %r123, %r118, %r120;
	mov.b32 	%f132, %r158;
	add.f32 	%f133, %f131, %f132;
	mov.b32 	%r159, %f133;
	shfl.sync.bfly.b32 	%r161|%p40, %r159, %r126, %r118, %r120;
	mov.b32 	%f134, %r161;
	add.f32 	%f135, %f133, %f134;
	mov.b32 	%r162, %f135;
	shfl.sync.bfly.b32 	%r164|%p41, %r162, %r129, %r118, %r120;
	mov.b32 	%f136, %r164;
	add.f32 	%f137, %f135, %f136;
	mov.b32 	%r165, %f137;
	shfl.sync.bfly.b32 	%r167|%p42, %r165, %r132, %r118, %r120;
	mov.b32 	%f138, %r167;
	add.f32 	%f139, %f137, %f138;
	st.local.f32 	[%rd1+12], %f139;
	st.shared.f32 	[%r7], %f139;
	bar.sync 	0;
	@%p1 bra 	$L__BB103_14;

	ld.shared.f32 	%f140, [%r3];
	mov.b32 	%r168, %f140;
	mov.u32 	%r169, 31;
	mov.u32 	%r170, 16;
	mov.u32 	%r171, -1;
	shfl.sync.bfly.b32 	%r172|%p43, %r168, %r170, %r169, %r171;
	mov.b32 	%f141, %r172;
	add.f32 	%f142, %f140, %f141;
	mov.b32 	%r173, %f142;
	mov.u32 	%r174, 8;
	shfl.sync.bfly.b32 	%r175|%p44, %r173, %r174, %r169, %r171;
	mov.b32 	%f143, %r175;
	add.f32 	%f144, %f142, %f143;
	mov.b32 	%r176, %f144;
	mov.u32 	%r177, 4;
	shfl.sync.bfly.b32 	%r178|%p45, %r176, %r177, %r169, %r171;
	mov.b32 	%f145, %r178;
	add.f32 	%f146, %f144, %f145;
	mov.b32 	%r179, %f146;
	mov.u32 	%r180, 2;
	shfl.sync.bfly.b32 	%r181|%p46, %r179, %r180, %r169, %r171;
	mov.b32 	%f147, %r181;
	add.f32 	%f148, %f146, %f147;
	mov.b32 	%r182, %f148;
	mov.u32 	%r183, 1;
	shfl.sync.bfly.b32 	%r184|%p47, %r182, %r183, %r169, %r171;
	mov.b32 	%f149, %r184;
	add.f32 	%f150, %f148, %f149;
	st.local.f32 	[%rd1+12], %f150;

$L__BB103_14:
	bar.sync 	0;
	mov.b32 	%r185, %f247;
	mov.u32 	%r186, 31;
	mov.u32 	%r187, 16;
	mov.u32 	%r188, -1;
	shfl.sync.bfly.b32 	%r189|%p49, %r185, %r187, %r186, %r188;
	mov.b32 	%f151, %r189;
	add.f32 	%f152, %f247, %f151;
	mov.b32 	%r190, %f152;
	mov.u32 	%r191, 8;
	shfl.sync.bfly.b32 	%r192|%p50, %r190, %r191, %r186, %r188;
	mov.b32 	%f153, %r192;
	add.f32 	%f154, %f152, %f153;
	mov.b32 	%r193, %f154;
	mov.u32 	%r194, 4;
	shfl.sync.bfly.b32 	%r195|%p51, %r193, %r194, %r186, %r188;
	mov.b32 	%f155, %r195;
	add.f32 	%f156, %f154, %f155;
	mov.b32 	%r196, %f156;
	mov.u32 	%r197, 2;
	shfl.sync.bfly.b32 	%r198|%p52, %r196, %r197, %r186, %r188;
	mov.b32 	%f157, %r198;
	add.f32 	%f158, %f156, %f157;
	mov.b32 	%r199, %f158;
	mov.u32 	%r200, 1;
	shfl.sync.bfly.b32 	%r201|%p53, %r199, %r200, %r186, %r188;
	mov.b32 	%f159, %r201;
	add.f32 	%f160, %f158, %f159;
	st.local.f32 	[%rd1+16], %f160;
	st.shared.f32 	[%r7], %f160;
	bar.sync 	0;
	@%p1 bra 	$L__BB103_16;

	ld.shared.f32 	%f161, [%r3];
	mov.b32 	%r202, %f161;
	shfl.sync.bfly.b32 	%r206|%p54, %r202, %r187, %r186, %r188;
	mov.b32 	%f162, %r206;
	add.f32 	%f163, %f161, %f162;
	mov.b32 	%r207, %f163;
	shfl.sync.bfly.b32 	%r209|%p55, %r207, %r191, %r186, %r188;
	mov.b32 	%f164, %r209;
	add.f32 	%f165, %f163, %f164;
	mov.b32 	%r210, %f165;
	shfl.sync.bfly.b32 	%r212|%p56, %r210, %r194, %r186, %r188;
	mov.b32 	%f166, %r212;
	add.f32 	%f167, %f165, %f166;
	mov.b32 	%r213, %f167;
	shfl.sync.bfly.b32 	%r215|%p57, %r213, %r197, %r186, %r188;
	mov.b32 	%f168, %r215;
	add.f32 	%f169, %f167, %f168;
	mov.b32 	%r216, %f169;
	shfl.sync.bfly.b32 	%r218|%p58, %r216, %r200, %r186, %r188;
	mov.b32 	%f170, %r218;
	add.f32 	%f171, %f169, %f170;
	st.local.f32 	[%rd1+16], %f171;

$L__BB103_16:
	bar.sync 	0;
	mov.b32 	%r219, %f246;
	shfl.sync.bfly.b32 	%r223|%p60, %r219, %r187, %r186, %r188;
	mov.b32 	%f172, %r223;
	add.f32 	%f173, %f246, %f172;
	mov.b32 	%r224, %f173;
	shfl.sync.bfly.b32 	%r226|%p61, %r224, %r191, %r186, %r188;
	mov.b32 	%f174, %r226;
	add.f32 	%f175, %f173, %f174;
	mov.b32 	%r227, %f175;
	shfl.sync.bfly.b32 	%r229|%p62, %r227, %r194, %r186, %r188;
	mov.b32 	%f176, %r229;
	add.f32 	%f177, %f175, %f176;
	mov.b32 	%r230, %f177;
	shfl.sync.bfly.b32 	%r232|%p63, %r230, %r197, %r186, %r188;
	mov.b32 	%f178, %r232;
	add.f32 	%f179, %f177, %f178;
	mov.b32 	%r233, %f179;
	shfl.sync.bfly.b32 	%r235|%p64, %r233, %r200, %r186, %r188;
	mov.b32 	%f180, %r235;
	add.f32 	%f181, %f179, %f180;
	st.local.f32 	[%rd1+20], %f181;
	st.shared.f32 	[%r7], %f181;
	bar.sync 	0;
	@%p1 bra 	$L__BB103_18;

	ld.shared.f32 	%f182, [%r3];
	mov.b32 	%r236, %f182;
	mov.u32 	%r237, 31;
	mov.u32 	%r238, 16;
	mov.u32 	%r239, -1;
	shfl.sync.bfly.b32 	%r240|%p65, %r236, %r238, %r237, %r239;
	mov.b32 	%f183, %r240;
	add.f32 	%f184, %f182, %f183;
	mov.b32 	%r241, %f184;
	mov.u32 	%r242, 8;
	shfl.sync.bfly.b32 	%r243|%p66, %r241, %r242, %r237, %r239;
	mov.b32 	%f185, %r243;
	add.f32 	%f186, %f184, %f185;
	mov.b32 	%r244, %f186;
	mov.u32 	%r245, 4;
	shfl.sync.bfly.b32 	%r246|%p67, %r244, %r245, %r237, %r239;
	mov.b32 	%f187, %r246;
	add.f32 	%f188, %f186, %f187;
	mov.b32 	%r247, %f188;
	mov.u32 	%r248, 2;
	shfl.sync.bfly.b32 	%r249|%p68, %r247, %r248, %r237, %r239;
	mov.b32 	%f189, %r249;
	add.f32 	%f190, %f188, %f189;
	mov.b32 	%r250, %f190;
	mov.u32 	%r251, 1;
	shfl.sync.bfly.b32 	%r252|%p69, %r250, %r251, %r237, %r239;
	mov.b32 	%f191, %r252;
	add.f32 	%f192, %f190, %f191;
	st.local.f32 	[%rd1+20], %f192;

$L__BB103_18:
	bar.sync 	0;
	mov.b32 	%r253, %f245;
	mov.u32 	%r254, 31;
	mov.u32 	%r255, 16;
	mov.u32 	%r256, -1;
	shfl.sync.bfly.b32 	%r257|%p71, %r253, %r255, %r254, %r256;
	mov.b32 	%f193, %r257;
	add.f32 	%f194, %f245, %f193;
	mov.b32 	%r258, %f194;
	mov.u32 	%r259, 8;
	shfl.sync.bfly.b32 	%r260|%p72, %r258, %r259, %r254, %r256;
	mov.b32 	%f195, %r260;
	add.f32 	%f196, %f194, %f195;
	mov.b32 	%r261, %f196;
	mov.u32 	%r262, 4;
	shfl.sync.bfly.b32 	%r263|%p73, %r261, %r262, %r254, %r256;
	mov.b32 	%f197, %r263;
	add.f32 	%f198, %f196, %f197;
	mov.b32 	%r264, %f198;
	mov.u32 	%r265, 2;
	shfl.sync.bfly.b32 	%r266|%p74, %r264, %r265, %r254, %r256;
	mov.b32 	%f199, %r266;
	add.f32 	%f200, %f198, %f199;
	mov.b32 	%r267, %f200;
	mov.u32 	%r268, 1;
	shfl.sync.bfly.b32 	%r269|%p75, %r267, %r268, %r254, %r256;
	mov.b32 	%f201, %r269;
	add.f32 	%f202, %f200, %f201;
	st.local.f32 	[%rd1+24], %f202;
	st.shared.f32 	[%r7], %f202;
	bar.sync 	0;
	@%p1 bra 	$L__BB103_20;

	ld.shared.f32 	%f203, [%r3];
	mov.b32 	%r270, %f203;
	shfl.sync.bfly.b32 	%r274|%p76, %r270, %r255, %r254, %r256;
	mov.b32 	%f204, %r274;
	add.f32 	%f205, %f203, %f204;
	mov.b32 	%r275, %f205;
	shfl.sync.bfly.b32 	%r277|%p77, %r275, %r259, %r254, %r256;
	mov.b32 	%f206, %r277;
	add.f32 	%f207, %f205, %f206;
	mov.b32 	%r278, %f207;
	shfl.sync.bfly.b32 	%r280|%p78, %r278, %r262, %r254, %r256;
	mov.b32 	%f208, %r280;
	add.f32 	%f209, %f207, %f208;
	mov.b32 	%r281, %f209;
	shfl.sync.bfly.b32 	%r283|%p79, %r281, %r265, %r254, %r256;
	mov.b32 	%f210, %r283;
	add.f32 	%f211, %f209, %f210;
	mov.b32 	%r284, %f211;
	shfl.sync.bfly.b32 	%r286|%p80, %r284, %r268, %r254, %r256;
	mov.b32 	%f212, %r286;
	add.f32 	%f213, %f211, %f212;
	st.local.f32 	[%rd1+24], %f213;

$L__BB103_20:
	bar.sync 	0;
	mov.b32 	%r287, %f244;
	shfl.sync.bfly.b32 	%r291|%p82, %r287, %r255, %r254, %r256;
	mov.b32 	%f214, %r291;
	add.f32 	%f215, %f244, %f214;
	mov.b32 	%r292, %f215;
	shfl.sync.bfly.b32 	%r294|%p83, %r292, %r259, %r254, %r256;
	mov.b32 	%f216, %r294;
	add.f32 	%f217, %f215, %f216;
	mov.b32 	%r295, %f217;
	shfl.sync.bfly.b32 	%r297|%p84, %r295, %r262, %r254, %r256;
	mov.b32 	%f218, %r297;
	add.f32 	%f219, %f217, %f218;
	mov.b32 	%r298, %f219;
	shfl.sync.bfly.b32 	%r300|%p85, %r298, %r265, %r254, %r256;
	mov.b32 	%f220, %r300;
	add.f32 	%f221, %f219, %f220;
	mov.b32 	%r301, %f221;
	shfl.sync.bfly.b32 	%r303|%p86, %r301, %r268, %r254, %r256;
	mov.b32 	%f222, %r303;
	add.f32 	%f223, %f221, %f222;
	st.local.f32 	[%rd1+28], %f223;
	st.shared.f32 	[%r7], %f223;
	bar.sync 	0;
	@%p1 bra 	$L__BB103_22;

	ld.shared.f32 	%f224, [%r3];
	mov.b32 	%r304, %f224;
	mov.u32 	%r305, 31;
	mov.u32 	%r306, 16;
	mov.u32 	%r307, -1;
	shfl.sync.bfly.b32 	%r308|%p87, %r304, %r306, %r305, %r307;
	mov.b32 	%f225, %r308;
	add.f32 	%f226, %f224, %f225;
	mov.b32 	%r309, %f226;
	mov.u32 	%r310, 8;
	shfl.sync.bfly.b32 	%r311|%p88, %r309, %r310, %r305, %r307;
	mov.b32 	%f227, %r311;
	add.f32 	%f228, %f226, %f227;
	mov.b32 	%r312, %f228;
	mov.u32 	%r313, 4;
	shfl.sync.bfly.b32 	%r314|%p89, %r312, %r313, %r305, %r307;
	mov.b32 	%f229, %r314;
	add.f32 	%f230, %f228, %f229;
	mov.b32 	%r315, %f230;
	mov.u32 	%r316, 2;
	shfl.sync.bfly.b32 	%r317|%p90, %r315, %r316, %r305, %r307;
	mov.b32 	%f231, %r317;
	add.f32 	%f232, %f230, %f231;
	mov.b32 	%r318, %f232;
	mov.u32 	%r319, 1;
	shfl.sync.bfly.b32 	%r320|%p91, %r318, %r319, %r305, %r307;
	mov.b32 	%f233, %r320;
	add.f32 	%f234, %f232, %f233;
	st.local.f32 	[%rd1+28], %f234;

$L__BB103_22:
	bar.sync 	0;
	setp.gt.s32 	%p92, %r2, 7;
	@%p92 bra 	$L__BB103_24;

	mad.lo.s32 	%r321, %r2, %r11, %r1;
	cvt.s64.s32 	%rd35, %r321;
	mul.lo.s32 	%r322, %r4, %r15;
	cvt.s64.s32 	%rd36, %r322;
	add.s64 	%rd37, %rd36, %rd35;
	mul.wide.s32 	%rd38, %r2, 4;
	add.s64 	%rd39, %rd1, %rd38;
	ld.local.f32 	%f235, [%rd39];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f235;}

	// end inline asm
	cvta.to.global.u64 	%rd40, %rd16;
	shl.b64 	%rd41, %rd37, 1;
	add.s64 	%rd42, %rd40, %rd41;
	st.global.u16 	[%rd42], %rs1;

$L__BB103_24:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_1_bs_192
.visible .entry ggml_matvec_f16_ncols_1_bs_192(
	.param .u64 ggml_matvec_f16_ncols_1_bs_192_param_0,
	.param .u64 ggml_matvec_f16_ncols_1_bs_192_param_1,
	.param .u64 ggml_matvec_f16_ncols_1_bs_192_param_2,
	.param .u32 ggml_matvec_f16_ncols_1_bs_192_param_3,
	.param .u32 ggml_matvec_f16_ncols_1_bs_192_param_4,
	.param .u32 ggml_matvec_f16_ncols_1_bs_192_param_5,
	.param .u32 ggml_matvec_f16_ncols_1_bs_192_param_6,
	.param .u32 ggml_matvec_f16_ncols_1_bs_192_param_7,
	.param .u32 ggml_matvec_f16_ncols_1_bs_192_param_8,
	.param .u32 ggml_matvec_f16_ncols_1_bs_192_param_9,
	.param .u32 ggml_matvec_f16_ncols_1_bs_192_param_10,
	.param .u32 ggml_matvec_f16_ncols_1_bs_192_param_11
)
{
	.reg .pred 	%p<19>;
	.reg .b16 	%rs<2>;
	.reg .f32 	%f<69>;
	.reg .b32 	%r<99>;
	.reg .b64 	%rd<44>;


	ld.param.u64 	%rd18, [ggml_matvec_f16_ncols_1_bs_192_param_0];
	ld.param.u64 	%rd19, [ggml_matvec_f16_ncols_1_bs_192_param_1];
	ld.param.u64 	%rd17, [ggml_matvec_f16_ncols_1_bs_192_param_2];
	ld.param.u32 	%r13, [ggml_matvec_f16_ncols_1_bs_192_param_3];
	ld.param.u32 	%r16, [ggml_matvec_f16_ncols_1_bs_192_param_5];
	ld.param.u32 	%r14, [ggml_matvec_f16_ncols_1_bs_192_param_7];
	ld.param.u32 	%r17, [ggml_matvec_f16_ncols_1_bs_192_param_8];
	ld.param.u32 	%r18, [ggml_matvec_f16_ncols_1_bs_192_param_9];
	ld.param.u32 	%r19, [ggml_matvec_f16_ncols_1_bs_192_param_10];
	ld.param.u32 	%r15, [ggml_matvec_f16_ncols_1_bs_192_param_11];
	cvta.to.global.u64 	%rd1, %rd19;
	cvta.to.global.u64 	%rd2, %rd18;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r20, %r1, %r17;
	mov.u32 	%r21, %ctaid.x;
	mul.lo.s32 	%r22, %r21, %r16;
	mad.lo.s32 	%r23, %r20, %r18, %r22;
	cvt.s64.s32 	%rd3, %r23;
	mul.lo.s32 	%r24, %r1, %r19;
	cvt.s64.s32 	%rd4, %r24;
	mov.u32 	%r2, %tid.x;
	setp.gt.s32 	%p1, %r2, 31;
	shl.b32 	%r25, %r2, 2;
	mov.u32 	%r26, data_mmv;
	add.s32 	%r3, %r26, %r25;
	@%p1 bra 	$L__BB104_2;

	mov.u32 	%r27, 0;
	st.shared.u32 	[%r3], %r27;

$L__BB104_2:
	bar.sync 	0;
	setp.ge.s32 	%p2, %r2, %r13;
	mov.f32 	%f67, 0f00000000;
	@%p2 bra 	$L__BB104_9;

	not.b32 	%r28, %r2;
	add.s32 	%r4, %r28, %r13;
	mul.wide.u32 	%rd20, %r4, -1431655765;
	shr.u64 	%rd21, %rd20, 39;
	cvt.u32.u64 	%r29, %rd21;
	add.s32 	%r30, %r29, 1;
	and.b32  	%r96, %r30, 3;
	setp.eq.s32 	%p3, %r96, 0;
	mov.f32 	%f67, 0f00000000;
	mov.u32 	%r97, %r2;
	@%p3 bra 	$L__BB104_6;

	mul.wide.s32 	%rd22, %r2, 2;
	add.s64 	%rd23, %rd22, %rd4;
	shl.b64 	%rd24, %rd23, 1;
	add.s64 	%rd41, %rd1, %rd24;
	add.s64 	%rd25, %rd22, %rd3;
	shl.b64 	%rd26, %rd25, 1;
	add.s64 	%rd40, %rd2, %rd26;
	mov.f32 	%f67, 0f00000000;
	mov.u32 	%r97, %r2;

$L__BB104_5:
	.pragma "nounroll";
	ld.global.nc.u32 	%r31, [%rd40];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r31;
  cvt.f32.f16 %f15, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r31;
  cvt.f32.f16 %f16, high;}

	// end inline asm
	ld.global.nc.u32 	%r33, [%rd41];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r33;
  cvt.f32.f16 %f17, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r33;
  cvt.f32.f16 %f18, high;}

	// end inline asm
	fma.rn.f32 	%f19, %f15, %f17, %f67;
	fma.rn.f32 	%f67, %f16, %f18, %f19;
	add.s32 	%r97, %r97, 192;
	add.s64 	%rd41, %rd41, 768;
	add.s64 	%rd40, %rd40, 768;
	add.s32 	%r96, %r96, -1;
	setp.ne.s32 	%p4, %r96, 0;
	@%p4 bra 	$L__BB104_5;

$L__BB104_6:
	setp.lt.u32 	%p5, %r4, 576;
	@%p5 bra 	$L__BB104_9;

	mul.wide.s32 	%rd27, %r97, 2;
	add.s64 	%rd28, %rd27, %rd3;
	shl.b64 	%rd29, %rd28, 1;
	add.s64 	%rd30, %rd2, %rd29;
	add.s64 	%rd43, %rd30, 1536;
	add.s64 	%rd31, %rd27, %rd4;
	shl.b64 	%rd32, %rd31, 1;
	add.s64 	%rd33, %rd1, %rd32;
	add.s64 	%rd42, %rd33, 1536;

$L__BB104_8:
	ld.global.nc.u32 	%r35, [%rd43+-1536];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r35;
  cvt.f32.f16 %f20, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r35;
  cvt.f32.f16 %f21, high;}

	// end inline asm
	ld.global.nc.u32 	%r37, [%rd42+-1536];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r37;
  cvt.f32.f16 %f22, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r37;
  cvt.f32.f16 %f23, high;}

	// end inline asm
	fma.rn.f32 	%f36, %f20, %f22, %f67;
	fma.rn.f32 	%f37, %f21, %f23, %f36;
	ld.global.nc.u32 	%r39, [%rd43+-768];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r39;
  cvt.f32.f16 %f24, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r39;
  cvt.f32.f16 %f25, high;}

	// end inline asm
	ld.global.nc.u32 	%r41, [%rd42+-768];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r41;
  cvt.f32.f16 %f26, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r41;
  cvt.f32.f16 %f27, high;}

	// end inline asm
	fma.rn.f32 	%f38, %f24, %f26, %f37;
	fma.rn.f32 	%f39, %f25, %f27, %f38;
	ld.global.nc.u32 	%r43, [%rd43];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r43;
  cvt.f32.f16 %f28, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r43;
  cvt.f32.f16 %f29, high;}

	// end inline asm
	ld.global.nc.u32 	%r45, [%rd42];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r45;
  cvt.f32.f16 %f30, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r45;
  cvt.f32.f16 %f31, high;}

	// end inline asm
	fma.rn.f32 	%f40, %f28, %f30, %f39;
	fma.rn.f32 	%f41, %f29, %f31, %f40;
	ld.global.nc.u32 	%r47, [%rd43+768];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r47;
  cvt.f32.f16 %f32, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r47;
  cvt.f32.f16 %f33, high;}

	// end inline asm
	ld.global.nc.u32 	%r49, [%rd42+768];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r49;
  cvt.f32.f16 %f34, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r49;
  cvt.f32.f16 %f35, high;}

	// end inline asm
	fma.rn.f32 	%f42, %f32, %f34, %f41;
	fma.rn.f32 	%f67, %f33, %f35, %f42;
	add.s64 	%rd43, %rd43, 3072;
	add.s64 	%rd42, %rd42, 3072;
	add.s32 	%r97, %r97, 768;
	setp.lt.s32 	%p6, %r97, %r13;
	@%p6 bra 	$L__BB104_8;

$L__BB104_9:
	mov.b32 	%r51, %f67;
	mov.u32 	%r52, 31;
	mov.u32 	%r53, 16;
	mov.u32 	%r54, -1;
	shfl.sync.bfly.b32 	%r55|%p7, %r51, %r53, %r52, %r54;
	mov.b32 	%f43, %r55;
	add.f32 	%f44, %f67, %f43;
	mov.b32 	%r56, %f44;
	mov.u32 	%r57, 8;
	shfl.sync.bfly.b32 	%r58|%p8, %r56, %r57, %r52, %r54;
	mov.b32 	%f45, %r58;
	add.f32 	%f46, %f44, %f45;
	mov.b32 	%r59, %f46;
	mov.u32 	%r60, 4;
	shfl.sync.bfly.b32 	%r61|%p9, %r59, %r60, %r52, %r54;
	mov.b32 	%f47, %r61;
	add.f32 	%f48, %f46, %f47;
	mov.b32 	%r62, %f48;
	mov.u32 	%r63, 2;
	shfl.sync.bfly.b32 	%r64|%p10, %r62, %r63, %r52, %r54;
	mov.b32 	%f49, %r64;
	add.f32 	%f50, %f48, %f49;
	mov.b32 	%r65, %f50;
	mov.u32 	%r66, 1;
	shfl.sync.bfly.b32 	%r67|%p11, %r65, %r66, %r52, %r54;
	mov.b32 	%f51, %r67;
	add.f32 	%f68, %f50, %f51;
	shr.s32 	%r68, %r2, 31;
	shr.u32 	%r69, %r68, 27;
	add.s32 	%r70, %r2, %r69;
	shr.s32 	%r71, %r70, 5;
	shl.b32 	%r72, %r71, 2;
	add.s32 	%r74, %r26, %r72;
	st.shared.f32 	[%r74], %f68;
	bar.sync 	0;
	@%p1 bra 	$L__BB104_11;

	ld.shared.f32 	%f52, [%r3];
	mov.b32 	%r75, %f52;
	shfl.sync.bfly.b32 	%r79|%p13, %r75, %r53, %r52, %r54;
	mov.b32 	%f53, %r79;
	add.f32 	%f54, %f52, %f53;
	mov.b32 	%r80, %f54;
	shfl.sync.bfly.b32 	%r82|%p14, %r80, %r57, %r52, %r54;
	mov.b32 	%f55, %r82;
	add.f32 	%f56, %f54, %f55;
	mov.b32 	%r83, %f56;
	shfl.sync.bfly.b32 	%r85|%p15, %r83, %r60, %r52, %r54;
	mov.b32 	%f57, %r85;
	add.f32 	%f58, %f56, %f57;
	mov.b32 	%r86, %f58;
	shfl.sync.bfly.b32 	%r88|%p16, %r86, %r63, %r52, %r54;
	mov.b32 	%f59, %r88;
	add.f32 	%f60, %f58, %f59;
	mov.b32 	%r89, %f60;
	shfl.sync.bfly.b32 	%r91|%p17, %r89, %r66, %r52, %r54;
	mov.b32 	%f61, %r91;
	add.f32 	%f68, %f60, %f61;

$L__BB104_11:
	bar.sync 	0;
	setp.gt.s32 	%p18, %r2, 0;
	@%p18 bra 	$L__BB104_13;

	mad.lo.s32 	%r93, %r2, %r14, %r21;
	cvt.s64.s32 	%rd34, %r93;
	mul.lo.s32 	%r94, %r1, %r15;
	cvt.s64.s32 	%rd35, %r94;
	add.s64 	%rd36, %rd35, %rd34;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f68;}

	// end inline asm
	cvta.to.global.u64 	%rd37, %rd17;
	shl.b64 	%rd38, %rd36, 1;
	add.s64 	%rd39, %rd37, %rd38;
	st.global.u16 	[%rd39], %rs1;

$L__BB104_13:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_2_bs_192
.visible .entry ggml_matvec_f16_ncols_2_bs_192(
	.param .u64 ggml_matvec_f16_ncols_2_bs_192_param_0,
	.param .u64 ggml_matvec_f16_ncols_2_bs_192_param_1,
	.param .u64 ggml_matvec_f16_ncols_2_bs_192_param_2,
	.param .u32 ggml_matvec_f16_ncols_2_bs_192_param_3,
	.param .u32 ggml_matvec_f16_ncols_2_bs_192_param_4,
	.param .u32 ggml_matvec_f16_ncols_2_bs_192_param_5,
	.param .u32 ggml_matvec_f16_ncols_2_bs_192_param_6,
	.param .u32 ggml_matvec_f16_ncols_2_bs_192_param_7,
	.param .u32 ggml_matvec_f16_ncols_2_bs_192_param_8,
	.param .u32 ggml_matvec_f16_ncols_2_bs_192_param_9,
	.param .u32 ggml_matvec_f16_ncols_2_bs_192_param_10,
	.param .u32 ggml_matvec_f16_ncols_2_bs_192_param_11
)
{
	.local .align 8 .b8 	__local_depot105[8];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<30>;
	.reg .b16 	%rs<2>;
	.reg .f32 	%f<116>;
	.reg .b32 	%r<143>;
	.reg .b64 	%rd<66>;


	mov.u64 	%SPL, __local_depot105;
	ld.param.u64 	%rd27, [ggml_matvec_f16_ncols_2_bs_192_param_0];
	ld.param.u64 	%rd28, [ggml_matvec_f16_ncols_2_bs_192_param_1];
	ld.param.u64 	%rd26, [ggml_matvec_f16_ncols_2_bs_192_param_2];
	ld.param.u32 	%r15, [ggml_matvec_f16_ncols_2_bs_192_param_3];
	ld.param.u32 	%r19, [ggml_matvec_f16_ncols_2_bs_192_param_5];
	ld.param.u32 	%r16, [ggml_matvec_f16_ncols_2_bs_192_param_6];
	ld.param.u32 	%r17, [ggml_matvec_f16_ncols_2_bs_192_param_7];
	ld.param.u32 	%r20, [ggml_matvec_f16_ncols_2_bs_192_param_8];
	ld.param.u32 	%r21, [ggml_matvec_f16_ncols_2_bs_192_param_9];
	ld.param.u32 	%r22, [ggml_matvec_f16_ncols_2_bs_192_param_10];
	ld.param.u32 	%r18, [ggml_matvec_f16_ncols_2_bs_192_param_11];
	cvta.to.global.u64 	%rd1, %rd28;
	cvta.to.global.u64 	%rd2, %rd27;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r23, %r1, %r20;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r24, %r2, %r19;
	mad.lo.s32 	%r25, %r23, %r21, %r24;
	cvt.s64.s32 	%rd4, %r25;
	mul.lo.s32 	%r26, %r1, %r22;
	cvt.s64.s32 	%rd5, %r26;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r27, %r3, 2;
	mov.u32 	%r28, data_mmv;
	add.s32 	%r4, %r28, %r27;
	@%p1 bra 	$L__BB105_2;

	mov.u32 	%r29, 0;
	st.shared.u32 	[%r4], %r29;

$L__BB105_2:
	bar.sync 	0;
	mov.f32 	%f114, 0f00000000;
	st.local.v2.f32 	[%rd3], {%f114, %f114};
	setp.ge.s32 	%p2, %r3, %r15;
	mov.f32 	%f115, %f114;
	@%p2 bra 	$L__BB105_11;

	not.b32 	%r30, %r3;
	add.s32 	%r5, %r30, %r15;
	mul.wide.u32 	%rd30, %r5, -1431655765;
	shr.u64 	%rd31, %rd30, 39;
	cvt.u32.u64 	%r31, %rd31;
	add.s32 	%r32, %r31, 1;
	and.b32  	%r140, %r32, 3;
	setp.eq.s32 	%p3, %r140, 0;
	mov.f32 	%f114, 0f00000000;
	mov.u32 	%r141, %r3;
	@%p3 bra 	$L__BB105_7;

	mul.wide.s32 	%rd32, %r16, 2;
	mul.wide.s32 	%rd33, %r3, 2;
	add.s64 	%rd34, %rd32, %rd33;
	add.s64 	%rd35, %rd34, %rd5;
	shl.b64 	%rd36, %rd35, 1;
	add.s64 	%rd62, %rd1, %rd36;
	add.s64 	%rd37, %rd33, %rd5;
	shl.b64 	%rd38, %rd37, 1;
	add.s64 	%rd61, %rd1, %rd38;
	add.s64 	%rd39, %rd33, %rd4;
	shl.b64 	%rd40, %rd39, 1;
	add.s64 	%rd60, %rd2, %rd40;
	mov.f32 	%f114, 0f00000000;
	mov.f32 	%f115, %f114;
	mov.u32 	%r141, %r3;

$L__BB105_5:
	.pragma "nounroll";
	ld.global.nc.u32 	%r33, [%rd60];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r33;
  cvt.f32.f16 %f19, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r33;
  cvt.f32.f16 %f20, high;}

	// end inline asm
	ld.global.nc.u32 	%r35, [%rd61];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r35;
  cvt.f32.f16 %f21, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r35;
  cvt.f32.f16 %f22, high;}

	// end inline asm
	fma.rn.f32 	%f25, %f19, %f21, %f115;
	fma.rn.f32 	%f115, %f20, %f22, %f25;
	ld.global.nc.u32 	%r37, [%rd62];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r37;
  cvt.f32.f16 %f23, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r37;
  cvt.f32.f16 %f24, high;}

	// end inline asm
	fma.rn.f32 	%f26, %f19, %f23, %f114;
	fma.rn.f32 	%f114, %f20, %f24, %f26;
	add.s32 	%r141, %r141, 192;
	add.s64 	%rd62, %rd62, 768;
	add.s64 	%rd61, %rd61, 768;
	add.s64 	%rd60, %rd60, 768;
	add.s32 	%r140, %r140, -1;
	setp.ne.s32 	%p4, %r140, 0;
	@%p4 bra 	$L__BB105_5;

	st.local.v2.f32 	[%rd3], {%f115, %f114};

$L__BB105_7:
	setp.lt.u32 	%p5, %r5, 576;
	@%p5 bra 	$L__BB105_11;

	mul.wide.s32 	%rd41, %r141, 2;
	add.s64 	%rd42, %rd41, %rd4;
	shl.b64 	%rd43, %rd42, 1;
	add.s64 	%rd44, %rd2, %rd43;
	add.s64 	%rd65, %rd44, 1536;
	add.s64 	%rd45, %rd41, %rd5;
	shl.b64 	%rd46, %rd45, 1;
	add.s64 	%rd47, %rd1, %rd46;
	add.s64 	%rd64, %rd47, 2304;
	mul.wide.s32 	%rd48, %r16, 2;
	add.s64 	%rd49, %rd45, %rd48;
	shl.b64 	%rd50, %rd49, 1;
	add.s64 	%rd51, %rd1, %rd50;
	add.s64 	%rd63, %rd51, 1536;

$L__BB105_9:
	ld.global.nc.u32 	%r39, [%rd65+-1536];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r39;
  cvt.f32.f16 %f27, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r39;
  cvt.f32.f16 %f28, high;}

	// end inline asm
	ld.global.nc.u32 	%r41, [%rd64+-2304];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r41;
  cvt.f32.f16 %f29, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r41;
  cvt.f32.f16 %f30, high;}

	// end inline asm
	fma.rn.f32 	%f51, %f27, %f29, %f115;
	fma.rn.f32 	%f52, %f28, %f30, %f51;
	ld.global.nc.u32 	%r43, [%rd63+-1536];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r43;
  cvt.f32.f16 %f31, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r43;
  cvt.f32.f16 %f32, high;}

	// end inline asm
	fma.rn.f32 	%f53, %f27, %f31, %f114;
	fma.rn.f32 	%f54, %f28, %f32, %f53;
	ld.global.nc.u32 	%r45, [%rd65+-768];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r45;
  cvt.f32.f16 %f33, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r45;
  cvt.f32.f16 %f34, high;}

	// end inline asm
	ld.global.nc.u32 	%r47, [%rd64+-1536];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r47;
  cvt.f32.f16 %f35, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r47;
  cvt.f32.f16 %f36, high;}

	// end inline asm
	fma.rn.f32 	%f55, %f33, %f35, %f52;
	fma.rn.f32 	%f56, %f34, %f36, %f55;
	ld.global.nc.u32 	%r49, [%rd63+-768];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r49;
  cvt.f32.f16 %f37, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r49;
  cvt.f32.f16 %f38, high;}

	// end inline asm
	fma.rn.f32 	%f57, %f33, %f37, %f54;
	fma.rn.f32 	%f58, %f34, %f38, %f57;
	ld.global.nc.u32 	%r51, [%rd65];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r51;
  cvt.f32.f16 %f39, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r51;
  cvt.f32.f16 %f40, high;}

	// end inline asm
	ld.global.nc.u32 	%r53, [%rd64+-768];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r53;
  cvt.f32.f16 %f41, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r53;
  cvt.f32.f16 %f42, high;}

	// end inline asm
	fma.rn.f32 	%f59, %f39, %f41, %f56;
	fma.rn.f32 	%f60, %f40, %f42, %f59;
	ld.global.nc.u32 	%r55, [%rd63];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r55;
  cvt.f32.f16 %f43, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r55;
  cvt.f32.f16 %f44, high;}

	// end inline asm
	fma.rn.f32 	%f61, %f39, %f43, %f58;
	fma.rn.f32 	%f62, %f40, %f44, %f61;
	ld.global.nc.u32 	%r57, [%rd65+768];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r57;
  cvt.f32.f16 %f45, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r57;
  cvt.f32.f16 %f46, high;}

	// end inline asm
	ld.global.nc.u32 	%r59, [%rd64];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r59;
  cvt.f32.f16 %f47, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r59;
  cvt.f32.f16 %f48, high;}

	// end inline asm
	fma.rn.f32 	%f63, %f45, %f47, %f60;
	fma.rn.f32 	%f115, %f46, %f48, %f63;
	ld.global.nc.u32 	%r61, [%rd63+768];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r61;
  cvt.f32.f16 %f49, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r61;
  cvt.f32.f16 %f50, high;}

	// end inline asm
	fma.rn.f32 	%f64, %f45, %f49, %f62;
	fma.rn.f32 	%f114, %f46, %f50, %f64;
	add.s64 	%rd65, %rd65, 3072;
	add.s64 	%rd64, %rd64, 3072;
	add.s64 	%rd63, %rd63, 3072;
	add.s32 	%r141, %r141, 768;
	setp.lt.s32 	%p6, %r141, %r15;
	@%p6 bra 	$L__BB105_9;

	st.local.v2.f32 	[%rd3], {%f115, %f114};

$L__BB105_11:
	shr.s32 	%r63, %r3, 31;
	shr.u32 	%r64, %r63, 27;
	add.s32 	%r65, %r3, %r64;
	shr.s32 	%r66, %r65, 5;
	shl.b32 	%r67, %r66, 2;
	add.s32 	%r14, %r28, %r67;
	mov.u32 	%r69, 2;
	mov.b32 	%r70, %f115;
	mov.u32 	%r71, 31;
	mov.u32 	%r72, 16;
	mov.u32 	%r73, -1;
	shfl.sync.bfly.b32 	%r74|%p7, %r70, %r72, %r71, %r73;
	mov.b32 	%f65, %r74;
	add.f32 	%f66, %f115, %f65;
	mov.b32 	%r75, %f66;
	mov.u32 	%r76, 8;
	shfl.sync.bfly.b32 	%r77|%p8, %r75, %r76, %r71, %r73;
	mov.b32 	%f67, %r77;
	add.f32 	%f68, %f66, %f67;
	mov.b32 	%r78, %f68;
	mov.u32 	%r79, 4;
	shfl.sync.bfly.b32 	%r80|%p9, %r78, %r79, %r71, %r73;
	mov.b32 	%f69, %r80;
	add.f32 	%f70, %f68, %f69;
	mov.b32 	%r81, %f70;
	shfl.sync.bfly.b32 	%r82|%p10, %r81, %r69, %r71, %r73;
	mov.b32 	%f71, %r82;
	add.f32 	%f72, %f70, %f71;
	mov.b32 	%r83, %f72;
	mov.u32 	%r84, 1;
	shfl.sync.bfly.b32 	%r85|%p11, %r83, %r84, %r71, %r73;
	mov.b32 	%f73, %r85;
	add.f32 	%f74, %f72, %f73;
	st.local.f32 	[%rd3], %f74;
	st.shared.f32 	[%r14], %f74;
	bar.sync 	0;
	@%p1 bra 	$L__BB105_13;

	ld.shared.f32 	%f75, [%r4];
	mov.b32 	%r86, %f75;
	shfl.sync.bfly.b32 	%r90|%p13, %r86, %r72, %r71, %r73;
	mov.b32 	%f76, %r90;
	add.f32 	%f77, %f75, %f76;
	mov.b32 	%r91, %f77;
	shfl.sync.bfly.b32 	%r93|%p14, %r91, %r76, %r71, %r73;
	mov.b32 	%f78, %r93;
	add.f32 	%f79, %f77, %f78;
	mov.b32 	%r94, %f79;
	shfl.sync.bfly.b32 	%r96|%p15, %r94, %r79, %r71, %r73;
	mov.b32 	%f80, %r96;
	add.f32 	%f81, %f79, %f80;
	mov.b32 	%r97, %f81;
	shfl.sync.bfly.b32 	%r99|%p16, %r97, %r69, %r71, %r73;
	mov.b32 	%f82, %r99;
	add.f32 	%f83, %f81, %f82;
	mov.b32 	%r100, %f83;
	shfl.sync.bfly.b32 	%r102|%p17, %r100, %r84, %r71, %r73;
	mov.b32 	%f84, %r102;
	add.f32 	%f85, %f83, %f84;
	st.local.f32 	[%rd3], %f85;

$L__BB105_13:
	bar.sync 	0;
	mov.b32 	%r103, %f114;
	shfl.sync.bfly.b32 	%r107|%p19, %r103, %r72, %r71, %r73;
	mov.b32 	%f86, %r107;
	add.f32 	%f87, %f114, %f86;
	mov.b32 	%r108, %f87;
	shfl.sync.bfly.b32 	%r110|%p20, %r108, %r76, %r71, %r73;
	mov.b32 	%f88, %r110;
	add.f32 	%f89, %f87, %f88;
	mov.b32 	%r111, %f89;
	shfl.sync.bfly.b32 	%r113|%p21, %r111, %r79, %r71, %r73;
	mov.b32 	%f90, %r113;
	add.f32 	%f91, %f89, %f90;
	mov.b32 	%r114, %f91;
	shfl.sync.bfly.b32 	%r116|%p22, %r114, %r69, %r71, %r73;
	mov.b32 	%f92, %r116;
	add.f32 	%f93, %f91, %f92;
	mov.b32 	%r117, %f93;
	shfl.sync.bfly.b32 	%r119|%p23, %r117, %r84, %r71, %r73;
	mov.b32 	%f94, %r119;
	add.f32 	%f95, %f93, %f94;
	st.local.f32 	[%rd3+4], %f95;
	st.shared.f32 	[%r14], %f95;
	bar.sync 	0;
	@%p1 bra 	$L__BB105_15;

	ld.shared.f32 	%f96, [%r4];
	mov.b32 	%r120, %f96;
	mov.u32 	%r121, 31;
	mov.u32 	%r122, 16;
	mov.u32 	%r123, -1;
	shfl.sync.bfly.b32 	%r124|%p24, %r120, %r122, %r121, %r123;
	mov.b32 	%f97, %r124;
	add.f32 	%f98, %f96, %f97;
	mov.b32 	%r125, %f98;
	mov.u32 	%r126, 8;
	shfl.sync.bfly.b32 	%r127|%p25, %r125, %r126, %r121, %r123;
	mov.b32 	%f99, %r127;
	add.f32 	%f100, %f98, %f99;
	mov.b32 	%r128, %f100;
	mov.u32 	%r129, 4;
	shfl.sync.bfly.b32 	%r130|%p26, %r128, %r129, %r121, %r123;
	mov.b32 	%f101, %r130;
	add.f32 	%f102, %f100, %f101;
	mov.b32 	%r131, %f102;
	mov.u32 	%r132, 2;
	shfl.sync.bfly.b32 	%r133|%p27, %r131, %r132, %r121, %r123;
	mov.b32 	%f103, %r133;
	add.f32 	%f104, %f102, %f103;
	mov.b32 	%r134, %f104;
	mov.u32 	%r135, 1;
	shfl.sync.bfly.b32 	%r136|%p28, %r134, %r135, %r121, %r123;
	mov.b32 	%f105, %r136;
	add.f32 	%f106, %f104, %f105;
	st.local.f32 	[%rd3+4], %f106;

$L__BB105_15:
	bar.sync 	0;
	setp.gt.s32 	%p29, %r3, 1;
	@%p29 bra 	$L__BB105_17;

	mad.lo.s32 	%r137, %r3, %r17, %r2;
	cvt.s64.s32 	%rd52, %r137;
	mul.lo.s32 	%r138, %r1, %r18;
	cvt.s64.s32 	%rd53, %r138;
	add.s64 	%rd54, %rd53, %rd52;
	mul.wide.s32 	%rd55, %r3, 4;
	add.s64 	%rd56, %rd3, %rd55;
	ld.local.f32 	%f107, [%rd56];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f107;}

	// end inline asm
	cvta.to.global.u64 	%rd57, %rd26;
	shl.b64 	%rd58, %rd54, 1;
	add.s64 	%rd59, %rd57, %rd58;
	st.global.u16 	[%rd59], %rs1;

$L__BB105_17:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_3_bs_192
.visible .entry ggml_matvec_f16_ncols_3_bs_192(
	.param .u64 ggml_matvec_f16_ncols_3_bs_192_param_0,
	.param .u64 ggml_matvec_f16_ncols_3_bs_192_param_1,
	.param .u64 ggml_matvec_f16_ncols_3_bs_192_param_2,
	.param .u32 ggml_matvec_f16_ncols_3_bs_192_param_3,
	.param .u32 ggml_matvec_f16_ncols_3_bs_192_param_4,
	.param .u32 ggml_matvec_f16_ncols_3_bs_192_param_5,
	.param .u32 ggml_matvec_f16_ncols_3_bs_192_param_6,
	.param .u32 ggml_matvec_f16_ncols_3_bs_192_param_7,
	.param .u32 ggml_matvec_f16_ncols_3_bs_192_param_8,
	.param .u32 ggml_matvec_f16_ncols_3_bs_192_param_9,
	.param .u32 ggml_matvec_f16_ncols_3_bs_192_param_10,
	.param .u32 ggml_matvec_f16_ncols_3_bs_192_param_11
)
{
	.local .align 4 .b8 	__local_depot106[12];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<41>;
	.reg .b16 	%rs<2>;
	.reg .f32 	%f<168>;
	.reg .b32 	%r<194>;
	.reg .b64 	%rd<74>;


	mov.u64 	%SPL, __local_depot106;
	ld.param.u64 	%rd29, [ggml_matvec_f16_ncols_3_bs_192_param_0];
	ld.param.u64 	%rd30, [ggml_matvec_f16_ncols_3_bs_192_param_1];
	ld.param.u64 	%rd28, [ggml_matvec_f16_ncols_3_bs_192_param_2];
	ld.param.u32 	%r15, [ggml_matvec_f16_ncols_3_bs_192_param_3];
	ld.param.u32 	%r19, [ggml_matvec_f16_ncols_3_bs_192_param_5];
	ld.param.u32 	%r16, [ggml_matvec_f16_ncols_3_bs_192_param_6];
	ld.param.u32 	%r17, [ggml_matvec_f16_ncols_3_bs_192_param_7];
	ld.param.u32 	%r20, [ggml_matvec_f16_ncols_3_bs_192_param_8];
	ld.param.u32 	%r21, [ggml_matvec_f16_ncols_3_bs_192_param_9];
	ld.param.u32 	%r22, [ggml_matvec_f16_ncols_3_bs_192_param_10];
	ld.param.u32 	%r18, [ggml_matvec_f16_ncols_3_bs_192_param_11];
	cvta.to.global.u64 	%rd73, %rd30;
	cvta.to.global.u64 	%rd2, %rd29;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r23, %r1, %r20;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r24, %r2, %r19;
	mad.lo.s32 	%r25, %r23, %r21, %r24;
	cvt.s64.s32 	%rd4, %r25;
	mul.lo.s32 	%r26, %r1, %r22;
	cvt.s64.s32 	%rd5, %r26;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r27, %r3, 2;
	mov.u32 	%r28, data_mmv;
	add.s32 	%r4, %r28, %r27;
	@%p1 bra 	$L__BB106_2;

	mov.u32 	%r29, 0;
	st.shared.u32 	[%r4], %r29;

$L__BB106_2:
	bar.sync 	0;
	mov.f32 	%f165, 0f00000000;
	mov.u32 	%r30, 0;
	st.local.u32 	[%rd3], %r30;
	st.local.u32 	[%rd3+4], %r30;
	st.local.u32 	[%rd3+8], %r30;
	setp.ge.s32 	%p2, %r3, %r15;
	mov.f32 	%f166, %f165;
	mov.f32 	%f167, %f165;
	@%p2 bra 	$L__BB106_11;

	not.b32 	%r31, %r3;
	add.s32 	%r5, %r31, %r15;
	mul.wide.u32 	%rd32, %r5, -1431655765;
	shr.u64 	%rd33, %rd32, 39;
	cvt.u32.u64 	%r32, %rd33;
	add.s32 	%r33, %r32, 1;
	and.b32  	%r191, %r33, 3;
	setp.eq.s32 	%p3, %r191, 0;
	mov.f32 	%f165, 0f00000000;
	mov.u32 	%r192, %r3;
	@%p3 bra 	$L__BB106_7;

	shl.b32 	%r34, %r16, 1;
	add.s32 	%r35, %r3, %r34;
	mul.wide.s32 	%rd34, %r35, 2;
	add.s64 	%rd35, %rd34, %rd5;
	shl.b64 	%rd36, %rd35, 1;
	add.s64 	%rd71, %rd73, %rd36;
	mul.wide.s32 	%rd37, %r16, 2;
	mul.wide.s32 	%rd38, %r3, 2;
	add.s64 	%rd39, %rd37, %rd38;
	add.s64 	%rd40, %rd39, %rd5;
	shl.b64 	%rd41, %rd40, 1;
	add.s64 	%rd70, %rd73, %rd41;
	add.s64 	%rd42, %rd38, %rd5;
	shl.b64 	%rd43, %rd42, 1;
	add.s64 	%rd69, %rd73, %rd43;
	add.s64 	%rd44, %rd38, %rd4;
	shl.b64 	%rd45, %rd44, 1;
	add.s64 	%rd68, %rd2, %rd45;
	mov.f32 	%f165, 0f00000000;
	mov.f32 	%f166, %f165;
	mov.f32 	%f167, %f165;
	mov.u32 	%r192, %r3;

$L__BB106_5:
	.pragma "nounroll";
	ld.global.nc.u32 	%r36, [%rd68];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r36;
  cvt.f32.f16 %f28, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r36;
  cvt.f32.f16 %f29, high;}

	// end inline asm
	ld.global.nc.u32 	%r38, [%rd69];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r38;
  cvt.f32.f16 %f30, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r38;
  cvt.f32.f16 %f31, high;}

	// end inline asm
	fma.rn.f32 	%f36, %f28, %f30, %f167;
	fma.rn.f32 	%f167, %f29, %f31, %f36;
	ld.global.nc.u32 	%r40, [%rd70];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r40;
  cvt.f32.f16 %f32, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r40;
  cvt.f32.f16 %f33, high;}

	// end inline asm
	fma.rn.f32 	%f37, %f28, %f32, %f166;
	fma.rn.f32 	%f166, %f29, %f33, %f37;
	ld.global.nc.u32 	%r42, [%rd71];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r42;
  cvt.f32.f16 %f34, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r42;
  cvt.f32.f16 %f35, high;}

	// end inline asm
	fma.rn.f32 	%f38, %f28, %f34, %f165;
	fma.rn.f32 	%f165, %f29, %f35, %f38;
	add.s32 	%r192, %r192, 192;
	add.s64 	%rd71, %rd71, 768;
	add.s64 	%rd70, %rd70, 768;
	add.s64 	%rd69, %rd69, 768;
	add.s64 	%rd68, %rd68, 768;
	add.s32 	%r191, %r191, -1;
	setp.ne.s32 	%p4, %r191, 0;
	@%p4 bra 	$L__BB106_5;

	st.local.f32 	[%rd3], %f167;
	st.local.f32 	[%rd3+4], %f166;
	st.local.f32 	[%rd3+8], %f165;

$L__BB106_7:
	setp.lt.u32 	%p5, %r5, 576;
	@%p5 bra 	$L__BB106_11;

	add.s32 	%r44, %r192, %r16;
	shl.b32 	%r45, %r16, 1;
	add.s32 	%r46, %r192, %r45;
	add.s32 	%r47, %r44, 192;
	mul.wide.s32 	%rd46, %r47, 4;
	shl.b64 	%rd47, %rd5, 1;
	add.s64 	%rd19, %rd46, %rd47;
	mul.wide.s32 	%rd48, %r46, 4;
	add.s64 	%rd20, %rd48, %rd47;
	mul.wide.s32 	%rd49, %r192, 2;
	add.s64 	%rd50, %rd49, %rd4;
	shl.b64 	%rd51, %rd50, 1;
	add.s64 	%rd52, %rd2, %rd51;
	add.s64 	%rd72, %rd52, 1536;
	mul.wide.s32 	%rd53, %r192, 4;
	add.s64 	%rd22, %rd53, %rd47;
	mul.wide.s32 	%rd54, %r16, 4;
	add.s64 	%rd55, %rd53, %rd54;
	add.s64 	%rd23, %rd55, %rd47;

$L__BB106_9:
	ld.global.nc.u32 	%r48, [%rd72+-1536];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r48;
  cvt.f32.f16 %f39, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r48;
  cvt.f32.f16 %f40, high;}

	// end inline asm
	add.s64 	%rd56, %rd73, %rd22;
	ld.global.nc.u32 	%r50, [%rd56];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r50;
  cvt.f32.f16 %f41, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r50;
  cvt.f32.f16 %f42, high;}

	// end inline asm
	fma.rn.f32 	%f71, %f39, %f41, %f167;
	fma.rn.f32 	%f72, %f40, %f42, %f71;
	add.s64 	%rd57, %rd73, %rd23;
	ld.global.nc.u32 	%r52, [%rd57];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r52;
  cvt.f32.f16 %f43, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r52;
  cvt.f32.f16 %f44, high;}

	// end inline asm
	fma.rn.f32 	%f73, %f39, %f43, %f166;
	fma.rn.f32 	%f74, %f40, %f44, %f73;
	add.s64 	%rd58, %rd73, %rd20;
	ld.global.nc.u32 	%r54, [%rd58];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r54;
  cvt.f32.f16 %f45, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r54;
  cvt.f32.f16 %f46, high;}

	// end inline asm
	fma.rn.f32 	%f75, %f39, %f45, %f165;
	fma.rn.f32 	%f76, %f40, %f46, %f75;
	ld.global.nc.u32 	%r56, [%rd72+-768];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r56;
  cvt.f32.f16 %f47, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r56;
  cvt.f32.f16 %f48, high;}

	// end inline asm
	ld.global.nc.u32 	%r58, [%rd56+768];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r58;
  cvt.f32.f16 %f49, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r58;
  cvt.f32.f16 %f50, high;}

	// end inline asm
	fma.rn.f32 	%f77, %f47, %f49, %f72;
	fma.rn.f32 	%f78, %f48, %f50, %f77;
	add.s64 	%rd59, %rd73, %rd19;
	ld.global.nc.u32 	%r60, [%rd59];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r60;
  cvt.f32.f16 %f51, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r60;
  cvt.f32.f16 %f52, high;}

	// end inline asm
	fma.rn.f32 	%f79, %f47, %f51, %f74;
	fma.rn.f32 	%f80, %f48, %f52, %f79;
	ld.global.nc.u32 	%r62, [%rd58+768];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r62;
  cvt.f32.f16 %f53, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r62;
  cvt.f32.f16 %f54, high;}

	// end inline asm
	fma.rn.f32 	%f81, %f47, %f53, %f76;
	fma.rn.f32 	%f82, %f48, %f54, %f81;
	ld.global.nc.u32 	%r64, [%rd72];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r64;
  cvt.f32.f16 %f55, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r64;
  cvt.f32.f16 %f56, high;}

	// end inline asm
	ld.global.nc.u32 	%r66, [%rd56+1536];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r66;
  cvt.f32.f16 %f57, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r66;
  cvt.f32.f16 %f58, high;}

	// end inline asm
	fma.rn.f32 	%f83, %f55, %f57, %f78;
	fma.rn.f32 	%f84, %f56, %f58, %f83;
	ld.global.nc.u32 	%r68, [%rd59+768];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r68;
  cvt.f32.f16 %f59, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r68;
  cvt.f32.f16 %f60, high;}

	// end inline asm
	fma.rn.f32 	%f85, %f55, %f59, %f80;
	fma.rn.f32 	%f86, %f56, %f60, %f85;
	ld.global.nc.u32 	%r70, [%rd58+1536];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r70;
  cvt.f32.f16 %f61, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r70;
  cvt.f32.f16 %f62, high;}

	// end inline asm
	fma.rn.f32 	%f87, %f55, %f61, %f82;
	fma.rn.f32 	%f88, %f56, %f62, %f87;
	ld.global.nc.u32 	%r72, [%rd72+768];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r72;
  cvt.f32.f16 %f63, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r72;
  cvt.f32.f16 %f64, high;}

	// end inline asm
	ld.global.nc.u32 	%r74, [%rd56+2304];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r74;
  cvt.f32.f16 %f65, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r74;
  cvt.f32.f16 %f66, high;}

	// end inline asm
	fma.rn.f32 	%f89, %f63, %f65, %f84;
	fma.rn.f32 	%f167, %f64, %f66, %f89;
	ld.global.nc.u32 	%r76, [%rd59+1536];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r76;
  cvt.f32.f16 %f67, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r76;
  cvt.f32.f16 %f68, high;}

	// end inline asm
	fma.rn.f32 	%f90, %f63, %f67, %f86;
	fma.rn.f32 	%f166, %f64, %f68, %f90;
	ld.global.nc.u32 	%r78, [%rd58+2304];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r78;
  cvt.f32.f16 %f69, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r78;
  cvt.f32.f16 %f70, high;}

	// end inline asm
	fma.rn.f32 	%f91, %f63, %f69, %f88;
	fma.rn.f32 	%f165, %f64, %f70, %f91;
	add.s64 	%rd73, %rd73, 3072;
	add.s64 	%rd72, %rd72, 3072;
	add.s32 	%r192, %r192, 768;
	setp.lt.s32 	%p6, %r192, %r15;
	@%p6 bra 	$L__BB106_9;

	st.local.f32 	[%rd3], %f167;
	st.local.f32 	[%rd3+4], %f166;
	st.local.f32 	[%rd3+8], %f165;

$L__BB106_11:
	shr.s32 	%r80, %r3, 31;
	shr.u32 	%r81, %r80, 27;
	add.s32 	%r82, %r3, %r81;
	shr.s32 	%r83, %r82, 5;
	shl.b32 	%r84, %r83, 2;
	add.s32 	%r14, %r28, %r84;
	mov.u32 	%r86, 2;
	mov.b32 	%r87, %f167;
	mov.u32 	%r88, 31;
	mov.u32 	%r89, 16;
	mov.u32 	%r90, -1;
	shfl.sync.bfly.b32 	%r91|%p7, %r87, %r89, %r88, %r90;
	mov.b32 	%f92, %r91;
	add.f32 	%f93, %f167, %f92;
	mov.b32 	%r92, %f93;
	mov.u32 	%r93, 8;
	shfl.sync.bfly.b32 	%r94|%p8, %r92, %r93, %r88, %r90;
	mov.b32 	%f94, %r94;
	add.f32 	%f95, %f93, %f94;
	mov.b32 	%r95, %f95;
	mov.u32 	%r96, 4;
	shfl.sync.bfly.b32 	%r97|%p9, %r95, %r96, %r88, %r90;
	mov.b32 	%f96, %r97;
	add.f32 	%f97, %f95, %f96;
	mov.b32 	%r98, %f97;
	shfl.sync.bfly.b32 	%r99|%p10, %r98, %r86, %r88, %r90;
	mov.b32 	%f98, %r99;
	add.f32 	%f99, %f97, %f98;
	mov.b32 	%r100, %f99;
	mov.u32 	%r101, 1;
	shfl.sync.bfly.b32 	%r102|%p11, %r100, %r101, %r88, %r90;
	mov.b32 	%f100, %r102;
	add.f32 	%f101, %f99, %f100;
	st.local.f32 	[%rd3], %f101;
	st.shared.f32 	[%r14], %f101;
	bar.sync 	0;
	@%p1 bra 	$L__BB106_13;

	ld.shared.f32 	%f102, [%r4];
	mov.b32 	%r103, %f102;
	shfl.sync.bfly.b32 	%r107|%p13, %r103, %r89, %r88, %r90;
	mov.b32 	%f103, %r107;
	add.f32 	%f104, %f102, %f103;
	mov.b32 	%r108, %f104;
	shfl.sync.bfly.b32 	%r110|%p14, %r108, %r93, %r88, %r90;
	mov.b32 	%f105, %r110;
	add.f32 	%f106, %f104, %f105;
	mov.b32 	%r111, %f106;
	shfl.sync.bfly.b32 	%r113|%p15, %r111, %r96, %r88, %r90;
	mov.b32 	%f107, %r113;
	add.f32 	%f108, %f106, %f107;
	mov.b32 	%r114, %f108;
	shfl.sync.bfly.b32 	%r116|%p16, %r114, %r86, %r88, %r90;
	mov.b32 	%f109, %r116;
	add.f32 	%f110, %f108, %f109;
	mov.b32 	%r117, %f110;
	shfl.sync.bfly.b32 	%r119|%p17, %r117, %r101, %r88, %r90;
	mov.b32 	%f111, %r119;
	add.f32 	%f112, %f110, %f111;
	st.local.f32 	[%rd3], %f112;

$L__BB106_13:
	bar.sync 	0;
	mov.b32 	%r120, %f166;
	shfl.sync.bfly.b32 	%r124|%p19, %r120, %r89, %r88, %r90;
	mov.b32 	%f113, %r124;
	add.f32 	%f114, %f166, %f113;
	mov.b32 	%r125, %f114;
	shfl.sync.bfly.b32 	%r127|%p20, %r125, %r93, %r88, %r90;
	mov.b32 	%f115, %r127;
	add.f32 	%f116, %f114, %f115;
	mov.b32 	%r128, %f116;
	shfl.sync.bfly.b32 	%r130|%p21, %r128, %r96, %r88, %r90;
	mov.b32 	%f117, %r130;
	add.f32 	%f118, %f116, %f117;
	mov.b32 	%r131, %f118;
	shfl.sync.bfly.b32 	%r133|%p22, %r131, %r86, %r88, %r90;
	mov.b32 	%f119, %r133;
	add.f32 	%f120, %f118, %f119;
	mov.b32 	%r134, %f120;
	shfl.sync.bfly.b32 	%r136|%p23, %r134, %r101, %r88, %r90;
	mov.b32 	%f121, %r136;
	add.f32 	%f122, %f120, %f121;
	st.local.f32 	[%rd3+4], %f122;
	st.shared.f32 	[%r14], %f122;
	bar.sync 	0;
	@%p1 bra 	$L__BB106_15;

	ld.shared.f32 	%f123, [%r4];
	mov.b32 	%r137, %f123;
	mov.u32 	%r138, 31;
	mov.u32 	%r139, 16;
	mov.u32 	%r140, -1;
	shfl.sync.bfly.b32 	%r141|%p24, %r137, %r139, %r138, %r140;
	mov.b32 	%f124, %r141;
	add.f32 	%f125, %f123, %f124;
	mov.b32 	%r142, %f125;
	mov.u32 	%r143, 8;
	shfl.sync.bfly.b32 	%r144|%p25, %r142, %r143, %r138, %r140;
	mov.b32 	%f126, %r144;
	add.f32 	%f127, %f125, %f126;
	mov.b32 	%r145, %f127;
	mov.u32 	%r146, 4;
	shfl.sync.bfly.b32 	%r147|%p26, %r145, %r146, %r138, %r140;
	mov.b32 	%f128, %r147;
	add.f32 	%f129, %f127, %f128;
	mov.b32 	%r148, %f129;
	mov.u32 	%r149, 2;
	shfl.sync.bfly.b32 	%r150|%p27, %r148, %r149, %r138, %r140;
	mov.b32 	%f130, %r150;
	add.f32 	%f131, %f129, %f130;
	mov.b32 	%r151, %f131;
	mov.u32 	%r152, 1;
	shfl.sync.bfly.b32 	%r153|%p28, %r151, %r152, %r138, %r140;
	mov.b32 	%f132, %r153;
	add.f32 	%f133, %f131, %f132;
	st.local.f32 	[%rd3+4], %f133;

$L__BB106_15:
	bar.sync 	0;
	mov.b32 	%r154, %f165;
	mov.u32 	%r155, 31;
	mov.u32 	%r156, 16;
	mov.u32 	%r157, -1;
	shfl.sync.bfly.b32 	%r158|%p30, %r154, %r156, %r155, %r157;
	mov.b32 	%f134, %r158;
	add.f32 	%f135, %f165, %f134;
	mov.b32 	%r159, %f135;
	mov.u32 	%r160, 8;
	shfl.sync.bfly.b32 	%r161|%p31, %r159, %r160, %r155, %r157;
	mov.b32 	%f136, %r161;
	add.f32 	%f137, %f135, %f136;
	mov.b32 	%r162, %f137;
	mov.u32 	%r163, 4;
	shfl.sync.bfly.b32 	%r164|%p32, %r162, %r163, %r155, %r157;
	mov.b32 	%f138, %r164;
	add.f32 	%f139, %f137, %f138;
	mov.b32 	%r165, %f139;
	mov.u32 	%r166, 2;
	shfl.sync.bfly.b32 	%r167|%p33, %r165, %r166, %r155, %r157;
	mov.b32 	%f140, %r167;
	add.f32 	%f141, %f139, %f140;
	mov.b32 	%r168, %f141;
	mov.u32 	%r169, 1;
	shfl.sync.bfly.b32 	%r170|%p34, %r168, %r169, %r155, %r157;
	mov.b32 	%f142, %r170;
	add.f32 	%f143, %f141, %f142;
	st.local.f32 	[%rd3+8], %f143;
	st.shared.f32 	[%r14], %f143;
	bar.sync 	0;
	@%p1 bra 	$L__BB106_17;

	ld.shared.f32 	%f144, [%r4];
	mov.b32 	%r171, %f144;
	shfl.sync.bfly.b32 	%r175|%p35, %r171, %r156, %r155, %r157;
	mov.b32 	%f145, %r175;
	add.f32 	%f146, %f144, %f145;
	mov.b32 	%r176, %f146;
	shfl.sync.bfly.b32 	%r178|%p36, %r176, %r160, %r155, %r157;
	mov.b32 	%f147, %r178;
	add.f32 	%f148, %f146, %f147;
	mov.b32 	%r179, %f148;
	shfl.sync.bfly.b32 	%r181|%p37, %r179, %r163, %r155, %r157;
	mov.b32 	%f149, %r181;
	add.f32 	%f150, %f148, %f149;
	mov.b32 	%r182, %f150;
	shfl.sync.bfly.b32 	%r184|%p38, %r182, %r166, %r155, %r157;
	mov.b32 	%f151, %r184;
	add.f32 	%f152, %f150, %f151;
	mov.b32 	%r185, %f152;
	shfl.sync.bfly.b32 	%r187|%p39, %r185, %r169, %r155, %r157;
	mov.b32 	%f153, %r187;
	add.f32 	%f154, %f152, %f153;
	st.local.f32 	[%rd3+8], %f154;

$L__BB106_17:
	bar.sync 	0;
	setp.gt.s32 	%p40, %r3, 2;
	@%p40 bra 	$L__BB106_19;

	mad.lo.s32 	%r188, %r3, %r17, %r2;
	cvt.s64.s32 	%rd60, %r188;
	mul.lo.s32 	%r189, %r1, %r18;
	cvt.s64.s32 	%rd61, %r189;
	add.s64 	%rd62, %rd61, %rd60;
	mul.wide.s32 	%rd63, %r3, 4;
	add.s64 	%rd64, %rd3, %rd63;
	ld.local.f32 	%f155, [%rd64];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f155;}

	// end inline asm
	cvta.to.global.u64 	%rd65, %rd28;
	shl.b64 	%rd66, %rd62, 1;
	add.s64 	%rd67, %rd65, %rd66;
	st.global.u16 	[%rd67], %rs1;

$L__BB106_19:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_4_bs_192
.visible .entry ggml_matvec_f16_ncols_4_bs_192(
	.param .u64 ggml_matvec_f16_ncols_4_bs_192_param_0,
	.param .u64 ggml_matvec_f16_ncols_4_bs_192_param_1,
	.param .u64 ggml_matvec_f16_ncols_4_bs_192_param_2,
	.param .u32 ggml_matvec_f16_ncols_4_bs_192_param_3,
	.param .u32 ggml_matvec_f16_ncols_4_bs_192_param_4,
	.param .u32 ggml_matvec_f16_ncols_4_bs_192_param_5,
	.param .u32 ggml_matvec_f16_ncols_4_bs_192_param_6,
	.param .u32 ggml_matvec_f16_ncols_4_bs_192_param_7,
	.param .u32 ggml_matvec_f16_ncols_4_bs_192_param_8,
	.param .u32 ggml_matvec_f16_ncols_4_bs_192_param_9,
	.param .u32 ggml_matvec_f16_ncols_4_bs_192_param_10,
	.param .u32 ggml_matvec_f16_ncols_4_bs_192_param_11
)
{
	.local .align 16 .b8 	__local_depot107[16];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<53>;
	.reg .b16 	%rs<2>;
	.reg .f32 	%f<172>;
	.reg .b32 	%r<211>;
	.reg .b64 	%rd<64>;


	mov.u64 	%SPL, __local_depot107;
	ld.param.u64 	%rd19, [ggml_matvec_f16_ncols_4_bs_192_param_0];
	ld.param.u64 	%rd20, [ggml_matvec_f16_ncols_4_bs_192_param_1];
	ld.param.u64 	%rd18, [ggml_matvec_f16_ncols_4_bs_192_param_2];
	ld.param.u32 	%r11, [ggml_matvec_f16_ncols_4_bs_192_param_3];
	ld.param.u32 	%r15, [ggml_matvec_f16_ncols_4_bs_192_param_5];
	ld.param.u32 	%r12, [ggml_matvec_f16_ncols_4_bs_192_param_6];
	ld.param.u32 	%r13, [ggml_matvec_f16_ncols_4_bs_192_param_7];
	ld.param.u32 	%r16, [ggml_matvec_f16_ncols_4_bs_192_param_8];
	ld.param.u32 	%r17, [ggml_matvec_f16_ncols_4_bs_192_param_9];
	ld.param.u32 	%r18, [ggml_matvec_f16_ncols_4_bs_192_param_10];
	ld.param.u32 	%r14, [ggml_matvec_f16_ncols_4_bs_192_param_11];
	cvta.to.global.u64 	%rd63, %rd20;
	add.u64 	%rd2, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r19, %r1, %r16;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r20, %r2, %r15;
	mad.lo.s32 	%r21, %r19, %r17, %r20;
	cvt.s64.s32 	%rd3, %r21;
	cvta.to.global.u64 	%rd4, %rd19;
	mul.lo.s32 	%r22, %r1, %r18;
	cvt.s64.s32 	%rd5, %r22;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r23, %r3, 2;
	mov.u32 	%r24, data_mmv;
	add.s32 	%r4, %r24, %r23;
	@%p1 bra 	$L__BB107_2;

	mov.u32 	%r25, 0;
	st.shared.u32 	[%r4], %r25;

$L__BB107_2:
	bar.sync 	0;
	mov.f32 	%f168, 0f00000000;
	st.local.v4.f32 	[%rd2], {%f168, %f168, %f168, %f168};
	setp.ge.s32 	%p2, %r3, %r11;
	mov.f32 	%f169, %f168;
	mov.f32 	%f170, %f168;
	mov.f32 	%f171, %f168;
	@%p2 bra 	$L__BB107_9;

	not.b32 	%r26, %r3;
	add.s32 	%r5, %r26, %r11;
	mul.wide.u32 	%rd22, %r5, -1431655765;
	shr.u64 	%rd23, %rd22, 39;
	and.b64  	%rd24, %rd23, 1;
	setp.eq.b64 	%p3, %rd24, 1;
	mov.pred 	%p4, 0;
	xor.pred  	%p5, %p3, %p4;
	mov.f32 	%f168, 0f00000000;
	mov.u32 	%r210, %r3;
	@%p5 bra 	$L__BB107_5;

	shl.b64 	%rd25, %rd5, 1;
	add.s64 	%rd26, %rd63, %rd25;
	shl.b64 	%rd27, %rd3, 1;
	add.s64 	%rd28, %rd4, %rd27;
	mul.wide.s32 	%rd29, %r3, 4;
	add.s64 	%rd30, %rd28, %rd29;
	ld.global.nc.u32 	%r27, [%rd30];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r27;
  cvt.f32.f16 %f29, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r27;
  cvt.f32.f16 %f30, high;}

	// end inline asm
	add.s64 	%rd31, %rd26, %rd29;
	ld.global.nc.u32 	%r29, [%rd31];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r29;
  cvt.f32.f16 %f31, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r29;
  cvt.f32.f16 %f32, high;}

	// end inline asm
	fma.rn.f32 	%f39, %f29, %f31, 0f00000000;
	fma.rn.f32 	%f171, %f30, %f32, %f39;
	st.local.f32 	[%rd2], %f171;
	mul.wide.s32 	%rd32, %r12, 4;
	add.s64 	%rd33, %rd31, %rd32;
	ld.global.nc.u32 	%r31, [%rd33];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r31;
  cvt.f32.f16 %f33, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r31;
  cvt.f32.f16 %f34, high;}

	// end inline asm
	fma.rn.f32 	%f40, %f29, %f33, 0f00000000;
	fma.rn.f32 	%f170, %f30, %f34, %f40;
	st.local.f32 	[%rd2+4], %f170;
	add.s32 	%r37, %r3, %r12;
	add.s32 	%r38, %r37, %r12;
	mul.wide.s32 	%rd34, %r38, 4;
	add.s64 	%rd35, %rd26, %rd34;
	ld.global.nc.u32 	%r33, [%rd35];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r33;
  cvt.f32.f16 %f35, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r33;
  cvt.f32.f16 %f36, high;}

	// end inline asm
	fma.rn.f32 	%f41, %f29, %f35, 0f00000000;
	fma.rn.f32 	%f169, %f30, %f36, %f41;
	st.local.f32 	[%rd2+8], %f169;
	add.s32 	%r39, %r38, %r12;
	mul.wide.s32 	%rd36, %r39, 4;
	add.s64 	%rd37, %rd26, %rd36;
	ld.global.nc.u32 	%r35, [%rd37];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r35;
  cvt.f32.f16 %f37, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r35;
  cvt.f32.f16 %f38, high;}

	// end inline asm
	fma.rn.f32 	%f42, %f29, %f37, 0f00000000;
	fma.rn.f32 	%f168, %f30, %f38, %f42;
	st.local.f32 	[%rd2+12], %f168;
	add.s32 	%r210, %r3, 192;

$L__BB107_5:
	setp.lt.u32 	%p6, %r5, 192;
	@%p6 bra 	$L__BB107_9;

	add.s32 	%r40, %r210, %r12;
	add.s32 	%r41, %r40, 192;
	mul.wide.s32 	%rd38, %r41, 4;
	shl.b64 	%rd39, %rd5, 1;
	add.s64 	%rd8, %rd38, %rd39;
	shl.b32 	%r42, %r12, 1;
	add.s32 	%r43, %r210, %r42;
	mad.lo.s32 	%r44, %r12, 3, %r210;
	mul.wide.s32 	%rd40, %r43, 4;
	add.s64 	%rd9, %rd40, %rd39;
	mul.wide.s32 	%rd41, %r44, 4;
	add.s64 	%rd10, %rd41, %rd39;
	mul.wide.s32 	%rd42, %r210, 2;
	add.s64 	%rd43, %rd42, %rd3;
	shl.b64 	%rd44, %rd43, 1;
	add.s64 	%rd45, %rd4, %rd44;
	add.s64 	%rd62, %rd45, 768;
	mul.wide.s32 	%rd46, %r210, 4;
	mul.wide.s32 	%rd47, %r12, 4;
	add.s64 	%rd48, %rd46, %rd47;
	add.s64 	%rd12, %rd48, %rd39;
	add.s64 	%rd13, %rd46, %rd39;

$L__BB107_7:
	ld.global.nc.u32 	%r45, [%rd62+-768];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r45;
  cvt.f32.f16 %f43, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r45;
  cvt.f32.f16 %f44, high;}

	// end inline asm
	add.s64 	%rd49, %rd63, %rd13;
	ld.global.nc.u32 	%r47, [%rd49];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r47;
  cvt.f32.f16 %f45, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r47;
  cvt.f32.f16 %f46, high;}

	// end inline asm
	fma.rn.f32 	%f63, %f43, %f45, %f171;
	fma.rn.f32 	%f64, %f44, %f46, %f63;
	add.s64 	%rd50, %rd63, %rd12;
	ld.global.nc.u32 	%r49, [%rd50];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r49;
  cvt.f32.f16 %f47, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r49;
  cvt.f32.f16 %f48, high;}

	// end inline asm
	fma.rn.f32 	%f65, %f43, %f47, %f170;
	fma.rn.f32 	%f66, %f44, %f48, %f65;
	add.s64 	%rd51, %rd63, %rd9;
	ld.global.nc.u32 	%r51, [%rd51];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r51;
  cvt.f32.f16 %f49, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r51;
  cvt.f32.f16 %f50, high;}

	// end inline asm
	fma.rn.f32 	%f67, %f43, %f49, %f169;
	fma.rn.f32 	%f68, %f44, %f50, %f67;
	add.s64 	%rd52, %rd63, %rd10;
	ld.global.nc.u32 	%r53, [%rd52];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r53;
  cvt.f32.f16 %f51, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r53;
  cvt.f32.f16 %f52, high;}

	// end inline asm
	fma.rn.f32 	%f69, %f43, %f51, %f168;
	fma.rn.f32 	%f70, %f44, %f52, %f69;
	ld.global.nc.u32 	%r55, [%rd62];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r55;
  cvt.f32.f16 %f53, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r55;
  cvt.f32.f16 %f54, high;}

	// end inline asm
	ld.global.nc.u32 	%r57, [%rd49+768];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r57;
  cvt.f32.f16 %f55, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r57;
  cvt.f32.f16 %f56, high;}

	// end inline asm
	fma.rn.f32 	%f71, %f53, %f55, %f64;
	fma.rn.f32 	%f171, %f54, %f56, %f71;
	add.s64 	%rd53, %rd63, %rd8;
	ld.global.nc.u32 	%r59, [%rd53];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r59;
  cvt.f32.f16 %f57, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r59;
  cvt.f32.f16 %f58, high;}

	// end inline asm
	fma.rn.f32 	%f72, %f53, %f57, %f66;
	fma.rn.f32 	%f170, %f54, %f58, %f72;
	ld.global.nc.u32 	%r61, [%rd51+768];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r61;
  cvt.f32.f16 %f59, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r61;
  cvt.f32.f16 %f60, high;}

	// end inline asm
	fma.rn.f32 	%f73, %f53, %f59, %f68;
	fma.rn.f32 	%f169, %f54, %f60, %f73;
	ld.global.nc.u32 	%r63, [%rd52+768];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r63;
  cvt.f32.f16 %f61, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r63;
  cvt.f32.f16 %f62, high;}

	// end inline asm
	fma.rn.f32 	%f74, %f53, %f61, %f70;
	fma.rn.f32 	%f168, %f54, %f62, %f74;
	add.s64 	%rd63, %rd63, 1536;
	add.s64 	%rd62, %rd62, 1536;
	add.s32 	%r210, %r210, 384;
	setp.lt.s32 	%p7, %r210, %r11;
	@%p7 bra 	$L__BB107_7;

	st.local.v4.f32 	[%rd2], {%f171, %f170, %f169, %f168};

$L__BB107_9:
	shr.s32 	%r65, %r3, 31;
	shr.u32 	%r66, %r65, 27;
	add.s32 	%r67, %r3, %r66;
	shr.s32 	%r68, %r67, 5;
	shl.b32 	%r69, %r68, 2;
	add.s32 	%r10, %r24, %r69;
	mov.u32 	%r71, 2;
	mov.b32 	%r72, %f171;
	mov.u32 	%r73, 31;
	mov.u32 	%r74, 16;
	mov.u32 	%r75, -1;
	shfl.sync.bfly.b32 	%r76|%p8, %r72, %r74, %r73, %r75;
	mov.b32 	%f75, %r76;
	add.f32 	%f76, %f171, %f75;
	mov.b32 	%r77, %f76;
	mov.u32 	%r78, 8;
	shfl.sync.bfly.b32 	%r79|%p9, %r77, %r78, %r73, %r75;
	mov.b32 	%f77, %r79;
	add.f32 	%f78, %f76, %f77;
	mov.b32 	%r80, %f78;
	mov.u32 	%r81, 4;
	shfl.sync.bfly.b32 	%r82|%p10, %r80, %r81, %r73, %r75;
	mov.b32 	%f79, %r82;
	add.f32 	%f80, %f78, %f79;
	mov.b32 	%r83, %f80;
	shfl.sync.bfly.b32 	%r84|%p11, %r83, %r71, %r73, %r75;
	mov.b32 	%f81, %r84;
	add.f32 	%f82, %f80, %f81;
	mov.b32 	%r85, %f82;
	mov.u32 	%r86, 1;
	shfl.sync.bfly.b32 	%r87|%p12, %r85, %r86, %r73, %r75;
	mov.b32 	%f83, %r87;
	add.f32 	%f84, %f82, %f83;
	st.local.f32 	[%rd2], %f84;
	st.shared.f32 	[%r10], %f84;
	bar.sync 	0;
	@%p1 bra 	$L__BB107_11;

	ld.shared.f32 	%f85, [%r4];
	mov.b32 	%r88, %f85;
	shfl.sync.bfly.b32 	%r92|%p14, %r88, %r74, %r73, %r75;
	mov.b32 	%f86, %r92;
	add.f32 	%f87, %f85, %f86;
	mov.b32 	%r93, %f87;
	shfl.sync.bfly.b32 	%r95|%p15, %r93, %r78, %r73, %r75;
	mov.b32 	%f88, %r95;
	add.f32 	%f89, %f87, %f88;
	mov.b32 	%r96, %f89;
	shfl.sync.bfly.b32 	%r98|%p16, %r96, %r81, %r73, %r75;
	mov.b32 	%f90, %r98;
	add.f32 	%f91, %f89, %f90;
	mov.b32 	%r99, %f91;
	shfl.sync.bfly.b32 	%r101|%p17, %r99, %r71, %r73, %r75;
	mov.b32 	%f92, %r101;
	add.f32 	%f93, %f91, %f92;
	mov.b32 	%r102, %f93;
	shfl.sync.bfly.b32 	%r104|%p18, %r102, %r86, %r73, %r75;
	mov.b32 	%f94, %r104;
	add.f32 	%f95, %f93, %f94;
	st.local.f32 	[%rd2], %f95;

$L__BB107_11:
	bar.sync 	0;
	mov.b32 	%r105, %f170;
	shfl.sync.bfly.b32 	%r109|%p20, %r105, %r74, %r73, %r75;
	mov.b32 	%f96, %r109;
	add.f32 	%f97, %f170, %f96;
	mov.b32 	%r110, %f97;
	shfl.sync.bfly.b32 	%r112|%p21, %r110, %r78, %r73, %r75;
	mov.b32 	%f98, %r112;
	add.f32 	%f99, %f97, %f98;
	mov.b32 	%r113, %f99;
	shfl.sync.bfly.b32 	%r115|%p22, %r113, %r81, %r73, %r75;
	mov.b32 	%f100, %r115;
	add.f32 	%f101, %f99, %f100;
	mov.b32 	%r116, %f101;
	shfl.sync.bfly.b32 	%r118|%p23, %r116, %r71, %r73, %r75;
	mov.b32 	%f102, %r118;
	add.f32 	%f103, %f101, %f102;
	mov.b32 	%r119, %f103;
	shfl.sync.bfly.b32 	%r121|%p24, %r119, %r86, %r73, %r75;
	mov.b32 	%f104, %r121;
	add.f32 	%f105, %f103, %f104;
	st.local.f32 	[%rd2+4], %f105;
	st.shared.f32 	[%r10], %f105;
	bar.sync 	0;
	@%p1 bra 	$L__BB107_13;

	ld.shared.f32 	%f106, [%r4];
	mov.b32 	%r122, %f106;
	mov.u32 	%r123, 31;
	mov.u32 	%r124, 16;
	mov.u32 	%r125, -1;
	shfl.sync.bfly.b32 	%r126|%p25, %r122, %r124, %r123, %r125;
	mov.b32 	%f107, %r126;
	add.f32 	%f108, %f106, %f107;
	mov.b32 	%r127, %f108;
	mov.u32 	%r128, 8;
	shfl.sync.bfly.b32 	%r129|%p26, %r127, %r128, %r123, %r125;
	mov.b32 	%f109, %r129;
	add.f32 	%f110, %f108, %f109;
	mov.b32 	%r130, %f110;
	mov.u32 	%r131, 4;
	shfl.sync.bfly.b32 	%r132|%p27, %r130, %r131, %r123, %r125;
	mov.b32 	%f111, %r132;
	add.f32 	%f112, %f110, %f111;
	mov.b32 	%r133, %f112;
	mov.u32 	%r134, 2;
	shfl.sync.bfly.b32 	%r135|%p28, %r133, %r134, %r123, %r125;
	mov.b32 	%f113, %r135;
	add.f32 	%f114, %f112, %f113;
	mov.b32 	%r136, %f114;
	mov.u32 	%r137, 1;
	shfl.sync.bfly.b32 	%r138|%p29, %r136, %r137, %r123, %r125;
	mov.b32 	%f115, %r138;
	add.f32 	%f116, %f114, %f115;
	st.local.f32 	[%rd2+4], %f116;

$L__BB107_13:
	bar.sync 	0;
	mov.b32 	%r139, %f169;
	mov.u32 	%r140, 31;
	mov.u32 	%r141, 16;
	mov.u32 	%r142, -1;
	shfl.sync.bfly.b32 	%r143|%p31, %r139, %r141, %r140, %r142;
	mov.b32 	%f117, %r143;
	add.f32 	%f118, %f169, %f117;
	mov.b32 	%r144, %f118;
	mov.u32 	%r145, 8;
	shfl.sync.bfly.b32 	%r146|%p32, %r144, %r145, %r140, %r142;
	mov.b32 	%f119, %r146;
	add.f32 	%f120, %f118, %f119;
	mov.b32 	%r147, %f120;
	mov.u32 	%r148, 4;
	shfl.sync.bfly.b32 	%r149|%p33, %r147, %r148, %r140, %r142;
	mov.b32 	%f121, %r149;
	add.f32 	%f122, %f120, %f121;
	mov.b32 	%r150, %f122;
	mov.u32 	%r151, 2;
	shfl.sync.bfly.b32 	%r152|%p34, %r150, %r151, %r140, %r142;
	mov.b32 	%f123, %r152;
	add.f32 	%f124, %f122, %f123;
	mov.b32 	%r153, %f124;
	mov.u32 	%r154, 1;
	shfl.sync.bfly.b32 	%r155|%p35, %r153, %r154, %r140, %r142;
	mov.b32 	%f125, %r155;
	add.f32 	%f126, %f124, %f125;
	st.local.f32 	[%rd2+8], %f126;
	st.shared.f32 	[%r10], %f126;
	bar.sync 	0;
	@%p1 bra 	$L__BB107_15;

	ld.shared.f32 	%f127, [%r4];
	mov.b32 	%r156, %f127;
	shfl.sync.bfly.b32 	%r160|%p36, %r156, %r141, %r140, %r142;
	mov.b32 	%f128, %r160;
	add.f32 	%f129, %f127, %f128;
	mov.b32 	%r161, %f129;
	shfl.sync.bfly.b32 	%r163|%p37, %r161, %r145, %r140, %r142;
	mov.b32 	%f130, %r163;
	add.f32 	%f131, %f129, %f130;
	mov.b32 	%r164, %f131;
	shfl.sync.bfly.b32 	%r166|%p38, %r164, %r148, %r140, %r142;
	mov.b32 	%f132, %r166;
	add.f32 	%f133, %f131, %f132;
	mov.b32 	%r167, %f133;
	shfl.sync.bfly.b32 	%r169|%p39, %r167, %r151, %r140, %r142;
	mov.b32 	%f134, %r169;
	add.f32 	%f135, %f133, %f134;
	mov.b32 	%r170, %f135;
	shfl.sync.bfly.b32 	%r172|%p40, %r170, %r154, %r140, %r142;
	mov.b32 	%f136, %r172;
	add.f32 	%f137, %f135, %f136;
	st.local.f32 	[%rd2+8], %f137;

$L__BB107_15:
	bar.sync 	0;
	mov.b32 	%r173, %f168;
	shfl.sync.bfly.b32 	%r177|%p42, %r173, %r141, %r140, %r142;
	mov.b32 	%f138, %r177;
	add.f32 	%f139, %f168, %f138;
	mov.b32 	%r178, %f139;
	shfl.sync.bfly.b32 	%r180|%p43, %r178, %r145, %r140, %r142;
	mov.b32 	%f140, %r180;
	add.f32 	%f141, %f139, %f140;
	mov.b32 	%r181, %f141;
	shfl.sync.bfly.b32 	%r183|%p44, %r181, %r148, %r140, %r142;
	mov.b32 	%f142, %r183;
	add.f32 	%f143, %f141, %f142;
	mov.b32 	%r184, %f143;
	shfl.sync.bfly.b32 	%r186|%p45, %r184, %r151, %r140, %r142;
	mov.b32 	%f144, %r186;
	add.f32 	%f145, %f143, %f144;
	mov.b32 	%r187, %f145;
	shfl.sync.bfly.b32 	%r189|%p46, %r187, %r154, %r140, %r142;
	mov.b32 	%f146, %r189;
	add.f32 	%f147, %f145, %f146;
	st.local.f32 	[%rd2+12], %f147;
	st.shared.f32 	[%r10], %f147;
	bar.sync 	0;
	@%p1 bra 	$L__BB107_17;

	ld.shared.f32 	%f148, [%r4];
	mov.b32 	%r190, %f148;
	mov.u32 	%r191, 31;
	mov.u32 	%r192, 16;
	mov.u32 	%r193, -1;
	shfl.sync.bfly.b32 	%r194|%p47, %r190, %r192, %r191, %r193;
	mov.b32 	%f149, %r194;
	add.f32 	%f150, %f148, %f149;
	mov.b32 	%r195, %f150;
	mov.u32 	%r196, 8;
	shfl.sync.bfly.b32 	%r197|%p48, %r195, %r196, %r191, %r193;
	mov.b32 	%f151, %r197;
	add.f32 	%f152, %f150, %f151;
	mov.b32 	%r198, %f152;
	mov.u32 	%r199, 4;
	shfl.sync.bfly.b32 	%r200|%p49, %r198, %r199, %r191, %r193;
	mov.b32 	%f153, %r200;
	add.f32 	%f154, %f152, %f153;
	mov.b32 	%r201, %f154;
	mov.u32 	%r202, 2;
	shfl.sync.bfly.b32 	%r203|%p50, %r201, %r202, %r191, %r193;
	mov.b32 	%f155, %r203;
	add.f32 	%f156, %f154, %f155;
	mov.b32 	%r204, %f156;
	mov.u32 	%r205, 1;
	shfl.sync.bfly.b32 	%r206|%p51, %r204, %r205, %r191, %r193;
	mov.b32 	%f157, %r206;
	add.f32 	%f158, %f156, %f157;
	st.local.f32 	[%rd2+12], %f158;

$L__BB107_17:
	bar.sync 	0;
	setp.gt.s32 	%p52, %r3, 3;
	@%p52 bra 	$L__BB107_19;

	mad.lo.s32 	%r207, %r3, %r13, %r2;
	cvt.s64.s32 	%rd54, %r207;
	mul.lo.s32 	%r208, %r1, %r14;
	cvt.s64.s32 	%rd55, %r208;
	add.s64 	%rd56, %rd55, %rd54;
	mul.wide.s32 	%rd57, %r3, 4;
	add.s64 	%rd58, %rd2, %rd57;
	ld.local.f32 	%f159, [%rd58];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f159;}

	// end inline asm
	cvta.to.global.u64 	%rd59, %rd18;
	shl.b64 	%rd60, %rd56, 1;
	add.s64 	%rd61, %rd59, %rd60;
	st.global.u16 	[%rd61], %rs1;

$L__BB107_19:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_5_bs_192
.visible .entry ggml_matvec_f16_ncols_5_bs_192(
	.param .u64 ggml_matvec_f16_ncols_5_bs_192_param_0,
	.param .u64 ggml_matvec_f16_ncols_5_bs_192_param_1,
	.param .u64 ggml_matvec_f16_ncols_5_bs_192_param_2,
	.param .u32 ggml_matvec_f16_ncols_5_bs_192_param_3,
	.param .u32 ggml_matvec_f16_ncols_5_bs_192_param_4,
	.param .u32 ggml_matvec_f16_ncols_5_bs_192_param_5,
	.param .u32 ggml_matvec_f16_ncols_5_bs_192_param_6,
	.param .u32 ggml_matvec_f16_ncols_5_bs_192_param_7,
	.param .u32 ggml_matvec_f16_ncols_5_bs_192_param_8,
	.param .u32 ggml_matvec_f16_ncols_5_bs_192_param_9,
	.param .u32 ggml_matvec_f16_ncols_5_bs_192_param_10,
	.param .u32 ggml_matvec_f16_ncols_5_bs_192_param_11
)
{
	.local .align 4 .b8 	__local_depot108[20];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<64>;
	.reg .b16 	%rs<2>;
	.reg .f32 	%f<213>;
	.reg .b32 	%r<255>;
	.reg .b64 	%rd<67>;


	mov.u64 	%SPL, __local_depot108;
	ld.param.u64 	%rd19, [ggml_matvec_f16_ncols_5_bs_192_param_0];
	ld.param.u64 	%rd20, [ggml_matvec_f16_ncols_5_bs_192_param_1];
	ld.param.u64 	%rd18, [ggml_matvec_f16_ncols_5_bs_192_param_2];
	ld.param.u32 	%r11, [ggml_matvec_f16_ncols_5_bs_192_param_3];
	ld.param.u32 	%r15, [ggml_matvec_f16_ncols_5_bs_192_param_5];
	ld.param.u32 	%r12, [ggml_matvec_f16_ncols_5_bs_192_param_6];
	ld.param.u32 	%r13, [ggml_matvec_f16_ncols_5_bs_192_param_7];
	ld.param.u32 	%r16, [ggml_matvec_f16_ncols_5_bs_192_param_8];
	ld.param.u32 	%r17, [ggml_matvec_f16_ncols_5_bs_192_param_9];
	ld.param.u32 	%r18, [ggml_matvec_f16_ncols_5_bs_192_param_10];
	ld.param.u32 	%r14, [ggml_matvec_f16_ncols_5_bs_192_param_11];
	cvta.to.global.u64 	%rd66, %rd20;
	add.u64 	%rd2, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r19, %r1, %r16;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r20, %r2, %r15;
	mad.lo.s32 	%r21, %r19, %r17, %r20;
	cvt.s64.s32 	%rd3, %r21;
	cvta.to.global.u64 	%rd4, %rd19;
	mul.lo.s32 	%r22, %r1, %r18;
	cvt.s64.s32 	%rd5, %r22;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r23, %r3, 2;
	mov.u32 	%r24, data_mmv;
	add.s32 	%r4, %r24, %r23;
	@%p1 bra 	$L__BB108_2;

	mov.u32 	%r25, 0;
	st.shared.u32 	[%r4], %r25;

$L__BB108_2:
	bar.sync 	0;
	mov.f32 	%f208, 0f00000000;
	mov.u32 	%r26, 0;
	st.local.u32 	[%rd2], %r26;
	st.local.u32 	[%rd2+4], %r26;
	st.local.u32 	[%rd2+8], %r26;
	st.local.u32 	[%rd2+12], %r26;
	st.local.u32 	[%rd2+16], %r26;
	setp.ge.s32 	%p2, %r3, %r11;
	mov.f32 	%f209, %f208;
	mov.f32 	%f210, %f208;
	mov.f32 	%f211, %f208;
	mov.f32 	%f212, %f208;
	@%p2 bra 	$L__BB108_9;

	not.b32 	%r27, %r3;
	add.s32 	%r5, %r27, %r11;
	mul.wide.u32 	%rd22, %r5, -1431655765;
	shr.u64 	%rd23, %rd22, 39;
	and.b64  	%rd24, %rd23, 1;
	setp.eq.b64 	%p3, %rd24, 1;
	mov.pred 	%p4, 0;
	xor.pred  	%p5, %p3, %p4;
	mov.f32 	%f208, 0f00000000;
	mov.u32 	%r254, %r3;
	@%p5 bra 	$L__BB108_5;

	shl.b64 	%rd25, %rd5, 1;
	add.s64 	%rd26, %rd66, %rd25;
	shl.b64 	%rd27, %rd3, 1;
	add.s64 	%rd28, %rd4, %rd27;
	mul.wide.s32 	%rd29, %r3, 4;
	add.s64 	%rd30, %rd28, %rd29;
	ld.global.nc.u32 	%r28, [%rd30];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r28;
  cvt.f32.f16 %f36, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r28;
  cvt.f32.f16 %f37, high;}

	// end inline asm
	add.s64 	%rd31, %rd26, %rd29;
	ld.global.nc.u32 	%r30, [%rd31];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r30;
  cvt.f32.f16 %f38, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r30;
  cvt.f32.f16 %f39, high;}

	// end inline asm
	fma.rn.f32 	%f48, %f36, %f38, 0f00000000;
	fma.rn.f32 	%f212, %f37, %f39, %f48;
	st.local.f32 	[%rd2], %f212;
	mul.wide.s32 	%rd32, %r12, 4;
	add.s64 	%rd33, %rd31, %rd32;
	ld.global.nc.u32 	%r32, [%rd33];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r32;
  cvt.f32.f16 %f40, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r32;
  cvt.f32.f16 %f41, high;}

	// end inline asm
	fma.rn.f32 	%f49, %f36, %f40, 0f00000000;
	fma.rn.f32 	%f211, %f37, %f41, %f49;
	st.local.f32 	[%rd2+4], %f211;
	add.s32 	%r40, %r3, %r12;
	add.s32 	%r41, %r40, %r12;
	shl.b32 	%r42, %r12, 1;
	mul.wide.s32 	%rd34, %r42, 4;
	add.s64 	%rd35, %rd31, %rd34;
	ld.global.nc.u32 	%r34, [%rd35];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r34;
  cvt.f32.f16 %f42, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r34;
  cvt.f32.f16 %f43, high;}

	// end inline asm
	fma.rn.f32 	%f50, %f36, %f42, 0f00000000;
	fma.rn.f32 	%f210, %f37, %f43, %f50;
	st.local.f32 	[%rd2+8], %f210;
	add.s32 	%r43, %r41, %r12;
	mul.wide.s32 	%rd36, %r43, 4;
	add.s64 	%rd37, %rd26, %rd36;
	ld.global.nc.u32 	%r36, [%rd37];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r36;
  cvt.f32.f16 %f44, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r36;
  cvt.f32.f16 %f45, high;}

	// end inline asm
	fma.rn.f32 	%f51, %f36, %f44, 0f00000000;
	fma.rn.f32 	%f209, %f37, %f45, %f51;
	st.local.f32 	[%rd2+12], %f209;
	add.s64 	%rd38, %rd35, %rd34;
	ld.global.nc.u32 	%r38, [%rd38];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r38;
  cvt.f32.f16 %f46, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r38;
  cvt.f32.f16 %f47, high;}

	// end inline asm
	fma.rn.f32 	%f52, %f36, %f46, 0f00000000;
	fma.rn.f32 	%f208, %f37, %f47, %f52;
	st.local.f32 	[%rd2+16], %f208;
	add.s32 	%r254, %r3, 192;

$L__BB108_5:
	setp.lt.u32 	%p6, %r5, 192;
	@%p6 bra 	$L__BB108_9;

	add.s32 	%r44, %r254, %r12;
	add.s32 	%r45, %r44, 192;
	mul.wide.s32 	%rd39, %r45, 4;
	shl.b64 	%rd40, %rd5, 1;
	add.s64 	%rd7, %rd39, %rd40;
	shl.b32 	%r46, %r12, 1;
	add.s32 	%r47, %r254, %r46;
	mad.lo.s32 	%r48, %r12, 3, %r254;
	shl.b32 	%r49, %r12, 2;
	add.s32 	%r50, %r254, %r49;
	mul.wide.s32 	%rd41, %r47, 4;
	add.s64 	%rd8, %rd41, %rd40;
	mul.wide.s32 	%rd42, %r48, 4;
	add.s64 	%rd9, %rd42, %rd40;
	mul.wide.s32 	%rd43, %r50, 4;
	add.s64 	%rd10, %rd43, %rd40;
	mul.wide.s32 	%rd44, %r254, 2;
	add.s64 	%rd45, %rd44, %rd3;
	shl.b64 	%rd46, %rd45, 1;
	add.s64 	%rd47, %rd4, %rd46;
	add.s64 	%rd65, %rd47, 768;
	mul.wide.s32 	%rd48, %r254, 4;
	mul.wide.s32 	%rd49, %r12, 4;
	add.s64 	%rd50, %rd48, %rd49;
	add.s64 	%rd12, %rd50, %rd40;
	add.s64 	%rd13, %rd48, %rd40;

$L__BB108_7:
	ld.global.nc.u32 	%r51, [%rd65+-768];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r51;
  cvt.f32.f16 %f53, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r51;
  cvt.f32.f16 %f54, high;}

	// end inline asm
	add.s64 	%rd51, %rd66, %rd13;
	ld.global.nc.u32 	%r53, [%rd51];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r53;
  cvt.f32.f16 %f55, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r53;
  cvt.f32.f16 %f56, high;}

	// end inline asm
	fma.rn.f32 	%f77, %f53, %f55, %f212;
	fma.rn.f32 	%f78, %f54, %f56, %f77;
	add.s64 	%rd52, %rd66, %rd12;
	ld.global.nc.u32 	%r55, [%rd52];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r55;
  cvt.f32.f16 %f57, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r55;
  cvt.f32.f16 %f58, high;}

	// end inline asm
	fma.rn.f32 	%f79, %f53, %f57, %f211;
	fma.rn.f32 	%f80, %f54, %f58, %f79;
	add.s64 	%rd53, %rd66, %rd8;
	ld.global.nc.u32 	%r57, [%rd53];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r57;
  cvt.f32.f16 %f59, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r57;
  cvt.f32.f16 %f60, high;}

	// end inline asm
	fma.rn.f32 	%f81, %f53, %f59, %f210;
	fma.rn.f32 	%f82, %f54, %f60, %f81;
	add.s64 	%rd54, %rd66, %rd9;
	ld.global.nc.u32 	%r59, [%rd54];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r59;
  cvt.f32.f16 %f61, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r59;
  cvt.f32.f16 %f62, high;}

	// end inline asm
	fma.rn.f32 	%f83, %f53, %f61, %f209;
	fma.rn.f32 	%f84, %f54, %f62, %f83;
	add.s64 	%rd55, %rd66, %rd10;
	ld.global.nc.u32 	%r61, [%rd55];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r61;
  cvt.f32.f16 %f63, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r61;
  cvt.f32.f16 %f64, high;}

	// end inline asm
	fma.rn.f32 	%f85, %f53, %f63, %f208;
	fma.rn.f32 	%f86, %f54, %f64, %f85;
	ld.global.nc.u32 	%r63, [%rd65];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r63;
  cvt.f32.f16 %f65, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r63;
  cvt.f32.f16 %f66, high;}

	// end inline asm
	ld.global.nc.u32 	%r65, [%rd51+768];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r65;
  cvt.f32.f16 %f67, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r65;
  cvt.f32.f16 %f68, high;}

	// end inline asm
	fma.rn.f32 	%f87, %f65, %f67, %f78;
	fma.rn.f32 	%f212, %f66, %f68, %f87;
	add.s64 	%rd56, %rd66, %rd7;
	ld.global.nc.u32 	%r67, [%rd56];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r67;
  cvt.f32.f16 %f69, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r67;
  cvt.f32.f16 %f70, high;}

	// end inline asm
	fma.rn.f32 	%f88, %f65, %f69, %f80;
	fma.rn.f32 	%f211, %f66, %f70, %f88;
	ld.global.nc.u32 	%r69, [%rd53+768];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r69;
  cvt.f32.f16 %f71, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r69;
  cvt.f32.f16 %f72, high;}

	// end inline asm
	fma.rn.f32 	%f89, %f65, %f71, %f82;
	fma.rn.f32 	%f210, %f66, %f72, %f89;
	ld.global.nc.u32 	%r71, [%rd54+768];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r71;
  cvt.f32.f16 %f73, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r71;
  cvt.f32.f16 %f74, high;}

	// end inline asm
	fma.rn.f32 	%f90, %f65, %f73, %f84;
	fma.rn.f32 	%f209, %f66, %f74, %f90;
	ld.global.nc.u32 	%r73, [%rd55+768];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r73;
  cvt.f32.f16 %f75, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r73;
  cvt.f32.f16 %f76, high;}

	// end inline asm
	fma.rn.f32 	%f91, %f65, %f75, %f86;
	fma.rn.f32 	%f208, %f66, %f76, %f91;
	add.s64 	%rd66, %rd66, 1536;
	add.s64 	%rd65, %rd65, 1536;
	add.s32 	%r254, %r254, 384;
	setp.lt.s32 	%p7, %r254, %r11;
	@%p7 bra 	$L__BB108_7;

	st.local.f32 	[%rd2], %f212;
	st.local.f32 	[%rd2+4], %f211;
	st.local.f32 	[%rd2+8], %f210;
	st.local.f32 	[%rd2+12], %f209;
	st.local.f32 	[%rd2+16], %f208;

$L__BB108_9:
	shr.s32 	%r75, %r3, 31;
	shr.u32 	%r76, %r75, 27;
	add.s32 	%r77, %r3, %r76;
	shr.s32 	%r78, %r77, 5;
	shl.b32 	%r79, %r78, 2;
	add.s32 	%r10, %r24, %r79;
	mov.u32 	%r81, 2;
	mov.b32 	%r82, %f212;
	mov.u32 	%r83, 31;
	mov.u32 	%r84, 16;
	mov.u32 	%r85, -1;
	shfl.sync.bfly.b32 	%r86|%p8, %r82, %r84, %r83, %r85;
	mov.b32 	%f92, %r86;
	add.f32 	%f93, %f212, %f92;
	mov.b32 	%r87, %f93;
	mov.u32 	%r88, 8;
	shfl.sync.bfly.b32 	%r89|%p9, %r87, %r88, %r83, %r85;
	mov.b32 	%f94, %r89;
	add.f32 	%f95, %f93, %f94;
	mov.b32 	%r90, %f95;
	mov.u32 	%r91, 4;
	shfl.sync.bfly.b32 	%r92|%p10, %r90, %r91, %r83, %r85;
	mov.b32 	%f96, %r92;
	add.f32 	%f97, %f95, %f96;
	mov.b32 	%r93, %f97;
	shfl.sync.bfly.b32 	%r94|%p11, %r93, %r81, %r83, %r85;
	mov.b32 	%f98, %r94;
	add.f32 	%f99, %f97, %f98;
	mov.b32 	%r95, %f99;
	mov.u32 	%r96, 1;
	shfl.sync.bfly.b32 	%r97|%p12, %r95, %r96, %r83, %r85;
	mov.b32 	%f100, %r97;
	add.f32 	%f101, %f99, %f100;
	st.local.f32 	[%rd2], %f101;
	st.shared.f32 	[%r10], %f101;
	bar.sync 	0;
	@%p1 bra 	$L__BB108_11;

	ld.shared.f32 	%f102, [%r4];
	mov.b32 	%r98, %f102;
	shfl.sync.bfly.b32 	%r102|%p14, %r98, %r84, %r83, %r85;
	mov.b32 	%f103, %r102;
	add.f32 	%f104, %f102, %f103;
	mov.b32 	%r103, %f104;
	shfl.sync.bfly.b32 	%r105|%p15, %r103, %r88, %r83, %r85;
	mov.b32 	%f105, %r105;
	add.f32 	%f106, %f104, %f105;
	mov.b32 	%r106, %f106;
	shfl.sync.bfly.b32 	%r108|%p16, %r106, %r91, %r83, %r85;
	mov.b32 	%f107, %r108;
	add.f32 	%f108, %f106, %f107;
	mov.b32 	%r109, %f108;
	shfl.sync.bfly.b32 	%r111|%p17, %r109, %r81, %r83, %r85;
	mov.b32 	%f109, %r111;
	add.f32 	%f110, %f108, %f109;
	mov.b32 	%r112, %f110;
	shfl.sync.bfly.b32 	%r114|%p18, %r112, %r96, %r83, %r85;
	mov.b32 	%f111, %r114;
	add.f32 	%f112, %f110, %f111;
	st.local.f32 	[%rd2], %f112;

$L__BB108_11:
	bar.sync 	0;
	mov.b32 	%r115, %f211;
	shfl.sync.bfly.b32 	%r119|%p20, %r115, %r84, %r83, %r85;
	mov.b32 	%f113, %r119;
	add.f32 	%f114, %f211, %f113;
	mov.b32 	%r120, %f114;
	shfl.sync.bfly.b32 	%r122|%p21, %r120, %r88, %r83, %r85;
	mov.b32 	%f115, %r122;
	add.f32 	%f116, %f114, %f115;
	mov.b32 	%r123, %f116;
	shfl.sync.bfly.b32 	%r125|%p22, %r123, %r91, %r83, %r85;
	mov.b32 	%f117, %r125;
	add.f32 	%f118, %f116, %f117;
	mov.b32 	%r126, %f118;
	shfl.sync.bfly.b32 	%r128|%p23, %r126, %r81, %r83, %r85;
	mov.b32 	%f119, %r128;
	add.f32 	%f120, %f118, %f119;
	mov.b32 	%r129, %f120;
	shfl.sync.bfly.b32 	%r131|%p24, %r129, %r96, %r83, %r85;
	mov.b32 	%f121, %r131;
	add.f32 	%f122, %f120, %f121;
	st.local.f32 	[%rd2+4], %f122;
	st.shared.f32 	[%r10], %f122;
	bar.sync 	0;
	@%p1 bra 	$L__BB108_13;

	ld.shared.f32 	%f123, [%r4];
	mov.b32 	%r132, %f123;
	mov.u32 	%r133, 31;
	mov.u32 	%r134, 16;
	mov.u32 	%r135, -1;
	shfl.sync.bfly.b32 	%r136|%p25, %r132, %r134, %r133, %r135;
	mov.b32 	%f124, %r136;
	add.f32 	%f125, %f123, %f124;
	mov.b32 	%r137, %f125;
	mov.u32 	%r138, 8;
	shfl.sync.bfly.b32 	%r139|%p26, %r137, %r138, %r133, %r135;
	mov.b32 	%f126, %r139;
	add.f32 	%f127, %f125, %f126;
	mov.b32 	%r140, %f127;
	mov.u32 	%r141, 4;
	shfl.sync.bfly.b32 	%r142|%p27, %r140, %r141, %r133, %r135;
	mov.b32 	%f128, %r142;
	add.f32 	%f129, %f127, %f128;
	mov.b32 	%r143, %f129;
	mov.u32 	%r144, 2;
	shfl.sync.bfly.b32 	%r145|%p28, %r143, %r144, %r133, %r135;
	mov.b32 	%f130, %r145;
	add.f32 	%f131, %f129, %f130;
	mov.b32 	%r146, %f131;
	mov.u32 	%r147, 1;
	shfl.sync.bfly.b32 	%r148|%p29, %r146, %r147, %r133, %r135;
	mov.b32 	%f132, %r148;
	add.f32 	%f133, %f131, %f132;
	st.local.f32 	[%rd2+4], %f133;

$L__BB108_13:
	bar.sync 	0;
	mov.b32 	%r149, %f210;
	mov.u32 	%r150, 31;
	mov.u32 	%r151, 16;
	mov.u32 	%r152, -1;
	shfl.sync.bfly.b32 	%r153|%p31, %r149, %r151, %r150, %r152;
	mov.b32 	%f134, %r153;
	add.f32 	%f135, %f210, %f134;
	mov.b32 	%r154, %f135;
	mov.u32 	%r155, 8;
	shfl.sync.bfly.b32 	%r156|%p32, %r154, %r155, %r150, %r152;
	mov.b32 	%f136, %r156;
	add.f32 	%f137, %f135, %f136;
	mov.b32 	%r157, %f137;
	mov.u32 	%r158, 4;
	shfl.sync.bfly.b32 	%r159|%p33, %r157, %r158, %r150, %r152;
	mov.b32 	%f138, %r159;
	add.f32 	%f139, %f137, %f138;
	mov.b32 	%r160, %f139;
	mov.u32 	%r161, 2;
	shfl.sync.bfly.b32 	%r162|%p34, %r160, %r161, %r150, %r152;
	mov.b32 	%f140, %r162;
	add.f32 	%f141, %f139, %f140;
	mov.b32 	%r163, %f141;
	mov.u32 	%r164, 1;
	shfl.sync.bfly.b32 	%r165|%p35, %r163, %r164, %r150, %r152;
	mov.b32 	%f142, %r165;
	add.f32 	%f143, %f141, %f142;
	st.local.f32 	[%rd2+8], %f143;
	st.shared.f32 	[%r10], %f143;
	bar.sync 	0;
	@%p1 bra 	$L__BB108_15;

	ld.shared.f32 	%f144, [%r4];
	mov.b32 	%r166, %f144;
	shfl.sync.bfly.b32 	%r170|%p36, %r166, %r151, %r150, %r152;
	mov.b32 	%f145, %r170;
	add.f32 	%f146, %f144, %f145;
	mov.b32 	%r171, %f146;
	shfl.sync.bfly.b32 	%r173|%p37, %r171, %r155, %r150, %r152;
	mov.b32 	%f147, %r173;
	add.f32 	%f148, %f146, %f147;
	mov.b32 	%r174, %f148;
	shfl.sync.bfly.b32 	%r176|%p38, %r174, %r158, %r150, %r152;
	mov.b32 	%f149, %r176;
	add.f32 	%f150, %f148, %f149;
	mov.b32 	%r177, %f150;
	shfl.sync.bfly.b32 	%r179|%p39, %r177, %r161, %r150, %r152;
	mov.b32 	%f151, %r179;
	add.f32 	%f152, %f150, %f151;
	mov.b32 	%r180, %f152;
	shfl.sync.bfly.b32 	%r182|%p40, %r180, %r164, %r150, %r152;
	mov.b32 	%f153, %r182;
	add.f32 	%f154, %f152, %f153;
	st.local.f32 	[%rd2+8], %f154;

$L__BB108_15:
	bar.sync 	0;
	mov.b32 	%r183, %f209;
	shfl.sync.bfly.b32 	%r187|%p42, %r183, %r151, %r150, %r152;
	mov.b32 	%f155, %r187;
	add.f32 	%f156, %f209, %f155;
	mov.b32 	%r188, %f156;
	shfl.sync.bfly.b32 	%r190|%p43, %r188, %r155, %r150, %r152;
	mov.b32 	%f157, %r190;
	add.f32 	%f158, %f156, %f157;
	mov.b32 	%r191, %f158;
	shfl.sync.bfly.b32 	%r193|%p44, %r191, %r158, %r150, %r152;
	mov.b32 	%f159, %r193;
	add.f32 	%f160, %f158, %f159;
	mov.b32 	%r194, %f160;
	shfl.sync.bfly.b32 	%r196|%p45, %r194, %r161, %r150, %r152;
	mov.b32 	%f161, %r196;
	add.f32 	%f162, %f160, %f161;
	mov.b32 	%r197, %f162;
	shfl.sync.bfly.b32 	%r199|%p46, %r197, %r164, %r150, %r152;
	mov.b32 	%f163, %r199;
	add.f32 	%f164, %f162, %f163;
	st.local.f32 	[%rd2+12], %f164;
	st.shared.f32 	[%r10], %f164;
	bar.sync 	0;
	@%p1 bra 	$L__BB108_17;

	ld.shared.f32 	%f165, [%r4];
	mov.b32 	%r200, %f165;
	mov.u32 	%r201, 31;
	mov.u32 	%r202, 16;
	mov.u32 	%r203, -1;
	shfl.sync.bfly.b32 	%r204|%p47, %r200, %r202, %r201, %r203;
	mov.b32 	%f166, %r204;
	add.f32 	%f167, %f165, %f166;
	mov.b32 	%r205, %f167;
	mov.u32 	%r206, 8;
	shfl.sync.bfly.b32 	%r207|%p48, %r205, %r206, %r201, %r203;
	mov.b32 	%f168, %r207;
	add.f32 	%f169, %f167, %f168;
	mov.b32 	%r208, %f169;
	mov.u32 	%r209, 4;
	shfl.sync.bfly.b32 	%r210|%p49, %r208, %r209, %r201, %r203;
	mov.b32 	%f170, %r210;
	add.f32 	%f171, %f169, %f170;
	mov.b32 	%r211, %f171;
	mov.u32 	%r212, 2;
	shfl.sync.bfly.b32 	%r213|%p50, %r211, %r212, %r201, %r203;
	mov.b32 	%f172, %r213;
	add.f32 	%f173, %f171, %f172;
	mov.b32 	%r214, %f173;
	mov.u32 	%r215, 1;
	shfl.sync.bfly.b32 	%r216|%p51, %r214, %r215, %r201, %r203;
	mov.b32 	%f174, %r216;
	add.f32 	%f175, %f173, %f174;
	st.local.f32 	[%rd2+12], %f175;

$L__BB108_17:
	bar.sync 	0;
	mov.b32 	%r217, %f208;
	mov.u32 	%r218, 31;
	mov.u32 	%r219, 16;
	mov.u32 	%r220, -1;
	shfl.sync.bfly.b32 	%r221|%p53, %r217, %r219, %r218, %r220;
	mov.b32 	%f176, %r221;
	add.f32 	%f177, %f208, %f176;
	mov.b32 	%r222, %f177;
	mov.u32 	%r223, 8;
	shfl.sync.bfly.b32 	%r224|%p54, %r222, %r223, %r218, %r220;
	mov.b32 	%f178, %r224;
	add.f32 	%f179, %f177, %f178;
	mov.b32 	%r225, %f179;
	mov.u32 	%r226, 4;
	shfl.sync.bfly.b32 	%r227|%p55, %r225, %r226, %r218, %r220;
	mov.b32 	%f180, %r227;
	add.f32 	%f181, %f179, %f180;
	mov.b32 	%r228, %f181;
	mov.u32 	%r229, 2;
	shfl.sync.bfly.b32 	%r230|%p56, %r228, %r229, %r218, %r220;
	mov.b32 	%f182, %r230;
	add.f32 	%f183, %f181, %f182;
	mov.b32 	%r231, %f183;
	mov.u32 	%r232, 1;
	shfl.sync.bfly.b32 	%r233|%p57, %r231, %r232, %r218, %r220;
	mov.b32 	%f184, %r233;
	add.f32 	%f185, %f183, %f184;
	st.local.f32 	[%rd2+16], %f185;
	st.shared.f32 	[%r10], %f185;
	bar.sync 	0;
	@%p1 bra 	$L__BB108_19;

	ld.shared.f32 	%f186, [%r4];
	mov.b32 	%r234, %f186;
	shfl.sync.bfly.b32 	%r238|%p58, %r234, %r219, %r218, %r220;
	mov.b32 	%f187, %r238;
	add.f32 	%f188, %f186, %f187;
	mov.b32 	%r239, %f188;
	shfl.sync.bfly.b32 	%r241|%p59, %r239, %r223, %r218, %r220;
	mov.b32 	%f189, %r241;
	add.f32 	%f190, %f188, %f189;
	mov.b32 	%r242, %f190;
	shfl.sync.bfly.b32 	%r244|%p60, %r242, %r226, %r218, %r220;
	mov.b32 	%f191, %r244;
	add.f32 	%f192, %f190, %f191;
	mov.b32 	%r245, %f192;
	shfl.sync.bfly.b32 	%r247|%p61, %r245, %r229, %r218, %r220;
	mov.b32 	%f193, %r247;
	add.f32 	%f194, %f192, %f193;
	mov.b32 	%r248, %f194;
	shfl.sync.bfly.b32 	%r250|%p62, %r248, %r232, %r218, %r220;
	mov.b32 	%f195, %r250;
	add.f32 	%f196, %f194, %f195;
	st.local.f32 	[%rd2+16], %f196;

$L__BB108_19:
	bar.sync 	0;
	setp.gt.s32 	%p63, %r3, 4;
	@%p63 bra 	$L__BB108_21;

	mad.lo.s32 	%r251, %r3, %r13, %r2;
	cvt.s64.s32 	%rd57, %r251;
	mul.lo.s32 	%r252, %r1, %r14;
	cvt.s64.s32 	%rd58, %r252;
	add.s64 	%rd59, %rd58, %rd57;
	mul.wide.s32 	%rd60, %r3, 4;
	add.s64 	%rd61, %rd2, %rd60;
	ld.local.f32 	%f197, [%rd61];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f197;}

	// end inline asm
	cvta.to.global.u64 	%rd62, %rd18;
	shl.b64 	%rd63, %rd59, 1;
	add.s64 	%rd64, %rd62, %rd63;
	st.global.u16 	[%rd64], %rs1;

$L__BB108_21:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_6_bs_192
.visible .entry ggml_matvec_f16_ncols_6_bs_192(
	.param .u64 ggml_matvec_f16_ncols_6_bs_192_param_0,
	.param .u64 ggml_matvec_f16_ncols_6_bs_192_param_1,
	.param .u64 ggml_matvec_f16_ncols_6_bs_192_param_2,
	.param .u32 ggml_matvec_f16_ncols_6_bs_192_param_3,
	.param .u32 ggml_matvec_f16_ncols_6_bs_192_param_4,
	.param .u32 ggml_matvec_f16_ncols_6_bs_192_param_5,
	.param .u32 ggml_matvec_f16_ncols_6_bs_192_param_6,
	.param .u32 ggml_matvec_f16_ncols_6_bs_192_param_7,
	.param .u32 ggml_matvec_f16_ncols_6_bs_192_param_8,
	.param .u32 ggml_matvec_f16_ncols_6_bs_192_param_9,
	.param .u32 ggml_matvec_f16_ncols_6_bs_192_param_10,
	.param .u32 ggml_matvec_f16_ncols_6_bs_192_param_11
)
{
	.local .align 8 .b8 	__local_depot109[24];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<75>;
	.reg .b16 	%rs<2>;
	.reg .f32 	%f<254>;
	.reg .b32 	%r<293>;
	.reg .b64 	%rd<70>;


	mov.u64 	%SPL, __local_depot109;
	ld.param.u64 	%rd20, [ggml_matvec_f16_ncols_6_bs_192_param_0];
	ld.param.u64 	%rd21, [ggml_matvec_f16_ncols_6_bs_192_param_1];
	ld.param.u64 	%rd19, [ggml_matvec_f16_ncols_6_bs_192_param_2];
	ld.param.u32 	%r11, [ggml_matvec_f16_ncols_6_bs_192_param_3];
	ld.param.u32 	%r15, [ggml_matvec_f16_ncols_6_bs_192_param_5];
	ld.param.u32 	%r12, [ggml_matvec_f16_ncols_6_bs_192_param_6];
	ld.param.u32 	%r13, [ggml_matvec_f16_ncols_6_bs_192_param_7];
	ld.param.u32 	%r16, [ggml_matvec_f16_ncols_6_bs_192_param_8];
	ld.param.u32 	%r17, [ggml_matvec_f16_ncols_6_bs_192_param_9];
	ld.param.u32 	%r18, [ggml_matvec_f16_ncols_6_bs_192_param_10];
	ld.param.u32 	%r14, [ggml_matvec_f16_ncols_6_bs_192_param_11];
	cvta.to.global.u64 	%rd69, %rd21;
	add.u64 	%rd2, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r19, %r1, %r16;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r20, %r2, %r15;
	mad.lo.s32 	%r21, %r19, %r17, %r20;
	cvt.s64.s32 	%rd3, %r21;
	cvta.to.global.u64 	%rd4, %rd20;
	mul.lo.s32 	%r22, %r1, %r18;
	cvt.s64.s32 	%rd5, %r22;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r23, %r3, 2;
	mov.u32 	%r24, data_mmv;
	add.s32 	%r4, %r24, %r23;
	@%p1 bra 	$L__BB109_2;

	mov.u32 	%r25, 0;
	st.shared.u32 	[%r4], %r25;

$L__BB109_2:
	bar.sync 	0;
	mov.f32 	%f248, 0f00000000;
	st.local.v2.f32 	[%rd2], {%f248, %f248};
	st.local.v2.f32 	[%rd2+8], {%f248, %f248};
	st.local.v2.f32 	[%rd2+16], {%f248, %f248};
	setp.ge.s32 	%p2, %r3, %r11;
	mov.f32 	%f249, %f248;
	mov.f32 	%f250, %f248;
	mov.f32 	%f251, %f248;
	mov.f32 	%f252, %f248;
	mov.f32 	%f253, %f248;
	@%p2 bra 	$L__BB109_9;

	not.b32 	%r26, %r3;
	add.s32 	%r5, %r26, %r11;
	mul.wide.u32 	%rd23, %r5, -1431655765;
	shr.u64 	%rd24, %rd23, 39;
	and.b64  	%rd25, %rd24, 1;
	setp.eq.b64 	%p3, %rd25, 1;
	mov.pred 	%p4, 0;
	xor.pred  	%p5, %p3, %p4;
	mov.f32 	%f248, 0f00000000;
	mov.u32 	%r292, %r3;
	@%p5 bra 	$L__BB109_5;

	shl.b64 	%rd26, %rd5, 1;
	add.s64 	%rd27, %rd69, %rd26;
	shl.b64 	%rd28, %rd3, 1;
	add.s64 	%rd29, %rd4, %rd28;
	mul.wide.s32 	%rd30, %r3, 4;
	add.s64 	%rd31, %rd29, %rd30;
	ld.global.nc.u32 	%r27, [%rd31];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r27;
  cvt.f32.f16 %f43, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r27;
  cvt.f32.f16 %f44, high;}

	// end inline asm
	add.s64 	%rd32, %rd27, %rd30;
	ld.global.nc.u32 	%r29, [%rd32];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r29;
  cvt.f32.f16 %f45, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r29;
  cvt.f32.f16 %f46, high;}

	// end inline asm
	fma.rn.f32 	%f57, %f43, %f45, 0f00000000;
	fma.rn.f32 	%f253, %f44, %f46, %f57;
	st.local.f32 	[%rd2], %f253;
	mul.wide.s32 	%rd33, %r12, 4;
	add.s64 	%rd34, %rd32, %rd33;
	ld.global.nc.u32 	%r31, [%rd34];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r31;
  cvt.f32.f16 %f47, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r31;
  cvt.f32.f16 %f48, high;}

	// end inline asm
	fma.rn.f32 	%f58, %f43, %f47, 0f00000000;
	fma.rn.f32 	%f252, %f44, %f48, %f58;
	st.local.f32 	[%rd2+4], %f252;
	add.s32 	%r41, %r3, %r12;
	add.s32 	%r42, %r41, %r12;
	mul.wide.s32 	%rd35, %r42, 4;
	add.s64 	%rd36, %rd27, %rd35;
	ld.global.nc.u32 	%r33, [%rd36];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r33;
  cvt.f32.f16 %f49, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r33;
  cvt.f32.f16 %f50, high;}

	// end inline asm
	fma.rn.f32 	%f59, %f43, %f49, 0f00000000;
	fma.rn.f32 	%f251, %f44, %f50, %f59;
	st.local.f32 	[%rd2+8], %f251;
	add.s64 	%rd37, %rd36, %rd33;
	ld.global.nc.u32 	%r35, [%rd37];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r35;
  cvt.f32.f16 %f51, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r35;
  cvt.f32.f16 %f52, high;}

	// end inline asm
	fma.rn.f32 	%f60, %f43, %f51, 0f00000000;
	fma.rn.f32 	%f250, %f44, %f52, %f60;
	st.local.f32 	[%rd2+12], %f250;
	add.s64 	%rd38, %rd37, %rd33;
	ld.global.nc.u32 	%r37, [%rd38];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r37;
  cvt.f32.f16 %f53, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r37;
  cvt.f32.f16 %f54, high;}

	// end inline asm
	fma.rn.f32 	%f61, %f43, %f53, 0f00000000;
	fma.rn.f32 	%f249, %f44, %f54, %f61;
	st.local.f32 	[%rd2+16], %f249;
	add.s64 	%rd39, %rd38, %rd33;
	ld.global.nc.u32 	%r39, [%rd39];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r39;
  cvt.f32.f16 %f55, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r39;
  cvt.f32.f16 %f56, high;}

	// end inline asm
	fma.rn.f32 	%f62, %f43, %f55, 0f00000000;
	fma.rn.f32 	%f248, %f44, %f56, %f62;
	st.local.f32 	[%rd2+20], %f248;
	add.s32 	%r292, %r3, 192;

$L__BB109_5:
	setp.lt.u32 	%p6, %r5, 192;
	@%p6 bra 	$L__BB109_9;

	add.s32 	%r43, %r292, %r12;
	add.s32 	%r44, %r43, 192;
	mul.wide.s32 	%rd40, %r44, 4;
	shl.b64 	%rd41, %rd5, 1;
	add.s64 	%rd7, %rd40, %rd41;
	shl.b32 	%r45, %r12, 1;
	add.s32 	%r46, %r292, %r45;
	mad.lo.s32 	%r47, %r12, 3, %r292;
	shl.b32 	%r48, %r12, 2;
	add.s32 	%r49, %r292, %r48;
	mad.lo.s32 	%r50, %r12, 5, %r292;
	mul.wide.s32 	%rd42, %r46, 4;
	add.s64 	%rd8, %rd42, %rd41;
	mul.wide.s32 	%rd43, %r47, 4;
	add.s64 	%rd9, %rd43, %rd41;
	mul.wide.s32 	%rd44, %r49, 4;
	add.s64 	%rd10, %rd44, %rd41;
	mul.wide.s32 	%rd45, %r50, 4;
	add.s64 	%rd11, %rd45, %rd41;
	mul.wide.s32 	%rd46, %r292, 2;
	add.s64 	%rd47, %rd46, %rd3;
	shl.b64 	%rd48, %rd47, 1;
	add.s64 	%rd49, %rd4, %rd48;
	add.s64 	%rd68, %rd49, 768;
	mul.wide.s32 	%rd50, %r292, 4;
	mul.wide.s32 	%rd51, %r12, 4;
	add.s64 	%rd52, %rd50, %rd51;
	add.s64 	%rd13, %rd52, %rd41;
	add.s64 	%rd14, %rd50, %rd41;

$L__BB109_7:
	ld.global.nc.u32 	%r51, [%rd68+-768];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r51;
  cvt.f32.f16 %f63, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r51;
  cvt.f32.f16 %f64, high;}

	// end inline asm
	add.s64 	%rd53, %rd69, %rd14;
	ld.global.nc.u32 	%r53, [%rd53];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r53;
  cvt.f32.f16 %f65, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r53;
  cvt.f32.f16 %f66, high;}

	// end inline asm
	fma.rn.f32 	%f91, %f63, %f65, %f253;
	fma.rn.f32 	%f92, %f64, %f66, %f91;
	add.s64 	%rd54, %rd69, %rd13;
	ld.global.nc.u32 	%r55, [%rd54];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r55;
  cvt.f32.f16 %f67, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r55;
  cvt.f32.f16 %f68, high;}

	// end inline asm
	fma.rn.f32 	%f93, %f63, %f67, %f252;
	fma.rn.f32 	%f94, %f64, %f68, %f93;
	add.s64 	%rd55, %rd69, %rd8;
	ld.global.nc.u32 	%r57, [%rd55];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r57;
  cvt.f32.f16 %f69, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r57;
  cvt.f32.f16 %f70, high;}

	// end inline asm
	fma.rn.f32 	%f95, %f63, %f69, %f251;
	fma.rn.f32 	%f96, %f64, %f70, %f95;
	add.s64 	%rd56, %rd69, %rd9;
	ld.global.nc.u32 	%r59, [%rd56];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r59;
  cvt.f32.f16 %f71, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r59;
  cvt.f32.f16 %f72, high;}

	// end inline asm
	fma.rn.f32 	%f97, %f63, %f71, %f250;
	fma.rn.f32 	%f98, %f64, %f72, %f97;
	add.s64 	%rd57, %rd69, %rd10;
	ld.global.nc.u32 	%r61, [%rd57];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r61;
  cvt.f32.f16 %f73, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r61;
  cvt.f32.f16 %f74, high;}

	// end inline asm
	fma.rn.f32 	%f99, %f63, %f73, %f249;
	fma.rn.f32 	%f100, %f64, %f74, %f99;
	add.s64 	%rd58, %rd69, %rd11;
	ld.global.nc.u32 	%r63, [%rd58];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r63;
  cvt.f32.f16 %f75, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r63;
  cvt.f32.f16 %f76, high;}

	// end inline asm
	fma.rn.f32 	%f101, %f63, %f75, %f248;
	fma.rn.f32 	%f102, %f64, %f76, %f101;
	ld.global.nc.u32 	%r65, [%rd68];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r65;
  cvt.f32.f16 %f77, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r65;
  cvt.f32.f16 %f78, high;}

	// end inline asm
	ld.global.nc.u32 	%r67, [%rd53+768];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r67;
  cvt.f32.f16 %f79, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r67;
  cvt.f32.f16 %f80, high;}

	// end inline asm
	fma.rn.f32 	%f103, %f77, %f79, %f92;
	fma.rn.f32 	%f253, %f78, %f80, %f103;
	add.s64 	%rd59, %rd69, %rd7;
	ld.global.nc.u32 	%r69, [%rd59];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r69;
  cvt.f32.f16 %f81, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r69;
  cvt.f32.f16 %f82, high;}

	// end inline asm
	fma.rn.f32 	%f104, %f77, %f81, %f94;
	fma.rn.f32 	%f252, %f78, %f82, %f104;
	ld.global.nc.u32 	%r71, [%rd55+768];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r71;
  cvt.f32.f16 %f83, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r71;
  cvt.f32.f16 %f84, high;}

	// end inline asm
	fma.rn.f32 	%f105, %f77, %f83, %f96;
	fma.rn.f32 	%f251, %f78, %f84, %f105;
	ld.global.nc.u32 	%r73, [%rd56+768];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r73;
  cvt.f32.f16 %f85, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r73;
  cvt.f32.f16 %f86, high;}

	// end inline asm
	fma.rn.f32 	%f106, %f77, %f85, %f98;
	fma.rn.f32 	%f250, %f78, %f86, %f106;
	ld.global.nc.u32 	%r75, [%rd57+768];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r75;
  cvt.f32.f16 %f87, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r75;
  cvt.f32.f16 %f88, high;}

	// end inline asm
	fma.rn.f32 	%f107, %f77, %f87, %f100;
	fma.rn.f32 	%f249, %f78, %f88, %f107;
	ld.global.nc.u32 	%r77, [%rd58+768];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r77;
  cvt.f32.f16 %f89, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r77;
  cvt.f32.f16 %f90, high;}

	// end inline asm
	fma.rn.f32 	%f108, %f77, %f89, %f102;
	fma.rn.f32 	%f248, %f78, %f90, %f108;
	add.s64 	%rd69, %rd69, 1536;
	add.s64 	%rd68, %rd68, 1536;
	add.s32 	%r292, %r292, 384;
	setp.lt.s32 	%p7, %r292, %r11;
	@%p7 bra 	$L__BB109_7;

	st.local.v2.f32 	[%rd2], {%f253, %f252};
	st.local.v2.f32 	[%rd2+8], {%f251, %f250};
	st.local.v2.f32 	[%rd2+16], {%f249, %f248};

$L__BB109_9:
	shr.s32 	%r79, %r3, 31;
	shr.u32 	%r80, %r79, 27;
	add.s32 	%r81, %r3, %r80;
	shr.s32 	%r82, %r81, 5;
	shl.b32 	%r83, %r82, 2;
	add.s32 	%r10, %r24, %r83;
	mov.u32 	%r85, 2;
	mov.b32 	%r86, %f253;
	mov.u32 	%r87, 31;
	mov.u32 	%r88, 16;
	mov.u32 	%r89, -1;
	shfl.sync.bfly.b32 	%r90|%p8, %r86, %r88, %r87, %r89;
	mov.b32 	%f109, %r90;
	add.f32 	%f110, %f253, %f109;
	mov.b32 	%r91, %f110;
	mov.u32 	%r92, 8;
	shfl.sync.bfly.b32 	%r93|%p9, %r91, %r92, %r87, %r89;
	mov.b32 	%f111, %r93;
	add.f32 	%f112, %f110, %f111;
	mov.b32 	%r94, %f112;
	mov.u32 	%r95, 4;
	shfl.sync.bfly.b32 	%r96|%p10, %r94, %r95, %r87, %r89;
	mov.b32 	%f113, %r96;
	add.f32 	%f114, %f112, %f113;
	mov.b32 	%r97, %f114;
	shfl.sync.bfly.b32 	%r98|%p11, %r97, %r85, %r87, %r89;
	mov.b32 	%f115, %r98;
	add.f32 	%f116, %f114, %f115;
	mov.b32 	%r99, %f116;
	mov.u32 	%r100, 1;
	shfl.sync.bfly.b32 	%r101|%p12, %r99, %r100, %r87, %r89;
	mov.b32 	%f117, %r101;
	add.f32 	%f118, %f116, %f117;
	st.local.f32 	[%rd2], %f118;
	st.shared.f32 	[%r10], %f118;
	bar.sync 	0;
	@%p1 bra 	$L__BB109_11;

	ld.shared.f32 	%f119, [%r4];
	mov.b32 	%r102, %f119;
	shfl.sync.bfly.b32 	%r106|%p14, %r102, %r88, %r87, %r89;
	mov.b32 	%f120, %r106;
	add.f32 	%f121, %f119, %f120;
	mov.b32 	%r107, %f121;
	shfl.sync.bfly.b32 	%r109|%p15, %r107, %r92, %r87, %r89;
	mov.b32 	%f122, %r109;
	add.f32 	%f123, %f121, %f122;
	mov.b32 	%r110, %f123;
	shfl.sync.bfly.b32 	%r112|%p16, %r110, %r95, %r87, %r89;
	mov.b32 	%f124, %r112;
	add.f32 	%f125, %f123, %f124;
	mov.b32 	%r113, %f125;
	shfl.sync.bfly.b32 	%r115|%p17, %r113, %r85, %r87, %r89;
	mov.b32 	%f126, %r115;
	add.f32 	%f127, %f125, %f126;
	mov.b32 	%r116, %f127;
	shfl.sync.bfly.b32 	%r118|%p18, %r116, %r100, %r87, %r89;
	mov.b32 	%f128, %r118;
	add.f32 	%f129, %f127, %f128;
	st.local.f32 	[%rd2], %f129;

$L__BB109_11:
	bar.sync 	0;
	mov.b32 	%r119, %f252;
	shfl.sync.bfly.b32 	%r123|%p20, %r119, %r88, %r87, %r89;
	mov.b32 	%f130, %r123;
	add.f32 	%f131, %f252, %f130;
	mov.b32 	%r124, %f131;
	shfl.sync.bfly.b32 	%r126|%p21, %r124, %r92, %r87, %r89;
	mov.b32 	%f132, %r126;
	add.f32 	%f133, %f131, %f132;
	mov.b32 	%r127, %f133;
	shfl.sync.bfly.b32 	%r129|%p22, %r127, %r95, %r87, %r89;
	mov.b32 	%f134, %r129;
	add.f32 	%f135, %f133, %f134;
	mov.b32 	%r130, %f135;
	shfl.sync.bfly.b32 	%r132|%p23, %r130, %r85, %r87, %r89;
	mov.b32 	%f136, %r132;
	add.f32 	%f137, %f135, %f136;
	mov.b32 	%r133, %f137;
	shfl.sync.bfly.b32 	%r135|%p24, %r133, %r100, %r87, %r89;
	mov.b32 	%f138, %r135;
	add.f32 	%f139, %f137, %f138;
	st.local.f32 	[%rd2+4], %f139;
	st.shared.f32 	[%r10], %f139;
	bar.sync 	0;
	@%p1 bra 	$L__BB109_13;

	ld.shared.f32 	%f140, [%r4];
	mov.b32 	%r136, %f140;
	mov.u32 	%r137, 31;
	mov.u32 	%r138, 16;
	mov.u32 	%r139, -1;
	shfl.sync.bfly.b32 	%r140|%p25, %r136, %r138, %r137, %r139;
	mov.b32 	%f141, %r140;
	add.f32 	%f142, %f140, %f141;
	mov.b32 	%r141, %f142;
	mov.u32 	%r142, 8;
	shfl.sync.bfly.b32 	%r143|%p26, %r141, %r142, %r137, %r139;
	mov.b32 	%f143, %r143;
	add.f32 	%f144, %f142, %f143;
	mov.b32 	%r144, %f144;
	mov.u32 	%r145, 4;
	shfl.sync.bfly.b32 	%r146|%p27, %r144, %r145, %r137, %r139;
	mov.b32 	%f145, %r146;
	add.f32 	%f146, %f144, %f145;
	mov.b32 	%r147, %f146;
	mov.u32 	%r148, 2;
	shfl.sync.bfly.b32 	%r149|%p28, %r147, %r148, %r137, %r139;
	mov.b32 	%f147, %r149;
	add.f32 	%f148, %f146, %f147;
	mov.b32 	%r150, %f148;
	mov.u32 	%r151, 1;
	shfl.sync.bfly.b32 	%r152|%p29, %r150, %r151, %r137, %r139;
	mov.b32 	%f149, %r152;
	add.f32 	%f150, %f148, %f149;
	st.local.f32 	[%rd2+4], %f150;

$L__BB109_13:
	bar.sync 	0;
	mov.b32 	%r153, %f251;
	mov.u32 	%r154, 31;
	mov.u32 	%r155, 16;
	mov.u32 	%r156, -1;
	shfl.sync.bfly.b32 	%r157|%p31, %r153, %r155, %r154, %r156;
	mov.b32 	%f151, %r157;
	add.f32 	%f152, %f251, %f151;
	mov.b32 	%r158, %f152;
	mov.u32 	%r159, 8;
	shfl.sync.bfly.b32 	%r160|%p32, %r158, %r159, %r154, %r156;
	mov.b32 	%f153, %r160;
	add.f32 	%f154, %f152, %f153;
	mov.b32 	%r161, %f154;
	mov.u32 	%r162, 4;
	shfl.sync.bfly.b32 	%r163|%p33, %r161, %r162, %r154, %r156;
	mov.b32 	%f155, %r163;
	add.f32 	%f156, %f154, %f155;
	mov.b32 	%r164, %f156;
	mov.u32 	%r165, 2;
	shfl.sync.bfly.b32 	%r166|%p34, %r164, %r165, %r154, %r156;
	mov.b32 	%f157, %r166;
	add.f32 	%f158, %f156, %f157;
	mov.b32 	%r167, %f158;
	mov.u32 	%r168, 1;
	shfl.sync.bfly.b32 	%r169|%p35, %r167, %r168, %r154, %r156;
	mov.b32 	%f159, %r169;
	add.f32 	%f160, %f158, %f159;
	st.local.f32 	[%rd2+8], %f160;
	st.shared.f32 	[%r10], %f160;
	bar.sync 	0;
	@%p1 bra 	$L__BB109_15;

	ld.shared.f32 	%f161, [%r4];
	mov.b32 	%r170, %f161;
	shfl.sync.bfly.b32 	%r174|%p36, %r170, %r155, %r154, %r156;
	mov.b32 	%f162, %r174;
	add.f32 	%f163, %f161, %f162;
	mov.b32 	%r175, %f163;
	shfl.sync.bfly.b32 	%r177|%p37, %r175, %r159, %r154, %r156;
	mov.b32 	%f164, %r177;
	add.f32 	%f165, %f163, %f164;
	mov.b32 	%r178, %f165;
	shfl.sync.bfly.b32 	%r180|%p38, %r178, %r162, %r154, %r156;
	mov.b32 	%f166, %r180;
	add.f32 	%f167, %f165, %f166;
	mov.b32 	%r181, %f167;
	shfl.sync.bfly.b32 	%r183|%p39, %r181, %r165, %r154, %r156;
	mov.b32 	%f168, %r183;
	add.f32 	%f169, %f167, %f168;
	mov.b32 	%r184, %f169;
	shfl.sync.bfly.b32 	%r186|%p40, %r184, %r168, %r154, %r156;
	mov.b32 	%f170, %r186;
	add.f32 	%f171, %f169, %f170;
	st.local.f32 	[%rd2+8], %f171;

$L__BB109_15:
	bar.sync 	0;
	mov.b32 	%r187, %f250;
	shfl.sync.bfly.b32 	%r191|%p42, %r187, %r155, %r154, %r156;
	mov.b32 	%f172, %r191;
	add.f32 	%f173, %f250, %f172;
	mov.b32 	%r192, %f173;
	shfl.sync.bfly.b32 	%r194|%p43, %r192, %r159, %r154, %r156;
	mov.b32 	%f174, %r194;
	add.f32 	%f175, %f173, %f174;
	mov.b32 	%r195, %f175;
	shfl.sync.bfly.b32 	%r197|%p44, %r195, %r162, %r154, %r156;
	mov.b32 	%f176, %r197;
	add.f32 	%f177, %f175, %f176;
	mov.b32 	%r198, %f177;
	shfl.sync.bfly.b32 	%r200|%p45, %r198, %r165, %r154, %r156;
	mov.b32 	%f178, %r200;
	add.f32 	%f179, %f177, %f178;
	mov.b32 	%r201, %f179;
	shfl.sync.bfly.b32 	%r203|%p46, %r201, %r168, %r154, %r156;
	mov.b32 	%f180, %r203;
	add.f32 	%f181, %f179, %f180;
	st.local.f32 	[%rd2+12], %f181;
	st.shared.f32 	[%r10], %f181;
	bar.sync 	0;
	@%p1 bra 	$L__BB109_17;

	ld.shared.f32 	%f182, [%r4];
	mov.b32 	%r204, %f182;
	mov.u32 	%r205, 31;
	mov.u32 	%r206, 16;
	mov.u32 	%r207, -1;
	shfl.sync.bfly.b32 	%r208|%p47, %r204, %r206, %r205, %r207;
	mov.b32 	%f183, %r208;
	add.f32 	%f184, %f182, %f183;
	mov.b32 	%r209, %f184;
	mov.u32 	%r210, 8;
	shfl.sync.bfly.b32 	%r211|%p48, %r209, %r210, %r205, %r207;
	mov.b32 	%f185, %r211;
	add.f32 	%f186, %f184, %f185;
	mov.b32 	%r212, %f186;
	mov.u32 	%r213, 4;
	shfl.sync.bfly.b32 	%r214|%p49, %r212, %r213, %r205, %r207;
	mov.b32 	%f187, %r214;
	add.f32 	%f188, %f186, %f187;
	mov.b32 	%r215, %f188;
	mov.u32 	%r216, 2;
	shfl.sync.bfly.b32 	%r217|%p50, %r215, %r216, %r205, %r207;
	mov.b32 	%f189, %r217;
	add.f32 	%f190, %f188, %f189;
	mov.b32 	%r218, %f190;
	mov.u32 	%r219, 1;
	shfl.sync.bfly.b32 	%r220|%p51, %r218, %r219, %r205, %r207;
	mov.b32 	%f191, %r220;
	add.f32 	%f192, %f190, %f191;
	st.local.f32 	[%rd2+12], %f192;

$L__BB109_17:
	bar.sync 	0;
	mov.b32 	%r221, %f249;
	mov.u32 	%r222, 31;
	mov.u32 	%r223, 16;
	mov.u32 	%r224, -1;
	shfl.sync.bfly.b32 	%r225|%p53, %r221, %r223, %r222, %r224;
	mov.b32 	%f193, %r225;
	add.f32 	%f194, %f249, %f193;
	mov.b32 	%r226, %f194;
	mov.u32 	%r227, 8;
	shfl.sync.bfly.b32 	%r228|%p54, %r226, %r227, %r222, %r224;
	mov.b32 	%f195, %r228;
	add.f32 	%f196, %f194, %f195;
	mov.b32 	%r229, %f196;
	mov.u32 	%r230, 4;
	shfl.sync.bfly.b32 	%r231|%p55, %r229, %r230, %r222, %r224;
	mov.b32 	%f197, %r231;
	add.f32 	%f198, %f196, %f197;
	mov.b32 	%r232, %f198;
	mov.u32 	%r233, 2;
	shfl.sync.bfly.b32 	%r234|%p56, %r232, %r233, %r222, %r224;
	mov.b32 	%f199, %r234;
	add.f32 	%f200, %f198, %f199;
	mov.b32 	%r235, %f200;
	mov.u32 	%r236, 1;
	shfl.sync.bfly.b32 	%r237|%p57, %r235, %r236, %r222, %r224;
	mov.b32 	%f201, %r237;
	add.f32 	%f202, %f200, %f201;
	st.local.f32 	[%rd2+16], %f202;
	st.shared.f32 	[%r10], %f202;
	bar.sync 	0;
	@%p1 bra 	$L__BB109_19;

	ld.shared.f32 	%f203, [%r4];
	mov.b32 	%r238, %f203;
	shfl.sync.bfly.b32 	%r242|%p58, %r238, %r223, %r222, %r224;
	mov.b32 	%f204, %r242;
	add.f32 	%f205, %f203, %f204;
	mov.b32 	%r243, %f205;
	shfl.sync.bfly.b32 	%r245|%p59, %r243, %r227, %r222, %r224;
	mov.b32 	%f206, %r245;
	add.f32 	%f207, %f205, %f206;
	mov.b32 	%r246, %f207;
	shfl.sync.bfly.b32 	%r248|%p60, %r246, %r230, %r222, %r224;
	mov.b32 	%f208, %r248;
	add.f32 	%f209, %f207, %f208;
	mov.b32 	%r249, %f209;
	shfl.sync.bfly.b32 	%r251|%p61, %r249, %r233, %r222, %r224;
	mov.b32 	%f210, %r251;
	add.f32 	%f211, %f209, %f210;
	mov.b32 	%r252, %f211;
	shfl.sync.bfly.b32 	%r254|%p62, %r252, %r236, %r222, %r224;
	mov.b32 	%f212, %r254;
	add.f32 	%f213, %f211, %f212;
	st.local.f32 	[%rd2+16], %f213;

$L__BB109_19:
	bar.sync 	0;
	mov.b32 	%r255, %f248;
	shfl.sync.bfly.b32 	%r259|%p64, %r255, %r223, %r222, %r224;
	mov.b32 	%f214, %r259;
	add.f32 	%f215, %f248, %f214;
	mov.b32 	%r260, %f215;
	shfl.sync.bfly.b32 	%r262|%p65, %r260, %r227, %r222, %r224;
	mov.b32 	%f216, %r262;
	add.f32 	%f217, %f215, %f216;
	mov.b32 	%r263, %f217;
	shfl.sync.bfly.b32 	%r265|%p66, %r263, %r230, %r222, %r224;
	mov.b32 	%f218, %r265;
	add.f32 	%f219, %f217, %f218;
	mov.b32 	%r266, %f219;
	shfl.sync.bfly.b32 	%r268|%p67, %r266, %r233, %r222, %r224;
	mov.b32 	%f220, %r268;
	add.f32 	%f221, %f219, %f220;
	mov.b32 	%r269, %f221;
	shfl.sync.bfly.b32 	%r271|%p68, %r269, %r236, %r222, %r224;
	mov.b32 	%f222, %r271;
	add.f32 	%f223, %f221, %f222;
	st.local.f32 	[%rd2+20], %f223;
	st.shared.f32 	[%r10], %f223;
	bar.sync 	0;
	@%p1 bra 	$L__BB109_21;

	ld.shared.f32 	%f224, [%r4];
	mov.b32 	%r272, %f224;
	mov.u32 	%r273, 31;
	mov.u32 	%r274, 16;
	mov.u32 	%r275, -1;
	shfl.sync.bfly.b32 	%r276|%p69, %r272, %r274, %r273, %r275;
	mov.b32 	%f225, %r276;
	add.f32 	%f226, %f224, %f225;
	mov.b32 	%r277, %f226;
	mov.u32 	%r278, 8;
	shfl.sync.bfly.b32 	%r279|%p70, %r277, %r278, %r273, %r275;
	mov.b32 	%f227, %r279;
	add.f32 	%f228, %f226, %f227;
	mov.b32 	%r280, %f228;
	mov.u32 	%r281, 4;
	shfl.sync.bfly.b32 	%r282|%p71, %r280, %r281, %r273, %r275;
	mov.b32 	%f229, %r282;
	add.f32 	%f230, %f228, %f229;
	mov.b32 	%r283, %f230;
	mov.u32 	%r284, 2;
	shfl.sync.bfly.b32 	%r285|%p72, %r283, %r284, %r273, %r275;
	mov.b32 	%f231, %r285;
	add.f32 	%f232, %f230, %f231;
	mov.b32 	%r286, %f232;
	mov.u32 	%r287, 1;
	shfl.sync.bfly.b32 	%r288|%p73, %r286, %r287, %r273, %r275;
	mov.b32 	%f233, %r288;
	add.f32 	%f234, %f232, %f233;
	st.local.f32 	[%rd2+20], %f234;

$L__BB109_21:
	bar.sync 	0;
	setp.gt.s32 	%p74, %r3, 5;
	@%p74 bra 	$L__BB109_23;

	mad.lo.s32 	%r289, %r3, %r13, %r2;
	cvt.s64.s32 	%rd60, %r289;
	mul.lo.s32 	%r290, %r1, %r14;
	cvt.s64.s32 	%rd61, %r290;
	add.s64 	%rd62, %rd61, %rd60;
	mul.wide.s32 	%rd63, %r3, 4;
	add.s64 	%rd64, %rd2, %rd63;
	ld.local.f32 	%f235, [%rd64];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f235;}

	// end inline asm
	cvta.to.global.u64 	%rd65, %rd19;
	shl.b64 	%rd66, %rd62, 1;
	add.s64 	%rd67, %rd65, %rd66;
	st.global.u16 	[%rd67], %rs1;

$L__BB109_23:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_7_bs_192
.visible .entry ggml_matvec_f16_ncols_7_bs_192(
	.param .u64 ggml_matvec_f16_ncols_7_bs_192_param_0,
	.param .u64 ggml_matvec_f16_ncols_7_bs_192_param_1,
	.param .u64 ggml_matvec_f16_ncols_7_bs_192_param_2,
	.param .u32 ggml_matvec_f16_ncols_7_bs_192_param_3,
	.param .u32 ggml_matvec_f16_ncols_7_bs_192_param_4,
	.param .u32 ggml_matvec_f16_ncols_7_bs_192_param_5,
	.param .u32 ggml_matvec_f16_ncols_7_bs_192_param_6,
	.param .u32 ggml_matvec_f16_ncols_7_bs_192_param_7,
	.param .u32 ggml_matvec_f16_ncols_7_bs_192_param_8,
	.param .u32 ggml_matvec_f16_ncols_7_bs_192_param_9,
	.param .u32 ggml_matvec_f16_ncols_7_bs_192_param_10,
	.param .u32 ggml_matvec_f16_ncols_7_bs_192_param_11
)
{
	.local .align 4 .b8 	__local_depot110[28];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<82>;
	.reg .b16 	%rs<2>;
	.reg .f32 	%f<221>;
	.reg .b32 	%r<289>;
	.reg .b64 	%rd<43>;


	mov.u64 	%SPL, __local_depot110;
	ld.param.u64 	%rd13, [ggml_matvec_f16_ncols_7_bs_192_param_0];
	ld.param.u64 	%rd14, [ggml_matvec_f16_ncols_7_bs_192_param_1];
	ld.param.u64 	%rd15, [ggml_matvec_f16_ncols_7_bs_192_param_2];
	ld.param.u32 	%r8, [ggml_matvec_f16_ncols_7_bs_192_param_3];
	ld.param.u32 	%r9, [ggml_matvec_f16_ncols_7_bs_192_param_5];
	ld.param.u32 	%r10, [ggml_matvec_f16_ncols_7_bs_192_param_6];
	ld.param.u32 	%r11, [ggml_matvec_f16_ncols_7_bs_192_param_7];
	ld.param.u32 	%r12, [ggml_matvec_f16_ncols_7_bs_192_param_8];
	ld.param.u32 	%r13, [ggml_matvec_f16_ncols_7_bs_192_param_9];
	ld.param.u32 	%r14, [ggml_matvec_f16_ncols_7_bs_192_param_10];
	ld.param.u32 	%r15, [ggml_matvec_f16_ncols_7_bs_192_param_11];
	add.u64 	%rd1, %SPL, 0;
	mov.u32 	%r1, %ctaid.x;
	mov.u32 	%r2, %tid.x;
	setp.gt.s32 	%p1, %r2, 31;
	shl.b32 	%r16, %r2, 2;
	mov.u32 	%r17, data_mmv;
	add.s32 	%r3, %r17, %r16;
	@%p1 bra 	$L__BB110_2;

	mov.u32 	%r18, 0;
	st.shared.u32 	[%r3], %r18;

$L__BB110_2:
	mov.u32 	%r4, %ctaid.y;
	bar.sync 	0;
	mov.f32 	%f214, 0f00000000;
	mov.u32 	%r19, 0;
	st.local.u32 	[%rd1], %r19;
	st.local.u32 	[%rd1+4], %r19;
	st.local.u32 	[%rd1+8], %r19;
	st.local.u32 	[%rd1+12], %r19;
	st.local.u32 	[%rd1+16], %r19;
	st.local.u32 	[%rd1+20], %r19;
	st.local.u32 	[%rd1+24], %r19;
	setp.ge.s32 	%p2, %r2, %r8;
	mov.f32 	%f215, %f214;
	mov.f32 	%f216, %f214;
	mov.f32 	%f217, %f214;
	mov.f32 	%f218, %f214;
	mov.f32 	%f219, %f214;
	mov.f32 	%f220, %f214;
	@%p2 bra 	$L__BB110_6;

	shl.b32 	%r20, %r10, 1;
	add.s32 	%r21, %r2, %r20;
	mul.wide.s32 	%rd17, %r21, 4;
	mul.lo.s32 	%r22, %r4, %r14;
	mul.wide.s32 	%rd18, %r22, 2;
	add.s64 	%rd3, %rd17, %rd18;
	mul.wide.s32 	%rd19, %r2, 4;
	mul.wide.s32 	%rd4, %r10, 4;
	add.s64 	%rd20, %rd19, %rd4;
	add.s64 	%rd5, %rd20, %rd18;
	add.s64 	%rd6, %rd19, %rd18;
	mul.wide.s32 	%rd21, %r2, 2;
	div.s32 	%r23, %r4, %r12;
	mul.lo.s32 	%r24, %r1, %r9;
	mad.lo.s32 	%r25, %r23, %r13, %r24;
	cvt.s64.s32 	%rd22, %r25;
	add.s64 	%rd23, %rd21, %rd22;
	cvta.to.global.u64 	%rd24, %rd13;
	shl.b64 	%rd25, %rd23, 1;
	add.s64 	%rd41, %rd24, %rd25;
	cvta.to.global.u64 	%rd42, %rd14;
	mov.f32 	%f214, 0f00000000;
	mov.u32 	%r288, %r2;

$L__BB110_4:
	ld.global.nc.u32 	%r26, [%rd41];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r26;
  cvt.f32.f16 %f36, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r26;
  cvt.f32.f16 %f37, high;}

	// end inline asm
	add.s64 	%rd26, %rd42, %rd6;
	ld.global.nc.u32 	%r28, [%rd26];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r28;
  cvt.f32.f16 %f38, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r28;
  cvt.f32.f16 %f39, high;}

	// end inline asm
	fma.rn.f32 	%f52, %f36, %f38, %f220;
	fma.rn.f32 	%f220, %f37, %f39, %f52;
	add.s64 	%rd27, %rd42, %rd5;
	ld.global.nc.u32 	%r30, [%rd27];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r30;
  cvt.f32.f16 %f40, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r30;
  cvt.f32.f16 %f41, high;}

	// end inline asm
	fma.rn.f32 	%f53, %f36, %f40, %f219;
	fma.rn.f32 	%f219, %f37, %f41, %f53;
	add.s64 	%rd28, %rd42, %rd3;
	ld.global.nc.u32 	%r32, [%rd28];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r32;
  cvt.f32.f16 %f42, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r32;
  cvt.f32.f16 %f43, high;}

	// end inline asm
	fma.rn.f32 	%f54, %f36, %f42, %f218;
	fma.rn.f32 	%f218, %f37, %f43, %f54;
	add.s64 	%rd29, %rd28, %rd4;
	ld.global.nc.u32 	%r34, [%rd29];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r34;
  cvt.f32.f16 %f44, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r34;
  cvt.f32.f16 %f45, high;}

	// end inline asm
	fma.rn.f32 	%f55, %f36, %f44, %f217;
	fma.rn.f32 	%f217, %f37, %f45, %f55;
	add.s64 	%rd30, %rd29, %rd4;
	ld.global.nc.u32 	%r36, [%rd30];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r36;
  cvt.f32.f16 %f46, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r36;
  cvt.f32.f16 %f47, high;}

	// end inline asm
	fma.rn.f32 	%f56, %f36, %f46, %f216;
	fma.rn.f32 	%f216, %f37, %f47, %f56;
	add.s64 	%rd31, %rd30, %rd4;
	ld.global.nc.u32 	%r38, [%rd31];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r38;
  cvt.f32.f16 %f48, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r38;
  cvt.f32.f16 %f49, high;}

	// end inline asm
	fma.rn.f32 	%f57, %f36, %f48, %f215;
	fma.rn.f32 	%f215, %f37, %f49, %f57;
	add.s64 	%rd32, %rd31, %rd4;
	ld.global.nc.u32 	%r40, [%rd32];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r40;
  cvt.f32.f16 %f50, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r40;
  cvt.f32.f16 %f51, high;}

	// end inline asm
	fma.rn.f32 	%f58, %f36, %f50, %f214;
	fma.rn.f32 	%f214, %f37, %f51, %f58;
	add.s64 	%rd42, %rd42, 768;
	add.s64 	%rd41, %rd41, 768;
	add.s32 	%r288, %r288, 192;
	setp.lt.s32 	%p3, %r288, %r8;
	@%p3 bra 	$L__BB110_4;

	st.local.f32 	[%rd1], %f220;
	st.local.f32 	[%rd1+4], %f219;
	st.local.f32 	[%rd1+8], %f218;
	st.local.f32 	[%rd1+12], %f217;
	st.local.f32 	[%rd1+16], %f216;
	st.local.f32 	[%rd1+20], %f215;
	st.local.f32 	[%rd1+24], %f214;

$L__BB110_6:
	shr.s32 	%r42, %r2, 31;
	shr.u32 	%r43, %r42, 27;
	add.s32 	%r44, %r2, %r43;
	shr.s32 	%r45, %r44, 5;
	shl.b32 	%r46, %r45, 2;
	add.s32 	%r7, %r17, %r46;
	mov.u32 	%r48, 2;
	mov.b32 	%r49, %f220;
	mov.u32 	%r50, 31;
	mov.u32 	%r51, 16;
	mov.u32 	%r52, -1;
	shfl.sync.bfly.b32 	%r53|%p4, %r49, %r51, %r50, %r52;
	mov.b32 	%f59, %r53;
	add.f32 	%f60, %f220, %f59;
	mov.b32 	%r54, %f60;
	mov.u32 	%r55, 8;
	shfl.sync.bfly.b32 	%r56|%p5, %r54, %r55, %r50, %r52;
	mov.b32 	%f61, %r56;
	add.f32 	%f62, %f60, %f61;
	mov.b32 	%r57, %f62;
	mov.u32 	%r58, 4;
	shfl.sync.bfly.b32 	%r59|%p6, %r57, %r58, %r50, %r52;
	mov.b32 	%f63, %r59;
	add.f32 	%f64, %f62, %f63;
	mov.b32 	%r60, %f64;
	shfl.sync.bfly.b32 	%r61|%p7, %r60, %r48, %r50, %r52;
	mov.b32 	%f65, %r61;
	add.f32 	%f66, %f64, %f65;
	mov.b32 	%r62, %f66;
	mov.u32 	%r63, 1;
	shfl.sync.bfly.b32 	%r64|%p8, %r62, %r63, %r50, %r52;
	mov.b32 	%f67, %r64;
	add.f32 	%f68, %f66, %f67;
	st.local.f32 	[%rd1], %f68;
	st.shared.f32 	[%r7], %f68;
	bar.sync 	0;
	@%p1 bra 	$L__BB110_8;

	ld.shared.f32 	%f69, [%r3];
	mov.b32 	%r65, %f69;
	shfl.sync.bfly.b32 	%r69|%p10, %r65, %r51, %r50, %r52;
	mov.b32 	%f70, %r69;
	add.f32 	%f71, %f69, %f70;
	mov.b32 	%r70, %f71;
	shfl.sync.bfly.b32 	%r72|%p11, %r70, %r55, %r50, %r52;
	mov.b32 	%f72, %r72;
	add.f32 	%f73, %f71, %f72;
	mov.b32 	%r73, %f73;
	shfl.sync.bfly.b32 	%r75|%p12, %r73, %r58, %r50, %r52;
	mov.b32 	%f74, %r75;
	add.f32 	%f75, %f73, %f74;
	mov.b32 	%r76, %f75;
	shfl.sync.bfly.b32 	%r78|%p13, %r76, %r48, %r50, %r52;
	mov.b32 	%f76, %r78;
	add.f32 	%f77, %f75, %f76;
	mov.b32 	%r79, %f77;
	shfl.sync.bfly.b32 	%r81|%p14, %r79, %r63, %r50, %r52;
	mov.b32 	%f78, %r81;
	add.f32 	%f79, %f77, %f78;
	st.local.f32 	[%rd1], %f79;

$L__BB110_8:
	bar.sync 	0;
	mov.b32 	%r82, %f219;
	shfl.sync.bfly.b32 	%r86|%p16, %r82, %r51, %r50, %r52;
	mov.b32 	%f80, %r86;
	add.f32 	%f81, %f219, %f80;
	mov.b32 	%r87, %f81;
	shfl.sync.bfly.b32 	%r89|%p17, %r87, %r55, %r50, %r52;
	mov.b32 	%f82, %r89;
	add.f32 	%f83, %f81, %f82;
	mov.b32 	%r90, %f83;
	shfl.sync.bfly.b32 	%r92|%p18, %r90, %r58, %r50, %r52;
	mov.b32 	%f84, %r92;
	add.f32 	%f85, %f83, %f84;
	mov.b32 	%r93, %f85;
	shfl.sync.bfly.b32 	%r95|%p19, %r93, %r48, %r50, %r52;
	mov.b32 	%f86, %r95;
	add.f32 	%f87, %f85, %f86;
	mov.b32 	%r96, %f87;
	shfl.sync.bfly.b32 	%r98|%p20, %r96, %r63, %r50, %r52;
	mov.b32 	%f88, %r98;
	add.f32 	%f89, %f87, %f88;
	st.local.f32 	[%rd1+4], %f89;
	st.shared.f32 	[%r7], %f89;
	bar.sync 	0;
	@%p1 bra 	$L__BB110_10;

	ld.shared.f32 	%f90, [%r3];
	mov.b32 	%r99, %f90;
	mov.u32 	%r100, 31;
	mov.u32 	%r101, 16;
	mov.u32 	%r102, -1;
	shfl.sync.bfly.b32 	%r103|%p21, %r99, %r101, %r100, %r102;
	mov.b32 	%f91, %r103;
	add.f32 	%f92, %f90, %f91;
	mov.b32 	%r104, %f92;
	mov.u32 	%r105, 8;
	shfl.sync.bfly.b32 	%r106|%p22, %r104, %r105, %r100, %r102;
	mov.b32 	%f93, %r106;
	add.f32 	%f94, %f92, %f93;
	mov.b32 	%r107, %f94;
	mov.u32 	%r108, 4;
	shfl.sync.bfly.b32 	%r109|%p23, %r107, %r108, %r100, %r102;
	mov.b32 	%f95, %r109;
	add.f32 	%f96, %f94, %f95;
	mov.b32 	%r110, %f96;
	mov.u32 	%r111, 2;
	shfl.sync.bfly.b32 	%r112|%p24, %r110, %r111, %r100, %r102;
	mov.b32 	%f97, %r112;
	add.f32 	%f98, %f96, %f97;
	mov.b32 	%r113, %f98;
	mov.u32 	%r114, 1;
	shfl.sync.bfly.b32 	%r115|%p25, %r113, %r114, %r100, %r102;
	mov.b32 	%f99, %r115;
	add.f32 	%f100, %f98, %f99;
	st.local.f32 	[%rd1+4], %f100;

$L__BB110_10:
	bar.sync 	0;
	mov.b32 	%r116, %f218;
	mov.u32 	%r117, 31;
	mov.u32 	%r118, 16;
	mov.u32 	%r119, -1;
	shfl.sync.bfly.b32 	%r120|%p27, %r116, %r118, %r117, %r119;
	mov.b32 	%f101, %r120;
	add.f32 	%f102, %f218, %f101;
	mov.b32 	%r121, %f102;
	mov.u32 	%r122, 8;
	shfl.sync.bfly.b32 	%r123|%p28, %r121, %r122, %r117, %r119;
	mov.b32 	%f103, %r123;
	add.f32 	%f104, %f102, %f103;
	mov.b32 	%r124, %f104;
	mov.u32 	%r125, 4;
	shfl.sync.bfly.b32 	%r126|%p29, %r124, %r125, %r117, %r119;
	mov.b32 	%f105, %r126;
	add.f32 	%f106, %f104, %f105;
	mov.b32 	%r127, %f106;
	mov.u32 	%r128, 2;
	shfl.sync.bfly.b32 	%r129|%p30, %r127, %r128, %r117, %r119;
	mov.b32 	%f107, %r129;
	add.f32 	%f108, %f106, %f107;
	mov.b32 	%r130, %f108;
	mov.u32 	%r131, 1;
	shfl.sync.bfly.b32 	%r132|%p31, %r130, %r131, %r117, %r119;
	mov.b32 	%f109, %r132;
	add.f32 	%f110, %f108, %f109;
	st.local.f32 	[%rd1+8], %f110;
	st.shared.f32 	[%r7], %f110;
	bar.sync 	0;
	@%p1 bra 	$L__BB110_12;

	ld.shared.f32 	%f111, [%r3];
	mov.b32 	%r133, %f111;
	shfl.sync.bfly.b32 	%r137|%p32, %r133, %r118, %r117, %r119;
	mov.b32 	%f112, %r137;
	add.f32 	%f113, %f111, %f112;
	mov.b32 	%r138, %f113;
	shfl.sync.bfly.b32 	%r140|%p33, %r138, %r122, %r117, %r119;
	mov.b32 	%f114, %r140;
	add.f32 	%f115, %f113, %f114;
	mov.b32 	%r141, %f115;
	shfl.sync.bfly.b32 	%r143|%p34, %r141, %r125, %r117, %r119;
	mov.b32 	%f116, %r143;
	add.f32 	%f117, %f115, %f116;
	mov.b32 	%r144, %f117;
	shfl.sync.bfly.b32 	%r146|%p35, %r144, %r128, %r117, %r119;
	mov.b32 	%f118, %r146;
	add.f32 	%f119, %f117, %f118;
	mov.b32 	%r147, %f119;
	shfl.sync.bfly.b32 	%r149|%p36, %r147, %r131, %r117, %r119;
	mov.b32 	%f120, %r149;
	add.f32 	%f121, %f119, %f120;
	st.local.f32 	[%rd1+8], %f121;

$L__BB110_12:
	bar.sync 	0;
	mov.b32 	%r150, %f217;
	shfl.sync.bfly.b32 	%r154|%p38, %r150, %r118, %r117, %r119;
	mov.b32 	%f122, %r154;
	add.f32 	%f123, %f217, %f122;
	mov.b32 	%r155, %f123;
	shfl.sync.bfly.b32 	%r157|%p39, %r155, %r122, %r117, %r119;
	mov.b32 	%f124, %r157;
	add.f32 	%f125, %f123, %f124;
	mov.b32 	%r158, %f125;
	shfl.sync.bfly.b32 	%r160|%p40, %r158, %r125, %r117, %r119;
	mov.b32 	%f126, %r160;
	add.f32 	%f127, %f125, %f126;
	mov.b32 	%r161, %f127;
	shfl.sync.bfly.b32 	%r163|%p41, %r161, %r128, %r117, %r119;
	mov.b32 	%f128, %r163;
	add.f32 	%f129, %f127, %f128;
	mov.b32 	%r164, %f129;
	shfl.sync.bfly.b32 	%r166|%p42, %r164, %r131, %r117, %r119;
	mov.b32 	%f130, %r166;
	add.f32 	%f131, %f129, %f130;
	st.local.f32 	[%rd1+12], %f131;
	st.shared.f32 	[%r7], %f131;
	bar.sync 	0;
	@%p1 bra 	$L__BB110_14;

	ld.shared.f32 	%f132, [%r3];
	mov.b32 	%r167, %f132;
	mov.u32 	%r168, 31;
	mov.u32 	%r169, 16;
	mov.u32 	%r170, -1;
	shfl.sync.bfly.b32 	%r171|%p43, %r167, %r169, %r168, %r170;
	mov.b32 	%f133, %r171;
	add.f32 	%f134, %f132, %f133;
	mov.b32 	%r172, %f134;
	mov.u32 	%r173, 8;
	shfl.sync.bfly.b32 	%r174|%p44, %r172, %r173, %r168, %r170;
	mov.b32 	%f135, %r174;
	add.f32 	%f136, %f134, %f135;
	mov.b32 	%r175, %f136;
	mov.u32 	%r176, 4;
	shfl.sync.bfly.b32 	%r177|%p45, %r175, %r176, %r168, %r170;
	mov.b32 	%f137, %r177;
	add.f32 	%f138, %f136, %f137;
	mov.b32 	%r178, %f138;
	mov.u32 	%r179, 2;
	shfl.sync.bfly.b32 	%r180|%p46, %r178, %r179, %r168, %r170;
	mov.b32 	%f139, %r180;
	add.f32 	%f140, %f138, %f139;
	mov.b32 	%r181, %f140;
	mov.u32 	%r182, 1;
	shfl.sync.bfly.b32 	%r183|%p47, %r181, %r182, %r168, %r170;
	mov.b32 	%f141, %r183;
	add.f32 	%f142, %f140, %f141;
	st.local.f32 	[%rd1+12], %f142;

$L__BB110_14:
	bar.sync 	0;
	mov.b32 	%r184, %f216;
	mov.u32 	%r185, 31;
	mov.u32 	%r186, 16;
	mov.u32 	%r187, -1;
	shfl.sync.bfly.b32 	%r188|%p49, %r184, %r186, %r185, %r187;
	mov.b32 	%f143, %r188;
	add.f32 	%f144, %f216, %f143;
	mov.b32 	%r189, %f144;
	mov.u32 	%r190, 8;
	shfl.sync.bfly.b32 	%r191|%p50, %r189, %r190, %r185, %r187;
	mov.b32 	%f145, %r191;
	add.f32 	%f146, %f144, %f145;
	mov.b32 	%r192, %f146;
	mov.u32 	%r193, 4;
	shfl.sync.bfly.b32 	%r194|%p51, %r192, %r193, %r185, %r187;
	mov.b32 	%f147, %r194;
	add.f32 	%f148, %f146, %f147;
	mov.b32 	%r195, %f148;
	mov.u32 	%r196, 2;
	shfl.sync.bfly.b32 	%r197|%p52, %r195, %r196, %r185, %r187;
	mov.b32 	%f149, %r197;
	add.f32 	%f150, %f148, %f149;
	mov.b32 	%r198, %f150;
	mov.u32 	%r199, 1;
	shfl.sync.bfly.b32 	%r200|%p53, %r198, %r199, %r185, %r187;
	mov.b32 	%f151, %r200;
	add.f32 	%f152, %f150, %f151;
	st.local.f32 	[%rd1+16], %f152;
	st.shared.f32 	[%r7], %f152;
	bar.sync 	0;
	@%p1 bra 	$L__BB110_16;

	ld.shared.f32 	%f153, [%r3];
	mov.b32 	%r201, %f153;
	shfl.sync.bfly.b32 	%r205|%p54, %r201, %r186, %r185, %r187;
	mov.b32 	%f154, %r205;
	add.f32 	%f155, %f153, %f154;
	mov.b32 	%r206, %f155;
	shfl.sync.bfly.b32 	%r208|%p55, %r206, %r190, %r185, %r187;
	mov.b32 	%f156, %r208;
	add.f32 	%f157, %f155, %f156;
	mov.b32 	%r209, %f157;
	shfl.sync.bfly.b32 	%r211|%p56, %r209, %r193, %r185, %r187;
	mov.b32 	%f158, %r211;
	add.f32 	%f159, %f157, %f158;
	mov.b32 	%r212, %f159;
	shfl.sync.bfly.b32 	%r214|%p57, %r212, %r196, %r185, %r187;
	mov.b32 	%f160, %r214;
	add.f32 	%f161, %f159, %f160;
	mov.b32 	%r215, %f161;
	shfl.sync.bfly.b32 	%r217|%p58, %r215, %r199, %r185, %r187;
	mov.b32 	%f162, %r217;
	add.f32 	%f163, %f161, %f162;
	st.local.f32 	[%rd1+16], %f163;

$L__BB110_16:
	bar.sync 	0;
	mov.b32 	%r218, %f215;
	shfl.sync.bfly.b32 	%r222|%p60, %r218, %r186, %r185, %r187;
	mov.b32 	%f164, %r222;
	add.f32 	%f165, %f215, %f164;
	mov.b32 	%r223, %f165;
	shfl.sync.bfly.b32 	%r225|%p61, %r223, %r190, %r185, %r187;
	mov.b32 	%f166, %r225;
	add.f32 	%f167, %f165, %f166;
	mov.b32 	%r226, %f167;
	shfl.sync.bfly.b32 	%r228|%p62, %r226, %r193, %r185, %r187;
	mov.b32 	%f168, %r228;
	add.f32 	%f169, %f167, %f168;
	mov.b32 	%r229, %f169;
	shfl.sync.bfly.b32 	%r231|%p63, %r229, %r196, %r185, %r187;
	mov.b32 	%f170, %r231;
	add.f32 	%f171, %f169, %f170;
	mov.b32 	%r232, %f171;
	shfl.sync.bfly.b32 	%r234|%p64, %r232, %r199, %r185, %r187;
	mov.b32 	%f172, %r234;
	add.f32 	%f173, %f171, %f172;
	st.local.f32 	[%rd1+20], %f173;
	st.shared.f32 	[%r7], %f173;
	bar.sync 	0;
	@%p1 bra 	$L__BB110_18;

	ld.shared.f32 	%f174, [%r3];
	mov.b32 	%r235, %f174;
	mov.u32 	%r236, 31;
	mov.u32 	%r237, 16;
	mov.u32 	%r238, -1;
	shfl.sync.bfly.b32 	%r239|%p65, %r235, %r237, %r236, %r238;
	mov.b32 	%f175, %r239;
	add.f32 	%f176, %f174, %f175;
	mov.b32 	%r240, %f176;
	mov.u32 	%r241, 8;
	shfl.sync.bfly.b32 	%r242|%p66, %r240, %r241, %r236, %r238;
	mov.b32 	%f177, %r242;
	add.f32 	%f178, %f176, %f177;
	mov.b32 	%r243, %f178;
	mov.u32 	%r244, 4;
	shfl.sync.bfly.b32 	%r245|%p67, %r243, %r244, %r236, %r238;
	mov.b32 	%f179, %r245;
	add.f32 	%f180, %f178, %f179;
	mov.b32 	%r246, %f180;
	mov.u32 	%r247, 2;
	shfl.sync.bfly.b32 	%r248|%p68, %r246, %r247, %r236, %r238;
	mov.b32 	%f181, %r248;
	add.f32 	%f182, %f180, %f181;
	mov.b32 	%r249, %f182;
	mov.u32 	%r250, 1;
	shfl.sync.bfly.b32 	%r251|%p69, %r249, %r250, %r236, %r238;
	mov.b32 	%f183, %r251;
	add.f32 	%f184, %f182, %f183;
	st.local.f32 	[%rd1+20], %f184;

$L__BB110_18:
	bar.sync 	0;
	mov.b32 	%r252, %f214;
	mov.u32 	%r253, 31;
	mov.u32 	%r254, 16;
	mov.u32 	%r255, -1;
	shfl.sync.bfly.b32 	%r256|%p71, %r252, %r254, %r253, %r255;
	mov.b32 	%f185, %r256;
	add.f32 	%f186, %f214, %f185;
	mov.b32 	%r257, %f186;
	mov.u32 	%r258, 8;
	shfl.sync.bfly.b32 	%r259|%p72, %r257, %r258, %r253, %r255;
	mov.b32 	%f187, %r259;
	add.f32 	%f188, %f186, %f187;
	mov.b32 	%r260, %f188;
	mov.u32 	%r261, 4;
	shfl.sync.bfly.b32 	%r262|%p73, %r260, %r261, %r253, %r255;
	mov.b32 	%f189, %r262;
	add.f32 	%f190, %f188, %f189;
	mov.b32 	%r263, %f190;
	mov.u32 	%r264, 2;
	shfl.sync.bfly.b32 	%r265|%p74, %r263, %r264, %r253, %r255;
	mov.b32 	%f191, %r265;
	add.f32 	%f192, %f190, %f191;
	mov.b32 	%r266, %f192;
	mov.u32 	%r267, 1;
	shfl.sync.bfly.b32 	%r268|%p75, %r266, %r267, %r253, %r255;
	mov.b32 	%f193, %r268;
	add.f32 	%f194, %f192, %f193;
	st.local.f32 	[%rd1+24], %f194;
	st.shared.f32 	[%r7], %f194;
	bar.sync 	0;
	@%p1 bra 	$L__BB110_20;

	ld.shared.f32 	%f195, [%r3];
	mov.b32 	%r269, %f195;
	shfl.sync.bfly.b32 	%r273|%p76, %r269, %r254, %r253, %r255;
	mov.b32 	%f196, %r273;
	add.f32 	%f197, %f195, %f196;
	mov.b32 	%r274, %f197;
	shfl.sync.bfly.b32 	%r276|%p77, %r274, %r258, %r253, %r255;
	mov.b32 	%f198, %r276;
	add.f32 	%f199, %f197, %f198;
	mov.b32 	%r277, %f199;
	shfl.sync.bfly.b32 	%r279|%p78, %r277, %r261, %r253, %r255;
	mov.b32 	%f200, %r279;
	add.f32 	%f201, %f199, %f200;
	mov.b32 	%r280, %f201;
	shfl.sync.bfly.b32 	%r282|%p79, %r280, %r264, %r253, %r255;
	mov.b32 	%f202, %r282;
	add.f32 	%f203, %f201, %f202;
	mov.b32 	%r283, %f203;
	shfl.sync.bfly.b32 	%r285|%p80, %r283, %r267, %r253, %r255;
	mov.b32 	%f204, %r285;
	add.f32 	%f205, %f203, %f204;
	st.local.f32 	[%rd1+24], %f205;

$L__BB110_20:
	bar.sync 	0;
	setp.gt.s32 	%p81, %r2, 6;
	@%p81 bra 	$L__BB110_22;

	mad.lo.s32 	%r286, %r2, %r11, %r1;
	cvt.s64.s32 	%rd33, %r286;
	mul.lo.s32 	%r287, %r4, %r15;
	cvt.s64.s32 	%rd34, %r287;
	add.s64 	%rd35, %rd34, %rd33;
	mul.wide.s32 	%rd36, %r2, 4;
	add.s64 	%rd37, %rd1, %rd36;
	ld.local.f32 	%f206, [%rd37];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f206;}

	// end inline asm
	cvta.to.global.u64 	%rd38, %rd15;
	shl.b64 	%rd39, %rd35, 1;
	add.s64 	%rd40, %rd38, %rd39;
	st.global.u16 	[%rd40], %rs1;

$L__BB110_22:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_8_bs_192
.visible .entry ggml_matvec_f16_ncols_8_bs_192(
	.param .u64 ggml_matvec_f16_ncols_8_bs_192_param_0,
	.param .u64 ggml_matvec_f16_ncols_8_bs_192_param_1,
	.param .u64 ggml_matvec_f16_ncols_8_bs_192_param_2,
	.param .u32 ggml_matvec_f16_ncols_8_bs_192_param_3,
	.param .u32 ggml_matvec_f16_ncols_8_bs_192_param_4,
	.param .u32 ggml_matvec_f16_ncols_8_bs_192_param_5,
	.param .u32 ggml_matvec_f16_ncols_8_bs_192_param_6,
	.param .u32 ggml_matvec_f16_ncols_8_bs_192_param_7,
	.param .u32 ggml_matvec_f16_ncols_8_bs_192_param_8,
	.param .u32 ggml_matvec_f16_ncols_8_bs_192_param_9,
	.param .u32 ggml_matvec_f16_ncols_8_bs_192_param_10,
	.param .u32 ggml_matvec_f16_ncols_8_bs_192_param_11
)
{
	.local .align 16 .b8 	__local_depot111[32];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<93>;
	.reg .b16 	%rs<2>;
	.reg .f32 	%f<252>;
	.reg .b32 	%r<324>;
	.reg .b64 	%rd<45>;


	mov.u64 	%SPL, __local_depot111;
	ld.param.u64 	%rd14, [ggml_matvec_f16_ncols_8_bs_192_param_0];
	ld.param.u64 	%rd15, [ggml_matvec_f16_ncols_8_bs_192_param_1];
	ld.param.u64 	%rd16, [ggml_matvec_f16_ncols_8_bs_192_param_2];
	ld.param.u32 	%r8, [ggml_matvec_f16_ncols_8_bs_192_param_3];
	ld.param.u32 	%r9, [ggml_matvec_f16_ncols_8_bs_192_param_5];
	ld.param.u32 	%r10, [ggml_matvec_f16_ncols_8_bs_192_param_6];
	ld.param.u32 	%r11, [ggml_matvec_f16_ncols_8_bs_192_param_7];
	ld.param.u32 	%r12, [ggml_matvec_f16_ncols_8_bs_192_param_8];
	ld.param.u32 	%r13, [ggml_matvec_f16_ncols_8_bs_192_param_9];
	ld.param.u32 	%r14, [ggml_matvec_f16_ncols_8_bs_192_param_10];
	ld.param.u32 	%r15, [ggml_matvec_f16_ncols_8_bs_192_param_11];
	add.u64 	%rd1, %SPL, 0;
	mov.u32 	%r1, %ctaid.x;
	mov.u32 	%r2, %tid.x;
	setp.gt.s32 	%p1, %r2, 31;
	shl.b32 	%r16, %r2, 2;
	mov.u32 	%r17, data_mmv;
	add.s32 	%r3, %r17, %r16;
	@%p1 bra 	$L__BB111_2;

	mov.u32 	%r18, 0;
	st.shared.u32 	[%r3], %r18;

$L__BB111_2:
	mov.u32 	%r4, %ctaid.y;
	bar.sync 	0;
	mov.f32 	%f244, 0f00000000;
	st.local.v4.f32 	[%rd1], {%f244, %f244, %f244, %f244};
	st.local.v4.f32 	[%rd1+16], {%f244, %f244, %f244, %f244};
	setp.ge.s32 	%p2, %r2, %r8;
	mov.f32 	%f245, %f244;
	mov.f32 	%f246, %f244;
	mov.f32 	%f247, %f244;
	mov.f32 	%f248, %f244;
	mov.f32 	%f249, %f244;
	mov.f32 	%f250, %f244;
	mov.f32 	%f251, %f244;
	@%p2 bra 	$L__BB111_6;

	shl.b32 	%r19, %r10, 1;
	add.s32 	%r20, %r2, %r19;
	mul.wide.s32 	%rd18, %r20, 4;
	mul.lo.s32 	%r21, %r4, %r14;
	mul.wide.s32 	%rd19, %r21, 2;
	add.s64 	%rd4, %rd18, %rd19;
	mul.wide.s32 	%rd20, %r2, 4;
	mul.wide.s32 	%rd5, %r10, 4;
	add.s64 	%rd21, %rd20, %rd5;
	add.s64 	%rd6, %rd21, %rd19;
	add.s64 	%rd7, %rd20, %rd19;
	mul.wide.s32 	%rd22, %r2, 2;
	div.s32 	%r22, %r4, %r12;
	mul.lo.s32 	%r23, %r1, %r9;
	mad.lo.s32 	%r24, %r22, %r13, %r23;
	cvt.s64.s32 	%rd23, %r24;
	add.s64 	%rd24, %rd22, %rd23;
	cvta.to.global.u64 	%rd25, %rd14;
	shl.b64 	%rd26, %rd24, 1;
	add.s64 	%rd43, %rd25, %rd26;
	cvta.to.global.u64 	%rd44, %rd15;
	mov.f32 	%f244, 0f00000000;
	mov.u32 	%r323, %r2;

$L__BB111_4:
	ld.global.nc.u32 	%r25, [%rd43];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r25;
  cvt.f32.f16 %f41, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r25;
  cvt.f32.f16 %f42, high;}

	// end inline asm
	add.s64 	%rd27, %rd44, %rd7;
	ld.global.nc.u32 	%r27, [%rd27];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r27;
  cvt.f32.f16 %f43, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r27;
  cvt.f32.f16 %f44, high;}

	// end inline asm
	fma.rn.f32 	%f59, %f41, %f43, %f251;
	fma.rn.f32 	%f251, %f42, %f44, %f59;
	add.s64 	%rd28, %rd44, %rd6;
	ld.global.nc.u32 	%r29, [%rd28];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r29;
  cvt.f32.f16 %f45, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r29;
  cvt.f32.f16 %f46, high;}

	// end inline asm
	fma.rn.f32 	%f60, %f41, %f45, %f250;
	fma.rn.f32 	%f250, %f42, %f46, %f60;
	add.s64 	%rd29, %rd44, %rd4;
	ld.global.nc.u32 	%r31, [%rd29];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r31;
  cvt.f32.f16 %f47, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r31;
  cvt.f32.f16 %f48, high;}

	// end inline asm
	fma.rn.f32 	%f61, %f41, %f47, %f249;
	fma.rn.f32 	%f249, %f42, %f48, %f61;
	add.s64 	%rd30, %rd29, %rd5;
	ld.global.nc.u32 	%r33, [%rd30];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r33;
  cvt.f32.f16 %f49, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r33;
  cvt.f32.f16 %f50, high;}

	// end inline asm
	fma.rn.f32 	%f62, %f41, %f49, %f248;
	fma.rn.f32 	%f248, %f42, %f50, %f62;
	add.s64 	%rd31, %rd30, %rd5;
	ld.global.nc.u32 	%r35, [%rd31];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r35;
  cvt.f32.f16 %f51, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r35;
  cvt.f32.f16 %f52, high;}

	// end inline asm
	fma.rn.f32 	%f63, %f41, %f51, %f247;
	fma.rn.f32 	%f247, %f42, %f52, %f63;
	add.s64 	%rd32, %rd31, %rd5;
	ld.global.nc.u32 	%r37, [%rd32];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r37;
  cvt.f32.f16 %f53, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r37;
  cvt.f32.f16 %f54, high;}

	// end inline asm
	fma.rn.f32 	%f64, %f41, %f53, %f246;
	fma.rn.f32 	%f246, %f42, %f54, %f64;
	add.s64 	%rd33, %rd32, %rd5;
	ld.global.nc.u32 	%r39, [%rd33];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r39;
  cvt.f32.f16 %f55, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r39;
  cvt.f32.f16 %f56, high;}

	// end inline asm
	fma.rn.f32 	%f65, %f41, %f55, %f245;
	fma.rn.f32 	%f245, %f42, %f56, %f65;
	add.s64 	%rd34, %rd33, %rd5;
	ld.global.nc.u32 	%r41, [%rd34];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r41;
  cvt.f32.f16 %f57, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r41;
  cvt.f32.f16 %f58, high;}

	// end inline asm
	fma.rn.f32 	%f66, %f41, %f57, %f244;
	fma.rn.f32 	%f244, %f42, %f58, %f66;
	add.s64 	%rd44, %rd44, 768;
	add.s64 	%rd43, %rd43, 768;
	add.s32 	%r323, %r323, 192;
	setp.lt.s32 	%p3, %r323, %r8;
	@%p3 bra 	$L__BB111_4;

	st.local.v4.f32 	[%rd1], {%f251, %f250, %f249, %f248};
	st.local.v4.f32 	[%rd1+16], {%f247, %f246, %f245, %f244};

$L__BB111_6:
	shr.s32 	%r43, %r2, 31;
	shr.u32 	%r44, %r43, 27;
	add.s32 	%r45, %r2, %r44;
	shr.s32 	%r46, %r45, 5;
	shl.b32 	%r47, %r46, 2;
	add.s32 	%r7, %r17, %r47;
	mov.u32 	%r49, 2;
	mov.b32 	%r50, %f251;
	mov.u32 	%r51, 31;
	mov.u32 	%r52, 16;
	mov.u32 	%r53, -1;
	shfl.sync.bfly.b32 	%r54|%p4, %r50, %r52, %r51, %r53;
	mov.b32 	%f67, %r54;
	add.f32 	%f68, %f251, %f67;
	mov.b32 	%r55, %f68;
	mov.u32 	%r56, 8;
	shfl.sync.bfly.b32 	%r57|%p5, %r55, %r56, %r51, %r53;
	mov.b32 	%f69, %r57;
	add.f32 	%f70, %f68, %f69;
	mov.b32 	%r58, %f70;
	mov.u32 	%r59, 4;
	shfl.sync.bfly.b32 	%r60|%p6, %r58, %r59, %r51, %r53;
	mov.b32 	%f71, %r60;
	add.f32 	%f72, %f70, %f71;
	mov.b32 	%r61, %f72;
	shfl.sync.bfly.b32 	%r62|%p7, %r61, %r49, %r51, %r53;
	mov.b32 	%f73, %r62;
	add.f32 	%f74, %f72, %f73;
	mov.b32 	%r63, %f74;
	mov.u32 	%r64, 1;
	shfl.sync.bfly.b32 	%r65|%p8, %r63, %r64, %r51, %r53;
	mov.b32 	%f75, %r65;
	add.f32 	%f76, %f74, %f75;
	st.local.f32 	[%rd1], %f76;
	st.shared.f32 	[%r7], %f76;
	bar.sync 	0;
	@%p1 bra 	$L__BB111_8;

	ld.shared.f32 	%f77, [%r3];
	mov.b32 	%r66, %f77;
	shfl.sync.bfly.b32 	%r70|%p10, %r66, %r52, %r51, %r53;
	mov.b32 	%f78, %r70;
	add.f32 	%f79, %f77, %f78;
	mov.b32 	%r71, %f79;
	shfl.sync.bfly.b32 	%r73|%p11, %r71, %r56, %r51, %r53;
	mov.b32 	%f80, %r73;
	add.f32 	%f81, %f79, %f80;
	mov.b32 	%r74, %f81;
	shfl.sync.bfly.b32 	%r76|%p12, %r74, %r59, %r51, %r53;
	mov.b32 	%f82, %r76;
	add.f32 	%f83, %f81, %f82;
	mov.b32 	%r77, %f83;
	shfl.sync.bfly.b32 	%r79|%p13, %r77, %r49, %r51, %r53;
	mov.b32 	%f84, %r79;
	add.f32 	%f85, %f83, %f84;
	mov.b32 	%r80, %f85;
	shfl.sync.bfly.b32 	%r82|%p14, %r80, %r64, %r51, %r53;
	mov.b32 	%f86, %r82;
	add.f32 	%f87, %f85, %f86;
	st.local.f32 	[%rd1], %f87;

$L__BB111_8:
	bar.sync 	0;
	mov.b32 	%r83, %f250;
	shfl.sync.bfly.b32 	%r87|%p16, %r83, %r52, %r51, %r53;
	mov.b32 	%f88, %r87;
	add.f32 	%f89, %f250, %f88;
	mov.b32 	%r88, %f89;
	shfl.sync.bfly.b32 	%r90|%p17, %r88, %r56, %r51, %r53;
	mov.b32 	%f90, %r90;
	add.f32 	%f91, %f89, %f90;
	mov.b32 	%r91, %f91;
	shfl.sync.bfly.b32 	%r93|%p18, %r91, %r59, %r51, %r53;
	mov.b32 	%f92, %r93;
	add.f32 	%f93, %f91, %f92;
	mov.b32 	%r94, %f93;
	shfl.sync.bfly.b32 	%r96|%p19, %r94, %r49, %r51, %r53;
	mov.b32 	%f94, %r96;
	add.f32 	%f95, %f93, %f94;
	mov.b32 	%r97, %f95;
	shfl.sync.bfly.b32 	%r99|%p20, %r97, %r64, %r51, %r53;
	mov.b32 	%f96, %r99;
	add.f32 	%f97, %f95, %f96;
	st.local.f32 	[%rd1+4], %f97;
	st.shared.f32 	[%r7], %f97;
	bar.sync 	0;
	@%p1 bra 	$L__BB111_10;

	ld.shared.f32 	%f98, [%r3];
	mov.b32 	%r100, %f98;
	mov.u32 	%r101, 31;
	mov.u32 	%r102, 16;
	mov.u32 	%r103, -1;
	shfl.sync.bfly.b32 	%r104|%p21, %r100, %r102, %r101, %r103;
	mov.b32 	%f99, %r104;
	add.f32 	%f100, %f98, %f99;
	mov.b32 	%r105, %f100;
	mov.u32 	%r106, 8;
	shfl.sync.bfly.b32 	%r107|%p22, %r105, %r106, %r101, %r103;
	mov.b32 	%f101, %r107;
	add.f32 	%f102, %f100, %f101;
	mov.b32 	%r108, %f102;
	mov.u32 	%r109, 4;
	shfl.sync.bfly.b32 	%r110|%p23, %r108, %r109, %r101, %r103;
	mov.b32 	%f103, %r110;
	add.f32 	%f104, %f102, %f103;
	mov.b32 	%r111, %f104;
	mov.u32 	%r112, 2;
	shfl.sync.bfly.b32 	%r113|%p24, %r111, %r112, %r101, %r103;
	mov.b32 	%f105, %r113;
	add.f32 	%f106, %f104, %f105;
	mov.b32 	%r114, %f106;
	mov.u32 	%r115, 1;
	shfl.sync.bfly.b32 	%r116|%p25, %r114, %r115, %r101, %r103;
	mov.b32 	%f107, %r116;
	add.f32 	%f108, %f106, %f107;
	st.local.f32 	[%rd1+4], %f108;

$L__BB111_10:
	bar.sync 	0;
	mov.b32 	%r117, %f249;
	mov.u32 	%r118, 31;
	mov.u32 	%r119, 16;
	mov.u32 	%r120, -1;
	shfl.sync.bfly.b32 	%r121|%p27, %r117, %r119, %r118, %r120;
	mov.b32 	%f109, %r121;
	add.f32 	%f110, %f249, %f109;
	mov.b32 	%r122, %f110;
	mov.u32 	%r123, 8;
	shfl.sync.bfly.b32 	%r124|%p28, %r122, %r123, %r118, %r120;
	mov.b32 	%f111, %r124;
	add.f32 	%f112, %f110, %f111;
	mov.b32 	%r125, %f112;
	mov.u32 	%r126, 4;
	shfl.sync.bfly.b32 	%r127|%p29, %r125, %r126, %r118, %r120;
	mov.b32 	%f113, %r127;
	add.f32 	%f114, %f112, %f113;
	mov.b32 	%r128, %f114;
	mov.u32 	%r129, 2;
	shfl.sync.bfly.b32 	%r130|%p30, %r128, %r129, %r118, %r120;
	mov.b32 	%f115, %r130;
	add.f32 	%f116, %f114, %f115;
	mov.b32 	%r131, %f116;
	mov.u32 	%r132, 1;
	shfl.sync.bfly.b32 	%r133|%p31, %r131, %r132, %r118, %r120;
	mov.b32 	%f117, %r133;
	add.f32 	%f118, %f116, %f117;
	st.local.f32 	[%rd1+8], %f118;
	st.shared.f32 	[%r7], %f118;
	bar.sync 	0;
	@%p1 bra 	$L__BB111_12;

	ld.shared.f32 	%f119, [%r3];
	mov.b32 	%r134, %f119;
	shfl.sync.bfly.b32 	%r138|%p32, %r134, %r119, %r118, %r120;
	mov.b32 	%f120, %r138;
	add.f32 	%f121, %f119, %f120;
	mov.b32 	%r139, %f121;
	shfl.sync.bfly.b32 	%r141|%p33, %r139, %r123, %r118, %r120;
	mov.b32 	%f122, %r141;
	add.f32 	%f123, %f121, %f122;
	mov.b32 	%r142, %f123;
	shfl.sync.bfly.b32 	%r144|%p34, %r142, %r126, %r118, %r120;
	mov.b32 	%f124, %r144;
	add.f32 	%f125, %f123, %f124;
	mov.b32 	%r145, %f125;
	shfl.sync.bfly.b32 	%r147|%p35, %r145, %r129, %r118, %r120;
	mov.b32 	%f126, %r147;
	add.f32 	%f127, %f125, %f126;
	mov.b32 	%r148, %f127;
	shfl.sync.bfly.b32 	%r150|%p36, %r148, %r132, %r118, %r120;
	mov.b32 	%f128, %r150;
	add.f32 	%f129, %f127, %f128;
	st.local.f32 	[%rd1+8], %f129;

$L__BB111_12:
	bar.sync 	0;
	mov.b32 	%r151, %f248;
	shfl.sync.bfly.b32 	%r155|%p38, %r151, %r119, %r118, %r120;
	mov.b32 	%f130, %r155;
	add.f32 	%f131, %f248, %f130;
	mov.b32 	%r156, %f131;
	shfl.sync.bfly.b32 	%r158|%p39, %r156, %r123, %r118, %r120;
	mov.b32 	%f132, %r158;
	add.f32 	%f133, %f131, %f132;
	mov.b32 	%r159, %f133;
	shfl.sync.bfly.b32 	%r161|%p40, %r159, %r126, %r118, %r120;
	mov.b32 	%f134, %r161;
	add.f32 	%f135, %f133, %f134;
	mov.b32 	%r162, %f135;
	shfl.sync.bfly.b32 	%r164|%p41, %r162, %r129, %r118, %r120;
	mov.b32 	%f136, %r164;
	add.f32 	%f137, %f135, %f136;
	mov.b32 	%r165, %f137;
	shfl.sync.bfly.b32 	%r167|%p42, %r165, %r132, %r118, %r120;
	mov.b32 	%f138, %r167;
	add.f32 	%f139, %f137, %f138;
	st.local.f32 	[%rd1+12], %f139;
	st.shared.f32 	[%r7], %f139;
	bar.sync 	0;
	@%p1 bra 	$L__BB111_14;

	ld.shared.f32 	%f140, [%r3];
	mov.b32 	%r168, %f140;
	mov.u32 	%r169, 31;
	mov.u32 	%r170, 16;
	mov.u32 	%r171, -1;
	shfl.sync.bfly.b32 	%r172|%p43, %r168, %r170, %r169, %r171;
	mov.b32 	%f141, %r172;
	add.f32 	%f142, %f140, %f141;
	mov.b32 	%r173, %f142;
	mov.u32 	%r174, 8;
	shfl.sync.bfly.b32 	%r175|%p44, %r173, %r174, %r169, %r171;
	mov.b32 	%f143, %r175;
	add.f32 	%f144, %f142, %f143;
	mov.b32 	%r176, %f144;
	mov.u32 	%r177, 4;
	shfl.sync.bfly.b32 	%r178|%p45, %r176, %r177, %r169, %r171;
	mov.b32 	%f145, %r178;
	add.f32 	%f146, %f144, %f145;
	mov.b32 	%r179, %f146;
	mov.u32 	%r180, 2;
	shfl.sync.bfly.b32 	%r181|%p46, %r179, %r180, %r169, %r171;
	mov.b32 	%f147, %r181;
	add.f32 	%f148, %f146, %f147;
	mov.b32 	%r182, %f148;
	mov.u32 	%r183, 1;
	shfl.sync.bfly.b32 	%r184|%p47, %r182, %r183, %r169, %r171;
	mov.b32 	%f149, %r184;
	add.f32 	%f150, %f148, %f149;
	st.local.f32 	[%rd1+12], %f150;

$L__BB111_14:
	bar.sync 	0;
	mov.b32 	%r185, %f247;
	mov.u32 	%r186, 31;
	mov.u32 	%r187, 16;
	mov.u32 	%r188, -1;
	shfl.sync.bfly.b32 	%r189|%p49, %r185, %r187, %r186, %r188;
	mov.b32 	%f151, %r189;
	add.f32 	%f152, %f247, %f151;
	mov.b32 	%r190, %f152;
	mov.u32 	%r191, 8;
	shfl.sync.bfly.b32 	%r192|%p50, %r190, %r191, %r186, %r188;
	mov.b32 	%f153, %r192;
	add.f32 	%f154, %f152, %f153;
	mov.b32 	%r193, %f154;
	mov.u32 	%r194, 4;
	shfl.sync.bfly.b32 	%r195|%p51, %r193, %r194, %r186, %r188;
	mov.b32 	%f155, %r195;
	add.f32 	%f156, %f154, %f155;
	mov.b32 	%r196, %f156;
	mov.u32 	%r197, 2;
	shfl.sync.bfly.b32 	%r198|%p52, %r196, %r197, %r186, %r188;
	mov.b32 	%f157, %r198;
	add.f32 	%f158, %f156, %f157;
	mov.b32 	%r199, %f158;
	mov.u32 	%r200, 1;
	shfl.sync.bfly.b32 	%r201|%p53, %r199, %r200, %r186, %r188;
	mov.b32 	%f159, %r201;
	add.f32 	%f160, %f158, %f159;
	st.local.f32 	[%rd1+16], %f160;
	st.shared.f32 	[%r7], %f160;
	bar.sync 	0;
	@%p1 bra 	$L__BB111_16;

	ld.shared.f32 	%f161, [%r3];
	mov.b32 	%r202, %f161;
	shfl.sync.bfly.b32 	%r206|%p54, %r202, %r187, %r186, %r188;
	mov.b32 	%f162, %r206;
	add.f32 	%f163, %f161, %f162;
	mov.b32 	%r207, %f163;
	shfl.sync.bfly.b32 	%r209|%p55, %r207, %r191, %r186, %r188;
	mov.b32 	%f164, %r209;
	add.f32 	%f165, %f163, %f164;
	mov.b32 	%r210, %f165;
	shfl.sync.bfly.b32 	%r212|%p56, %r210, %r194, %r186, %r188;
	mov.b32 	%f166, %r212;
	add.f32 	%f167, %f165, %f166;
	mov.b32 	%r213, %f167;
	shfl.sync.bfly.b32 	%r215|%p57, %r213, %r197, %r186, %r188;
	mov.b32 	%f168, %r215;
	add.f32 	%f169, %f167, %f168;
	mov.b32 	%r216, %f169;
	shfl.sync.bfly.b32 	%r218|%p58, %r216, %r200, %r186, %r188;
	mov.b32 	%f170, %r218;
	add.f32 	%f171, %f169, %f170;
	st.local.f32 	[%rd1+16], %f171;

$L__BB111_16:
	bar.sync 	0;
	mov.b32 	%r219, %f246;
	shfl.sync.bfly.b32 	%r223|%p60, %r219, %r187, %r186, %r188;
	mov.b32 	%f172, %r223;
	add.f32 	%f173, %f246, %f172;
	mov.b32 	%r224, %f173;
	shfl.sync.bfly.b32 	%r226|%p61, %r224, %r191, %r186, %r188;
	mov.b32 	%f174, %r226;
	add.f32 	%f175, %f173, %f174;
	mov.b32 	%r227, %f175;
	shfl.sync.bfly.b32 	%r229|%p62, %r227, %r194, %r186, %r188;
	mov.b32 	%f176, %r229;
	add.f32 	%f177, %f175, %f176;
	mov.b32 	%r230, %f177;
	shfl.sync.bfly.b32 	%r232|%p63, %r230, %r197, %r186, %r188;
	mov.b32 	%f178, %r232;
	add.f32 	%f179, %f177, %f178;
	mov.b32 	%r233, %f179;
	shfl.sync.bfly.b32 	%r235|%p64, %r233, %r200, %r186, %r188;
	mov.b32 	%f180, %r235;
	add.f32 	%f181, %f179, %f180;
	st.local.f32 	[%rd1+20], %f181;
	st.shared.f32 	[%r7], %f181;
	bar.sync 	0;
	@%p1 bra 	$L__BB111_18;

	ld.shared.f32 	%f182, [%r3];
	mov.b32 	%r236, %f182;
	mov.u32 	%r237, 31;
	mov.u32 	%r238, 16;
	mov.u32 	%r239, -1;
	shfl.sync.bfly.b32 	%r240|%p65, %r236, %r238, %r237, %r239;
	mov.b32 	%f183, %r240;
	add.f32 	%f184, %f182, %f183;
	mov.b32 	%r241, %f184;
	mov.u32 	%r242, 8;
	shfl.sync.bfly.b32 	%r243|%p66, %r241, %r242, %r237, %r239;
	mov.b32 	%f185, %r243;
	add.f32 	%f186, %f184, %f185;
	mov.b32 	%r244, %f186;
	mov.u32 	%r245, 4;
	shfl.sync.bfly.b32 	%r246|%p67, %r244, %r245, %r237, %r239;
	mov.b32 	%f187, %r246;
	add.f32 	%f188, %f186, %f187;
	mov.b32 	%r247, %f188;
	mov.u32 	%r248, 2;
	shfl.sync.bfly.b32 	%r249|%p68, %r247, %r248, %r237, %r239;
	mov.b32 	%f189, %r249;
	add.f32 	%f190, %f188, %f189;
	mov.b32 	%r250, %f190;
	mov.u32 	%r251, 1;
	shfl.sync.bfly.b32 	%r252|%p69, %r250, %r251, %r237, %r239;
	mov.b32 	%f191, %r252;
	add.f32 	%f192, %f190, %f191;
	st.local.f32 	[%rd1+20], %f192;

$L__BB111_18:
	bar.sync 	0;
	mov.b32 	%r253, %f245;
	mov.u32 	%r254, 31;
	mov.u32 	%r255, 16;
	mov.u32 	%r256, -1;
	shfl.sync.bfly.b32 	%r257|%p71, %r253, %r255, %r254, %r256;
	mov.b32 	%f193, %r257;
	add.f32 	%f194, %f245, %f193;
	mov.b32 	%r258, %f194;
	mov.u32 	%r259, 8;
	shfl.sync.bfly.b32 	%r260|%p72, %r258, %r259, %r254, %r256;
	mov.b32 	%f195, %r260;
	add.f32 	%f196, %f194, %f195;
	mov.b32 	%r261, %f196;
	mov.u32 	%r262, 4;
	shfl.sync.bfly.b32 	%r263|%p73, %r261, %r262, %r254, %r256;
	mov.b32 	%f197, %r263;
	add.f32 	%f198, %f196, %f197;
	mov.b32 	%r264, %f198;
	mov.u32 	%r265, 2;
	shfl.sync.bfly.b32 	%r266|%p74, %r264, %r265, %r254, %r256;
	mov.b32 	%f199, %r266;
	add.f32 	%f200, %f198, %f199;
	mov.b32 	%r267, %f200;
	mov.u32 	%r268, 1;
	shfl.sync.bfly.b32 	%r269|%p75, %r267, %r268, %r254, %r256;
	mov.b32 	%f201, %r269;
	add.f32 	%f202, %f200, %f201;
	st.local.f32 	[%rd1+24], %f202;
	st.shared.f32 	[%r7], %f202;
	bar.sync 	0;
	@%p1 bra 	$L__BB111_20;

	ld.shared.f32 	%f203, [%r3];
	mov.b32 	%r270, %f203;
	shfl.sync.bfly.b32 	%r274|%p76, %r270, %r255, %r254, %r256;
	mov.b32 	%f204, %r274;
	add.f32 	%f205, %f203, %f204;
	mov.b32 	%r275, %f205;
	shfl.sync.bfly.b32 	%r277|%p77, %r275, %r259, %r254, %r256;
	mov.b32 	%f206, %r277;
	add.f32 	%f207, %f205, %f206;
	mov.b32 	%r278, %f207;
	shfl.sync.bfly.b32 	%r280|%p78, %r278, %r262, %r254, %r256;
	mov.b32 	%f208, %r280;
	add.f32 	%f209, %f207, %f208;
	mov.b32 	%r281, %f209;
	shfl.sync.bfly.b32 	%r283|%p79, %r281, %r265, %r254, %r256;
	mov.b32 	%f210, %r283;
	add.f32 	%f211, %f209, %f210;
	mov.b32 	%r284, %f211;
	shfl.sync.bfly.b32 	%r286|%p80, %r284, %r268, %r254, %r256;
	mov.b32 	%f212, %r286;
	add.f32 	%f213, %f211, %f212;
	st.local.f32 	[%rd1+24], %f213;

$L__BB111_20:
	bar.sync 	0;
	mov.b32 	%r287, %f244;
	shfl.sync.bfly.b32 	%r291|%p82, %r287, %r255, %r254, %r256;
	mov.b32 	%f214, %r291;
	add.f32 	%f215, %f244, %f214;
	mov.b32 	%r292, %f215;
	shfl.sync.bfly.b32 	%r294|%p83, %r292, %r259, %r254, %r256;
	mov.b32 	%f216, %r294;
	add.f32 	%f217, %f215, %f216;
	mov.b32 	%r295, %f217;
	shfl.sync.bfly.b32 	%r297|%p84, %r295, %r262, %r254, %r256;
	mov.b32 	%f218, %r297;
	add.f32 	%f219, %f217, %f218;
	mov.b32 	%r298, %f219;
	shfl.sync.bfly.b32 	%r300|%p85, %r298, %r265, %r254, %r256;
	mov.b32 	%f220, %r300;
	add.f32 	%f221, %f219, %f220;
	mov.b32 	%r301, %f221;
	shfl.sync.bfly.b32 	%r303|%p86, %r301, %r268, %r254, %r256;
	mov.b32 	%f222, %r303;
	add.f32 	%f223, %f221, %f222;
	st.local.f32 	[%rd1+28], %f223;
	st.shared.f32 	[%r7], %f223;
	bar.sync 	0;
	@%p1 bra 	$L__BB111_22;

	ld.shared.f32 	%f224, [%r3];
	mov.b32 	%r304, %f224;
	mov.u32 	%r305, 31;
	mov.u32 	%r306, 16;
	mov.u32 	%r307, -1;
	shfl.sync.bfly.b32 	%r308|%p87, %r304, %r306, %r305, %r307;
	mov.b32 	%f225, %r308;
	add.f32 	%f226, %f224, %f225;
	mov.b32 	%r309, %f226;
	mov.u32 	%r310, 8;
	shfl.sync.bfly.b32 	%r311|%p88, %r309, %r310, %r305, %r307;
	mov.b32 	%f227, %r311;
	add.f32 	%f228, %f226, %f227;
	mov.b32 	%r312, %f228;
	mov.u32 	%r313, 4;
	shfl.sync.bfly.b32 	%r314|%p89, %r312, %r313, %r305, %r307;
	mov.b32 	%f229, %r314;
	add.f32 	%f230, %f228, %f229;
	mov.b32 	%r315, %f230;
	mov.u32 	%r316, 2;
	shfl.sync.bfly.b32 	%r317|%p90, %r315, %r316, %r305, %r307;
	mov.b32 	%f231, %r317;
	add.f32 	%f232, %f230, %f231;
	mov.b32 	%r318, %f232;
	mov.u32 	%r319, 1;
	shfl.sync.bfly.b32 	%r320|%p91, %r318, %r319, %r305, %r307;
	mov.b32 	%f233, %r320;
	add.f32 	%f234, %f232, %f233;
	st.local.f32 	[%rd1+28], %f234;

$L__BB111_22:
	bar.sync 	0;
	setp.gt.s32 	%p92, %r2, 7;
	@%p92 bra 	$L__BB111_24;

	mad.lo.s32 	%r321, %r2, %r11, %r1;
	cvt.s64.s32 	%rd35, %r321;
	mul.lo.s32 	%r322, %r4, %r15;
	cvt.s64.s32 	%rd36, %r322;
	add.s64 	%rd37, %rd36, %rd35;
	mul.wide.s32 	%rd38, %r2, 4;
	add.s64 	%rd39, %rd1, %rd38;
	ld.local.f32 	%f235, [%rd39];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f235;}

	// end inline asm
	cvta.to.global.u64 	%rd40, %rd16;
	shl.b64 	%rd41, %rd37, 1;
	add.s64 	%rd42, %rd40, %rd41;
	st.global.u16 	[%rd42], %rs1;

$L__BB111_24:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_1_bs_224
.visible .entry ggml_matvec_f16_ncols_1_bs_224(
	.param .u64 ggml_matvec_f16_ncols_1_bs_224_param_0,
	.param .u64 ggml_matvec_f16_ncols_1_bs_224_param_1,
	.param .u64 ggml_matvec_f16_ncols_1_bs_224_param_2,
	.param .u32 ggml_matvec_f16_ncols_1_bs_224_param_3,
	.param .u32 ggml_matvec_f16_ncols_1_bs_224_param_4,
	.param .u32 ggml_matvec_f16_ncols_1_bs_224_param_5,
	.param .u32 ggml_matvec_f16_ncols_1_bs_224_param_6,
	.param .u32 ggml_matvec_f16_ncols_1_bs_224_param_7,
	.param .u32 ggml_matvec_f16_ncols_1_bs_224_param_8,
	.param .u32 ggml_matvec_f16_ncols_1_bs_224_param_9,
	.param .u32 ggml_matvec_f16_ncols_1_bs_224_param_10,
	.param .u32 ggml_matvec_f16_ncols_1_bs_224_param_11
)
{
	.reg .pred 	%p<19>;
	.reg .b16 	%rs<2>;
	.reg .f32 	%f<69>;
	.reg .b32 	%r<100>;
	.reg .b64 	%rd<44>;


	ld.param.u64 	%rd18, [ggml_matvec_f16_ncols_1_bs_224_param_0];
	ld.param.u64 	%rd19, [ggml_matvec_f16_ncols_1_bs_224_param_1];
	ld.param.u64 	%rd17, [ggml_matvec_f16_ncols_1_bs_224_param_2];
	ld.param.u32 	%r13, [ggml_matvec_f16_ncols_1_bs_224_param_3];
	ld.param.u32 	%r16, [ggml_matvec_f16_ncols_1_bs_224_param_5];
	ld.param.u32 	%r14, [ggml_matvec_f16_ncols_1_bs_224_param_7];
	ld.param.u32 	%r17, [ggml_matvec_f16_ncols_1_bs_224_param_8];
	ld.param.u32 	%r18, [ggml_matvec_f16_ncols_1_bs_224_param_9];
	ld.param.u32 	%r19, [ggml_matvec_f16_ncols_1_bs_224_param_10];
	ld.param.u32 	%r15, [ggml_matvec_f16_ncols_1_bs_224_param_11];
	cvta.to.global.u64 	%rd1, %rd19;
	cvta.to.global.u64 	%rd2, %rd18;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r20, %r1, %r17;
	mov.u32 	%r21, %ctaid.x;
	mul.lo.s32 	%r22, %r21, %r16;
	mad.lo.s32 	%r23, %r20, %r18, %r22;
	cvt.s64.s32 	%rd3, %r23;
	mul.lo.s32 	%r24, %r1, %r19;
	cvt.s64.s32 	%rd4, %r24;
	mov.u32 	%r2, %tid.x;
	setp.gt.s32 	%p1, %r2, 31;
	shl.b32 	%r25, %r2, 2;
	mov.u32 	%r26, data_mmv;
	add.s32 	%r3, %r26, %r25;
	@%p1 bra 	$L__BB112_2;

	mov.u32 	%r27, 0;
	st.shared.u32 	[%r3], %r27;

$L__BB112_2:
	bar.sync 	0;
	setp.ge.s32 	%p2, %r2, %r13;
	mov.f32 	%f67, 0f00000000;
	@%p2 bra 	$L__BB112_9;

	not.b32 	%r28, %r2;
	add.s32 	%r4, %r28, %r13;
	shr.u32 	%r29, %r4, 5;
	mul.wide.u32 	%rd20, %r29, 613566757;
	shr.u64 	%rd21, %rd20, 32;
	cvt.u32.u64 	%r30, %rd21;
	add.s32 	%r31, %r30, 1;
	and.b32  	%r97, %r31, 3;
	setp.eq.s32 	%p3, %r97, 0;
	mov.f32 	%f67, 0f00000000;
	mov.u32 	%r98, %r2;
	@%p3 bra 	$L__BB112_6;

	mul.wide.s32 	%rd22, %r2, 2;
	add.s64 	%rd23, %rd22, %rd4;
	shl.b64 	%rd24, %rd23, 1;
	add.s64 	%rd41, %rd1, %rd24;
	add.s64 	%rd25, %rd22, %rd3;
	shl.b64 	%rd26, %rd25, 1;
	add.s64 	%rd40, %rd2, %rd26;
	mov.f32 	%f67, 0f00000000;
	mov.u32 	%r98, %r2;

$L__BB112_5:
	.pragma "nounroll";
	ld.global.nc.u32 	%r32, [%rd40];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r32;
  cvt.f32.f16 %f15, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r32;
  cvt.f32.f16 %f16, high;}

	// end inline asm
	ld.global.nc.u32 	%r34, [%rd41];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r34;
  cvt.f32.f16 %f17, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r34;
  cvt.f32.f16 %f18, high;}

	// end inline asm
	fma.rn.f32 	%f19, %f15, %f17, %f67;
	fma.rn.f32 	%f67, %f16, %f18, %f19;
	add.s32 	%r98, %r98, 224;
	add.s64 	%rd41, %rd41, 896;
	add.s64 	%rd40, %rd40, 896;
	add.s32 	%r97, %r97, -1;
	setp.ne.s32 	%p4, %r97, 0;
	@%p4 bra 	$L__BB112_5;

$L__BB112_6:
	setp.lt.u32 	%p5, %r4, 672;
	@%p5 bra 	$L__BB112_9;

	mul.wide.s32 	%rd27, %r98, 2;
	add.s64 	%rd28, %rd27, %rd3;
	shl.b64 	%rd29, %rd28, 1;
	add.s64 	%rd30, %rd2, %rd29;
	add.s64 	%rd43, %rd30, 1792;
	add.s64 	%rd31, %rd27, %rd4;
	shl.b64 	%rd32, %rd31, 1;
	add.s64 	%rd33, %rd1, %rd32;
	add.s64 	%rd42, %rd33, 1792;

$L__BB112_8:
	ld.global.nc.u32 	%r36, [%rd43+-1792];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r36;
  cvt.f32.f16 %f20, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r36;
  cvt.f32.f16 %f21, high;}

	// end inline asm
	ld.global.nc.u32 	%r38, [%rd42+-1792];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r38;
  cvt.f32.f16 %f22, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r38;
  cvt.f32.f16 %f23, high;}

	// end inline asm
	fma.rn.f32 	%f36, %f20, %f22, %f67;
	fma.rn.f32 	%f37, %f21, %f23, %f36;
	ld.global.nc.u32 	%r40, [%rd43+-896];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r40;
  cvt.f32.f16 %f24, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r40;
  cvt.f32.f16 %f25, high;}

	// end inline asm
	ld.global.nc.u32 	%r42, [%rd42+-896];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r42;
  cvt.f32.f16 %f26, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r42;
  cvt.f32.f16 %f27, high;}

	// end inline asm
	fma.rn.f32 	%f38, %f24, %f26, %f37;
	fma.rn.f32 	%f39, %f25, %f27, %f38;
	ld.global.nc.u32 	%r44, [%rd43];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r44;
  cvt.f32.f16 %f28, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r44;
  cvt.f32.f16 %f29, high;}

	// end inline asm
	ld.global.nc.u32 	%r46, [%rd42];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r46;
  cvt.f32.f16 %f30, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r46;
  cvt.f32.f16 %f31, high;}

	// end inline asm
	fma.rn.f32 	%f40, %f28, %f30, %f39;
	fma.rn.f32 	%f41, %f29, %f31, %f40;
	ld.global.nc.u32 	%r48, [%rd43+896];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r48;
  cvt.f32.f16 %f32, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r48;
  cvt.f32.f16 %f33, high;}

	// end inline asm
	ld.global.nc.u32 	%r50, [%rd42+896];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r50;
  cvt.f32.f16 %f34, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r50;
  cvt.f32.f16 %f35, high;}

	// end inline asm
	fma.rn.f32 	%f42, %f32, %f34, %f41;
	fma.rn.f32 	%f67, %f33, %f35, %f42;
	add.s64 	%rd43, %rd43, 3584;
	add.s64 	%rd42, %rd42, 3584;
	add.s32 	%r98, %r98, 896;
	setp.lt.s32 	%p6, %r98, %r13;
	@%p6 bra 	$L__BB112_8;

$L__BB112_9:
	mov.b32 	%r52, %f67;
	mov.u32 	%r53, 31;
	mov.u32 	%r54, 16;
	mov.u32 	%r55, -1;
	shfl.sync.bfly.b32 	%r56|%p7, %r52, %r54, %r53, %r55;
	mov.b32 	%f43, %r56;
	add.f32 	%f44, %f67, %f43;
	mov.b32 	%r57, %f44;
	mov.u32 	%r58, 8;
	shfl.sync.bfly.b32 	%r59|%p8, %r57, %r58, %r53, %r55;
	mov.b32 	%f45, %r59;
	add.f32 	%f46, %f44, %f45;
	mov.b32 	%r60, %f46;
	mov.u32 	%r61, 4;
	shfl.sync.bfly.b32 	%r62|%p9, %r60, %r61, %r53, %r55;
	mov.b32 	%f47, %r62;
	add.f32 	%f48, %f46, %f47;
	mov.b32 	%r63, %f48;
	mov.u32 	%r64, 2;
	shfl.sync.bfly.b32 	%r65|%p10, %r63, %r64, %r53, %r55;
	mov.b32 	%f49, %r65;
	add.f32 	%f50, %f48, %f49;
	mov.b32 	%r66, %f50;
	mov.u32 	%r67, 1;
	shfl.sync.bfly.b32 	%r68|%p11, %r66, %r67, %r53, %r55;
	mov.b32 	%f51, %r68;
	add.f32 	%f68, %f50, %f51;
	shr.s32 	%r69, %r2, 31;
	shr.u32 	%r70, %r69, 27;
	add.s32 	%r71, %r2, %r70;
	shr.s32 	%r72, %r71, 5;
	shl.b32 	%r73, %r72, 2;
	add.s32 	%r75, %r26, %r73;
	st.shared.f32 	[%r75], %f68;
	bar.sync 	0;
	@%p1 bra 	$L__BB112_11;

	ld.shared.f32 	%f52, [%r3];
	mov.b32 	%r76, %f52;
	shfl.sync.bfly.b32 	%r80|%p13, %r76, %r54, %r53, %r55;
	mov.b32 	%f53, %r80;
	add.f32 	%f54, %f52, %f53;
	mov.b32 	%r81, %f54;
	shfl.sync.bfly.b32 	%r83|%p14, %r81, %r58, %r53, %r55;
	mov.b32 	%f55, %r83;
	add.f32 	%f56, %f54, %f55;
	mov.b32 	%r84, %f56;
	shfl.sync.bfly.b32 	%r86|%p15, %r84, %r61, %r53, %r55;
	mov.b32 	%f57, %r86;
	add.f32 	%f58, %f56, %f57;
	mov.b32 	%r87, %f58;
	shfl.sync.bfly.b32 	%r89|%p16, %r87, %r64, %r53, %r55;
	mov.b32 	%f59, %r89;
	add.f32 	%f60, %f58, %f59;
	mov.b32 	%r90, %f60;
	shfl.sync.bfly.b32 	%r92|%p17, %r90, %r67, %r53, %r55;
	mov.b32 	%f61, %r92;
	add.f32 	%f68, %f60, %f61;

$L__BB112_11:
	bar.sync 	0;
	setp.gt.s32 	%p18, %r2, 0;
	@%p18 bra 	$L__BB112_13;

	mad.lo.s32 	%r94, %r2, %r14, %r21;
	cvt.s64.s32 	%rd34, %r94;
	mul.lo.s32 	%r95, %r1, %r15;
	cvt.s64.s32 	%rd35, %r95;
	add.s64 	%rd36, %rd35, %rd34;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f68;}

	// end inline asm
	cvta.to.global.u64 	%rd37, %rd17;
	shl.b64 	%rd38, %rd36, 1;
	add.s64 	%rd39, %rd37, %rd38;
	st.global.u16 	[%rd39], %rs1;

$L__BB112_13:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_2_bs_224
.visible .entry ggml_matvec_f16_ncols_2_bs_224(
	.param .u64 ggml_matvec_f16_ncols_2_bs_224_param_0,
	.param .u64 ggml_matvec_f16_ncols_2_bs_224_param_1,
	.param .u64 ggml_matvec_f16_ncols_2_bs_224_param_2,
	.param .u32 ggml_matvec_f16_ncols_2_bs_224_param_3,
	.param .u32 ggml_matvec_f16_ncols_2_bs_224_param_4,
	.param .u32 ggml_matvec_f16_ncols_2_bs_224_param_5,
	.param .u32 ggml_matvec_f16_ncols_2_bs_224_param_6,
	.param .u32 ggml_matvec_f16_ncols_2_bs_224_param_7,
	.param .u32 ggml_matvec_f16_ncols_2_bs_224_param_8,
	.param .u32 ggml_matvec_f16_ncols_2_bs_224_param_9,
	.param .u32 ggml_matvec_f16_ncols_2_bs_224_param_10,
	.param .u32 ggml_matvec_f16_ncols_2_bs_224_param_11
)
{
	.local .align 8 .b8 	__local_depot113[8];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<30>;
	.reg .b16 	%rs<2>;
	.reg .f32 	%f<116>;
	.reg .b32 	%r<144>;
	.reg .b64 	%rd<66>;


	mov.u64 	%SPL, __local_depot113;
	ld.param.u64 	%rd27, [ggml_matvec_f16_ncols_2_bs_224_param_0];
	ld.param.u64 	%rd28, [ggml_matvec_f16_ncols_2_bs_224_param_1];
	ld.param.u64 	%rd26, [ggml_matvec_f16_ncols_2_bs_224_param_2];
	ld.param.u32 	%r15, [ggml_matvec_f16_ncols_2_bs_224_param_3];
	ld.param.u32 	%r19, [ggml_matvec_f16_ncols_2_bs_224_param_5];
	ld.param.u32 	%r16, [ggml_matvec_f16_ncols_2_bs_224_param_6];
	ld.param.u32 	%r17, [ggml_matvec_f16_ncols_2_bs_224_param_7];
	ld.param.u32 	%r20, [ggml_matvec_f16_ncols_2_bs_224_param_8];
	ld.param.u32 	%r21, [ggml_matvec_f16_ncols_2_bs_224_param_9];
	ld.param.u32 	%r22, [ggml_matvec_f16_ncols_2_bs_224_param_10];
	ld.param.u32 	%r18, [ggml_matvec_f16_ncols_2_bs_224_param_11];
	cvta.to.global.u64 	%rd1, %rd28;
	cvta.to.global.u64 	%rd2, %rd27;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r23, %r1, %r20;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r24, %r2, %r19;
	mad.lo.s32 	%r25, %r23, %r21, %r24;
	cvt.s64.s32 	%rd4, %r25;
	mul.lo.s32 	%r26, %r1, %r22;
	cvt.s64.s32 	%rd5, %r26;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r27, %r3, 2;
	mov.u32 	%r28, data_mmv;
	add.s32 	%r4, %r28, %r27;
	@%p1 bra 	$L__BB113_2;

	mov.u32 	%r29, 0;
	st.shared.u32 	[%r4], %r29;

$L__BB113_2:
	bar.sync 	0;
	mov.f32 	%f114, 0f00000000;
	st.local.v2.f32 	[%rd3], {%f114, %f114};
	setp.ge.s32 	%p2, %r3, %r15;
	mov.f32 	%f115, %f114;
	@%p2 bra 	$L__BB113_11;

	not.b32 	%r30, %r3;
	add.s32 	%r5, %r30, %r15;
	shr.u32 	%r31, %r5, 5;
	mul.wide.u32 	%rd30, %r31, 613566757;
	shr.u64 	%rd31, %rd30, 32;
	cvt.u32.u64 	%r32, %rd31;
	add.s32 	%r33, %r32, 1;
	and.b32  	%r141, %r33, 3;
	setp.eq.s32 	%p3, %r141, 0;
	mov.f32 	%f114, 0f00000000;
	mov.u32 	%r142, %r3;
	@%p3 bra 	$L__BB113_7;

	mul.wide.s32 	%rd32, %r16, 2;
	mul.wide.s32 	%rd33, %r3, 2;
	add.s64 	%rd34, %rd32, %rd33;
	add.s64 	%rd35, %rd34, %rd5;
	shl.b64 	%rd36, %rd35, 1;
	add.s64 	%rd62, %rd1, %rd36;
	add.s64 	%rd37, %rd33, %rd5;
	shl.b64 	%rd38, %rd37, 1;
	add.s64 	%rd61, %rd1, %rd38;
	add.s64 	%rd39, %rd33, %rd4;
	shl.b64 	%rd40, %rd39, 1;
	add.s64 	%rd60, %rd2, %rd40;
	mov.f32 	%f114, 0f00000000;
	mov.f32 	%f115, %f114;
	mov.u32 	%r142, %r3;

$L__BB113_5:
	.pragma "nounroll";
	ld.global.nc.u32 	%r34, [%rd60];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r34;
  cvt.f32.f16 %f19, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r34;
  cvt.f32.f16 %f20, high;}

	// end inline asm
	ld.global.nc.u32 	%r36, [%rd61];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r36;
  cvt.f32.f16 %f21, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r36;
  cvt.f32.f16 %f22, high;}

	// end inline asm
	fma.rn.f32 	%f25, %f19, %f21, %f115;
	fma.rn.f32 	%f115, %f20, %f22, %f25;
	ld.global.nc.u32 	%r38, [%rd62];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r38;
  cvt.f32.f16 %f23, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r38;
  cvt.f32.f16 %f24, high;}

	// end inline asm
	fma.rn.f32 	%f26, %f19, %f23, %f114;
	fma.rn.f32 	%f114, %f20, %f24, %f26;
	add.s32 	%r142, %r142, 224;
	add.s64 	%rd62, %rd62, 896;
	add.s64 	%rd61, %rd61, 896;
	add.s64 	%rd60, %rd60, 896;
	add.s32 	%r141, %r141, -1;
	setp.ne.s32 	%p4, %r141, 0;
	@%p4 bra 	$L__BB113_5;

	st.local.v2.f32 	[%rd3], {%f115, %f114};

$L__BB113_7:
	setp.lt.u32 	%p5, %r5, 672;
	@%p5 bra 	$L__BB113_11;

	mul.wide.s32 	%rd41, %r142, 2;
	add.s64 	%rd42, %rd41, %rd4;
	shl.b64 	%rd43, %rd42, 1;
	add.s64 	%rd44, %rd2, %rd43;
	add.s64 	%rd65, %rd44, 1792;
	add.s64 	%rd45, %rd41, %rd5;
	shl.b64 	%rd46, %rd45, 1;
	add.s64 	%rd47, %rd1, %rd46;
	add.s64 	%rd64, %rd47, 2688;
	mul.wide.s32 	%rd48, %r16, 2;
	add.s64 	%rd49, %rd45, %rd48;
	shl.b64 	%rd50, %rd49, 1;
	add.s64 	%rd51, %rd1, %rd50;
	add.s64 	%rd63, %rd51, 1792;

$L__BB113_9:
	ld.global.nc.u32 	%r40, [%rd65+-1792];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r40;
  cvt.f32.f16 %f27, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r40;
  cvt.f32.f16 %f28, high;}

	// end inline asm
	ld.global.nc.u32 	%r42, [%rd64+-2688];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r42;
  cvt.f32.f16 %f29, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r42;
  cvt.f32.f16 %f30, high;}

	// end inline asm
	fma.rn.f32 	%f51, %f27, %f29, %f115;
	fma.rn.f32 	%f52, %f28, %f30, %f51;
	ld.global.nc.u32 	%r44, [%rd63+-1792];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r44;
  cvt.f32.f16 %f31, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r44;
  cvt.f32.f16 %f32, high;}

	// end inline asm
	fma.rn.f32 	%f53, %f27, %f31, %f114;
	fma.rn.f32 	%f54, %f28, %f32, %f53;
	ld.global.nc.u32 	%r46, [%rd65+-896];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r46;
  cvt.f32.f16 %f33, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r46;
  cvt.f32.f16 %f34, high;}

	// end inline asm
	ld.global.nc.u32 	%r48, [%rd64+-1792];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r48;
  cvt.f32.f16 %f35, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r48;
  cvt.f32.f16 %f36, high;}

	// end inline asm
	fma.rn.f32 	%f55, %f33, %f35, %f52;
	fma.rn.f32 	%f56, %f34, %f36, %f55;
	ld.global.nc.u32 	%r50, [%rd63+-896];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r50;
  cvt.f32.f16 %f37, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r50;
  cvt.f32.f16 %f38, high;}

	// end inline asm
	fma.rn.f32 	%f57, %f33, %f37, %f54;
	fma.rn.f32 	%f58, %f34, %f38, %f57;
	ld.global.nc.u32 	%r52, [%rd65];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r52;
  cvt.f32.f16 %f39, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r52;
  cvt.f32.f16 %f40, high;}

	// end inline asm
	ld.global.nc.u32 	%r54, [%rd64+-896];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r54;
  cvt.f32.f16 %f41, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r54;
  cvt.f32.f16 %f42, high;}

	// end inline asm
	fma.rn.f32 	%f59, %f39, %f41, %f56;
	fma.rn.f32 	%f60, %f40, %f42, %f59;
	ld.global.nc.u32 	%r56, [%rd63];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r56;
  cvt.f32.f16 %f43, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r56;
  cvt.f32.f16 %f44, high;}

	// end inline asm
	fma.rn.f32 	%f61, %f39, %f43, %f58;
	fma.rn.f32 	%f62, %f40, %f44, %f61;
	ld.global.nc.u32 	%r58, [%rd65+896];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r58;
  cvt.f32.f16 %f45, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r58;
  cvt.f32.f16 %f46, high;}

	// end inline asm
	ld.global.nc.u32 	%r60, [%rd64];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r60;
  cvt.f32.f16 %f47, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r60;
  cvt.f32.f16 %f48, high;}

	// end inline asm
	fma.rn.f32 	%f63, %f45, %f47, %f60;
	fma.rn.f32 	%f115, %f46, %f48, %f63;
	ld.global.nc.u32 	%r62, [%rd63+896];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r62;
  cvt.f32.f16 %f49, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r62;
  cvt.f32.f16 %f50, high;}

	// end inline asm
	fma.rn.f32 	%f64, %f45, %f49, %f62;
	fma.rn.f32 	%f114, %f46, %f50, %f64;
	add.s64 	%rd65, %rd65, 3584;
	add.s64 	%rd64, %rd64, 3584;
	add.s64 	%rd63, %rd63, 3584;
	add.s32 	%r142, %r142, 896;
	setp.lt.s32 	%p6, %r142, %r15;
	@%p6 bra 	$L__BB113_9;

	st.local.v2.f32 	[%rd3], {%f115, %f114};

$L__BB113_11:
	shr.s32 	%r64, %r3, 31;
	shr.u32 	%r65, %r64, 27;
	add.s32 	%r66, %r3, %r65;
	shr.s32 	%r67, %r66, 5;
	shl.b32 	%r68, %r67, 2;
	add.s32 	%r14, %r28, %r68;
	mov.u32 	%r70, 2;
	mov.b32 	%r71, %f115;
	mov.u32 	%r72, 31;
	mov.u32 	%r73, 16;
	mov.u32 	%r74, -1;
	shfl.sync.bfly.b32 	%r75|%p7, %r71, %r73, %r72, %r74;
	mov.b32 	%f65, %r75;
	add.f32 	%f66, %f115, %f65;
	mov.b32 	%r76, %f66;
	mov.u32 	%r77, 8;
	shfl.sync.bfly.b32 	%r78|%p8, %r76, %r77, %r72, %r74;
	mov.b32 	%f67, %r78;
	add.f32 	%f68, %f66, %f67;
	mov.b32 	%r79, %f68;
	mov.u32 	%r80, 4;
	shfl.sync.bfly.b32 	%r81|%p9, %r79, %r80, %r72, %r74;
	mov.b32 	%f69, %r81;
	add.f32 	%f70, %f68, %f69;
	mov.b32 	%r82, %f70;
	shfl.sync.bfly.b32 	%r83|%p10, %r82, %r70, %r72, %r74;
	mov.b32 	%f71, %r83;
	add.f32 	%f72, %f70, %f71;
	mov.b32 	%r84, %f72;
	mov.u32 	%r85, 1;
	shfl.sync.bfly.b32 	%r86|%p11, %r84, %r85, %r72, %r74;
	mov.b32 	%f73, %r86;
	add.f32 	%f74, %f72, %f73;
	st.local.f32 	[%rd3], %f74;
	st.shared.f32 	[%r14], %f74;
	bar.sync 	0;
	@%p1 bra 	$L__BB113_13;

	ld.shared.f32 	%f75, [%r4];
	mov.b32 	%r87, %f75;
	shfl.sync.bfly.b32 	%r91|%p13, %r87, %r73, %r72, %r74;
	mov.b32 	%f76, %r91;
	add.f32 	%f77, %f75, %f76;
	mov.b32 	%r92, %f77;
	shfl.sync.bfly.b32 	%r94|%p14, %r92, %r77, %r72, %r74;
	mov.b32 	%f78, %r94;
	add.f32 	%f79, %f77, %f78;
	mov.b32 	%r95, %f79;
	shfl.sync.bfly.b32 	%r97|%p15, %r95, %r80, %r72, %r74;
	mov.b32 	%f80, %r97;
	add.f32 	%f81, %f79, %f80;
	mov.b32 	%r98, %f81;
	shfl.sync.bfly.b32 	%r100|%p16, %r98, %r70, %r72, %r74;
	mov.b32 	%f82, %r100;
	add.f32 	%f83, %f81, %f82;
	mov.b32 	%r101, %f83;
	shfl.sync.bfly.b32 	%r103|%p17, %r101, %r85, %r72, %r74;
	mov.b32 	%f84, %r103;
	add.f32 	%f85, %f83, %f84;
	st.local.f32 	[%rd3], %f85;

$L__BB113_13:
	bar.sync 	0;
	mov.b32 	%r104, %f114;
	shfl.sync.bfly.b32 	%r108|%p19, %r104, %r73, %r72, %r74;
	mov.b32 	%f86, %r108;
	add.f32 	%f87, %f114, %f86;
	mov.b32 	%r109, %f87;
	shfl.sync.bfly.b32 	%r111|%p20, %r109, %r77, %r72, %r74;
	mov.b32 	%f88, %r111;
	add.f32 	%f89, %f87, %f88;
	mov.b32 	%r112, %f89;
	shfl.sync.bfly.b32 	%r114|%p21, %r112, %r80, %r72, %r74;
	mov.b32 	%f90, %r114;
	add.f32 	%f91, %f89, %f90;
	mov.b32 	%r115, %f91;
	shfl.sync.bfly.b32 	%r117|%p22, %r115, %r70, %r72, %r74;
	mov.b32 	%f92, %r117;
	add.f32 	%f93, %f91, %f92;
	mov.b32 	%r118, %f93;
	shfl.sync.bfly.b32 	%r120|%p23, %r118, %r85, %r72, %r74;
	mov.b32 	%f94, %r120;
	add.f32 	%f95, %f93, %f94;
	st.local.f32 	[%rd3+4], %f95;
	st.shared.f32 	[%r14], %f95;
	bar.sync 	0;
	@%p1 bra 	$L__BB113_15;

	ld.shared.f32 	%f96, [%r4];
	mov.b32 	%r121, %f96;
	mov.u32 	%r122, 31;
	mov.u32 	%r123, 16;
	mov.u32 	%r124, -1;
	shfl.sync.bfly.b32 	%r125|%p24, %r121, %r123, %r122, %r124;
	mov.b32 	%f97, %r125;
	add.f32 	%f98, %f96, %f97;
	mov.b32 	%r126, %f98;
	mov.u32 	%r127, 8;
	shfl.sync.bfly.b32 	%r128|%p25, %r126, %r127, %r122, %r124;
	mov.b32 	%f99, %r128;
	add.f32 	%f100, %f98, %f99;
	mov.b32 	%r129, %f100;
	mov.u32 	%r130, 4;
	shfl.sync.bfly.b32 	%r131|%p26, %r129, %r130, %r122, %r124;
	mov.b32 	%f101, %r131;
	add.f32 	%f102, %f100, %f101;
	mov.b32 	%r132, %f102;
	mov.u32 	%r133, 2;
	shfl.sync.bfly.b32 	%r134|%p27, %r132, %r133, %r122, %r124;
	mov.b32 	%f103, %r134;
	add.f32 	%f104, %f102, %f103;
	mov.b32 	%r135, %f104;
	mov.u32 	%r136, 1;
	shfl.sync.bfly.b32 	%r137|%p28, %r135, %r136, %r122, %r124;
	mov.b32 	%f105, %r137;
	add.f32 	%f106, %f104, %f105;
	st.local.f32 	[%rd3+4], %f106;

$L__BB113_15:
	bar.sync 	0;
	setp.gt.s32 	%p29, %r3, 1;
	@%p29 bra 	$L__BB113_17;

	mad.lo.s32 	%r138, %r3, %r17, %r2;
	cvt.s64.s32 	%rd52, %r138;
	mul.lo.s32 	%r139, %r1, %r18;
	cvt.s64.s32 	%rd53, %r139;
	add.s64 	%rd54, %rd53, %rd52;
	mul.wide.s32 	%rd55, %r3, 4;
	add.s64 	%rd56, %rd3, %rd55;
	ld.local.f32 	%f107, [%rd56];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f107;}

	// end inline asm
	cvta.to.global.u64 	%rd57, %rd26;
	shl.b64 	%rd58, %rd54, 1;
	add.s64 	%rd59, %rd57, %rd58;
	st.global.u16 	[%rd59], %rs1;

$L__BB113_17:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_3_bs_224
.visible .entry ggml_matvec_f16_ncols_3_bs_224(
	.param .u64 ggml_matvec_f16_ncols_3_bs_224_param_0,
	.param .u64 ggml_matvec_f16_ncols_3_bs_224_param_1,
	.param .u64 ggml_matvec_f16_ncols_3_bs_224_param_2,
	.param .u32 ggml_matvec_f16_ncols_3_bs_224_param_3,
	.param .u32 ggml_matvec_f16_ncols_3_bs_224_param_4,
	.param .u32 ggml_matvec_f16_ncols_3_bs_224_param_5,
	.param .u32 ggml_matvec_f16_ncols_3_bs_224_param_6,
	.param .u32 ggml_matvec_f16_ncols_3_bs_224_param_7,
	.param .u32 ggml_matvec_f16_ncols_3_bs_224_param_8,
	.param .u32 ggml_matvec_f16_ncols_3_bs_224_param_9,
	.param .u32 ggml_matvec_f16_ncols_3_bs_224_param_10,
	.param .u32 ggml_matvec_f16_ncols_3_bs_224_param_11
)
{
	.local .align 4 .b8 	__local_depot114[12];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<41>;
	.reg .b16 	%rs<2>;
	.reg .f32 	%f<168>;
	.reg .b32 	%r<195>;
	.reg .b64 	%rd<74>;


	mov.u64 	%SPL, __local_depot114;
	ld.param.u64 	%rd29, [ggml_matvec_f16_ncols_3_bs_224_param_0];
	ld.param.u64 	%rd30, [ggml_matvec_f16_ncols_3_bs_224_param_1];
	ld.param.u64 	%rd28, [ggml_matvec_f16_ncols_3_bs_224_param_2];
	ld.param.u32 	%r15, [ggml_matvec_f16_ncols_3_bs_224_param_3];
	ld.param.u32 	%r19, [ggml_matvec_f16_ncols_3_bs_224_param_5];
	ld.param.u32 	%r16, [ggml_matvec_f16_ncols_3_bs_224_param_6];
	ld.param.u32 	%r17, [ggml_matvec_f16_ncols_3_bs_224_param_7];
	ld.param.u32 	%r20, [ggml_matvec_f16_ncols_3_bs_224_param_8];
	ld.param.u32 	%r21, [ggml_matvec_f16_ncols_3_bs_224_param_9];
	ld.param.u32 	%r22, [ggml_matvec_f16_ncols_3_bs_224_param_10];
	ld.param.u32 	%r18, [ggml_matvec_f16_ncols_3_bs_224_param_11];
	cvta.to.global.u64 	%rd73, %rd30;
	cvta.to.global.u64 	%rd2, %rd29;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r23, %r1, %r20;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r24, %r2, %r19;
	mad.lo.s32 	%r25, %r23, %r21, %r24;
	cvt.s64.s32 	%rd4, %r25;
	mul.lo.s32 	%r26, %r1, %r22;
	cvt.s64.s32 	%rd5, %r26;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r27, %r3, 2;
	mov.u32 	%r28, data_mmv;
	add.s32 	%r4, %r28, %r27;
	@%p1 bra 	$L__BB114_2;

	mov.u32 	%r29, 0;
	st.shared.u32 	[%r4], %r29;

$L__BB114_2:
	bar.sync 	0;
	mov.f32 	%f165, 0f00000000;
	mov.u32 	%r30, 0;
	st.local.u32 	[%rd3], %r30;
	st.local.u32 	[%rd3+4], %r30;
	st.local.u32 	[%rd3+8], %r30;
	setp.ge.s32 	%p2, %r3, %r15;
	mov.f32 	%f166, %f165;
	mov.f32 	%f167, %f165;
	@%p2 bra 	$L__BB114_11;

	not.b32 	%r31, %r3;
	add.s32 	%r5, %r31, %r15;
	shr.u32 	%r32, %r5, 5;
	mul.wide.u32 	%rd32, %r32, 613566757;
	shr.u64 	%rd33, %rd32, 32;
	cvt.u32.u64 	%r33, %rd33;
	add.s32 	%r34, %r33, 1;
	and.b32  	%r192, %r34, 3;
	setp.eq.s32 	%p3, %r192, 0;
	mov.f32 	%f165, 0f00000000;
	mov.u32 	%r193, %r3;
	@%p3 bra 	$L__BB114_7;

	shl.b32 	%r35, %r16, 1;
	add.s32 	%r36, %r3, %r35;
	mul.wide.s32 	%rd34, %r36, 2;
	add.s64 	%rd35, %rd34, %rd5;
	shl.b64 	%rd36, %rd35, 1;
	add.s64 	%rd71, %rd73, %rd36;
	mul.wide.s32 	%rd37, %r16, 2;
	mul.wide.s32 	%rd38, %r3, 2;
	add.s64 	%rd39, %rd37, %rd38;
	add.s64 	%rd40, %rd39, %rd5;
	shl.b64 	%rd41, %rd40, 1;
	add.s64 	%rd70, %rd73, %rd41;
	add.s64 	%rd42, %rd38, %rd5;
	shl.b64 	%rd43, %rd42, 1;
	add.s64 	%rd69, %rd73, %rd43;
	add.s64 	%rd44, %rd38, %rd4;
	shl.b64 	%rd45, %rd44, 1;
	add.s64 	%rd68, %rd2, %rd45;
	mov.f32 	%f165, 0f00000000;
	mov.f32 	%f166, %f165;
	mov.f32 	%f167, %f165;
	mov.u32 	%r193, %r3;

$L__BB114_5:
	.pragma "nounroll";
	ld.global.nc.u32 	%r37, [%rd68];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r37;
  cvt.f32.f16 %f28, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r37;
  cvt.f32.f16 %f29, high;}

	// end inline asm
	ld.global.nc.u32 	%r39, [%rd69];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r39;
  cvt.f32.f16 %f30, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r39;
  cvt.f32.f16 %f31, high;}

	// end inline asm
	fma.rn.f32 	%f36, %f28, %f30, %f167;
	fma.rn.f32 	%f167, %f29, %f31, %f36;
	ld.global.nc.u32 	%r41, [%rd70];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r41;
  cvt.f32.f16 %f32, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r41;
  cvt.f32.f16 %f33, high;}

	// end inline asm
	fma.rn.f32 	%f37, %f28, %f32, %f166;
	fma.rn.f32 	%f166, %f29, %f33, %f37;
	ld.global.nc.u32 	%r43, [%rd71];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r43;
  cvt.f32.f16 %f34, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r43;
  cvt.f32.f16 %f35, high;}

	// end inline asm
	fma.rn.f32 	%f38, %f28, %f34, %f165;
	fma.rn.f32 	%f165, %f29, %f35, %f38;
	add.s32 	%r193, %r193, 224;
	add.s64 	%rd71, %rd71, 896;
	add.s64 	%rd70, %rd70, 896;
	add.s64 	%rd69, %rd69, 896;
	add.s64 	%rd68, %rd68, 896;
	add.s32 	%r192, %r192, -1;
	setp.ne.s32 	%p4, %r192, 0;
	@%p4 bra 	$L__BB114_5;

	st.local.f32 	[%rd3], %f167;
	st.local.f32 	[%rd3+4], %f166;
	st.local.f32 	[%rd3+8], %f165;

$L__BB114_7:
	setp.lt.u32 	%p5, %r5, 672;
	@%p5 bra 	$L__BB114_11;

	add.s32 	%r45, %r193, %r16;
	shl.b32 	%r46, %r16, 1;
	add.s32 	%r47, %r193, %r46;
	add.s32 	%r48, %r45, 224;
	mul.wide.s32 	%rd46, %r48, 4;
	shl.b64 	%rd47, %rd5, 1;
	add.s64 	%rd19, %rd46, %rd47;
	mul.wide.s32 	%rd48, %r47, 4;
	add.s64 	%rd20, %rd48, %rd47;
	mul.wide.s32 	%rd49, %r193, 2;
	add.s64 	%rd50, %rd49, %rd4;
	shl.b64 	%rd51, %rd50, 1;
	add.s64 	%rd52, %rd2, %rd51;
	add.s64 	%rd72, %rd52, 1792;
	mul.wide.s32 	%rd53, %r193, 4;
	add.s64 	%rd22, %rd53, %rd47;
	mul.wide.s32 	%rd54, %r16, 4;
	add.s64 	%rd55, %rd53, %rd54;
	add.s64 	%rd23, %rd55, %rd47;

$L__BB114_9:
	ld.global.nc.u32 	%r49, [%rd72+-1792];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r49;
  cvt.f32.f16 %f39, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r49;
  cvt.f32.f16 %f40, high;}

	// end inline asm
	add.s64 	%rd56, %rd73, %rd22;
	ld.global.nc.u32 	%r51, [%rd56];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r51;
  cvt.f32.f16 %f41, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r51;
  cvt.f32.f16 %f42, high;}

	// end inline asm
	fma.rn.f32 	%f71, %f39, %f41, %f167;
	fma.rn.f32 	%f72, %f40, %f42, %f71;
	add.s64 	%rd57, %rd73, %rd23;
	ld.global.nc.u32 	%r53, [%rd57];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r53;
  cvt.f32.f16 %f43, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r53;
  cvt.f32.f16 %f44, high;}

	// end inline asm
	fma.rn.f32 	%f73, %f39, %f43, %f166;
	fma.rn.f32 	%f74, %f40, %f44, %f73;
	add.s64 	%rd58, %rd73, %rd20;
	ld.global.nc.u32 	%r55, [%rd58];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r55;
  cvt.f32.f16 %f45, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r55;
  cvt.f32.f16 %f46, high;}

	// end inline asm
	fma.rn.f32 	%f75, %f39, %f45, %f165;
	fma.rn.f32 	%f76, %f40, %f46, %f75;
	ld.global.nc.u32 	%r57, [%rd72+-896];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r57;
  cvt.f32.f16 %f47, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r57;
  cvt.f32.f16 %f48, high;}

	// end inline asm
	ld.global.nc.u32 	%r59, [%rd56+896];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r59;
  cvt.f32.f16 %f49, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r59;
  cvt.f32.f16 %f50, high;}

	// end inline asm
	fma.rn.f32 	%f77, %f47, %f49, %f72;
	fma.rn.f32 	%f78, %f48, %f50, %f77;
	add.s64 	%rd59, %rd73, %rd19;
	ld.global.nc.u32 	%r61, [%rd59];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r61;
  cvt.f32.f16 %f51, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r61;
  cvt.f32.f16 %f52, high;}

	// end inline asm
	fma.rn.f32 	%f79, %f47, %f51, %f74;
	fma.rn.f32 	%f80, %f48, %f52, %f79;
	ld.global.nc.u32 	%r63, [%rd58+896];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r63;
  cvt.f32.f16 %f53, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r63;
  cvt.f32.f16 %f54, high;}

	// end inline asm
	fma.rn.f32 	%f81, %f47, %f53, %f76;
	fma.rn.f32 	%f82, %f48, %f54, %f81;
	ld.global.nc.u32 	%r65, [%rd72];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r65;
  cvt.f32.f16 %f55, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r65;
  cvt.f32.f16 %f56, high;}

	// end inline asm
	ld.global.nc.u32 	%r67, [%rd56+1792];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r67;
  cvt.f32.f16 %f57, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r67;
  cvt.f32.f16 %f58, high;}

	// end inline asm
	fma.rn.f32 	%f83, %f55, %f57, %f78;
	fma.rn.f32 	%f84, %f56, %f58, %f83;
	ld.global.nc.u32 	%r69, [%rd59+896];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r69;
  cvt.f32.f16 %f59, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r69;
  cvt.f32.f16 %f60, high;}

	// end inline asm
	fma.rn.f32 	%f85, %f55, %f59, %f80;
	fma.rn.f32 	%f86, %f56, %f60, %f85;
	ld.global.nc.u32 	%r71, [%rd58+1792];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r71;
  cvt.f32.f16 %f61, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r71;
  cvt.f32.f16 %f62, high;}

	// end inline asm
	fma.rn.f32 	%f87, %f55, %f61, %f82;
	fma.rn.f32 	%f88, %f56, %f62, %f87;
	ld.global.nc.u32 	%r73, [%rd72+896];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r73;
  cvt.f32.f16 %f63, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r73;
  cvt.f32.f16 %f64, high;}

	// end inline asm
	ld.global.nc.u32 	%r75, [%rd56+2688];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r75;
  cvt.f32.f16 %f65, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r75;
  cvt.f32.f16 %f66, high;}

	// end inline asm
	fma.rn.f32 	%f89, %f63, %f65, %f84;
	fma.rn.f32 	%f167, %f64, %f66, %f89;
	ld.global.nc.u32 	%r77, [%rd59+1792];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r77;
  cvt.f32.f16 %f67, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r77;
  cvt.f32.f16 %f68, high;}

	// end inline asm
	fma.rn.f32 	%f90, %f63, %f67, %f86;
	fma.rn.f32 	%f166, %f64, %f68, %f90;
	ld.global.nc.u32 	%r79, [%rd58+2688];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r79;
  cvt.f32.f16 %f69, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r79;
  cvt.f32.f16 %f70, high;}

	// end inline asm
	fma.rn.f32 	%f91, %f63, %f69, %f88;
	fma.rn.f32 	%f165, %f64, %f70, %f91;
	add.s64 	%rd73, %rd73, 3584;
	add.s64 	%rd72, %rd72, 3584;
	add.s32 	%r193, %r193, 896;
	setp.lt.s32 	%p6, %r193, %r15;
	@%p6 bra 	$L__BB114_9;

	st.local.f32 	[%rd3], %f167;
	st.local.f32 	[%rd3+4], %f166;
	st.local.f32 	[%rd3+8], %f165;

$L__BB114_11:
	shr.s32 	%r81, %r3, 31;
	shr.u32 	%r82, %r81, 27;
	add.s32 	%r83, %r3, %r82;
	shr.s32 	%r84, %r83, 5;
	shl.b32 	%r85, %r84, 2;
	add.s32 	%r14, %r28, %r85;
	mov.u32 	%r87, 2;
	mov.b32 	%r88, %f167;
	mov.u32 	%r89, 31;
	mov.u32 	%r90, 16;
	mov.u32 	%r91, -1;
	shfl.sync.bfly.b32 	%r92|%p7, %r88, %r90, %r89, %r91;
	mov.b32 	%f92, %r92;
	add.f32 	%f93, %f167, %f92;
	mov.b32 	%r93, %f93;
	mov.u32 	%r94, 8;
	shfl.sync.bfly.b32 	%r95|%p8, %r93, %r94, %r89, %r91;
	mov.b32 	%f94, %r95;
	add.f32 	%f95, %f93, %f94;
	mov.b32 	%r96, %f95;
	mov.u32 	%r97, 4;
	shfl.sync.bfly.b32 	%r98|%p9, %r96, %r97, %r89, %r91;
	mov.b32 	%f96, %r98;
	add.f32 	%f97, %f95, %f96;
	mov.b32 	%r99, %f97;
	shfl.sync.bfly.b32 	%r100|%p10, %r99, %r87, %r89, %r91;
	mov.b32 	%f98, %r100;
	add.f32 	%f99, %f97, %f98;
	mov.b32 	%r101, %f99;
	mov.u32 	%r102, 1;
	shfl.sync.bfly.b32 	%r103|%p11, %r101, %r102, %r89, %r91;
	mov.b32 	%f100, %r103;
	add.f32 	%f101, %f99, %f100;
	st.local.f32 	[%rd3], %f101;
	st.shared.f32 	[%r14], %f101;
	bar.sync 	0;
	@%p1 bra 	$L__BB114_13;

	ld.shared.f32 	%f102, [%r4];
	mov.b32 	%r104, %f102;
	shfl.sync.bfly.b32 	%r108|%p13, %r104, %r90, %r89, %r91;
	mov.b32 	%f103, %r108;
	add.f32 	%f104, %f102, %f103;
	mov.b32 	%r109, %f104;
	shfl.sync.bfly.b32 	%r111|%p14, %r109, %r94, %r89, %r91;
	mov.b32 	%f105, %r111;
	add.f32 	%f106, %f104, %f105;
	mov.b32 	%r112, %f106;
	shfl.sync.bfly.b32 	%r114|%p15, %r112, %r97, %r89, %r91;
	mov.b32 	%f107, %r114;
	add.f32 	%f108, %f106, %f107;
	mov.b32 	%r115, %f108;
	shfl.sync.bfly.b32 	%r117|%p16, %r115, %r87, %r89, %r91;
	mov.b32 	%f109, %r117;
	add.f32 	%f110, %f108, %f109;
	mov.b32 	%r118, %f110;
	shfl.sync.bfly.b32 	%r120|%p17, %r118, %r102, %r89, %r91;
	mov.b32 	%f111, %r120;
	add.f32 	%f112, %f110, %f111;
	st.local.f32 	[%rd3], %f112;

$L__BB114_13:
	bar.sync 	0;
	mov.b32 	%r121, %f166;
	shfl.sync.bfly.b32 	%r125|%p19, %r121, %r90, %r89, %r91;
	mov.b32 	%f113, %r125;
	add.f32 	%f114, %f166, %f113;
	mov.b32 	%r126, %f114;
	shfl.sync.bfly.b32 	%r128|%p20, %r126, %r94, %r89, %r91;
	mov.b32 	%f115, %r128;
	add.f32 	%f116, %f114, %f115;
	mov.b32 	%r129, %f116;
	shfl.sync.bfly.b32 	%r131|%p21, %r129, %r97, %r89, %r91;
	mov.b32 	%f117, %r131;
	add.f32 	%f118, %f116, %f117;
	mov.b32 	%r132, %f118;
	shfl.sync.bfly.b32 	%r134|%p22, %r132, %r87, %r89, %r91;
	mov.b32 	%f119, %r134;
	add.f32 	%f120, %f118, %f119;
	mov.b32 	%r135, %f120;
	shfl.sync.bfly.b32 	%r137|%p23, %r135, %r102, %r89, %r91;
	mov.b32 	%f121, %r137;
	add.f32 	%f122, %f120, %f121;
	st.local.f32 	[%rd3+4], %f122;
	st.shared.f32 	[%r14], %f122;
	bar.sync 	0;
	@%p1 bra 	$L__BB114_15;

	ld.shared.f32 	%f123, [%r4];
	mov.b32 	%r138, %f123;
	mov.u32 	%r139, 31;
	mov.u32 	%r140, 16;
	mov.u32 	%r141, -1;
	shfl.sync.bfly.b32 	%r142|%p24, %r138, %r140, %r139, %r141;
	mov.b32 	%f124, %r142;
	add.f32 	%f125, %f123, %f124;
	mov.b32 	%r143, %f125;
	mov.u32 	%r144, 8;
	shfl.sync.bfly.b32 	%r145|%p25, %r143, %r144, %r139, %r141;
	mov.b32 	%f126, %r145;
	add.f32 	%f127, %f125, %f126;
	mov.b32 	%r146, %f127;
	mov.u32 	%r147, 4;
	shfl.sync.bfly.b32 	%r148|%p26, %r146, %r147, %r139, %r141;
	mov.b32 	%f128, %r148;
	add.f32 	%f129, %f127, %f128;
	mov.b32 	%r149, %f129;
	mov.u32 	%r150, 2;
	shfl.sync.bfly.b32 	%r151|%p27, %r149, %r150, %r139, %r141;
	mov.b32 	%f130, %r151;
	add.f32 	%f131, %f129, %f130;
	mov.b32 	%r152, %f131;
	mov.u32 	%r153, 1;
	shfl.sync.bfly.b32 	%r154|%p28, %r152, %r153, %r139, %r141;
	mov.b32 	%f132, %r154;
	add.f32 	%f133, %f131, %f132;
	st.local.f32 	[%rd3+4], %f133;

$L__BB114_15:
	bar.sync 	0;
	mov.b32 	%r155, %f165;
	mov.u32 	%r156, 31;
	mov.u32 	%r157, 16;
	mov.u32 	%r158, -1;
	shfl.sync.bfly.b32 	%r159|%p30, %r155, %r157, %r156, %r158;
	mov.b32 	%f134, %r159;
	add.f32 	%f135, %f165, %f134;
	mov.b32 	%r160, %f135;
	mov.u32 	%r161, 8;
	shfl.sync.bfly.b32 	%r162|%p31, %r160, %r161, %r156, %r158;
	mov.b32 	%f136, %r162;
	add.f32 	%f137, %f135, %f136;
	mov.b32 	%r163, %f137;
	mov.u32 	%r164, 4;
	shfl.sync.bfly.b32 	%r165|%p32, %r163, %r164, %r156, %r158;
	mov.b32 	%f138, %r165;
	add.f32 	%f139, %f137, %f138;
	mov.b32 	%r166, %f139;
	mov.u32 	%r167, 2;
	shfl.sync.bfly.b32 	%r168|%p33, %r166, %r167, %r156, %r158;
	mov.b32 	%f140, %r168;
	add.f32 	%f141, %f139, %f140;
	mov.b32 	%r169, %f141;
	mov.u32 	%r170, 1;
	shfl.sync.bfly.b32 	%r171|%p34, %r169, %r170, %r156, %r158;
	mov.b32 	%f142, %r171;
	add.f32 	%f143, %f141, %f142;
	st.local.f32 	[%rd3+8], %f143;
	st.shared.f32 	[%r14], %f143;
	bar.sync 	0;
	@%p1 bra 	$L__BB114_17;

	ld.shared.f32 	%f144, [%r4];
	mov.b32 	%r172, %f144;
	shfl.sync.bfly.b32 	%r176|%p35, %r172, %r157, %r156, %r158;
	mov.b32 	%f145, %r176;
	add.f32 	%f146, %f144, %f145;
	mov.b32 	%r177, %f146;
	shfl.sync.bfly.b32 	%r179|%p36, %r177, %r161, %r156, %r158;
	mov.b32 	%f147, %r179;
	add.f32 	%f148, %f146, %f147;
	mov.b32 	%r180, %f148;
	shfl.sync.bfly.b32 	%r182|%p37, %r180, %r164, %r156, %r158;
	mov.b32 	%f149, %r182;
	add.f32 	%f150, %f148, %f149;
	mov.b32 	%r183, %f150;
	shfl.sync.bfly.b32 	%r185|%p38, %r183, %r167, %r156, %r158;
	mov.b32 	%f151, %r185;
	add.f32 	%f152, %f150, %f151;
	mov.b32 	%r186, %f152;
	shfl.sync.bfly.b32 	%r188|%p39, %r186, %r170, %r156, %r158;
	mov.b32 	%f153, %r188;
	add.f32 	%f154, %f152, %f153;
	st.local.f32 	[%rd3+8], %f154;

$L__BB114_17:
	bar.sync 	0;
	setp.gt.s32 	%p40, %r3, 2;
	@%p40 bra 	$L__BB114_19;

	mad.lo.s32 	%r189, %r3, %r17, %r2;
	cvt.s64.s32 	%rd60, %r189;
	mul.lo.s32 	%r190, %r1, %r18;
	cvt.s64.s32 	%rd61, %r190;
	add.s64 	%rd62, %rd61, %rd60;
	mul.wide.s32 	%rd63, %r3, 4;
	add.s64 	%rd64, %rd3, %rd63;
	ld.local.f32 	%f155, [%rd64];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f155;}

	// end inline asm
	cvta.to.global.u64 	%rd65, %rd28;
	shl.b64 	%rd66, %rd62, 1;
	add.s64 	%rd67, %rd65, %rd66;
	st.global.u16 	[%rd67], %rs1;

$L__BB114_19:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_4_bs_224
.visible .entry ggml_matvec_f16_ncols_4_bs_224(
	.param .u64 ggml_matvec_f16_ncols_4_bs_224_param_0,
	.param .u64 ggml_matvec_f16_ncols_4_bs_224_param_1,
	.param .u64 ggml_matvec_f16_ncols_4_bs_224_param_2,
	.param .u32 ggml_matvec_f16_ncols_4_bs_224_param_3,
	.param .u32 ggml_matvec_f16_ncols_4_bs_224_param_4,
	.param .u32 ggml_matvec_f16_ncols_4_bs_224_param_5,
	.param .u32 ggml_matvec_f16_ncols_4_bs_224_param_6,
	.param .u32 ggml_matvec_f16_ncols_4_bs_224_param_7,
	.param .u32 ggml_matvec_f16_ncols_4_bs_224_param_8,
	.param .u32 ggml_matvec_f16_ncols_4_bs_224_param_9,
	.param .u32 ggml_matvec_f16_ncols_4_bs_224_param_10,
	.param .u32 ggml_matvec_f16_ncols_4_bs_224_param_11
)
{
	.local .align 16 .b8 	__local_depot115[16];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<53>;
	.reg .b16 	%rs<2>;
	.reg .f32 	%f<172>;
	.reg .b32 	%r<212>;
	.reg .b64 	%rd<64>;


	mov.u64 	%SPL, __local_depot115;
	ld.param.u64 	%rd19, [ggml_matvec_f16_ncols_4_bs_224_param_0];
	ld.param.u64 	%rd20, [ggml_matvec_f16_ncols_4_bs_224_param_1];
	ld.param.u64 	%rd18, [ggml_matvec_f16_ncols_4_bs_224_param_2];
	ld.param.u32 	%r11, [ggml_matvec_f16_ncols_4_bs_224_param_3];
	ld.param.u32 	%r15, [ggml_matvec_f16_ncols_4_bs_224_param_5];
	ld.param.u32 	%r12, [ggml_matvec_f16_ncols_4_bs_224_param_6];
	ld.param.u32 	%r13, [ggml_matvec_f16_ncols_4_bs_224_param_7];
	ld.param.u32 	%r16, [ggml_matvec_f16_ncols_4_bs_224_param_8];
	ld.param.u32 	%r17, [ggml_matvec_f16_ncols_4_bs_224_param_9];
	ld.param.u32 	%r18, [ggml_matvec_f16_ncols_4_bs_224_param_10];
	ld.param.u32 	%r14, [ggml_matvec_f16_ncols_4_bs_224_param_11];
	cvta.to.global.u64 	%rd63, %rd20;
	add.u64 	%rd2, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r19, %r1, %r16;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r20, %r2, %r15;
	mad.lo.s32 	%r21, %r19, %r17, %r20;
	cvt.s64.s32 	%rd3, %r21;
	cvta.to.global.u64 	%rd4, %rd19;
	mul.lo.s32 	%r22, %r1, %r18;
	cvt.s64.s32 	%rd5, %r22;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r23, %r3, 2;
	mov.u32 	%r24, data_mmv;
	add.s32 	%r4, %r24, %r23;
	@%p1 bra 	$L__BB115_2;

	mov.u32 	%r25, 0;
	st.shared.u32 	[%r4], %r25;

$L__BB115_2:
	bar.sync 	0;
	mov.f32 	%f168, 0f00000000;
	st.local.v4.f32 	[%rd2], {%f168, %f168, %f168, %f168};
	setp.ge.s32 	%p2, %r3, %r11;
	mov.f32 	%f169, %f168;
	mov.f32 	%f170, %f168;
	mov.f32 	%f171, %f168;
	@%p2 bra 	$L__BB115_9;

	not.b32 	%r26, %r3;
	add.s32 	%r5, %r26, %r11;
	shr.u32 	%r27, %r5, 5;
	mul.wide.u32 	%rd22, %r27, 613566757;
	shr.u64 	%rd23, %rd22, 32;
	and.b64  	%rd24, %rd23, 1;
	setp.eq.b64 	%p3, %rd24, 1;
	mov.pred 	%p4, 0;
	xor.pred  	%p5, %p3, %p4;
	mov.f32 	%f168, 0f00000000;
	mov.u32 	%r211, %r3;
	@%p5 bra 	$L__BB115_5;

	shl.b64 	%rd25, %rd5, 1;
	add.s64 	%rd26, %rd63, %rd25;
	shl.b64 	%rd27, %rd3, 1;
	add.s64 	%rd28, %rd4, %rd27;
	mul.wide.s32 	%rd29, %r3, 4;
	add.s64 	%rd30, %rd28, %rd29;
	ld.global.nc.u32 	%r28, [%rd30];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r28;
  cvt.f32.f16 %f29, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r28;
  cvt.f32.f16 %f30, high;}

	// end inline asm
	add.s64 	%rd31, %rd26, %rd29;
	ld.global.nc.u32 	%r30, [%rd31];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r30;
  cvt.f32.f16 %f31, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r30;
  cvt.f32.f16 %f32, high;}

	// end inline asm
	fma.rn.f32 	%f39, %f29, %f31, 0f00000000;
	fma.rn.f32 	%f171, %f30, %f32, %f39;
	st.local.f32 	[%rd2], %f171;
	mul.wide.s32 	%rd32, %r12, 4;
	add.s64 	%rd33, %rd31, %rd32;
	ld.global.nc.u32 	%r32, [%rd33];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r32;
  cvt.f32.f16 %f33, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r32;
  cvt.f32.f16 %f34, high;}

	// end inline asm
	fma.rn.f32 	%f40, %f29, %f33, 0f00000000;
	fma.rn.f32 	%f170, %f30, %f34, %f40;
	st.local.f32 	[%rd2+4], %f170;
	add.s32 	%r38, %r3, %r12;
	add.s32 	%r39, %r38, %r12;
	mul.wide.s32 	%rd34, %r39, 4;
	add.s64 	%rd35, %rd26, %rd34;
	ld.global.nc.u32 	%r34, [%rd35];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r34;
  cvt.f32.f16 %f35, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r34;
  cvt.f32.f16 %f36, high;}

	// end inline asm
	fma.rn.f32 	%f41, %f29, %f35, 0f00000000;
	fma.rn.f32 	%f169, %f30, %f36, %f41;
	st.local.f32 	[%rd2+8], %f169;
	add.s32 	%r40, %r39, %r12;
	mul.wide.s32 	%rd36, %r40, 4;
	add.s64 	%rd37, %rd26, %rd36;
	ld.global.nc.u32 	%r36, [%rd37];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r36;
  cvt.f32.f16 %f37, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r36;
  cvt.f32.f16 %f38, high;}

	// end inline asm
	fma.rn.f32 	%f42, %f29, %f37, 0f00000000;
	fma.rn.f32 	%f168, %f30, %f38, %f42;
	st.local.f32 	[%rd2+12], %f168;
	add.s32 	%r211, %r3, 224;

$L__BB115_5:
	setp.lt.u32 	%p6, %r5, 224;
	@%p6 bra 	$L__BB115_9;

	add.s32 	%r41, %r211, %r12;
	add.s32 	%r42, %r41, 224;
	mul.wide.s32 	%rd38, %r42, 4;
	shl.b64 	%rd39, %rd5, 1;
	add.s64 	%rd8, %rd38, %rd39;
	shl.b32 	%r43, %r12, 1;
	add.s32 	%r44, %r211, %r43;
	mad.lo.s32 	%r45, %r12, 3, %r211;
	mul.wide.s32 	%rd40, %r44, 4;
	add.s64 	%rd9, %rd40, %rd39;
	mul.wide.s32 	%rd41, %r45, 4;
	add.s64 	%rd10, %rd41, %rd39;
	mul.wide.s32 	%rd42, %r211, 2;
	add.s64 	%rd43, %rd42, %rd3;
	shl.b64 	%rd44, %rd43, 1;
	add.s64 	%rd45, %rd4, %rd44;
	add.s64 	%rd62, %rd45, 896;
	mul.wide.s32 	%rd46, %r211, 4;
	mul.wide.s32 	%rd47, %r12, 4;
	add.s64 	%rd48, %rd46, %rd47;
	add.s64 	%rd12, %rd48, %rd39;
	add.s64 	%rd13, %rd46, %rd39;

$L__BB115_7:
	ld.global.nc.u32 	%r46, [%rd62+-896];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r46;
  cvt.f32.f16 %f43, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r46;
  cvt.f32.f16 %f44, high;}

	// end inline asm
	add.s64 	%rd49, %rd63, %rd13;
	ld.global.nc.u32 	%r48, [%rd49];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r48;
  cvt.f32.f16 %f45, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r48;
  cvt.f32.f16 %f46, high;}

	// end inline asm
	fma.rn.f32 	%f63, %f43, %f45, %f171;
	fma.rn.f32 	%f64, %f44, %f46, %f63;
	add.s64 	%rd50, %rd63, %rd12;
	ld.global.nc.u32 	%r50, [%rd50];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r50;
  cvt.f32.f16 %f47, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r50;
  cvt.f32.f16 %f48, high;}

	// end inline asm
	fma.rn.f32 	%f65, %f43, %f47, %f170;
	fma.rn.f32 	%f66, %f44, %f48, %f65;
	add.s64 	%rd51, %rd63, %rd9;
	ld.global.nc.u32 	%r52, [%rd51];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r52;
  cvt.f32.f16 %f49, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r52;
  cvt.f32.f16 %f50, high;}

	// end inline asm
	fma.rn.f32 	%f67, %f43, %f49, %f169;
	fma.rn.f32 	%f68, %f44, %f50, %f67;
	add.s64 	%rd52, %rd63, %rd10;
	ld.global.nc.u32 	%r54, [%rd52];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r54;
  cvt.f32.f16 %f51, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r54;
  cvt.f32.f16 %f52, high;}

	// end inline asm
	fma.rn.f32 	%f69, %f43, %f51, %f168;
	fma.rn.f32 	%f70, %f44, %f52, %f69;
	ld.global.nc.u32 	%r56, [%rd62];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r56;
  cvt.f32.f16 %f53, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r56;
  cvt.f32.f16 %f54, high;}

	// end inline asm
	ld.global.nc.u32 	%r58, [%rd49+896];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r58;
  cvt.f32.f16 %f55, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r58;
  cvt.f32.f16 %f56, high;}

	// end inline asm
	fma.rn.f32 	%f71, %f53, %f55, %f64;
	fma.rn.f32 	%f171, %f54, %f56, %f71;
	add.s64 	%rd53, %rd63, %rd8;
	ld.global.nc.u32 	%r60, [%rd53];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r60;
  cvt.f32.f16 %f57, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r60;
  cvt.f32.f16 %f58, high;}

	// end inline asm
	fma.rn.f32 	%f72, %f53, %f57, %f66;
	fma.rn.f32 	%f170, %f54, %f58, %f72;
	ld.global.nc.u32 	%r62, [%rd51+896];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r62;
  cvt.f32.f16 %f59, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r62;
  cvt.f32.f16 %f60, high;}

	// end inline asm
	fma.rn.f32 	%f73, %f53, %f59, %f68;
	fma.rn.f32 	%f169, %f54, %f60, %f73;
	ld.global.nc.u32 	%r64, [%rd52+896];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r64;
  cvt.f32.f16 %f61, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r64;
  cvt.f32.f16 %f62, high;}

	// end inline asm
	fma.rn.f32 	%f74, %f53, %f61, %f70;
	fma.rn.f32 	%f168, %f54, %f62, %f74;
	add.s64 	%rd63, %rd63, 1792;
	add.s64 	%rd62, %rd62, 1792;
	add.s32 	%r211, %r211, 448;
	setp.lt.s32 	%p7, %r211, %r11;
	@%p7 bra 	$L__BB115_7;

	st.local.v4.f32 	[%rd2], {%f171, %f170, %f169, %f168};

$L__BB115_9:
	shr.s32 	%r66, %r3, 31;
	shr.u32 	%r67, %r66, 27;
	add.s32 	%r68, %r3, %r67;
	shr.s32 	%r69, %r68, 5;
	shl.b32 	%r70, %r69, 2;
	add.s32 	%r10, %r24, %r70;
	mov.u32 	%r72, 2;
	mov.b32 	%r73, %f171;
	mov.u32 	%r74, 31;
	mov.u32 	%r75, 16;
	mov.u32 	%r76, -1;
	shfl.sync.bfly.b32 	%r77|%p8, %r73, %r75, %r74, %r76;
	mov.b32 	%f75, %r77;
	add.f32 	%f76, %f171, %f75;
	mov.b32 	%r78, %f76;
	mov.u32 	%r79, 8;
	shfl.sync.bfly.b32 	%r80|%p9, %r78, %r79, %r74, %r76;
	mov.b32 	%f77, %r80;
	add.f32 	%f78, %f76, %f77;
	mov.b32 	%r81, %f78;
	mov.u32 	%r82, 4;
	shfl.sync.bfly.b32 	%r83|%p10, %r81, %r82, %r74, %r76;
	mov.b32 	%f79, %r83;
	add.f32 	%f80, %f78, %f79;
	mov.b32 	%r84, %f80;
	shfl.sync.bfly.b32 	%r85|%p11, %r84, %r72, %r74, %r76;
	mov.b32 	%f81, %r85;
	add.f32 	%f82, %f80, %f81;
	mov.b32 	%r86, %f82;
	mov.u32 	%r87, 1;
	shfl.sync.bfly.b32 	%r88|%p12, %r86, %r87, %r74, %r76;
	mov.b32 	%f83, %r88;
	add.f32 	%f84, %f82, %f83;
	st.local.f32 	[%rd2], %f84;
	st.shared.f32 	[%r10], %f84;
	bar.sync 	0;
	@%p1 bra 	$L__BB115_11;

	ld.shared.f32 	%f85, [%r4];
	mov.b32 	%r89, %f85;
	shfl.sync.bfly.b32 	%r93|%p14, %r89, %r75, %r74, %r76;
	mov.b32 	%f86, %r93;
	add.f32 	%f87, %f85, %f86;
	mov.b32 	%r94, %f87;
	shfl.sync.bfly.b32 	%r96|%p15, %r94, %r79, %r74, %r76;
	mov.b32 	%f88, %r96;
	add.f32 	%f89, %f87, %f88;
	mov.b32 	%r97, %f89;
	shfl.sync.bfly.b32 	%r99|%p16, %r97, %r82, %r74, %r76;
	mov.b32 	%f90, %r99;
	add.f32 	%f91, %f89, %f90;
	mov.b32 	%r100, %f91;
	shfl.sync.bfly.b32 	%r102|%p17, %r100, %r72, %r74, %r76;
	mov.b32 	%f92, %r102;
	add.f32 	%f93, %f91, %f92;
	mov.b32 	%r103, %f93;
	shfl.sync.bfly.b32 	%r105|%p18, %r103, %r87, %r74, %r76;
	mov.b32 	%f94, %r105;
	add.f32 	%f95, %f93, %f94;
	st.local.f32 	[%rd2], %f95;

$L__BB115_11:
	bar.sync 	0;
	mov.b32 	%r106, %f170;
	shfl.sync.bfly.b32 	%r110|%p20, %r106, %r75, %r74, %r76;
	mov.b32 	%f96, %r110;
	add.f32 	%f97, %f170, %f96;
	mov.b32 	%r111, %f97;
	shfl.sync.bfly.b32 	%r113|%p21, %r111, %r79, %r74, %r76;
	mov.b32 	%f98, %r113;
	add.f32 	%f99, %f97, %f98;
	mov.b32 	%r114, %f99;
	shfl.sync.bfly.b32 	%r116|%p22, %r114, %r82, %r74, %r76;
	mov.b32 	%f100, %r116;
	add.f32 	%f101, %f99, %f100;
	mov.b32 	%r117, %f101;
	shfl.sync.bfly.b32 	%r119|%p23, %r117, %r72, %r74, %r76;
	mov.b32 	%f102, %r119;
	add.f32 	%f103, %f101, %f102;
	mov.b32 	%r120, %f103;
	shfl.sync.bfly.b32 	%r122|%p24, %r120, %r87, %r74, %r76;
	mov.b32 	%f104, %r122;
	add.f32 	%f105, %f103, %f104;
	st.local.f32 	[%rd2+4], %f105;
	st.shared.f32 	[%r10], %f105;
	bar.sync 	0;
	@%p1 bra 	$L__BB115_13;

	ld.shared.f32 	%f106, [%r4];
	mov.b32 	%r123, %f106;
	mov.u32 	%r124, 31;
	mov.u32 	%r125, 16;
	mov.u32 	%r126, -1;
	shfl.sync.bfly.b32 	%r127|%p25, %r123, %r125, %r124, %r126;
	mov.b32 	%f107, %r127;
	add.f32 	%f108, %f106, %f107;
	mov.b32 	%r128, %f108;
	mov.u32 	%r129, 8;
	shfl.sync.bfly.b32 	%r130|%p26, %r128, %r129, %r124, %r126;
	mov.b32 	%f109, %r130;
	add.f32 	%f110, %f108, %f109;
	mov.b32 	%r131, %f110;
	mov.u32 	%r132, 4;
	shfl.sync.bfly.b32 	%r133|%p27, %r131, %r132, %r124, %r126;
	mov.b32 	%f111, %r133;
	add.f32 	%f112, %f110, %f111;
	mov.b32 	%r134, %f112;
	mov.u32 	%r135, 2;
	shfl.sync.bfly.b32 	%r136|%p28, %r134, %r135, %r124, %r126;
	mov.b32 	%f113, %r136;
	add.f32 	%f114, %f112, %f113;
	mov.b32 	%r137, %f114;
	mov.u32 	%r138, 1;
	shfl.sync.bfly.b32 	%r139|%p29, %r137, %r138, %r124, %r126;
	mov.b32 	%f115, %r139;
	add.f32 	%f116, %f114, %f115;
	st.local.f32 	[%rd2+4], %f116;

$L__BB115_13:
	bar.sync 	0;
	mov.b32 	%r140, %f169;
	mov.u32 	%r141, 31;
	mov.u32 	%r142, 16;
	mov.u32 	%r143, -1;
	shfl.sync.bfly.b32 	%r144|%p31, %r140, %r142, %r141, %r143;
	mov.b32 	%f117, %r144;
	add.f32 	%f118, %f169, %f117;
	mov.b32 	%r145, %f118;
	mov.u32 	%r146, 8;
	shfl.sync.bfly.b32 	%r147|%p32, %r145, %r146, %r141, %r143;
	mov.b32 	%f119, %r147;
	add.f32 	%f120, %f118, %f119;
	mov.b32 	%r148, %f120;
	mov.u32 	%r149, 4;
	shfl.sync.bfly.b32 	%r150|%p33, %r148, %r149, %r141, %r143;
	mov.b32 	%f121, %r150;
	add.f32 	%f122, %f120, %f121;
	mov.b32 	%r151, %f122;
	mov.u32 	%r152, 2;
	shfl.sync.bfly.b32 	%r153|%p34, %r151, %r152, %r141, %r143;
	mov.b32 	%f123, %r153;
	add.f32 	%f124, %f122, %f123;
	mov.b32 	%r154, %f124;
	mov.u32 	%r155, 1;
	shfl.sync.bfly.b32 	%r156|%p35, %r154, %r155, %r141, %r143;
	mov.b32 	%f125, %r156;
	add.f32 	%f126, %f124, %f125;
	st.local.f32 	[%rd2+8], %f126;
	st.shared.f32 	[%r10], %f126;
	bar.sync 	0;
	@%p1 bra 	$L__BB115_15;

	ld.shared.f32 	%f127, [%r4];
	mov.b32 	%r157, %f127;
	shfl.sync.bfly.b32 	%r161|%p36, %r157, %r142, %r141, %r143;
	mov.b32 	%f128, %r161;
	add.f32 	%f129, %f127, %f128;
	mov.b32 	%r162, %f129;
	shfl.sync.bfly.b32 	%r164|%p37, %r162, %r146, %r141, %r143;
	mov.b32 	%f130, %r164;
	add.f32 	%f131, %f129, %f130;
	mov.b32 	%r165, %f131;
	shfl.sync.bfly.b32 	%r167|%p38, %r165, %r149, %r141, %r143;
	mov.b32 	%f132, %r167;
	add.f32 	%f133, %f131, %f132;
	mov.b32 	%r168, %f133;
	shfl.sync.bfly.b32 	%r170|%p39, %r168, %r152, %r141, %r143;
	mov.b32 	%f134, %r170;
	add.f32 	%f135, %f133, %f134;
	mov.b32 	%r171, %f135;
	shfl.sync.bfly.b32 	%r173|%p40, %r171, %r155, %r141, %r143;
	mov.b32 	%f136, %r173;
	add.f32 	%f137, %f135, %f136;
	st.local.f32 	[%rd2+8], %f137;

$L__BB115_15:
	bar.sync 	0;
	mov.b32 	%r174, %f168;
	shfl.sync.bfly.b32 	%r178|%p42, %r174, %r142, %r141, %r143;
	mov.b32 	%f138, %r178;
	add.f32 	%f139, %f168, %f138;
	mov.b32 	%r179, %f139;
	shfl.sync.bfly.b32 	%r181|%p43, %r179, %r146, %r141, %r143;
	mov.b32 	%f140, %r181;
	add.f32 	%f141, %f139, %f140;
	mov.b32 	%r182, %f141;
	shfl.sync.bfly.b32 	%r184|%p44, %r182, %r149, %r141, %r143;
	mov.b32 	%f142, %r184;
	add.f32 	%f143, %f141, %f142;
	mov.b32 	%r185, %f143;
	shfl.sync.bfly.b32 	%r187|%p45, %r185, %r152, %r141, %r143;
	mov.b32 	%f144, %r187;
	add.f32 	%f145, %f143, %f144;
	mov.b32 	%r188, %f145;
	shfl.sync.bfly.b32 	%r190|%p46, %r188, %r155, %r141, %r143;
	mov.b32 	%f146, %r190;
	add.f32 	%f147, %f145, %f146;
	st.local.f32 	[%rd2+12], %f147;
	st.shared.f32 	[%r10], %f147;
	bar.sync 	0;
	@%p1 bra 	$L__BB115_17;

	ld.shared.f32 	%f148, [%r4];
	mov.b32 	%r191, %f148;
	mov.u32 	%r192, 31;
	mov.u32 	%r193, 16;
	mov.u32 	%r194, -1;
	shfl.sync.bfly.b32 	%r195|%p47, %r191, %r193, %r192, %r194;
	mov.b32 	%f149, %r195;
	add.f32 	%f150, %f148, %f149;
	mov.b32 	%r196, %f150;
	mov.u32 	%r197, 8;
	shfl.sync.bfly.b32 	%r198|%p48, %r196, %r197, %r192, %r194;
	mov.b32 	%f151, %r198;
	add.f32 	%f152, %f150, %f151;
	mov.b32 	%r199, %f152;
	mov.u32 	%r200, 4;
	shfl.sync.bfly.b32 	%r201|%p49, %r199, %r200, %r192, %r194;
	mov.b32 	%f153, %r201;
	add.f32 	%f154, %f152, %f153;
	mov.b32 	%r202, %f154;
	mov.u32 	%r203, 2;
	shfl.sync.bfly.b32 	%r204|%p50, %r202, %r203, %r192, %r194;
	mov.b32 	%f155, %r204;
	add.f32 	%f156, %f154, %f155;
	mov.b32 	%r205, %f156;
	mov.u32 	%r206, 1;
	shfl.sync.bfly.b32 	%r207|%p51, %r205, %r206, %r192, %r194;
	mov.b32 	%f157, %r207;
	add.f32 	%f158, %f156, %f157;
	st.local.f32 	[%rd2+12], %f158;

$L__BB115_17:
	bar.sync 	0;
	setp.gt.s32 	%p52, %r3, 3;
	@%p52 bra 	$L__BB115_19;

	mad.lo.s32 	%r208, %r3, %r13, %r2;
	cvt.s64.s32 	%rd54, %r208;
	mul.lo.s32 	%r209, %r1, %r14;
	cvt.s64.s32 	%rd55, %r209;
	add.s64 	%rd56, %rd55, %rd54;
	mul.wide.s32 	%rd57, %r3, 4;
	add.s64 	%rd58, %rd2, %rd57;
	ld.local.f32 	%f159, [%rd58];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f159;}

	// end inline asm
	cvta.to.global.u64 	%rd59, %rd18;
	shl.b64 	%rd60, %rd56, 1;
	add.s64 	%rd61, %rd59, %rd60;
	st.global.u16 	[%rd61], %rs1;

$L__BB115_19:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_5_bs_224
.visible .entry ggml_matvec_f16_ncols_5_bs_224(
	.param .u64 ggml_matvec_f16_ncols_5_bs_224_param_0,
	.param .u64 ggml_matvec_f16_ncols_5_bs_224_param_1,
	.param .u64 ggml_matvec_f16_ncols_5_bs_224_param_2,
	.param .u32 ggml_matvec_f16_ncols_5_bs_224_param_3,
	.param .u32 ggml_matvec_f16_ncols_5_bs_224_param_4,
	.param .u32 ggml_matvec_f16_ncols_5_bs_224_param_5,
	.param .u32 ggml_matvec_f16_ncols_5_bs_224_param_6,
	.param .u32 ggml_matvec_f16_ncols_5_bs_224_param_7,
	.param .u32 ggml_matvec_f16_ncols_5_bs_224_param_8,
	.param .u32 ggml_matvec_f16_ncols_5_bs_224_param_9,
	.param .u32 ggml_matvec_f16_ncols_5_bs_224_param_10,
	.param .u32 ggml_matvec_f16_ncols_5_bs_224_param_11
)
{
	.local .align 4 .b8 	__local_depot116[20];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<64>;
	.reg .b16 	%rs<2>;
	.reg .f32 	%f<213>;
	.reg .b32 	%r<256>;
	.reg .b64 	%rd<67>;


	mov.u64 	%SPL, __local_depot116;
	ld.param.u64 	%rd19, [ggml_matvec_f16_ncols_5_bs_224_param_0];
	ld.param.u64 	%rd20, [ggml_matvec_f16_ncols_5_bs_224_param_1];
	ld.param.u64 	%rd18, [ggml_matvec_f16_ncols_5_bs_224_param_2];
	ld.param.u32 	%r11, [ggml_matvec_f16_ncols_5_bs_224_param_3];
	ld.param.u32 	%r15, [ggml_matvec_f16_ncols_5_bs_224_param_5];
	ld.param.u32 	%r12, [ggml_matvec_f16_ncols_5_bs_224_param_6];
	ld.param.u32 	%r13, [ggml_matvec_f16_ncols_5_bs_224_param_7];
	ld.param.u32 	%r16, [ggml_matvec_f16_ncols_5_bs_224_param_8];
	ld.param.u32 	%r17, [ggml_matvec_f16_ncols_5_bs_224_param_9];
	ld.param.u32 	%r18, [ggml_matvec_f16_ncols_5_bs_224_param_10];
	ld.param.u32 	%r14, [ggml_matvec_f16_ncols_5_bs_224_param_11];
	cvta.to.global.u64 	%rd66, %rd20;
	add.u64 	%rd2, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r19, %r1, %r16;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r20, %r2, %r15;
	mad.lo.s32 	%r21, %r19, %r17, %r20;
	cvt.s64.s32 	%rd3, %r21;
	cvta.to.global.u64 	%rd4, %rd19;
	mul.lo.s32 	%r22, %r1, %r18;
	cvt.s64.s32 	%rd5, %r22;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r23, %r3, 2;
	mov.u32 	%r24, data_mmv;
	add.s32 	%r4, %r24, %r23;
	@%p1 bra 	$L__BB116_2;

	mov.u32 	%r25, 0;
	st.shared.u32 	[%r4], %r25;

$L__BB116_2:
	bar.sync 	0;
	mov.f32 	%f208, 0f00000000;
	mov.u32 	%r26, 0;
	st.local.u32 	[%rd2], %r26;
	st.local.u32 	[%rd2+4], %r26;
	st.local.u32 	[%rd2+8], %r26;
	st.local.u32 	[%rd2+12], %r26;
	st.local.u32 	[%rd2+16], %r26;
	setp.ge.s32 	%p2, %r3, %r11;
	mov.f32 	%f209, %f208;
	mov.f32 	%f210, %f208;
	mov.f32 	%f211, %f208;
	mov.f32 	%f212, %f208;
	@%p2 bra 	$L__BB116_9;

	not.b32 	%r27, %r3;
	add.s32 	%r5, %r27, %r11;
	shr.u32 	%r28, %r5, 5;
	mul.wide.u32 	%rd22, %r28, 613566757;
	shr.u64 	%rd23, %rd22, 32;
	and.b64  	%rd24, %rd23, 1;
	setp.eq.b64 	%p3, %rd24, 1;
	mov.pred 	%p4, 0;
	xor.pred  	%p5, %p3, %p4;
	mov.f32 	%f208, 0f00000000;
	mov.u32 	%r255, %r3;
	@%p5 bra 	$L__BB116_5;

	shl.b64 	%rd25, %rd5, 1;
	add.s64 	%rd26, %rd66, %rd25;
	shl.b64 	%rd27, %rd3, 1;
	add.s64 	%rd28, %rd4, %rd27;
	mul.wide.s32 	%rd29, %r3, 4;
	add.s64 	%rd30, %rd28, %rd29;
	ld.global.nc.u32 	%r29, [%rd30];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r29;
  cvt.f32.f16 %f36, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r29;
  cvt.f32.f16 %f37, high;}

	// end inline asm
	add.s64 	%rd31, %rd26, %rd29;
	ld.global.nc.u32 	%r31, [%rd31];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r31;
  cvt.f32.f16 %f38, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r31;
  cvt.f32.f16 %f39, high;}

	// end inline asm
	fma.rn.f32 	%f48, %f36, %f38, 0f00000000;
	fma.rn.f32 	%f212, %f37, %f39, %f48;
	st.local.f32 	[%rd2], %f212;
	mul.wide.s32 	%rd32, %r12, 4;
	add.s64 	%rd33, %rd31, %rd32;
	ld.global.nc.u32 	%r33, [%rd33];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r33;
  cvt.f32.f16 %f40, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r33;
  cvt.f32.f16 %f41, high;}

	// end inline asm
	fma.rn.f32 	%f49, %f36, %f40, 0f00000000;
	fma.rn.f32 	%f211, %f37, %f41, %f49;
	st.local.f32 	[%rd2+4], %f211;
	add.s32 	%r41, %r3, %r12;
	add.s32 	%r42, %r41, %r12;
	shl.b32 	%r43, %r12, 1;
	mul.wide.s32 	%rd34, %r43, 4;
	add.s64 	%rd35, %rd31, %rd34;
	ld.global.nc.u32 	%r35, [%rd35];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r35;
  cvt.f32.f16 %f42, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r35;
  cvt.f32.f16 %f43, high;}

	// end inline asm
	fma.rn.f32 	%f50, %f36, %f42, 0f00000000;
	fma.rn.f32 	%f210, %f37, %f43, %f50;
	st.local.f32 	[%rd2+8], %f210;
	add.s32 	%r44, %r42, %r12;
	mul.wide.s32 	%rd36, %r44, 4;
	add.s64 	%rd37, %rd26, %rd36;
	ld.global.nc.u32 	%r37, [%rd37];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r37;
  cvt.f32.f16 %f44, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r37;
  cvt.f32.f16 %f45, high;}

	// end inline asm
	fma.rn.f32 	%f51, %f36, %f44, 0f00000000;
	fma.rn.f32 	%f209, %f37, %f45, %f51;
	st.local.f32 	[%rd2+12], %f209;
	add.s64 	%rd38, %rd35, %rd34;
	ld.global.nc.u32 	%r39, [%rd38];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r39;
  cvt.f32.f16 %f46, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r39;
  cvt.f32.f16 %f47, high;}

	// end inline asm
	fma.rn.f32 	%f52, %f36, %f46, 0f00000000;
	fma.rn.f32 	%f208, %f37, %f47, %f52;
	st.local.f32 	[%rd2+16], %f208;
	add.s32 	%r255, %r3, 224;

$L__BB116_5:
	setp.lt.u32 	%p6, %r5, 224;
	@%p6 bra 	$L__BB116_9;

	add.s32 	%r45, %r255, %r12;
	add.s32 	%r46, %r45, 224;
	mul.wide.s32 	%rd39, %r46, 4;
	shl.b64 	%rd40, %rd5, 1;
	add.s64 	%rd7, %rd39, %rd40;
	shl.b32 	%r47, %r12, 1;
	add.s32 	%r48, %r255, %r47;
	mad.lo.s32 	%r49, %r12, 3, %r255;
	shl.b32 	%r50, %r12, 2;
	add.s32 	%r51, %r255, %r50;
	mul.wide.s32 	%rd41, %r48, 4;
	add.s64 	%rd8, %rd41, %rd40;
	mul.wide.s32 	%rd42, %r49, 4;
	add.s64 	%rd9, %rd42, %rd40;
	mul.wide.s32 	%rd43, %r51, 4;
	add.s64 	%rd10, %rd43, %rd40;
	mul.wide.s32 	%rd44, %r255, 2;
	add.s64 	%rd45, %rd44, %rd3;
	shl.b64 	%rd46, %rd45, 1;
	add.s64 	%rd47, %rd4, %rd46;
	add.s64 	%rd65, %rd47, 896;
	mul.wide.s32 	%rd48, %r255, 4;
	mul.wide.s32 	%rd49, %r12, 4;
	add.s64 	%rd50, %rd48, %rd49;
	add.s64 	%rd12, %rd50, %rd40;
	add.s64 	%rd13, %rd48, %rd40;

$L__BB116_7:
	ld.global.nc.u32 	%r52, [%rd65+-896];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r52;
  cvt.f32.f16 %f53, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r52;
  cvt.f32.f16 %f54, high;}

	// end inline asm
	add.s64 	%rd51, %rd66, %rd13;
	ld.global.nc.u32 	%r54, [%rd51];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r54;
  cvt.f32.f16 %f55, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r54;
  cvt.f32.f16 %f56, high;}

	// end inline asm
	fma.rn.f32 	%f77, %f53, %f55, %f212;
	fma.rn.f32 	%f78, %f54, %f56, %f77;
	add.s64 	%rd52, %rd66, %rd12;
	ld.global.nc.u32 	%r56, [%rd52];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r56;
  cvt.f32.f16 %f57, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r56;
  cvt.f32.f16 %f58, high;}

	// end inline asm
	fma.rn.f32 	%f79, %f53, %f57, %f211;
	fma.rn.f32 	%f80, %f54, %f58, %f79;
	add.s64 	%rd53, %rd66, %rd8;
	ld.global.nc.u32 	%r58, [%rd53];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r58;
  cvt.f32.f16 %f59, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r58;
  cvt.f32.f16 %f60, high;}

	// end inline asm
	fma.rn.f32 	%f81, %f53, %f59, %f210;
	fma.rn.f32 	%f82, %f54, %f60, %f81;
	add.s64 	%rd54, %rd66, %rd9;
	ld.global.nc.u32 	%r60, [%rd54];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r60;
  cvt.f32.f16 %f61, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r60;
  cvt.f32.f16 %f62, high;}

	// end inline asm
	fma.rn.f32 	%f83, %f53, %f61, %f209;
	fma.rn.f32 	%f84, %f54, %f62, %f83;
	add.s64 	%rd55, %rd66, %rd10;
	ld.global.nc.u32 	%r62, [%rd55];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r62;
  cvt.f32.f16 %f63, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r62;
  cvt.f32.f16 %f64, high;}

	// end inline asm
	fma.rn.f32 	%f85, %f53, %f63, %f208;
	fma.rn.f32 	%f86, %f54, %f64, %f85;
	ld.global.nc.u32 	%r64, [%rd65];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r64;
  cvt.f32.f16 %f65, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r64;
  cvt.f32.f16 %f66, high;}

	// end inline asm
	ld.global.nc.u32 	%r66, [%rd51+896];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r66;
  cvt.f32.f16 %f67, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r66;
  cvt.f32.f16 %f68, high;}

	// end inline asm
	fma.rn.f32 	%f87, %f65, %f67, %f78;
	fma.rn.f32 	%f212, %f66, %f68, %f87;
	add.s64 	%rd56, %rd66, %rd7;
	ld.global.nc.u32 	%r68, [%rd56];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r68;
  cvt.f32.f16 %f69, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r68;
  cvt.f32.f16 %f70, high;}

	// end inline asm
	fma.rn.f32 	%f88, %f65, %f69, %f80;
	fma.rn.f32 	%f211, %f66, %f70, %f88;
	ld.global.nc.u32 	%r70, [%rd53+896];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r70;
  cvt.f32.f16 %f71, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r70;
  cvt.f32.f16 %f72, high;}

	// end inline asm
	fma.rn.f32 	%f89, %f65, %f71, %f82;
	fma.rn.f32 	%f210, %f66, %f72, %f89;
	ld.global.nc.u32 	%r72, [%rd54+896];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r72;
  cvt.f32.f16 %f73, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r72;
  cvt.f32.f16 %f74, high;}

	// end inline asm
	fma.rn.f32 	%f90, %f65, %f73, %f84;
	fma.rn.f32 	%f209, %f66, %f74, %f90;
	ld.global.nc.u32 	%r74, [%rd55+896];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r74;
  cvt.f32.f16 %f75, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r74;
  cvt.f32.f16 %f76, high;}

	// end inline asm
	fma.rn.f32 	%f91, %f65, %f75, %f86;
	fma.rn.f32 	%f208, %f66, %f76, %f91;
	add.s64 	%rd66, %rd66, 1792;
	add.s64 	%rd65, %rd65, 1792;
	add.s32 	%r255, %r255, 448;
	setp.lt.s32 	%p7, %r255, %r11;
	@%p7 bra 	$L__BB116_7;

	st.local.f32 	[%rd2], %f212;
	st.local.f32 	[%rd2+4], %f211;
	st.local.f32 	[%rd2+8], %f210;
	st.local.f32 	[%rd2+12], %f209;
	st.local.f32 	[%rd2+16], %f208;

$L__BB116_9:
	shr.s32 	%r76, %r3, 31;
	shr.u32 	%r77, %r76, 27;
	add.s32 	%r78, %r3, %r77;
	shr.s32 	%r79, %r78, 5;
	shl.b32 	%r80, %r79, 2;
	add.s32 	%r10, %r24, %r80;
	mov.u32 	%r82, 2;
	mov.b32 	%r83, %f212;
	mov.u32 	%r84, 31;
	mov.u32 	%r85, 16;
	mov.u32 	%r86, -1;
	shfl.sync.bfly.b32 	%r87|%p8, %r83, %r85, %r84, %r86;
	mov.b32 	%f92, %r87;
	add.f32 	%f93, %f212, %f92;
	mov.b32 	%r88, %f93;
	mov.u32 	%r89, 8;
	shfl.sync.bfly.b32 	%r90|%p9, %r88, %r89, %r84, %r86;
	mov.b32 	%f94, %r90;
	add.f32 	%f95, %f93, %f94;
	mov.b32 	%r91, %f95;
	mov.u32 	%r92, 4;
	shfl.sync.bfly.b32 	%r93|%p10, %r91, %r92, %r84, %r86;
	mov.b32 	%f96, %r93;
	add.f32 	%f97, %f95, %f96;
	mov.b32 	%r94, %f97;
	shfl.sync.bfly.b32 	%r95|%p11, %r94, %r82, %r84, %r86;
	mov.b32 	%f98, %r95;
	add.f32 	%f99, %f97, %f98;
	mov.b32 	%r96, %f99;
	mov.u32 	%r97, 1;
	shfl.sync.bfly.b32 	%r98|%p12, %r96, %r97, %r84, %r86;
	mov.b32 	%f100, %r98;
	add.f32 	%f101, %f99, %f100;
	st.local.f32 	[%rd2], %f101;
	st.shared.f32 	[%r10], %f101;
	bar.sync 	0;
	@%p1 bra 	$L__BB116_11;

	ld.shared.f32 	%f102, [%r4];
	mov.b32 	%r99, %f102;
	shfl.sync.bfly.b32 	%r103|%p14, %r99, %r85, %r84, %r86;
	mov.b32 	%f103, %r103;
	add.f32 	%f104, %f102, %f103;
	mov.b32 	%r104, %f104;
	shfl.sync.bfly.b32 	%r106|%p15, %r104, %r89, %r84, %r86;
	mov.b32 	%f105, %r106;
	add.f32 	%f106, %f104, %f105;
	mov.b32 	%r107, %f106;
	shfl.sync.bfly.b32 	%r109|%p16, %r107, %r92, %r84, %r86;
	mov.b32 	%f107, %r109;
	add.f32 	%f108, %f106, %f107;
	mov.b32 	%r110, %f108;
	shfl.sync.bfly.b32 	%r112|%p17, %r110, %r82, %r84, %r86;
	mov.b32 	%f109, %r112;
	add.f32 	%f110, %f108, %f109;
	mov.b32 	%r113, %f110;
	shfl.sync.bfly.b32 	%r115|%p18, %r113, %r97, %r84, %r86;
	mov.b32 	%f111, %r115;
	add.f32 	%f112, %f110, %f111;
	st.local.f32 	[%rd2], %f112;

$L__BB116_11:
	bar.sync 	0;
	mov.b32 	%r116, %f211;
	shfl.sync.bfly.b32 	%r120|%p20, %r116, %r85, %r84, %r86;
	mov.b32 	%f113, %r120;
	add.f32 	%f114, %f211, %f113;
	mov.b32 	%r121, %f114;
	shfl.sync.bfly.b32 	%r123|%p21, %r121, %r89, %r84, %r86;
	mov.b32 	%f115, %r123;
	add.f32 	%f116, %f114, %f115;
	mov.b32 	%r124, %f116;
	shfl.sync.bfly.b32 	%r126|%p22, %r124, %r92, %r84, %r86;
	mov.b32 	%f117, %r126;
	add.f32 	%f118, %f116, %f117;
	mov.b32 	%r127, %f118;
	shfl.sync.bfly.b32 	%r129|%p23, %r127, %r82, %r84, %r86;
	mov.b32 	%f119, %r129;
	add.f32 	%f120, %f118, %f119;
	mov.b32 	%r130, %f120;
	shfl.sync.bfly.b32 	%r132|%p24, %r130, %r97, %r84, %r86;
	mov.b32 	%f121, %r132;
	add.f32 	%f122, %f120, %f121;
	st.local.f32 	[%rd2+4], %f122;
	st.shared.f32 	[%r10], %f122;
	bar.sync 	0;
	@%p1 bra 	$L__BB116_13;

	ld.shared.f32 	%f123, [%r4];
	mov.b32 	%r133, %f123;
	mov.u32 	%r134, 31;
	mov.u32 	%r135, 16;
	mov.u32 	%r136, -1;
	shfl.sync.bfly.b32 	%r137|%p25, %r133, %r135, %r134, %r136;
	mov.b32 	%f124, %r137;
	add.f32 	%f125, %f123, %f124;
	mov.b32 	%r138, %f125;
	mov.u32 	%r139, 8;
	shfl.sync.bfly.b32 	%r140|%p26, %r138, %r139, %r134, %r136;
	mov.b32 	%f126, %r140;
	add.f32 	%f127, %f125, %f126;
	mov.b32 	%r141, %f127;
	mov.u32 	%r142, 4;
	shfl.sync.bfly.b32 	%r143|%p27, %r141, %r142, %r134, %r136;
	mov.b32 	%f128, %r143;
	add.f32 	%f129, %f127, %f128;
	mov.b32 	%r144, %f129;
	mov.u32 	%r145, 2;
	shfl.sync.bfly.b32 	%r146|%p28, %r144, %r145, %r134, %r136;
	mov.b32 	%f130, %r146;
	add.f32 	%f131, %f129, %f130;
	mov.b32 	%r147, %f131;
	mov.u32 	%r148, 1;
	shfl.sync.bfly.b32 	%r149|%p29, %r147, %r148, %r134, %r136;
	mov.b32 	%f132, %r149;
	add.f32 	%f133, %f131, %f132;
	st.local.f32 	[%rd2+4], %f133;

$L__BB116_13:
	bar.sync 	0;
	mov.b32 	%r150, %f210;
	mov.u32 	%r151, 31;
	mov.u32 	%r152, 16;
	mov.u32 	%r153, -1;
	shfl.sync.bfly.b32 	%r154|%p31, %r150, %r152, %r151, %r153;
	mov.b32 	%f134, %r154;
	add.f32 	%f135, %f210, %f134;
	mov.b32 	%r155, %f135;
	mov.u32 	%r156, 8;
	shfl.sync.bfly.b32 	%r157|%p32, %r155, %r156, %r151, %r153;
	mov.b32 	%f136, %r157;
	add.f32 	%f137, %f135, %f136;
	mov.b32 	%r158, %f137;
	mov.u32 	%r159, 4;
	shfl.sync.bfly.b32 	%r160|%p33, %r158, %r159, %r151, %r153;
	mov.b32 	%f138, %r160;
	add.f32 	%f139, %f137, %f138;
	mov.b32 	%r161, %f139;
	mov.u32 	%r162, 2;
	shfl.sync.bfly.b32 	%r163|%p34, %r161, %r162, %r151, %r153;
	mov.b32 	%f140, %r163;
	add.f32 	%f141, %f139, %f140;
	mov.b32 	%r164, %f141;
	mov.u32 	%r165, 1;
	shfl.sync.bfly.b32 	%r166|%p35, %r164, %r165, %r151, %r153;
	mov.b32 	%f142, %r166;
	add.f32 	%f143, %f141, %f142;
	st.local.f32 	[%rd2+8], %f143;
	st.shared.f32 	[%r10], %f143;
	bar.sync 	0;
	@%p1 bra 	$L__BB116_15;

	ld.shared.f32 	%f144, [%r4];
	mov.b32 	%r167, %f144;
	shfl.sync.bfly.b32 	%r171|%p36, %r167, %r152, %r151, %r153;
	mov.b32 	%f145, %r171;
	add.f32 	%f146, %f144, %f145;
	mov.b32 	%r172, %f146;
	shfl.sync.bfly.b32 	%r174|%p37, %r172, %r156, %r151, %r153;
	mov.b32 	%f147, %r174;
	add.f32 	%f148, %f146, %f147;
	mov.b32 	%r175, %f148;
	shfl.sync.bfly.b32 	%r177|%p38, %r175, %r159, %r151, %r153;
	mov.b32 	%f149, %r177;
	add.f32 	%f150, %f148, %f149;
	mov.b32 	%r178, %f150;
	shfl.sync.bfly.b32 	%r180|%p39, %r178, %r162, %r151, %r153;
	mov.b32 	%f151, %r180;
	add.f32 	%f152, %f150, %f151;
	mov.b32 	%r181, %f152;
	shfl.sync.bfly.b32 	%r183|%p40, %r181, %r165, %r151, %r153;
	mov.b32 	%f153, %r183;
	add.f32 	%f154, %f152, %f153;
	st.local.f32 	[%rd2+8], %f154;

$L__BB116_15:
	bar.sync 	0;
	mov.b32 	%r184, %f209;
	shfl.sync.bfly.b32 	%r188|%p42, %r184, %r152, %r151, %r153;
	mov.b32 	%f155, %r188;
	add.f32 	%f156, %f209, %f155;
	mov.b32 	%r189, %f156;
	shfl.sync.bfly.b32 	%r191|%p43, %r189, %r156, %r151, %r153;
	mov.b32 	%f157, %r191;
	add.f32 	%f158, %f156, %f157;
	mov.b32 	%r192, %f158;
	shfl.sync.bfly.b32 	%r194|%p44, %r192, %r159, %r151, %r153;
	mov.b32 	%f159, %r194;
	add.f32 	%f160, %f158, %f159;
	mov.b32 	%r195, %f160;
	shfl.sync.bfly.b32 	%r197|%p45, %r195, %r162, %r151, %r153;
	mov.b32 	%f161, %r197;
	add.f32 	%f162, %f160, %f161;
	mov.b32 	%r198, %f162;
	shfl.sync.bfly.b32 	%r200|%p46, %r198, %r165, %r151, %r153;
	mov.b32 	%f163, %r200;
	add.f32 	%f164, %f162, %f163;
	st.local.f32 	[%rd2+12], %f164;
	st.shared.f32 	[%r10], %f164;
	bar.sync 	0;
	@%p1 bra 	$L__BB116_17;

	ld.shared.f32 	%f165, [%r4];
	mov.b32 	%r201, %f165;
	mov.u32 	%r202, 31;
	mov.u32 	%r203, 16;
	mov.u32 	%r204, -1;
	shfl.sync.bfly.b32 	%r205|%p47, %r201, %r203, %r202, %r204;
	mov.b32 	%f166, %r205;
	add.f32 	%f167, %f165, %f166;
	mov.b32 	%r206, %f167;
	mov.u32 	%r207, 8;
	shfl.sync.bfly.b32 	%r208|%p48, %r206, %r207, %r202, %r204;
	mov.b32 	%f168, %r208;
	add.f32 	%f169, %f167, %f168;
	mov.b32 	%r209, %f169;
	mov.u32 	%r210, 4;
	shfl.sync.bfly.b32 	%r211|%p49, %r209, %r210, %r202, %r204;
	mov.b32 	%f170, %r211;
	add.f32 	%f171, %f169, %f170;
	mov.b32 	%r212, %f171;
	mov.u32 	%r213, 2;
	shfl.sync.bfly.b32 	%r214|%p50, %r212, %r213, %r202, %r204;
	mov.b32 	%f172, %r214;
	add.f32 	%f173, %f171, %f172;
	mov.b32 	%r215, %f173;
	mov.u32 	%r216, 1;
	shfl.sync.bfly.b32 	%r217|%p51, %r215, %r216, %r202, %r204;
	mov.b32 	%f174, %r217;
	add.f32 	%f175, %f173, %f174;
	st.local.f32 	[%rd2+12], %f175;

$L__BB116_17:
	bar.sync 	0;
	mov.b32 	%r218, %f208;
	mov.u32 	%r219, 31;
	mov.u32 	%r220, 16;
	mov.u32 	%r221, -1;
	shfl.sync.bfly.b32 	%r222|%p53, %r218, %r220, %r219, %r221;
	mov.b32 	%f176, %r222;
	add.f32 	%f177, %f208, %f176;
	mov.b32 	%r223, %f177;
	mov.u32 	%r224, 8;
	shfl.sync.bfly.b32 	%r225|%p54, %r223, %r224, %r219, %r221;
	mov.b32 	%f178, %r225;
	add.f32 	%f179, %f177, %f178;
	mov.b32 	%r226, %f179;
	mov.u32 	%r227, 4;
	shfl.sync.bfly.b32 	%r228|%p55, %r226, %r227, %r219, %r221;
	mov.b32 	%f180, %r228;
	add.f32 	%f181, %f179, %f180;
	mov.b32 	%r229, %f181;
	mov.u32 	%r230, 2;
	shfl.sync.bfly.b32 	%r231|%p56, %r229, %r230, %r219, %r221;
	mov.b32 	%f182, %r231;
	add.f32 	%f183, %f181, %f182;
	mov.b32 	%r232, %f183;
	mov.u32 	%r233, 1;
	shfl.sync.bfly.b32 	%r234|%p57, %r232, %r233, %r219, %r221;
	mov.b32 	%f184, %r234;
	add.f32 	%f185, %f183, %f184;
	st.local.f32 	[%rd2+16], %f185;
	st.shared.f32 	[%r10], %f185;
	bar.sync 	0;
	@%p1 bra 	$L__BB116_19;

	ld.shared.f32 	%f186, [%r4];
	mov.b32 	%r235, %f186;
	shfl.sync.bfly.b32 	%r239|%p58, %r235, %r220, %r219, %r221;
	mov.b32 	%f187, %r239;
	add.f32 	%f188, %f186, %f187;
	mov.b32 	%r240, %f188;
	shfl.sync.bfly.b32 	%r242|%p59, %r240, %r224, %r219, %r221;
	mov.b32 	%f189, %r242;
	add.f32 	%f190, %f188, %f189;
	mov.b32 	%r243, %f190;
	shfl.sync.bfly.b32 	%r245|%p60, %r243, %r227, %r219, %r221;
	mov.b32 	%f191, %r245;
	add.f32 	%f192, %f190, %f191;
	mov.b32 	%r246, %f192;
	shfl.sync.bfly.b32 	%r248|%p61, %r246, %r230, %r219, %r221;
	mov.b32 	%f193, %r248;
	add.f32 	%f194, %f192, %f193;
	mov.b32 	%r249, %f194;
	shfl.sync.bfly.b32 	%r251|%p62, %r249, %r233, %r219, %r221;
	mov.b32 	%f195, %r251;
	add.f32 	%f196, %f194, %f195;
	st.local.f32 	[%rd2+16], %f196;

$L__BB116_19:
	bar.sync 	0;
	setp.gt.s32 	%p63, %r3, 4;
	@%p63 bra 	$L__BB116_21;

	mad.lo.s32 	%r252, %r3, %r13, %r2;
	cvt.s64.s32 	%rd57, %r252;
	mul.lo.s32 	%r253, %r1, %r14;
	cvt.s64.s32 	%rd58, %r253;
	add.s64 	%rd59, %rd58, %rd57;
	mul.wide.s32 	%rd60, %r3, 4;
	add.s64 	%rd61, %rd2, %rd60;
	ld.local.f32 	%f197, [%rd61];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f197;}

	// end inline asm
	cvta.to.global.u64 	%rd62, %rd18;
	shl.b64 	%rd63, %rd59, 1;
	add.s64 	%rd64, %rd62, %rd63;
	st.global.u16 	[%rd64], %rs1;

$L__BB116_21:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_6_bs_224
.visible .entry ggml_matvec_f16_ncols_6_bs_224(
	.param .u64 ggml_matvec_f16_ncols_6_bs_224_param_0,
	.param .u64 ggml_matvec_f16_ncols_6_bs_224_param_1,
	.param .u64 ggml_matvec_f16_ncols_6_bs_224_param_2,
	.param .u32 ggml_matvec_f16_ncols_6_bs_224_param_3,
	.param .u32 ggml_matvec_f16_ncols_6_bs_224_param_4,
	.param .u32 ggml_matvec_f16_ncols_6_bs_224_param_5,
	.param .u32 ggml_matvec_f16_ncols_6_bs_224_param_6,
	.param .u32 ggml_matvec_f16_ncols_6_bs_224_param_7,
	.param .u32 ggml_matvec_f16_ncols_6_bs_224_param_8,
	.param .u32 ggml_matvec_f16_ncols_6_bs_224_param_9,
	.param .u32 ggml_matvec_f16_ncols_6_bs_224_param_10,
	.param .u32 ggml_matvec_f16_ncols_6_bs_224_param_11
)
{
	.local .align 8 .b8 	__local_depot117[24];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<75>;
	.reg .b16 	%rs<2>;
	.reg .f32 	%f<254>;
	.reg .b32 	%r<294>;
	.reg .b64 	%rd<70>;


	mov.u64 	%SPL, __local_depot117;
	ld.param.u64 	%rd20, [ggml_matvec_f16_ncols_6_bs_224_param_0];
	ld.param.u64 	%rd21, [ggml_matvec_f16_ncols_6_bs_224_param_1];
	ld.param.u64 	%rd19, [ggml_matvec_f16_ncols_6_bs_224_param_2];
	ld.param.u32 	%r11, [ggml_matvec_f16_ncols_6_bs_224_param_3];
	ld.param.u32 	%r15, [ggml_matvec_f16_ncols_6_bs_224_param_5];
	ld.param.u32 	%r12, [ggml_matvec_f16_ncols_6_bs_224_param_6];
	ld.param.u32 	%r13, [ggml_matvec_f16_ncols_6_bs_224_param_7];
	ld.param.u32 	%r16, [ggml_matvec_f16_ncols_6_bs_224_param_8];
	ld.param.u32 	%r17, [ggml_matvec_f16_ncols_6_bs_224_param_9];
	ld.param.u32 	%r18, [ggml_matvec_f16_ncols_6_bs_224_param_10];
	ld.param.u32 	%r14, [ggml_matvec_f16_ncols_6_bs_224_param_11];
	cvta.to.global.u64 	%rd69, %rd21;
	add.u64 	%rd2, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r19, %r1, %r16;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r20, %r2, %r15;
	mad.lo.s32 	%r21, %r19, %r17, %r20;
	cvt.s64.s32 	%rd3, %r21;
	cvta.to.global.u64 	%rd4, %rd20;
	mul.lo.s32 	%r22, %r1, %r18;
	cvt.s64.s32 	%rd5, %r22;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r23, %r3, 2;
	mov.u32 	%r24, data_mmv;
	add.s32 	%r4, %r24, %r23;
	@%p1 bra 	$L__BB117_2;

	mov.u32 	%r25, 0;
	st.shared.u32 	[%r4], %r25;

$L__BB117_2:
	bar.sync 	0;
	mov.f32 	%f248, 0f00000000;
	st.local.v2.f32 	[%rd2], {%f248, %f248};
	st.local.v2.f32 	[%rd2+8], {%f248, %f248};
	st.local.v2.f32 	[%rd2+16], {%f248, %f248};
	setp.ge.s32 	%p2, %r3, %r11;
	mov.f32 	%f249, %f248;
	mov.f32 	%f250, %f248;
	mov.f32 	%f251, %f248;
	mov.f32 	%f252, %f248;
	mov.f32 	%f253, %f248;
	@%p2 bra 	$L__BB117_9;

	not.b32 	%r26, %r3;
	add.s32 	%r5, %r26, %r11;
	shr.u32 	%r27, %r5, 5;
	mul.wide.u32 	%rd23, %r27, 613566757;
	shr.u64 	%rd24, %rd23, 32;
	and.b64  	%rd25, %rd24, 1;
	setp.eq.b64 	%p3, %rd25, 1;
	mov.pred 	%p4, 0;
	xor.pred  	%p5, %p3, %p4;
	mov.f32 	%f248, 0f00000000;
	mov.u32 	%r293, %r3;
	@%p5 bra 	$L__BB117_5;

	shl.b64 	%rd26, %rd5, 1;
	add.s64 	%rd27, %rd69, %rd26;
	shl.b64 	%rd28, %rd3, 1;
	add.s64 	%rd29, %rd4, %rd28;
	mul.wide.s32 	%rd30, %r3, 4;
	add.s64 	%rd31, %rd29, %rd30;
	ld.global.nc.u32 	%r28, [%rd31];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r28;
  cvt.f32.f16 %f43, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r28;
  cvt.f32.f16 %f44, high;}

	// end inline asm
	add.s64 	%rd32, %rd27, %rd30;
	ld.global.nc.u32 	%r30, [%rd32];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r30;
  cvt.f32.f16 %f45, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r30;
  cvt.f32.f16 %f46, high;}

	// end inline asm
	fma.rn.f32 	%f57, %f43, %f45, 0f00000000;
	fma.rn.f32 	%f253, %f44, %f46, %f57;
	st.local.f32 	[%rd2], %f253;
	mul.wide.s32 	%rd33, %r12, 4;
	add.s64 	%rd34, %rd32, %rd33;
	ld.global.nc.u32 	%r32, [%rd34];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r32;
  cvt.f32.f16 %f47, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r32;
  cvt.f32.f16 %f48, high;}

	// end inline asm
	fma.rn.f32 	%f58, %f43, %f47, 0f00000000;
	fma.rn.f32 	%f252, %f44, %f48, %f58;
	st.local.f32 	[%rd2+4], %f252;
	add.s32 	%r42, %r3, %r12;
	add.s32 	%r43, %r42, %r12;
	mul.wide.s32 	%rd35, %r43, 4;
	add.s64 	%rd36, %rd27, %rd35;
	ld.global.nc.u32 	%r34, [%rd36];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r34;
  cvt.f32.f16 %f49, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r34;
  cvt.f32.f16 %f50, high;}

	// end inline asm
	fma.rn.f32 	%f59, %f43, %f49, 0f00000000;
	fma.rn.f32 	%f251, %f44, %f50, %f59;
	st.local.f32 	[%rd2+8], %f251;
	add.s64 	%rd37, %rd36, %rd33;
	ld.global.nc.u32 	%r36, [%rd37];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r36;
  cvt.f32.f16 %f51, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r36;
  cvt.f32.f16 %f52, high;}

	// end inline asm
	fma.rn.f32 	%f60, %f43, %f51, 0f00000000;
	fma.rn.f32 	%f250, %f44, %f52, %f60;
	st.local.f32 	[%rd2+12], %f250;
	add.s64 	%rd38, %rd37, %rd33;
	ld.global.nc.u32 	%r38, [%rd38];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r38;
  cvt.f32.f16 %f53, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r38;
  cvt.f32.f16 %f54, high;}

	// end inline asm
	fma.rn.f32 	%f61, %f43, %f53, 0f00000000;
	fma.rn.f32 	%f249, %f44, %f54, %f61;
	st.local.f32 	[%rd2+16], %f249;
	add.s64 	%rd39, %rd38, %rd33;
	ld.global.nc.u32 	%r40, [%rd39];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r40;
  cvt.f32.f16 %f55, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r40;
  cvt.f32.f16 %f56, high;}

	// end inline asm
	fma.rn.f32 	%f62, %f43, %f55, 0f00000000;
	fma.rn.f32 	%f248, %f44, %f56, %f62;
	st.local.f32 	[%rd2+20], %f248;
	add.s32 	%r293, %r3, 224;

$L__BB117_5:
	setp.lt.u32 	%p6, %r5, 224;
	@%p6 bra 	$L__BB117_9;

	add.s32 	%r44, %r293, %r12;
	add.s32 	%r45, %r44, 224;
	mul.wide.s32 	%rd40, %r45, 4;
	shl.b64 	%rd41, %rd5, 1;
	add.s64 	%rd7, %rd40, %rd41;
	shl.b32 	%r46, %r12, 1;
	add.s32 	%r47, %r293, %r46;
	mad.lo.s32 	%r48, %r12, 3, %r293;
	shl.b32 	%r49, %r12, 2;
	add.s32 	%r50, %r293, %r49;
	mad.lo.s32 	%r51, %r12, 5, %r293;
	mul.wide.s32 	%rd42, %r47, 4;
	add.s64 	%rd8, %rd42, %rd41;
	mul.wide.s32 	%rd43, %r48, 4;
	add.s64 	%rd9, %rd43, %rd41;
	mul.wide.s32 	%rd44, %r50, 4;
	add.s64 	%rd10, %rd44, %rd41;
	mul.wide.s32 	%rd45, %r51, 4;
	add.s64 	%rd11, %rd45, %rd41;
	mul.wide.s32 	%rd46, %r293, 2;
	add.s64 	%rd47, %rd46, %rd3;
	shl.b64 	%rd48, %rd47, 1;
	add.s64 	%rd49, %rd4, %rd48;
	add.s64 	%rd68, %rd49, 896;
	mul.wide.s32 	%rd50, %r293, 4;
	mul.wide.s32 	%rd51, %r12, 4;
	add.s64 	%rd52, %rd50, %rd51;
	add.s64 	%rd13, %rd52, %rd41;
	add.s64 	%rd14, %rd50, %rd41;

$L__BB117_7:
	ld.global.nc.u32 	%r52, [%rd68+-896];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r52;
  cvt.f32.f16 %f63, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r52;
  cvt.f32.f16 %f64, high;}

	// end inline asm
	add.s64 	%rd53, %rd69, %rd14;
	ld.global.nc.u32 	%r54, [%rd53];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r54;
  cvt.f32.f16 %f65, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r54;
  cvt.f32.f16 %f66, high;}

	// end inline asm
	fma.rn.f32 	%f91, %f63, %f65, %f253;
	fma.rn.f32 	%f92, %f64, %f66, %f91;
	add.s64 	%rd54, %rd69, %rd13;
	ld.global.nc.u32 	%r56, [%rd54];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r56;
  cvt.f32.f16 %f67, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r56;
  cvt.f32.f16 %f68, high;}

	// end inline asm
	fma.rn.f32 	%f93, %f63, %f67, %f252;
	fma.rn.f32 	%f94, %f64, %f68, %f93;
	add.s64 	%rd55, %rd69, %rd8;
	ld.global.nc.u32 	%r58, [%rd55];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r58;
  cvt.f32.f16 %f69, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r58;
  cvt.f32.f16 %f70, high;}

	// end inline asm
	fma.rn.f32 	%f95, %f63, %f69, %f251;
	fma.rn.f32 	%f96, %f64, %f70, %f95;
	add.s64 	%rd56, %rd69, %rd9;
	ld.global.nc.u32 	%r60, [%rd56];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r60;
  cvt.f32.f16 %f71, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r60;
  cvt.f32.f16 %f72, high;}

	// end inline asm
	fma.rn.f32 	%f97, %f63, %f71, %f250;
	fma.rn.f32 	%f98, %f64, %f72, %f97;
	add.s64 	%rd57, %rd69, %rd10;
	ld.global.nc.u32 	%r62, [%rd57];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r62;
  cvt.f32.f16 %f73, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r62;
  cvt.f32.f16 %f74, high;}

	// end inline asm
	fma.rn.f32 	%f99, %f63, %f73, %f249;
	fma.rn.f32 	%f100, %f64, %f74, %f99;
	add.s64 	%rd58, %rd69, %rd11;
	ld.global.nc.u32 	%r64, [%rd58];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r64;
  cvt.f32.f16 %f75, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r64;
  cvt.f32.f16 %f76, high;}

	// end inline asm
	fma.rn.f32 	%f101, %f63, %f75, %f248;
	fma.rn.f32 	%f102, %f64, %f76, %f101;
	ld.global.nc.u32 	%r66, [%rd68];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r66;
  cvt.f32.f16 %f77, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r66;
  cvt.f32.f16 %f78, high;}

	// end inline asm
	ld.global.nc.u32 	%r68, [%rd53+896];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r68;
  cvt.f32.f16 %f79, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r68;
  cvt.f32.f16 %f80, high;}

	// end inline asm
	fma.rn.f32 	%f103, %f77, %f79, %f92;
	fma.rn.f32 	%f253, %f78, %f80, %f103;
	add.s64 	%rd59, %rd69, %rd7;
	ld.global.nc.u32 	%r70, [%rd59];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r70;
  cvt.f32.f16 %f81, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r70;
  cvt.f32.f16 %f82, high;}

	// end inline asm
	fma.rn.f32 	%f104, %f77, %f81, %f94;
	fma.rn.f32 	%f252, %f78, %f82, %f104;
	ld.global.nc.u32 	%r72, [%rd55+896];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r72;
  cvt.f32.f16 %f83, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r72;
  cvt.f32.f16 %f84, high;}

	// end inline asm
	fma.rn.f32 	%f105, %f77, %f83, %f96;
	fma.rn.f32 	%f251, %f78, %f84, %f105;
	ld.global.nc.u32 	%r74, [%rd56+896];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r74;
  cvt.f32.f16 %f85, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r74;
  cvt.f32.f16 %f86, high;}

	// end inline asm
	fma.rn.f32 	%f106, %f77, %f85, %f98;
	fma.rn.f32 	%f250, %f78, %f86, %f106;
	ld.global.nc.u32 	%r76, [%rd57+896];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r76;
  cvt.f32.f16 %f87, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r76;
  cvt.f32.f16 %f88, high;}

	// end inline asm
	fma.rn.f32 	%f107, %f77, %f87, %f100;
	fma.rn.f32 	%f249, %f78, %f88, %f107;
	ld.global.nc.u32 	%r78, [%rd58+896];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r78;
  cvt.f32.f16 %f89, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r78;
  cvt.f32.f16 %f90, high;}

	// end inline asm
	fma.rn.f32 	%f108, %f77, %f89, %f102;
	fma.rn.f32 	%f248, %f78, %f90, %f108;
	add.s64 	%rd69, %rd69, 1792;
	add.s64 	%rd68, %rd68, 1792;
	add.s32 	%r293, %r293, 448;
	setp.lt.s32 	%p7, %r293, %r11;
	@%p7 bra 	$L__BB117_7;

	st.local.v2.f32 	[%rd2], {%f253, %f252};
	st.local.v2.f32 	[%rd2+8], {%f251, %f250};
	st.local.v2.f32 	[%rd2+16], {%f249, %f248};

$L__BB117_9:
	shr.s32 	%r80, %r3, 31;
	shr.u32 	%r81, %r80, 27;
	add.s32 	%r82, %r3, %r81;
	shr.s32 	%r83, %r82, 5;
	shl.b32 	%r84, %r83, 2;
	add.s32 	%r10, %r24, %r84;
	mov.u32 	%r86, 2;
	mov.b32 	%r87, %f253;
	mov.u32 	%r88, 31;
	mov.u32 	%r89, 16;
	mov.u32 	%r90, -1;
	shfl.sync.bfly.b32 	%r91|%p8, %r87, %r89, %r88, %r90;
	mov.b32 	%f109, %r91;
	add.f32 	%f110, %f253, %f109;
	mov.b32 	%r92, %f110;
	mov.u32 	%r93, 8;
	shfl.sync.bfly.b32 	%r94|%p9, %r92, %r93, %r88, %r90;
	mov.b32 	%f111, %r94;
	add.f32 	%f112, %f110, %f111;
	mov.b32 	%r95, %f112;
	mov.u32 	%r96, 4;
	shfl.sync.bfly.b32 	%r97|%p10, %r95, %r96, %r88, %r90;
	mov.b32 	%f113, %r97;
	add.f32 	%f114, %f112, %f113;
	mov.b32 	%r98, %f114;
	shfl.sync.bfly.b32 	%r99|%p11, %r98, %r86, %r88, %r90;
	mov.b32 	%f115, %r99;
	add.f32 	%f116, %f114, %f115;
	mov.b32 	%r100, %f116;
	mov.u32 	%r101, 1;
	shfl.sync.bfly.b32 	%r102|%p12, %r100, %r101, %r88, %r90;
	mov.b32 	%f117, %r102;
	add.f32 	%f118, %f116, %f117;
	st.local.f32 	[%rd2], %f118;
	st.shared.f32 	[%r10], %f118;
	bar.sync 	0;
	@%p1 bra 	$L__BB117_11;

	ld.shared.f32 	%f119, [%r4];
	mov.b32 	%r103, %f119;
	shfl.sync.bfly.b32 	%r107|%p14, %r103, %r89, %r88, %r90;
	mov.b32 	%f120, %r107;
	add.f32 	%f121, %f119, %f120;
	mov.b32 	%r108, %f121;
	shfl.sync.bfly.b32 	%r110|%p15, %r108, %r93, %r88, %r90;
	mov.b32 	%f122, %r110;
	add.f32 	%f123, %f121, %f122;
	mov.b32 	%r111, %f123;
	shfl.sync.bfly.b32 	%r113|%p16, %r111, %r96, %r88, %r90;
	mov.b32 	%f124, %r113;
	add.f32 	%f125, %f123, %f124;
	mov.b32 	%r114, %f125;
	shfl.sync.bfly.b32 	%r116|%p17, %r114, %r86, %r88, %r90;
	mov.b32 	%f126, %r116;
	add.f32 	%f127, %f125, %f126;
	mov.b32 	%r117, %f127;
	shfl.sync.bfly.b32 	%r119|%p18, %r117, %r101, %r88, %r90;
	mov.b32 	%f128, %r119;
	add.f32 	%f129, %f127, %f128;
	st.local.f32 	[%rd2], %f129;

$L__BB117_11:
	bar.sync 	0;
	mov.b32 	%r120, %f252;
	shfl.sync.bfly.b32 	%r124|%p20, %r120, %r89, %r88, %r90;
	mov.b32 	%f130, %r124;
	add.f32 	%f131, %f252, %f130;
	mov.b32 	%r125, %f131;
	shfl.sync.bfly.b32 	%r127|%p21, %r125, %r93, %r88, %r90;
	mov.b32 	%f132, %r127;
	add.f32 	%f133, %f131, %f132;
	mov.b32 	%r128, %f133;
	shfl.sync.bfly.b32 	%r130|%p22, %r128, %r96, %r88, %r90;
	mov.b32 	%f134, %r130;
	add.f32 	%f135, %f133, %f134;
	mov.b32 	%r131, %f135;
	shfl.sync.bfly.b32 	%r133|%p23, %r131, %r86, %r88, %r90;
	mov.b32 	%f136, %r133;
	add.f32 	%f137, %f135, %f136;
	mov.b32 	%r134, %f137;
	shfl.sync.bfly.b32 	%r136|%p24, %r134, %r101, %r88, %r90;
	mov.b32 	%f138, %r136;
	add.f32 	%f139, %f137, %f138;
	st.local.f32 	[%rd2+4], %f139;
	st.shared.f32 	[%r10], %f139;
	bar.sync 	0;
	@%p1 bra 	$L__BB117_13;

	ld.shared.f32 	%f140, [%r4];
	mov.b32 	%r137, %f140;
	mov.u32 	%r138, 31;
	mov.u32 	%r139, 16;
	mov.u32 	%r140, -1;
	shfl.sync.bfly.b32 	%r141|%p25, %r137, %r139, %r138, %r140;
	mov.b32 	%f141, %r141;
	add.f32 	%f142, %f140, %f141;
	mov.b32 	%r142, %f142;
	mov.u32 	%r143, 8;
	shfl.sync.bfly.b32 	%r144|%p26, %r142, %r143, %r138, %r140;
	mov.b32 	%f143, %r144;
	add.f32 	%f144, %f142, %f143;
	mov.b32 	%r145, %f144;
	mov.u32 	%r146, 4;
	shfl.sync.bfly.b32 	%r147|%p27, %r145, %r146, %r138, %r140;
	mov.b32 	%f145, %r147;
	add.f32 	%f146, %f144, %f145;
	mov.b32 	%r148, %f146;
	mov.u32 	%r149, 2;
	shfl.sync.bfly.b32 	%r150|%p28, %r148, %r149, %r138, %r140;
	mov.b32 	%f147, %r150;
	add.f32 	%f148, %f146, %f147;
	mov.b32 	%r151, %f148;
	mov.u32 	%r152, 1;
	shfl.sync.bfly.b32 	%r153|%p29, %r151, %r152, %r138, %r140;
	mov.b32 	%f149, %r153;
	add.f32 	%f150, %f148, %f149;
	st.local.f32 	[%rd2+4], %f150;

$L__BB117_13:
	bar.sync 	0;
	mov.b32 	%r154, %f251;
	mov.u32 	%r155, 31;
	mov.u32 	%r156, 16;
	mov.u32 	%r157, -1;
	shfl.sync.bfly.b32 	%r158|%p31, %r154, %r156, %r155, %r157;
	mov.b32 	%f151, %r158;
	add.f32 	%f152, %f251, %f151;
	mov.b32 	%r159, %f152;
	mov.u32 	%r160, 8;
	shfl.sync.bfly.b32 	%r161|%p32, %r159, %r160, %r155, %r157;
	mov.b32 	%f153, %r161;
	add.f32 	%f154, %f152, %f153;
	mov.b32 	%r162, %f154;
	mov.u32 	%r163, 4;
	shfl.sync.bfly.b32 	%r164|%p33, %r162, %r163, %r155, %r157;
	mov.b32 	%f155, %r164;
	add.f32 	%f156, %f154, %f155;
	mov.b32 	%r165, %f156;
	mov.u32 	%r166, 2;
	shfl.sync.bfly.b32 	%r167|%p34, %r165, %r166, %r155, %r157;
	mov.b32 	%f157, %r167;
	add.f32 	%f158, %f156, %f157;
	mov.b32 	%r168, %f158;
	mov.u32 	%r169, 1;
	shfl.sync.bfly.b32 	%r170|%p35, %r168, %r169, %r155, %r157;
	mov.b32 	%f159, %r170;
	add.f32 	%f160, %f158, %f159;
	st.local.f32 	[%rd2+8], %f160;
	st.shared.f32 	[%r10], %f160;
	bar.sync 	0;
	@%p1 bra 	$L__BB117_15;

	ld.shared.f32 	%f161, [%r4];
	mov.b32 	%r171, %f161;
	shfl.sync.bfly.b32 	%r175|%p36, %r171, %r156, %r155, %r157;
	mov.b32 	%f162, %r175;
	add.f32 	%f163, %f161, %f162;
	mov.b32 	%r176, %f163;
	shfl.sync.bfly.b32 	%r178|%p37, %r176, %r160, %r155, %r157;
	mov.b32 	%f164, %r178;
	add.f32 	%f165, %f163, %f164;
	mov.b32 	%r179, %f165;
	shfl.sync.bfly.b32 	%r181|%p38, %r179, %r163, %r155, %r157;
	mov.b32 	%f166, %r181;
	add.f32 	%f167, %f165, %f166;
	mov.b32 	%r182, %f167;
	shfl.sync.bfly.b32 	%r184|%p39, %r182, %r166, %r155, %r157;
	mov.b32 	%f168, %r184;
	add.f32 	%f169, %f167, %f168;
	mov.b32 	%r185, %f169;
	shfl.sync.bfly.b32 	%r187|%p40, %r185, %r169, %r155, %r157;
	mov.b32 	%f170, %r187;
	add.f32 	%f171, %f169, %f170;
	st.local.f32 	[%rd2+8], %f171;

$L__BB117_15:
	bar.sync 	0;
	mov.b32 	%r188, %f250;
	shfl.sync.bfly.b32 	%r192|%p42, %r188, %r156, %r155, %r157;
	mov.b32 	%f172, %r192;
	add.f32 	%f173, %f250, %f172;
	mov.b32 	%r193, %f173;
	shfl.sync.bfly.b32 	%r195|%p43, %r193, %r160, %r155, %r157;
	mov.b32 	%f174, %r195;
	add.f32 	%f175, %f173, %f174;
	mov.b32 	%r196, %f175;
	shfl.sync.bfly.b32 	%r198|%p44, %r196, %r163, %r155, %r157;
	mov.b32 	%f176, %r198;
	add.f32 	%f177, %f175, %f176;
	mov.b32 	%r199, %f177;
	shfl.sync.bfly.b32 	%r201|%p45, %r199, %r166, %r155, %r157;
	mov.b32 	%f178, %r201;
	add.f32 	%f179, %f177, %f178;
	mov.b32 	%r202, %f179;
	shfl.sync.bfly.b32 	%r204|%p46, %r202, %r169, %r155, %r157;
	mov.b32 	%f180, %r204;
	add.f32 	%f181, %f179, %f180;
	st.local.f32 	[%rd2+12], %f181;
	st.shared.f32 	[%r10], %f181;
	bar.sync 	0;
	@%p1 bra 	$L__BB117_17;

	ld.shared.f32 	%f182, [%r4];
	mov.b32 	%r205, %f182;
	mov.u32 	%r206, 31;
	mov.u32 	%r207, 16;
	mov.u32 	%r208, -1;
	shfl.sync.bfly.b32 	%r209|%p47, %r205, %r207, %r206, %r208;
	mov.b32 	%f183, %r209;
	add.f32 	%f184, %f182, %f183;
	mov.b32 	%r210, %f184;
	mov.u32 	%r211, 8;
	shfl.sync.bfly.b32 	%r212|%p48, %r210, %r211, %r206, %r208;
	mov.b32 	%f185, %r212;
	add.f32 	%f186, %f184, %f185;
	mov.b32 	%r213, %f186;
	mov.u32 	%r214, 4;
	shfl.sync.bfly.b32 	%r215|%p49, %r213, %r214, %r206, %r208;
	mov.b32 	%f187, %r215;
	add.f32 	%f188, %f186, %f187;
	mov.b32 	%r216, %f188;
	mov.u32 	%r217, 2;
	shfl.sync.bfly.b32 	%r218|%p50, %r216, %r217, %r206, %r208;
	mov.b32 	%f189, %r218;
	add.f32 	%f190, %f188, %f189;
	mov.b32 	%r219, %f190;
	mov.u32 	%r220, 1;
	shfl.sync.bfly.b32 	%r221|%p51, %r219, %r220, %r206, %r208;
	mov.b32 	%f191, %r221;
	add.f32 	%f192, %f190, %f191;
	st.local.f32 	[%rd2+12], %f192;

$L__BB117_17:
	bar.sync 	0;
	mov.b32 	%r222, %f249;
	mov.u32 	%r223, 31;
	mov.u32 	%r224, 16;
	mov.u32 	%r225, -1;
	shfl.sync.bfly.b32 	%r226|%p53, %r222, %r224, %r223, %r225;
	mov.b32 	%f193, %r226;
	add.f32 	%f194, %f249, %f193;
	mov.b32 	%r227, %f194;
	mov.u32 	%r228, 8;
	shfl.sync.bfly.b32 	%r229|%p54, %r227, %r228, %r223, %r225;
	mov.b32 	%f195, %r229;
	add.f32 	%f196, %f194, %f195;
	mov.b32 	%r230, %f196;
	mov.u32 	%r231, 4;
	shfl.sync.bfly.b32 	%r232|%p55, %r230, %r231, %r223, %r225;
	mov.b32 	%f197, %r232;
	add.f32 	%f198, %f196, %f197;
	mov.b32 	%r233, %f198;
	mov.u32 	%r234, 2;
	shfl.sync.bfly.b32 	%r235|%p56, %r233, %r234, %r223, %r225;
	mov.b32 	%f199, %r235;
	add.f32 	%f200, %f198, %f199;
	mov.b32 	%r236, %f200;
	mov.u32 	%r237, 1;
	shfl.sync.bfly.b32 	%r238|%p57, %r236, %r237, %r223, %r225;
	mov.b32 	%f201, %r238;
	add.f32 	%f202, %f200, %f201;
	st.local.f32 	[%rd2+16], %f202;
	st.shared.f32 	[%r10], %f202;
	bar.sync 	0;
	@%p1 bra 	$L__BB117_19;

	ld.shared.f32 	%f203, [%r4];
	mov.b32 	%r239, %f203;
	shfl.sync.bfly.b32 	%r243|%p58, %r239, %r224, %r223, %r225;
	mov.b32 	%f204, %r243;
	add.f32 	%f205, %f203, %f204;
	mov.b32 	%r244, %f205;
	shfl.sync.bfly.b32 	%r246|%p59, %r244, %r228, %r223, %r225;
	mov.b32 	%f206, %r246;
	add.f32 	%f207, %f205, %f206;
	mov.b32 	%r247, %f207;
	shfl.sync.bfly.b32 	%r249|%p60, %r247, %r231, %r223, %r225;
	mov.b32 	%f208, %r249;
	add.f32 	%f209, %f207, %f208;
	mov.b32 	%r250, %f209;
	shfl.sync.bfly.b32 	%r252|%p61, %r250, %r234, %r223, %r225;
	mov.b32 	%f210, %r252;
	add.f32 	%f211, %f209, %f210;
	mov.b32 	%r253, %f211;
	shfl.sync.bfly.b32 	%r255|%p62, %r253, %r237, %r223, %r225;
	mov.b32 	%f212, %r255;
	add.f32 	%f213, %f211, %f212;
	st.local.f32 	[%rd2+16], %f213;

$L__BB117_19:
	bar.sync 	0;
	mov.b32 	%r256, %f248;
	shfl.sync.bfly.b32 	%r260|%p64, %r256, %r224, %r223, %r225;
	mov.b32 	%f214, %r260;
	add.f32 	%f215, %f248, %f214;
	mov.b32 	%r261, %f215;
	shfl.sync.bfly.b32 	%r263|%p65, %r261, %r228, %r223, %r225;
	mov.b32 	%f216, %r263;
	add.f32 	%f217, %f215, %f216;
	mov.b32 	%r264, %f217;
	shfl.sync.bfly.b32 	%r266|%p66, %r264, %r231, %r223, %r225;
	mov.b32 	%f218, %r266;
	add.f32 	%f219, %f217, %f218;
	mov.b32 	%r267, %f219;
	shfl.sync.bfly.b32 	%r269|%p67, %r267, %r234, %r223, %r225;
	mov.b32 	%f220, %r269;
	add.f32 	%f221, %f219, %f220;
	mov.b32 	%r270, %f221;
	shfl.sync.bfly.b32 	%r272|%p68, %r270, %r237, %r223, %r225;
	mov.b32 	%f222, %r272;
	add.f32 	%f223, %f221, %f222;
	st.local.f32 	[%rd2+20], %f223;
	st.shared.f32 	[%r10], %f223;
	bar.sync 	0;
	@%p1 bra 	$L__BB117_21;

	ld.shared.f32 	%f224, [%r4];
	mov.b32 	%r273, %f224;
	mov.u32 	%r274, 31;
	mov.u32 	%r275, 16;
	mov.u32 	%r276, -1;
	shfl.sync.bfly.b32 	%r277|%p69, %r273, %r275, %r274, %r276;
	mov.b32 	%f225, %r277;
	add.f32 	%f226, %f224, %f225;
	mov.b32 	%r278, %f226;
	mov.u32 	%r279, 8;
	shfl.sync.bfly.b32 	%r280|%p70, %r278, %r279, %r274, %r276;
	mov.b32 	%f227, %r280;
	add.f32 	%f228, %f226, %f227;
	mov.b32 	%r281, %f228;
	mov.u32 	%r282, 4;
	shfl.sync.bfly.b32 	%r283|%p71, %r281, %r282, %r274, %r276;
	mov.b32 	%f229, %r283;
	add.f32 	%f230, %f228, %f229;
	mov.b32 	%r284, %f230;
	mov.u32 	%r285, 2;
	shfl.sync.bfly.b32 	%r286|%p72, %r284, %r285, %r274, %r276;
	mov.b32 	%f231, %r286;
	add.f32 	%f232, %f230, %f231;
	mov.b32 	%r287, %f232;
	mov.u32 	%r288, 1;
	shfl.sync.bfly.b32 	%r289|%p73, %r287, %r288, %r274, %r276;
	mov.b32 	%f233, %r289;
	add.f32 	%f234, %f232, %f233;
	st.local.f32 	[%rd2+20], %f234;

$L__BB117_21:
	bar.sync 	0;
	setp.gt.s32 	%p74, %r3, 5;
	@%p74 bra 	$L__BB117_23;

	mad.lo.s32 	%r290, %r3, %r13, %r2;
	cvt.s64.s32 	%rd60, %r290;
	mul.lo.s32 	%r291, %r1, %r14;
	cvt.s64.s32 	%rd61, %r291;
	add.s64 	%rd62, %rd61, %rd60;
	mul.wide.s32 	%rd63, %r3, 4;
	add.s64 	%rd64, %rd2, %rd63;
	ld.local.f32 	%f235, [%rd64];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f235;}

	// end inline asm
	cvta.to.global.u64 	%rd65, %rd19;
	shl.b64 	%rd66, %rd62, 1;
	add.s64 	%rd67, %rd65, %rd66;
	st.global.u16 	[%rd67], %rs1;

$L__BB117_23:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_7_bs_224
.visible .entry ggml_matvec_f16_ncols_7_bs_224(
	.param .u64 ggml_matvec_f16_ncols_7_bs_224_param_0,
	.param .u64 ggml_matvec_f16_ncols_7_bs_224_param_1,
	.param .u64 ggml_matvec_f16_ncols_7_bs_224_param_2,
	.param .u32 ggml_matvec_f16_ncols_7_bs_224_param_3,
	.param .u32 ggml_matvec_f16_ncols_7_bs_224_param_4,
	.param .u32 ggml_matvec_f16_ncols_7_bs_224_param_5,
	.param .u32 ggml_matvec_f16_ncols_7_bs_224_param_6,
	.param .u32 ggml_matvec_f16_ncols_7_bs_224_param_7,
	.param .u32 ggml_matvec_f16_ncols_7_bs_224_param_8,
	.param .u32 ggml_matvec_f16_ncols_7_bs_224_param_9,
	.param .u32 ggml_matvec_f16_ncols_7_bs_224_param_10,
	.param .u32 ggml_matvec_f16_ncols_7_bs_224_param_11
)
{
	.local .align 4 .b8 	__local_depot118[28];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<82>;
	.reg .b16 	%rs<2>;
	.reg .f32 	%f<221>;
	.reg .b32 	%r<289>;
	.reg .b64 	%rd<43>;


	mov.u64 	%SPL, __local_depot118;
	ld.param.u64 	%rd13, [ggml_matvec_f16_ncols_7_bs_224_param_0];
	ld.param.u64 	%rd14, [ggml_matvec_f16_ncols_7_bs_224_param_1];
	ld.param.u64 	%rd15, [ggml_matvec_f16_ncols_7_bs_224_param_2];
	ld.param.u32 	%r8, [ggml_matvec_f16_ncols_7_bs_224_param_3];
	ld.param.u32 	%r9, [ggml_matvec_f16_ncols_7_bs_224_param_5];
	ld.param.u32 	%r10, [ggml_matvec_f16_ncols_7_bs_224_param_6];
	ld.param.u32 	%r11, [ggml_matvec_f16_ncols_7_bs_224_param_7];
	ld.param.u32 	%r12, [ggml_matvec_f16_ncols_7_bs_224_param_8];
	ld.param.u32 	%r13, [ggml_matvec_f16_ncols_7_bs_224_param_9];
	ld.param.u32 	%r14, [ggml_matvec_f16_ncols_7_bs_224_param_10];
	ld.param.u32 	%r15, [ggml_matvec_f16_ncols_7_bs_224_param_11];
	add.u64 	%rd1, %SPL, 0;
	mov.u32 	%r1, %ctaid.x;
	mov.u32 	%r2, %tid.x;
	setp.gt.s32 	%p1, %r2, 31;
	shl.b32 	%r16, %r2, 2;
	mov.u32 	%r17, data_mmv;
	add.s32 	%r3, %r17, %r16;
	@%p1 bra 	$L__BB118_2;

	mov.u32 	%r18, 0;
	st.shared.u32 	[%r3], %r18;

$L__BB118_2:
	mov.u32 	%r4, %ctaid.y;
	bar.sync 	0;
	mov.f32 	%f214, 0f00000000;
	mov.u32 	%r19, 0;
	st.local.u32 	[%rd1], %r19;
	st.local.u32 	[%rd1+4], %r19;
	st.local.u32 	[%rd1+8], %r19;
	st.local.u32 	[%rd1+12], %r19;
	st.local.u32 	[%rd1+16], %r19;
	st.local.u32 	[%rd1+20], %r19;
	st.local.u32 	[%rd1+24], %r19;
	setp.ge.s32 	%p2, %r2, %r8;
	mov.f32 	%f215, %f214;
	mov.f32 	%f216, %f214;
	mov.f32 	%f217, %f214;
	mov.f32 	%f218, %f214;
	mov.f32 	%f219, %f214;
	mov.f32 	%f220, %f214;
	@%p2 bra 	$L__BB118_6;

	shl.b32 	%r20, %r10, 1;
	add.s32 	%r21, %r2, %r20;
	mul.wide.s32 	%rd17, %r21, 4;
	mul.lo.s32 	%r22, %r4, %r14;
	mul.wide.s32 	%rd18, %r22, 2;
	add.s64 	%rd3, %rd17, %rd18;
	mul.wide.s32 	%rd19, %r2, 4;
	mul.wide.s32 	%rd4, %r10, 4;
	add.s64 	%rd20, %rd19, %rd4;
	add.s64 	%rd5, %rd20, %rd18;
	add.s64 	%rd6, %rd19, %rd18;
	mul.wide.s32 	%rd21, %r2, 2;
	div.s32 	%r23, %r4, %r12;
	mul.lo.s32 	%r24, %r1, %r9;
	mad.lo.s32 	%r25, %r23, %r13, %r24;
	cvt.s64.s32 	%rd22, %r25;
	add.s64 	%rd23, %rd21, %rd22;
	cvta.to.global.u64 	%rd24, %rd13;
	shl.b64 	%rd25, %rd23, 1;
	add.s64 	%rd41, %rd24, %rd25;
	cvta.to.global.u64 	%rd42, %rd14;
	mov.f32 	%f214, 0f00000000;
	mov.u32 	%r288, %r2;

$L__BB118_4:
	ld.global.nc.u32 	%r26, [%rd41];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r26;
  cvt.f32.f16 %f36, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r26;
  cvt.f32.f16 %f37, high;}

	// end inline asm
	add.s64 	%rd26, %rd42, %rd6;
	ld.global.nc.u32 	%r28, [%rd26];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r28;
  cvt.f32.f16 %f38, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r28;
  cvt.f32.f16 %f39, high;}

	// end inline asm
	fma.rn.f32 	%f52, %f36, %f38, %f220;
	fma.rn.f32 	%f220, %f37, %f39, %f52;
	add.s64 	%rd27, %rd42, %rd5;
	ld.global.nc.u32 	%r30, [%rd27];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r30;
  cvt.f32.f16 %f40, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r30;
  cvt.f32.f16 %f41, high;}

	// end inline asm
	fma.rn.f32 	%f53, %f36, %f40, %f219;
	fma.rn.f32 	%f219, %f37, %f41, %f53;
	add.s64 	%rd28, %rd42, %rd3;
	ld.global.nc.u32 	%r32, [%rd28];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r32;
  cvt.f32.f16 %f42, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r32;
  cvt.f32.f16 %f43, high;}

	// end inline asm
	fma.rn.f32 	%f54, %f36, %f42, %f218;
	fma.rn.f32 	%f218, %f37, %f43, %f54;
	add.s64 	%rd29, %rd28, %rd4;
	ld.global.nc.u32 	%r34, [%rd29];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r34;
  cvt.f32.f16 %f44, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r34;
  cvt.f32.f16 %f45, high;}

	// end inline asm
	fma.rn.f32 	%f55, %f36, %f44, %f217;
	fma.rn.f32 	%f217, %f37, %f45, %f55;
	add.s64 	%rd30, %rd29, %rd4;
	ld.global.nc.u32 	%r36, [%rd30];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r36;
  cvt.f32.f16 %f46, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r36;
  cvt.f32.f16 %f47, high;}

	// end inline asm
	fma.rn.f32 	%f56, %f36, %f46, %f216;
	fma.rn.f32 	%f216, %f37, %f47, %f56;
	add.s64 	%rd31, %rd30, %rd4;
	ld.global.nc.u32 	%r38, [%rd31];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r38;
  cvt.f32.f16 %f48, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r38;
  cvt.f32.f16 %f49, high;}

	// end inline asm
	fma.rn.f32 	%f57, %f36, %f48, %f215;
	fma.rn.f32 	%f215, %f37, %f49, %f57;
	add.s64 	%rd32, %rd31, %rd4;
	ld.global.nc.u32 	%r40, [%rd32];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r40;
  cvt.f32.f16 %f50, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r40;
  cvt.f32.f16 %f51, high;}

	// end inline asm
	fma.rn.f32 	%f58, %f36, %f50, %f214;
	fma.rn.f32 	%f214, %f37, %f51, %f58;
	add.s64 	%rd42, %rd42, 896;
	add.s64 	%rd41, %rd41, 896;
	add.s32 	%r288, %r288, 224;
	setp.lt.s32 	%p3, %r288, %r8;
	@%p3 bra 	$L__BB118_4;

	st.local.f32 	[%rd1], %f220;
	st.local.f32 	[%rd1+4], %f219;
	st.local.f32 	[%rd1+8], %f218;
	st.local.f32 	[%rd1+12], %f217;
	st.local.f32 	[%rd1+16], %f216;
	st.local.f32 	[%rd1+20], %f215;
	st.local.f32 	[%rd1+24], %f214;

$L__BB118_6:
	shr.s32 	%r42, %r2, 31;
	shr.u32 	%r43, %r42, 27;
	add.s32 	%r44, %r2, %r43;
	shr.s32 	%r45, %r44, 5;
	shl.b32 	%r46, %r45, 2;
	add.s32 	%r7, %r17, %r46;
	mov.u32 	%r48, 2;
	mov.b32 	%r49, %f220;
	mov.u32 	%r50, 31;
	mov.u32 	%r51, 16;
	mov.u32 	%r52, -1;
	shfl.sync.bfly.b32 	%r53|%p4, %r49, %r51, %r50, %r52;
	mov.b32 	%f59, %r53;
	add.f32 	%f60, %f220, %f59;
	mov.b32 	%r54, %f60;
	mov.u32 	%r55, 8;
	shfl.sync.bfly.b32 	%r56|%p5, %r54, %r55, %r50, %r52;
	mov.b32 	%f61, %r56;
	add.f32 	%f62, %f60, %f61;
	mov.b32 	%r57, %f62;
	mov.u32 	%r58, 4;
	shfl.sync.bfly.b32 	%r59|%p6, %r57, %r58, %r50, %r52;
	mov.b32 	%f63, %r59;
	add.f32 	%f64, %f62, %f63;
	mov.b32 	%r60, %f64;
	shfl.sync.bfly.b32 	%r61|%p7, %r60, %r48, %r50, %r52;
	mov.b32 	%f65, %r61;
	add.f32 	%f66, %f64, %f65;
	mov.b32 	%r62, %f66;
	mov.u32 	%r63, 1;
	shfl.sync.bfly.b32 	%r64|%p8, %r62, %r63, %r50, %r52;
	mov.b32 	%f67, %r64;
	add.f32 	%f68, %f66, %f67;
	st.local.f32 	[%rd1], %f68;
	st.shared.f32 	[%r7], %f68;
	bar.sync 	0;
	@%p1 bra 	$L__BB118_8;

	ld.shared.f32 	%f69, [%r3];
	mov.b32 	%r65, %f69;
	shfl.sync.bfly.b32 	%r69|%p10, %r65, %r51, %r50, %r52;
	mov.b32 	%f70, %r69;
	add.f32 	%f71, %f69, %f70;
	mov.b32 	%r70, %f71;
	shfl.sync.bfly.b32 	%r72|%p11, %r70, %r55, %r50, %r52;
	mov.b32 	%f72, %r72;
	add.f32 	%f73, %f71, %f72;
	mov.b32 	%r73, %f73;
	shfl.sync.bfly.b32 	%r75|%p12, %r73, %r58, %r50, %r52;
	mov.b32 	%f74, %r75;
	add.f32 	%f75, %f73, %f74;
	mov.b32 	%r76, %f75;
	shfl.sync.bfly.b32 	%r78|%p13, %r76, %r48, %r50, %r52;
	mov.b32 	%f76, %r78;
	add.f32 	%f77, %f75, %f76;
	mov.b32 	%r79, %f77;
	shfl.sync.bfly.b32 	%r81|%p14, %r79, %r63, %r50, %r52;
	mov.b32 	%f78, %r81;
	add.f32 	%f79, %f77, %f78;
	st.local.f32 	[%rd1], %f79;

$L__BB118_8:
	bar.sync 	0;
	mov.b32 	%r82, %f219;
	shfl.sync.bfly.b32 	%r86|%p16, %r82, %r51, %r50, %r52;
	mov.b32 	%f80, %r86;
	add.f32 	%f81, %f219, %f80;
	mov.b32 	%r87, %f81;
	shfl.sync.bfly.b32 	%r89|%p17, %r87, %r55, %r50, %r52;
	mov.b32 	%f82, %r89;
	add.f32 	%f83, %f81, %f82;
	mov.b32 	%r90, %f83;
	shfl.sync.bfly.b32 	%r92|%p18, %r90, %r58, %r50, %r52;
	mov.b32 	%f84, %r92;
	add.f32 	%f85, %f83, %f84;
	mov.b32 	%r93, %f85;
	shfl.sync.bfly.b32 	%r95|%p19, %r93, %r48, %r50, %r52;
	mov.b32 	%f86, %r95;
	add.f32 	%f87, %f85, %f86;
	mov.b32 	%r96, %f87;
	shfl.sync.bfly.b32 	%r98|%p20, %r96, %r63, %r50, %r52;
	mov.b32 	%f88, %r98;
	add.f32 	%f89, %f87, %f88;
	st.local.f32 	[%rd1+4], %f89;
	st.shared.f32 	[%r7], %f89;
	bar.sync 	0;
	@%p1 bra 	$L__BB118_10;

	ld.shared.f32 	%f90, [%r3];
	mov.b32 	%r99, %f90;
	mov.u32 	%r100, 31;
	mov.u32 	%r101, 16;
	mov.u32 	%r102, -1;
	shfl.sync.bfly.b32 	%r103|%p21, %r99, %r101, %r100, %r102;
	mov.b32 	%f91, %r103;
	add.f32 	%f92, %f90, %f91;
	mov.b32 	%r104, %f92;
	mov.u32 	%r105, 8;
	shfl.sync.bfly.b32 	%r106|%p22, %r104, %r105, %r100, %r102;
	mov.b32 	%f93, %r106;
	add.f32 	%f94, %f92, %f93;
	mov.b32 	%r107, %f94;
	mov.u32 	%r108, 4;
	shfl.sync.bfly.b32 	%r109|%p23, %r107, %r108, %r100, %r102;
	mov.b32 	%f95, %r109;
	add.f32 	%f96, %f94, %f95;
	mov.b32 	%r110, %f96;
	mov.u32 	%r111, 2;
	shfl.sync.bfly.b32 	%r112|%p24, %r110, %r111, %r100, %r102;
	mov.b32 	%f97, %r112;
	add.f32 	%f98, %f96, %f97;
	mov.b32 	%r113, %f98;
	mov.u32 	%r114, 1;
	shfl.sync.bfly.b32 	%r115|%p25, %r113, %r114, %r100, %r102;
	mov.b32 	%f99, %r115;
	add.f32 	%f100, %f98, %f99;
	st.local.f32 	[%rd1+4], %f100;

$L__BB118_10:
	bar.sync 	0;
	mov.b32 	%r116, %f218;
	mov.u32 	%r117, 31;
	mov.u32 	%r118, 16;
	mov.u32 	%r119, -1;
	shfl.sync.bfly.b32 	%r120|%p27, %r116, %r118, %r117, %r119;
	mov.b32 	%f101, %r120;
	add.f32 	%f102, %f218, %f101;
	mov.b32 	%r121, %f102;
	mov.u32 	%r122, 8;
	shfl.sync.bfly.b32 	%r123|%p28, %r121, %r122, %r117, %r119;
	mov.b32 	%f103, %r123;
	add.f32 	%f104, %f102, %f103;
	mov.b32 	%r124, %f104;
	mov.u32 	%r125, 4;
	shfl.sync.bfly.b32 	%r126|%p29, %r124, %r125, %r117, %r119;
	mov.b32 	%f105, %r126;
	add.f32 	%f106, %f104, %f105;
	mov.b32 	%r127, %f106;
	mov.u32 	%r128, 2;
	shfl.sync.bfly.b32 	%r129|%p30, %r127, %r128, %r117, %r119;
	mov.b32 	%f107, %r129;
	add.f32 	%f108, %f106, %f107;
	mov.b32 	%r130, %f108;
	mov.u32 	%r131, 1;
	shfl.sync.bfly.b32 	%r132|%p31, %r130, %r131, %r117, %r119;
	mov.b32 	%f109, %r132;
	add.f32 	%f110, %f108, %f109;
	st.local.f32 	[%rd1+8], %f110;
	st.shared.f32 	[%r7], %f110;
	bar.sync 	0;
	@%p1 bra 	$L__BB118_12;

	ld.shared.f32 	%f111, [%r3];
	mov.b32 	%r133, %f111;
	shfl.sync.bfly.b32 	%r137|%p32, %r133, %r118, %r117, %r119;
	mov.b32 	%f112, %r137;
	add.f32 	%f113, %f111, %f112;
	mov.b32 	%r138, %f113;
	shfl.sync.bfly.b32 	%r140|%p33, %r138, %r122, %r117, %r119;
	mov.b32 	%f114, %r140;
	add.f32 	%f115, %f113, %f114;
	mov.b32 	%r141, %f115;
	shfl.sync.bfly.b32 	%r143|%p34, %r141, %r125, %r117, %r119;
	mov.b32 	%f116, %r143;
	add.f32 	%f117, %f115, %f116;
	mov.b32 	%r144, %f117;
	shfl.sync.bfly.b32 	%r146|%p35, %r144, %r128, %r117, %r119;
	mov.b32 	%f118, %r146;
	add.f32 	%f119, %f117, %f118;
	mov.b32 	%r147, %f119;
	shfl.sync.bfly.b32 	%r149|%p36, %r147, %r131, %r117, %r119;
	mov.b32 	%f120, %r149;
	add.f32 	%f121, %f119, %f120;
	st.local.f32 	[%rd1+8], %f121;

$L__BB118_12:
	bar.sync 	0;
	mov.b32 	%r150, %f217;
	shfl.sync.bfly.b32 	%r154|%p38, %r150, %r118, %r117, %r119;
	mov.b32 	%f122, %r154;
	add.f32 	%f123, %f217, %f122;
	mov.b32 	%r155, %f123;
	shfl.sync.bfly.b32 	%r157|%p39, %r155, %r122, %r117, %r119;
	mov.b32 	%f124, %r157;
	add.f32 	%f125, %f123, %f124;
	mov.b32 	%r158, %f125;
	shfl.sync.bfly.b32 	%r160|%p40, %r158, %r125, %r117, %r119;
	mov.b32 	%f126, %r160;
	add.f32 	%f127, %f125, %f126;
	mov.b32 	%r161, %f127;
	shfl.sync.bfly.b32 	%r163|%p41, %r161, %r128, %r117, %r119;
	mov.b32 	%f128, %r163;
	add.f32 	%f129, %f127, %f128;
	mov.b32 	%r164, %f129;
	shfl.sync.bfly.b32 	%r166|%p42, %r164, %r131, %r117, %r119;
	mov.b32 	%f130, %r166;
	add.f32 	%f131, %f129, %f130;
	st.local.f32 	[%rd1+12], %f131;
	st.shared.f32 	[%r7], %f131;
	bar.sync 	0;
	@%p1 bra 	$L__BB118_14;

	ld.shared.f32 	%f132, [%r3];
	mov.b32 	%r167, %f132;
	mov.u32 	%r168, 31;
	mov.u32 	%r169, 16;
	mov.u32 	%r170, -1;
	shfl.sync.bfly.b32 	%r171|%p43, %r167, %r169, %r168, %r170;
	mov.b32 	%f133, %r171;
	add.f32 	%f134, %f132, %f133;
	mov.b32 	%r172, %f134;
	mov.u32 	%r173, 8;
	shfl.sync.bfly.b32 	%r174|%p44, %r172, %r173, %r168, %r170;
	mov.b32 	%f135, %r174;
	add.f32 	%f136, %f134, %f135;
	mov.b32 	%r175, %f136;
	mov.u32 	%r176, 4;
	shfl.sync.bfly.b32 	%r177|%p45, %r175, %r176, %r168, %r170;
	mov.b32 	%f137, %r177;
	add.f32 	%f138, %f136, %f137;
	mov.b32 	%r178, %f138;
	mov.u32 	%r179, 2;
	shfl.sync.bfly.b32 	%r180|%p46, %r178, %r179, %r168, %r170;
	mov.b32 	%f139, %r180;
	add.f32 	%f140, %f138, %f139;
	mov.b32 	%r181, %f140;
	mov.u32 	%r182, 1;
	shfl.sync.bfly.b32 	%r183|%p47, %r181, %r182, %r168, %r170;
	mov.b32 	%f141, %r183;
	add.f32 	%f142, %f140, %f141;
	st.local.f32 	[%rd1+12], %f142;

$L__BB118_14:
	bar.sync 	0;
	mov.b32 	%r184, %f216;
	mov.u32 	%r185, 31;
	mov.u32 	%r186, 16;
	mov.u32 	%r187, -1;
	shfl.sync.bfly.b32 	%r188|%p49, %r184, %r186, %r185, %r187;
	mov.b32 	%f143, %r188;
	add.f32 	%f144, %f216, %f143;
	mov.b32 	%r189, %f144;
	mov.u32 	%r190, 8;
	shfl.sync.bfly.b32 	%r191|%p50, %r189, %r190, %r185, %r187;
	mov.b32 	%f145, %r191;
	add.f32 	%f146, %f144, %f145;
	mov.b32 	%r192, %f146;
	mov.u32 	%r193, 4;
	shfl.sync.bfly.b32 	%r194|%p51, %r192, %r193, %r185, %r187;
	mov.b32 	%f147, %r194;
	add.f32 	%f148, %f146, %f147;
	mov.b32 	%r195, %f148;
	mov.u32 	%r196, 2;
	shfl.sync.bfly.b32 	%r197|%p52, %r195, %r196, %r185, %r187;
	mov.b32 	%f149, %r197;
	add.f32 	%f150, %f148, %f149;
	mov.b32 	%r198, %f150;
	mov.u32 	%r199, 1;
	shfl.sync.bfly.b32 	%r200|%p53, %r198, %r199, %r185, %r187;
	mov.b32 	%f151, %r200;
	add.f32 	%f152, %f150, %f151;
	st.local.f32 	[%rd1+16], %f152;
	st.shared.f32 	[%r7], %f152;
	bar.sync 	0;
	@%p1 bra 	$L__BB118_16;

	ld.shared.f32 	%f153, [%r3];
	mov.b32 	%r201, %f153;
	shfl.sync.bfly.b32 	%r205|%p54, %r201, %r186, %r185, %r187;
	mov.b32 	%f154, %r205;
	add.f32 	%f155, %f153, %f154;
	mov.b32 	%r206, %f155;
	shfl.sync.bfly.b32 	%r208|%p55, %r206, %r190, %r185, %r187;
	mov.b32 	%f156, %r208;
	add.f32 	%f157, %f155, %f156;
	mov.b32 	%r209, %f157;
	shfl.sync.bfly.b32 	%r211|%p56, %r209, %r193, %r185, %r187;
	mov.b32 	%f158, %r211;
	add.f32 	%f159, %f157, %f158;
	mov.b32 	%r212, %f159;
	shfl.sync.bfly.b32 	%r214|%p57, %r212, %r196, %r185, %r187;
	mov.b32 	%f160, %r214;
	add.f32 	%f161, %f159, %f160;
	mov.b32 	%r215, %f161;
	shfl.sync.bfly.b32 	%r217|%p58, %r215, %r199, %r185, %r187;
	mov.b32 	%f162, %r217;
	add.f32 	%f163, %f161, %f162;
	st.local.f32 	[%rd1+16], %f163;

$L__BB118_16:
	bar.sync 	0;
	mov.b32 	%r218, %f215;
	shfl.sync.bfly.b32 	%r222|%p60, %r218, %r186, %r185, %r187;
	mov.b32 	%f164, %r222;
	add.f32 	%f165, %f215, %f164;
	mov.b32 	%r223, %f165;
	shfl.sync.bfly.b32 	%r225|%p61, %r223, %r190, %r185, %r187;
	mov.b32 	%f166, %r225;
	add.f32 	%f167, %f165, %f166;
	mov.b32 	%r226, %f167;
	shfl.sync.bfly.b32 	%r228|%p62, %r226, %r193, %r185, %r187;
	mov.b32 	%f168, %r228;
	add.f32 	%f169, %f167, %f168;
	mov.b32 	%r229, %f169;
	shfl.sync.bfly.b32 	%r231|%p63, %r229, %r196, %r185, %r187;
	mov.b32 	%f170, %r231;
	add.f32 	%f171, %f169, %f170;
	mov.b32 	%r232, %f171;
	shfl.sync.bfly.b32 	%r234|%p64, %r232, %r199, %r185, %r187;
	mov.b32 	%f172, %r234;
	add.f32 	%f173, %f171, %f172;
	st.local.f32 	[%rd1+20], %f173;
	st.shared.f32 	[%r7], %f173;
	bar.sync 	0;
	@%p1 bra 	$L__BB118_18;

	ld.shared.f32 	%f174, [%r3];
	mov.b32 	%r235, %f174;
	mov.u32 	%r236, 31;
	mov.u32 	%r237, 16;
	mov.u32 	%r238, -1;
	shfl.sync.bfly.b32 	%r239|%p65, %r235, %r237, %r236, %r238;
	mov.b32 	%f175, %r239;
	add.f32 	%f176, %f174, %f175;
	mov.b32 	%r240, %f176;
	mov.u32 	%r241, 8;
	shfl.sync.bfly.b32 	%r242|%p66, %r240, %r241, %r236, %r238;
	mov.b32 	%f177, %r242;
	add.f32 	%f178, %f176, %f177;
	mov.b32 	%r243, %f178;
	mov.u32 	%r244, 4;
	shfl.sync.bfly.b32 	%r245|%p67, %r243, %r244, %r236, %r238;
	mov.b32 	%f179, %r245;
	add.f32 	%f180, %f178, %f179;
	mov.b32 	%r246, %f180;
	mov.u32 	%r247, 2;
	shfl.sync.bfly.b32 	%r248|%p68, %r246, %r247, %r236, %r238;
	mov.b32 	%f181, %r248;
	add.f32 	%f182, %f180, %f181;
	mov.b32 	%r249, %f182;
	mov.u32 	%r250, 1;
	shfl.sync.bfly.b32 	%r251|%p69, %r249, %r250, %r236, %r238;
	mov.b32 	%f183, %r251;
	add.f32 	%f184, %f182, %f183;
	st.local.f32 	[%rd1+20], %f184;

$L__BB118_18:
	bar.sync 	0;
	mov.b32 	%r252, %f214;
	mov.u32 	%r253, 31;
	mov.u32 	%r254, 16;
	mov.u32 	%r255, -1;
	shfl.sync.bfly.b32 	%r256|%p71, %r252, %r254, %r253, %r255;
	mov.b32 	%f185, %r256;
	add.f32 	%f186, %f214, %f185;
	mov.b32 	%r257, %f186;
	mov.u32 	%r258, 8;
	shfl.sync.bfly.b32 	%r259|%p72, %r257, %r258, %r253, %r255;
	mov.b32 	%f187, %r259;
	add.f32 	%f188, %f186, %f187;
	mov.b32 	%r260, %f188;
	mov.u32 	%r261, 4;
	shfl.sync.bfly.b32 	%r262|%p73, %r260, %r261, %r253, %r255;
	mov.b32 	%f189, %r262;
	add.f32 	%f190, %f188, %f189;
	mov.b32 	%r263, %f190;
	mov.u32 	%r264, 2;
	shfl.sync.bfly.b32 	%r265|%p74, %r263, %r264, %r253, %r255;
	mov.b32 	%f191, %r265;
	add.f32 	%f192, %f190, %f191;
	mov.b32 	%r266, %f192;
	mov.u32 	%r267, 1;
	shfl.sync.bfly.b32 	%r268|%p75, %r266, %r267, %r253, %r255;
	mov.b32 	%f193, %r268;
	add.f32 	%f194, %f192, %f193;
	st.local.f32 	[%rd1+24], %f194;
	st.shared.f32 	[%r7], %f194;
	bar.sync 	0;
	@%p1 bra 	$L__BB118_20;

	ld.shared.f32 	%f195, [%r3];
	mov.b32 	%r269, %f195;
	shfl.sync.bfly.b32 	%r273|%p76, %r269, %r254, %r253, %r255;
	mov.b32 	%f196, %r273;
	add.f32 	%f197, %f195, %f196;
	mov.b32 	%r274, %f197;
	shfl.sync.bfly.b32 	%r276|%p77, %r274, %r258, %r253, %r255;
	mov.b32 	%f198, %r276;
	add.f32 	%f199, %f197, %f198;
	mov.b32 	%r277, %f199;
	shfl.sync.bfly.b32 	%r279|%p78, %r277, %r261, %r253, %r255;
	mov.b32 	%f200, %r279;
	add.f32 	%f201, %f199, %f200;
	mov.b32 	%r280, %f201;
	shfl.sync.bfly.b32 	%r282|%p79, %r280, %r264, %r253, %r255;
	mov.b32 	%f202, %r282;
	add.f32 	%f203, %f201, %f202;
	mov.b32 	%r283, %f203;
	shfl.sync.bfly.b32 	%r285|%p80, %r283, %r267, %r253, %r255;
	mov.b32 	%f204, %r285;
	add.f32 	%f205, %f203, %f204;
	st.local.f32 	[%rd1+24], %f205;

$L__BB118_20:
	bar.sync 	0;
	setp.gt.s32 	%p81, %r2, 6;
	@%p81 bra 	$L__BB118_22;

	mad.lo.s32 	%r286, %r2, %r11, %r1;
	cvt.s64.s32 	%rd33, %r286;
	mul.lo.s32 	%r287, %r4, %r15;
	cvt.s64.s32 	%rd34, %r287;
	add.s64 	%rd35, %rd34, %rd33;
	mul.wide.s32 	%rd36, %r2, 4;
	add.s64 	%rd37, %rd1, %rd36;
	ld.local.f32 	%f206, [%rd37];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f206;}

	// end inline asm
	cvta.to.global.u64 	%rd38, %rd15;
	shl.b64 	%rd39, %rd35, 1;
	add.s64 	%rd40, %rd38, %rd39;
	st.global.u16 	[%rd40], %rs1;

$L__BB118_22:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_8_bs_224
.visible .entry ggml_matvec_f16_ncols_8_bs_224(
	.param .u64 ggml_matvec_f16_ncols_8_bs_224_param_0,
	.param .u64 ggml_matvec_f16_ncols_8_bs_224_param_1,
	.param .u64 ggml_matvec_f16_ncols_8_bs_224_param_2,
	.param .u32 ggml_matvec_f16_ncols_8_bs_224_param_3,
	.param .u32 ggml_matvec_f16_ncols_8_bs_224_param_4,
	.param .u32 ggml_matvec_f16_ncols_8_bs_224_param_5,
	.param .u32 ggml_matvec_f16_ncols_8_bs_224_param_6,
	.param .u32 ggml_matvec_f16_ncols_8_bs_224_param_7,
	.param .u32 ggml_matvec_f16_ncols_8_bs_224_param_8,
	.param .u32 ggml_matvec_f16_ncols_8_bs_224_param_9,
	.param .u32 ggml_matvec_f16_ncols_8_bs_224_param_10,
	.param .u32 ggml_matvec_f16_ncols_8_bs_224_param_11
)
{
	.local .align 16 .b8 	__local_depot119[32];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<93>;
	.reg .b16 	%rs<2>;
	.reg .f32 	%f<252>;
	.reg .b32 	%r<324>;
	.reg .b64 	%rd<45>;


	mov.u64 	%SPL, __local_depot119;
	ld.param.u64 	%rd14, [ggml_matvec_f16_ncols_8_bs_224_param_0];
	ld.param.u64 	%rd15, [ggml_matvec_f16_ncols_8_bs_224_param_1];
	ld.param.u64 	%rd16, [ggml_matvec_f16_ncols_8_bs_224_param_2];
	ld.param.u32 	%r8, [ggml_matvec_f16_ncols_8_bs_224_param_3];
	ld.param.u32 	%r9, [ggml_matvec_f16_ncols_8_bs_224_param_5];
	ld.param.u32 	%r10, [ggml_matvec_f16_ncols_8_bs_224_param_6];
	ld.param.u32 	%r11, [ggml_matvec_f16_ncols_8_bs_224_param_7];
	ld.param.u32 	%r12, [ggml_matvec_f16_ncols_8_bs_224_param_8];
	ld.param.u32 	%r13, [ggml_matvec_f16_ncols_8_bs_224_param_9];
	ld.param.u32 	%r14, [ggml_matvec_f16_ncols_8_bs_224_param_10];
	ld.param.u32 	%r15, [ggml_matvec_f16_ncols_8_bs_224_param_11];
	add.u64 	%rd1, %SPL, 0;
	mov.u32 	%r1, %ctaid.x;
	mov.u32 	%r2, %tid.x;
	setp.gt.s32 	%p1, %r2, 31;
	shl.b32 	%r16, %r2, 2;
	mov.u32 	%r17, data_mmv;
	add.s32 	%r3, %r17, %r16;
	@%p1 bra 	$L__BB119_2;

	mov.u32 	%r18, 0;
	st.shared.u32 	[%r3], %r18;

$L__BB119_2:
	mov.u32 	%r4, %ctaid.y;
	bar.sync 	0;
	mov.f32 	%f244, 0f00000000;
	st.local.v4.f32 	[%rd1], {%f244, %f244, %f244, %f244};
	st.local.v4.f32 	[%rd1+16], {%f244, %f244, %f244, %f244};
	setp.ge.s32 	%p2, %r2, %r8;
	mov.f32 	%f245, %f244;
	mov.f32 	%f246, %f244;
	mov.f32 	%f247, %f244;
	mov.f32 	%f248, %f244;
	mov.f32 	%f249, %f244;
	mov.f32 	%f250, %f244;
	mov.f32 	%f251, %f244;
	@%p2 bra 	$L__BB119_6;

	shl.b32 	%r19, %r10, 1;
	add.s32 	%r20, %r2, %r19;
	mul.wide.s32 	%rd18, %r20, 4;
	mul.lo.s32 	%r21, %r4, %r14;
	mul.wide.s32 	%rd19, %r21, 2;
	add.s64 	%rd4, %rd18, %rd19;
	mul.wide.s32 	%rd20, %r2, 4;
	mul.wide.s32 	%rd5, %r10, 4;
	add.s64 	%rd21, %rd20, %rd5;
	add.s64 	%rd6, %rd21, %rd19;
	add.s64 	%rd7, %rd20, %rd19;
	mul.wide.s32 	%rd22, %r2, 2;
	div.s32 	%r22, %r4, %r12;
	mul.lo.s32 	%r23, %r1, %r9;
	mad.lo.s32 	%r24, %r22, %r13, %r23;
	cvt.s64.s32 	%rd23, %r24;
	add.s64 	%rd24, %rd22, %rd23;
	cvta.to.global.u64 	%rd25, %rd14;
	shl.b64 	%rd26, %rd24, 1;
	add.s64 	%rd43, %rd25, %rd26;
	cvta.to.global.u64 	%rd44, %rd15;
	mov.f32 	%f244, 0f00000000;
	mov.u32 	%r323, %r2;

$L__BB119_4:
	ld.global.nc.u32 	%r25, [%rd43];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r25;
  cvt.f32.f16 %f41, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r25;
  cvt.f32.f16 %f42, high;}

	// end inline asm
	add.s64 	%rd27, %rd44, %rd7;
	ld.global.nc.u32 	%r27, [%rd27];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r27;
  cvt.f32.f16 %f43, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r27;
  cvt.f32.f16 %f44, high;}

	// end inline asm
	fma.rn.f32 	%f59, %f41, %f43, %f251;
	fma.rn.f32 	%f251, %f42, %f44, %f59;
	add.s64 	%rd28, %rd44, %rd6;
	ld.global.nc.u32 	%r29, [%rd28];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r29;
  cvt.f32.f16 %f45, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r29;
  cvt.f32.f16 %f46, high;}

	// end inline asm
	fma.rn.f32 	%f60, %f41, %f45, %f250;
	fma.rn.f32 	%f250, %f42, %f46, %f60;
	add.s64 	%rd29, %rd44, %rd4;
	ld.global.nc.u32 	%r31, [%rd29];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r31;
  cvt.f32.f16 %f47, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r31;
  cvt.f32.f16 %f48, high;}

	// end inline asm
	fma.rn.f32 	%f61, %f41, %f47, %f249;
	fma.rn.f32 	%f249, %f42, %f48, %f61;
	add.s64 	%rd30, %rd29, %rd5;
	ld.global.nc.u32 	%r33, [%rd30];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r33;
  cvt.f32.f16 %f49, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r33;
  cvt.f32.f16 %f50, high;}

	// end inline asm
	fma.rn.f32 	%f62, %f41, %f49, %f248;
	fma.rn.f32 	%f248, %f42, %f50, %f62;
	add.s64 	%rd31, %rd30, %rd5;
	ld.global.nc.u32 	%r35, [%rd31];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r35;
  cvt.f32.f16 %f51, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r35;
  cvt.f32.f16 %f52, high;}

	// end inline asm
	fma.rn.f32 	%f63, %f41, %f51, %f247;
	fma.rn.f32 	%f247, %f42, %f52, %f63;
	add.s64 	%rd32, %rd31, %rd5;
	ld.global.nc.u32 	%r37, [%rd32];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r37;
  cvt.f32.f16 %f53, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r37;
  cvt.f32.f16 %f54, high;}

	// end inline asm
	fma.rn.f32 	%f64, %f41, %f53, %f246;
	fma.rn.f32 	%f246, %f42, %f54, %f64;
	add.s64 	%rd33, %rd32, %rd5;
	ld.global.nc.u32 	%r39, [%rd33];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r39;
  cvt.f32.f16 %f55, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r39;
  cvt.f32.f16 %f56, high;}

	// end inline asm
	fma.rn.f32 	%f65, %f41, %f55, %f245;
	fma.rn.f32 	%f245, %f42, %f56, %f65;
	add.s64 	%rd34, %rd33, %rd5;
	ld.global.nc.u32 	%r41, [%rd34];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r41;
  cvt.f32.f16 %f57, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r41;
  cvt.f32.f16 %f58, high;}

	// end inline asm
	fma.rn.f32 	%f66, %f41, %f57, %f244;
	fma.rn.f32 	%f244, %f42, %f58, %f66;
	add.s64 	%rd44, %rd44, 896;
	add.s64 	%rd43, %rd43, 896;
	add.s32 	%r323, %r323, 224;
	setp.lt.s32 	%p3, %r323, %r8;
	@%p3 bra 	$L__BB119_4;

	st.local.v4.f32 	[%rd1], {%f251, %f250, %f249, %f248};
	st.local.v4.f32 	[%rd1+16], {%f247, %f246, %f245, %f244};

$L__BB119_6:
	shr.s32 	%r43, %r2, 31;
	shr.u32 	%r44, %r43, 27;
	add.s32 	%r45, %r2, %r44;
	shr.s32 	%r46, %r45, 5;
	shl.b32 	%r47, %r46, 2;
	add.s32 	%r7, %r17, %r47;
	mov.u32 	%r49, 2;
	mov.b32 	%r50, %f251;
	mov.u32 	%r51, 31;
	mov.u32 	%r52, 16;
	mov.u32 	%r53, -1;
	shfl.sync.bfly.b32 	%r54|%p4, %r50, %r52, %r51, %r53;
	mov.b32 	%f67, %r54;
	add.f32 	%f68, %f251, %f67;
	mov.b32 	%r55, %f68;
	mov.u32 	%r56, 8;
	shfl.sync.bfly.b32 	%r57|%p5, %r55, %r56, %r51, %r53;
	mov.b32 	%f69, %r57;
	add.f32 	%f70, %f68, %f69;
	mov.b32 	%r58, %f70;
	mov.u32 	%r59, 4;
	shfl.sync.bfly.b32 	%r60|%p6, %r58, %r59, %r51, %r53;
	mov.b32 	%f71, %r60;
	add.f32 	%f72, %f70, %f71;
	mov.b32 	%r61, %f72;
	shfl.sync.bfly.b32 	%r62|%p7, %r61, %r49, %r51, %r53;
	mov.b32 	%f73, %r62;
	add.f32 	%f74, %f72, %f73;
	mov.b32 	%r63, %f74;
	mov.u32 	%r64, 1;
	shfl.sync.bfly.b32 	%r65|%p8, %r63, %r64, %r51, %r53;
	mov.b32 	%f75, %r65;
	add.f32 	%f76, %f74, %f75;
	st.local.f32 	[%rd1], %f76;
	st.shared.f32 	[%r7], %f76;
	bar.sync 	0;
	@%p1 bra 	$L__BB119_8;

	ld.shared.f32 	%f77, [%r3];
	mov.b32 	%r66, %f77;
	shfl.sync.bfly.b32 	%r70|%p10, %r66, %r52, %r51, %r53;
	mov.b32 	%f78, %r70;
	add.f32 	%f79, %f77, %f78;
	mov.b32 	%r71, %f79;
	shfl.sync.bfly.b32 	%r73|%p11, %r71, %r56, %r51, %r53;
	mov.b32 	%f80, %r73;
	add.f32 	%f81, %f79, %f80;
	mov.b32 	%r74, %f81;
	shfl.sync.bfly.b32 	%r76|%p12, %r74, %r59, %r51, %r53;
	mov.b32 	%f82, %r76;
	add.f32 	%f83, %f81, %f82;
	mov.b32 	%r77, %f83;
	shfl.sync.bfly.b32 	%r79|%p13, %r77, %r49, %r51, %r53;
	mov.b32 	%f84, %r79;
	add.f32 	%f85, %f83, %f84;
	mov.b32 	%r80, %f85;
	shfl.sync.bfly.b32 	%r82|%p14, %r80, %r64, %r51, %r53;
	mov.b32 	%f86, %r82;
	add.f32 	%f87, %f85, %f86;
	st.local.f32 	[%rd1], %f87;

$L__BB119_8:
	bar.sync 	0;
	mov.b32 	%r83, %f250;
	shfl.sync.bfly.b32 	%r87|%p16, %r83, %r52, %r51, %r53;
	mov.b32 	%f88, %r87;
	add.f32 	%f89, %f250, %f88;
	mov.b32 	%r88, %f89;
	shfl.sync.bfly.b32 	%r90|%p17, %r88, %r56, %r51, %r53;
	mov.b32 	%f90, %r90;
	add.f32 	%f91, %f89, %f90;
	mov.b32 	%r91, %f91;
	shfl.sync.bfly.b32 	%r93|%p18, %r91, %r59, %r51, %r53;
	mov.b32 	%f92, %r93;
	add.f32 	%f93, %f91, %f92;
	mov.b32 	%r94, %f93;
	shfl.sync.bfly.b32 	%r96|%p19, %r94, %r49, %r51, %r53;
	mov.b32 	%f94, %r96;
	add.f32 	%f95, %f93, %f94;
	mov.b32 	%r97, %f95;
	shfl.sync.bfly.b32 	%r99|%p20, %r97, %r64, %r51, %r53;
	mov.b32 	%f96, %r99;
	add.f32 	%f97, %f95, %f96;
	st.local.f32 	[%rd1+4], %f97;
	st.shared.f32 	[%r7], %f97;
	bar.sync 	0;
	@%p1 bra 	$L__BB119_10;

	ld.shared.f32 	%f98, [%r3];
	mov.b32 	%r100, %f98;
	mov.u32 	%r101, 31;
	mov.u32 	%r102, 16;
	mov.u32 	%r103, -1;
	shfl.sync.bfly.b32 	%r104|%p21, %r100, %r102, %r101, %r103;
	mov.b32 	%f99, %r104;
	add.f32 	%f100, %f98, %f99;
	mov.b32 	%r105, %f100;
	mov.u32 	%r106, 8;
	shfl.sync.bfly.b32 	%r107|%p22, %r105, %r106, %r101, %r103;
	mov.b32 	%f101, %r107;
	add.f32 	%f102, %f100, %f101;
	mov.b32 	%r108, %f102;
	mov.u32 	%r109, 4;
	shfl.sync.bfly.b32 	%r110|%p23, %r108, %r109, %r101, %r103;
	mov.b32 	%f103, %r110;
	add.f32 	%f104, %f102, %f103;
	mov.b32 	%r111, %f104;
	mov.u32 	%r112, 2;
	shfl.sync.bfly.b32 	%r113|%p24, %r111, %r112, %r101, %r103;
	mov.b32 	%f105, %r113;
	add.f32 	%f106, %f104, %f105;
	mov.b32 	%r114, %f106;
	mov.u32 	%r115, 1;
	shfl.sync.bfly.b32 	%r116|%p25, %r114, %r115, %r101, %r103;
	mov.b32 	%f107, %r116;
	add.f32 	%f108, %f106, %f107;
	st.local.f32 	[%rd1+4], %f108;

$L__BB119_10:
	bar.sync 	0;
	mov.b32 	%r117, %f249;
	mov.u32 	%r118, 31;
	mov.u32 	%r119, 16;
	mov.u32 	%r120, -1;
	shfl.sync.bfly.b32 	%r121|%p27, %r117, %r119, %r118, %r120;
	mov.b32 	%f109, %r121;
	add.f32 	%f110, %f249, %f109;
	mov.b32 	%r122, %f110;
	mov.u32 	%r123, 8;
	shfl.sync.bfly.b32 	%r124|%p28, %r122, %r123, %r118, %r120;
	mov.b32 	%f111, %r124;
	add.f32 	%f112, %f110, %f111;
	mov.b32 	%r125, %f112;
	mov.u32 	%r126, 4;
	shfl.sync.bfly.b32 	%r127|%p29, %r125, %r126, %r118, %r120;
	mov.b32 	%f113, %r127;
	add.f32 	%f114, %f112, %f113;
	mov.b32 	%r128, %f114;
	mov.u32 	%r129, 2;
	shfl.sync.bfly.b32 	%r130|%p30, %r128, %r129, %r118, %r120;
	mov.b32 	%f115, %r130;
	add.f32 	%f116, %f114, %f115;
	mov.b32 	%r131, %f116;
	mov.u32 	%r132, 1;
	shfl.sync.bfly.b32 	%r133|%p31, %r131, %r132, %r118, %r120;
	mov.b32 	%f117, %r133;
	add.f32 	%f118, %f116, %f117;
	st.local.f32 	[%rd1+8], %f118;
	st.shared.f32 	[%r7], %f118;
	bar.sync 	0;
	@%p1 bra 	$L__BB119_12;

	ld.shared.f32 	%f119, [%r3];
	mov.b32 	%r134, %f119;
	shfl.sync.bfly.b32 	%r138|%p32, %r134, %r119, %r118, %r120;
	mov.b32 	%f120, %r138;
	add.f32 	%f121, %f119, %f120;
	mov.b32 	%r139, %f121;
	shfl.sync.bfly.b32 	%r141|%p33, %r139, %r123, %r118, %r120;
	mov.b32 	%f122, %r141;
	add.f32 	%f123, %f121, %f122;
	mov.b32 	%r142, %f123;
	shfl.sync.bfly.b32 	%r144|%p34, %r142, %r126, %r118, %r120;
	mov.b32 	%f124, %r144;
	add.f32 	%f125, %f123, %f124;
	mov.b32 	%r145, %f125;
	shfl.sync.bfly.b32 	%r147|%p35, %r145, %r129, %r118, %r120;
	mov.b32 	%f126, %r147;
	add.f32 	%f127, %f125, %f126;
	mov.b32 	%r148, %f127;
	shfl.sync.bfly.b32 	%r150|%p36, %r148, %r132, %r118, %r120;
	mov.b32 	%f128, %r150;
	add.f32 	%f129, %f127, %f128;
	st.local.f32 	[%rd1+8], %f129;

$L__BB119_12:
	bar.sync 	0;
	mov.b32 	%r151, %f248;
	shfl.sync.bfly.b32 	%r155|%p38, %r151, %r119, %r118, %r120;
	mov.b32 	%f130, %r155;
	add.f32 	%f131, %f248, %f130;
	mov.b32 	%r156, %f131;
	shfl.sync.bfly.b32 	%r158|%p39, %r156, %r123, %r118, %r120;
	mov.b32 	%f132, %r158;
	add.f32 	%f133, %f131, %f132;
	mov.b32 	%r159, %f133;
	shfl.sync.bfly.b32 	%r161|%p40, %r159, %r126, %r118, %r120;
	mov.b32 	%f134, %r161;
	add.f32 	%f135, %f133, %f134;
	mov.b32 	%r162, %f135;
	shfl.sync.bfly.b32 	%r164|%p41, %r162, %r129, %r118, %r120;
	mov.b32 	%f136, %r164;
	add.f32 	%f137, %f135, %f136;
	mov.b32 	%r165, %f137;
	shfl.sync.bfly.b32 	%r167|%p42, %r165, %r132, %r118, %r120;
	mov.b32 	%f138, %r167;
	add.f32 	%f139, %f137, %f138;
	st.local.f32 	[%rd1+12], %f139;
	st.shared.f32 	[%r7], %f139;
	bar.sync 	0;
	@%p1 bra 	$L__BB119_14;

	ld.shared.f32 	%f140, [%r3];
	mov.b32 	%r168, %f140;
	mov.u32 	%r169, 31;
	mov.u32 	%r170, 16;
	mov.u32 	%r171, -1;
	shfl.sync.bfly.b32 	%r172|%p43, %r168, %r170, %r169, %r171;
	mov.b32 	%f141, %r172;
	add.f32 	%f142, %f140, %f141;
	mov.b32 	%r173, %f142;
	mov.u32 	%r174, 8;
	shfl.sync.bfly.b32 	%r175|%p44, %r173, %r174, %r169, %r171;
	mov.b32 	%f143, %r175;
	add.f32 	%f144, %f142, %f143;
	mov.b32 	%r176, %f144;
	mov.u32 	%r177, 4;
	shfl.sync.bfly.b32 	%r178|%p45, %r176, %r177, %r169, %r171;
	mov.b32 	%f145, %r178;
	add.f32 	%f146, %f144, %f145;
	mov.b32 	%r179, %f146;
	mov.u32 	%r180, 2;
	shfl.sync.bfly.b32 	%r181|%p46, %r179, %r180, %r169, %r171;
	mov.b32 	%f147, %r181;
	add.f32 	%f148, %f146, %f147;
	mov.b32 	%r182, %f148;
	mov.u32 	%r183, 1;
	shfl.sync.bfly.b32 	%r184|%p47, %r182, %r183, %r169, %r171;
	mov.b32 	%f149, %r184;
	add.f32 	%f150, %f148, %f149;
	st.local.f32 	[%rd1+12], %f150;

$L__BB119_14:
	bar.sync 	0;
	mov.b32 	%r185, %f247;
	mov.u32 	%r186, 31;
	mov.u32 	%r187, 16;
	mov.u32 	%r188, -1;
	shfl.sync.bfly.b32 	%r189|%p49, %r185, %r187, %r186, %r188;
	mov.b32 	%f151, %r189;
	add.f32 	%f152, %f247, %f151;
	mov.b32 	%r190, %f152;
	mov.u32 	%r191, 8;
	shfl.sync.bfly.b32 	%r192|%p50, %r190, %r191, %r186, %r188;
	mov.b32 	%f153, %r192;
	add.f32 	%f154, %f152, %f153;
	mov.b32 	%r193, %f154;
	mov.u32 	%r194, 4;
	shfl.sync.bfly.b32 	%r195|%p51, %r193, %r194, %r186, %r188;
	mov.b32 	%f155, %r195;
	add.f32 	%f156, %f154, %f155;
	mov.b32 	%r196, %f156;
	mov.u32 	%r197, 2;
	shfl.sync.bfly.b32 	%r198|%p52, %r196, %r197, %r186, %r188;
	mov.b32 	%f157, %r198;
	add.f32 	%f158, %f156, %f157;
	mov.b32 	%r199, %f158;
	mov.u32 	%r200, 1;
	shfl.sync.bfly.b32 	%r201|%p53, %r199, %r200, %r186, %r188;
	mov.b32 	%f159, %r201;
	add.f32 	%f160, %f158, %f159;
	st.local.f32 	[%rd1+16], %f160;
	st.shared.f32 	[%r7], %f160;
	bar.sync 	0;
	@%p1 bra 	$L__BB119_16;

	ld.shared.f32 	%f161, [%r3];
	mov.b32 	%r202, %f161;
	shfl.sync.bfly.b32 	%r206|%p54, %r202, %r187, %r186, %r188;
	mov.b32 	%f162, %r206;
	add.f32 	%f163, %f161, %f162;
	mov.b32 	%r207, %f163;
	shfl.sync.bfly.b32 	%r209|%p55, %r207, %r191, %r186, %r188;
	mov.b32 	%f164, %r209;
	add.f32 	%f165, %f163, %f164;
	mov.b32 	%r210, %f165;
	shfl.sync.bfly.b32 	%r212|%p56, %r210, %r194, %r186, %r188;
	mov.b32 	%f166, %r212;
	add.f32 	%f167, %f165, %f166;
	mov.b32 	%r213, %f167;
	shfl.sync.bfly.b32 	%r215|%p57, %r213, %r197, %r186, %r188;
	mov.b32 	%f168, %r215;
	add.f32 	%f169, %f167, %f168;
	mov.b32 	%r216, %f169;
	shfl.sync.bfly.b32 	%r218|%p58, %r216, %r200, %r186, %r188;
	mov.b32 	%f170, %r218;
	add.f32 	%f171, %f169, %f170;
	st.local.f32 	[%rd1+16], %f171;

$L__BB119_16:
	bar.sync 	0;
	mov.b32 	%r219, %f246;
	shfl.sync.bfly.b32 	%r223|%p60, %r219, %r187, %r186, %r188;
	mov.b32 	%f172, %r223;
	add.f32 	%f173, %f246, %f172;
	mov.b32 	%r224, %f173;
	shfl.sync.bfly.b32 	%r226|%p61, %r224, %r191, %r186, %r188;
	mov.b32 	%f174, %r226;
	add.f32 	%f175, %f173, %f174;
	mov.b32 	%r227, %f175;
	shfl.sync.bfly.b32 	%r229|%p62, %r227, %r194, %r186, %r188;
	mov.b32 	%f176, %r229;
	add.f32 	%f177, %f175, %f176;
	mov.b32 	%r230, %f177;
	shfl.sync.bfly.b32 	%r232|%p63, %r230, %r197, %r186, %r188;
	mov.b32 	%f178, %r232;
	add.f32 	%f179, %f177, %f178;
	mov.b32 	%r233, %f179;
	shfl.sync.bfly.b32 	%r235|%p64, %r233, %r200, %r186, %r188;
	mov.b32 	%f180, %r235;
	add.f32 	%f181, %f179, %f180;
	st.local.f32 	[%rd1+20], %f181;
	st.shared.f32 	[%r7], %f181;
	bar.sync 	0;
	@%p1 bra 	$L__BB119_18;

	ld.shared.f32 	%f182, [%r3];
	mov.b32 	%r236, %f182;
	mov.u32 	%r237, 31;
	mov.u32 	%r238, 16;
	mov.u32 	%r239, -1;
	shfl.sync.bfly.b32 	%r240|%p65, %r236, %r238, %r237, %r239;
	mov.b32 	%f183, %r240;
	add.f32 	%f184, %f182, %f183;
	mov.b32 	%r241, %f184;
	mov.u32 	%r242, 8;
	shfl.sync.bfly.b32 	%r243|%p66, %r241, %r242, %r237, %r239;
	mov.b32 	%f185, %r243;
	add.f32 	%f186, %f184, %f185;
	mov.b32 	%r244, %f186;
	mov.u32 	%r245, 4;
	shfl.sync.bfly.b32 	%r246|%p67, %r244, %r245, %r237, %r239;
	mov.b32 	%f187, %r246;
	add.f32 	%f188, %f186, %f187;
	mov.b32 	%r247, %f188;
	mov.u32 	%r248, 2;
	shfl.sync.bfly.b32 	%r249|%p68, %r247, %r248, %r237, %r239;
	mov.b32 	%f189, %r249;
	add.f32 	%f190, %f188, %f189;
	mov.b32 	%r250, %f190;
	mov.u32 	%r251, 1;
	shfl.sync.bfly.b32 	%r252|%p69, %r250, %r251, %r237, %r239;
	mov.b32 	%f191, %r252;
	add.f32 	%f192, %f190, %f191;
	st.local.f32 	[%rd1+20], %f192;

$L__BB119_18:
	bar.sync 	0;
	mov.b32 	%r253, %f245;
	mov.u32 	%r254, 31;
	mov.u32 	%r255, 16;
	mov.u32 	%r256, -1;
	shfl.sync.bfly.b32 	%r257|%p71, %r253, %r255, %r254, %r256;
	mov.b32 	%f193, %r257;
	add.f32 	%f194, %f245, %f193;
	mov.b32 	%r258, %f194;
	mov.u32 	%r259, 8;
	shfl.sync.bfly.b32 	%r260|%p72, %r258, %r259, %r254, %r256;
	mov.b32 	%f195, %r260;
	add.f32 	%f196, %f194, %f195;
	mov.b32 	%r261, %f196;
	mov.u32 	%r262, 4;
	shfl.sync.bfly.b32 	%r263|%p73, %r261, %r262, %r254, %r256;
	mov.b32 	%f197, %r263;
	add.f32 	%f198, %f196, %f197;
	mov.b32 	%r264, %f198;
	mov.u32 	%r265, 2;
	shfl.sync.bfly.b32 	%r266|%p74, %r264, %r265, %r254, %r256;
	mov.b32 	%f199, %r266;
	add.f32 	%f200, %f198, %f199;
	mov.b32 	%r267, %f200;
	mov.u32 	%r268, 1;
	shfl.sync.bfly.b32 	%r269|%p75, %r267, %r268, %r254, %r256;
	mov.b32 	%f201, %r269;
	add.f32 	%f202, %f200, %f201;
	st.local.f32 	[%rd1+24], %f202;
	st.shared.f32 	[%r7], %f202;
	bar.sync 	0;
	@%p1 bra 	$L__BB119_20;

	ld.shared.f32 	%f203, [%r3];
	mov.b32 	%r270, %f203;
	shfl.sync.bfly.b32 	%r274|%p76, %r270, %r255, %r254, %r256;
	mov.b32 	%f204, %r274;
	add.f32 	%f205, %f203, %f204;
	mov.b32 	%r275, %f205;
	shfl.sync.bfly.b32 	%r277|%p77, %r275, %r259, %r254, %r256;
	mov.b32 	%f206, %r277;
	add.f32 	%f207, %f205, %f206;
	mov.b32 	%r278, %f207;
	shfl.sync.bfly.b32 	%r280|%p78, %r278, %r262, %r254, %r256;
	mov.b32 	%f208, %r280;
	add.f32 	%f209, %f207, %f208;
	mov.b32 	%r281, %f209;
	shfl.sync.bfly.b32 	%r283|%p79, %r281, %r265, %r254, %r256;
	mov.b32 	%f210, %r283;
	add.f32 	%f211, %f209, %f210;
	mov.b32 	%r284, %f211;
	shfl.sync.bfly.b32 	%r286|%p80, %r284, %r268, %r254, %r256;
	mov.b32 	%f212, %r286;
	add.f32 	%f213, %f211, %f212;
	st.local.f32 	[%rd1+24], %f213;

$L__BB119_20:
	bar.sync 	0;
	mov.b32 	%r287, %f244;
	shfl.sync.bfly.b32 	%r291|%p82, %r287, %r255, %r254, %r256;
	mov.b32 	%f214, %r291;
	add.f32 	%f215, %f244, %f214;
	mov.b32 	%r292, %f215;
	shfl.sync.bfly.b32 	%r294|%p83, %r292, %r259, %r254, %r256;
	mov.b32 	%f216, %r294;
	add.f32 	%f217, %f215, %f216;
	mov.b32 	%r295, %f217;
	shfl.sync.bfly.b32 	%r297|%p84, %r295, %r262, %r254, %r256;
	mov.b32 	%f218, %r297;
	add.f32 	%f219, %f217, %f218;
	mov.b32 	%r298, %f219;
	shfl.sync.bfly.b32 	%r300|%p85, %r298, %r265, %r254, %r256;
	mov.b32 	%f220, %r300;
	add.f32 	%f221, %f219, %f220;
	mov.b32 	%r301, %f221;
	shfl.sync.bfly.b32 	%r303|%p86, %r301, %r268, %r254, %r256;
	mov.b32 	%f222, %r303;
	add.f32 	%f223, %f221, %f222;
	st.local.f32 	[%rd1+28], %f223;
	st.shared.f32 	[%r7], %f223;
	bar.sync 	0;
	@%p1 bra 	$L__BB119_22;

	ld.shared.f32 	%f224, [%r3];
	mov.b32 	%r304, %f224;
	mov.u32 	%r305, 31;
	mov.u32 	%r306, 16;
	mov.u32 	%r307, -1;
	shfl.sync.bfly.b32 	%r308|%p87, %r304, %r306, %r305, %r307;
	mov.b32 	%f225, %r308;
	add.f32 	%f226, %f224, %f225;
	mov.b32 	%r309, %f226;
	mov.u32 	%r310, 8;
	shfl.sync.bfly.b32 	%r311|%p88, %r309, %r310, %r305, %r307;
	mov.b32 	%f227, %r311;
	add.f32 	%f228, %f226, %f227;
	mov.b32 	%r312, %f228;
	mov.u32 	%r313, 4;
	shfl.sync.bfly.b32 	%r314|%p89, %r312, %r313, %r305, %r307;
	mov.b32 	%f229, %r314;
	add.f32 	%f230, %f228, %f229;
	mov.b32 	%r315, %f230;
	mov.u32 	%r316, 2;
	shfl.sync.bfly.b32 	%r317|%p90, %r315, %r316, %r305, %r307;
	mov.b32 	%f231, %r317;
	add.f32 	%f232, %f230, %f231;
	mov.b32 	%r318, %f232;
	mov.u32 	%r319, 1;
	shfl.sync.bfly.b32 	%r320|%p91, %r318, %r319, %r305, %r307;
	mov.b32 	%f233, %r320;
	add.f32 	%f234, %f232, %f233;
	st.local.f32 	[%rd1+28], %f234;

$L__BB119_22:
	bar.sync 	0;
	setp.gt.s32 	%p92, %r2, 7;
	@%p92 bra 	$L__BB119_24;

	mad.lo.s32 	%r321, %r2, %r11, %r1;
	cvt.s64.s32 	%rd35, %r321;
	mul.lo.s32 	%r322, %r4, %r15;
	cvt.s64.s32 	%rd36, %r322;
	add.s64 	%rd37, %rd36, %rd35;
	mul.wide.s32 	%rd38, %r2, 4;
	add.s64 	%rd39, %rd1, %rd38;
	ld.local.f32 	%f235, [%rd39];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f235;}

	// end inline asm
	cvta.to.global.u64 	%rd40, %rd16;
	shl.b64 	%rd41, %rd37, 1;
	add.s64 	%rd42, %rd40, %rd41;
	st.global.u16 	[%rd42], %rs1;

$L__BB119_24:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_1_bs_256
.visible .entry ggml_matvec_f16_ncols_1_bs_256(
	.param .u64 ggml_matvec_f16_ncols_1_bs_256_param_0,
	.param .u64 ggml_matvec_f16_ncols_1_bs_256_param_1,
	.param .u64 ggml_matvec_f16_ncols_1_bs_256_param_2,
	.param .u32 ggml_matvec_f16_ncols_1_bs_256_param_3,
	.param .u32 ggml_matvec_f16_ncols_1_bs_256_param_4,
	.param .u32 ggml_matvec_f16_ncols_1_bs_256_param_5,
	.param .u32 ggml_matvec_f16_ncols_1_bs_256_param_6,
	.param .u32 ggml_matvec_f16_ncols_1_bs_256_param_7,
	.param .u32 ggml_matvec_f16_ncols_1_bs_256_param_8,
	.param .u32 ggml_matvec_f16_ncols_1_bs_256_param_9,
	.param .u32 ggml_matvec_f16_ncols_1_bs_256_param_10,
	.param .u32 ggml_matvec_f16_ncols_1_bs_256_param_11
)
{
	.reg .pred 	%p<19>;
	.reg .b16 	%rs<2>;
	.reg .f32 	%f<69>;
	.reg .b32 	%r<99>;
	.reg .b64 	%rd<42>;


	ld.param.u64 	%rd18, [ggml_matvec_f16_ncols_1_bs_256_param_0];
	ld.param.u64 	%rd19, [ggml_matvec_f16_ncols_1_bs_256_param_1];
	ld.param.u64 	%rd17, [ggml_matvec_f16_ncols_1_bs_256_param_2];
	ld.param.u32 	%r13, [ggml_matvec_f16_ncols_1_bs_256_param_3];
	ld.param.u32 	%r16, [ggml_matvec_f16_ncols_1_bs_256_param_5];
	ld.param.u32 	%r14, [ggml_matvec_f16_ncols_1_bs_256_param_7];
	ld.param.u32 	%r17, [ggml_matvec_f16_ncols_1_bs_256_param_8];
	ld.param.u32 	%r18, [ggml_matvec_f16_ncols_1_bs_256_param_9];
	ld.param.u32 	%r19, [ggml_matvec_f16_ncols_1_bs_256_param_10];
	ld.param.u32 	%r15, [ggml_matvec_f16_ncols_1_bs_256_param_11];
	cvta.to.global.u64 	%rd1, %rd19;
	cvta.to.global.u64 	%rd2, %rd18;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r20, %r1, %r17;
	mov.u32 	%r21, %ctaid.x;
	mul.lo.s32 	%r22, %r21, %r16;
	mad.lo.s32 	%r23, %r20, %r18, %r22;
	cvt.s64.s32 	%rd3, %r23;
	mul.lo.s32 	%r24, %r1, %r19;
	cvt.s64.s32 	%rd4, %r24;
	mov.u32 	%r2, %tid.x;
	setp.gt.s32 	%p1, %r2, 31;
	shl.b32 	%r25, %r2, 2;
	mov.u32 	%r26, data_mmv;
	add.s32 	%r3, %r26, %r25;
	@%p1 bra 	$L__BB120_2;

	mov.u32 	%r27, 0;
	st.shared.u32 	[%r3], %r27;

$L__BB120_2:
	bar.sync 	0;
	setp.ge.s32 	%p2, %r2, %r13;
	mov.f32 	%f67, 0f00000000;
	@%p2 bra 	$L__BB120_9;

	not.b32 	%r28, %r2;
	add.s32 	%r4, %r28, %r13;
	shr.u32 	%r29, %r4, 8;
	add.s32 	%r30, %r29, 1;
	and.b32  	%r96, %r30, 3;
	setp.eq.s32 	%p3, %r96, 0;
	mov.f32 	%f67, 0f00000000;
	mov.u32 	%r97, %r2;
	@%p3 bra 	$L__BB120_6;

	mul.wide.s32 	%rd20, %r2, 2;
	add.s64 	%rd21, %rd20, %rd4;
	shl.b64 	%rd22, %rd21, 1;
	add.s64 	%rd39, %rd1, %rd22;
	add.s64 	%rd23, %rd20, %rd3;
	shl.b64 	%rd24, %rd23, 1;
	add.s64 	%rd38, %rd2, %rd24;
	mov.f32 	%f67, 0f00000000;
	mov.u32 	%r97, %r2;

$L__BB120_5:
	.pragma "nounroll";
	ld.global.nc.u32 	%r31, [%rd38];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r31;
  cvt.f32.f16 %f15, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r31;
  cvt.f32.f16 %f16, high;}

	// end inline asm
	ld.global.nc.u32 	%r33, [%rd39];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r33;
  cvt.f32.f16 %f17, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r33;
  cvt.f32.f16 %f18, high;}

	// end inline asm
	fma.rn.f32 	%f19, %f15, %f17, %f67;
	fma.rn.f32 	%f67, %f16, %f18, %f19;
	add.s32 	%r97, %r97, 256;
	add.s64 	%rd39, %rd39, 1024;
	add.s64 	%rd38, %rd38, 1024;
	add.s32 	%r96, %r96, -1;
	setp.ne.s32 	%p4, %r96, 0;
	@%p4 bra 	$L__BB120_5;

$L__BB120_6:
	setp.lt.u32 	%p5, %r4, 768;
	@%p5 bra 	$L__BB120_9;

	mul.wide.s32 	%rd25, %r97, 2;
	add.s64 	%rd26, %rd25, %rd3;
	shl.b64 	%rd27, %rd26, 1;
	add.s64 	%rd28, %rd2, %rd27;
	add.s64 	%rd41, %rd28, 2048;
	add.s64 	%rd29, %rd25, %rd4;
	shl.b64 	%rd30, %rd29, 1;
	add.s64 	%rd31, %rd1, %rd30;
	add.s64 	%rd40, %rd31, 2048;

$L__BB120_8:
	ld.global.nc.u32 	%r35, [%rd41+-2048];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r35;
  cvt.f32.f16 %f20, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r35;
  cvt.f32.f16 %f21, high;}

	// end inline asm
	ld.global.nc.u32 	%r37, [%rd40+-2048];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r37;
  cvt.f32.f16 %f22, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r37;
  cvt.f32.f16 %f23, high;}

	// end inline asm
	fma.rn.f32 	%f36, %f20, %f22, %f67;
	fma.rn.f32 	%f37, %f21, %f23, %f36;
	ld.global.nc.u32 	%r39, [%rd41+-1024];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r39;
  cvt.f32.f16 %f24, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r39;
  cvt.f32.f16 %f25, high;}

	// end inline asm
	ld.global.nc.u32 	%r41, [%rd40+-1024];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r41;
  cvt.f32.f16 %f26, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r41;
  cvt.f32.f16 %f27, high;}

	// end inline asm
	fma.rn.f32 	%f38, %f24, %f26, %f37;
	fma.rn.f32 	%f39, %f25, %f27, %f38;
	ld.global.nc.u32 	%r43, [%rd41];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r43;
  cvt.f32.f16 %f28, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r43;
  cvt.f32.f16 %f29, high;}

	// end inline asm
	ld.global.nc.u32 	%r45, [%rd40];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r45;
  cvt.f32.f16 %f30, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r45;
  cvt.f32.f16 %f31, high;}

	// end inline asm
	fma.rn.f32 	%f40, %f28, %f30, %f39;
	fma.rn.f32 	%f41, %f29, %f31, %f40;
	ld.global.nc.u32 	%r47, [%rd41+1024];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r47;
  cvt.f32.f16 %f32, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r47;
  cvt.f32.f16 %f33, high;}

	// end inline asm
	ld.global.nc.u32 	%r49, [%rd40+1024];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r49;
  cvt.f32.f16 %f34, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r49;
  cvt.f32.f16 %f35, high;}

	// end inline asm
	fma.rn.f32 	%f42, %f32, %f34, %f41;
	fma.rn.f32 	%f67, %f33, %f35, %f42;
	add.s64 	%rd41, %rd41, 4096;
	add.s64 	%rd40, %rd40, 4096;
	add.s32 	%r97, %r97, 1024;
	setp.lt.s32 	%p6, %r97, %r13;
	@%p6 bra 	$L__BB120_8;

$L__BB120_9:
	mov.b32 	%r51, %f67;
	mov.u32 	%r52, 31;
	mov.u32 	%r53, 16;
	mov.u32 	%r54, -1;
	shfl.sync.bfly.b32 	%r55|%p7, %r51, %r53, %r52, %r54;
	mov.b32 	%f43, %r55;
	add.f32 	%f44, %f67, %f43;
	mov.b32 	%r56, %f44;
	mov.u32 	%r57, 8;
	shfl.sync.bfly.b32 	%r58|%p8, %r56, %r57, %r52, %r54;
	mov.b32 	%f45, %r58;
	add.f32 	%f46, %f44, %f45;
	mov.b32 	%r59, %f46;
	mov.u32 	%r60, 4;
	shfl.sync.bfly.b32 	%r61|%p9, %r59, %r60, %r52, %r54;
	mov.b32 	%f47, %r61;
	add.f32 	%f48, %f46, %f47;
	mov.b32 	%r62, %f48;
	mov.u32 	%r63, 2;
	shfl.sync.bfly.b32 	%r64|%p10, %r62, %r63, %r52, %r54;
	mov.b32 	%f49, %r64;
	add.f32 	%f50, %f48, %f49;
	mov.b32 	%r65, %f50;
	mov.u32 	%r66, 1;
	shfl.sync.bfly.b32 	%r67|%p11, %r65, %r66, %r52, %r54;
	mov.b32 	%f51, %r67;
	add.f32 	%f68, %f50, %f51;
	shr.s32 	%r68, %r2, 31;
	shr.u32 	%r69, %r68, 27;
	add.s32 	%r70, %r2, %r69;
	shr.s32 	%r71, %r70, 5;
	shl.b32 	%r72, %r71, 2;
	add.s32 	%r74, %r26, %r72;
	st.shared.f32 	[%r74], %f68;
	bar.sync 	0;
	@%p1 bra 	$L__BB120_11;

	ld.shared.f32 	%f52, [%r3];
	mov.b32 	%r75, %f52;
	shfl.sync.bfly.b32 	%r79|%p13, %r75, %r53, %r52, %r54;
	mov.b32 	%f53, %r79;
	add.f32 	%f54, %f52, %f53;
	mov.b32 	%r80, %f54;
	shfl.sync.bfly.b32 	%r82|%p14, %r80, %r57, %r52, %r54;
	mov.b32 	%f55, %r82;
	add.f32 	%f56, %f54, %f55;
	mov.b32 	%r83, %f56;
	shfl.sync.bfly.b32 	%r85|%p15, %r83, %r60, %r52, %r54;
	mov.b32 	%f57, %r85;
	add.f32 	%f58, %f56, %f57;
	mov.b32 	%r86, %f58;
	shfl.sync.bfly.b32 	%r88|%p16, %r86, %r63, %r52, %r54;
	mov.b32 	%f59, %r88;
	add.f32 	%f60, %f58, %f59;
	mov.b32 	%r89, %f60;
	shfl.sync.bfly.b32 	%r91|%p17, %r89, %r66, %r52, %r54;
	mov.b32 	%f61, %r91;
	add.f32 	%f68, %f60, %f61;

$L__BB120_11:
	bar.sync 	0;
	setp.gt.s32 	%p18, %r2, 0;
	@%p18 bra 	$L__BB120_13;

	mad.lo.s32 	%r93, %r2, %r14, %r21;
	cvt.s64.s32 	%rd32, %r93;
	mul.lo.s32 	%r94, %r1, %r15;
	cvt.s64.s32 	%rd33, %r94;
	add.s64 	%rd34, %rd33, %rd32;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f68;}

	// end inline asm
	cvta.to.global.u64 	%rd35, %rd17;
	shl.b64 	%rd36, %rd34, 1;
	add.s64 	%rd37, %rd35, %rd36;
	st.global.u16 	[%rd37], %rs1;

$L__BB120_13:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_2_bs_256
.visible .entry ggml_matvec_f16_ncols_2_bs_256(
	.param .u64 ggml_matvec_f16_ncols_2_bs_256_param_0,
	.param .u64 ggml_matvec_f16_ncols_2_bs_256_param_1,
	.param .u64 ggml_matvec_f16_ncols_2_bs_256_param_2,
	.param .u32 ggml_matvec_f16_ncols_2_bs_256_param_3,
	.param .u32 ggml_matvec_f16_ncols_2_bs_256_param_4,
	.param .u32 ggml_matvec_f16_ncols_2_bs_256_param_5,
	.param .u32 ggml_matvec_f16_ncols_2_bs_256_param_6,
	.param .u32 ggml_matvec_f16_ncols_2_bs_256_param_7,
	.param .u32 ggml_matvec_f16_ncols_2_bs_256_param_8,
	.param .u32 ggml_matvec_f16_ncols_2_bs_256_param_9,
	.param .u32 ggml_matvec_f16_ncols_2_bs_256_param_10,
	.param .u32 ggml_matvec_f16_ncols_2_bs_256_param_11
)
{
	.local .align 8 .b8 	__local_depot121[8];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<30>;
	.reg .b16 	%rs<2>;
	.reg .f32 	%f<116>;
	.reg .b32 	%r<143>;
	.reg .b64 	%rd<64>;


	mov.u64 	%SPL, __local_depot121;
	ld.param.u64 	%rd27, [ggml_matvec_f16_ncols_2_bs_256_param_0];
	ld.param.u64 	%rd28, [ggml_matvec_f16_ncols_2_bs_256_param_1];
	ld.param.u64 	%rd26, [ggml_matvec_f16_ncols_2_bs_256_param_2];
	ld.param.u32 	%r15, [ggml_matvec_f16_ncols_2_bs_256_param_3];
	ld.param.u32 	%r19, [ggml_matvec_f16_ncols_2_bs_256_param_5];
	ld.param.u32 	%r16, [ggml_matvec_f16_ncols_2_bs_256_param_6];
	ld.param.u32 	%r17, [ggml_matvec_f16_ncols_2_bs_256_param_7];
	ld.param.u32 	%r20, [ggml_matvec_f16_ncols_2_bs_256_param_8];
	ld.param.u32 	%r21, [ggml_matvec_f16_ncols_2_bs_256_param_9];
	ld.param.u32 	%r22, [ggml_matvec_f16_ncols_2_bs_256_param_10];
	ld.param.u32 	%r18, [ggml_matvec_f16_ncols_2_bs_256_param_11];
	cvta.to.global.u64 	%rd1, %rd28;
	cvta.to.global.u64 	%rd2, %rd27;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r23, %r1, %r20;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r24, %r2, %r19;
	mad.lo.s32 	%r25, %r23, %r21, %r24;
	cvt.s64.s32 	%rd4, %r25;
	mul.lo.s32 	%r26, %r1, %r22;
	cvt.s64.s32 	%rd5, %r26;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r27, %r3, 2;
	mov.u32 	%r28, data_mmv;
	add.s32 	%r4, %r28, %r27;
	@%p1 bra 	$L__BB121_2;

	mov.u32 	%r29, 0;
	st.shared.u32 	[%r4], %r29;

$L__BB121_2:
	bar.sync 	0;
	mov.f32 	%f114, 0f00000000;
	st.local.v2.f32 	[%rd3], {%f114, %f114};
	setp.ge.s32 	%p2, %r3, %r15;
	mov.f32 	%f115, %f114;
	@%p2 bra 	$L__BB121_11;

	not.b32 	%r30, %r3;
	add.s32 	%r5, %r30, %r15;
	shr.u32 	%r31, %r5, 8;
	add.s32 	%r32, %r31, 1;
	and.b32  	%r140, %r32, 3;
	setp.eq.s32 	%p3, %r140, 0;
	mov.f32 	%f114, 0f00000000;
	mov.u32 	%r141, %r3;
	@%p3 bra 	$L__BB121_7;

	mul.wide.s32 	%rd30, %r16, 2;
	mul.wide.s32 	%rd31, %r3, 2;
	add.s64 	%rd32, %rd30, %rd31;
	add.s64 	%rd33, %rd32, %rd5;
	shl.b64 	%rd34, %rd33, 1;
	add.s64 	%rd60, %rd1, %rd34;
	add.s64 	%rd35, %rd31, %rd5;
	shl.b64 	%rd36, %rd35, 1;
	add.s64 	%rd59, %rd1, %rd36;
	add.s64 	%rd37, %rd31, %rd4;
	shl.b64 	%rd38, %rd37, 1;
	add.s64 	%rd58, %rd2, %rd38;
	mov.f32 	%f114, 0f00000000;
	mov.f32 	%f115, %f114;
	mov.u32 	%r141, %r3;

$L__BB121_5:
	.pragma "nounroll";
	ld.global.nc.u32 	%r33, [%rd58];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r33;
  cvt.f32.f16 %f19, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r33;
  cvt.f32.f16 %f20, high;}

	// end inline asm
	ld.global.nc.u32 	%r35, [%rd59];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r35;
  cvt.f32.f16 %f21, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r35;
  cvt.f32.f16 %f22, high;}

	// end inline asm
	fma.rn.f32 	%f25, %f19, %f21, %f115;
	fma.rn.f32 	%f115, %f20, %f22, %f25;
	ld.global.nc.u32 	%r37, [%rd60];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r37;
  cvt.f32.f16 %f23, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r37;
  cvt.f32.f16 %f24, high;}

	// end inline asm
	fma.rn.f32 	%f26, %f19, %f23, %f114;
	fma.rn.f32 	%f114, %f20, %f24, %f26;
	add.s32 	%r141, %r141, 256;
	add.s64 	%rd60, %rd60, 1024;
	add.s64 	%rd59, %rd59, 1024;
	add.s64 	%rd58, %rd58, 1024;
	add.s32 	%r140, %r140, -1;
	setp.ne.s32 	%p4, %r140, 0;
	@%p4 bra 	$L__BB121_5;

	st.local.v2.f32 	[%rd3], {%f115, %f114};

$L__BB121_7:
	setp.lt.u32 	%p5, %r5, 768;
	@%p5 bra 	$L__BB121_11;

	mul.wide.s32 	%rd39, %r141, 2;
	add.s64 	%rd40, %rd39, %rd4;
	shl.b64 	%rd41, %rd40, 1;
	add.s64 	%rd42, %rd2, %rd41;
	add.s64 	%rd63, %rd42, 2048;
	add.s64 	%rd43, %rd39, %rd5;
	shl.b64 	%rd44, %rd43, 1;
	add.s64 	%rd45, %rd1, %rd44;
	add.s64 	%rd62, %rd45, 3072;
	mul.wide.s32 	%rd46, %r16, 2;
	add.s64 	%rd47, %rd43, %rd46;
	shl.b64 	%rd48, %rd47, 1;
	add.s64 	%rd49, %rd1, %rd48;
	add.s64 	%rd61, %rd49, 2048;

$L__BB121_9:
	ld.global.nc.u32 	%r39, [%rd63+-2048];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r39;
  cvt.f32.f16 %f27, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r39;
  cvt.f32.f16 %f28, high;}

	// end inline asm
	ld.global.nc.u32 	%r41, [%rd62+-3072];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r41;
  cvt.f32.f16 %f29, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r41;
  cvt.f32.f16 %f30, high;}

	// end inline asm
	fma.rn.f32 	%f51, %f27, %f29, %f115;
	fma.rn.f32 	%f52, %f28, %f30, %f51;
	ld.global.nc.u32 	%r43, [%rd61+-2048];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r43;
  cvt.f32.f16 %f31, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r43;
  cvt.f32.f16 %f32, high;}

	// end inline asm
	fma.rn.f32 	%f53, %f27, %f31, %f114;
	fma.rn.f32 	%f54, %f28, %f32, %f53;
	ld.global.nc.u32 	%r45, [%rd63+-1024];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r45;
  cvt.f32.f16 %f33, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r45;
  cvt.f32.f16 %f34, high;}

	// end inline asm
	ld.global.nc.u32 	%r47, [%rd62+-2048];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r47;
  cvt.f32.f16 %f35, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r47;
  cvt.f32.f16 %f36, high;}

	// end inline asm
	fma.rn.f32 	%f55, %f33, %f35, %f52;
	fma.rn.f32 	%f56, %f34, %f36, %f55;
	ld.global.nc.u32 	%r49, [%rd61+-1024];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r49;
  cvt.f32.f16 %f37, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r49;
  cvt.f32.f16 %f38, high;}

	// end inline asm
	fma.rn.f32 	%f57, %f33, %f37, %f54;
	fma.rn.f32 	%f58, %f34, %f38, %f57;
	ld.global.nc.u32 	%r51, [%rd63];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r51;
  cvt.f32.f16 %f39, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r51;
  cvt.f32.f16 %f40, high;}

	// end inline asm
	ld.global.nc.u32 	%r53, [%rd62+-1024];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r53;
  cvt.f32.f16 %f41, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r53;
  cvt.f32.f16 %f42, high;}

	// end inline asm
	fma.rn.f32 	%f59, %f39, %f41, %f56;
	fma.rn.f32 	%f60, %f40, %f42, %f59;
	ld.global.nc.u32 	%r55, [%rd61];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r55;
  cvt.f32.f16 %f43, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r55;
  cvt.f32.f16 %f44, high;}

	// end inline asm
	fma.rn.f32 	%f61, %f39, %f43, %f58;
	fma.rn.f32 	%f62, %f40, %f44, %f61;
	ld.global.nc.u32 	%r57, [%rd63+1024];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r57;
  cvt.f32.f16 %f45, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r57;
  cvt.f32.f16 %f46, high;}

	// end inline asm
	ld.global.nc.u32 	%r59, [%rd62];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r59;
  cvt.f32.f16 %f47, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r59;
  cvt.f32.f16 %f48, high;}

	// end inline asm
	fma.rn.f32 	%f63, %f45, %f47, %f60;
	fma.rn.f32 	%f115, %f46, %f48, %f63;
	ld.global.nc.u32 	%r61, [%rd61+1024];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r61;
  cvt.f32.f16 %f49, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r61;
  cvt.f32.f16 %f50, high;}

	// end inline asm
	fma.rn.f32 	%f64, %f45, %f49, %f62;
	fma.rn.f32 	%f114, %f46, %f50, %f64;
	add.s64 	%rd63, %rd63, 4096;
	add.s64 	%rd62, %rd62, 4096;
	add.s64 	%rd61, %rd61, 4096;
	add.s32 	%r141, %r141, 1024;
	setp.lt.s32 	%p6, %r141, %r15;
	@%p6 bra 	$L__BB121_9;

	st.local.v2.f32 	[%rd3], {%f115, %f114};

$L__BB121_11:
	shr.s32 	%r63, %r3, 31;
	shr.u32 	%r64, %r63, 27;
	add.s32 	%r65, %r3, %r64;
	shr.s32 	%r66, %r65, 5;
	shl.b32 	%r67, %r66, 2;
	add.s32 	%r14, %r28, %r67;
	mov.u32 	%r69, 2;
	mov.b32 	%r70, %f115;
	mov.u32 	%r71, 31;
	mov.u32 	%r72, 16;
	mov.u32 	%r73, -1;
	shfl.sync.bfly.b32 	%r74|%p7, %r70, %r72, %r71, %r73;
	mov.b32 	%f65, %r74;
	add.f32 	%f66, %f115, %f65;
	mov.b32 	%r75, %f66;
	mov.u32 	%r76, 8;
	shfl.sync.bfly.b32 	%r77|%p8, %r75, %r76, %r71, %r73;
	mov.b32 	%f67, %r77;
	add.f32 	%f68, %f66, %f67;
	mov.b32 	%r78, %f68;
	mov.u32 	%r79, 4;
	shfl.sync.bfly.b32 	%r80|%p9, %r78, %r79, %r71, %r73;
	mov.b32 	%f69, %r80;
	add.f32 	%f70, %f68, %f69;
	mov.b32 	%r81, %f70;
	shfl.sync.bfly.b32 	%r82|%p10, %r81, %r69, %r71, %r73;
	mov.b32 	%f71, %r82;
	add.f32 	%f72, %f70, %f71;
	mov.b32 	%r83, %f72;
	mov.u32 	%r84, 1;
	shfl.sync.bfly.b32 	%r85|%p11, %r83, %r84, %r71, %r73;
	mov.b32 	%f73, %r85;
	add.f32 	%f74, %f72, %f73;
	st.local.f32 	[%rd3], %f74;
	st.shared.f32 	[%r14], %f74;
	bar.sync 	0;
	@%p1 bra 	$L__BB121_13;

	ld.shared.f32 	%f75, [%r4];
	mov.b32 	%r86, %f75;
	shfl.sync.bfly.b32 	%r90|%p13, %r86, %r72, %r71, %r73;
	mov.b32 	%f76, %r90;
	add.f32 	%f77, %f75, %f76;
	mov.b32 	%r91, %f77;
	shfl.sync.bfly.b32 	%r93|%p14, %r91, %r76, %r71, %r73;
	mov.b32 	%f78, %r93;
	add.f32 	%f79, %f77, %f78;
	mov.b32 	%r94, %f79;
	shfl.sync.bfly.b32 	%r96|%p15, %r94, %r79, %r71, %r73;
	mov.b32 	%f80, %r96;
	add.f32 	%f81, %f79, %f80;
	mov.b32 	%r97, %f81;
	shfl.sync.bfly.b32 	%r99|%p16, %r97, %r69, %r71, %r73;
	mov.b32 	%f82, %r99;
	add.f32 	%f83, %f81, %f82;
	mov.b32 	%r100, %f83;
	shfl.sync.bfly.b32 	%r102|%p17, %r100, %r84, %r71, %r73;
	mov.b32 	%f84, %r102;
	add.f32 	%f85, %f83, %f84;
	st.local.f32 	[%rd3], %f85;

$L__BB121_13:
	bar.sync 	0;
	mov.b32 	%r103, %f114;
	shfl.sync.bfly.b32 	%r107|%p19, %r103, %r72, %r71, %r73;
	mov.b32 	%f86, %r107;
	add.f32 	%f87, %f114, %f86;
	mov.b32 	%r108, %f87;
	shfl.sync.bfly.b32 	%r110|%p20, %r108, %r76, %r71, %r73;
	mov.b32 	%f88, %r110;
	add.f32 	%f89, %f87, %f88;
	mov.b32 	%r111, %f89;
	shfl.sync.bfly.b32 	%r113|%p21, %r111, %r79, %r71, %r73;
	mov.b32 	%f90, %r113;
	add.f32 	%f91, %f89, %f90;
	mov.b32 	%r114, %f91;
	shfl.sync.bfly.b32 	%r116|%p22, %r114, %r69, %r71, %r73;
	mov.b32 	%f92, %r116;
	add.f32 	%f93, %f91, %f92;
	mov.b32 	%r117, %f93;
	shfl.sync.bfly.b32 	%r119|%p23, %r117, %r84, %r71, %r73;
	mov.b32 	%f94, %r119;
	add.f32 	%f95, %f93, %f94;
	st.local.f32 	[%rd3+4], %f95;
	st.shared.f32 	[%r14], %f95;
	bar.sync 	0;
	@%p1 bra 	$L__BB121_15;

	ld.shared.f32 	%f96, [%r4];
	mov.b32 	%r120, %f96;
	mov.u32 	%r121, 31;
	mov.u32 	%r122, 16;
	mov.u32 	%r123, -1;
	shfl.sync.bfly.b32 	%r124|%p24, %r120, %r122, %r121, %r123;
	mov.b32 	%f97, %r124;
	add.f32 	%f98, %f96, %f97;
	mov.b32 	%r125, %f98;
	mov.u32 	%r126, 8;
	shfl.sync.bfly.b32 	%r127|%p25, %r125, %r126, %r121, %r123;
	mov.b32 	%f99, %r127;
	add.f32 	%f100, %f98, %f99;
	mov.b32 	%r128, %f100;
	mov.u32 	%r129, 4;
	shfl.sync.bfly.b32 	%r130|%p26, %r128, %r129, %r121, %r123;
	mov.b32 	%f101, %r130;
	add.f32 	%f102, %f100, %f101;
	mov.b32 	%r131, %f102;
	mov.u32 	%r132, 2;
	shfl.sync.bfly.b32 	%r133|%p27, %r131, %r132, %r121, %r123;
	mov.b32 	%f103, %r133;
	add.f32 	%f104, %f102, %f103;
	mov.b32 	%r134, %f104;
	mov.u32 	%r135, 1;
	shfl.sync.bfly.b32 	%r136|%p28, %r134, %r135, %r121, %r123;
	mov.b32 	%f105, %r136;
	add.f32 	%f106, %f104, %f105;
	st.local.f32 	[%rd3+4], %f106;

$L__BB121_15:
	bar.sync 	0;
	setp.gt.s32 	%p29, %r3, 1;
	@%p29 bra 	$L__BB121_17;

	mad.lo.s32 	%r137, %r3, %r17, %r2;
	cvt.s64.s32 	%rd50, %r137;
	mul.lo.s32 	%r138, %r1, %r18;
	cvt.s64.s32 	%rd51, %r138;
	add.s64 	%rd52, %rd51, %rd50;
	mul.wide.s32 	%rd53, %r3, 4;
	add.s64 	%rd54, %rd3, %rd53;
	ld.local.f32 	%f107, [%rd54];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f107;}

	// end inline asm
	cvta.to.global.u64 	%rd55, %rd26;
	shl.b64 	%rd56, %rd52, 1;
	add.s64 	%rd57, %rd55, %rd56;
	st.global.u16 	[%rd57], %rs1;

$L__BB121_17:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_3_bs_256
.visible .entry ggml_matvec_f16_ncols_3_bs_256(
	.param .u64 ggml_matvec_f16_ncols_3_bs_256_param_0,
	.param .u64 ggml_matvec_f16_ncols_3_bs_256_param_1,
	.param .u64 ggml_matvec_f16_ncols_3_bs_256_param_2,
	.param .u32 ggml_matvec_f16_ncols_3_bs_256_param_3,
	.param .u32 ggml_matvec_f16_ncols_3_bs_256_param_4,
	.param .u32 ggml_matvec_f16_ncols_3_bs_256_param_5,
	.param .u32 ggml_matvec_f16_ncols_3_bs_256_param_6,
	.param .u32 ggml_matvec_f16_ncols_3_bs_256_param_7,
	.param .u32 ggml_matvec_f16_ncols_3_bs_256_param_8,
	.param .u32 ggml_matvec_f16_ncols_3_bs_256_param_9,
	.param .u32 ggml_matvec_f16_ncols_3_bs_256_param_10,
	.param .u32 ggml_matvec_f16_ncols_3_bs_256_param_11
)
{
	.local .align 4 .b8 	__local_depot122[12];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<41>;
	.reg .b16 	%rs<2>;
	.reg .f32 	%f<168>;
	.reg .b32 	%r<194>;
	.reg .b64 	%rd<72>;


	mov.u64 	%SPL, __local_depot122;
	ld.param.u64 	%rd29, [ggml_matvec_f16_ncols_3_bs_256_param_0];
	ld.param.u64 	%rd30, [ggml_matvec_f16_ncols_3_bs_256_param_1];
	ld.param.u64 	%rd28, [ggml_matvec_f16_ncols_3_bs_256_param_2];
	ld.param.u32 	%r15, [ggml_matvec_f16_ncols_3_bs_256_param_3];
	ld.param.u32 	%r19, [ggml_matvec_f16_ncols_3_bs_256_param_5];
	ld.param.u32 	%r16, [ggml_matvec_f16_ncols_3_bs_256_param_6];
	ld.param.u32 	%r17, [ggml_matvec_f16_ncols_3_bs_256_param_7];
	ld.param.u32 	%r20, [ggml_matvec_f16_ncols_3_bs_256_param_8];
	ld.param.u32 	%r21, [ggml_matvec_f16_ncols_3_bs_256_param_9];
	ld.param.u32 	%r22, [ggml_matvec_f16_ncols_3_bs_256_param_10];
	ld.param.u32 	%r18, [ggml_matvec_f16_ncols_3_bs_256_param_11];
	cvta.to.global.u64 	%rd71, %rd30;
	cvta.to.global.u64 	%rd2, %rd29;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r23, %r1, %r20;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r24, %r2, %r19;
	mad.lo.s32 	%r25, %r23, %r21, %r24;
	cvt.s64.s32 	%rd4, %r25;
	mul.lo.s32 	%r26, %r1, %r22;
	cvt.s64.s32 	%rd5, %r26;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r27, %r3, 2;
	mov.u32 	%r28, data_mmv;
	add.s32 	%r4, %r28, %r27;
	@%p1 bra 	$L__BB122_2;

	mov.u32 	%r29, 0;
	st.shared.u32 	[%r4], %r29;

$L__BB122_2:
	bar.sync 	0;
	mov.f32 	%f165, 0f00000000;
	mov.u32 	%r30, 0;
	st.local.u32 	[%rd3], %r30;
	st.local.u32 	[%rd3+4], %r30;
	st.local.u32 	[%rd3+8], %r30;
	setp.ge.s32 	%p2, %r3, %r15;
	mov.f32 	%f166, %f165;
	mov.f32 	%f167, %f165;
	@%p2 bra 	$L__BB122_11;

	not.b32 	%r31, %r3;
	add.s32 	%r5, %r31, %r15;
	shr.u32 	%r32, %r5, 8;
	add.s32 	%r33, %r32, 1;
	and.b32  	%r191, %r33, 3;
	setp.eq.s32 	%p3, %r191, 0;
	mov.f32 	%f165, 0f00000000;
	mov.u32 	%r192, %r3;
	@%p3 bra 	$L__BB122_7;

	shl.b32 	%r34, %r16, 1;
	add.s32 	%r35, %r3, %r34;
	mul.wide.s32 	%rd32, %r35, 2;
	add.s64 	%rd33, %rd32, %rd5;
	shl.b64 	%rd34, %rd33, 1;
	add.s64 	%rd69, %rd71, %rd34;
	mul.wide.s32 	%rd35, %r16, 2;
	mul.wide.s32 	%rd36, %r3, 2;
	add.s64 	%rd37, %rd35, %rd36;
	add.s64 	%rd38, %rd37, %rd5;
	shl.b64 	%rd39, %rd38, 1;
	add.s64 	%rd68, %rd71, %rd39;
	add.s64 	%rd40, %rd36, %rd5;
	shl.b64 	%rd41, %rd40, 1;
	add.s64 	%rd67, %rd71, %rd41;
	add.s64 	%rd42, %rd36, %rd4;
	shl.b64 	%rd43, %rd42, 1;
	add.s64 	%rd66, %rd2, %rd43;
	mov.f32 	%f165, 0f00000000;
	mov.f32 	%f166, %f165;
	mov.f32 	%f167, %f165;
	mov.u32 	%r192, %r3;

$L__BB122_5:
	.pragma "nounroll";
	ld.global.nc.u32 	%r36, [%rd66];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r36;
  cvt.f32.f16 %f28, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r36;
  cvt.f32.f16 %f29, high;}

	// end inline asm
	ld.global.nc.u32 	%r38, [%rd67];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r38;
  cvt.f32.f16 %f30, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r38;
  cvt.f32.f16 %f31, high;}

	// end inline asm
	fma.rn.f32 	%f36, %f28, %f30, %f167;
	fma.rn.f32 	%f167, %f29, %f31, %f36;
	ld.global.nc.u32 	%r40, [%rd68];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r40;
  cvt.f32.f16 %f32, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r40;
  cvt.f32.f16 %f33, high;}

	// end inline asm
	fma.rn.f32 	%f37, %f28, %f32, %f166;
	fma.rn.f32 	%f166, %f29, %f33, %f37;
	ld.global.nc.u32 	%r42, [%rd69];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r42;
  cvt.f32.f16 %f34, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r42;
  cvt.f32.f16 %f35, high;}

	// end inline asm
	fma.rn.f32 	%f38, %f28, %f34, %f165;
	fma.rn.f32 	%f165, %f29, %f35, %f38;
	add.s32 	%r192, %r192, 256;
	add.s64 	%rd69, %rd69, 1024;
	add.s64 	%rd68, %rd68, 1024;
	add.s64 	%rd67, %rd67, 1024;
	add.s64 	%rd66, %rd66, 1024;
	add.s32 	%r191, %r191, -1;
	setp.ne.s32 	%p4, %r191, 0;
	@%p4 bra 	$L__BB122_5;

	st.local.f32 	[%rd3], %f167;
	st.local.f32 	[%rd3+4], %f166;
	st.local.f32 	[%rd3+8], %f165;

$L__BB122_7:
	setp.lt.u32 	%p5, %r5, 768;
	@%p5 bra 	$L__BB122_11;

	add.s32 	%r44, %r192, %r16;
	shl.b32 	%r45, %r16, 1;
	add.s32 	%r46, %r192, %r45;
	add.s32 	%r47, %r44, 256;
	mul.wide.s32 	%rd44, %r47, 4;
	shl.b64 	%rd45, %rd5, 1;
	add.s64 	%rd19, %rd44, %rd45;
	mul.wide.s32 	%rd46, %r46, 4;
	add.s64 	%rd20, %rd46, %rd45;
	mul.wide.s32 	%rd47, %r192, 2;
	add.s64 	%rd48, %rd47, %rd4;
	shl.b64 	%rd49, %rd48, 1;
	add.s64 	%rd50, %rd2, %rd49;
	add.s64 	%rd70, %rd50, 2048;
	mul.wide.s32 	%rd51, %r192, 4;
	add.s64 	%rd22, %rd51, %rd45;
	mul.wide.s32 	%rd52, %r16, 4;
	add.s64 	%rd53, %rd51, %rd52;
	add.s64 	%rd23, %rd53, %rd45;

$L__BB122_9:
	ld.global.nc.u32 	%r48, [%rd70+-2048];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r48;
  cvt.f32.f16 %f39, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r48;
  cvt.f32.f16 %f40, high;}

	// end inline asm
	add.s64 	%rd54, %rd71, %rd22;
	ld.global.nc.u32 	%r50, [%rd54];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r50;
  cvt.f32.f16 %f41, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r50;
  cvt.f32.f16 %f42, high;}

	// end inline asm
	fma.rn.f32 	%f71, %f39, %f41, %f167;
	fma.rn.f32 	%f72, %f40, %f42, %f71;
	add.s64 	%rd55, %rd71, %rd23;
	ld.global.nc.u32 	%r52, [%rd55];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r52;
  cvt.f32.f16 %f43, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r52;
  cvt.f32.f16 %f44, high;}

	// end inline asm
	fma.rn.f32 	%f73, %f39, %f43, %f166;
	fma.rn.f32 	%f74, %f40, %f44, %f73;
	add.s64 	%rd56, %rd71, %rd20;
	ld.global.nc.u32 	%r54, [%rd56];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r54;
  cvt.f32.f16 %f45, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r54;
  cvt.f32.f16 %f46, high;}

	// end inline asm
	fma.rn.f32 	%f75, %f39, %f45, %f165;
	fma.rn.f32 	%f76, %f40, %f46, %f75;
	ld.global.nc.u32 	%r56, [%rd70+-1024];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r56;
  cvt.f32.f16 %f47, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r56;
  cvt.f32.f16 %f48, high;}

	// end inline asm
	ld.global.nc.u32 	%r58, [%rd54+1024];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r58;
  cvt.f32.f16 %f49, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r58;
  cvt.f32.f16 %f50, high;}

	// end inline asm
	fma.rn.f32 	%f77, %f47, %f49, %f72;
	fma.rn.f32 	%f78, %f48, %f50, %f77;
	add.s64 	%rd57, %rd71, %rd19;
	ld.global.nc.u32 	%r60, [%rd57];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r60;
  cvt.f32.f16 %f51, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r60;
  cvt.f32.f16 %f52, high;}

	// end inline asm
	fma.rn.f32 	%f79, %f47, %f51, %f74;
	fma.rn.f32 	%f80, %f48, %f52, %f79;
	ld.global.nc.u32 	%r62, [%rd56+1024];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r62;
  cvt.f32.f16 %f53, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r62;
  cvt.f32.f16 %f54, high;}

	// end inline asm
	fma.rn.f32 	%f81, %f47, %f53, %f76;
	fma.rn.f32 	%f82, %f48, %f54, %f81;
	ld.global.nc.u32 	%r64, [%rd70];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r64;
  cvt.f32.f16 %f55, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r64;
  cvt.f32.f16 %f56, high;}

	// end inline asm
	ld.global.nc.u32 	%r66, [%rd54+2048];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r66;
  cvt.f32.f16 %f57, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r66;
  cvt.f32.f16 %f58, high;}

	// end inline asm
	fma.rn.f32 	%f83, %f55, %f57, %f78;
	fma.rn.f32 	%f84, %f56, %f58, %f83;
	ld.global.nc.u32 	%r68, [%rd57+1024];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r68;
  cvt.f32.f16 %f59, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r68;
  cvt.f32.f16 %f60, high;}

	// end inline asm
	fma.rn.f32 	%f85, %f55, %f59, %f80;
	fma.rn.f32 	%f86, %f56, %f60, %f85;
	ld.global.nc.u32 	%r70, [%rd56+2048];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r70;
  cvt.f32.f16 %f61, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r70;
  cvt.f32.f16 %f62, high;}

	// end inline asm
	fma.rn.f32 	%f87, %f55, %f61, %f82;
	fma.rn.f32 	%f88, %f56, %f62, %f87;
	ld.global.nc.u32 	%r72, [%rd70+1024];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r72;
  cvt.f32.f16 %f63, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r72;
  cvt.f32.f16 %f64, high;}

	// end inline asm
	ld.global.nc.u32 	%r74, [%rd54+3072];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r74;
  cvt.f32.f16 %f65, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r74;
  cvt.f32.f16 %f66, high;}

	// end inline asm
	fma.rn.f32 	%f89, %f63, %f65, %f84;
	fma.rn.f32 	%f167, %f64, %f66, %f89;
	ld.global.nc.u32 	%r76, [%rd57+2048];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r76;
  cvt.f32.f16 %f67, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r76;
  cvt.f32.f16 %f68, high;}

	// end inline asm
	fma.rn.f32 	%f90, %f63, %f67, %f86;
	fma.rn.f32 	%f166, %f64, %f68, %f90;
	ld.global.nc.u32 	%r78, [%rd56+3072];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r78;
  cvt.f32.f16 %f69, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r78;
  cvt.f32.f16 %f70, high;}

	// end inline asm
	fma.rn.f32 	%f91, %f63, %f69, %f88;
	fma.rn.f32 	%f165, %f64, %f70, %f91;
	add.s64 	%rd71, %rd71, 4096;
	add.s64 	%rd70, %rd70, 4096;
	add.s32 	%r192, %r192, 1024;
	setp.lt.s32 	%p6, %r192, %r15;
	@%p6 bra 	$L__BB122_9;

	st.local.f32 	[%rd3], %f167;
	st.local.f32 	[%rd3+4], %f166;
	st.local.f32 	[%rd3+8], %f165;

$L__BB122_11:
	shr.s32 	%r80, %r3, 31;
	shr.u32 	%r81, %r80, 27;
	add.s32 	%r82, %r3, %r81;
	shr.s32 	%r83, %r82, 5;
	shl.b32 	%r84, %r83, 2;
	add.s32 	%r14, %r28, %r84;
	mov.u32 	%r86, 2;
	mov.b32 	%r87, %f167;
	mov.u32 	%r88, 31;
	mov.u32 	%r89, 16;
	mov.u32 	%r90, -1;
	shfl.sync.bfly.b32 	%r91|%p7, %r87, %r89, %r88, %r90;
	mov.b32 	%f92, %r91;
	add.f32 	%f93, %f167, %f92;
	mov.b32 	%r92, %f93;
	mov.u32 	%r93, 8;
	shfl.sync.bfly.b32 	%r94|%p8, %r92, %r93, %r88, %r90;
	mov.b32 	%f94, %r94;
	add.f32 	%f95, %f93, %f94;
	mov.b32 	%r95, %f95;
	mov.u32 	%r96, 4;
	shfl.sync.bfly.b32 	%r97|%p9, %r95, %r96, %r88, %r90;
	mov.b32 	%f96, %r97;
	add.f32 	%f97, %f95, %f96;
	mov.b32 	%r98, %f97;
	shfl.sync.bfly.b32 	%r99|%p10, %r98, %r86, %r88, %r90;
	mov.b32 	%f98, %r99;
	add.f32 	%f99, %f97, %f98;
	mov.b32 	%r100, %f99;
	mov.u32 	%r101, 1;
	shfl.sync.bfly.b32 	%r102|%p11, %r100, %r101, %r88, %r90;
	mov.b32 	%f100, %r102;
	add.f32 	%f101, %f99, %f100;
	st.local.f32 	[%rd3], %f101;
	st.shared.f32 	[%r14], %f101;
	bar.sync 	0;
	@%p1 bra 	$L__BB122_13;

	ld.shared.f32 	%f102, [%r4];
	mov.b32 	%r103, %f102;
	shfl.sync.bfly.b32 	%r107|%p13, %r103, %r89, %r88, %r90;
	mov.b32 	%f103, %r107;
	add.f32 	%f104, %f102, %f103;
	mov.b32 	%r108, %f104;
	shfl.sync.bfly.b32 	%r110|%p14, %r108, %r93, %r88, %r90;
	mov.b32 	%f105, %r110;
	add.f32 	%f106, %f104, %f105;
	mov.b32 	%r111, %f106;
	shfl.sync.bfly.b32 	%r113|%p15, %r111, %r96, %r88, %r90;
	mov.b32 	%f107, %r113;
	add.f32 	%f108, %f106, %f107;
	mov.b32 	%r114, %f108;
	shfl.sync.bfly.b32 	%r116|%p16, %r114, %r86, %r88, %r90;
	mov.b32 	%f109, %r116;
	add.f32 	%f110, %f108, %f109;
	mov.b32 	%r117, %f110;
	shfl.sync.bfly.b32 	%r119|%p17, %r117, %r101, %r88, %r90;
	mov.b32 	%f111, %r119;
	add.f32 	%f112, %f110, %f111;
	st.local.f32 	[%rd3], %f112;

$L__BB122_13:
	bar.sync 	0;
	mov.b32 	%r120, %f166;
	shfl.sync.bfly.b32 	%r124|%p19, %r120, %r89, %r88, %r90;
	mov.b32 	%f113, %r124;
	add.f32 	%f114, %f166, %f113;
	mov.b32 	%r125, %f114;
	shfl.sync.bfly.b32 	%r127|%p20, %r125, %r93, %r88, %r90;
	mov.b32 	%f115, %r127;
	add.f32 	%f116, %f114, %f115;
	mov.b32 	%r128, %f116;
	shfl.sync.bfly.b32 	%r130|%p21, %r128, %r96, %r88, %r90;
	mov.b32 	%f117, %r130;
	add.f32 	%f118, %f116, %f117;
	mov.b32 	%r131, %f118;
	shfl.sync.bfly.b32 	%r133|%p22, %r131, %r86, %r88, %r90;
	mov.b32 	%f119, %r133;
	add.f32 	%f120, %f118, %f119;
	mov.b32 	%r134, %f120;
	shfl.sync.bfly.b32 	%r136|%p23, %r134, %r101, %r88, %r90;
	mov.b32 	%f121, %r136;
	add.f32 	%f122, %f120, %f121;
	st.local.f32 	[%rd3+4], %f122;
	st.shared.f32 	[%r14], %f122;
	bar.sync 	0;
	@%p1 bra 	$L__BB122_15;

	ld.shared.f32 	%f123, [%r4];
	mov.b32 	%r137, %f123;
	mov.u32 	%r138, 31;
	mov.u32 	%r139, 16;
	mov.u32 	%r140, -1;
	shfl.sync.bfly.b32 	%r141|%p24, %r137, %r139, %r138, %r140;
	mov.b32 	%f124, %r141;
	add.f32 	%f125, %f123, %f124;
	mov.b32 	%r142, %f125;
	mov.u32 	%r143, 8;
	shfl.sync.bfly.b32 	%r144|%p25, %r142, %r143, %r138, %r140;
	mov.b32 	%f126, %r144;
	add.f32 	%f127, %f125, %f126;
	mov.b32 	%r145, %f127;
	mov.u32 	%r146, 4;
	shfl.sync.bfly.b32 	%r147|%p26, %r145, %r146, %r138, %r140;
	mov.b32 	%f128, %r147;
	add.f32 	%f129, %f127, %f128;
	mov.b32 	%r148, %f129;
	mov.u32 	%r149, 2;
	shfl.sync.bfly.b32 	%r150|%p27, %r148, %r149, %r138, %r140;
	mov.b32 	%f130, %r150;
	add.f32 	%f131, %f129, %f130;
	mov.b32 	%r151, %f131;
	mov.u32 	%r152, 1;
	shfl.sync.bfly.b32 	%r153|%p28, %r151, %r152, %r138, %r140;
	mov.b32 	%f132, %r153;
	add.f32 	%f133, %f131, %f132;
	st.local.f32 	[%rd3+4], %f133;

$L__BB122_15:
	bar.sync 	0;
	mov.b32 	%r154, %f165;
	mov.u32 	%r155, 31;
	mov.u32 	%r156, 16;
	mov.u32 	%r157, -1;
	shfl.sync.bfly.b32 	%r158|%p30, %r154, %r156, %r155, %r157;
	mov.b32 	%f134, %r158;
	add.f32 	%f135, %f165, %f134;
	mov.b32 	%r159, %f135;
	mov.u32 	%r160, 8;
	shfl.sync.bfly.b32 	%r161|%p31, %r159, %r160, %r155, %r157;
	mov.b32 	%f136, %r161;
	add.f32 	%f137, %f135, %f136;
	mov.b32 	%r162, %f137;
	mov.u32 	%r163, 4;
	shfl.sync.bfly.b32 	%r164|%p32, %r162, %r163, %r155, %r157;
	mov.b32 	%f138, %r164;
	add.f32 	%f139, %f137, %f138;
	mov.b32 	%r165, %f139;
	mov.u32 	%r166, 2;
	shfl.sync.bfly.b32 	%r167|%p33, %r165, %r166, %r155, %r157;
	mov.b32 	%f140, %r167;
	add.f32 	%f141, %f139, %f140;
	mov.b32 	%r168, %f141;
	mov.u32 	%r169, 1;
	shfl.sync.bfly.b32 	%r170|%p34, %r168, %r169, %r155, %r157;
	mov.b32 	%f142, %r170;
	add.f32 	%f143, %f141, %f142;
	st.local.f32 	[%rd3+8], %f143;
	st.shared.f32 	[%r14], %f143;
	bar.sync 	0;
	@%p1 bra 	$L__BB122_17;

	ld.shared.f32 	%f144, [%r4];
	mov.b32 	%r171, %f144;
	shfl.sync.bfly.b32 	%r175|%p35, %r171, %r156, %r155, %r157;
	mov.b32 	%f145, %r175;
	add.f32 	%f146, %f144, %f145;
	mov.b32 	%r176, %f146;
	shfl.sync.bfly.b32 	%r178|%p36, %r176, %r160, %r155, %r157;
	mov.b32 	%f147, %r178;
	add.f32 	%f148, %f146, %f147;
	mov.b32 	%r179, %f148;
	shfl.sync.bfly.b32 	%r181|%p37, %r179, %r163, %r155, %r157;
	mov.b32 	%f149, %r181;
	add.f32 	%f150, %f148, %f149;
	mov.b32 	%r182, %f150;
	shfl.sync.bfly.b32 	%r184|%p38, %r182, %r166, %r155, %r157;
	mov.b32 	%f151, %r184;
	add.f32 	%f152, %f150, %f151;
	mov.b32 	%r185, %f152;
	shfl.sync.bfly.b32 	%r187|%p39, %r185, %r169, %r155, %r157;
	mov.b32 	%f153, %r187;
	add.f32 	%f154, %f152, %f153;
	st.local.f32 	[%rd3+8], %f154;

$L__BB122_17:
	bar.sync 	0;
	setp.gt.s32 	%p40, %r3, 2;
	@%p40 bra 	$L__BB122_19;

	mad.lo.s32 	%r188, %r3, %r17, %r2;
	cvt.s64.s32 	%rd58, %r188;
	mul.lo.s32 	%r189, %r1, %r18;
	cvt.s64.s32 	%rd59, %r189;
	add.s64 	%rd60, %rd59, %rd58;
	mul.wide.s32 	%rd61, %r3, 4;
	add.s64 	%rd62, %rd3, %rd61;
	ld.local.f32 	%f155, [%rd62];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f155;}

	// end inline asm
	cvta.to.global.u64 	%rd63, %rd28;
	shl.b64 	%rd64, %rd60, 1;
	add.s64 	%rd65, %rd63, %rd64;
	st.global.u16 	[%rd65], %rs1;

$L__BB122_19:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_4_bs_256
.visible .entry ggml_matvec_f16_ncols_4_bs_256(
	.param .u64 ggml_matvec_f16_ncols_4_bs_256_param_0,
	.param .u64 ggml_matvec_f16_ncols_4_bs_256_param_1,
	.param .u64 ggml_matvec_f16_ncols_4_bs_256_param_2,
	.param .u32 ggml_matvec_f16_ncols_4_bs_256_param_3,
	.param .u32 ggml_matvec_f16_ncols_4_bs_256_param_4,
	.param .u32 ggml_matvec_f16_ncols_4_bs_256_param_5,
	.param .u32 ggml_matvec_f16_ncols_4_bs_256_param_6,
	.param .u32 ggml_matvec_f16_ncols_4_bs_256_param_7,
	.param .u32 ggml_matvec_f16_ncols_4_bs_256_param_8,
	.param .u32 ggml_matvec_f16_ncols_4_bs_256_param_9,
	.param .u32 ggml_matvec_f16_ncols_4_bs_256_param_10,
	.param .u32 ggml_matvec_f16_ncols_4_bs_256_param_11
)
{
	.local .align 16 .b8 	__local_depot123[16];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<51>;
	.reg .b16 	%rs<2>;
	.reg .f32 	%f<172>;
	.reg .b32 	%r<213>;
	.reg .b64 	%rd<61>;


	mov.u64 	%SPL, __local_depot123;
	ld.param.u64 	%rd19, [ggml_matvec_f16_ncols_4_bs_256_param_0];
	ld.param.u64 	%rd20, [ggml_matvec_f16_ncols_4_bs_256_param_1];
	ld.param.u64 	%rd18, [ggml_matvec_f16_ncols_4_bs_256_param_2];
	ld.param.u32 	%r11, [ggml_matvec_f16_ncols_4_bs_256_param_3];
	ld.param.u32 	%r15, [ggml_matvec_f16_ncols_4_bs_256_param_5];
	ld.param.u32 	%r12, [ggml_matvec_f16_ncols_4_bs_256_param_6];
	ld.param.u32 	%r13, [ggml_matvec_f16_ncols_4_bs_256_param_7];
	ld.param.u32 	%r16, [ggml_matvec_f16_ncols_4_bs_256_param_8];
	ld.param.u32 	%r17, [ggml_matvec_f16_ncols_4_bs_256_param_9];
	ld.param.u32 	%r18, [ggml_matvec_f16_ncols_4_bs_256_param_10];
	ld.param.u32 	%r14, [ggml_matvec_f16_ncols_4_bs_256_param_11];
	cvta.to.global.u64 	%rd60, %rd20;
	add.u64 	%rd2, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r19, %r1, %r16;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r20, %r2, %r15;
	mad.lo.s32 	%r21, %r19, %r17, %r20;
	cvt.s64.s32 	%rd3, %r21;
	cvta.to.global.u64 	%rd4, %rd19;
	mul.lo.s32 	%r22, %r1, %r18;
	cvt.s64.s32 	%rd5, %r22;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r23, %r3, 2;
	mov.u32 	%r24, data_mmv;
	add.s32 	%r4, %r24, %r23;
	@%p1 bra 	$L__BB123_2;

	mov.u32 	%r25, 0;
	st.shared.u32 	[%r4], %r25;

$L__BB123_2:
	bar.sync 	0;
	mov.f32 	%f168, 0f00000000;
	st.local.v4.f32 	[%rd2], {%f168, %f168, %f168, %f168};
	setp.ge.s32 	%p2, %r3, %r11;
	mov.f32 	%f169, %f168;
	mov.f32 	%f170, %f168;
	mov.f32 	%f171, %f168;
	@%p2 bra 	$L__BB123_9;

	not.b32 	%r26, %r3;
	add.s32 	%r5, %r26, %r11;
	and.b32  	%r27, %r5, 256;
	setp.ne.s32 	%p3, %r27, 0;
	mov.f32 	%f168, 0f00000000;
	mov.u32 	%r212, %r3;
	@%p3 bra 	$L__BB123_5;

	shl.b64 	%rd22, %rd5, 1;
	add.s64 	%rd23, %rd60, %rd22;
	shl.b64 	%rd24, %rd3, 1;
	add.s64 	%rd25, %rd4, %rd24;
	mul.wide.s32 	%rd26, %r3, 4;
	add.s64 	%rd27, %rd25, %rd26;
	ld.global.nc.u32 	%r28, [%rd27];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r28;
  cvt.f32.f16 %f29, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r28;
  cvt.f32.f16 %f30, high;}

	// end inline asm
	add.s64 	%rd28, %rd23, %rd26;
	ld.global.nc.u32 	%r30, [%rd28];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r30;
  cvt.f32.f16 %f31, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r30;
  cvt.f32.f16 %f32, high;}

	// end inline asm
	fma.rn.f32 	%f39, %f29, %f31, 0f00000000;
	fma.rn.f32 	%f171, %f30, %f32, %f39;
	st.local.f32 	[%rd2], %f171;
	mul.wide.s32 	%rd29, %r12, 4;
	add.s64 	%rd30, %rd28, %rd29;
	ld.global.nc.u32 	%r32, [%rd30];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r32;
  cvt.f32.f16 %f33, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r32;
  cvt.f32.f16 %f34, high;}

	// end inline asm
	fma.rn.f32 	%f40, %f29, %f33, 0f00000000;
	fma.rn.f32 	%f170, %f30, %f34, %f40;
	st.local.f32 	[%rd2+4], %f170;
	add.s32 	%r38, %r3, %r12;
	add.s32 	%r39, %r38, %r12;
	mul.wide.s32 	%rd31, %r39, 4;
	add.s64 	%rd32, %rd23, %rd31;
	ld.global.nc.u32 	%r34, [%rd32];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r34;
  cvt.f32.f16 %f35, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r34;
  cvt.f32.f16 %f36, high;}

	// end inline asm
	fma.rn.f32 	%f41, %f29, %f35, 0f00000000;
	fma.rn.f32 	%f169, %f30, %f36, %f41;
	st.local.f32 	[%rd2+8], %f169;
	add.s32 	%r40, %r39, %r12;
	mul.wide.s32 	%rd33, %r40, 4;
	add.s64 	%rd34, %rd23, %rd33;
	ld.global.nc.u32 	%r36, [%rd34];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r36;
  cvt.f32.f16 %f37, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r36;
  cvt.f32.f16 %f38, high;}

	// end inline asm
	fma.rn.f32 	%f42, %f29, %f37, 0f00000000;
	fma.rn.f32 	%f168, %f30, %f38, %f42;
	st.local.f32 	[%rd2+12], %f168;
	add.s32 	%r212, %r3, 256;

$L__BB123_5:
	and.b32  	%r41, %r5, -256;
	setp.eq.s32 	%p4, %r41, 0;
	@%p4 bra 	$L__BB123_9;

	add.s32 	%r42, %r212, %r12;
	add.s32 	%r43, %r42, 256;
	mul.wide.s32 	%rd35, %r43, 4;
	shl.b64 	%rd36, %rd5, 1;
	add.s64 	%rd8, %rd35, %rd36;
	shl.b32 	%r44, %r12, 1;
	add.s32 	%r45, %r212, %r44;
	mad.lo.s32 	%r46, %r12, 3, %r212;
	mul.wide.s32 	%rd37, %r45, 4;
	add.s64 	%rd9, %rd37, %rd36;
	mul.wide.s32 	%rd38, %r46, 4;
	add.s64 	%rd10, %rd38, %rd36;
	mul.wide.s32 	%rd39, %r212, 2;
	add.s64 	%rd40, %rd39, %rd3;
	shl.b64 	%rd41, %rd40, 1;
	add.s64 	%rd42, %rd4, %rd41;
	add.s64 	%rd59, %rd42, 1024;
	mul.wide.s32 	%rd43, %r212, 4;
	mul.wide.s32 	%rd44, %r12, 4;
	add.s64 	%rd45, %rd43, %rd44;
	add.s64 	%rd12, %rd45, %rd36;
	add.s64 	%rd13, %rd43, %rd36;

$L__BB123_7:
	ld.global.nc.u32 	%r47, [%rd59+-1024];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r47;
  cvt.f32.f16 %f43, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r47;
  cvt.f32.f16 %f44, high;}

	// end inline asm
	add.s64 	%rd46, %rd60, %rd13;
	ld.global.nc.u32 	%r49, [%rd46];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r49;
  cvt.f32.f16 %f45, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r49;
  cvt.f32.f16 %f46, high;}

	// end inline asm
	fma.rn.f32 	%f63, %f43, %f45, %f171;
	fma.rn.f32 	%f64, %f44, %f46, %f63;
	add.s64 	%rd47, %rd60, %rd12;
	ld.global.nc.u32 	%r51, [%rd47];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r51;
  cvt.f32.f16 %f47, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r51;
  cvt.f32.f16 %f48, high;}

	// end inline asm
	fma.rn.f32 	%f65, %f43, %f47, %f170;
	fma.rn.f32 	%f66, %f44, %f48, %f65;
	add.s64 	%rd48, %rd60, %rd9;
	ld.global.nc.u32 	%r53, [%rd48];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r53;
  cvt.f32.f16 %f49, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r53;
  cvt.f32.f16 %f50, high;}

	// end inline asm
	fma.rn.f32 	%f67, %f43, %f49, %f169;
	fma.rn.f32 	%f68, %f44, %f50, %f67;
	add.s64 	%rd49, %rd60, %rd10;
	ld.global.nc.u32 	%r55, [%rd49];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r55;
  cvt.f32.f16 %f51, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r55;
  cvt.f32.f16 %f52, high;}

	// end inline asm
	fma.rn.f32 	%f69, %f43, %f51, %f168;
	fma.rn.f32 	%f70, %f44, %f52, %f69;
	ld.global.nc.u32 	%r57, [%rd59];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r57;
  cvt.f32.f16 %f53, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r57;
  cvt.f32.f16 %f54, high;}

	// end inline asm
	ld.global.nc.u32 	%r59, [%rd46+1024];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r59;
  cvt.f32.f16 %f55, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r59;
  cvt.f32.f16 %f56, high;}

	// end inline asm
	fma.rn.f32 	%f71, %f53, %f55, %f64;
	fma.rn.f32 	%f171, %f54, %f56, %f71;
	add.s64 	%rd50, %rd60, %rd8;
	ld.global.nc.u32 	%r61, [%rd50];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r61;
  cvt.f32.f16 %f57, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r61;
  cvt.f32.f16 %f58, high;}

	// end inline asm
	fma.rn.f32 	%f72, %f53, %f57, %f66;
	fma.rn.f32 	%f170, %f54, %f58, %f72;
	ld.global.nc.u32 	%r63, [%rd48+1024];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r63;
  cvt.f32.f16 %f59, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r63;
  cvt.f32.f16 %f60, high;}

	// end inline asm
	fma.rn.f32 	%f73, %f53, %f59, %f68;
	fma.rn.f32 	%f169, %f54, %f60, %f73;
	ld.global.nc.u32 	%r65, [%rd49+1024];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r65;
  cvt.f32.f16 %f61, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r65;
  cvt.f32.f16 %f62, high;}

	// end inline asm
	fma.rn.f32 	%f74, %f53, %f61, %f70;
	fma.rn.f32 	%f168, %f54, %f62, %f74;
	add.s64 	%rd60, %rd60, 2048;
	add.s64 	%rd59, %rd59, 2048;
	add.s32 	%r212, %r212, 512;
	setp.lt.s32 	%p5, %r212, %r11;
	@%p5 bra 	$L__BB123_7;

	st.local.v4.f32 	[%rd2], {%f171, %f170, %f169, %f168};

$L__BB123_9:
	shr.s32 	%r67, %r3, 31;
	shr.u32 	%r68, %r67, 27;
	add.s32 	%r69, %r3, %r68;
	shr.s32 	%r70, %r69, 5;
	shl.b32 	%r71, %r70, 2;
	add.s32 	%r10, %r24, %r71;
	mov.u32 	%r73, 2;
	mov.b32 	%r74, %f171;
	mov.u32 	%r75, 31;
	mov.u32 	%r76, 16;
	mov.u32 	%r77, -1;
	shfl.sync.bfly.b32 	%r78|%p6, %r74, %r76, %r75, %r77;
	mov.b32 	%f75, %r78;
	add.f32 	%f76, %f171, %f75;
	mov.b32 	%r79, %f76;
	mov.u32 	%r80, 8;
	shfl.sync.bfly.b32 	%r81|%p7, %r79, %r80, %r75, %r77;
	mov.b32 	%f77, %r81;
	add.f32 	%f78, %f76, %f77;
	mov.b32 	%r82, %f78;
	mov.u32 	%r83, 4;
	shfl.sync.bfly.b32 	%r84|%p8, %r82, %r83, %r75, %r77;
	mov.b32 	%f79, %r84;
	add.f32 	%f80, %f78, %f79;
	mov.b32 	%r85, %f80;
	shfl.sync.bfly.b32 	%r86|%p9, %r85, %r73, %r75, %r77;
	mov.b32 	%f81, %r86;
	add.f32 	%f82, %f80, %f81;
	mov.b32 	%r87, %f82;
	mov.u32 	%r88, 1;
	shfl.sync.bfly.b32 	%r89|%p10, %r87, %r88, %r75, %r77;
	mov.b32 	%f83, %r89;
	add.f32 	%f84, %f82, %f83;
	st.local.f32 	[%rd2], %f84;
	st.shared.f32 	[%r10], %f84;
	bar.sync 	0;
	@%p1 bra 	$L__BB123_11;

	ld.shared.f32 	%f85, [%r4];
	mov.b32 	%r90, %f85;
	shfl.sync.bfly.b32 	%r94|%p12, %r90, %r76, %r75, %r77;
	mov.b32 	%f86, %r94;
	add.f32 	%f87, %f85, %f86;
	mov.b32 	%r95, %f87;
	shfl.sync.bfly.b32 	%r97|%p13, %r95, %r80, %r75, %r77;
	mov.b32 	%f88, %r97;
	add.f32 	%f89, %f87, %f88;
	mov.b32 	%r98, %f89;
	shfl.sync.bfly.b32 	%r100|%p14, %r98, %r83, %r75, %r77;
	mov.b32 	%f90, %r100;
	add.f32 	%f91, %f89, %f90;
	mov.b32 	%r101, %f91;
	shfl.sync.bfly.b32 	%r103|%p15, %r101, %r73, %r75, %r77;
	mov.b32 	%f92, %r103;
	add.f32 	%f93, %f91, %f92;
	mov.b32 	%r104, %f93;
	shfl.sync.bfly.b32 	%r106|%p16, %r104, %r88, %r75, %r77;
	mov.b32 	%f94, %r106;
	add.f32 	%f95, %f93, %f94;
	st.local.f32 	[%rd2], %f95;

$L__BB123_11:
	bar.sync 	0;
	mov.b32 	%r107, %f170;
	shfl.sync.bfly.b32 	%r111|%p18, %r107, %r76, %r75, %r77;
	mov.b32 	%f96, %r111;
	add.f32 	%f97, %f170, %f96;
	mov.b32 	%r112, %f97;
	shfl.sync.bfly.b32 	%r114|%p19, %r112, %r80, %r75, %r77;
	mov.b32 	%f98, %r114;
	add.f32 	%f99, %f97, %f98;
	mov.b32 	%r115, %f99;
	shfl.sync.bfly.b32 	%r117|%p20, %r115, %r83, %r75, %r77;
	mov.b32 	%f100, %r117;
	add.f32 	%f101, %f99, %f100;
	mov.b32 	%r118, %f101;
	shfl.sync.bfly.b32 	%r120|%p21, %r118, %r73, %r75, %r77;
	mov.b32 	%f102, %r120;
	add.f32 	%f103, %f101, %f102;
	mov.b32 	%r121, %f103;
	shfl.sync.bfly.b32 	%r123|%p22, %r121, %r88, %r75, %r77;
	mov.b32 	%f104, %r123;
	add.f32 	%f105, %f103, %f104;
	st.local.f32 	[%rd2+4], %f105;
	st.shared.f32 	[%r10], %f105;
	bar.sync 	0;
	@%p1 bra 	$L__BB123_13;

	ld.shared.f32 	%f106, [%r4];
	mov.b32 	%r124, %f106;
	mov.u32 	%r125, 31;
	mov.u32 	%r126, 16;
	mov.u32 	%r127, -1;
	shfl.sync.bfly.b32 	%r128|%p23, %r124, %r126, %r125, %r127;
	mov.b32 	%f107, %r128;
	add.f32 	%f108, %f106, %f107;
	mov.b32 	%r129, %f108;
	mov.u32 	%r130, 8;
	shfl.sync.bfly.b32 	%r131|%p24, %r129, %r130, %r125, %r127;
	mov.b32 	%f109, %r131;
	add.f32 	%f110, %f108, %f109;
	mov.b32 	%r132, %f110;
	mov.u32 	%r133, 4;
	shfl.sync.bfly.b32 	%r134|%p25, %r132, %r133, %r125, %r127;
	mov.b32 	%f111, %r134;
	add.f32 	%f112, %f110, %f111;
	mov.b32 	%r135, %f112;
	mov.u32 	%r136, 2;
	shfl.sync.bfly.b32 	%r137|%p26, %r135, %r136, %r125, %r127;
	mov.b32 	%f113, %r137;
	add.f32 	%f114, %f112, %f113;
	mov.b32 	%r138, %f114;
	mov.u32 	%r139, 1;
	shfl.sync.bfly.b32 	%r140|%p27, %r138, %r139, %r125, %r127;
	mov.b32 	%f115, %r140;
	add.f32 	%f116, %f114, %f115;
	st.local.f32 	[%rd2+4], %f116;

$L__BB123_13:
	bar.sync 	0;
	mov.b32 	%r141, %f169;
	mov.u32 	%r142, 31;
	mov.u32 	%r143, 16;
	mov.u32 	%r144, -1;
	shfl.sync.bfly.b32 	%r145|%p29, %r141, %r143, %r142, %r144;
	mov.b32 	%f117, %r145;
	add.f32 	%f118, %f169, %f117;
	mov.b32 	%r146, %f118;
	mov.u32 	%r147, 8;
	shfl.sync.bfly.b32 	%r148|%p30, %r146, %r147, %r142, %r144;
	mov.b32 	%f119, %r148;
	add.f32 	%f120, %f118, %f119;
	mov.b32 	%r149, %f120;
	mov.u32 	%r150, 4;
	shfl.sync.bfly.b32 	%r151|%p31, %r149, %r150, %r142, %r144;
	mov.b32 	%f121, %r151;
	add.f32 	%f122, %f120, %f121;
	mov.b32 	%r152, %f122;
	mov.u32 	%r153, 2;
	shfl.sync.bfly.b32 	%r154|%p32, %r152, %r153, %r142, %r144;
	mov.b32 	%f123, %r154;
	add.f32 	%f124, %f122, %f123;
	mov.b32 	%r155, %f124;
	mov.u32 	%r156, 1;
	shfl.sync.bfly.b32 	%r157|%p33, %r155, %r156, %r142, %r144;
	mov.b32 	%f125, %r157;
	add.f32 	%f126, %f124, %f125;
	st.local.f32 	[%rd2+8], %f126;
	st.shared.f32 	[%r10], %f126;
	bar.sync 	0;
	@%p1 bra 	$L__BB123_15;

	ld.shared.f32 	%f127, [%r4];
	mov.b32 	%r158, %f127;
	shfl.sync.bfly.b32 	%r162|%p34, %r158, %r143, %r142, %r144;
	mov.b32 	%f128, %r162;
	add.f32 	%f129, %f127, %f128;
	mov.b32 	%r163, %f129;
	shfl.sync.bfly.b32 	%r165|%p35, %r163, %r147, %r142, %r144;
	mov.b32 	%f130, %r165;
	add.f32 	%f131, %f129, %f130;
	mov.b32 	%r166, %f131;
	shfl.sync.bfly.b32 	%r168|%p36, %r166, %r150, %r142, %r144;
	mov.b32 	%f132, %r168;
	add.f32 	%f133, %f131, %f132;
	mov.b32 	%r169, %f133;
	shfl.sync.bfly.b32 	%r171|%p37, %r169, %r153, %r142, %r144;
	mov.b32 	%f134, %r171;
	add.f32 	%f135, %f133, %f134;
	mov.b32 	%r172, %f135;
	shfl.sync.bfly.b32 	%r174|%p38, %r172, %r156, %r142, %r144;
	mov.b32 	%f136, %r174;
	add.f32 	%f137, %f135, %f136;
	st.local.f32 	[%rd2+8], %f137;

$L__BB123_15:
	bar.sync 	0;
	mov.b32 	%r175, %f168;
	shfl.sync.bfly.b32 	%r179|%p40, %r175, %r143, %r142, %r144;
	mov.b32 	%f138, %r179;
	add.f32 	%f139, %f168, %f138;
	mov.b32 	%r180, %f139;
	shfl.sync.bfly.b32 	%r182|%p41, %r180, %r147, %r142, %r144;
	mov.b32 	%f140, %r182;
	add.f32 	%f141, %f139, %f140;
	mov.b32 	%r183, %f141;
	shfl.sync.bfly.b32 	%r185|%p42, %r183, %r150, %r142, %r144;
	mov.b32 	%f142, %r185;
	add.f32 	%f143, %f141, %f142;
	mov.b32 	%r186, %f143;
	shfl.sync.bfly.b32 	%r188|%p43, %r186, %r153, %r142, %r144;
	mov.b32 	%f144, %r188;
	add.f32 	%f145, %f143, %f144;
	mov.b32 	%r189, %f145;
	shfl.sync.bfly.b32 	%r191|%p44, %r189, %r156, %r142, %r144;
	mov.b32 	%f146, %r191;
	add.f32 	%f147, %f145, %f146;
	st.local.f32 	[%rd2+12], %f147;
	st.shared.f32 	[%r10], %f147;
	bar.sync 	0;
	@%p1 bra 	$L__BB123_17;

	ld.shared.f32 	%f148, [%r4];
	mov.b32 	%r192, %f148;
	mov.u32 	%r193, 31;
	mov.u32 	%r194, 16;
	mov.u32 	%r195, -1;
	shfl.sync.bfly.b32 	%r196|%p45, %r192, %r194, %r193, %r195;
	mov.b32 	%f149, %r196;
	add.f32 	%f150, %f148, %f149;
	mov.b32 	%r197, %f150;
	mov.u32 	%r198, 8;
	shfl.sync.bfly.b32 	%r199|%p46, %r197, %r198, %r193, %r195;
	mov.b32 	%f151, %r199;
	add.f32 	%f152, %f150, %f151;
	mov.b32 	%r200, %f152;
	mov.u32 	%r201, 4;
	shfl.sync.bfly.b32 	%r202|%p47, %r200, %r201, %r193, %r195;
	mov.b32 	%f153, %r202;
	add.f32 	%f154, %f152, %f153;
	mov.b32 	%r203, %f154;
	mov.u32 	%r204, 2;
	shfl.sync.bfly.b32 	%r205|%p48, %r203, %r204, %r193, %r195;
	mov.b32 	%f155, %r205;
	add.f32 	%f156, %f154, %f155;
	mov.b32 	%r206, %f156;
	mov.u32 	%r207, 1;
	shfl.sync.bfly.b32 	%r208|%p49, %r206, %r207, %r193, %r195;
	mov.b32 	%f157, %r208;
	add.f32 	%f158, %f156, %f157;
	st.local.f32 	[%rd2+12], %f158;

$L__BB123_17:
	bar.sync 	0;
	setp.gt.s32 	%p50, %r3, 3;
	@%p50 bra 	$L__BB123_19;

	mad.lo.s32 	%r209, %r3, %r13, %r2;
	cvt.s64.s32 	%rd51, %r209;
	mul.lo.s32 	%r210, %r1, %r14;
	cvt.s64.s32 	%rd52, %r210;
	add.s64 	%rd53, %rd52, %rd51;
	mul.wide.s32 	%rd54, %r3, 4;
	add.s64 	%rd55, %rd2, %rd54;
	ld.local.f32 	%f159, [%rd55];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f159;}

	// end inline asm
	cvta.to.global.u64 	%rd56, %rd18;
	shl.b64 	%rd57, %rd53, 1;
	add.s64 	%rd58, %rd56, %rd57;
	st.global.u16 	[%rd58], %rs1;

$L__BB123_19:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_5_bs_256
.visible .entry ggml_matvec_f16_ncols_5_bs_256(
	.param .u64 ggml_matvec_f16_ncols_5_bs_256_param_0,
	.param .u64 ggml_matvec_f16_ncols_5_bs_256_param_1,
	.param .u64 ggml_matvec_f16_ncols_5_bs_256_param_2,
	.param .u32 ggml_matvec_f16_ncols_5_bs_256_param_3,
	.param .u32 ggml_matvec_f16_ncols_5_bs_256_param_4,
	.param .u32 ggml_matvec_f16_ncols_5_bs_256_param_5,
	.param .u32 ggml_matvec_f16_ncols_5_bs_256_param_6,
	.param .u32 ggml_matvec_f16_ncols_5_bs_256_param_7,
	.param .u32 ggml_matvec_f16_ncols_5_bs_256_param_8,
	.param .u32 ggml_matvec_f16_ncols_5_bs_256_param_9,
	.param .u32 ggml_matvec_f16_ncols_5_bs_256_param_10,
	.param .u32 ggml_matvec_f16_ncols_5_bs_256_param_11
)
{
	.local .align 4 .b8 	__local_depot124[20];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<62>;
	.reg .b16 	%rs<2>;
	.reg .f32 	%f<213>;
	.reg .b32 	%r<257>;
	.reg .b64 	%rd<64>;


	mov.u64 	%SPL, __local_depot124;
	ld.param.u64 	%rd19, [ggml_matvec_f16_ncols_5_bs_256_param_0];
	ld.param.u64 	%rd20, [ggml_matvec_f16_ncols_5_bs_256_param_1];
	ld.param.u64 	%rd18, [ggml_matvec_f16_ncols_5_bs_256_param_2];
	ld.param.u32 	%r11, [ggml_matvec_f16_ncols_5_bs_256_param_3];
	ld.param.u32 	%r15, [ggml_matvec_f16_ncols_5_bs_256_param_5];
	ld.param.u32 	%r12, [ggml_matvec_f16_ncols_5_bs_256_param_6];
	ld.param.u32 	%r13, [ggml_matvec_f16_ncols_5_bs_256_param_7];
	ld.param.u32 	%r16, [ggml_matvec_f16_ncols_5_bs_256_param_8];
	ld.param.u32 	%r17, [ggml_matvec_f16_ncols_5_bs_256_param_9];
	ld.param.u32 	%r18, [ggml_matvec_f16_ncols_5_bs_256_param_10];
	ld.param.u32 	%r14, [ggml_matvec_f16_ncols_5_bs_256_param_11];
	cvta.to.global.u64 	%rd63, %rd20;
	add.u64 	%rd2, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r19, %r1, %r16;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r20, %r2, %r15;
	mad.lo.s32 	%r21, %r19, %r17, %r20;
	cvt.s64.s32 	%rd3, %r21;
	cvta.to.global.u64 	%rd4, %rd19;
	mul.lo.s32 	%r22, %r1, %r18;
	cvt.s64.s32 	%rd5, %r22;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r23, %r3, 2;
	mov.u32 	%r24, data_mmv;
	add.s32 	%r4, %r24, %r23;
	@%p1 bra 	$L__BB124_2;

	mov.u32 	%r25, 0;
	st.shared.u32 	[%r4], %r25;

$L__BB124_2:
	bar.sync 	0;
	mov.f32 	%f208, 0f00000000;
	mov.u32 	%r26, 0;
	st.local.u32 	[%rd2], %r26;
	st.local.u32 	[%rd2+4], %r26;
	st.local.u32 	[%rd2+8], %r26;
	st.local.u32 	[%rd2+12], %r26;
	st.local.u32 	[%rd2+16], %r26;
	setp.ge.s32 	%p2, %r3, %r11;
	mov.f32 	%f209, %f208;
	mov.f32 	%f210, %f208;
	mov.f32 	%f211, %f208;
	mov.f32 	%f212, %f208;
	@%p2 bra 	$L__BB124_9;

	not.b32 	%r27, %r3;
	add.s32 	%r5, %r27, %r11;
	and.b32  	%r28, %r5, 256;
	setp.ne.s32 	%p3, %r28, 0;
	mov.f32 	%f208, 0f00000000;
	mov.u32 	%r256, %r3;
	@%p3 bra 	$L__BB124_5;

	shl.b64 	%rd22, %rd5, 1;
	add.s64 	%rd23, %rd63, %rd22;
	shl.b64 	%rd24, %rd3, 1;
	add.s64 	%rd25, %rd4, %rd24;
	mul.wide.s32 	%rd26, %r3, 4;
	add.s64 	%rd27, %rd25, %rd26;
	ld.global.nc.u32 	%r29, [%rd27];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r29;
  cvt.f32.f16 %f36, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r29;
  cvt.f32.f16 %f37, high;}

	// end inline asm
	add.s64 	%rd28, %rd23, %rd26;
	ld.global.nc.u32 	%r31, [%rd28];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r31;
  cvt.f32.f16 %f38, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r31;
  cvt.f32.f16 %f39, high;}

	// end inline asm
	fma.rn.f32 	%f48, %f36, %f38, 0f00000000;
	fma.rn.f32 	%f212, %f37, %f39, %f48;
	st.local.f32 	[%rd2], %f212;
	mul.wide.s32 	%rd29, %r12, 4;
	add.s64 	%rd30, %rd28, %rd29;
	ld.global.nc.u32 	%r33, [%rd30];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r33;
  cvt.f32.f16 %f40, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r33;
  cvt.f32.f16 %f41, high;}

	// end inline asm
	fma.rn.f32 	%f49, %f36, %f40, 0f00000000;
	fma.rn.f32 	%f211, %f37, %f41, %f49;
	st.local.f32 	[%rd2+4], %f211;
	add.s32 	%r41, %r3, %r12;
	add.s32 	%r42, %r41, %r12;
	shl.b32 	%r43, %r12, 1;
	mul.wide.s32 	%rd31, %r43, 4;
	add.s64 	%rd32, %rd28, %rd31;
	ld.global.nc.u32 	%r35, [%rd32];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r35;
  cvt.f32.f16 %f42, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r35;
  cvt.f32.f16 %f43, high;}

	// end inline asm
	fma.rn.f32 	%f50, %f36, %f42, 0f00000000;
	fma.rn.f32 	%f210, %f37, %f43, %f50;
	st.local.f32 	[%rd2+8], %f210;
	add.s32 	%r44, %r42, %r12;
	mul.wide.s32 	%rd33, %r44, 4;
	add.s64 	%rd34, %rd23, %rd33;
	ld.global.nc.u32 	%r37, [%rd34];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r37;
  cvt.f32.f16 %f44, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r37;
  cvt.f32.f16 %f45, high;}

	// end inline asm
	fma.rn.f32 	%f51, %f36, %f44, 0f00000000;
	fma.rn.f32 	%f209, %f37, %f45, %f51;
	st.local.f32 	[%rd2+12], %f209;
	add.s64 	%rd35, %rd32, %rd31;
	ld.global.nc.u32 	%r39, [%rd35];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r39;
  cvt.f32.f16 %f46, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r39;
  cvt.f32.f16 %f47, high;}

	// end inline asm
	fma.rn.f32 	%f52, %f36, %f46, 0f00000000;
	fma.rn.f32 	%f208, %f37, %f47, %f52;
	st.local.f32 	[%rd2+16], %f208;
	add.s32 	%r256, %r3, 256;

$L__BB124_5:
	and.b32  	%r45, %r5, -256;
	setp.eq.s32 	%p4, %r45, 0;
	@%p4 bra 	$L__BB124_9;

	add.s32 	%r46, %r256, %r12;
	add.s32 	%r47, %r46, 256;
	mul.wide.s32 	%rd36, %r47, 4;
	shl.b64 	%rd37, %rd5, 1;
	add.s64 	%rd7, %rd36, %rd37;
	shl.b32 	%r48, %r12, 1;
	add.s32 	%r49, %r256, %r48;
	mad.lo.s32 	%r50, %r12, 3, %r256;
	shl.b32 	%r51, %r12, 2;
	add.s32 	%r52, %r256, %r51;
	mul.wide.s32 	%rd38, %r49, 4;
	add.s64 	%rd8, %rd38, %rd37;
	mul.wide.s32 	%rd39, %r50, 4;
	add.s64 	%rd9, %rd39, %rd37;
	mul.wide.s32 	%rd40, %r52, 4;
	add.s64 	%rd10, %rd40, %rd37;
	mul.wide.s32 	%rd41, %r256, 2;
	add.s64 	%rd42, %rd41, %rd3;
	shl.b64 	%rd43, %rd42, 1;
	add.s64 	%rd44, %rd4, %rd43;
	add.s64 	%rd62, %rd44, 1024;
	mul.wide.s32 	%rd45, %r256, 4;
	mul.wide.s32 	%rd46, %r12, 4;
	add.s64 	%rd47, %rd45, %rd46;
	add.s64 	%rd12, %rd47, %rd37;
	add.s64 	%rd13, %rd45, %rd37;

$L__BB124_7:
	ld.global.nc.u32 	%r53, [%rd62+-1024];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r53;
  cvt.f32.f16 %f53, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r53;
  cvt.f32.f16 %f54, high;}

	// end inline asm
	add.s64 	%rd48, %rd63, %rd13;
	ld.global.nc.u32 	%r55, [%rd48];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r55;
  cvt.f32.f16 %f55, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r55;
  cvt.f32.f16 %f56, high;}

	// end inline asm
	fma.rn.f32 	%f77, %f53, %f55, %f212;
	fma.rn.f32 	%f78, %f54, %f56, %f77;
	add.s64 	%rd49, %rd63, %rd12;
	ld.global.nc.u32 	%r57, [%rd49];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r57;
  cvt.f32.f16 %f57, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r57;
  cvt.f32.f16 %f58, high;}

	// end inline asm
	fma.rn.f32 	%f79, %f53, %f57, %f211;
	fma.rn.f32 	%f80, %f54, %f58, %f79;
	add.s64 	%rd50, %rd63, %rd8;
	ld.global.nc.u32 	%r59, [%rd50];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r59;
  cvt.f32.f16 %f59, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r59;
  cvt.f32.f16 %f60, high;}

	// end inline asm
	fma.rn.f32 	%f81, %f53, %f59, %f210;
	fma.rn.f32 	%f82, %f54, %f60, %f81;
	add.s64 	%rd51, %rd63, %rd9;
	ld.global.nc.u32 	%r61, [%rd51];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r61;
  cvt.f32.f16 %f61, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r61;
  cvt.f32.f16 %f62, high;}

	// end inline asm
	fma.rn.f32 	%f83, %f53, %f61, %f209;
	fma.rn.f32 	%f84, %f54, %f62, %f83;
	add.s64 	%rd52, %rd63, %rd10;
	ld.global.nc.u32 	%r63, [%rd52];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r63;
  cvt.f32.f16 %f63, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r63;
  cvt.f32.f16 %f64, high;}

	// end inline asm
	fma.rn.f32 	%f85, %f53, %f63, %f208;
	fma.rn.f32 	%f86, %f54, %f64, %f85;
	ld.global.nc.u32 	%r65, [%rd62];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r65;
  cvt.f32.f16 %f65, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r65;
  cvt.f32.f16 %f66, high;}

	// end inline asm
	ld.global.nc.u32 	%r67, [%rd48+1024];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r67;
  cvt.f32.f16 %f67, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r67;
  cvt.f32.f16 %f68, high;}

	// end inline asm
	fma.rn.f32 	%f87, %f65, %f67, %f78;
	fma.rn.f32 	%f212, %f66, %f68, %f87;
	add.s64 	%rd53, %rd63, %rd7;
	ld.global.nc.u32 	%r69, [%rd53];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r69;
  cvt.f32.f16 %f69, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r69;
  cvt.f32.f16 %f70, high;}

	// end inline asm
	fma.rn.f32 	%f88, %f65, %f69, %f80;
	fma.rn.f32 	%f211, %f66, %f70, %f88;
	ld.global.nc.u32 	%r71, [%rd50+1024];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r71;
  cvt.f32.f16 %f71, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r71;
  cvt.f32.f16 %f72, high;}

	// end inline asm
	fma.rn.f32 	%f89, %f65, %f71, %f82;
	fma.rn.f32 	%f210, %f66, %f72, %f89;
	ld.global.nc.u32 	%r73, [%rd51+1024];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r73;
  cvt.f32.f16 %f73, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r73;
  cvt.f32.f16 %f74, high;}

	// end inline asm
	fma.rn.f32 	%f90, %f65, %f73, %f84;
	fma.rn.f32 	%f209, %f66, %f74, %f90;
	ld.global.nc.u32 	%r75, [%rd52+1024];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r75;
  cvt.f32.f16 %f75, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r75;
  cvt.f32.f16 %f76, high;}

	// end inline asm
	fma.rn.f32 	%f91, %f65, %f75, %f86;
	fma.rn.f32 	%f208, %f66, %f76, %f91;
	add.s64 	%rd63, %rd63, 2048;
	add.s64 	%rd62, %rd62, 2048;
	add.s32 	%r256, %r256, 512;
	setp.lt.s32 	%p5, %r256, %r11;
	@%p5 bra 	$L__BB124_7;

	st.local.f32 	[%rd2], %f212;
	st.local.f32 	[%rd2+4], %f211;
	st.local.f32 	[%rd2+8], %f210;
	st.local.f32 	[%rd2+12], %f209;
	st.local.f32 	[%rd2+16], %f208;

$L__BB124_9:
	shr.s32 	%r77, %r3, 31;
	shr.u32 	%r78, %r77, 27;
	add.s32 	%r79, %r3, %r78;
	shr.s32 	%r80, %r79, 5;
	shl.b32 	%r81, %r80, 2;
	add.s32 	%r10, %r24, %r81;
	mov.u32 	%r83, 2;
	mov.b32 	%r84, %f212;
	mov.u32 	%r85, 31;
	mov.u32 	%r86, 16;
	mov.u32 	%r87, -1;
	shfl.sync.bfly.b32 	%r88|%p6, %r84, %r86, %r85, %r87;
	mov.b32 	%f92, %r88;
	add.f32 	%f93, %f212, %f92;
	mov.b32 	%r89, %f93;
	mov.u32 	%r90, 8;
	shfl.sync.bfly.b32 	%r91|%p7, %r89, %r90, %r85, %r87;
	mov.b32 	%f94, %r91;
	add.f32 	%f95, %f93, %f94;
	mov.b32 	%r92, %f95;
	mov.u32 	%r93, 4;
	shfl.sync.bfly.b32 	%r94|%p8, %r92, %r93, %r85, %r87;
	mov.b32 	%f96, %r94;
	add.f32 	%f97, %f95, %f96;
	mov.b32 	%r95, %f97;
	shfl.sync.bfly.b32 	%r96|%p9, %r95, %r83, %r85, %r87;
	mov.b32 	%f98, %r96;
	add.f32 	%f99, %f97, %f98;
	mov.b32 	%r97, %f99;
	mov.u32 	%r98, 1;
	shfl.sync.bfly.b32 	%r99|%p10, %r97, %r98, %r85, %r87;
	mov.b32 	%f100, %r99;
	add.f32 	%f101, %f99, %f100;
	st.local.f32 	[%rd2], %f101;
	st.shared.f32 	[%r10], %f101;
	bar.sync 	0;
	@%p1 bra 	$L__BB124_11;

	ld.shared.f32 	%f102, [%r4];
	mov.b32 	%r100, %f102;
	shfl.sync.bfly.b32 	%r104|%p12, %r100, %r86, %r85, %r87;
	mov.b32 	%f103, %r104;
	add.f32 	%f104, %f102, %f103;
	mov.b32 	%r105, %f104;
	shfl.sync.bfly.b32 	%r107|%p13, %r105, %r90, %r85, %r87;
	mov.b32 	%f105, %r107;
	add.f32 	%f106, %f104, %f105;
	mov.b32 	%r108, %f106;
	shfl.sync.bfly.b32 	%r110|%p14, %r108, %r93, %r85, %r87;
	mov.b32 	%f107, %r110;
	add.f32 	%f108, %f106, %f107;
	mov.b32 	%r111, %f108;
	shfl.sync.bfly.b32 	%r113|%p15, %r111, %r83, %r85, %r87;
	mov.b32 	%f109, %r113;
	add.f32 	%f110, %f108, %f109;
	mov.b32 	%r114, %f110;
	shfl.sync.bfly.b32 	%r116|%p16, %r114, %r98, %r85, %r87;
	mov.b32 	%f111, %r116;
	add.f32 	%f112, %f110, %f111;
	st.local.f32 	[%rd2], %f112;

$L__BB124_11:
	bar.sync 	0;
	mov.b32 	%r117, %f211;
	shfl.sync.bfly.b32 	%r121|%p18, %r117, %r86, %r85, %r87;
	mov.b32 	%f113, %r121;
	add.f32 	%f114, %f211, %f113;
	mov.b32 	%r122, %f114;
	shfl.sync.bfly.b32 	%r124|%p19, %r122, %r90, %r85, %r87;
	mov.b32 	%f115, %r124;
	add.f32 	%f116, %f114, %f115;
	mov.b32 	%r125, %f116;
	shfl.sync.bfly.b32 	%r127|%p20, %r125, %r93, %r85, %r87;
	mov.b32 	%f117, %r127;
	add.f32 	%f118, %f116, %f117;
	mov.b32 	%r128, %f118;
	shfl.sync.bfly.b32 	%r130|%p21, %r128, %r83, %r85, %r87;
	mov.b32 	%f119, %r130;
	add.f32 	%f120, %f118, %f119;
	mov.b32 	%r131, %f120;
	shfl.sync.bfly.b32 	%r133|%p22, %r131, %r98, %r85, %r87;
	mov.b32 	%f121, %r133;
	add.f32 	%f122, %f120, %f121;
	st.local.f32 	[%rd2+4], %f122;
	st.shared.f32 	[%r10], %f122;
	bar.sync 	0;
	@%p1 bra 	$L__BB124_13;

	ld.shared.f32 	%f123, [%r4];
	mov.b32 	%r134, %f123;
	mov.u32 	%r135, 31;
	mov.u32 	%r136, 16;
	mov.u32 	%r137, -1;
	shfl.sync.bfly.b32 	%r138|%p23, %r134, %r136, %r135, %r137;
	mov.b32 	%f124, %r138;
	add.f32 	%f125, %f123, %f124;
	mov.b32 	%r139, %f125;
	mov.u32 	%r140, 8;
	shfl.sync.bfly.b32 	%r141|%p24, %r139, %r140, %r135, %r137;
	mov.b32 	%f126, %r141;
	add.f32 	%f127, %f125, %f126;
	mov.b32 	%r142, %f127;
	mov.u32 	%r143, 4;
	shfl.sync.bfly.b32 	%r144|%p25, %r142, %r143, %r135, %r137;
	mov.b32 	%f128, %r144;
	add.f32 	%f129, %f127, %f128;
	mov.b32 	%r145, %f129;
	mov.u32 	%r146, 2;
	shfl.sync.bfly.b32 	%r147|%p26, %r145, %r146, %r135, %r137;
	mov.b32 	%f130, %r147;
	add.f32 	%f131, %f129, %f130;
	mov.b32 	%r148, %f131;
	mov.u32 	%r149, 1;
	shfl.sync.bfly.b32 	%r150|%p27, %r148, %r149, %r135, %r137;
	mov.b32 	%f132, %r150;
	add.f32 	%f133, %f131, %f132;
	st.local.f32 	[%rd2+4], %f133;

$L__BB124_13:
	bar.sync 	0;
	mov.b32 	%r151, %f210;
	mov.u32 	%r152, 31;
	mov.u32 	%r153, 16;
	mov.u32 	%r154, -1;
	shfl.sync.bfly.b32 	%r155|%p29, %r151, %r153, %r152, %r154;
	mov.b32 	%f134, %r155;
	add.f32 	%f135, %f210, %f134;
	mov.b32 	%r156, %f135;
	mov.u32 	%r157, 8;
	shfl.sync.bfly.b32 	%r158|%p30, %r156, %r157, %r152, %r154;
	mov.b32 	%f136, %r158;
	add.f32 	%f137, %f135, %f136;
	mov.b32 	%r159, %f137;
	mov.u32 	%r160, 4;
	shfl.sync.bfly.b32 	%r161|%p31, %r159, %r160, %r152, %r154;
	mov.b32 	%f138, %r161;
	add.f32 	%f139, %f137, %f138;
	mov.b32 	%r162, %f139;
	mov.u32 	%r163, 2;
	shfl.sync.bfly.b32 	%r164|%p32, %r162, %r163, %r152, %r154;
	mov.b32 	%f140, %r164;
	add.f32 	%f141, %f139, %f140;
	mov.b32 	%r165, %f141;
	mov.u32 	%r166, 1;
	shfl.sync.bfly.b32 	%r167|%p33, %r165, %r166, %r152, %r154;
	mov.b32 	%f142, %r167;
	add.f32 	%f143, %f141, %f142;
	st.local.f32 	[%rd2+8], %f143;
	st.shared.f32 	[%r10], %f143;
	bar.sync 	0;
	@%p1 bra 	$L__BB124_15;

	ld.shared.f32 	%f144, [%r4];
	mov.b32 	%r168, %f144;
	shfl.sync.bfly.b32 	%r172|%p34, %r168, %r153, %r152, %r154;
	mov.b32 	%f145, %r172;
	add.f32 	%f146, %f144, %f145;
	mov.b32 	%r173, %f146;
	shfl.sync.bfly.b32 	%r175|%p35, %r173, %r157, %r152, %r154;
	mov.b32 	%f147, %r175;
	add.f32 	%f148, %f146, %f147;
	mov.b32 	%r176, %f148;
	shfl.sync.bfly.b32 	%r178|%p36, %r176, %r160, %r152, %r154;
	mov.b32 	%f149, %r178;
	add.f32 	%f150, %f148, %f149;
	mov.b32 	%r179, %f150;
	shfl.sync.bfly.b32 	%r181|%p37, %r179, %r163, %r152, %r154;
	mov.b32 	%f151, %r181;
	add.f32 	%f152, %f150, %f151;
	mov.b32 	%r182, %f152;
	shfl.sync.bfly.b32 	%r184|%p38, %r182, %r166, %r152, %r154;
	mov.b32 	%f153, %r184;
	add.f32 	%f154, %f152, %f153;
	st.local.f32 	[%rd2+8], %f154;

$L__BB124_15:
	bar.sync 	0;
	mov.b32 	%r185, %f209;
	shfl.sync.bfly.b32 	%r189|%p40, %r185, %r153, %r152, %r154;
	mov.b32 	%f155, %r189;
	add.f32 	%f156, %f209, %f155;
	mov.b32 	%r190, %f156;
	shfl.sync.bfly.b32 	%r192|%p41, %r190, %r157, %r152, %r154;
	mov.b32 	%f157, %r192;
	add.f32 	%f158, %f156, %f157;
	mov.b32 	%r193, %f158;
	shfl.sync.bfly.b32 	%r195|%p42, %r193, %r160, %r152, %r154;
	mov.b32 	%f159, %r195;
	add.f32 	%f160, %f158, %f159;
	mov.b32 	%r196, %f160;
	shfl.sync.bfly.b32 	%r198|%p43, %r196, %r163, %r152, %r154;
	mov.b32 	%f161, %r198;
	add.f32 	%f162, %f160, %f161;
	mov.b32 	%r199, %f162;
	shfl.sync.bfly.b32 	%r201|%p44, %r199, %r166, %r152, %r154;
	mov.b32 	%f163, %r201;
	add.f32 	%f164, %f162, %f163;
	st.local.f32 	[%rd2+12], %f164;
	st.shared.f32 	[%r10], %f164;
	bar.sync 	0;
	@%p1 bra 	$L__BB124_17;

	ld.shared.f32 	%f165, [%r4];
	mov.b32 	%r202, %f165;
	mov.u32 	%r203, 31;
	mov.u32 	%r204, 16;
	mov.u32 	%r205, -1;
	shfl.sync.bfly.b32 	%r206|%p45, %r202, %r204, %r203, %r205;
	mov.b32 	%f166, %r206;
	add.f32 	%f167, %f165, %f166;
	mov.b32 	%r207, %f167;
	mov.u32 	%r208, 8;
	shfl.sync.bfly.b32 	%r209|%p46, %r207, %r208, %r203, %r205;
	mov.b32 	%f168, %r209;
	add.f32 	%f169, %f167, %f168;
	mov.b32 	%r210, %f169;
	mov.u32 	%r211, 4;
	shfl.sync.bfly.b32 	%r212|%p47, %r210, %r211, %r203, %r205;
	mov.b32 	%f170, %r212;
	add.f32 	%f171, %f169, %f170;
	mov.b32 	%r213, %f171;
	mov.u32 	%r214, 2;
	shfl.sync.bfly.b32 	%r215|%p48, %r213, %r214, %r203, %r205;
	mov.b32 	%f172, %r215;
	add.f32 	%f173, %f171, %f172;
	mov.b32 	%r216, %f173;
	mov.u32 	%r217, 1;
	shfl.sync.bfly.b32 	%r218|%p49, %r216, %r217, %r203, %r205;
	mov.b32 	%f174, %r218;
	add.f32 	%f175, %f173, %f174;
	st.local.f32 	[%rd2+12], %f175;

$L__BB124_17:
	bar.sync 	0;
	mov.b32 	%r219, %f208;
	mov.u32 	%r220, 31;
	mov.u32 	%r221, 16;
	mov.u32 	%r222, -1;
	shfl.sync.bfly.b32 	%r223|%p51, %r219, %r221, %r220, %r222;
	mov.b32 	%f176, %r223;
	add.f32 	%f177, %f208, %f176;
	mov.b32 	%r224, %f177;
	mov.u32 	%r225, 8;
	shfl.sync.bfly.b32 	%r226|%p52, %r224, %r225, %r220, %r222;
	mov.b32 	%f178, %r226;
	add.f32 	%f179, %f177, %f178;
	mov.b32 	%r227, %f179;
	mov.u32 	%r228, 4;
	shfl.sync.bfly.b32 	%r229|%p53, %r227, %r228, %r220, %r222;
	mov.b32 	%f180, %r229;
	add.f32 	%f181, %f179, %f180;
	mov.b32 	%r230, %f181;
	mov.u32 	%r231, 2;
	shfl.sync.bfly.b32 	%r232|%p54, %r230, %r231, %r220, %r222;
	mov.b32 	%f182, %r232;
	add.f32 	%f183, %f181, %f182;
	mov.b32 	%r233, %f183;
	mov.u32 	%r234, 1;
	shfl.sync.bfly.b32 	%r235|%p55, %r233, %r234, %r220, %r222;
	mov.b32 	%f184, %r235;
	add.f32 	%f185, %f183, %f184;
	st.local.f32 	[%rd2+16], %f185;
	st.shared.f32 	[%r10], %f185;
	bar.sync 	0;
	@%p1 bra 	$L__BB124_19;

	ld.shared.f32 	%f186, [%r4];
	mov.b32 	%r236, %f186;
	shfl.sync.bfly.b32 	%r240|%p56, %r236, %r221, %r220, %r222;
	mov.b32 	%f187, %r240;
	add.f32 	%f188, %f186, %f187;
	mov.b32 	%r241, %f188;
	shfl.sync.bfly.b32 	%r243|%p57, %r241, %r225, %r220, %r222;
	mov.b32 	%f189, %r243;
	add.f32 	%f190, %f188, %f189;
	mov.b32 	%r244, %f190;
	shfl.sync.bfly.b32 	%r246|%p58, %r244, %r228, %r220, %r222;
	mov.b32 	%f191, %r246;
	add.f32 	%f192, %f190, %f191;
	mov.b32 	%r247, %f192;
	shfl.sync.bfly.b32 	%r249|%p59, %r247, %r231, %r220, %r222;
	mov.b32 	%f193, %r249;
	add.f32 	%f194, %f192, %f193;
	mov.b32 	%r250, %f194;
	shfl.sync.bfly.b32 	%r252|%p60, %r250, %r234, %r220, %r222;
	mov.b32 	%f195, %r252;
	add.f32 	%f196, %f194, %f195;
	st.local.f32 	[%rd2+16], %f196;

$L__BB124_19:
	bar.sync 	0;
	setp.gt.s32 	%p61, %r3, 4;
	@%p61 bra 	$L__BB124_21;

	mad.lo.s32 	%r253, %r3, %r13, %r2;
	cvt.s64.s32 	%rd54, %r253;
	mul.lo.s32 	%r254, %r1, %r14;
	cvt.s64.s32 	%rd55, %r254;
	add.s64 	%rd56, %rd55, %rd54;
	mul.wide.s32 	%rd57, %r3, 4;
	add.s64 	%rd58, %rd2, %rd57;
	ld.local.f32 	%f197, [%rd58];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f197;}

	// end inline asm
	cvta.to.global.u64 	%rd59, %rd18;
	shl.b64 	%rd60, %rd56, 1;
	add.s64 	%rd61, %rd59, %rd60;
	st.global.u16 	[%rd61], %rs1;

$L__BB124_21:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_6_bs_256
.visible .entry ggml_matvec_f16_ncols_6_bs_256(
	.param .u64 ggml_matvec_f16_ncols_6_bs_256_param_0,
	.param .u64 ggml_matvec_f16_ncols_6_bs_256_param_1,
	.param .u64 ggml_matvec_f16_ncols_6_bs_256_param_2,
	.param .u32 ggml_matvec_f16_ncols_6_bs_256_param_3,
	.param .u32 ggml_matvec_f16_ncols_6_bs_256_param_4,
	.param .u32 ggml_matvec_f16_ncols_6_bs_256_param_5,
	.param .u32 ggml_matvec_f16_ncols_6_bs_256_param_6,
	.param .u32 ggml_matvec_f16_ncols_6_bs_256_param_7,
	.param .u32 ggml_matvec_f16_ncols_6_bs_256_param_8,
	.param .u32 ggml_matvec_f16_ncols_6_bs_256_param_9,
	.param .u32 ggml_matvec_f16_ncols_6_bs_256_param_10,
	.param .u32 ggml_matvec_f16_ncols_6_bs_256_param_11
)
{
	.local .align 8 .b8 	__local_depot125[24];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<73>;
	.reg .b16 	%rs<2>;
	.reg .f32 	%f<254>;
	.reg .b32 	%r<295>;
	.reg .b64 	%rd<67>;


	mov.u64 	%SPL, __local_depot125;
	ld.param.u64 	%rd20, [ggml_matvec_f16_ncols_6_bs_256_param_0];
	ld.param.u64 	%rd21, [ggml_matvec_f16_ncols_6_bs_256_param_1];
	ld.param.u64 	%rd19, [ggml_matvec_f16_ncols_6_bs_256_param_2];
	ld.param.u32 	%r11, [ggml_matvec_f16_ncols_6_bs_256_param_3];
	ld.param.u32 	%r15, [ggml_matvec_f16_ncols_6_bs_256_param_5];
	ld.param.u32 	%r12, [ggml_matvec_f16_ncols_6_bs_256_param_6];
	ld.param.u32 	%r13, [ggml_matvec_f16_ncols_6_bs_256_param_7];
	ld.param.u32 	%r16, [ggml_matvec_f16_ncols_6_bs_256_param_8];
	ld.param.u32 	%r17, [ggml_matvec_f16_ncols_6_bs_256_param_9];
	ld.param.u32 	%r18, [ggml_matvec_f16_ncols_6_bs_256_param_10];
	ld.param.u32 	%r14, [ggml_matvec_f16_ncols_6_bs_256_param_11];
	cvta.to.global.u64 	%rd66, %rd21;
	add.u64 	%rd2, %SPL, 0;
	mov.u32 	%r1, %ctaid.y;
	div.s32 	%r19, %r1, %r16;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r20, %r2, %r15;
	mad.lo.s32 	%r21, %r19, %r17, %r20;
	cvt.s64.s32 	%rd3, %r21;
	cvta.to.global.u64 	%rd4, %rd20;
	mul.lo.s32 	%r22, %r1, %r18;
	cvt.s64.s32 	%rd5, %r22;
	mov.u32 	%r3, %tid.x;
	setp.gt.s32 	%p1, %r3, 31;
	shl.b32 	%r23, %r3, 2;
	mov.u32 	%r24, data_mmv;
	add.s32 	%r4, %r24, %r23;
	@%p1 bra 	$L__BB125_2;

	mov.u32 	%r25, 0;
	st.shared.u32 	[%r4], %r25;

$L__BB125_2:
	bar.sync 	0;
	mov.f32 	%f248, 0f00000000;
	st.local.v2.f32 	[%rd2], {%f248, %f248};
	st.local.v2.f32 	[%rd2+8], {%f248, %f248};
	st.local.v2.f32 	[%rd2+16], {%f248, %f248};
	setp.ge.s32 	%p2, %r3, %r11;
	mov.f32 	%f249, %f248;
	mov.f32 	%f250, %f248;
	mov.f32 	%f251, %f248;
	mov.f32 	%f252, %f248;
	mov.f32 	%f253, %f248;
	@%p2 bra 	$L__BB125_9;

	not.b32 	%r26, %r3;
	add.s32 	%r5, %r26, %r11;
	and.b32  	%r27, %r5, 256;
	setp.ne.s32 	%p3, %r27, 0;
	mov.f32 	%f248, 0f00000000;
	mov.u32 	%r294, %r3;
	@%p3 bra 	$L__BB125_5;

	shl.b64 	%rd23, %rd5, 1;
	add.s64 	%rd24, %rd66, %rd23;
	shl.b64 	%rd25, %rd3, 1;
	add.s64 	%rd26, %rd4, %rd25;
	mul.wide.s32 	%rd27, %r3, 4;
	add.s64 	%rd28, %rd26, %rd27;
	ld.global.nc.u32 	%r28, [%rd28];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r28;
  cvt.f32.f16 %f43, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r28;
  cvt.f32.f16 %f44, high;}

	// end inline asm
	add.s64 	%rd29, %rd24, %rd27;
	ld.global.nc.u32 	%r30, [%rd29];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r30;
  cvt.f32.f16 %f45, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r30;
  cvt.f32.f16 %f46, high;}

	// end inline asm
	fma.rn.f32 	%f57, %f43, %f45, 0f00000000;
	fma.rn.f32 	%f253, %f44, %f46, %f57;
	st.local.f32 	[%rd2], %f253;
	mul.wide.s32 	%rd30, %r12, 4;
	add.s64 	%rd31, %rd29, %rd30;
	ld.global.nc.u32 	%r32, [%rd31];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r32;
  cvt.f32.f16 %f47, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r32;
  cvt.f32.f16 %f48, high;}

	// end inline asm
	fma.rn.f32 	%f58, %f43, %f47, 0f00000000;
	fma.rn.f32 	%f252, %f44, %f48, %f58;
	st.local.f32 	[%rd2+4], %f252;
	add.s32 	%r42, %r3, %r12;
	add.s32 	%r43, %r42, %r12;
	mul.wide.s32 	%rd32, %r43, 4;
	add.s64 	%rd33, %rd24, %rd32;
	ld.global.nc.u32 	%r34, [%rd33];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r34;
  cvt.f32.f16 %f49, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r34;
  cvt.f32.f16 %f50, high;}

	// end inline asm
	fma.rn.f32 	%f59, %f43, %f49, 0f00000000;
	fma.rn.f32 	%f251, %f44, %f50, %f59;
	st.local.f32 	[%rd2+8], %f251;
	add.s64 	%rd34, %rd33, %rd30;
	ld.global.nc.u32 	%r36, [%rd34];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r36;
  cvt.f32.f16 %f51, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r36;
  cvt.f32.f16 %f52, high;}

	// end inline asm
	fma.rn.f32 	%f60, %f43, %f51, 0f00000000;
	fma.rn.f32 	%f250, %f44, %f52, %f60;
	st.local.f32 	[%rd2+12], %f250;
	add.s64 	%rd35, %rd34, %rd30;
	ld.global.nc.u32 	%r38, [%rd35];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r38;
  cvt.f32.f16 %f53, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r38;
  cvt.f32.f16 %f54, high;}

	// end inline asm
	fma.rn.f32 	%f61, %f43, %f53, 0f00000000;
	fma.rn.f32 	%f249, %f44, %f54, %f61;
	st.local.f32 	[%rd2+16], %f249;
	add.s64 	%rd36, %rd35, %rd30;
	ld.global.nc.u32 	%r40, [%rd36];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r40;
  cvt.f32.f16 %f55, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r40;
  cvt.f32.f16 %f56, high;}

	// end inline asm
	fma.rn.f32 	%f62, %f43, %f55, 0f00000000;
	fma.rn.f32 	%f248, %f44, %f56, %f62;
	st.local.f32 	[%rd2+20], %f248;
	add.s32 	%r294, %r3, 256;

$L__BB125_5:
	and.b32  	%r44, %r5, -256;
	setp.eq.s32 	%p4, %r44, 0;
	@%p4 bra 	$L__BB125_9;

	add.s32 	%r45, %r294, %r12;
	add.s32 	%r46, %r45, 256;
	mul.wide.s32 	%rd37, %r46, 4;
	shl.b64 	%rd38, %rd5, 1;
	add.s64 	%rd7, %rd37, %rd38;
	shl.b32 	%r47, %r12, 1;
	add.s32 	%r48, %r294, %r47;
	mad.lo.s32 	%r49, %r12, 3, %r294;
	shl.b32 	%r50, %r12, 2;
	add.s32 	%r51, %r294, %r50;
	mad.lo.s32 	%r52, %r12, 5, %r294;
	mul.wide.s32 	%rd39, %r48, 4;
	add.s64 	%rd8, %rd39, %rd38;
	mul.wide.s32 	%rd40, %r49, 4;
	add.s64 	%rd9, %rd40, %rd38;
	mul.wide.s32 	%rd41, %r51, 4;
	add.s64 	%rd10, %rd41, %rd38;
	mul.wide.s32 	%rd42, %r52, 4;
	add.s64 	%rd11, %rd42, %rd38;
	mul.wide.s32 	%rd43, %r294, 2;
	add.s64 	%rd44, %rd43, %rd3;
	shl.b64 	%rd45, %rd44, 1;
	add.s64 	%rd46, %rd4, %rd45;
	add.s64 	%rd65, %rd46, 1024;
	mul.wide.s32 	%rd47, %r294, 4;
	mul.wide.s32 	%rd48, %r12, 4;
	add.s64 	%rd49, %rd47, %rd48;
	add.s64 	%rd13, %rd49, %rd38;
	add.s64 	%rd14, %rd47, %rd38;

$L__BB125_7:
	ld.global.nc.u32 	%r53, [%rd65+-1024];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r53;
  cvt.f32.f16 %f63, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r53;
  cvt.f32.f16 %f64, high;}

	// end inline asm
	add.s64 	%rd50, %rd66, %rd14;
	ld.global.nc.u32 	%r55, [%rd50];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r55;
  cvt.f32.f16 %f65, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r55;
  cvt.f32.f16 %f66, high;}

	// end inline asm
	fma.rn.f32 	%f91, %f63, %f65, %f253;
	fma.rn.f32 	%f92, %f64, %f66, %f91;
	add.s64 	%rd51, %rd66, %rd13;
	ld.global.nc.u32 	%r57, [%rd51];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r57;
  cvt.f32.f16 %f67, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r57;
  cvt.f32.f16 %f68, high;}

	// end inline asm
	fma.rn.f32 	%f93, %f63, %f67, %f252;
	fma.rn.f32 	%f94, %f64, %f68, %f93;
	add.s64 	%rd52, %rd66, %rd8;
	ld.global.nc.u32 	%r59, [%rd52];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r59;
  cvt.f32.f16 %f69, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r59;
  cvt.f32.f16 %f70, high;}

	// end inline asm
	fma.rn.f32 	%f95, %f63, %f69, %f251;
	fma.rn.f32 	%f96, %f64, %f70, %f95;
	add.s64 	%rd53, %rd66, %rd9;
	ld.global.nc.u32 	%r61, [%rd53];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r61;
  cvt.f32.f16 %f71, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r61;
  cvt.f32.f16 %f72, high;}

	// end inline asm
	fma.rn.f32 	%f97, %f63, %f71, %f250;
	fma.rn.f32 	%f98, %f64, %f72, %f97;
	add.s64 	%rd54, %rd66, %rd10;
	ld.global.nc.u32 	%r63, [%rd54];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r63;
  cvt.f32.f16 %f73, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r63;
  cvt.f32.f16 %f74, high;}

	// end inline asm
	fma.rn.f32 	%f99, %f63, %f73, %f249;
	fma.rn.f32 	%f100, %f64, %f74, %f99;
	add.s64 	%rd55, %rd66, %rd11;
	ld.global.nc.u32 	%r65, [%rd55];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r65;
  cvt.f32.f16 %f75, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r65;
  cvt.f32.f16 %f76, high;}

	// end inline asm
	fma.rn.f32 	%f101, %f63, %f75, %f248;
	fma.rn.f32 	%f102, %f64, %f76, %f101;
	ld.global.nc.u32 	%r67, [%rd65];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r67;
  cvt.f32.f16 %f77, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r67;
  cvt.f32.f16 %f78, high;}

	// end inline asm
	ld.global.nc.u32 	%r69, [%rd50+1024];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r69;
  cvt.f32.f16 %f79, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r69;
  cvt.f32.f16 %f80, high;}

	// end inline asm
	fma.rn.f32 	%f103, %f77, %f79, %f92;
	fma.rn.f32 	%f253, %f78, %f80, %f103;
	add.s64 	%rd56, %rd66, %rd7;
	ld.global.nc.u32 	%r71, [%rd56];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r71;
  cvt.f32.f16 %f81, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r71;
  cvt.f32.f16 %f82, high;}

	// end inline asm
	fma.rn.f32 	%f104, %f77, %f81, %f94;
	fma.rn.f32 	%f252, %f78, %f82, %f104;
	ld.global.nc.u32 	%r73, [%rd52+1024];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r73;
  cvt.f32.f16 %f83, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r73;
  cvt.f32.f16 %f84, high;}

	// end inline asm
	fma.rn.f32 	%f105, %f77, %f83, %f96;
	fma.rn.f32 	%f251, %f78, %f84, %f105;
	ld.global.nc.u32 	%r75, [%rd53+1024];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r75;
  cvt.f32.f16 %f85, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r75;
  cvt.f32.f16 %f86, high;}

	// end inline asm
	fma.rn.f32 	%f106, %f77, %f85, %f98;
	fma.rn.f32 	%f250, %f78, %f86, %f106;
	ld.global.nc.u32 	%r77, [%rd54+1024];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r77;
  cvt.f32.f16 %f87, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r77;
  cvt.f32.f16 %f88, high;}

	// end inline asm
	fma.rn.f32 	%f107, %f77, %f87, %f100;
	fma.rn.f32 	%f249, %f78, %f88, %f107;
	ld.global.nc.u32 	%r79, [%rd55+1024];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r79;
  cvt.f32.f16 %f89, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r79;
  cvt.f32.f16 %f90, high;}

	// end inline asm
	fma.rn.f32 	%f108, %f77, %f89, %f102;
	fma.rn.f32 	%f248, %f78, %f90, %f108;
	add.s64 	%rd66, %rd66, 2048;
	add.s64 	%rd65, %rd65, 2048;
	add.s32 	%r294, %r294, 512;
	setp.lt.s32 	%p5, %r294, %r11;
	@%p5 bra 	$L__BB125_7;

	st.local.v2.f32 	[%rd2], {%f253, %f252};
	st.local.v2.f32 	[%rd2+8], {%f251, %f250};
	st.local.v2.f32 	[%rd2+16], {%f249, %f248};

$L__BB125_9:
	shr.s32 	%r81, %r3, 31;
	shr.u32 	%r82, %r81, 27;
	add.s32 	%r83, %r3, %r82;
	shr.s32 	%r84, %r83, 5;
	shl.b32 	%r85, %r84, 2;
	add.s32 	%r10, %r24, %r85;
	mov.u32 	%r87, 2;
	mov.b32 	%r88, %f253;
	mov.u32 	%r89, 31;
	mov.u32 	%r90, 16;
	mov.u32 	%r91, -1;
	shfl.sync.bfly.b32 	%r92|%p6, %r88, %r90, %r89, %r91;
	mov.b32 	%f109, %r92;
	add.f32 	%f110, %f253, %f109;
	mov.b32 	%r93, %f110;
	mov.u32 	%r94, 8;
	shfl.sync.bfly.b32 	%r95|%p7, %r93, %r94, %r89, %r91;
	mov.b32 	%f111, %r95;
	add.f32 	%f112, %f110, %f111;
	mov.b32 	%r96, %f112;
	mov.u32 	%r97, 4;
	shfl.sync.bfly.b32 	%r98|%p8, %r96, %r97, %r89, %r91;
	mov.b32 	%f113, %r98;
	add.f32 	%f114, %f112, %f113;
	mov.b32 	%r99, %f114;
	shfl.sync.bfly.b32 	%r100|%p9, %r99, %r87, %r89, %r91;
	mov.b32 	%f115, %r100;
	add.f32 	%f116, %f114, %f115;
	mov.b32 	%r101, %f116;
	mov.u32 	%r102, 1;
	shfl.sync.bfly.b32 	%r103|%p10, %r101, %r102, %r89, %r91;
	mov.b32 	%f117, %r103;
	add.f32 	%f118, %f116, %f117;
	st.local.f32 	[%rd2], %f118;
	st.shared.f32 	[%r10], %f118;
	bar.sync 	0;
	@%p1 bra 	$L__BB125_11;

	ld.shared.f32 	%f119, [%r4];
	mov.b32 	%r104, %f119;
	shfl.sync.bfly.b32 	%r108|%p12, %r104, %r90, %r89, %r91;
	mov.b32 	%f120, %r108;
	add.f32 	%f121, %f119, %f120;
	mov.b32 	%r109, %f121;
	shfl.sync.bfly.b32 	%r111|%p13, %r109, %r94, %r89, %r91;
	mov.b32 	%f122, %r111;
	add.f32 	%f123, %f121, %f122;
	mov.b32 	%r112, %f123;
	shfl.sync.bfly.b32 	%r114|%p14, %r112, %r97, %r89, %r91;
	mov.b32 	%f124, %r114;
	add.f32 	%f125, %f123, %f124;
	mov.b32 	%r115, %f125;
	shfl.sync.bfly.b32 	%r117|%p15, %r115, %r87, %r89, %r91;
	mov.b32 	%f126, %r117;
	add.f32 	%f127, %f125, %f126;
	mov.b32 	%r118, %f127;
	shfl.sync.bfly.b32 	%r120|%p16, %r118, %r102, %r89, %r91;
	mov.b32 	%f128, %r120;
	add.f32 	%f129, %f127, %f128;
	st.local.f32 	[%rd2], %f129;

$L__BB125_11:
	bar.sync 	0;
	mov.b32 	%r121, %f252;
	shfl.sync.bfly.b32 	%r125|%p18, %r121, %r90, %r89, %r91;
	mov.b32 	%f130, %r125;
	add.f32 	%f131, %f252, %f130;
	mov.b32 	%r126, %f131;
	shfl.sync.bfly.b32 	%r128|%p19, %r126, %r94, %r89, %r91;
	mov.b32 	%f132, %r128;
	add.f32 	%f133, %f131, %f132;
	mov.b32 	%r129, %f133;
	shfl.sync.bfly.b32 	%r131|%p20, %r129, %r97, %r89, %r91;
	mov.b32 	%f134, %r131;
	add.f32 	%f135, %f133, %f134;
	mov.b32 	%r132, %f135;
	shfl.sync.bfly.b32 	%r134|%p21, %r132, %r87, %r89, %r91;
	mov.b32 	%f136, %r134;
	add.f32 	%f137, %f135, %f136;
	mov.b32 	%r135, %f137;
	shfl.sync.bfly.b32 	%r137|%p22, %r135, %r102, %r89, %r91;
	mov.b32 	%f138, %r137;
	add.f32 	%f139, %f137, %f138;
	st.local.f32 	[%rd2+4], %f139;
	st.shared.f32 	[%r10], %f139;
	bar.sync 	0;
	@%p1 bra 	$L__BB125_13;

	ld.shared.f32 	%f140, [%r4];
	mov.b32 	%r138, %f140;
	mov.u32 	%r139, 31;
	mov.u32 	%r140, 16;
	mov.u32 	%r141, -1;
	shfl.sync.bfly.b32 	%r142|%p23, %r138, %r140, %r139, %r141;
	mov.b32 	%f141, %r142;
	add.f32 	%f142, %f140, %f141;
	mov.b32 	%r143, %f142;
	mov.u32 	%r144, 8;
	shfl.sync.bfly.b32 	%r145|%p24, %r143, %r144, %r139, %r141;
	mov.b32 	%f143, %r145;
	add.f32 	%f144, %f142, %f143;
	mov.b32 	%r146, %f144;
	mov.u32 	%r147, 4;
	shfl.sync.bfly.b32 	%r148|%p25, %r146, %r147, %r139, %r141;
	mov.b32 	%f145, %r148;
	add.f32 	%f146, %f144, %f145;
	mov.b32 	%r149, %f146;
	mov.u32 	%r150, 2;
	shfl.sync.bfly.b32 	%r151|%p26, %r149, %r150, %r139, %r141;
	mov.b32 	%f147, %r151;
	add.f32 	%f148, %f146, %f147;
	mov.b32 	%r152, %f148;
	mov.u32 	%r153, 1;
	shfl.sync.bfly.b32 	%r154|%p27, %r152, %r153, %r139, %r141;
	mov.b32 	%f149, %r154;
	add.f32 	%f150, %f148, %f149;
	st.local.f32 	[%rd2+4], %f150;

$L__BB125_13:
	bar.sync 	0;
	mov.b32 	%r155, %f251;
	mov.u32 	%r156, 31;
	mov.u32 	%r157, 16;
	mov.u32 	%r158, -1;
	shfl.sync.bfly.b32 	%r159|%p29, %r155, %r157, %r156, %r158;
	mov.b32 	%f151, %r159;
	add.f32 	%f152, %f251, %f151;
	mov.b32 	%r160, %f152;
	mov.u32 	%r161, 8;
	shfl.sync.bfly.b32 	%r162|%p30, %r160, %r161, %r156, %r158;
	mov.b32 	%f153, %r162;
	add.f32 	%f154, %f152, %f153;
	mov.b32 	%r163, %f154;
	mov.u32 	%r164, 4;
	shfl.sync.bfly.b32 	%r165|%p31, %r163, %r164, %r156, %r158;
	mov.b32 	%f155, %r165;
	add.f32 	%f156, %f154, %f155;
	mov.b32 	%r166, %f156;
	mov.u32 	%r167, 2;
	shfl.sync.bfly.b32 	%r168|%p32, %r166, %r167, %r156, %r158;
	mov.b32 	%f157, %r168;
	add.f32 	%f158, %f156, %f157;
	mov.b32 	%r169, %f158;
	mov.u32 	%r170, 1;
	shfl.sync.bfly.b32 	%r171|%p33, %r169, %r170, %r156, %r158;
	mov.b32 	%f159, %r171;
	add.f32 	%f160, %f158, %f159;
	st.local.f32 	[%rd2+8], %f160;
	st.shared.f32 	[%r10], %f160;
	bar.sync 	0;
	@%p1 bra 	$L__BB125_15;

	ld.shared.f32 	%f161, [%r4];
	mov.b32 	%r172, %f161;
	shfl.sync.bfly.b32 	%r176|%p34, %r172, %r157, %r156, %r158;
	mov.b32 	%f162, %r176;
	add.f32 	%f163, %f161, %f162;
	mov.b32 	%r177, %f163;
	shfl.sync.bfly.b32 	%r179|%p35, %r177, %r161, %r156, %r158;
	mov.b32 	%f164, %r179;
	add.f32 	%f165, %f163, %f164;
	mov.b32 	%r180, %f165;
	shfl.sync.bfly.b32 	%r182|%p36, %r180, %r164, %r156, %r158;
	mov.b32 	%f166, %r182;
	add.f32 	%f167, %f165, %f166;
	mov.b32 	%r183, %f167;
	shfl.sync.bfly.b32 	%r185|%p37, %r183, %r167, %r156, %r158;
	mov.b32 	%f168, %r185;
	add.f32 	%f169, %f167, %f168;
	mov.b32 	%r186, %f169;
	shfl.sync.bfly.b32 	%r188|%p38, %r186, %r170, %r156, %r158;
	mov.b32 	%f170, %r188;
	add.f32 	%f171, %f169, %f170;
	st.local.f32 	[%rd2+8], %f171;

$L__BB125_15:
	bar.sync 	0;
	mov.b32 	%r189, %f250;
	shfl.sync.bfly.b32 	%r193|%p40, %r189, %r157, %r156, %r158;
	mov.b32 	%f172, %r193;
	add.f32 	%f173, %f250, %f172;
	mov.b32 	%r194, %f173;
	shfl.sync.bfly.b32 	%r196|%p41, %r194, %r161, %r156, %r158;
	mov.b32 	%f174, %r196;
	add.f32 	%f175, %f173, %f174;
	mov.b32 	%r197, %f175;
	shfl.sync.bfly.b32 	%r199|%p42, %r197, %r164, %r156, %r158;
	mov.b32 	%f176, %r199;
	add.f32 	%f177, %f175, %f176;
	mov.b32 	%r200, %f177;
	shfl.sync.bfly.b32 	%r202|%p43, %r200, %r167, %r156, %r158;
	mov.b32 	%f178, %r202;
	add.f32 	%f179, %f177, %f178;
	mov.b32 	%r203, %f179;
	shfl.sync.bfly.b32 	%r205|%p44, %r203, %r170, %r156, %r158;
	mov.b32 	%f180, %r205;
	add.f32 	%f181, %f179, %f180;
	st.local.f32 	[%rd2+12], %f181;
	st.shared.f32 	[%r10], %f181;
	bar.sync 	0;
	@%p1 bra 	$L__BB125_17;

	ld.shared.f32 	%f182, [%r4];
	mov.b32 	%r206, %f182;
	mov.u32 	%r207, 31;
	mov.u32 	%r208, 16;
	mov.u32 	%r209, -1;
	shfl.sync.bfly.b32 	%r210|%p45, %r206, %r208, %r207, %r209;
	mov.b32 	%f183, %r210;
	add.f32 	%f184, %f182, %f183;
	mov.b32 	%r211, %f184;
	mov.u32 	%r212, 8;
	shfl.sync.bfly.b32 	%r213|%p46, %r211, %r212, %r207, %r209;
	mov.b32 	%f185, %r213;
	add.f32 	%f186, %f184, %f185;
	mov.b32 	%r214, %f186;
	mov.u32 	%r215, 4;
	shfl.sync.bfly.b32 	%r216|%p47, %r214, %r215, %r207, %r209;
	mov.b32 	%f187, %r216;
	add.f32 	%f188, %f186, %f187;
	mov.b32 	%r217, %f188;
	mov.u32 	%r218, 2;
	shfl.sync.bfly.b32 	%r219|%p48, %r217, %r218, %r207, %r209;
	mov.b32 	%f189, %r219;
	add.f32 	%f190, %f188, %f189;
	mov.b32 	%r220, %f190;
	mov.u32 	%r221, 1;
	shfl.sync.bfly.b32 	%r222|%p49, %r220, %r221, %r207, %r209;
	mov.b32 	%f191, %r222;
	add.f32 	%f192, %f190, %f191;
	st.local.f32 	[%rd2+12], %f192;

$L__BB125_17:
	bar.sync 	0;
	mov.b32 	%r223, %f249;
	mov.u32 	%r224, 31;
	mov.u32 	%r225, 16;
	mov.u32 	%r226, -1;
	shfl.sync.bfly.b32 	%r227|%p51, %r223, %r225, %r224, %r226;
	mov.b32 	%f193, %r227;
	add.f32 	%f194, %f249, %f193;
	mov.b32 	%r228, %f194;
	mov.u32 	%r229, 8;
	shfl.sync.bfly.b32 	%r230|%p52, %r228, %r229, %r224, %r226;
	mov.b32 	%f195, %r230;
	add.f32 	%f196, %f194, %f195;
	mov.b32 	%r231, %f196;
	mov.u32 	%r232, 4;
	shfl.sync.bfly.b32 	%r233|%p53, %r231, %r232, %r224, %r226;
	mov.b32 	%f197, %r233;
	add.f32 	%f198, %f196, %f197;
	mov.b32 	%r234, %f198;
	mov.u32 	%r235, 2;
	shfl.sync.bfly.b32 	%r236|%p54, %r234, %r235, %r224, %r226;
	mov.b32 	%f199, %r236;
	add.f32 	%f200, %f198, %f199;
	mov.b32 	%r237, %f200;
	mov.u32 	%r238, 1;
	shfl.sync.bfly.b32 	%r239|%p55, %r237, %r238, %r224, %r226;
	mov.b32 	%f201, %r239;
	add.f32 	%f202, %f200, %f201;
	st.local.f32 	[%rd2+16], %f202;
	st.shared.f32 	[%r10], %f202;
	bar.sync 	0;
	@%p1 bra 	$L__BB125_19;

	ld.shared.f32 	%f203, [%r4];
	mov.b32 	%r240, %f203;
	shfl.sync.bfly.b32 	%r244|%p56, %r240, %r225, %r224, %r226;
	mov.b32 	%f204, %r244;
	add.f32 	%f205, %f203, %f204;
	mov.b32 	%r245, %f205;
	shfl.sync.bfly.b32 	%r247|%p57, %r245, %r229, %r224, %r226;
	mov.b32 	%f206, %r247;
	add.f32 	%f207, %f205, %f206;
	mov.b32 	%r248, %f207;
	shfl.sync.bfly.b32 	%r250|%p58, %r248, %r232, %r224, %r226;
	mov.b32 	%f208, %r250;
	add.f32 	%f209, %f207, %f208;
	mov.b32 	%r251, %f209;
	shfl.sync.bfly.b32 	%r253|%p59, %r251, %r235, %r224, %r226;
	mov.b32 	%f210, %r253;
	add.f32 	%f211, %f209, %f210;
	mov.b32 	%r254, %f211;
	shfl.sync.bfly.b32 	%r256|%p60, %r254, %r238, %r224, %r226;
	mov.b32 	%f212, %r256;
	add.f32 	%f213, %f211, %f212;
	st.local.f32 	[%rd2+16], %f213;

$L__BB125_19:
	bar.sync 	0;
	mov.b32 	%r257, %f248;
	shfl.sync.bfly.b32 	%r261|%p62, %r257, %r225, %r224, %r226;
	mov.b32 	%f214, %r261;
	add.f32 	%f215, %f248, %f214;
	mov.b32 	%r262, %f215;
	shfl.sync.bfly.b32 	%r264|%p63, %r262, %r229, %r224, %r226;
	mov.b32 	%f216, %r264;
	add.f32 	%f217, %f215, %f216;
	mov.b32 	%r265, %f217;
	shfl.sync.bfly.b32 	%r267|%p64, %r265, %r232, %r224, %r226;
	mov.b32 	%f218, %r267;
	add.f32 	%f219, %f217, %f218;
	mov.b32 	%r268, %f219;
	shfl.sync.bfly.b32 	%r270|%p65, %r268, %r235, %r224, %r226;
	mov.b32 	%f220, %r270;
	add.f32 	%f221, %f219, %f220;
	mov.b32 	%r271, %f221;
	shfl.sync.bfly.b32 	%r273|%p66, %r271, %r238, %r224, %r226;
	mov.b32 	%f222, %r273;
	add.f32 	%f223, %f221, %f222;
	st.local.f32 	[%rd2+20], %f223;
	st.shared.f32 	[%r10], %f223;
	bar.sync 	0;
	@%p1 bra 	$L__BB125_21;

	ld.shared.f32 	%f224, [%r4];
	mov.b32 	%r274, %f224;
	mov.u32 	%r275, 31;
	mov.u32 	%r276, 16;
	mov.u32 	%r277, -1;
	shfl.sync.bfly.b32 	%r278|%p67, %r274, %r276, %r275, %r277;
	mov.b32 	%f225, %r278;
	add.f32 	%f226, %f224, %f225;
	mov.b32 	%r279, %f226;
	mov.u32 	%r280, 8;
	shfl.sync.bfly.b32 	%r281|%p68, %r279, %r280, %r275, %r277;
	mov.b32 	%f227, %r281;
	add.f32 	%f228, %f226, %f227;
	mov.b32 	%r282, %f228;
	mov.u32 	%r283, 4;
	shfl.sync.bfly.b32 	%r284|%p69, %r282, %r283, %r275, %r277;
	mov.b32 	%f229, %r284;
	add.f32 	%f230, %f228, %f229;
	mov.b32 	%r285, %f230;
	mov.u32 	%r286, 2;
	shfl.sync.bfly.b32 	%r287|%p70, %r285, %r286, %r275, %r277;
	mov.b32 	%f231, %r287;
	add.f32 	%f232, %f230, %f231;
	mov.b32 	%r288, %f232;
	mov.u32 	%r289, 1;
	shfl.sync.bfly.b32 	%r290|%p71, %r288, %r289, %r275, %r277;
	mov.b32 	%f233, %r290;
	add.f32 	%f234, %f232, %f233;
	st.local.f32 	[%rd2+20], %f234;

$L__BB125_21:
	bar.sync 	0;
	setp.gt.s32 	%p72, %r3, 5;
	@%p72 bra 	$L__BB125_23;

	mad.lo.s32 	%r291, %r3, %r13, %r2;
	cvt.s64.s32 	%rd57, %r291;
	mul.lo.s32 	%r292, %r1, %r14;
	cvt.s64.s32 	%rd58, %r292;
	add.s64 	%rd59, %rd58, %rd57;
	mul.wide.s32 	%rd60, %r3, 4;
	add.s64 	%rd61, %rd2, %rd60;
	ld.local.f32 	%f235, [%rd61];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f235;}

	// end inline asm
	cvta.to.global.u64 	%rd62, %rd19;
	shl.b64 	%rd63, %rd59, 1;
	add.s64 	%rd64, %rd62, %rd63;
	st.global.u16 	[%rd64], %rs1;

$L__BB125_23:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_7_bs_256
.visible .entry ggml_matvec_f16_ncols_7_bs_256(
	.param .u64 ggml_matvec_f16_ncols_7_bs_256_param_0,
	.param .u64 ggml_matvec_f16_ncols_7_bs_256_param_1,
	.param .u64 ggml_matvec_f16_ncols_7_bs_256_param_2,
	.param .u32 ggml_matvec_f16_ncols_7_bs_256_param_3,
	.param .u32 ggml_matvec_f16_ncols_7_bs_256_param_4,
	.param .u32 ggml_matvec_f16_ncols_7_bs_256_param_5,
	.param .u32 ggml_matvec_f16_ncols_7_bs_256_param_6,
	.param .u32 ggml_matvec_f16_ncols_7_bs_256_param_7,
	.param .u32 ggml_matvec_f16_ncols_7_bs_256_param_8,
	.param .u32 ggml_matvec_f16_ncols_7_bs_256_param_9,
	.param .u32 ggml_matvec_f16_ncols_7_bs_256_param_10,
	.param .u32 ggml_matvec_f16_ncols_7_bs_256_param_11
)
{
	.local .align 4 .b8 	__local_depot126[28];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<82>;
	.reg .b16 	%rs<2>;
	.reg .f32 	%f<221>;
	.reg .b32 	%r<289>;
	.reg .b64 	%rd<43>;


	mov.u64 	%SPL, __local_depot126;
	ld.param.u64 	%rd13, [ggml_matvec_f16_ncols_7_bs_256_param_0];
	ld.param.u64 	%rd14, [ggml_matvec_f16_ncols_7_bs_256_param_1];
	ld.param.u64 	%rd15, [ggml_matvec_f16_ncols_7_bs_256_param_2];
	ld.param.u32 	%r8, [ggml_matvec_f16_ncols_7_bs_256_param_3];
	ld.param.u32 	%r9, [ggml_matvec_f16_ncols_7_bs_256_param_5];
	ld.param.u32 	%r10, [ggml_matvec_f16_ncols_7_bs_256_param_6];
	ld.param.u32 	%r11, [ggml_matvec_f16_ncols_7_bs_256_param_7];
	ld.param.u32 	%r12, [ggml_matvec_f16_ncols_7_bs_256_param_8];
	ld.param.u32 	%r13, [ggml_matvec_f16_ncols_7_bs_256_param_9];
	ld.param.u32 	%r14, [ggml_matvec_f16_ncols_7_bs_256_param_10];
	ld.param.u32 	%r15, [ggml_matvec_f16_ncols_7_bs_256_param_11];
	add.u64 	%rd1, %SPL, 0;
	mov.u32 	%r1, %ctaid.x;
	mov.u32 	%r2, %tid.x;
	setp.gt.s32 	%p1, %r2, 31;
	shl.b32 	%r16, %r2, 2;
	mov.u32 	%r17, data_mmv;
	add.s32 	%r3, %r17, %r16;
	@%p1 bra 	$L__BB126_2;

	mov.u32 	%r18, 0;
	st.shared.u32 	[%r3], %r18;

$L__BB126_2:
	mov.u32 	%r4, %ctaid.y;
	bar.sync 	0;
	mov.f32 	%f214, 0f00000000;
	mov.u32 	%r19, 0;
	st.local.u32 	[%rd1], %r19;
	st.local.u32 	[%rd1+4], %r19;
	st.local.u32 	[%rd1+8], %r19;
	st.local.u32 	[%rd1+12], %r19;
	st.local.u32 	[%rd1+16], %r19;
	st.local.u32 	[%rd1+20], %r19;
	st.local.u32 	[%rd1+24], %r19;
	setp.ge.s32 	%p2, %r2, %r8;
	mov.f32 	%f215, %f214;
	mov.f32 	%f216, %f214;
	mov.f32 	%f217, %f214;
	mov.f32 	%f218, %f214;
	mov.f32 	%f219, %f214;
	mov.f32 	%f220, %f214;
	@%p2 bra 	$L__BB126_6;

	shl.b32 	%r20, %r10, 1;
	add.s32 	%r21, %r2, %r20;
	mul.wide.s32 	%rd17, %r21, 4;
	mul.lo.s32 	%r22, %r4, %r14;
	mul.wide.s32 	%rd18, %r22, 2;
	add.s64 	%rd3, %rd17, %rd18;
	mul.wide.s32 	%rd19, %r2, 4;
	mul.wide.s32 	%rd4, %r10, 4;
	add.s64 	%rd20, %rd19, %rd4;
	add.s64 	%rd5, %rd20, %rd18;
	add.s64 	%rd6, %rd19, %rd18;
	mul.wide.s32 	%rd21, %r2, 2;
	div.s32 	%r23, %r4, %r12;
	mul.lo.s32 	%r24, %r1, %r9;
	mad.lo.s32 	%r25, %r23, %r13, %r24;
	cvt.s64.s32 	%rd22, %r25;
	add.s64 	%rd23, %rd21, %rd22;
	cvta.to.global.u64 	%rd24, %rd13;
	shl.b64 	%rd25, %rd23, 1;
	add.s64 	%rd41, %rd24, %rd25;
	cvta.to.global.u64 	%rd42, %rd14;
	mov.f32 	%f214, 0f00000000;
	mov.u32 	%r288, %r2;

$L__BB126_4:
	ld.global.nc.u32 	%r26, [%rd41];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r26;
  cvt.f32.f16 %f36, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r26;
  cvt.f32.f16 %f37, high;}

	// end inline asm
	add.s64 	%rd26, %rd42, %rd6;
	ld.global.nc.u32 	%r28, [%rd26];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r28;
  cvt.f32.f16 %f38, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r28;
  cvt.f32.f16 %f39, high;}

	// end inline asm
	fma.rn.f32 	%f52, %f36, %f38, %f220;
	fma.rn.f32 	%f220, %f37, %f39, %f52;
	add.s64 	%rd27, %rd42, %rd5;
	ld.global.nc.u32 	%r30, [%rd27];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r30;
  cvt.f32.f16 %f40, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r30;
  cvt.f32.f16 %f41, high;}

	// end inline asm
	fma.rn.f32 	%f53, %f36, %f40, %f219;
	fma.rn.f32 	%f219, %f37, %f41, %f53;
	add.s64 	%rd28, %rd42, %rd3;
	ld.global.nc.u32 	%r32, [%rd28];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r32;
  cvt.f32.f16 %f42, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r32;
  cvt.f32.f16 %f43, high;}

	// end inline asm
	fma.rn.f32 	%f54, %f36, %f42, %f218;
	fma.rn.f32 	%f218, %f37, %f43, %f54;
	add.s64 	%rd29, %rd28, %rd4;
	ld.global.nc.u32 	%r34, [%rd29];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r34;
  cvt.f32.f16 %f44, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r34;
  cvt.f32.f16 %f45, high;}

	// end inline asm
	fma.rn.f32 	%f55, %f36, %f44, %f217;
	fma.rn.f32 	%f217, %f37, %f45, %f55;
	add.s64 	%rd30, %rd29, %rd4;
	ld.global.nc.u32 	%r36, [%rd30];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r36;
  cvt.f32.f16 %f46, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r36;
  cvt.f32.f16 %f47, high;}

	// end inline asm
	fma.rn.f32 	%f56, %f36, %f46, %f216;
	fma.rn.f32 	%f216, %f37, %f47, %f56;
	add.s64 	%rd31, %rd30, %rd4;
	ld.global.nc.u32 	%r38, [%rd31];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r38;
  cvt.f32.f16 %f48, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r38;
  cvt.f32.f16 %f49, high;}

	// end inline asm
	fma.rn.f32 	%f57, %f36, %f48, %f215;
	fma.rn.f32 	%f215, %f37, %f49, %f57;
	add.s64 	%rd32, %rd31, %rd4;
	ld.global.nc.u32 	%r40, [%rd32];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r40;
  cvt.f32.f16 %f50, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r40;
  cvt.f32.f16 %f51, high;}

	// end inline asm
	fma.rn.f32 	%f58, %f36, %f50, %f214;
	fma.rn.f32 	%f214, %f37, %f51, %f58;
	add.s64 	%rd42, %rd42, 1024;
	add.s64 	%rd41, %rd41, 1024;
	add.s32 	%r288, %r288, 256;
	setp.lt.s32 	%p3, %r288, %r8;
	@%p3 bra 	$L__BB126_4;

	st.local.f32 	[%rd1], %f220;
	st.local.f32 	[%rd1+4], %f219;
	st.local.f32 	[%rd1+8], %f218;
	st.local.f32 	[%rd1+12], %f217;
	st.local.f32 	[%rd1+16], %f216;
	st.local.f32 	[%rd1+20], %f215;
	st.local.f32 	[%rd1+24], %f214;

$L__BB126_6:
	shr.s32 	%r42, %r2, 31;
	shr.u32 	%r43, %r42, 27;
	add.s32 	%r44, %r2, %r43;
	shr.s32 	%r45, %r44, 5;
	shl.b32 	%r46, %r45, 2;
	add.s32 	%r7, %r17, %r46;
	mov.u32 	%r48, 2;
	mov.b32 	%r49, %f220;
	mov.u32 	%r50, 31;
	mov.u32 	%r51, 16;
	mov.u32 	%r52, -1;
	shfl.sync.bfly.b32 	%r53|%p4, %r49, %r51, %r50, %r52;
	mov.b32 	%f59, %r53;
	add.f32 	%f60, %f220, %f59;
	mov.b32 	%r54, %f60;
	mov.u32 	%r55, 8;
	shfl.sync.bfly.b32 	%r56|%p5, %r54, %r55, %r50, %r52;
	mov.b32 	%f61, %r56;
	add.f32 	%f62, %f60, %f61;
	mov.b32 	%r57, %f62;
	mov.u32 	%r58, 4;
	shfl.sync.bfly.b32 	%r59|%p6, %r57, %r58, %r50, %r52;
	mov.b32 	%f63, %r59;
	add.f32 	%f64, %f62, %f63;
	mov.b32 	%r60, %f64;
	shfl.sync.bfly.b32 	%r61|%p7, %r60, %r48, %r50, %r52;
	mov.b32 	%f65, %r61;
	add.f32 	%f66, %f64, %f65;
	mov.b32 	%r62, %f66;
	mov.u32 	%r63, 1;
	shfl.sync.bfly.b32 	%r64|%p8, %r62, %r63, %r50, %r52;
	mov.b32 	%f67, %r64;
	add.f32 	%f68, %f66, %f67;
	st.local.f32 	[%rd1], %f68;
	st.shared.f32 	[%r7], %f68;
	bar.sync 	0;
	@%p1 bra 	$L__BB126_8;

	ld.shared.f32 	%f69, [%r3];
	mov.b32 	%r65, %f69;
	shfl.sync.bfly.b32 	%r69|%p10, %r65, %r51, %r50, %r52;
	mov.b32 	%f70, %r69;
	add.f32 	%f71, %f69, %f70;
	mov.b32 	%r70, %f71;
	shfl.sync.bfly.b32 	%r72|%p11, %r70, %r55, %r50, %r52;
	mov.b32 	%f72, %r72;
	add.f32 	%f73, %f71, %f72;
	mov.b32 	%r73, %f73;
	shfl.sync.bfly.b32 	%r75|%p12, %r73, %r58, %r50, %r52;
	mov.b32 	%f74, %r75;
	add.f32 	%f75, %f73, %f74;
	mov.b32 	%r76, %f75;
	shfl.sync.bfly.b32 	%r78|%p13, %r76, %r48, %r50, %r52;
	mov.b32 	%f76, %r78;
	add.f32 	%f77, %f75, %f76;
	mov.b32 	%r79, %f77;
	shfl.sync.bfly.b32 	%r81|%p14, %r79, %r63, %r50, %r52;
	mov.b32 	%f78, %r81;
	add.f32 	%f79, %f77, %f78;
	st.local.f32 	[%rd1], %f79;

$L__BB126_8:
	bar.sync 	0;
	mov.b32 	%r82, %f219;
	shfl.sync.bfly.b32 	%r86|%p16, %r82, %r51, %r50, %r52;
	mov.b32 	%f80, %r86;
	add.f32 	%f81, %f219, %f80;
	mov.b32 	%r87, %f81;
	shfl.sync.bfly.b32 	%r89|%p17, %r87, %r55, %r50, %r52;
	mov.b32 	%f82, %r89;
	add.f32 	%f83, %f81, %f82;
	mov.b32 	%r90, %f83;
	shfl.sync.bfly.b32 	%r92|%p18, %r90, %r58, %r50, %r52;
	mov.b32 	%f84, %r92;
	add.f32 	%f85, %f83, %f84;
	mov.b32 	%r93, %f85;
	shfl.sync.bfly.b32 	%r95|%p19, %r93, %r48, %r50, %r52;
	mov.b32 	%f86, %r95;
	add.f32 	%f87, %f85, %f86;
	mov.b32 	%r96, %f87;
	shfl.sync.bfly.b32 	%r98|%p20, %r96, %r63, %r50, %r52;
	mov.b32 	%f88, %r98;
	add.f32 	%f89, %f87, %f88;
	st.local.f32 	[%rd1+4], %f89;
	st.shared.f32 	[%r7], %f89;
	bar.sync 	0;
	@%p1 bra 	$L__BB126_10;

	ld.shared.f32 	%f90, [%r3];
	mov.b32 	%r99, %f90;
	mov.u32 	%r100, 31;
	mov.u32 	%r101, 16;
	mov.u32 	%r102, -1;
	shfl.sync.bfly.b32 	%r103|%p21, %r99, %r101, %r100, %r102;
	mov.b32 	%f91, %r103;
	add.f32 	%f92, %f90, %f91;
	mov.b32 	%r104, %f92;
	mov.u32 	%r105, 8;
	shfl.sync.bfly.b32 	%r106|%p22, %r104, %r105, %r100, %r102;
	mov.b32 	%f93, %r106;
	add.f32 	%f94, %f92, %f93;
	mov.b32 	%r107, %f94;
	mov.u32 	%r108, 4;
	shfl.sync.bfly.b32 	%r109|%p23, %r107, %r108, %r100, %r102;
	mov.b32 	%f95, %r109;
	add.f32 	%f96, %f94, %f95;
	mov.b32 	%r110, %f96;
	mov.u32 	%r111, 2;
	shfl.sync.bfly.b32 	%r112|%p24, %r110, %r111, %r100, %r102;
	mov.b32 	%f97, %r112;
	add.f32 	%f98, %f96, %f97;
	mov.b32 	%r113, %f98;
	mov.u32 	%r114, 1;
	shfl.sync.bfly.b32 	%r115|%p25, %r113, %r114, %r100, %r102;
	mov.b32 	%f99, %r115;
	add.f32 	%f100, %f98, %f99;
	st.local.f32 	[%rd1+4], %f100;

$L__BB126_10:
	bar.sync 	0;
	mov.b32 	%r116, %f218;
	mov.u32 	%r117, 31;
	mov.u32 	%r118, 16;
	mov.u32 	%r119, -1;
	shfl.sync.bfly.b32 	%r120|%p27, %r116, %r118, %r117, %r119;
	mov.b32 	%f101, %r120;
	add.f32 	%f102, %f218, %f101;
	mov.b32 	%r121, %f102;
	mov.u32 	%r122, 8;
	shfl.sync.bfly.b32 	%r123|%p28, %r121, %r122, %r117, %r119;
	mov.b32 	%f103, %r123;
	add.f32 	%f104, %f102, %f103;
	mov.b32 	%r124, %f104;
	mov.u32 	%r125, 4;
	shfl.sync.bfly.b32 	%r126|%p29, %r124, %r125, %r117, %r119;
	mov.b32 	%f105, %r126;
	add.f32 	%f106, %f104, %f105;
	mov.b32 	%r127, %f106;
	mov.u32 	%r128, 2;
	shfl.sync.bfly.b32 	%r129|%p30, %r127, %r128, %r117, %r119;
	mov.b32 	%f107, %r129;
	add.f32 	%f108, %f106, %f107;
	mov.b32 	%r130, %f108;
	mov.u32 	%r131, 1;
	shfl.sync.bfly.b32 	%r132|%p31, %r130, %r131, %r117, %r119;
	mov.b32 	%f109, %r132;
	add.f32 	%f110, %f108, %f109;
	st.local.f32 	[%rd1+8], %f110;
	st.shared.f32 	[%r7], %f110;
	bar.sync 	0;
	@%p1 bra 	$L__BB126_12;

	ld.shared.f32 	%f111, [%r3];
	mov.b32 	%r133, %f111;
	shfl.sync.bfly.b32 	%r137|%p32, %r133, %r118, %r117, %r119;
	mov.b32 	%f112, %r137;
	add.f32 	%f113, %f111, %f112;
	mov.b32 	%r138, %f113;
	shfl.sync.bfly.b32 	%r140|%p33, %r138, %r122, %r117, %r119;
	mov.b32 	%f114, %r140;
	add.f32 	%f115, %f113, %f114;
	mov.b32 	%r141, %f115;
	shfl.sync.bfly.b32 	%r143|%p34, %r141, %r125, %r117, %r119;
	mov.b32 	%f116, %r143;
	add.f32 	%f117, %f115, %f116;
	mov.b32 	%r144, %f117;
	shfl.sync.bfly.b32 	%r146|%p35, %r144, %r128, %r117, %r119;
	mov.b32 	%f118, %r146;
	add.f32 	%f119, %f117, %f118;
	mov.b32 	%r147, %f119;
	shfl.sync.bfly.b32 	%r149|%p36, %r147, %r131, %r117, %r119;
	mov.b32 	%f120, %r149;
	add.f32 	%f121, %f119, %f120;
	st.local.f32 	[%rd1+8], %f121;

$L__BB126_12:
	bar.sync 	0;
	mov.b32 	%r150, %f217;
	shfl.sync.bfly.b32 	%r154|%p38, %r150, %r118, %r117, %r119;
	mov.b32 	%f122, %r154;
	add.f32 	%f123, %f217, %f122;
	mov.b32 	%r155, %f123;
	shfl.sync.bfly.b32 	%r157|%p39, %r155, %r122, %r117, %r119;
	mov.b32 	%f124, %r157;
	add.f32 	%f125, %f123, %f124;
	mov.b32 	%r158, %f125;
	shfl.sync.bfly.b32 	%r160|%p40, %r158, %r125, %r117, %r119;
	mov.b32 	%f126, %r160;
	add.f32 	%f127, %f125, %f126;
	mov.b32 	%r161, %f127;
	shfl.sync.bfly.b32 	%r163|%p41, %r161, %r128, %r117, %r119;
	mov.b32 	%f128, %r163;
	add.f32 	%f129, %f127, %f128;
	mov.b32 	%r164, %f129;
	shfl.sync.bfly.b32 	%r166|%p42, %r164, %r131, %r117, %r119;
	mov.b32 	%f130, %r166;
	add.f32 	%f131, %f129, %f130;
	st.local.f32 	[%rd1+12], %f131;
	st.shared.f32 	[%r7], %f131;
	bar.sync 	0;
	@%p1 bra 	$L__BB126_14;

	ld.shared.f32 	%f132, [%r3];
	mov.b32 	%r167, %f132;
	mov.u32 	%r168, 31;
	mov.u32 	%r169, 16;
	mov.u32 	%r170, -1;
	shfl.sync.bfly.b32 	%r171|%p43, %r167, %r169, %r168, %r170;
	mov.b32 	%f133, %r171;
	add.f32 	%f134, %f132, %f133;
	mov.b32 	%r172, %f134;
	mov.u32 	%r173, 8;
	shfl.sync.bfly.b32 	%r174|%p44, %r172, %r173, %r168, %r170;
	mov.b32 	%f135, %r174;
	add.f32 	%f136, %f134, %f135;
	mov.b32 	%r175, %f136;
	mov.u32 	%r176, 4;
	shfl.sync.bfly.b32 	%r177|%p45, %r175, %r176, %r168, %r170;
	mov.b32 	%f137, %r177;
	add.f32 	%f138, %f136, %f137;
	mov.b32 	%r178, %f138;
	mov.u32 	%r179, 2;
	shfl.sync.bfly.b32 	%r180|%p46, %r178, %r179, %r168, %r170;
	mov.b32 	%f139, %r180;
	add.f32 	%f140, %f138, %f139;
	mov.b32 	%r181, %f140;
	mov.u32 	%r182, 1;
	shfl.sync.bfly.b32 	%r183|%p47, %r181, %r182, %r168, %r170;
	mov.b32 	%f141, %r183;
	add.f32 	%f142, %f140, %f141;
	st.local.f32 	[%rd1+12], %f142;

$L__BB126_14:
	bar.sync 	0;
	mov.b32 	%r184, %f216;
	mov.u32 	%r185, 31;
	mov.u32 	%r186, 16;
	mov.u32 	%r187, -1;
	shfl.sync.bfly.b32 	%r188|%p49, %r184, %r186, %r185, %r187;
	mov.b32 	%f143, %r188;
	add.f32 	%f144, %f216, %f143;
	mov.b32 	%r189, %f144;
	mov.u32 	%r190, 8;
	shfl.sync.bfly.b32 	%r191|%p50, %r189, %r190, %r185, %r187;
	mov.b32 	%f145, %r191;
	add.f32 	%f146, %f144, %f145;
	mov.b32 	%r192, %f146;
	mov.u32 	%r193, 4;
	shfl.sync.bfly.b32 	%r194|%p51, %r192, %r193, %r185, %r187;
	mov.b32 	%f147, %r194;
	add.f32 	%f148, %f146, %f147;
	mov.b32 	%r195, %f148;
	mov.u32 	%r196, 2;
	shfl.sync.bfly.b32 	%r197|%p52, %r195, %r196, %r185, %r187;
	mov.b32 	%f149, %r197;
	add.f32 	%f150, %f148, %f149;
	mov.b32 	%r198, %f150;
	mov.u32 	%r199, 1;
	shfl.sync.bfly.b32 	%r200|%p53, %r198, %r199, %r185, %r187;
	mov.b32 	%f151, %r200;
	add.f32 	%f152, %f150, %f151;
	st.local.f32 	[%rd1+16], %f152;
	st.shared.f32 	[%r7], %f152;
	bar.sync 	0;
	@%p1 bra 	$L__BB126_16;

	ld.shared.f32 	%f153, [%r3];
	mov.b32 	%r201, %f153;
	shfl.sync.bfly.b32 	%r205|%p54, %r201, %r186, %r185, %r187;
	mov.b32 	%f154, %r205;
	add.f32 	%f155, %f153, %f154;
	mov.b32 	%r206, %f155;
	shfl.sync.bfly.b32 	%r208|%p55, %r206, %r190, %r185, %r187;
	mov.b32 	%f156, %r208;
	add.f32 	%f157, %f155, %f156;
	mov.b32 	%r209, %f157;
	shfl.sync.bfly.b32 	%r211|%p56, %r209, %r193, %r185, %r187;
	mov.b32 	%f158, %r211;
	add.f32 	%f159, %f157, %f158;
	mov.b32 	%r212, %f159;
	shfl.sync.bfly.b32 	%r214|%p57, %r212, %r196, %r185, %r187;
	mov.b32 	%f160, %r214;
	add.f32 	%f161, %f159, %f160;
	mov.b32 	%r215, %f161;
	shfl.sync.bfly.b32 	%r217|%p58, %r215, %r199, %r185, %r187;
	mov.b32 	%f162, %r217;
	add.f32 	%f163, %f161, %f162;
	st.local.f32 	[%rd1+16], %f163;

$L__BB126_16:
	bar.sync 	0;
	mov.b32 	%r218, %f215;
	shfl.sync.bfly.b32 	%r222|%p60, %r218, %r186, %r185, %r187;
	mov.b32 	%f164, %r222;
	add.f32 	%f165, %f215, %f164;
	mov.b32 	%r223, %f165;
	shfl.sync.bfly.b32 	%r225|%p61, %r223, %r190, %r185, %r187;
	mov.b32 	%f166, %r225;
	add.f32 	%f167, %f165, %f166;
	mov.b32 	%r226, %f167;
	shfl.sync.bfly.b32 	%r228|%p62, %r226, %r193, %r185, %r187;
	mov.b32 	%f168, %r228;
	add.f32 	%f169, %f167, %f168;
	mov.b32 	%r229, %f169;
	shfl.sync.bfly.b32 	%r231|%p63, %r229, %r196, %r185, %r187;
	mov.b32 	%f170, %r231;
	add.f32 	%f171, %f169, %f170;
	mov.b32 	%r232, %f171;
	shfl.sync.bfly.b32 	%r234|%p64, %r232, %r199, %r185, %r187;
	mov.b32 	%f172, %r234;
	add.f32 	%f173, %f171, %f172;
	st.local.f32 	[%rd1+20], %f173;
	st.shared.f32 	[%r7], %f173;
	bar.sync 	0;
	@%p1 bra 	$L__BB126_18;

	ld.shared.f32 	%f174, [%r3];
	mov.b32 	%r235, %f174;
	mov.u32 	%r236, 31;
	mov.u32 	%r237, 16;
	mov.u32 	%r238, -1;
	shfl.sync.bfly.b32 	%r239|%p65, %r235, %r237, %r236, %r238;
	mov.b32 	%f175, %r239;
	add.f32 	%f176, %f174, %f175;
	mov.b32 	%r240, %f176;
	mov.u32 	%r241, 8;
	shfl.sync.bfly.b32 	%r242|%p66, %r240, %r241, %r236, %r238;
	mov.b32 	%f177, %r242;
	add.f32 	%f178, %f176, %f177;
	mov.b32 	%r243, %f178;
	mov.u32 	%r244, 4;
	shfl.sync.bfly.b32 	%r245|%p67, %r243, %r244, %r236, %r238;
	mov.b32 	%f179, %r245;
	add.f32 	%f180, %f178, %f179;
	mov.b32 	%r246, %f180;
	mov.u32 	%r247, 2;
	shfl.sync.bfly.b32 	%r248|%p68, %r246, %r247, %r236, %r238;
	mov.b32 	%f181, %r248;
	add.f32 	%f182, %f180, %f181;
	mov.b32 	%r249, %f182;
	mov.u32 	%r250, 1;
	shfl.sync.bfly.b32 	%r251|%p69, %r249, %r250, %r236, %r238;
	mov.b32 	%f183, %r251;
	add.f32 	%f184, %f182, %f183;
	st.local.f32 	[%rd1+20], %f184;

$L__BB126_18:
	bar.sync 	0;
	mov.b32 	%r252, %f214;
	mov.u32 	%r253, 31;
	mov.u32 	%r254, 16;
	mov.u32 	%r255, -1;
	shfl.sync.bfly.b32 	%r256|%p71, %r252, %r254, %r253, %r255;
	mov.b32 	%f185, %r256;
	add.f32 	%f186, %f214, %f185;
	mov.b32 	%r257, %f186;
	mov.u32 	%r258, 8;
	shfl.sync.bfly.b32 	%r259|%p72, %r257, %r258, %r253, %r255;
	mov.b32 	%f187, %r259;
	add.f32 	%f188, %f186, %f187;
	mov.b32 	%r260, %f188;
	mov.u32 	%r261, 4;
	shfl.sync.bfly.b32 	%r262|%p73, %r260, %r261, %r253, %r255;
	mov.b32 	%f189, %r262;
	add.f32 	%f190, %f188, %f189;
	mov.b32 	%r263, %f190;
	mov.u32 	%r264, 2;
	shfl.sync.bfly.b32 	%r265|%p74, %r263, %r264, %r253, %r255;
	mov.b32 	%f191, %r265;
	add.f32 	%f192, %f190, %f191;
	mov.b32 	%r266, %f192;
	mov.u32 	%r267, 1;
	shfl.sync.bfly.b32 	%r268|%p75, %r266, %r267, %r253, %r255;
	mov.b32 	%f193, %r268;
	add.f32 	%f194, %f192, %f193;
	st.local.f32 	[%rd1+24], %f194;
	st.shared.f32 	[%r7], %f194;
	bar.sync 	0;
	@%p1 bra 	$L__BB126_20;

	ld.shared.f32 	%f195, [%r3];
	mov.b32 	%r269, %f195;
	shfl.sync.bfly.b32 	%r273|%p76, %r269, %r254, %r253, %r255;
	mov.b32 	%f196, %r273;
	add.f32 	%f197, %f195, %f196;
	mov.b32 	%r274, %f197;
	shfl.sync.bfly.b32 	%r276|%p77, %r274, %r258, %r253, %r255;
	mov.b32 	%f198, %r276;
	add.f32 	%f199, %f197, %f198;
	mov.b32 	%r277, %f199;
	shfl.sync.bfly.b32 	%r279|%p78, %r277, %r261, %r253, %r255;
	mov.b32 	%f200, %r279;
	add.f32 	%f201, %f199, %f200;
	mov.b32 	%r280, %f201;
	shfl.sync.bfly.b32 	%r282|%p79, %r280, %r264, %r253, %r255;
	mov.b32 	%f202, %r282;
	add.f32 	%f203, %f201, %f202;
	mov.b32 	%r283, %f203;
	shfl.sync.bfly.b32 	%r285|%p80, %r283, %r267, %r253, %r255;
	mov.b32 	%f204, %r285;
	add.f32 	%f205, %f203, %f204;
	st.local.f32 	[%rd1+24], %f205;

$L__BB126_20:
	bar.sync 	0;
	setp.gt.s32 	%p81, %r2, 6;
	@%p81 bra 	$L__BB126_22;

	mad.lo.s32 	%r286, %r2, %r11, %r1;
	cvt.s64.s32 	%rd33, %r286;
	mul.lo.s32 	%r287, %r4, %r15;
	cvt.s64.s32 	%rd34, %r287;
	add.s64 	%rd35, %rd34, %rd33;
	mul.wide.s32 	%rd36, %r2, 4;
	add.s64 	%rd37, %rd1, %rd36;
	ld.local.f32 	%f206, [%rd37];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f206;}

	// end inline asm
	cvta.to.global.u64 	%rd38, %rd15;
	shl.b64 	%rd39, %rd35, 1;
	add.s64 	%rd40, %rd38, %rd39;
	st.global.u16 	[%rd40], %rs1;

$L__BB126_22:
	ret;

}
	// .globl	ggml_matvec_f16_ncols_8_bs_256
.visible .entry ggml_matvec_f16_ncols_8_bs_256(
	.param .u64 ggml_matvec_f16_ncols_8_bs_256_param_0,
	.param .u64 ggml_matvec_f16_ncols_8_bs_256_param_1,
	.param .u64 ggml_matvec_f16_ncols_8_bs_256_param_2,
	.param .u32 ggml_matvec_f16_ncols_8_bs_256_param_3,
	.param .u32 ggml_matvec_f16_ncols_8_bs_256_param_4,
	.param .u32 ggml_matvec_f16_ncols_8_bs_256_param_5,
	.param .u32 ggml_matvec_f16_ncols_8_bs_256_param_6,
	.param .u32 ggml_matvec_f16_ncols_8_bs_256_param_7,
	.param .u32 ggml_matvec_f16_ncols_8_bs_256_param_8,
	.param .u32 ggml_matvec_f16_ncols_8_bs_256_param_9,
	.param .u32 ggml_matvec_f16_ncols_8_bs_256_param_10,
	.param .u32 ggml_matvec_f16_ncols_8_bs_256_param_11
)
{
	.local .align 16 .b8 	__local_depot127[32];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<93>;
	.reg .b16 	%rs<2>;
	.reg .f32 	%f<252>;
	.reg .b32 	%r<324>;
	.reg .b64 	%rd<45>;


	mov.u64 	%SPL, __local_depot127;
	ld.param.u64 	%rd14, [ggml_matvec_f16_ncols_8_bs_256_param_0];
	ld.param.u64 	%rd15, [ggml_matvec_f16_ncols_8_bs_256_param_1];
	ld.param.u64 	%rd16, [ggml_matvec_f16_ncols_8_bs_256_param_2];
	ld.param.u32 	%r8, [ggml_matvec_f16_ncols_8_bs_256_param_3];
	ld.param.u32 	%r9, [ggml_matvec_f16_ncols_8_bs_256_param_5];
	ld.param.u32 	%r10, [ggml_matvec_f16_ncols_8_bs_256_param_6];
	ld.param.u32 	%r11, [ggml_matvec_f16_ncols_8_bs_256_param_7];
	ld.param.u32 	%r12, [ggml_matvec_f16_ncols_8_bs_256_param_8];
	ld.param.u32 	%r13, [ggml_matvec_f16_ncols_8_bs_256_param_9];
	ld.param.u32 	%r14, [ggml_matvec_f16_ncols_8_bs_256_param_10];
	ld.param.u32 	%r15, [ggml_matvec_f16_ncols_8_bs_256_param_11];
	add.u64 	%rd1, %SPL, 0;
	mov.u32 	%r1, %ctaid.x;
	mov.u32 	%r2, %tid.x;
	setp.gt.s32 	%p1, %r2, 31;
	shl.b32 	%r16, %r2, 2;
	mov.u32 	%r17, data_mmv;
	add.s32 	%r3, %r17, %r16;
	@%p1 bra 	$L__BB127_2;

	mov.u32 	%r18, 0;
	st.shared.u32 	[%r3], %r18;

$L__BB127_2:
	mov.u32 	%r4, %ctaid.y;
	bar.sync 	0;
	mov.f32 	%f244, 0f00000000;
	st.local.v4.f32 	[%rd1], {%f244, %f244, %f244, %f244};
	st.local.v4.f32 	[%rd1+16], {%f244, %f244, %f244, %f244};
	setp.ge.s32 	%p2, %r2, %r8;
	mov.f32 	%f245, %f244;
	mov.f32 	%f246, %f244;
	mov.f32 	%f247, %f244;
	mov.f32 	%f248, %f244;
	mov.f32 	%f249, %f244;
	mov.f32 	%f250, %f244;
	mov.f32 	%f251, %f244;
	@%p2 bra 	$L__BB127_6;

	shl.b32 	%r19, %r10, 1;
	add.s32 	%r20, %r2, %r19;
	mul.wide.s32 	%rd18, %r20, 4;
	mul.lo.s32 	%r21, %r4, %r14;
	mul.wide.s32 	%rd19, %r21, 2;
	add.s64 	%rd4, %rd18, %rd19;
	mul.wide.s32 	%rd20, %r2, 4;
	mul.wide.s32 	%rd5, %r10, 4;
	add.s64 	%rd21, %rd20, %rd5;
	add.s64 	%rd6, %rd21, %rd19;
	add.s64 	%rd7, %rd20, %rd19;
	mul.wide.s32 	%rd22, %r2, 2;
	div.s32 	%r22, %r4, %r12;
	mul.lo.s32 	%r23, %r1, %r9;
	mad.lo.s32 	%r24, %r22, %r13, %r23;
	cvt.s64.s32 	%rd23, %r24;
	add.s64 	%rd24, %rd22, %rd23;
	cvta.to.global.u64 	%rd25, %rd14;
	shl.b64 	%rd26, %rd24, 1;
	add.s64 	%rd43, %rd25, %rd26;
	cvta.to.global.u64 	%rd44, %rd15;
	mov.f32 	%f244, 0f00000000;
	mov.u32 	%r323, %r2;

$L__BB127_4:
	ld.global.nc.u32 	%r25, [%rd43];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r25;
  cvt.f32.f16 %f41, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r25;
  cvt.f32.f16 %f42, high;}

	// end inline asm
	add.s64 	%rd27, %rd44, %rd7;
	ld.global.nc.u32 	%r27, [%rd27];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r27;
  cvt.f32.f16 %f43, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r27;
  cvt.f32.f16 %f44, high;}

	// end inline asm
	fma.rn.f32 	%f59, %f41, %f43, %f251;
	fma.rn.f32 	%f251, %f42, %f44, %f59;
	add.s64 	%rd28, %rd44, %rd6;
	ld.global.nc.u32 	%r29, [%rd28];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r29;
  cvt.f32.f16 %f45, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r29;
  cvt.f32.f16 %f46, high;}

	// end inline asm
	fma.rn.f32 	%f60, %f41, %f45, %f250;
	fma.rn.f32 	%f250, %f42, %f46, %f60;
	add.s64 	%rd29, %rd44, %rd4;
	ld.global.nc.u32 	%r31, [%rd29];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r31;
  cvt.f32.f16 %f47, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r31;
  cvt.f32.f16 %f48, high;}

	// end inline asm
	fma.rn.f32 	%f61, %f41, %f47, %f249;
	fma.rn.f32 	%f249, %f42, %f48, %f61;
	add.s64 	%rd30, %rd29, %rd5;
	ld.global.nc.u32 	%r33, [%rd30];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r33;
  cvt.f32.f16 %f49, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r33;
  cvt.f32.f16 %f50, high;}

	// end inline asm
	fma.rn.f32 	%f62, %f41, %f49, %f248;
	fma.rn.f32 	%f248, %f42, %f50, %f62;
	add.s64 	%rd31, %rd30, %rd5;
	ld.global.nc.u32 	%r35, [%rd31];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r35;
  cvt.f32.f16 %f51, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r35;
  cvt.f32.f16 %f52, high;}

	// end inline asm
	fma.rn.f32 	%f63, %f41, %f51, %f247;
	fma.rn.f32 	%f247, %f42, %f52, %f63;
	add.s64 	%rd32, %rd31, %rd5;
	ld.global.nc.u32 	%r37, [%rd32];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r37;
  cvt.f32.f16 %f53, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r37;
  cvt.f32.f16 %f54, high;}

	// end inline asm
	fma.rn.f32 	%f64, %f41, %f53, %f246;
	fma.rn.f32 	%f246, %f42, %f54, %f64;
	add.s64 	%rd33, %rd32, %rd5;
	ld.global.nc.u32 	%r39, [%rd33];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r39;
  cvt.f32.f16 %f55, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r39;
  cvt.f32.f16 %f56, high;}

	// end inline asm
	fma.rn.f32 	%f65, %f41, %f55, %f245;
	fma.rn.f32 	%f245, %f42, %f56, %f65;
	add.s64 	%rd34, %rd33, %rd5;
	ld.global.nc.u32 	%r41, [%rd34];
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r41;
  cvt.f32.f16 %f57, low;}

	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
  mov.b32 {low,high},%r41;
  cvt.f32.f16 %f58, high;}

	// end inline asm
	fma.rn.f32 	%f66, %f41, %f57, %f244;
	fma.rn.f32 	%f244, %f42, %f58, %f66;
	add.s64 	%rd44, %rd44, 1024;
	add.s64 	%rd43, %rd43, 1024;
	add.s32 	%r323, %r323, 256;
	setp.lt.s32 	%p3, %r323, %r8;
	@%p3 bra 	$L__BB127_4;

	st.local.v4.f32 	[%rd1], {%f251, %f250, %f249, %f248};
	st.local.v4.f32 	[%rd1+16], {%f247, %f246, %f245, %f244};

$L__BB127_6:
	shr.s32 	%r43, %r2, 31;
	shr.u32 	%r44, %r43, 27;
	add.s32 	%r45, %r2, %r44;
	shr.s32 	%r46, %r45, 5;
	shl.b32 	%r47, %r46, 2;
	add.s32 	%r7, %r17, %r47;
	mov.u32 	%r49, 2;
	mov.b32 	%r50, %f251;
	mov.u32 	%r51, 31;
	mov.u32 	%r52, 16;
	mov.u32 	%r53, -1;
	shfl.sync.bfly.b32 	%r54|%p4, %r50, %r52, %r51, %r53;
	mov.b32 	%f67, %r54;
	add.f32 	%f68, %f251, %f67;
	mov.b32 	%r55, %f68;
	mov.u32 	%r56, 8;
	shfl.sync.bfly.b32 	%r57|%p5, %r55, %r56, %r51, %r53;
	mov.b32 	%f69, %r57;
	add.f32 	%f70, %f68, %f69;
	mov.b32 	%r58, %f70;
	mov.u32 	%r59, 4;
	shfl.sync.bfly.b32 	%r60|%p6, %r58, %r59, %r51, %r53;
	mov.b32 	%f71, %r60;
	add.f32 	%f72, %f70, %f71;
	mov.b32 	%r61, %f72;
	shfl.sync.bfly.b32 	%r62|%p7, %r61, %r49, %r51, %r53;
	mov.b32 	%f73, %r62;
	add.f32 	%f74, %f72, %f73;
	mov.b32 	%r63, %f74;
	mov.u32 	%r64, 1;
	shfl.sync.bfly.b32 	%r65|%p8, %r63, %r64, %r51, %r53;
	mov.b32 	%f75, %r65;
	add.f32 	%f76, %f74, %f75;
	st.local.f32 	[%rd1], %f76;
	st.shared.f32 	[%r7], %f76;
	bar.sync 	0;
	@%p1 bra 	$L__BB127_8;

	ld.shared.f32 	%f77, [%r3];
	mov.b32 	%r66, %f77;
	shfl.sync.bfly.b32 	%r70|%p10, %r66, %r52, %r51, %r53;
	mov.b32 	%f78, %r70;
	add.f32 	%f79, %f77, %f78;
	mov.b32 	%r71, %f79;
	shfl.sync.bfly.b32 	%r73|%p11, %r71, %r56, %r51, %r53;
	mov.b32 	%f80, %r73;
	add.f32 	%f81, %f79, %f80;
	mov.b32 	%r74, %f81;
	shfl.sync.bfly.b32 	%r76|%p12, %r74, %r59, %r51, %r53;
	mov.b32 	%f82, %r76;
	add.f32 	%f83, %f81, %f82;
	mov.b32 	%r77, %f83;
	shfl.sync.bfly.b32 	%r79|%p13, %r77, %r49, %r51, %r53;
	mov.b32 	%f84, %r79;
	add.f32 	%f85, %f83, %f84;
	mov.b32 	%r80, %f85;
	shfl.sync.bfly.b32 	%r82|%p14, %r80, %r64, %r51, %r53;
	mov.b32 	%f86, %r82;
	add.f32 	%f87, %f85, %f86;
	st.local.f32 	[%rd1], %f87;

$L__BB127_8:
	bar.sync 	0;
	mov.b32 	%r83, %f250;
	shfl.sync.bfly.b32 	%r87|%p16, %r83, %r52, %r51, %r53;
	mov.b32 	%f88, %r87;
	add.f32 	%f89, %f250, %f88;
	mov.b32 	%r88, %f89;
	shfl.sync.bfly.b32 	%r90|%p17, %r88, %r56, %r51, %r53;
	mov.b32 	%f90, %r90;
	add.f32 	%f91, %f89, %f90;
	mov.b32 	%r91, %f91;
	shfl.sync.bfly.b32 	%r93|%p18, %r91, %r59, %r51, %r53;
	mov.b32 	%f92, %r93;
	add.f32 	%f93, %f91, %f92;
	mov.b32 	%r94, %f93;
	shfl.sync.bfly.b32 	%r96|%p19, %r94, %r49, %r51, %r53;
	mov.b32 	%f94, %r96;
	add.f32 	%f95, %f93, %f94;
	mov.b32 	%r97, %f95;
	shfl.sync.bfly.b32 	%r99|%p20, %r97, %r64, %r51, %r53;
	mov.b32 	%f96, %r99;
	add.f32 	%f97, %f95, %f96;
	st.local.f32 	[%rd1+4], %f97;
	st.shared.f32 	[%r7], %f97;
	bar.sync 	0;
	@%p1 bra 	$L__BB127_10;

	ld.shared.f32 	%f98, [%r3];
	mov.b32 	%r100, %f98;
	mov.u32 	%r101, 31;
	mov.u32 	%r102, 16;
	mov.u32 	%r103, -1;
	shfl.sync.bfly.b32 	%r104|%p21, %r100, %r102, %r101, %r103;
	mov.b32 	%f99, %r104;
	add.f32 	%f100, %f98, %f99;
	mov.b32 	%r105, %f100;
	mov.u32 	%r106, 8;
	shfl.sync.bfly.b32 	%r107|%p22, %r105, %r106, %r101, %r103;
	mov.b32 	%f101, %r107;
	add.f32 	%f102, %f100, %f101;
	mov.b32 	%r108, %f102;
	mov.u32 	%r109, 4;
	shfl.sync.bfly.b32 	%r110|%p23, %r108, %r109, %r101, %r103;
	mov.b32 	%f103, %r110;
	add.f32 	%f104, %f102, %f103;
	mov.b32 	%r111, %f104;
	mov.u32 	%r112, 2;
	shfl.sync.bfly.b32 	%r113|%p24, %r111, %r112, %r101, %r103;
	mov.b32 	%f105, %r113;
	add.f32 	%f106, %f104, %f105;
	mov.b32 	%r114, %f106;
	mov.u32 	%r115, 1;
	shfl.sync.bfly.b32 	%r116|%p25, %r114, %r115, %r101, %r103;
	mov.b32 	%f107, %r116;
	add.f32 	%f108, %f106, %f107;
	st.local.f32 	[%rd1+4], %f108;

$L__BB127_10:
	bar.sync 	0;
	mov.b32 	%r117, %f249;
	mov.u32 	%r118, 31;
	mov.u32 	%r119, 16;
	mov.u32 	%r120, -1;
	shfl.sync.bfly.b32 	%r121|%p27, %r117, %r119, %r118, %r120;
	mov.b32 	%f109, %r121;
	add.f32 	%f110, %f249, %f109;
	mov.b32 	%r122, %f110;
	mov.u32 	%r123, 8;
	shfl.sync.bfly.b32 	%r124|%p28, %r122, %r123, %r118, %r120;
	mov.b32 	%f111, %r124;
	add.f32 	%f112, %f110, %f111;
	mov.b32 	%r125, %f112;
	mov.u32 	%r126, 4;
	shfl.sync.bfly.b32 	%r127|%p29, %r125, %r126, %r118, %r120;
	mov.b32 	%f113, %r127;
	add.f32 	%f114, %f112, %f113;
	mov.b32 	%r128, %f114;
	mov.u32 	%r129, 2;
	shfl.sync.bfly.b32 	%r130|%p30, %r128, %r129, %r118, %r120;
	mov.b32 	%f115, %r130;
	add.f32 	%f116, %f114, %f115;
	mov.b32 	%r131, %f116;
	mov.u32 	%r132, 1;
	shfl.sync.bfly.b32 	%r133|%p31, %r131, %r132, %r118, %r120;
	mov.b32 	%f117, %r133;
	add.f32 	%f118, %f116, %f117;
	st.local.f32 	[%rd1+8], %f118;
	st.shared.f32 	[%r7], %f118;
	bar.sync 	0;
	@%p1 bra 	$L__BB127_12;

	ld.shared.f32 	%f119, [%r3];
	mov.b32 	%r134, %f119;
	shfl.sync.bfly.b32 	%r138|%p32, %r134, %r119, %r118, %r120;
	mov.b32 	%f120, %r138;
	add.f32 	%f121, %f119, %f120;
	mov.b32 	%r139, %f121;
	shfl.sync.bfly.b32 	%r141|%p33, %r139, %r123, %r118, %r120;
	mov.b32 	%f122, %r141;
	add.f32 	%f123, %f121, %f122;
	mov.b32 	%r142, %f123;
	shfl.sync.bfly.b32 	%r144|%p34, %r142, %r126, %r118, %r120;
	mov.b32 	%f124, %r144;
	add.f32 	%f125, %f123, %f124;
	mov.b32 	%r145, %f125;
	shfl.sync.bfly.b32 	%r147|%p35, %r145, %r129, %r118, %r120;
	mov.b32 	%f126, %r147;
	add.f32 	%f127, %f125, %f126;
	mov.b32 	%r148, %f127;
	shfl.sync.bfly.b32 	%r150|%p36, %r148, %r132, %r118, %r120;
	mov.b32 	%f128, %r150;
	add.f32 	%f129, %f127, %f128;
	st.local.f32 	[%rd1+8], %f129;

$L__BB127_12:
	bar.sync 	0;
	mov.b32 	%r151, %f248;
	shfl.sync.bfly.b32 	%r155|%p38, %r151, %r119, %r118, %r120;
	mov.b32 	%f130, %r155;
	add.f32 	%f131, %f248, %f130;
	mov.b32 	%r156, %f131;
	shfl.sync.bfly.b32 	%r158|%p39, %r156, %r123, %r118, %r120;
	mov.b32 	%f132, %r158;
	add.f32 	%f133, %f131, %f132;
	mov.b32 	%r159, %f133;
	shfl.sync.bfly.b32 	%r161|%p40, %r159, %r126, %r118, %r120;
	mov.b32 	%f134, %r161;
	add.f32 	%f135, %f133, %f134;
	mov.b32 	%r162, %f135;
	shfl.sync.bfly.b32 	%r164|%p41, %r162, %r129, %r118, %r120;
	mov.b32 	%f136, %r164;
	add.f32 	%f137, %f135, %f136;
	mov.b32 	%r165, %f137;
	shfl.sync.bfly.b32 	%r167|%p42, %r165, %r132, %r118, %r120;
	mov.b32 	%f138, %r167;
	add.f32 	%f139, %f137, %f138;
	st.local.f32 	[%rd1+12], %f139;
	st.shared.f32 	[%r7], %f139;
	bar.sync 	0;
	@%p1 bra 	$L__BB127_14;

	ld.shared.f32 	%f140, [%r3];
	mov.b32 	%r168, %f140;
	mov.u32 	%r169, 31;
	mov.u32 	%r170, 16;
	mov.u32 	%r171, -1;
	shfl.sync.bfly.b32 	%r172|%p43, %r168, %r170, %r169, %r171;
	mov.b32 	%f141, %r172;
	add.f32 	%f142, %f140, %f141;
	mov.b32 	%r173, %f142;
	mov.u32 	%r174, 8;
	shfl.sync.bfly.b32 	%r175|%p44, %r173, %r174, %r169, %r171;
	mov.b32 	%f143, %r175;
	add.f32 	%f144, %f142, %f143;
	mov.b32 	%r176, %f144;
	mov.u32 	%r177, 4;
	shfl.sync.bfly.b32 	%r178|%p45, %r176, %r177, %r169, %r171;
	mov.b32 	%f145, %r178;
	add.f32 	%f146, %f144, %f145;
	mov.b32 	%r179, %f146;
	mov.u32 	%r180, 2;
	shfl.sync.bfly.b32 	%r181|%p46, %r179, %r180, %r169, %r171;
	mov.b32 	%f147, %r181;
	add.f32 	%f148, %f146, %f147;
	mov.b32 	%r182, %f148;
	mov.u32 	%r183, 1;
	shfl.sync.bfly.b32 	%r184|%p47, %r182, %r183, %r169, %r171;
	mov.b32 	%f149, %r184;
	add.f32 	%f150, %f148, %f149;
	st.local.f32 	[%rd1+12], %f150;

$L__BB127_14:
	bar.sync 	0;
	mov.b32 	%r185, %f247;
	mov.u32 	%r186, 31;
	mov.u32 	%r187, 16;
	mov.u32 	%r188, -1;
	shfl.sync.bfly.b32 	%r189|%p49, %r185, %r187, %r186, %r188;
	mov.b32 	%f151, %r189;
	add.f32 	%f152, %f247, %f151;
	mov.b32 	%r190, %f152;
	mov.u32 	%r191, 8;
	shfl.sync.bfly.b32 	%r192|%p50, %r190, %r191, %r186, %r188;
	mov.b32 	%f153, %r192;
	add.f32 	%f154, %f152, %f153;
	mov.b32 	%r193, %f154;
	mov.u32 	%r194, 4;
	shfl.sync.bfly.b32 	%r195|%p51, %r193, %r194, %r186, %r188;
	mov.b32 	%f155, %r195;
	add.f32 	%f156, %f154, %f155;
	mov.b32 	%r196, %f156;
	mov.u32 	%r197, 2;
	shfl.sync.bfly.b32 	%r198|%p52, %r196, %r197, %r186, %r188;
	mov.b32 	%f157, %r198;
	add.f32 	%f158, %f156, %f157;
	mov.b32 	%r199, %f158;
	mov.u32 	%r200, 1;
	shfl.sync.bfly.b32 	%r201|%p53, %r199, %r200, %r186, %r188;
	mov.b32 	%f159, %r201;
	add.f32 	%f160, %f158, %f159;
	st.local.f32 	[%rd1+16], %f160;
	st.shared.f32 	[%r7], %f160;
	bar.sync 	0;
	@%p1 bra 	$L__BB127_16;

	ld.shared.f32 	%f161, [%r3];
	mov.b32 	%r202, %f161;
	shfl.sync.bfly.b32 	%r206|%p54, %r202, %r187, %r186, %r188;
	mov.b32 	%f162, %r206;
	add.f32 	%f163, %f161, %f162;
	mov.b32 	%r207, %f163;
	shfl.sync.bfly.b32 	%r209|%p55, %r207, %r191, %r186, %r188;
	mov.b32 	%f164, %r209;
	add.f32 	%f165, %f163, %f164;
	mov.b32 	%r210, %f165;
	shfl.sync.bfly.b32 	%r212|%p56, %r210, %r194, %r186, %r188;
	mov.b32 	%f166, %r212;
	add.f32 	%f167, %f165, %f166;
	mov.b32 	%r213, %f167;
	shfl.sync.bfly.b32 	%r215|%p57, %r213, %r197, %r186, %r188;
	mov.b32 	%f168, %r215;
	add.f32 	%f169, %f167, %f168;
	mov.b32 	%r216, %f169;
	shfl.sync.bfly.b32 	%r218|%p58, %r216, %r200, %r186, %r188;
	mov.b32 	%f170, %r218;
	add.f32 	%f171, %f169, %f170;
	st.local.f32 	[%rd1+16], %f171;

$L__BB127_16:
	bar.sync 	0;
	mov.b32 	%r219, %f246;
	shfl.sync.bfly.b32 	%r223|%p60, %r219, %r187, %r186, %r188;
	mov.b32 	%f172, %r223;
	add.f32 	%f173, %f246, %f172;
	mov.b32 	%r224, %f173;
	shfl.sync.bfly.b32 	%r226|%p61, %r224, %r191, %r186, %r188;
	mov.b32 	%f174, %r226;
	add.f32 	%f175, %f173, %f174;
	mov.b32 	%r227, %f175;
	shfl.sync.bfly.b32 	%r229|%p62, %r227, %r194, %r186, %r188;
	mov.b32 	%f176, %r229;
	add.f32 	%f177, %f175, %f176;
	mov.b32 	%r230, %f177;
	shfl.sync.bfly.b32 	%r232|%p63, %r230, %r197, %r186, %r188;
	mov.b32 	%f178, %r232;
	add.f32 	%f179, %f177, %f178;
	mov.b32 	%r233, %f179;
	shfl.sync.bfly.b32 	%r235|%p64, %r233, %r200, %r186, %r188;
	mov.b32 	%f180, %r235;
	add.f32 	%f181, %f179, %f180;
	st.local.f32 	[%rd1+20], %f181;
	st.shared.f32 	[%r7], %f181;
	bar.sync 	0;
	@%p1 bra 	$L__BB127_18;

	ld.shared.f32 	%f182, [%r3];
	mov.b32 	%r236, %f182;
	mov.u32 	%r237, 31;
	mov.u32 	%r238, 16;
	mov.u32 	%r239, -1;
	shfl.sync.bfly.b32 	%r240|%p65, %r236, %r238, %r237, %r239;
	mov.b32 	%f183, %r240;
	add.f32 	%f184, %f182, %f183;
	mov.b32 	%r241, %f184;
	mov.u32 	%r242, 8;
	shfl.sync.bfly.b32 	%r243|%p66, %r241, %r242, %r237, %r239;
	mov.b32 	%f185, %r243;
	add.f32 	%f186, %f184, %f185;
	mov.b32 	%r244, %f186;
	mov.u32 	%r245, 4;
	shfl.sync.bfly.b32 	%r246|%p67, %r244, %r245, %r237, %r239;
	mov.b32 	%f187, %r246;
	add.f32 	%f188, %f186, %f187;
	mov.b32 	%r247, %f188;
	mov.u32 	%r248, 2;
	shfl.sync.bfly.b32 	%r249|%p68, %r247, %r248, %r237, %r239;
	mov.b32 	%f189, %r249;
	add.f32 	%f190, %f188, %f189;
	mov.b32 	%r250, %f190;
	mov.u32 	%r251, 1;
	shfl.sync.bfly.b32 	%r252|%p69, %r250, %r251, %r237, %r239;
	mov.b32 	%f191, %r252;
	add.f32 	%f192, %f190, %f191;
	st.local.f32 	[%rd1+20], %f192;

$L__BB127_18:
	bar.sync 	0;
	mov.b32 	%r253, %f245;
	mov.u32 	%r254, 31;
	mov.u32 	%r255, 16;
	mov.u32 	%r256, -1;
	shfl.sync.bfly.b32 	%r257|%p71, %r253, %r255, %r254, %r256;
	mov.b32 	%f193, %r257;
	add.f32 	%f194, %f245, %f193;
	mov.b32 	%r258, %f194;
	mov.u32 	%r259, 8;
	shfl.sync.bfly.b32 	%r260|%p72, %r258, %r259, %r254, %r256;
	mov.b32 	%f195, %r260;
	add.f32 	%f196, %f194, %f195;
	mov.b32 	%r261, %f196;
	mov.u32 	%r262, 4;
	shfl.sync.bfly.b32 	%r263|%p73, %r261, %r262, %r254, %r256;
	mov.b32 	%f197, %r263;
	add.f32 	%f198, %f196, %f197;
	mov.b32 	%r264, %f198;
	mov.u32 	%r265, 2;
	shfl.sync.bfly.b32 	%r266|%p74, %r264, %r265, %r254, %r256;
	mov.b32 	%f199, %r266;
	add.f32 	%f200, %f198, %f199;
	mov.b32 	%r267, %f200;
	mov.u32 	%r268, 1;
	shfl.sync.bfly.b32 	%r269|%p75, %r267, %r268, %r254, %r256;
	mov.b32 	%f201, %r269;
	add.f32 	%f202, %f200, %f201;
	st.local.f32 	[%rd1+24], %f202;
	st.shared.f32 	[%r7], %f202;
	bar.sync 	0;
	@%p1 bra 	$L__BB127_20;

	ld.shared.f32 	%f203, [%r3];
	mov.b32 	%r270, %f203;
	shfl.sync.bfly.b32 	%r274|%p76, %r270, %r255, %r254, %r256;
	mov.b32 	%f204, %r274;
	add.f32 	%f205, %f203, %f204;
	mov.b32 	%r275, %f205;
	shfl.sync.bfly.b32 	%r277|%p77, %r275, %r259, %r254, %r256;
	mov.b32 	%f206, %r277;
	add.f32 	%f207, %f205, %f206;
	mov.b32 	%r278, %f207;
	shfl.sync.bfly.b32 	%r280|%p78, %r278, %r262, %r254, %r256;
	mov.b32 	%f208, %r280;
	add.f32 	%f209, %f207, %f208;
	mov.b32 	%r281, %f209;
	shfl.sync.bfly.b32 	%r283|%p79, %r281, %r265, %r254, %r256;
	mov.b32 	%f210, %r283;
	add.f32 	%f211, %f209, %f210;
	mov.b32 	%r284, %f211;
	shfl.sync.bfly.b32 	%r286|%p80, %r284, %r268, %r254, %r256;
	mov.b32 	%f212, %r286;
	add.f32 	%f213, %f211, %f212;
	st.local.f32 	[%rd1+24], %f213;

$L__BB127_20:
	bar.sync 	0;
	mov.b32 	%r287, %f244;
	shfl.sync.bfly.b32 	%r291|%p82, %r287, %r255, %r254, %r256;
	mov.b32 	%f214, %r291;
	add.f32 	%f215, %f244, %f214;
	mov.b32 	%r292, %f215;
	shfl.sync.bfly.b32 	%r294|%p83, %r292, %r259, %r254, %r256;
	mov.b32 	%f216, %r294;
	add.f32 	%f217, %f215, %f216;
	mov.b32 	%r295, %f217;
	shfl.sync.bfly.b32 	%r297|%p84, %r295, %r262, %r254, %r256;
	mov.b32 	%f218, %r297;
	add.f32 	%f219, %f217, %f218;
	mov.b32 	%r298, %f219;
	shfl.sync.bfly.b32 	%r300|%p85, %r298, %r265, %r254, %r256;
	mov.b32 	%f220, %r300;
	add.f32 	%f221, %f219, %f220;
	mov.b32 	%r301, %f221;
	shfl.sync.bfly.b32 	%r303|%p86, %r301, %r268, %r254, %r256;
	mov.b32 	%f222, %r303;
	add.f32 	%f223, %f221, %f222;
	st.local.f32 	[%rd1+28], %f223;
	st.shared.f32 	[%r7], %f223;
	bar.sync 	0;
	@%p1 bra 	$L__BB127_22;

	ld.shared.f32 	%f224, [%r3];
	mov.b32 	%r304, %f224;
	mov.u32 	%r305, 31;
	mov.u32 	%r306, 16;
	mov.u32 	%r307, -1;
	shfl.sync.bfly.b32 	%r308|%p87, %r304, %r306, %r305, %r307;
	mov.b32 	%f225, %r308;
	add.f32 	%f226, %f224, %f225;
	mov.b32 	%r309, %f226;
	mov.u32 	%r310, 8;
	shfl.sync.bfly.b32 	%r311|%p88, %r309, %r310, %r305, %r307;
	mov.b32 	%f227, %r311;
	add.f32 	%f228, %f226, %f227;
	mov.b32 	%r312, %f228;
	mov.u32 	%r313, 4;
	shfl.sync.bfly.b32 	%r314|%p89, %r312, %r313, %r305, %r307;
	mov.b32 	%f229, %r314;
	add.f32 	%f230, %f228, %f229;
	mov.b32 	%r315, %f230;
	mov.u32 	%r316, 2;
	shfl.sync.bfly.b32 	%r317|%p90, %r315, %r316, %r305, %r307;
	mov.b32 	%f231, %r317;
	add.f32 	%f232, %f230, %f231;
	mov.b32 	%r318, %f232;
	mov.u32 	%r319, 1;
	shfl.sync.bfly.b32 	%r320|%p91, %r318, %r319, %r305, %r307;
	mov.b32 	%f233, %r320;
	add.f32 	%f234, %f232, %f233;
	st.local.f32 	[%rd1+28], %f234;

$L__BB127_22:
	bar.sync 	0;
	setp.gt.s32 	%p92, %r2, 7;
	@%p92 bra 	$L__BB127_24;

	mad.lo.s32 	%r321, %r2, %r11, %r1;
	cvt.s64.s32 	%rd35, %r321;
	mul.lo.s32 	%r322, %r4, %r15;
	cvt.s64.s32 	%rd36, %r322;
	add.s64 	%rd37, %rd36, %rd35;
	mul.wide.s32 	%rd38, %r2, 4;
	add.s64 	%rd39, %rd1, %rd38;
	ld.local.f32 	%f235, [%rd39];
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f235;}

	// end inline asm
	cvta.to.global.u64 	%rd40, %rd16;
	shl.b64 	%rd41, %rd37, 1;
	add.s64 	%rd42, %rd40, %rd41;
	st.global.u16 	[%rd42], %rs1;

$L__BB127_24:
	ret;

}

