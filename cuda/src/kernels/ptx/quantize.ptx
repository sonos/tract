//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-35583870
// Cuda compilation tools, release 12.8, V12.8.93
// Based on NVVM 7.0.1
//

.version 8.7
.target sm_87
.address_size 64

	// .globl	quantize_mmq_q8_1

.visible .entry quantize_mmq_q8_1(
	.param .u64 quantize_mmq_q8_1_param_0,
	.param .u64 quantize_mmq_q8_1_param_1,
	.param .u64 quantize_mmq_q8_1_param_2,
	.param .u64 quantize_mmq_q8_1_param_3,
	.param .u64 quantize_mmq_q8_1_param_4,
	.param .u64 quantize_mmq_q8_1_param_5,
	.param .u64 quantize_mmq_q8_1_param_6,
	.param .u32 quantize_mmq_q8_1_param_7,
	.param .u32 quantize_mmq_q8_1_param_8
)
{
	.reg .pred 	%p<10>;
	.reg .b16 	%rs<7>;
	.reg .f32 	%f<63>;
	.reg .b32 	%r<46>;
	.reg .b64 	%rd<50>;


	ld.param.u64 	%rd8, [quantize_mmq_q8_1_param_0];
	ld.param.u64 	%rd9, [quantize_mmq_q8_1_param_1];
	ld.param.u64 	%rd10, [quantize_mmq_q8_1_param_2];
	ld.param.u64 	%rd11, [quantize_mmq_q8_1_param_3];
	ld.param.u64 	%rd12, [quantize_mmq_q8_1_param_4];
	ld.param.u64 	%rd13, [quantize_mmq_q8_1_param_5];
	ld.param.u64 	%rd14, [quantize_mmq_q8_1_param_6];
	ld.param.u32 	%r2, [quantize_mmq_q8_1_param_7];
	ld.param.u32 	%r3, [quantize_mmq_q8_1_param_8];
	mov.u32 	%r4, %ntid.x;
	cvt.u64.u32 	%rd1, %r4;
	mov.u32 	%r5, %ctaid.y;
	mul.wide.u32 	%rd15, %r4, %r5;
	mov.u32 	%r6, %tid.x;
	cvt.u64.u32 	%rd16, %r6;
	add.s64 	%rd2, %rd15, %rd16;
	shl.b64 	%rd3, %rd2, 2;
	setp.ge.s64 	%p1, %rd3, %rd14;
	@%p1 bra 	$L__BB0_5;

	cvta.to.global.u64 	%rd4, %rd9;
	mov.u32 	%r1, %ctaid.z;
	cvt.u64.u32 	%rd17, %r1;
	mov.u32 	%r7, %nctaid.x;
	mov.u32 	%r8, %nctaid.y;
	mul.wide.u32 	%rd18, %r7, %r8;
	mul.lo.s64 	%rd19, %rd18, %rd1;
	shr.u64 	%rd20, %rd19, 5;
	mul.lo.s64 	%rd21, %rd20, %rd17;
	cvt.s64.s32 	%rd22, %r2;
	shr.u64 	%rd23, %rd2, 5;
	mul.lo.s64 	%rd24, %rd23, %rd22;
	mov.u32 	%r9, %ctaid.x;
	cvt.u64.u32 	%rd5, %r9;
	add.s64 	%rd25, %rd21, %rd5;
	add.s64 	%rd6, %rd25, %rd24;
	and.b64  	%rd7, %rd3, 124;
	setp.ge.s64 	%p2, %rd3, %rd10;
	mov.f32 	%f59, 0f00000000;
	mov.f32 	%f60, %f59;
	mov.f32 	%f61, %f59;
	mov.f32 	%f62, %f59;
	@%p2 bra 	$L__BB0_3;

	cvta.to.global.u64 	%rd26, %rd8;
	div.u32 	%r10, %r1, %r3;
	cvt.u64.u32 	%rd27, %r10;
	mul.lo.s64 	%rd28, %rd27, %rd13;
	mul.lo.s32 	%r11, %r10, %r3;
	sub.s32 	%r12, %r1, %r11;
	cvt.u64.u32 	%rd29, %r12;
	mul.lo.s64 	%rd30, %rd29, %rd12;
	mul.lo.s64 	%rd31, %rd5, %rd11;
	add.s64 	%rd32, %rd31, %rd3;
	add.s64 	%rd33, %rd32, %rd30;
	add.s64 	%rd34, %rd33, %rd28;
	shr.s64 	%rd35, %rd34, 63;
	shr.u64 	%rd36, %rd35, 62;
	add.s64 	%rd37, %rd34, %rd36;
	shl.b64 	%rd38, %rd37, 2;
	and.b64  	%rd39, %rd38, -16;
	add.s64 	%rd40, %rd26, %rd39;
	ld.global.nc.v4.f32 	{%f59, %f60, %f61, %f62}, [%rd40];

$L__BB0_3:
	abs.f32 	%f19, %f60;
	abs.f32 	%f20, %f59;
	max.f32 	%f21, %f20, %f19;
	abs.f32 	%f22, %f61;
	max.f32 	%f23, %f21, %f22;
	abs.f32 	%f24, %f62;
	max.f32 	%f25, %f23, %f24;
	mov.b32 	%r13, %f25;
	mov.u32 	%r14, 31;
	mov.u32 	%r15, 4;
	mov.u32 	%r16, -1;
	shfl.sync.bfly.b32 	%r17|%p3, %r13, %r15, %r14, %r16;
	mov.b32 	%f26, %r17;
	max.f32 	%f27, %f25, %f26;
	mov.b32 	%r18, %f27;
	mov.u32 	%r19, 2;
	shfl.sync.bfly.b32 	%r20|%p4, %r18, %r19, %r14, %r16;
	mov.b32 	%f28, %r20;
	max.f32 	%f29, %f27, %f28;
	mov.b32 	%r21, %f29;
	mov.u32 	%r22, 1;
	shfl.sync.bfly.b32 	%r23|%p5, %r21, %r22, %r14, %r16;
	mov.b32 	%f30, %r23;
	max.f32 	%f31, %f29, %f30;
	add.f32 	%f32, %f59, %f60;
	add.f32 	%f33, %f32, %f61;
	add.f32 	%f34, %f33, %f62;
	mov.b32 	%r24, %f34;
	shfl.sync.bfly.b32 	%r25|%p6, %r24, %r15, %r14, %r16;
	mov.b32 	%f35, %r25;
	add.f32 	%f36, %f34, %f35;
	mov.b32 	%r26, %f36;
	shfl.sync.bfly.b32 	%r27|%p7, %r26, %r19, %r14, %r16;
	mov.b32 	%f37, %r27;
	add.f32 	%f38, %f36, %f37;
	mov.b32 	%r28, %f38;
	shfl.sync.bfly.b32 	%r29|%p8, %r28, %r22, %r14, %r16;
	mov.b32 	%f39, %r29;
	add.f32 	%f9, %f38, %f39;
	mov.f32 	%f40, 0f42FE0000;
	div.rn.f32 	%f10, %f40, %f31;
	mul.f32 	%f41, %f59, %f10;
	mov.b32 	%r30, %f41;
	and.b32  	%r31, %r30, -2147483648;
	or.b32  	%r32, %r31, 1056964608;
	mov.b32 	%f42, %r32;
	add.rz.f32 	%f43, %f41, %f42;
	cvt.rzi.f32.f32 	%f44, %f43;
	cvt.rzi.s32.f32 	%r33, %f44;
	mul.f32 	%f45, %f60, %f10;
	mov.b32 	%r34, %f45;
	and.b32  	%r35, %r34, -2147483648;
	or.b32  	%r36, %r35, 1056964608;
	mov.b32 	%f46, %r36;
	add.rz.f32 	%f47, %f45, %f46;
	cvt.rzi.f32.f32 	%f48, %f47;
	cvt.rzi.s32.f32 	%r37, %f48;
	mul.f32 	%f49, %f61, %f10;
	mov.b32 	%r38, %f49;
	and.b32  	%r39, %r38, -2147483648;
	or.b32  	%r40, %r39, 1056964608;
	mov.b32 	%f50, %r40;
	add.rz.f32 	%f51, %f49, %f50;
	cvt.rzi.f32.f32 	%f52, %f51;
	cvt.rzi.s32.f32 	%r41, %f52;
	mul.f32 	%f53, %f62, %f10;
	mov.b32 	%r42, %f53;
	and.b32  	%r43, %r42, -2147483648;
	or.b32  	%r44, %r43, 1056964608;
	mov.b32 	%f54, %r44;
	add.rz.f32 	%f55, %f53, %f54;
	cvt.rzi.f32.f32 	%f56, %f55;
	cvt.rzi.s32.f32 	%r45, %f56;
	mul.lo.s64 	%rd41, %rd6, 144;
	add.s64 	%rd42, %rd4, %rd41;
	add.s64 	%rd43, %rd42, %rd7;
	cvt.u16.u32 	%rs1, %r45;
	cvt.u16.u32 	%rs2, %r41;
	cvt.u16.u32 	%rs3, %r37;
	cvt.u16.u32 	%rs4, %r33;
	st.global.v4.u8 	[%rd43+16], {%rs4, %rs3, %rs2, %rs1};
	and.b64  	%rd44, %rd2, 7;
	setp.ne.s64 	%p9, %rd44, 0;
	@%p9 bra 	$L__BB0_5;

	rcp.rn.f32 	%f57, %f10;
	shr.u64 	%rd47, %rd7, 3;
	and.b64  	%rd48, %rd47, 12;
	add.s64 	%rd49, %rd42, %rd48;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs6, %f9;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs5, %f57;}

	// end inline asm
	st.global.v2.u16 	[%rd49], {%rs5, %rs6};

$L__BB0_5:
	ret;

}
	// .globl	quantize_q8_1_nd2
.visible .entry quantize_q8_1_nd2(
	.param .u64 quantize_q8_1_nd2_param_0,
	.param .u64 quantize_q8_1_nd2_param_1,
	.param .u64 quantize_q8_1_nd2_param_2,
	.param .u64 quantize_q8_1_nd2_param_3,
	.param .u64 quantize_q8_1_nd2_param_4,
	.param .u64 quantize_q8_1_nd2_param_5
)
{
	.reg .pred 	%p<15>;
	.reg .b16 	%rs<3>;
	.reg .f32 	%f<36>;
	.reg .b32 	%r<36>;
	.reg .b64 	%rd<32>;


	ld.param.u64 	%rd5, [quantize_q8_1_nd2_param_0];
	ld.param.u64 	%rd6, [quantize_q8_1_nd2_param_1];
	ld.param.u64 	%rd7, [quantize_q8_1_nd2_param_2];
	ld.param.u64 	%rd8, [quantize_q8_1_nd2_param_3];
	ld.param.u64 	%rd9, [quantize_q8_1_nd2_param_4];
	ld.param.u64 	%rd10, [quantize_q8_1_nd2_param_5];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r2, %ctaid.x;
	mul.wide.u32 	%rd11, %r1, %r2;
	mov.u32 	%r3, %tid.x;
	cvt.u64.u32 	%rd12, %r3;
	add.s64 	%rd1, %rd11, %rd12;
	setp.ge.s64 	%p1, %rd1, %rd10;
	@%p1 bra 	$L__BB1_7;

	mov.u32 	%r4, %ctaid.y;
	cvt.u64.u32 	%rd2, %r4;
	setp.ge.s64 	%p2, %rd1, %rd7;
	mov.f32 	%f35, 0f00000000;
	mov.f32 	%f34, %f35;
	@%p2 bra 	$L__BB1_3;

	mul.lo.s64 	%rd13, %rd2, %rd8;
	mul.lo.s64 	%rd14, %rd1, %rd9;
	add.s64 	%rd15, %rd14, %rd13;
	cvta.to.global.u64 	%rd16, %rd5;
	shl.b64 	%rd17, %rd15, 2;
	add.s64 	%rd18, %rd16, %rd17;
	ld.global.nc.f32 	%f34, [%rd18];

$L__BB1_3:
	mul.lo.s64 	%rd19, %rd2, %rd10;
	add.s64 	%rd20, %rd19, %rd1;
	shr.s64 	%rd21, %rd20, 63;
	shr.u64 	%rd22, %rd21, 59;
	add.s64 	%rd23, %rd20, %rd22;
	shr.s64 	%rd3, %rd23, 5;
	abs.f32 	%f9, %f34;
	mov.b32 	%r5, %f9;
	mov.u32 	%r6, 31;
	mov.u32 	%r7, 16;
	mov.u32 	%r8, -1;
	shfl.sync.bfly.b32 	%r9|%p3, %r5, %r7, %r6, %r8;
	mov.b32 	%f10, %r9;
	max.f32 	%f11, %f9, %f10;
	mov.b32 	%r10, %f11;
	mov.u32 	%r11, 8;
	shfl.sync.bfly.b32 	%r12|%p4, %r10, %r11, %r6, %r8;
	mov.b32 	%f12, %r12;
	max.f32 	%f13, %f11, %f12;
	mov.b32 	%r13, %f13;
	mov.u32 	%r14, 4;
	shfl.sync.bfly.b32 	%r15|%p5, %r13, %r14, %r6, %r8;
	mov.b32 	%f14, %r15;
	max.f32 	%f15, %f13, %f14;
	mov.b32 	%r16, %f15;
	mov.u32 	%r17, 2;
	shfl.sync.bfly.b32 	%r18|%p6, %r16, %r17, %r6, %r8;
	mov.b32 	%f16, %r18;
	max.f32 	%f17, %f15, %f16;
	mov.b32 	%r19, %f17;
	mov.u32 	%r20, 1;
	shfl.sync.bfly.b32 	%r21|%p7, %r19, %r20, %r6, %r8;
	mov.b32 	%f18, %r21;
	max.f32 	%f19, %f17, %f18;
	and.b64  	%rd24, %rd23, -32;
	sub.s64 	%rd4, %rd20, %rd24;
	mov.b32 	%r22, %f34;
	shfl.sync.bfly.b32 	%r23|%p8, %r22, %r7, %r6, %r8;
	mov.b32 	%f20, %r23;
	add.f32 	%f21, %f34, %f20;
	mov.b32 	%r24, %f21;
	shfl.sync.bfly.b32 	%r25|%p9, %r24, %r11, %r6, %r8;
	mov.b32 	%f22, %r25;
	add.f32 	%f23, %f21, %f22;
	mov.b32 	%r26, %f23;
	shfl.sync.bfly.b32 	%r27|%p10, %r26, %r14, %r6, %r8;
	mov.b32 	%f24, %r27;
	add.f32 	%f25, %f23, %f24;
	mov.b32 	%r28, %f25;
	shfl.sync.bfly.b32 	%r29|%p11, %r28, %r17, %r6, %r8;
	mov.b32 	%f26, %r29;
	add.f32 	%f27, %f25, %f26;
	mov.b32 	%r30, %f27;
	shfl.sync.bfly.b32 	%r31|%p12, %r30, %r20, %r6, %r8;
	mov.b32 	%f28, %r31;
	add.f32 	%f3, %f27, %f28;
	div.rn.f32 	%f4, %f19, 0f42FE0000;
	setp.eq.f32 	%p13, %f19, 0f00000000;
	@%p13 bra 	$L__BB1_5;

	div.rn.f32 	%f29, %f34, %f4;
	mov.b32 	%r32, %f29;
	and.b32  	%r33, %r32, -2147483648;
	or.b32  	%r34, %r33, 1056964608;
	mov.b32 	%f30, %r34;
	add.rz.f32 	%f31, %f29, %f30;
	cvt.rzi.f32.f32 	%f35, %f31;

$L__BB1_5:
	cvta.to.global.u64 	%rd25, %rd6;
	cvt.rzi.s32.f32 	%r35, %f35;
	mul.lo.s64 	%rd26, %rd3, 36;
	add.s64 	%rd27, %rd25, %rd26;
	add.s64 	%rd28, %rd27, %rd4;
	st.global.u8 	[%rd28+4], %r35;
	setp.gt.s64 	%p14, %rd4, 0;
	@%p14 bra 	$L__BB1_7;

	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f4;}

	// end inline asm
	st.global.u16 	[%rd27], %rs1;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs2, %f3;}

	// end inline asm
	st.global.u16 	[%rd27+2], %rs2;

$L__BB1_7:
	ret;

}
	// .globl	quantize_q8_1_nd3
.visible .entry quantize_q8_1_nd3(
	.param .u64 quantize_q8_1_nd3_param_0,
	.param .u64 quantize_q8_1_nd3_param_1,
	.param .u64 quantize_q8_1_nd3_param_2,
	.param .u64 quantize_q8_1_nd3_param_3,
	.param .u64 quantize_q8_1_nd3_param_4,
	.param .u64 quantize_q8_1_nd3_param_5,
	.param .u64 quantize_q8_1_nd3_param_6,
	.param .u64 quantize_q8_1_nd3_param_7
)
{
	.reg .pred 	%p<15>;
	.reg .b16 	%rs<3>;
	.reg .f32 	%f<36>;
	.reg .b32 	%r<37>;
	.reg .b64 	%rd<39>;


	ld.param.u64 	%rd6, [quantize_q8_1_nd3_param_0];
	ld.param.u64 	%rd7, [quantize_q8_1_nd3_param_1];
	ld.param.u64 	%rd8, [quantize_q8_1_nd3_param_2];
	ld.param.u64 	%rd9, [quantize_q8_1_nd3_param_3];
	ld.param.u64 	%rd10, [quantize_q8_1_nd3_param_4];
	ld.param.u64 	%rd11, [quantize_q8_1_nd3_param_5];
	ld.param.u64 	%rd12, [quantize_q8_1_nd3_param_6];
	ld.param.u64 	%rd13, [quantize_q8_1_nd3_param_7];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r2, %ctaid.x;
	mul.wide.u32 	%rd14, %r1, %r2;
	mov.u32 	%r3, %tid.x;
	cvt.u64.u32 	%rd15, %r3;
	add.s64 	%rd1, %rd14, %rd15;
	setp.ge.s64 	%p1, %rd1, %rd13;
	@%p1 bra 	$L__BB2_7;

	mov.u32 	%r4, %ctaid.y;
	cvt.u64.u32 	%rd2, %r4;
	mov.u32 	%r5, %ctaid.z;
	cvt.u64.u32 	%rd3, %r5;
	setp.ge.s64 	%p2, %rd1, %rd8;
	mov.f32 	%f35, 0f00000000;
	mov.f32 	%f34, %f35;
	@%p2 bra 	$L__BB2_3;

	mul.lo.s64 	%rd16, %rd3, %rd9;
	mul.lo.s64 	%rd17, %rd2, %rd10;
	add.s64 	%rd18, %rd17, %rd16;
	mul.lo.s64 	%rd19, %rd1, %rd11;
	add.s64 	%rd20, %rd18, %rd19;
	cvta.to.global.u64 	%rd21, %rd6;
	shl.b64 	%rd22, %rd20, 2;
	add.s64 	%rd23, %rd21, %rd22;
	ld.global.nc.f32 	%f34, [%rd23];

$L__BB2_3:
	mul.lo.s64 	%rd24, %rd3, %rd12;
	add.s64 	%rd25, %rd24, %rd2;
	mul.lo.s64 	%rd26, %rd25, %rd13;
	add.s64 	%rd27, %rd26, %rd1;
	shr.s64 	%rd28, %rd27, 63;
	shr.u64 	%rd29, %rd28, 59;
	add.s64 	%rd30, %rd27, %rd29;
	shr.s64 	%rd4, %rd30, 5;
	abs.f32 	%f9, %f34;
	mov.b32 	%r6, %f9;
	mov.u32 	%r7, 31;
	mov.u32 	%r8, 16;
	mov.u32 	%r9, -1;
	shfl.sync.bfly.b32 	%r10|%p3, %r6, %r8, %r7, %r9;
	mov.b32 	%f10, %r10;
	max.f32 	%f11, %f9, %f10;
	mov.b32 	%r11, %f11;
	mov.u32 	%r12, 8;
	shfl.sync.bfly.b32 	%r13|%p4, %r11, %r12, %r7, %r9;
	mov.b32 	%f12, %r13;
	max.f32 	%f13, %f11, %f12;
	mov.b32 	%r14, %f13;
	mov.u32 	%r15, 4;
	shfl.sync.bfly.b32 	%r16|%p5, %r14, %r15, %r7, %r9;
	mov.b32 	%f14, %r16;
	max.f32 	%f15, %f13, %f14;
	mov.b32 	%r17, %f15;
	mov.u32 	%r18, 2;
	shfl.sync.bfly.b32 	%r19|%p6, %r17, %r18, %r7, %r9;
	mov.b32 	%f16, %r19;
	max.f32 	%f17, %f15, %f16;
	mov.b32 	%r20, %f17;
	mov.u32 	%r21, 1;
	shfl.sync.bfly.b32 	%r22|%p7, %r20, %r21, %r7, %r9;
	mov.b32 	%f18, %r22;
	max.f32 	%f19, %f17, %f18;
	and.b64  	%rd31, %rd30, -32;
	sub.s64 	%rd5, %rd27, %rd31;
	mov.b32 	%r23, %f34;
	shfl.sync.bfly.b32 	%r24|%p8, %r23, %r8, %r7, %r9;
	mov.b32 	%f20, %r24;
	add.f32 	%f21, %f34, %f20;
	mov.b32 	%r25, %f21;
	shfl.sync.bfly.b32 	%r26|%p9, %r25, %r12, %r7, %r9;
	mov.b32 	%f22, %r26;
	add.f32 	%f23, %f21, %f22;
	mov.b32 	%r27, %f23;
	shfl.sync.bfly.b32 	%r28|%p10, %r27, %r15, %r7, %r9;
	mov.b32 	%f24, %r28;
	add.f32 	%f25, %f23, %f24;
	mov.b32 	%r29, %f25;
	shfl.sync.bfly.b32 	%r30|%p11, %r29, %r18, %r7, %r9;
	mov.b32 	%f26, %r30;
	add.f32 	%f27, %f25, %f26;
	mov.b32 	%r31, %f27;
	shfl.sync.bfly.b32 	%r32|%p12, %r31, %r21, %r7, %r9;
	mov.b32 	%f28, %r32;
	add.f32 	%f3, %f27, %f28;
	div.rn.f32 	%f4, %f19, 0f42FE0000;
	setp.eq.f32 	%p13, %f19, 0f00000000;
	@%p13 bra 	$L__BB2_5;

	div.rn.f32 	%f29, %f34, %f4;
	mov.b32 	%r33, %f29;
	and.b32  	%r34, %r33, -2147483648;
	or.b32  	%r35, %r34, 1056964608;
	mov.b32 	%f30, %r35;
	add.rz.f32 	%f31, %f29, %f30;
	cvt.rzi.f32.f32 	%f35, %f31;

$L__BB2_5:
	cvta.to.global.u64 	%rd32, %rd7;
	cvt.rzi.s32.f32 	%r36, %f35;
	mul.lo.s64 	%rd33, %rd4, 36;
	add.s64 	%rd34, %rd32, %rd33;
	add.s64 	%rd35, %rd34, %rd5;
	st.global.u8 	[%rd35+4], %r36;
	setp.gt.s64 	%p14, %rd5, 0;
	@%p14 bra 	$L__BB2_7;

	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f4;}

	// end inline asm
	st.global.u16 	[%rd34], %rs1;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs2, %f3;}

	// end inline asm
	st.global.u16 	[%rd34+2], %rs2;

$L__BB2_7:
	ret;

}
	// .globl	quantize_q8_1_nd4
.visible .entry quantize_q8_1_nd4(
	.param .u64 quantize_q8_1_nd4_param_0,
	.param .u64 quantize_q8_1_nd4_param_1,
	.param .u64 quantize_q8_1_nd4_param_2,
	.param .u64 quantize_q8_1_nd4_param_3,
	.param .u64 quantize_q8_1_nd4_param_4,
	.param .u64 quantize_q8_1_nd4_param_5,
	.param .u64 quantize_q8_1_nd4_param_6,
	.param .u64 quantize_q8_1_nd4_param_7,
	.param .u64 quantize_q8_1_nd4_param_8,
	.param .u64 quantize_q8_1_nd4_param_9
)
{
	.reg .pred 	%p<16>;
	.reg .b16 	%rs<3>;
	.reg .f32 	%f<36>;
	.reg .b32 	%r<43>;
	.reg .b64 	%rd<56>;


	ld.param.u64 	%rd11, [quantize_q8_1_nd4_param_0];
	ld.param.u64 	%rd12, [quantize_q8_1_nd4_param_1];
	ld.param.u64 	%rd13, [quantize_q8_1_nd4_param_2];
	ld.param.u64 	%rd14, [quantize_q8_1_nd4_param_3];
	ld.param.u64 	%rd15, [quantize_q8_1_nd4_param_4];
	ld.param.u64 	%rd16, [quantize_q8_1_nd4_param_5];
	ld.param.u64 	%rd17, [quantize_q8_1_nd4_param_6];
	ld.param.u64 	%rd18, [quantize_q8_1_nd4_param_7];
	ld.param.u64 	%rd19, [quantize_q8_1_nd4_param_8];
	ld.param.u64 	%rd20, [quantize_q8_1_nd4_param_9];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r2, %ctaid.x;
	mul.wide.u32 	%rd21, %r1, %r2;
	mov.u32 	%r3, %tid.x;
	cvt.u64.u32 	%rd22, %r3;
	add.s64 	%rd1, %rd21, %rd22;
	setp.ge.s64 	%p1, %rd1, %rd20;
	@%p1 bra 	$L__BB3_10;

	mov.u32 	%r4, %ctaid.z;
	cvt.u64.u32 	%rd2, %r4;
	and.b64  	%rd23, %rd18, -4294967296;
	setp.eq.s64 	%p2, %rd23, 0;
	@%p2 bra 	$L__BB3_3;

	div.s64 	%rd54, %rd2, %rd18;
	mul.lo.s64 	%rd24, %rd54, %rd18;
	sub.s64 	%rd55, %rd2, %rd24;
	bra.uni 	$L__BB3_4;

$L__BB3_3:
	cvt.u32.u64 	%r5, %rd18;
	cvt.u32.u64 	%r6, %rd2;
	div.u32 	%r7, %r6, %r5;
	mul.lo.s32 	%r8, %r7, %r5;
	sub.s32 	%r9, %r6, %r8;
	cvt.u64.u32 	%rd54, %r7;
	cvt.u64.u32 	%rd55, %r9;

$L__BB3_4:
	setp.ge.s64 	%p3, %rd1, %rd13;
	mov.f32 	%f35, 0f00000000;
	mov.f32 	%f34, %f35;
	@%p3 bra 	$L__BB3_6;

	mul.lo.s64 	%rd25, %rd54, %rd14;
	mov.u32 	%r10, %ctaid.y;
	cvt.u64.u32 	%rd26, %r10;
	mul.lo.s64 	%rd27, %rd26, %rd16;
	mul.lo.s64 	%rd28, %rd1, %rd17;
	add.s64 	%rd29, %rd28, %rd27;
	mul.lo.s64 	%rd30, %rd55, %rd15;
	add.s64 	%rd31, %rd29, %rd30;
	add.s64 	%rd32, %rd31, %rd25;
	cvta.to.global.u64 	%rd33, %rd11;
	shl.b64 	%rd34, %rd32, 2;
	add.s64 	%rd35, %rd33, %rd34;
	ld.global.nc.f32 	%f34, [%rd35];

$L__BB3_6:
	mov.u32 	%r11, %ctaid.y;
	cvt.u64.u32 	%rd36, %r11;
	mul.lo.s64 	%rd37, %rd54, %rd18;
	add.s64 	%rd38, %rd37, %rd55;
	mul.lo.s64 	%rd39, %rd38, %rd19;
	add.s64 	%rd40, %rd39, %rd36;
	mul.lo.s64 	%rd41, %rd40, %rd20;
	add.s64 	%rd42, %rd41, %rd1;
	shr.s64 	%rd43, %rd42, 63;
	shr.u64 	%rd44, %rd43, 59;
	add.s64 	%rd45, %rd42, %rd44;
	shr.s64 	%rd9, %rd45, 5;
	abs.f32 	%f9, %f34;
	mov.b32 	%r12, %f9;
	mov.u32 	%r13, 31;
	mov.u32 	%r14, 16;
	mov.u32 	%r15, -1;
	shfl.sync.bfly.b32 	%r16|%p4, %r12, %r14, %r13, %r15;
	mov.b32 	%f10, %r16;
	max.f32 	%f11, %f9, %f10;
	mov.b32 	%r17, %f11;
	mov.u32 	%r18, 8;
	shfl.sync.bfly.b32 	%r19|%p5, %r17, %r18, %r13, %r15;
	mov.b32 	%f12, %r19;
	max.f32 	%f13, %f11, %f12;
	mov.b32 	%r20, %f13;
	mov.u32 	%r21, 4;
	shfl.sync.bfly.b32 	%r22|%p6, %r20, %r21, %r13, %r15;
	mov.b32 	%f14, %r22;
	max.f32 	%f15, %f13, %f14;
	mov.b32 	%r23, %f15;
	mov.u32 	%r24, 2;
	shfl.sync.bfly.b32 	%r25|%p7, %r23, %r24, %r13, %r15;
	mov.b32 	%f16, %r25;
	max.f32 	%f17, %f15, %f16;
	mov.b32 	%r26, %f17;
	mov.u32 	%r27, 1;
	shfl.sync.bfly.b32 	%r28|%p8, %r26, %r27, %r13, %r15;
	mov.b32 	%f18, %r28;
	max.f32 	%f19, %f17, %f18;
	and.b64  	%rd46, %rd45, -32;
	sub.s64 	%rd10, %rd42, %rd46;
	mov.b32 	%r29, %f34;
	shfl.sync.bfly.b32 	%r30|%p9, %r29, %r14, %r13, %r15;
	mov.b32 	%f20, %r30;
	add.f32 	%f21, %f34, %f20;
	mov.b32 	%r31, %f21;
	shfl.sync.bfly.b32 	%r32|%p10, %r31, %r18, %r13, %r15;
	mov.b32 	%f22, %r32;
	add.f32 	%f23, %f21, %f22;
	mov.b32 	%r33, %f23;
	shfl.sync.bfly.b32 	%r34|%p11, %r33, %r21, %r13, %r15;
	mov.b32 	%f24, %r34;
	add.f32 	%f25, %f23, %f24;
	mov.b32 	%r35, %f25;
	shfl.sync.bfly.b32 	%r36|%p12, %r35, %r24, %r13, %r15;
	mov.b32 	%f26, %r36;
	add.f32 	%f27, %f25, %f26;
	mov.b32 	%r37, %f27;
	shfl.sync.bfly.b32 	%r38|%p13, %r37, %r27, %r13, %r15;
	mov.b32 	%f28, %r38;
	add.f32 	%f3, %f27, %f28;
	div.rn.f32 	%f4, %f19, 0f42FE0000;
	setp.eq.f32 	%p14, %f19, 0f00000000;
	@%p14 bra 	$L__BB3_8;

	div.rn.f32 	%f29, %f34, %f4;
	mov.b32 	%r39, %f29;
	and.b32  	%r40, %r39, -2147483648;
	or.b32  	%r41, %r40, 1056964608;
	mov.b32 	%f30, %r41;
	add.rz.f32 	%f31, %f29, %f30;
	cvt.rzi.f32.f32 	%f35, %f31;

$L__BB3_8:
	cvta.to.global.u64 	%rd47, %rd12;
	cvt.rzi.s32.f32 	%r42, %f35;
	mul.lo.s64 	%rd48, %rd9, 36;
	add.s64 	%rd49, %rd47, %rd48;
	add.s64 	%rd50, %rd49, %rd10;
	st.global.u8 	[%rd50+4], %r42;
	setp.gt.s64 	%p15, %rd10, 0;
	@%p15 bra 	$L__BB3_10;

	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f4;}

	// end inline asm
	st.global.u16 	[%rd49], %rs1;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs2, %f3;}

	// end inline asm
	st.global.u16 	[%rd49+2], %rs2;

$L__BB3_10:
	ret;

}
	// .globl	quantize_q8_1_nd5
.visible .entry quantize_q8_1_nd5(
	.param .u64 quantize_q8_1_nd5_param_0,
	.param .u64 quantize_q8_1_nd5_param_1,
	.param .u64 quantize_q8_1_nd5_param_2,
	.param .u64 quantize_q8_1_nd5_param_3,
	.param .u64 quantize_q8_1_nd5_param_4,
	.param .u64 quantize_q8_1_nd5_param_5,
	.param .u64 quantize_q8_1_nd5_param_6,
	.param .u64 quantize_q8_1_nd5_param_7,
	.param .u64 quantize_q8_1_nd5_param_8,
	.param .u64 quantize_q8_1_nd5_param_9,
	.param .u64 quantize_q8_1_nd5_param_10,
	.param .u64 quantize_q8_1_nd5_param_11
)
{
	.reg .pred 	%p<18>;
	.reg .b16 	%rs<3>;
	.reg .f32 	%f<36>;
	.reg .b32 	%r<56>;
	.reg .b64 	%rd<81>;


	ld.param.u64 	%rd18, [quantize_q8_1_nd5_param_0];
	ld.param.u64 	%rd19, [quantize_q8_1_nd5_param_1];
	ld.param.u64 	%rd20, [quantize_q8_1_nd5_param_2];
	ld.param.u64 	%rd21, [quantize_q8_1_nd5_param_3];
	ld.param.u64 	%rd22, [quantize_q8_1_nd5_param_4];
	ld.param.u64 	%rd23, [quantize_q8_1_nd5_param_5];
	ld.param.u64 	%rd24, [quantize_q8_1_nd5_param_6];
	ld.param.u64 	%rd25, [quantize_q8_1_nd5_param_7];
	ld.param.u64 	%rd26, [quantize_q8_1_nd5_param_8];
	ld.param.u64 	%rd27, [quantize_q8_1_nd5_param_9];
	ld.param.u64 	%rd28, [quantize_q8_1_nd5_param_10];
	ld.param.u64 	%rd29, [quantize_q8_1_nd5_param_11];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r2, %ctaid.x;
	mul.wide.u32 	%rd30, %r1, %r2;
	mov.u32 	%r3, %tid.x;
	cvt.u64.u32 	%rd31, %r3;
	add.s64 	%rd32, %rd30, %rd31;
	setp.ge.s64 	%p1, %rd32, %rd29;
	@%p1 bra 	$L__BB4_16;

	mov.u32 	%r4, %ctaid.z;
	cvt.u64.u32 	%rd1, %r4;
	and.b64  	%rd33, %rd27, -4294967296;
	setp.eq.s64 	%p2, %rd33, 0;
	@%p2 bra 	$L__BB4_3;

	div.s64 	%rd77, %rd1, %rd27;
	mul.lo.s64 	%rd34, %rd77, %rd27;
	sub.s64 	%rd78, %rd1, %rd34;
	bra.uni 	$L__BB4_4;

$L__BB4_3:
	cvt.u32.u64 	%r5, %rd27;
	cvt.u32.u64 	%r6, %rd1;
	div.u32 	%r7, %r6, %r5;
	mul.lo.s32 	%r8, %r7, %r5;
	sub.s32 	%r9, %r6, %r8;
	cvt.u64.u32 	%rd77, %r7;
	cvt.u64.u32 	%rd78, %r9;

$L__BB4_4:
	or.b64  	%rd35, %rd77, %rd26;
	and.b64  	%rd36, %rd35, -4294967296;
	setp.eq.s64 	%p3, %rd36, 0;
	@%p3 bra 	$L__BB4_6;

	rem.s64 	%rd79, %rd77, %rd26;
	bra.uni 	$L__BB4_7;

$L__BB4_6:
	cvt.u32.u64 	%r10, %rd26;
	cvt.u32.u64 	%r11, %rd77;
	rem.u32 	%r12, %r11, %r10;
	cvt.u64.u32 	%rd79, %r12;

$L__BB4_7:
	mul.lo.s64 	%rd11, %rd27, %rd26;
	and.b64  	%rd37, %rd11, -4294967296;
	setp.eq.s64 	%p4, %rd37, 0;
	@%p4 bra 	$L__BB4_9;

	div.s64 	%rd80, %rd1, %rd11;
	bra.uni 	$L__BB4_10;

$L__BB4_9:
	cvt.u32.u64 	%r14, %rd11;
	div.u32 	%r16, %r4, %r14;
	cvt.u64.u32 	%rd80, %r16;

$L__BB4_10:
	setp.ge.s64 	%p5, %rd32, %rd20;
	mov.f32 	%f35, 0f00000000;
	mov.f32 	%f34, %f35;
	@%p5 bra 	$L__BB4_12;

	mul.lo.s64 	%rd41, %rd80, %rd25;
	mov.u32 	%r20, %ctaid.y;
	cvt.u64.u32 	%rd42, %r20;
	mul.lo.s64 	%rd43, %rd42, %rd22;
	mul.lo.s64 	%rd47, %rd32, %rd21;
	add.s64 	%rd48, %rd43, %rd47;
	mul.lo.s64 	%rd49, %rd78, %rd23;
	add.s64 	%rd50, %rd48, %rd49;
	mul.lo.s64 	%rd51, %rd79, %rd24;
	add.s64 	%rd52, %rd50, %rd51;
	add.s64 	%rd53, %rd52, %rd41;
	cvta.to.global.u64 	%rd54, %rd18;
	shl.b64 	%rd55, %rd53, 2;
	add.s64 	%rd56, %rd54, %rd55;
	ld.global.nc.f32 	%f34, [%rd56];

$L__BB4_12:
	mov.u32 	%r24, %ctaid.y;
	cvt.u64.u32 	%rd57, %r24;
	mul.lo.s64 	%rd58, %rd80, %rd26;
	add.s64 	%rd59, %rd58, %rd79;
	mul.lo.s64 	%rd60, %rd59, %rd27;
	add.s64 	%rd61, %rd60, %rd78;
	mul.lo.s64 	%rd62, %rd61, %rd28;
	add.s64 	%rd63, %rd62, %rd57;
	mul.lo.s64 	%rd64, %rd63, %rd29;
	add.s64 	%rd65, %rd64, %rd32;
	shr.s64 	%rd66, %rd65, 63;
	shr.u64 	%rd67, %rd66, 59;
	add.s64 	%rd68, %rd65, %rd67;
	shr.s64 	%rd16, %rd68, 5;
	abs.f32 	%f9, %f34;
	mov.b32 	%r25, %f9;
	mov.u32 	%r26, 31;
	mov.u32 	%r27, 16;
	mov.u32 	%r28, -1;
	shfl.sync.bfly.b32 	%r29|%p6, %r25, %r27, %r26, %r28;
	mov.b32 	%f10, %r29;
	max.f32 	%f11, %f9, %f10;
	mov.b32 	%r30, %f11;
	mov.u32 	%r31, 8;
	shfl.sync.bfly.b32 	%r32|%p7, %r30, %r31, %r26, %r28;
	mov.b32 	%f12, %r32;
	max.f32 	%f13, %f11, %f12;
	mov.b32 	%r33, %f13;
	mov.u32 	%r34, 4;
	shfl.sync.bfly.b32 	%r35|%p8, %r33, %r34, %r26, %r28;
	mov.b32 	%f14, %r35;
	max.f32 	%f15, %f13, %f14;
	mov.b32 	%r36, %f15;
	mov.u32 	%r37, 2;
	shfl.sync.bfly.b32 	%r38|%p9, %r36, %r37, %r26, %r28;
	mov.b32 	%f16, %r38;
	max.f32 	%f17, %f15, %f16;
	mov.b32 	%r39, %f17;
	mov.u32 	%r40, 1;
	shfl.sync.bfly.b32 	%r41|%p10, %r39, %r40, %r26, %r28;
	mov.b32 	%f18, %r41;
	max.f32 	%f19, %f17, %f18;
	and.b64  	%rd69, %rd68, -32;
	sub.s64 	%rd17, %rd65, %rd69;
	mov.b32 	%r42, %f34;
	shfl.sync.bfly.b32 	%r43|%p11, %r42, %r27, %r26, %r28;
	mov.b32 	%f20, %r43;
	add.f32 	%f21, %f34, %f20;
	mov.b32 	%r44, %f21;
	shfl.sync.bfly.b32 	%r45|%p12, %r44, %r31, %r26, %r28;
	mov.b32 	%f22, %r45;
	add.f32 	%f23, %f21, %f22;
	mov.b32 	%r46, %f23;
	shfl.sync.bfly.b32 	%r47|%p13, %r46, %r34, %r26, %r28;
	mov.b32 	%f24, %r47;
	add.f32 	%f25, %f23, %f24;
	mov.b32 	%r48, %f25;
	shfl.sync.bfly.b32 	%r49|%p14, %r48, %r37, %r26, %r28;
	mov.b32 	%f26, %r49;
	add.f32 	%f27, %f25, %f26;
	mov.b32 	%r50, %f27;
	shfl.sync.bfly.b32 	%r51|%p15, %r50, %r40, %r26, %r28;
	mov.b32 	%f28, %r51;
	add.f32 	%f3, %f27, %f28;
	div.rn.f32 	%f4, %f19, 0f42FE0000;
	setp.eq.f32 	%p16, %f19, 0f00000000;
	@%p16 bra 	$L__BB4_14;

	div.rn.f32 	%f29, %f34, %f4;
	mov.b32 	%r52, %f29;
	and.b32  	%r53, %r52, -2147483648;
	or.b32  	%r54, %r53, 1056964608;
	mov.b32 	%f30, %r54;
	add.rz.f32 	%f31, %f29, %f30;
	cvt.rzi.f32.f32 	%f35, %f31;

$L__BB4_14:
	cvta.to.global.u64 	%rd70, %rd19;
	cvt.rzi.s32.f32 	%r55, %f35;
	mul.lo.s64 	%rd71, %rd16, 36;
	add.s64 	%rd72, %rd70, %rd71;
	add.s64 	%rd73, %rd72, %rd17;
	st.global.u8 	[%rd73+4], %r55;
	setp.gt.s64 	%p17, %rd17, 0;
	@%p17 bra 	$L__BB4_16;

	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f4;}

	// end inline asm
	st.global.u16 	[%rd72], %rs1;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs2, %f3;}

	// end inline asm
	st.global.u16 	[%rd72+2], %rs2;

$L__BB4_16:
	ret;

}

