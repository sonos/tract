//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-35583870
// Cuda compilation tools, release 12.8, V12.8.93
// Based on NVVM 7.0.1
//

.version 8.7
.target sm_87
.address_size 64

	// .globl	quantize_mmq_q8_1_fast_nd2

.visible .entry quantize_mmq_q8_1_fast_nd2(
	.param .u64 quantize_mmq_q8_1_fast_nd2_param_0,
	.param .u64 quantize_mmq_q8_1_fast_nd2_param_1,
	.param .u64 quantize_mmq_q8_1_fast_nd2_param_2,
	.param .u64 quantize_mmq_q8_1_fast_nd2_param_3,
	.param .u64 quantize_mmq_q8_1_fast_nd2_param_4,
	.param .u64 quantize_mmq_q8_1_fast_nd2_param_5
)
{
	.reg .pred 	%p<12>;
	.reg .b16 	%rs<7>;
	.reg .f32 	%f<74>;
	.reg .b32 	%r<47>;
	.reg .b64 	%rd<52>;


	ld.param.u64 	%rd6, [quantize_mmq_q8_1_fast_nd2_param_0];
	ld.param.u64 	%rd7, [quantize_mmq_q8_1_fast_nd2_param_1];
	ld.param.u64 	%rd8, [quantize_mmq_q8_1_fast_nd2_param_2];
	ld.param.u64 	%rd9, [quantize_mmq_q8_1_fast_nd2_param_3];
	ld.param.u64 	%rd10, [quantize_mmq_q8_1_fast_nd2_param_5];
	mov.u32 	%r1, %ntid.x;
	cvt.u64.u32 	%rd1, %r1;
	mov.u32 	%r2, %ctaid.y;
	mul.wide.u32 	%rd11, %r1, %r2;
	mov.u32 	%r3, %tid.x;
	cvt.u64.u32 	%rd12, %r3;
	add.s64 	%rd2, %rd11, %rd12;
	shl.b64 	%rd3, %rd2, 2;
	setp.ge.s64 	%p1, %rd3, %rd10;
	@%p1 bra 	$L__BB0_9;

	mov.u32 	%r4, %ctaid.z;
	cvt.u64.u32 	%rd13, %r4;
	mov.u32 	%r5, %nctaid.x;
	cvt.u64.u32 	%rd14, %r5;
	mov.u32 	%r6, %nctaid.y;
	mul.wide.u32 	%rd15, %r5, %r6;
	mul.lo.s64 	%rd16, %rd15, %rd1;
	shr.u64 	%rd17, %rd16, 5;
	mul.lo.s64 	%rd18, %rd17, %rd13;
	shr.u64 	%rd19, %rd2, 5;
	mul.lo.s64 	%rd20, %rd19, %rd14;
	mov.u32 	%r7, %ctaid.x;
	cvt.u64.u32 	%rd4, %r7;
	add.s64 	%rd21, %rd20, %rd4;
	add.s64 	%rd5, %rd21, %rd18;
	setp.ge.s64 	%p2, %rd3, %rd8;
	mov.f32 	%f72, 0f00000000;
	mov.f32 	%f68, %f72;
	mov.f32 	%f69, %f72;
	mov.f32 	%f70, %f72;
	mov.f32 	%f71, %f72;
	@%p2 bra 	$L__BB0_3;

	cvta.to.global.u64 	%rd22, %rd6;
	mul.lo.s64 	%rd23, %rd4, %rd9;
	add.s64 	%rd24, %rd23, %rd3;
	shr.s64 	%rd25, %rd24, 63;
	shr.u64 	%rd26, %rd25, 62;
	add.s64 	%rd27, %rd24, %rd26;
	shl.b64 	%rd28, %rd27, 2;
	and.b64  	%rd29, %rd28, -16;
	add.s64 	%rd30, %rd22, %rd29;
	ld.global.nc.v4.f32 	{%f68, %f69, %f70, %f71}, [%rd30];

$L__BB0_3:
	abs.f32 	%f28, %f68;
	abs.f32 	%f29, %f69;
	max.f32 	%f30, %f28, %f29;
	abs.f32 	%f31, %f70;
	max.f32 	%f32, %f30, %f31;
	abs.f32 	%f33, %f71;
	max.f32 	%f34, %f32, %f33;
	mov.b32 	%r8, %f34;
	mov.u32 	%r9, 31;
	mov.u32 	%r10, 4;
	mov.u32 	%r11, -1;
	shfl.sync.bfly.b32 	%r12|%p3, %r8, %r10, %r9, %r11;
	mov.b32 	%f35, %r12;
	max.f32 	%f36, %f34, %f35;
	mov.b32 	%r13, %f36;
	mov.u32 	%r14, 2;
	shfl.sync.bfly.b32 	%r15|%p4, %r13, %r14, %r9, %r11;
	mov.b32 	%f37, %r15;
	max.f32 	%f38, %f36, %f37;
	mov.b32 	%r16, %f38;
	mov.u32 	%r17, 1;
	shfl.sync.bfly.b32 	%r18|%p5, %r16, %r17, %r9, %r11;
	mov.b32 	%f39, %r18;
	max.f32 	%f13, %f38, %f39;
	add.f32 	%f40, %f68, %f69;
	add.f32 	%f41, %f70, %f40;
	add.f32 	%f42, %f71, %f41;
	mov.b32 	%r19, %f42;
	shfl.sync.bfly.b32 	%r20|%p6, %r19, %r10, %r9, %r11;
	mov.b32 	%f43, %r20;
	add.f32 	%f44, %f42, %f43;
	mov.b32 	%r21, %f44;
	shfl.sync.bfly.b32 	%r22|%p7, %r21, %r14, %r9, %r11;
	mov.b32 	%f45, %r22;
	add.f32 	%f46, %f44, %f45;
	mov.b32 	%r23, %f46;
	shfl.sync.bfly.b32 	%r24|%p8, %r23, %r17, %r9, %r11;
	mov.b32 	%f47, %r24;
	add.f32 	%f14, %f46, %f47;
	setp.leu.f32 	%p9, %f13, 0f00000000;
	@%p9 bra 	$L__BB0_5;

	mov.f32 	%f48, 0f42FE0000;
	div.rn.f32 	%f72, %f48, %f13;

$L__BB0_5:
	cvta.to.global.u64 	%rd31, %rd7;
	mul.f32 	%f49, %f68, %f72;
	mov.b32 	%r25, %f49;
	and.b32  	%r26, %r25, -2147483648;
	or.b32  	%r27, %r26, 1056964608;
	mov.b32 	%f50, %r27;
	add.rz.f32 	%f51, %f49, %f50;
	cvt.rzi.f32.f32 	%f52, %f51;
	cvt.rzi.s32.f32 	%r28, %f52;
	mul.f32 	%f53, %f69, %f72;
	mov.b32 	%r29, %f53;
	and.b32  	%r30, %r29, -2147483648;
	or.b32  	%r31, %r30, 1056964608;
	mov.b32 	%f54, %r31;
	add.rz.f32 	%f55, %f53, %f54;
	cvt.rzi.f32.f32 	%f56, %f55;
	cvt.rzi.s32.f32 	%r32, %f56;
	mul.f32 	%f57, %f70, %f72;
	mov.b32 	%r33, %f57;
	and.b32  	%r34, %r33, -2147483648;
	or.b32  	%r35, %r34, 1056964608;
	mov.b32 	%f58, %r35;
	add.rz.f32 	%f59, %f57, %f58;
	cvt.rzi.f32.f32 	%f60, %f59;
	cvt.rzi.s32.f32 	%r36, %f60;
	mul.f32 	%f61, %f71, %f72;
	mov.b32 	%r37, %f61;
	and.b32  	%r38, %r37, -2147483648;
	or.b32  	%r39, %r38, 1056964608;
	mov.b32 	%f62, %r39;
	add.rz.f32 	%f63, %f61, %f62;
	cvt.rzi.f32.f32 	%f64, %f63;
	cvt.rzi.s32.f32 	%r40, %f64;
	mul.lo.s64 	%rd32, %rd5, 144;
	add.s64 	%rd33, %rd31, %rd32;
	and.b64  	%rd38, %rd3, 124;
	add.s64 	%rd39, %rd33, %rd38;
	cvt.u16.u32 	%rs1, %r40;
	cvt.u16.u32 	%rs2, %r36;
	cvt.u16.u32 	%rs3, %r32;
	cvt.u16.u32 	%rs4, %r28;
	st.global.v4.u8 	[%rd39+16], {%rs4, %rs3, %rs2, %rs1};
	and.b64  	%rd40, %rd2, 7;
	setp.ne.s64 	%p10, %rd40, 0;
	@%p10 bra 	$L__BB0_9;

	setp.leu.f32 	%p11, %f72, 0f00000000;
	mov.f32 	%f73, 0f00000000;
	@%p11 bra 	$L__BB0_8;

	rcp.rn.f32 	%f73, %f72;

$L__BB0_8:
	cvt.u64.u32 	%rd43, %r2;
	mul.lo.s64 	%rd44, %rd1, %rd43;
	add.s64 	%rd46, %rd44, %rd12;
	shr.u64 	%rd47, %rd46, 1;
	and.b64  	%rd50, %rd47, 12;
	add.s64 	%rd51, %rd33, %rd50;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs6, %f14;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs5, %f73;}

	// end inline asm
	st.global.v2.u16 	[%rd51], {%rs5, %rs6};

$L__BB0_9:
	ret;

}
	// .globl	quantize_mmq_q8_1_fast_nd3
.visible .entry quantize_mmq_q8_1_fast_nd3(
	.param .u64 quantize_mmq_q8_1_fast_nd3_param_0,
	.param .u64 quantize_mmq_q8_1_fast_nd3_param_1,
	.param .u64 quantize_mmq_q8_1_fast_nd3_param_2,
	.param .u64 quantize_mmq_q8_1_fast_nd3_param_3,
	.param .u64 quantize_mmq_q8_1_fast_nd3_param_4,
	.param .u64 quantize_mmq_q8_1_fast_nd3_param_5,
	.param .u32 quantize_mmq_q8_1_fast_nd3_param_6,
	.param .u64 quantize_mmq_q8_1_fast_nd3_param_7
)
{
	.reg .pred 	%p<12>;
	.reg .b16 	%rs<7>;
	.reg .f32 	%f<74>;
	.reg .b32 	%r<48>;
	.reg .b64 	%rd<56>;


	ld.param.u64 	%rd7, [quantize_mmq_q8_1_fast_nd3_param_0];
	ld.param.u64 	%rd8, [quantize_mmq_q8_1_fast_nd3_param_1];
	ld.param.u64 	%rd9, [quantize_mmq_q8_1_fast_nd3_param_2];
	ld.param.u64 	%rd10, [quantize_mmq_q8_1_fast_nd3_param_3];
	ld.param.u64 	%rd11, [quantize_mmq_q8_1_fast_nd3_param_4];
	ld.param.u32 	%r1, [quantize_mmq_q8_1_fast_nd3_param_6];
	ld.param.u64 	%rd12, [quantize_mmq_q8_1_fast_nd3_param_7];
	mov.u32 	%r2, %ntid.x;
	cvt.u64.u32 	%rd1, %r2;
	mov.u32 	%r3, %ctaid.y;
	mul.wide.u32 	%rd13, %r2, %r3;
	mov.u32 	%r4, %tid.x;
	cvt.u64.u32 	%rd14, %r4;
	add.s64 	%rd2, %rd13, %rd14;
	shl.b64 	%rd3, %rd2, 2;
	setp.ge.s64 	%p1, %rd3, %rd12;
	@%p1 bra 	$L__BB1_9;

	mov.u32 	%r5, %ctaid.z;
	cvt.u64.u32 	%rd4, %r5;
	mov.u32 	%r6, %nctaid.x;
	mov.u32 	%r7, %nctaid.y;
	mul.wide.u32 	%rd15, %r6, %r7;
	mul.lo.s64 	%rd16, %rd15, %rd1;
	shr.u64 	%rd17, %rd16, 5;
	mul.lo.s64 	%rd18, %rd17, %rd4;
	cvt.s64.s32 	%rd19, %r1;
	shr.u64 	%rd20, %rd2, 5;
	mul.lo.s64 	%rd21, %rd20, %rd19;
	mov.u32 	%r8, %ctaid.x;
	cvt.u64.u32 	%rd5, %r8;
	add.s64 	%rd22, %rd18, %rd5;
	add.s64 	%rd6, %rd22, %rd21;
	setp.ge.s64 	%p2, %rd3, %rd9;
	mov.f32 	%f72, 0f00000000;
	mov.f32 	%f68, %f72;
	mov.f32 	%f69, %f72;
	mov.f32 	%f70, %f72;
	mov.f32 	%f71, %f72;
	@%p2 bra 	$L__BB1_3;

	cvta.to.global.u64 	%rd23, %rd7;
	mul.lo.s64 	%rd25, %rd4, %rd10;
	add.s64 	%rd26, %rd25, %rd3;
	mul.lo.s64 	%rd27, %rd5, %rd11;
	add.s64 	%rd28, %rd26, %rd27;
	shr.s64 	%rd29, %rd28, 63;
	shr.u64 	%rd30, %rd29, 62;
	add.s64 	%rd31, %rd28, %rd30;
	shl.b64 	%rd32, %rd31, 2;
	and.b64  	%rd33, %rd32, -16;
	add.s64 	%rd34, %rd23, %rd33;
	ld.global.nc.v4.f32 	{%f68, %f69, %f70, %f71}, [%rd34];

$L__BB1_3:
	abs.f32 	%f28, %f68;
	abs.f32 	%f29, %f69;
	max.f32 	%f30, %f28, %f29;
	abs.f32 	%f31, %f70;
	max.f32 	%f32, %f30, %f31;
	abs.f32 	%f33, %f71;
	max.f32 	%f34, %f32, %f33;
	mov.b32 	%r9, %f34;
	mov.u32 	%r10, 31;
	mov.u32 	%r11, 4;
	mov.u32 	%r12, -1;
	shfl.sync.bfly.b32 	%r13|%p3, %r9, %r11, %r10, %r12;
	mov.b32 	%f35, %r13;
	max.f32 	%f36, %f34, %f35;
	mov.b32 	%r14, %f36;
	mov.u32 	%r15, 2;
	shfl.sync.bfly.b32 	%r16|%p4, %r14, %r15, %r10, %r12;
	mov.b32 	%f37, %r16;
	max.f32 	%f38, %f36, %f37;
	mov.b32 	%r17, %f38;
	mov.u32 	%r18, 1;
	shfl.sync.bfly.b32 	%r19|%p5, %r17, %r18, %r10, %r12;
	mov.b32 	%f39, %r19;
	max.f32 	%f13, %f38, %f39;
	add.f32 	%f40, %f68, %f69;
	add.f32 	%f41, %f70, %f40;
	add.f32 	%f42, %f71, %f41;
	mov.b32 	%r20, %f42;
	shfl.sync.bfly.b32 	%r21|%p6, %r20, %r11, %r10, %r12;
	mov.b32 	%f43, %r21;
	add.f32 	%f44, %f42, %f43;
	mov.b32 	%r22, %f44;
	shfl.sync.bfly.b32 	%r23|%p7, %r22, %r15, %r10, %r12;
	mov.b32 	%f45, %r23;
	add.f32 	%f46, %f44, %f45;
	mov.b32 	%r24, %f46;
	shfl.sync.bfly.b32 	%r25|%p8, %r24, %r18, %r10, %r12;
	mov.b32 	%f47, %r25;
	add.f32 	%f14, %f46, %f47;
	setp.leu.f32 	%p9, %f13, 0f00000000;
	@%p9 bra 	$L__BB1_5;

	mov.f32 	%f48, 0f42FE0000;
	div.rn.f32 	%f72, %f48, %f13;

$L__BB1_5:
	cvta.to.global.u64 	%rd35, %rd8;
	mul.f32 	%f49, %f68, %f72;
	mov.b32 	%r26, %f49;
	and.b32  	%r27, %r26, -2147483648;
	or.b32  	%r28, %r27, 1056964608;
	mov.b32 	%f50, %r28;
	add.rz.f32 	%f51, %f49, %f50;
	cvt.rzi.f32.f32 	%f52, %f51;
	cvt.rzi.s32.f32 	%r29, %f52;
	mul.f32 	%f53, %f69, %f72;
	mov.b32 	%r30, %f53;
	and.b32  	%r31, %r30, -2147483648;
	or.b32  	%r32, %r31, 1056964608;
	mov.b32 	%f54, %r32;
	add.rz.f32 	%f55, %f53, %f54;
	cvt.rzi.f32.f32 	%f56, %f55;
	cvt.rzi.s32.f32 	%r33, %f56;
	mul.f32 	%f57, %f70, %f72;
	mov.b32 	%r34, %f57;
	and.b32  	%r35, %r34, -2147483648;
	or.b32  	%r36, %r35, 1056964608;
	mov.b32 	%f58, %r36;
	add.rz.f32 	%f59, %f57, %f58;
	cvt.rzi.f32.f32 	%f60, %f59;
	cvt.rzi.s32.f32 	%r37, %f60;
	mul.f32 	%f61, %f71, %f72;
	mov.b32 	%r38, %f61;
	and.b32  	%r39, %r38, -2147483648;
	or.b32  	%r40, %r39, 1056964608;
	mov.b32 	%f62, %r40;
	add.rz.f32 	%f63, %f61, %f62;
	cvt.rzi.f32.f32 	%f64, %f63;
	cvt.rzi.s32.f32 	%r41, %f64;
	mul.lo.s64 	%rd36, %rd6, 144;
	add.s64 	%rd37, %rd35, %rd36;
	and.b64  	%rd42, %rd3, 124;
	add.s64 	%rd43, %rd37, %rd42;
	cvt.u16.u32 	%rs1, %r41;
	cvt.u16.u32 	%rs2, %r37;
	cvt.u16.u32 	%rs3, %r33;
	cvt.u16.u32 	%rs4, %r29;
	st.global.v4.u8 	[%rd43+16], {%rs4, %rs3, %rs2, %rs1};
	and.b64  	%rd44, %rd2, 7;
	setp.ne.s64 	%p10, %rd44, 0;
	@%p10 bra 	$L__BB1_9;

	setp.leu.f32 	%p11, %f72, 0f00000000;
	mov.f32 	%f73, 0f00000000;
	@%p11 bra 	$L__BB1_8;

	rcp.rn.f32 	%f73, %f72;

$L__BB1_8:
	cvt.u64.u32 	%rd47, %r3;
	mul.lo.s64 	%rd48, %rd1, %rd47;
	add.s64 	%rd50, %rd48, %rd14;
	shr.u64 	%rd51, %rd50, 1;
	and.b64  	%rd54, %rd51, 12;
	add.s64 	%rd55, %rd37, %rd54;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs6, %f14;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs5, %f73;}

	// end inline asm
	st.global.v2.u16 	[%rd55], {%rs5, %rs6};

$L__BB1_9:
	ret;

}
	// .globl	quantize_mmq_q8_1_fast_nd4
.visible .entry quantize_mmq_q8_1_fast_nd4(
	.param .u64 quantize_mmq_q8_1_fast_nd4_param_0,
	.param .u64 quantize_mmq_q8_1_fast_nd4_param_1,
	.param .u64 quantize_mmq_q8_1_fast_nd4_param_2,
	.param .u64 quantize_mmq_q8_1_fast_nd4_param_3,
	.param .u64 quantize_mmq_q8_1_fast_nd4_param_4,
	.param .u64 quantize_mmq_q8_1_fast_nd4_param_5,
	.param .u64 quantize_mmq_q8_1_fast_nd4_param_6,
	.param .u32 quantize_mmq_q8_1_fast_nd4_param_7,
	.param .u32 quantize_mmq_q8_1_fast_nd4_param_8,
	.param .u64 quantize_mmq_q8_1_fast_nd4_param_9
)
{
	.reg .pred 	%p<12>;
	.reg .b16 	%rs<7>;
	.reg .f32 	%f<74>;
	.reg .b32 	%r<52>;
	.reg .b64 	%rd<61>;


	ld.param.u64 	%rd6, [quantize_mmq_q8_1_fast_nd4_param_0];
	ld.param.u64 	%rd7, [quantize_mmq_q8_1_fast_nd4_param_1];
	ld.param.u64 	%rd8, [quantize_mmq_q8_1_fast_nd4_param_2];
	ld.param.u64 	%rd9, [quantize_mmq_q8_1_fast_nd4_param_3];
	ld.param.u64 	%rd10, [quantize_mmq_q8_1_fast_nd4_param_4];
	ld.param.u64 	%rd11, [quantize_mmq_q8_1_fast_nd4_param_5];
	ld.param.u32 	%r2, [quantize_mmq_q8_1_fast_nd4_param_7];
	ld.param.u32 	%r3, [quantize_mmq_q8_1_fast_nd4_param_8];
	ld.param.u64 	%rd12, [quantize_mmq_q8_1_fast_nd4_param_9];
	mov.u32 	%r4, %ntid.x;
	cvt.u64.u32 	%rd1, %r4;
	mov.u32 	%r5, %ctaid.y;
	mul.wide.u32 	%rd13, %r4, %r5;
	mov.u32 	%r6, %tid.x;
	cvt.u64.u32 	%rd14, %r6;
	add.s64 	%rd2, %rd13, %rd14;
	shl.b64 	%rd3, %rd2, 2;
	setp.ge.s64 	%p1, %rd3, %rd12;
	@%p1 bra 	$L__BB2_9;

	mov.u32 	%r1, %ctaid.z;
	cvt.u64.u32 	%rd15, %r1;
	mov.u32 	%r7, %nctaid.x;
	mov.u32 	%r8, %nctaid.y;
	mul.wide.u32 	%rd16, %r7, %r8;
	mul.lo.s64 	%rd17, %rd16, %rd1;
	shr.u64 	%rd18, %rd17, 5;
	mul.lo.s64 	%rd19, %rd18, %rd15;
	cvt.s64.s32 	%rd20, %r3;
	shr.u64 	%rd21, %rd2, 5;
	mul.lo.s64 	%rd22, %rd21, %rd20;
	mov.u32 	%r9, %ctaid.x;
	cvt.u64.u32 	%rd4, %r9;
	add.s64 	%rd23, %rd19, %rd4;
	add.s64 	%rd5, %rd23, %rd22;
	setp.ge.s64 	%p2, %rd3, %rd8;
	mov.f32 	%f72, 0f00000000;
	mov.f32 	%f68, %f72;
	mov.f32 	%f69, %f72;
	mov.f32 	%f70, %f72;
	mov.f32 	%f71, %f72;
	@%p2 bra 	$L__BB2_3;

	cvta.to.global.u64 	%rd24, %rd6;
	div.u32 	%r10, %r1, %r2;
	cvt.u64.u32 	%rd25, %r10;
	mul.lo.s64 	%rd26, %rd25, %rd9;
	mul.lo.s32 	%r11, %r10, %r2;
	sub.s32 	%r12, %r1, %r11;
	cvt.u64.u32 	%rd27, %r12;
	mul.lo.s64 	%rd28, %rd27, %rd10;
	mul.lo.s64 	%rd30, %rd4, %rd11;
	add.s64 	%rd31, %rd30, %rd3;
	add.s64 	%rd32, %rd31, %rd28;
	add.s64 	%rd33, %rd32, %rd26;
	shr.s64 	%rd34, %rd33, 63;
	shr.u64 	%rd35, %rd34, 62;
	add.s64 	%rd36, %rd33, %rd35;
	shl.b64 	%rd37, %rd36, 2;
	and.b64  	%rd38, %rd37, -16;
	add.s64 	%rd39, %rd24, %rd38;
	ld.global.nc.v4.f32 	{%f68, %f69, %f70, %f71}, [%rd39];

$L__BB2_3:
	abs.f32 	%f28, %f68;
	abs.f32 	%f29, %f69;
	max.f32 	%f30, %f28, %f29;
	abs.f32 	%f31, %f70;
	max.f32 	%f32, %f30, %f31;
	abs.f32 	%f33, %f71;
	max.f32 	%f34, %f32, %f33;
	mov.b32 	%r13, %f34;
	mov.u32 	%r14, 31;
	mov.u32 	%r15, 4;
	mov.u32 	%r16, -1;
	shfl.sync.bfly.b32 	%r17|%p3, %r13, %r15, %r14, %r16;
	mov.b32 	%f35, %r17;
	max.f32 	%f36, %f34, %f35;
	mov.b32 	%r18, %f36;
	mov.u32 	%r19, 2;
	shfl.sync.bfly.b32 	%r20|%p4, %r18, %r19, %r14, %r16;
	mov.b32 	%f37, %r20;
	max.f32 	%f38, %f36, %f37;
	mov.b32 	%r21, %f38;
	mov.u32 	%r22, 1;
	shfl.sync.bfly.b32 	%r23|%p5, %r21, %r22, %r14, %r16;
	mov.b32 	%f39, %r23;
	max.f32 	%f13, %f38, %f39;
	add.f32 	%f40, %f68, %f69;
	add.f32 	%f41, %f70, %f40;
	add.f32 	%f42, %f71, %f41;
	mov.b32 	%r24, %f42;
	shfl.sync.bfly.b32 	%r25|%p6, %r24, %r15, %r14, %r16;
	mov.b32 	%f43, %r25;
	add.f32 	%f44, %f42, %f43;
	mov.b32 	%r26, %f44;
	shfl.sync.bfly.b32 	%r27|%p7, %r26, %r19, %r14, %r16;
	mov.b32 	%f45, %r27;
	add.f32 	%f46, %f44, %f45;
	mov.b32 	%r28, %f46;
	shfl.sync.bfly.b32 	%r29|%p8, %r28, %r22, %r14, %r16;
	mov.b32 	%f47, %r29;
	add.f32 	%f14, %f46, %f47;
	setp.leu.f32 	%p9, %f13, 0f00000000;
	@%p9 bra 	$L__BB2_5;

	mov.f32 	%f48, 0f42FE0000;
	div.rn.f32 	%f72, %f48, %f13;

$L__BB2_5:
	cvta.to.global.u64 	%rd40, %rd7;
	mul.f32 	%f49, %f68, %f72;
	mov.b32 	%r30, %f49;
	and.b32  	%r31, %r30, -2147483648;
	or.b32  	%r32, %r31, 1056964608;
	mov.b32 	%f50, %r32;
	add.rz.f32 	%f51, %f49, %f50;
	cvt.rzi.f32.f32 	%f52, %f51;
	cvt.rzi.s32.f32 	%r33, %f52;
	mul.f32 	%f53, %f69, %f72;
	mov.b32 	%r34, %f53;
	and.b32  	%r35, %r34, -2147483648;
	or.b32  	%r36, %r35, 1056964608;
	mov.b32 	%f54, %r36;
	add.rz.f32 	%f55, %f53, %f54;
	cvt.rzi.f32.f32 	%f56, %f55;
	cvt.rzi.s32.f32 	%r37, %f56;
	mul.f32 	%f57, %f70, %f72;
	mov.b32 	%r38, %f57;
	and.b32  	%r39, %r38, -2147483648;
	or.b32  	%r40, %r39, 1056964608;
	mov.b32 	%f58, %r40;
	add.rz.f32 	%f59, %f57, %f58;
	cvt.rzi.f32.f32 	%f60, %f59;
	cvt.rzi.s32.f32 	%r41, %f60;
	mul.f32 	%f61, %f71, %f72;
	mov.b32 	%r42, %f61;
	and.b32  	%r43, %r42, -2147483648;
	or.b32  	%r44, %r43, 1056964608;
	mov.b32 	%f62, %r44;
	add.rz.f32 	%f63, %f61, %f62;
	cvt.rzi.f32.f32 	%f64, %f63;
	cvt.rzi.s32.f32 	%r45, %f64;
	mul.lo.s64 	%rd41, %rd5, 144;
	add.s64 	%rd42, %rd40, %rd41;
	and.b64  	%rd47, %rd3, 124;
	add.s64 	%rd48, %rd42, %rd47;
	cvt.u16.u32 	%rs1, %r45;
	cvt.u16.u32 	%rs2, %r41;
	cvt.u16.u32 	%rs3, %r37;
	cvt.u16.u32 	%rs4, %r33;
	st.global.v4.u8 	[%rd48+16], {%rs4, %rs3, %rs2, %rs1};
	and.b64  	%rd49, %rd2, 7;
	setp.ne.s64 	%p10, %rd49, 0;
	@%p10 bra 	$L__BB2_9;

	setp.leu.f32 	%p11, %f72, 0f00000000;
	mov.f32 	%f73, 0f00000000;
	@%p11 bra 	$L__BB2_8;

	rcp.rn.f32 	%f73, %f72;

$L__BB2_8:
	cvt.u64.u32 	%rd52, %r5;
	mul.lo.s64 	%rd53, %rd1, %rd52;
	add.s64 	%rd55, %rd53, %rd14;
	shr.u64 	%rd56, %rd55, 1;
	and.b64  	%rd59, %rd56, 12;
	add.s64 	%rd60, %rd42, %rd59;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs6, %f14;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs5, %f73;}

	// end inline asm
	st.global.v2.u16 	[%rd60], {%rs5, %rs6};

$L__BB2_9:
	ret;

}
	// .globl	quantize_mmq_q8_1_fast_nd5
.visible .entry quantize_mmq_q8_1_fast_nd5(
	.param .u64 quantize_mmq_q8_1_fast_nd5_param_0,
	.param .u64 quantize_mmq_q8_1_fast_nd5_param_1,
	.param .u64 quantize_mmq_q8_1_fast_nd5_param_2,
	.param .u64 quantize_mmq_q8_1_fast_nd5_param_3,
	.param .u64 quantize_mmq_q8_1_fast_nd5_param_4,
	.param .u64 quantize_mmq_q8_1_fast_nd5_param_5,
	.param .u64 quantize_mmq_q8_1_fast_nd5_param_6,
	.param .u64 quantize_mmq_q8_1_fast_nd5_param_7,
	.param .u32 quantize_mmq_q8_1_fast_nd5_param_8,
	.param .u32 quantize_mmq_q8_1_fast_nd5_param_9,
	.param .u32 quantize_mmq_q8_1_fast_nd5_param_10,
	.param .u64 quantize_mmq_q8_1_fast_nd5_param_11
)
{
	.reg .pred 	%p<12>;
	.reg .b16 	%rs<7>;
	.reg .f32 	%f<74>;
	.reg .b32 	%r<56>;
	.reg .b64 	%rd<64>;


	ld.param.u64 	%rd6, [quantize_mmq_q8_1_fast_nd5_param_0];
	ld.param.u64 	%rd7, [quantize_mmq_q8_1_fast_nd5_param_1];
	ld.param.u64 	%rd8, [quantize_mmq_q8_1_fast_nd5_param_2];
	ld.param.u64 	%rd9, [quantize_mmq_q8_1_fast_nd5_param_4];
	ld.param.u64 	%rd10, [quantize_mmq_q8_1_fast_nd5_param_5];
	ld.param.u64 	%rd11, [quantize_mmq_q8_1_fast_nd5_param_6];
	ld.param.u64 	%rd12, [quantize_mmq_q8_1_fast_nd5_param_7];
	ld.param.u32 	%r2, [quantize_mmq_q8_1_fast_nd5_param_8];
	ld.param.u32 	%r3, [quantize_mmq_q8_1_fast_nd5_param_9];
	ld.param.u32 	%r4, [quantize_mmq_q8_1_fast_nd5_param_10];
	ld.param.u64 	%rd13, [quantize_mmq_q8_1_fast_nd5_param_11];
	mov.u32 	%r5, %ntid.x;
	cvt.u64.u32 	%rd1, %r5;
	mov.u32 	%r6, %ctaid.y;
	mul.wide.u32 	%rd14, %r5, %r6;
	mov.u32 	%r7, %tid.x;
	cvt.u64.u32 	%rd15, %r7;
	add.s64 	%rd2, %rd14, %rd15;
	shl.b64 	%rd3, %rd2, 2;
	setp.ge.s64 	%p1, %rd3, %rd13;
	@%p1 bra 	$L__BB3_9;

	mov.u32 	%r1, %ctaid.z;
	cvt.u64.u32 	%rd16, %r1;
	mov.u32 	%r8, %nctaid.x;
	mov.u32 	%r9, %nctaid.y;
	mul.wide.u32 	%rd17, %r8, %r9;
	mul.lo.s64 	%rd18, %rd17, %rd1;
	shr.u64 	%rd19, %rd18, 5;
	mul.lo.s64 	%rd20, %rd19, %rd16;
	cvt.s64.s32 	%rd21, %r4;
	shr.u64 	%rd22, %rd2, 5;
	mul.lo.s64 	%rd23, %rd22, %rd21;
	mov.u32 	%r10, %ctaid.x;
	cvt.u64.u32 	%rd4, %r10;
	add.s64 	%rd24, %rd20, %rd4;
	add.s64 	%rd5, %rd24, %rd23;
	setp.ge.s64 	%p2, %rd3, %rd8;
	mov.f32 	%f72, 0f00000000;
	mov.f32 	%f68, %f72;
	mov.f32 	%f69, %f72;
	mov.f32 	%f70, %f72;
	mov.f32 	%f71, %f72;
	@%p2 bra 	$L__BB3_3;

	cvta.to.global.u64 	%rd25, %rd6;
	mul.lo.s32 	%r11, %r3, %r2;
	div.u32 	%r12, %r1, %r11;
	cvt.u64.u32 	%rd26, %r12;
	mul.lo.s64 	%rd27, %rd26, %rd12;
	div.u32 	%r13, %r1, %r3;
	rem.u32 	%r14, %r13, %r2;
	cvt.u64.u32 	%rd28, %r14;
	mul.lo.s64 	%rd29, %rd28, %rd9;
	mul.lo.s32 	%r15, %r13, %r3;
	sub.s32 	%r16, %r1, %r15;
	cvt.u64.u32 	%rd30, %r16;
	mul.lo.s64 	%rd31, %rd30, %rd10;
	mul.lo.s64 	%rd32, %rd4, %rd11;
	add.s64 	%rd33, %rd32, %rd3;
	add.s64 	%rd34, %rd33, %rd31;
	add.s64 	%rd35, %rd34, %rd29;
	add.s64 	%rd36, %rd35, %rd27;
	shr.s64 	%rd37, %rd36, 63;
	shr.u64 	%rd38, %rd37, 62;
	add.s64 	%rd39, %rd36, %rd38;
	shl.b64 	%rd40, %rd39, 2;
	and.b64  	%rd41, %rd40, -16;
	add.s64 	%rd42, %rd25, %rd41;
	ld.global.nc.v4.f32 	{%f68, %f69, %f70, %f71}, [%rd42];

$L__BB3_3:
	abs.f32 	%f28, %f68;
	abs.f32 	%f29, %f69;
	max.f32 	%f30, %f28, %f29;
	abs.f32 	%f31, %f70;
	max.f32 	%f32, %f30, %f31;
	abs.f32 	%f33, %f71;
	max.f32 	%f34, %f32, %f33;
	mov.b32 	%r17, %f34;
	mov.u32 	%r18, 31;
	mov.u32 	%r19, 4;
	mov.u32 	%r20, -1;
	shfl.sync.bfly.b32 	%r21|%p3, %r17, %r19, %r18, %r20;
	mov.b32 	%f35, %r21;
	max.f32 	%f36, %f34, %f35;
	mov.b32 	%r22, %f36;
	mov.u32 	%r23, 2;
	shfl.sync.bfly.b32 	%r24|%p4, %r22, %r23, %r18, %r20;
	mov.b32 	%f37, %r24;
	max.f32 	%f38, %f36, %f37;
	mov.b32 	%r25, %f38;
	mov.u32 	%r26, 1;
	shfl.sync.bfly.b32 	%r27|%p5, %r25, %r26, %r18, %r20;
	mov.b32 	%f39, %r27;
	max.f32 	%f13, %f38, %f39;
	add.f32 	%f40, %f68, %f69;
	add.f32 	%f41, %f70, %f40;
	add.f32 	%f42, %f71, %f41;
	mov.b32 	%r28, %f42;
	shfl.sync.bfly.b32 	%r29|%p6, %r28, %r19, %r18, %r20;
	mov.b32 	%f43, %r29;
	add.f32 	%f44, %f42, %f43;
	mov.b32 	%r30, %f44;
	shfl.sync.bfly.b32 	%r31|%p7, %r30, %r23, %r18, %r20;
	mov.b32 	%f45, %r31;
	add.f32 	%f46, %f44, %f45;
	mov.b32 	%r32, %f46;
	shfl.sync.bfly.b32 	%r33|%p8, %r32, %r26, %r18, %r20;
	mov.b32 	%f47, %r33;
	add.f32 	%f14, %f46, %f47;
	setp.leu.f32 	%p9, %f13, 0f00000000;
	@%p9 bra 	$L__BB3_5;

	mov.f32 	%f48, 0f42FE0000;
	div.rn.f32 	%f72, %f48, %f13;

$L__BB3_5:
	cvta.to.global.u64 	%rd43, %rd7;
	mul.f32 	%f49, %f68, %f72;
	mov.b32 	%r34, %f49;
	and.b32  	%r35, %r34, -2147483648;
	or.b32  	%r36, %r35, 1056964608;
	mov.b32 	%f50, %r36;
	add.rz.f32 	%f51, %f49, %f50;
	cvt.rzi.f32.f32 	%f52, %f51;
	cvt.rzi.s32.f32 	%r37, %f52;
	mul.f32 	%f53, %f69, %f72;
	mov.b32 	%r38, %f53;
	and.b32  	%r39, %r38, -2147483648;
	or.b32  	%r40, %r39, 1056964608;
	mov.b32 	%f54, %r40;
	add.rz.f32 	%f55, %f53, %f54;
	cvt.rzi.f32.f32 	%f56, %f55;
	cvt.rzi.s32.f32 	%r41, %f56;
	mul.f32 	%f57, %f70, %f72;
	mov.b32 	%r42, %f57;
	and.b32  	%r43, %r42, -2147483648;
	or.b32  	%r44, %r43, 1056964608;
	mov.b32 	%f58, %r44;
	add.rz.f32 	%f59, %f57, %f58;
	cvt.rzi.f32.f32 	%f60, %f59;
	cvt.rzi.s32.f32 	%r45, %f60;
	mul.f32 	%f61, %f71, %f72;
	mov.b32 	%r46, %f61;
	and.b32  	%r47, %r46, -2147483648;
	or.b32  	%r48, %r47, 1056964608;
	mov.b32 	%f62, %r48;
	add.rz.f32 	%f63, %f61, %f62;
	cvt.rzi.f32.f32 	%f64, %f63;
	cvt.rzi.s32.f32 	%r49, %f64;
	mul.lo.s64 	%rd44, %rd5, 144;
	add.s64 	%rd45, %rd43, %rd44;
	and.b64  	%rd50, %rd3, 124;
	add.s64 	%rd51, %rd45, %rd50;
	cvt.u16.u32 	%rs1, %r49;
	cvt.u16.u32 	%rs2, %r45;
	cvt.u16.u32 	%rs3, %r41;
	cvt.u16.u32 	%rs4, %r37;
	st.global.v4.u8 	[%rd51+16], {%rs4, %rs3, %rs2, %rs1};
	and.b64  	%rd52, %rd2, 7;
	setp.ne.s64 	%p10, %rd52, 0;
	@%p10 bra 	$L__BB3_9;

	setp.leu.f32 	%p11, %f72, 0f00000000;
	mov.f32 	%f73, 0f00000000;
	@%p11 bra 	$L__BB3_8;

	rcp.rn.f32 	%f73, %f72;

$L__BB3_8:
	cvt.u64.u32 	%rd55, %r6;
	mul.lo.s64 	%rd56, %rd1, %rd55;
	add.s64 	%rd58, %rd56, %rd15;
	shr.u64 	%rd59, %rd58, 1;
	and.b64  	%rd62, %rd59, 12;
	add.s64 	%rd63, %rd45, %rd62;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs6, %f14;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs5, %f73;}

	// end inline asm
	st.global.v2.u16 	[%rd63], {%rs5, %rs6};

$L__BB3_9:
	ret;

}
	// .globl	quantize_mmq_q8_1_nd2
.visible .entry quantize_mmq_q8_1_nd2(
	.param .u64 quantize_mmq_q8_1_nd2_param_0,
	.param .u64 quantize_mmq_q8_1_nd2_param_1,
	.param .u64 quantize_mmq_q8_1_nd2_param_2,
	.param .u64 quantize_mmq_q8_1_nd2_param_3,
	.param .u64 quantize_mmq_q8_1_nd2_param_4,
	.param .u64 quantize_mmq_q8_1_nd2_param_5
)
{
	.reg .pred 	%p<15>;
	.reg .b16 	%rs<7>;
	.reg .f32 	%f<66>;
	.reg .b32 	%r<41>;
	.reg .b64 	%rd<54>;


	ld.param.u64 	%rd13, [quantize_mmq_q8_1_nd2_param_0];
	ld.param.u64 	%rd14, [quantize_mmq_q8_1_nd2_param_1];
	ld.param.u64 	%rd10, [quantize_mmq_q8_1_nd2_param_2];
	ld.param.u64 	%rd11, [quantize_mmq_q8_1_nd2_param_3];
	ld.param.u64 	%rd12, [quantize_mmq_q8_1_nd2_param_4];
	ld.param.u64 	%rd15, [quantize_mmq_q8_1_nd2_param_5];
	cvta.to.global.u64 	%rd1, %rd14;
	cvta.to.global.u64 	%rd2, %rd13;
	mov.u32 	%r1, %ntid.x;
	cvt.u64.u32 	%rd3, %r1;
	mov.u32 	%r2, %ctaid.y;
	mul.wide.u32 	%rd16, %r1, %r2;
	mov.u32 	%r3, %tid.x;
	cvt.u64.u32 	%rd17, %r3;
	add.s64 	%rd4, %rd16, %rd17;
	shl.b64 	%rd5, %rd4, 2;
	setp.ge.s64 	%p1, %rd5, %rd15;
	@%p1 bra 	$L__BB4_15;

	mov.u32 	%r4, %ctaid.x;
	cvt.u64.u32 	%rd6, %r4;
	mul.lo.s64 	%rd18, %rd6, %rd11;
	mul.lo.s64 	%rd19, %rd5, %rd12;
	add.s64 	%rd7, %rd19, %rd18;
	setp.ge.s64 	%p2, %rd5, %rd10;
	mov.f32 	%f61, 0f00000000;
	mov.f32 	%f60, %f61;
	@%p2 bra 	$L__BB4_3;

	shl.b64 	%rd20, %rd7, 2;
	add.s64 	%rd21, %rd2, %rd20;
	ld.global.nc.f32 	%f60, [%rd21];

$L__BB4_3:
	add.s64 	%rd22, %rd5, 1;
	setp.ge.s64 	%p3, %rd22, %rd10;
	@%p3 bra 	$L__BB4_5;

	add.s64 	%rd23, %rd7, %rd12;
	shl.b64 	%rd24, %rd23, 2;
	add.s64 	%rd25, %rd2, %rd24;
	ld.global.nc.f32 	%f61, [%rd25];

$L__BB4_5:
	add.s64 	%rd26, %rd5, 2;
	setp.ge.s64 	%p4, %rd26, %rd10;
	mov.f32 	%f63, 0f00000000;
	mov.f32 	%f62, %f63;
	@%p4 bra 	$L__BB4_7;

	shl.b64 	%rd27, %rd12, 1;
	add.s64 	%rd28, %rd7, %rd27;
	shl.b64 	%rd29, %rd28, 2;
	add.s64 	%rd30, %rd2, %rd29;
	ld.global.nc.f32 	%f62, [%rd30];

$L__BB4_7:
	add.s64 	%rd31, %rd5, 3;
	setp.ge.s64 	%p5, %rd31, %rd10;
	@%p5 bra 	$L__BB4_9;

	mul.lo.s64 	%rd32, %rd12, 3;
	add.s64 	%rd33, %rd7, %rd32;
	shl.b64 	%rd34, %rd33, 2;
	add.s64 	%rd35, %rd2, %rd34;
	ld.global.nc.f32 	%f63, [%rd35];

$L__BB4_9:
	mov.u32 	%r5, %ctaid.z;
	cvt.u64.u32 	%rd36, %r5;
	mov.u32 	%r6, %nctaid.x;
	cvt.u64.u32 	%rd37, %r6;
	mov.u32 	%r7, %nctaid.y;
	mul.wide.u32 	%rd38, %r6, %r7;
	mul.lo.s64 	%rd39, %rd38, %rd3;
	shr.u64 	%rd40, %rd39, 5;
	mul.lo.s64 	%rd41, %rd40, %rd36;
	shr.u64 	%rd42, %rd4, 5;
	mul.lo.s64 	%rd43, %rd42, %rd37;
	add.s64 	%rd44, %rd43, %rd6;
	and.b64  	%rd8, %rd5, 124;
	abs.f32 	%f20, %f61;
	abs.f32 	%f21, %f60;
	max.f32 	%f22, %f21, %f20;
	abs.f32 	%f23, %f62;
	max.f32 	%f24, %f22, %f23;
	abs.f32 	%f25, %f63;
	max.f32 	%f26, %f24, %f25;
	mov.b32 	%r8, %f26;
	mov.u32 	%r9, 31;
	mov.u32 	%r10, 4;
	mov.u32 	%r11, -1;
	shfl.sync.bfly.b32 	%r12|%p6, %r8, %r10, %r9, %r11;
	mov.b32 	%f27, %r12;
	max.f32 	%f28, %f26, %f27;
	mov.b32 	%r13, %f28;
	mov.u32 	%r14, 2;
	shfl.sync.bfly.b32 	%r15|%p7, %r13, %r14, %r9, %r11;
	mov.b32 	%f29, %r15;
	max.f32 	%f30, %f28, %f29;
	mov.b32 	%r16, %f30;
	mov.u32 	%r17, 1;
	shfl.sync.bfly.b32 	%r18|%p8, %r16, %r17, %r9, %r11;
	mov.b32 	%f31, %r18;
	max.f32 	%f9, %f30, %f31;
	add.s64 	%rd9, %rd44, %rd41;
	add.f32 	%f32, %f60, %f61;
	add.f32 	%f33, %f32, %f62;
	add.f32 	%f34, %f33, %f63;
	mov.b32 	%r19, %f34;
	shfl.sync.bfly.b32 	%r20|%p9, %r19, %r10, %r9, %r11;
	mov.b32 	%f35, %r20;
	add.f32 	%f36, %f34, %f35;
	mov.b32 	%r21, %f36;
	shfl.sync.bfly.b32 	%r22|%p10, %r21, %r14, %r9, %r11;
	mov.b32 	%f37, %r22;
	add.f32 	%f38, %f36, %f37;
	mov.b32 	%r23, %f38;
	shfl.sync.bfly.b32 	%r24|%p11, %r23, %r17, %r9, %r11;
	mov.b32 	%f39, %r24;
	add.f32 	%f10, %f38, %f39;
	setp.leu.f32 	%p12, %f9, 0f00000000;
	mov.f32 	%f64, 0f00000000;
	@%p12 bra 	$L__BB4_11;

	mov.f32 	%f40, 0f42FE0000;
	div.rn.f32 	%f64, %f40, %f9;

$L__BB4_11:
	mul.f32 	%f41, %f60, %f64;
	mov.b32 	%r25, %f41;
	and.b32  	%r26, %r25, -2147483648;
	or.b32  	%r27, %r26, 1056964608;
	mov.b32 	%f42, %r27;
	add.rz.f32 	%f43, %f41, %f42;
	cvt.rzi.f32.f32 	%f44, %f43;
	cvt.rzi.s32.f32 	%r28, %f44;
	mul.f32 	%f45, %f61, %f64;
	mov.b32 	%r29, %f45;
	and.b32  	%r30, %r29, -2147483648;
	or.b32  	%r31, %r30, 1056964608;
	mov.b32 	%f46, %r31;
	add.rz.f32 	%f47, %f45, %f46;
	cvt.rzi.f32.f32 	%f48, %f47;
	cvt.rzi.s32.f32 	%r32, %f48;
	mul.f32 	%f49, %f62, %f64;
	mov.b32 	%r33, %f49;
	and.b32  	%r34, %r33, -2147483648;
	or.b32  	%r35, %r34, 1056964608;
	mov.b32 	%f50, %r35;
	add.rz.f32 	%f51, %f49, %f50;
	cvt.rzi.f32.f32 	%f52, %f51;
	cvt.rzi.s32.f32 	%r36, %f52;
	mul.f32 	%f53, %f63, %f64;
	mov.b32 	%r37, %f53;
	and.b32  	%r38, %r37, -2147483648;
	or.b32  	%r39, %r38, 1056964608;
	mov.b32 	%f54, %r39;
	add.rz.f32 	%f55, %f53, %f54;
	cvt.rzi.f32.f32 	%f56, %f55;
	cvt.rzi.s32.f32 	%r40, %f56;
	mul.lo.s64 	%rd45, %rd9, 144;
	add.s64 	%rd46, %rd1, %rd45;
	add.s64 	%rd47, %rd46, %rd8;
	cvt.u16.u32 	%rs1, %r40;
	cvt.u16.u32 	%rs2, %r36;
	cvt.u16.u32 	%rs3, %r32;
	cvt.u16.u32 	%rs4, %r28;
	st.global.v4.u8 	[%rd47+16], {%rs4, %rs3, %rs2, %rs1};
	and.b64  	%rd48, %rd4, 7;
	setp.ne.s64 	%p13, %rd48, 0;
	@%p13 bra 	$L__BB4_15;

	setp.leu.f32 	%p14, %f64, 0f00000000;
	mov.f32 	%f65, 0f00000000;
	@%p14 bra 	$L__BB4_14;

	rcp.rn.f32 	%f65, %f64;

$L__BB4_14:
	shr.u64 	%rd51, %rd8, 3;
	and.b64  	%rd52, %rd51, 12;
	add.s64 	%rd53, %rd46, %rd52;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs6, %f10;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs5, %f65;}

	// end inline asm
	st.global.v2.u16 	[%rd53], {%rs5, %rs6};

$L__BB4_15:
	ret;

}
	// .globl	quantize_mmq_q8_1_nd3
.visible .entry quantize_mmq_q8_1_nd3(
	.param .u64 quantize_mmq_q8_1_nd3_param_0,
	.param .u64 quantize_mmq_q8_1_nd3_param_1,
	.param .u64 quantize_mmq_q8_1_nd3_param_2,
	.param .u64 quantize_mmq_q8_1_nd3_param_3,
	.param .u64 quantize_mmq_q8_1_nd3_param_4,
	.param .u64 quantize_mmq_q8_1_nd3_param_5,
	.param .u32 quantize_mmq_q8_1_nd3_param_6,
	.param .u64 quantize_mmq_q8_1_nd3_param_7
)
{
	.reg .pred 	%p<15>;
	.reg .b16 	%rs<7>;
	.reg .f32 	%f<66>;
	.reg .b32 	%r<42>;
	.reg .b64 	%rd<57>;


	ld.param.u64 	%rd15, [quantize_mmq_q8_1_nd3_param_0];
	ld.param.u64 	%rd16, [quantize_mmq_q8_1_nd3_param_1];
	ld.param.u64 	%rd11, [quantize_mmq_q8_1_nd3_param_2];
	ld.param.u64 	%rd12, [quantize_mmq_q8_1_nd3_param_3];
	ld.param.u64 	%rd13, [quantize_mmq_q8_1_nd3_param_4];
	ld.param.u64 	%rd14, [quantize_mmq_q8_1_nd3_param_5];
	ld.param.u32 	%r1, [quantize_mmq_q8_1_nd3_param_6];
	ld.param.u64 	%rd17, [quantize_mmq_q8_1_nd3_param_7];
	cvta.to.global.u64 	%rd1, %rd16;
	cvta.to.global.u64 	%rd2, %rd15;
	mov.u32 	%r2, %ntid.x;
	cvt.u64.u32 	%rd3, %r2;
	mov.u32 	%r3, %ctaid.y;
	mul.wide.u32 	%rd18, %r2, %r3;
	mov.u32 	%r4, %tid.x;
	cvt.u64.u32 	%rd19, %r4;
	add.s64 	%rd4, %rd18, %rd19;
	shl.b64 	%rd5, %rd4, 2;
	setp.ge.s64 	%p1, %rd5, %rd17;
	@%p1 bra 	$L__BB5_15;

	mov.u32 	%r5, %ctaid.x;
	cvt.u64.u32 	%rd6, %r5;
	mov.u32 	%r6, %ctaid.z;
	cvt.u64.u32 	%rd7, %r6;
	mul.lo.s64 	%rd20, %rd7, %rd12;
	mul.lo.s64 	%rd21, %rd6, %rd13;
	add.s64 	%rd22, %rd21, %rd20;
	mul.lo.s64 	%rd23, %rd5, %rd14;
	add.s64 	%rd8, %rd22, %rd23;
	setp.ge.s64 	%p2, %rd5, %rd11;
	mov.f32 	%f61, 0f00000000;
	mov.f32 	%f60, %f61;
	@%p2 bra 	$L__BB5_3;

	shl.b64 	%rd24, %rd8, 2;
	add.s64 	%rd25, %rd2, %rd24;
	ld.global.nc.f32 	%f60, [%rd25];

$L__BB5_3:
	add.s64 	%rd26, %rd5, 1;
	setp.ge.s64 	%p3, %rd26, %rd11;
	@%p3 bra 	$L__BB5_5;

	add.s64 	%rd27, %rd8, %rd14;
	shl.b64 	%rd28, %rd27, 2;
	add.s64 	%rd29, %rd2, %rd28;
	ld.global.nc.f32 	%f61, [%rd29];

$L__BB5_5:
	add.s64 	%rd30, %rd5, 2;
	setp.ge.s64 	%p4, %rd30, %rd11;
	mov.f32 	%f63, 0f00000000;
	mov.f32 	%f62, %f63;
	@%p4 bra 	$L__BB5_7;

	shl.b64 	%rd31, %rd14, 1;
	add.s64 	%rd32, %rd8, %rd31;
	shl.b64 	%rd33, %rd32, 2;
	add.s64 	%rd34, %rd2, %rd33;
	ld.global.nc.f32 	%f62, [%rd34];

$L__BB5_7:
	add.s64 	%rd35, %rd5, 3;
	setp.ge.s64 	%p5, %rd35, %rd11;
	@%p5 bra 	$L__BB5_9;

	mul.lo.s64 	%rd36, %rd14, 3;
	add.s64 	%rd37, %rd8, %rd36;
	shl.b64 	%rd38, %rd37, 2;
	add.s64 	%rd39, %rd2, %rd38;
	ld.global.nc.f32 	%f63, [%rd39];

$L__BB5_9:
	mov.u32 	%r7, %nctaid.x;
	mov.u32 	%r8, %nctaid.y;
	mul.wide.u32 	%rd40, %r7, %r8;
	mul.lo.s64 	%rd41, %rd40, %rd3;
	shr.u64 	%rd42, %rd41, 5;
	mul.lo.s64 	%rd43, %rd42, %rd7;
	cvt.s64.s32 	%rd44, %r1;
	shr.u64 	%rd45, %rd4, 5;
	mul.lo.s64 	%rd46, %rd45, %rd44;
	add.s64 	%rd47, %rd43, %rd6;
	and.b64  	%rd9, %rd5, 124;
	abs.f32 	%f20, %f61;
	abs.f32 	%f21, %f60;
	max.f32 	%f22, %f21, %f20;
	abs.f32 	%f23, %f62;
	max.f32 	%f24, %f22, %f23;
	abs.f32 	%f25, %f63;
	max.f32 	%f26, %f24, %f25;
	mov.b32 	%r9, %f26;
	mov.u32 	%r10, 31;
	mov.u32 	%r11, 4;
	mov.u32 	%r12, -1;
	shfl.sync.bfly.b32 	%r13|%p6, %r9, %r11, %r10, %r12;
	mov.b32 	%f27, %r13;
	max.f32 	%f28, %f26, %f27;
	mov.b32 	%r14, %f28;
	mov.u32 	%r15, 2;
	shfl.sync.bfly.b32 	%r16|%p7, %r14, %r15, %r10, %r12;
	mov.b32 	%f29, %r16;
	max.f32 	%f30, %f28, %f29;
	mov.b32 	%r17, %f30;
	mov.u32 	%r18, 1;
	shfl.sync.bfly.b32 	%r19|%p8, %r17, %r18, %r10, %r12;
	mov.b32 	%f31, %r19;
	max.f32 	%f9, %f30, %f31;
	add.s64 	%rd10, %rd47, %rd46;
	add.f32 	%f32, %f60, %f61;
	add.f32 	%f33, %f32, %f62;
	add.f32 	%f34, %f33, %f63;
	mov.b32 	%r20, %f34;
	shfl.sync.bfly.b32 	%r21|%p9, %r20, %r11, %r10, %r12;
	mov.b32 	%f35, %r21;
	add.f32 	%f36, %f34, %f35;
	mov.b32 	%r22, %f36;
	shfl.sync.bfly.b32 	%r23|%p10, %r22, %r15, %r10, %r12;
	mov.b32 	%f37, %r23;
	add.f32 	%f38, %f36, %f37;
	mov.b32 	%r24, %f38;
	shfl.sync.bfly.b32 	%r25|%p11, %r24, %r18, %r10, %r12;
	mov.b32 	%f39, %r25;
	add.f32 	%f10, %f38, %f39;
	setp.leu.f32 	%p12, %f9, 0f00000000;
	mov.f32 	%f64, 0f00000000;
	@%p12 bra 	$L__BB5_11;

	mov.f32 	%f40, 0f42FE0000;
	div.rn.f32 	%f64, %f40, %f9;

$L__BB5_11:
	mul.f32 	%f41, %f60, %f64;
	mov.b32 	%r26, %f41;
	and.b32  	%r27, %r26, -2147483648;
	or.b32  	%r28, %r27, 1056964608;
	mov.b32 	%f42, %r28;
	add.rz.f32 	%f43, %f41, %f42;
	cvt.rzi.f32.f32 	%f44, %f43;
	cvt.rzi.s32.f32 	%r29, %f44;
	mul.f32 	%f45, %f61, %f64;
	mov.b32 	%r30, %f45;
	and.b32  	%r31, %r30, -2147483648;
	or.b32  	%r32, %r31, 1056964608;
	mov.b32 	%f46, %r32;
	add.rz.f32 	%f47, %f45, %f46;
	cvt.rzi.f32.f32 	%f48, %f47;
	cvt.rzi.s32.f32 	%r33, %f48;
	mul.f32 	%f49, %f62, %f64;
	mov.b32 	%r34, %f49;
	and.b32  	%r35, %r34, -2147483648;
	or.b32  	%r36, %r35, 1056964608;
	mov.b32 	%f50, %r36;
	add.rz.f32 	%f51, %f49, %f50;
	cvt.rzi.f32.f32 	%f52, %f51;
	cvt.rzi.s32.f32 	%r37, %f52;
	mul.f32 	%f53, %f63, %f64;
	mov.b32 	%r38, %f53;
	and.b32  	%r39, %r38, -2147483648;
	or.b32  	%r40, %r39, 1056964608;
	mov.b32 	%f54, %r40;
	add.rz.f32 	%f55, %f53, %f54;
	cvt.rzi.f32.f32 	%f56, %f55;
	cvt.rzi.s32.f32 	%r41, %f56;
	mul.lo.s64 	%rd48, %rd10, 144;
	add.s64 	%rd49, %rd1, %rd48;
	add.s64 	%rd50, %rd49, %rd9;
	cvt.u16.u32 	%rs1, %r41;
	cvt.u16.u32 	%rs2, %r37;
	cvt.u16.u32 	%rs3, %r33;
	cvt.u16.u32 	%rs4, %r29;
	st.global.v4.u8 	[%rd50+16], {%rs4, %rs3, %rs2, %rs1};
	and.b64  	%rd51, %rd4, 7;
	setp.ne.s64 	%p13, %rd51, 0;
	@%p13 bra 	$L__BB5_15;

	setp.leu.f32 	%p14, %f64, 0f00000000;
	mov.f32 	%f65, 0f00000000;
	@%p14 bra 	$L__BB5_14;

	rcp.rn.f32 	%f65, %f64;

$L__BB5_14:
	shr.u64 	%rd54, %rd9, 3;
	and.b64  	%rd55, %rd54, 12;
	add.s64 	%rd56, %rd49, %rd55;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs6, %f10;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs5, %f65;}

	// end inline asm
	st.global.v2.u16 	[%rd56], {%rs5, %rs6};

$L__BB5_15:
	ret;

}
	// .globl	quantize_mmq_q8_1_nd4
.visible .entry quantize_mmq_q8_1_nd4(
	.param .u64 quantize_mmq_q8_1_nd4_param_0,
	.param .u64 quantize_mmq_q8_1_nd4_param_1,
	.param .u64 quantize_mmq_q8_1_nd4_param_2,
	.param .u64 quantize_mmq_q8_1_nd4_param_3,
	.param .u64 quantize_mmq_q8_1_nd4_param_4,
	.param .u64 quantize_mmq_q8_1_nd4_param_5,
	.param .u64 quantize_mmq_q8_1_nd4_param_6,
	.param .u32 quantize_mmq_q8_1_nd4_param_7,
	.param .u32 quantize_mmq_q8_1_nd4_param_8,
	.param .u64 quantize_mmq_q8_1_nd4_param_9
)
{
	.reg .pred 	%p<15>;
	.reg .b16 	%rs<7>;
	.reg .f32 	%f<66>;
	.reg .b32 	%r<46>;
	.reg .b64 	%rd<62>;


	ld.param.u64 	%rd15, [quantize_mmq_q8_1_nd4_param_0];
	ld.param.u64 	%rd16, [quantize_mmq_q8_1_nd4_param_1];
	ld.param.u64 	%rd10, [quantize_mmq_q8_1_nd4_param_2];
	ld.param.u64 	%rd11, [quantize_mmq_q8_1_nd4_param_3];
	ld.param.u64 	%rd12, [quantize_mmq_q8_1_nd4_param_4];
	ld.param.u64 	%rd13, [quantize_mmq_q8_1_nd4_param_5];
	ld.param.u64 	%rd14, [quantize_mmq_q8_1_nd4_param_6];
	ld.param.u32 	%r2, [quantize_mmq_q8_1_nd4_param_7];
	ld.param.u32 	%r3, [quantize_mmq_q8_1_nd4_param_8];
	ld.param.u64 	%rd17, [quantize_mmq_q8_1_nd4_param_9];
	cvta.to.global.u64 	%rd1, %rd16;
	cvta.to.global.u64 	%rd2, %rd15;
	mov.u32 	%r4, %ntid.x;
	cvt.u64.u32 	%rd3, %r4;
	mov.u32 	%r5, %ctaid.y;
	mul.wide.u32 	%rd18, %r4, %r5;
	mov.u32 	%r6, %tid.x;
	cvt.u64.u32 	%rd19, %r6;
	add.s64 	%rd4, %rd18, %rd19;
	shl.b64 	%rd5, %rd4, 2;
	setp.ge.s64 	%p1, %rd5, %rd17;
	@%p1 bra 	$L__BB6_15;

	mov.u32 	%r7, %ctaid.x;
	cvt.u64.u32 	%rd6, %r7;
	mov.u32 	%r1, %ctaid.z;
	div.u32 	%r8, %r1, %r2;
	mul.lo.s32 	%r9, %r8, %r2;
	sub.s32 	%r10, %r1, %r9;
	cvt.u64.u32 	%rd20, %r10;
	cvt.u64.u32 	%rd21, %r8;
	mul.lo.s64 	%rd22, %rd21, %rd11;
	mul.lo.s64 	%rd23, %rd20, %rd12;
	mul.lo.s64 	%rd24, %rd6, %rd13;
	mul.lo.s64 	%rd25, %rd5, %rd14;
	add.s64 	%rd26, %rd25, %rd24;
	add.s64 	%rd27, %rd26, %rd23;
	add.s64 	%rd7, %rd27, %rd22;
	setp.ge.s64 	%p2, %rd5, %rd10;
	mov.f32 	%f61, 0f00000000;
	mov.f32 	%f60, %f61;
	@%p2 bra 	$L__BB6_3;

	shl.b64 	%rd28, %rd7, 2;
	add.s64 	%rd29, %rd2, %rd28;
	ld.global.nc.f32 	%f60, [%rd29];

$L__BB6_3:
	add.s64 	%rd30, %rd5, 1;
	setp.ge.s64 	%p3, %rd30, %rd10;
	@%p3 bra 	$L__BB6_5;

	add.s64 	%rd31, %rd7, %rd14;
	shl.b64 	%rd32, %rd31, 2;
	add.s64 	%rd33, %rd2, %rd32;
	ld.global.nc.f32 	%f61, [%rd33];

$L__BB6_5:
	add.s64 	%rd34, %rd5, 2;
	setp.ge.s64 	%p4, %rd34, %rd10;
	mov.f32 	%f63, 0f00000000;
	mov.f32 	%f62, %f63;
	@%p4 bra 	$L__BB6_7;

	shl.b64 	%rd35, %rd14, 1;
	add.s64 	%rd36, %rd7, %rd35;
	shl.b64 	%rd37, %rd36, 2;
	add.s64 	%rd38, %rd2, %rd37;
	ld.global.nc.f32 	%f62, [%rd38];

$L__BB6_7:
	add.s64 	%rd39, %rd5, 3;
	setp.ge.s64 	%p5, %rd39, %rd10;
	@%p5 bra 	$L__BB6_9;

	mul.lo.s64 	%rd40, %rd14, 3;
	add.s64 	%rd41, %rd7, %rd40;
	shl.b64 	%rd42, %rd41, 2;
	add.s64 	%rd43, %rd2, %rd42;
	ld.global.nc.f32 	%f63, [%rd43];

$L__BB6_9:
	mov.u32 	%r11, %nctaid.x;
	mov.u32 	%r12, %nctaid.y;
	mul.wide.u32 	%rd44, %r11, %r12;
	mul.lo.s64 	%rd45, %rd44, %rd3;
	shr.u64 	%rd46, %rd45, 5;
	cvt.u64.u32 	%rd47, %r1;
	mul.lo.s64 	%rd48, %rd46, %rd47;
	cvt.s64.s32 	%rd49, %r3;
	shr.u64 	%rd50, %rd4, 5;
	mul.lo.s64 	%rd51, %rd50, %rd49;
	add.s64 	%rd52, %rd48, %rd6;
	and.b64  	%rd8, %rd5, 124;
	abs.f32 	%f20, %f61;
	abs.f32 	%f21, %f60;
	max.f32 	%f22, %f21, %f20;
	abs.f32 	%f23, %f62;
	max.f32 	%f24, %f22, %f23;
	abs.f32 	%f25, %f63;
	max.f32 	%f26, %f24, %f25;
	mov.b32 	%r13, %f26;
	mov.u32 	%r14, 31;
	mov.u32 	%r15, 4;
	mov.u32 	%r16, -1;
	shfl.sync.bfly.b32 	%r17|%p6, %r13, %r15, %r14, %r16;
	mov.b32 	%f27, %r17;
	max.f32 	%f28, %f26, %f27;
	mov.b32 	%r18, %f28;
	mov.u32 	%r19, 2;
	shfl.sync.bfly.b32 	%r20|%p7, %r18, %r19, %r14, %r16;
	mov.b32 	%f29, %r20;
	max.f32 	%f30, %f28, %f29;
	mov.b32 	%r21, %f30;
	mov.u32 	%r22, 1;
	shfl.sync.bfly.b32 	%r23|%p8, %r21, %r22, %r14, %r16;
	mov.b32 	%f31, %r23;
	max.f32 	%f9, %f30, %f31;
	add.s64 	%rd9, %rd52, %rd51;
	add.f32 	%f32, %f60, %f61;
	add.f32 	%f33, %f32, %f62;
	add.f32 	%f34, %f33, %f63;
	mov.b32 	%r24, %f34;
	shfl.sync.bfly.b32 	%r25|%p9, %r24, %r15, %r14, %r16;
	mov.b32 	%f35, %r25;
	add.f32 	%f36, %f34, %f35;
	mov.b32 	%r26, %f36;
	shfl.sync.bfly.b32 	%r27|%p10, %r26, %r19, %r14, %r16;
	mov.b32 	%f37, %r27;
	add.f32 	%f38, %f36, %f37;
	mov.b32 	%r28, %f38;
	shfl.sync.bfly.b32 	%r29|%p11, %r28, %r22, %r14, %r16;
	mov.b32 	%f39, %r29;
	add.f32 	%f10, %f38, %f39;
	setp.leu.f32 	%p12, %f9, 0f00000000;
	mov.f32 	%f64, 0f00000000;
	@%p12 bra 	$L__BB6_11;

	mov.f32 	%f40, 0f42FE0000;
	div.rn.f32 	%f64, %f40, %f9;

$L__BB6_11:
	mul.f32 	%f41, %f60, %f64;
	mov.b32 	%r30, %f41;
	and.b32  	%r31, %r30, -2147483648;
	or.b32  	%r32, %r31, 1056964608;
	mov.b32 	%f42, %r32;
	add.rz.f32 	%f43, %f41, %f42;
	cvt.rzi.f32.f32 	%f44, %f43;
	cvt.rzi.s32.f32 	%r33, %f44;
	mul.f32 	%f45, %f61, %f64;
	mov.b32 	%r34, %f45;
	and.b32  	%r35, %r34, -2147483648;
	or.b32  	%r36, %r35, 1056964608;
	mov.b32 	%f46, %r36;
	add.rz.f32 	%f47, %f45, %f46;
	cvt.rzi.f32.f32 	%f48, %f47;
	cvt.rzi.s32.f32 	%r37, %f48;
	mul.f32 	%f49, %f62, %f64;
	mov.b32 	%r38, %f49;
	and.b32  	%r39, %r38, -2147483648;
	or.b32  	%r40, %r39, 1056964608;
	mov.b32 	%f50, %r40;
	add.rz.f32 	%f51, %f49, %f50;
	cvt.rzi.f32.f32 	%f52, %f51;
	cvt.rzi.s32.f32 	%r41, %f52;
	mul.f32 	%f53, %f63, %f64;
	mov.b32 	%r42, %f53;
	and.b32  	%r43, %r42, -2147483648;
	or.b32  	%r44, %r43, 1056964608;
	mov.b32 	%f54, %r44;
	add.rz.f32 	%f55, %f53, %f54;
	cvt.rzi.f32.f32 	%f56, %f55;
	cvt.rzi.s32.f32 	%r45, %f56;
	mul.lo.s64 	%rd53, %rd9, 144;
	add.s64 	%rd54, %rd1, %rd53;
	add.s64 	%rd55, %rd54, %rd8;
	cvt.u16.u32 	%rs1, %r45;
	cvt.u16.u32 	%rs2, %r41;
	cvt.u16.u32 	%rs3, %r37;
	cvt.u16.u32 	%rs4, %r33;
	st.global.v4.u8 	[%rd55+16], {%rs4, %rs3, %rs2, %rs1};
	and.b64  	%rd56, %rd4, 7;
	setp.ne.s64 	%p13, %rd56, 0;
	@%p13 bra 	$L__BB6_15;

	setp.leu.f32 	%p14, %f64, 0f00000000;
	mov.f32 	%f65, 0f00000000;
	@%p14 bra 	$L__BB6_14;

	rcp.rn.f32 	%f65, %f64;

$L__BB6_14:
	shr.u64 	%rd59, %rd8, 3;
	and.b64  	%rd60, %rd59, 12;
	add.s64 	%rd61, %rd54, %rd60;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs6, %f10;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs5, %f65;}

	// end inline asm
	st.global.v2.u16 	[%rd61], {%rs5, %rs6};

$L__BB6_15:
	ret;

}
	// .globl	quantize_mmq_q8_1_nd5
.visible .entry quantize_mmq_q8_1_nd5(
	.param .u64 quantize_mmq_q8_1_nd5_param_0,
	.param .u64 quantize_mmq_q8_1_nd5_param_1,
	.param .u64 quantize_mmq_q8_1_nd5_param_2,
	.param .u64 quantize_mmq_q8_1_nd5_param_3,
	.param .u64 quantize_mmq_q8_1_nd5_param_4,
	.param .u64 quantize_mmq_q8_1_nd5_param_5,
	.param .u64 quantize_mmq_q8_1_nd5_param_6,
	.param .u64 quantize_mmq_q8_1_nd5_param_7,
	.param .u32 quantize_mmq_q8_1_nd5_param_8,
	.param .u32 quantize_mmq_q8_1_nd5_param_9,
	.param .u32 quantize_mmq_q8_1_nd5_param_10,
	.param .u64 quantize_mmq_q8_1_nd5_param_11
)
{
	.reg .pred 	%p<15>;
	.reg .b16 	%rs<7>;
	.reg .f32 	%f<66>;
	.reg .b32 	%r<50>;
	.reg .b64 	%rd<66>;


	ld.param.u64 	%rd16, [quantize_mmq_q8_1_nd5_param_0];
	ld.param.u64 	%rd17, [quantize_mmq_q8_1_nd5_param_1];
	ld.param.u64 	%rd10, [quantize_mmq_q8_1_nd5_param_2];
	ld.param.u64 	%rd11, [quantize_mmq_q8_1_nd5_param_3];
	ld.param.u64 	%rd12, [quantize_mmq_q8_1_nd5_param_4];
	ld.param.u64 	%rd13, [quantize_mmq_q8_1_nd5_param_5];
	ld.param.u64 	%rd14, [quantize_mmq_q8_1_nd5_param_6];
	ld.param.u64 	%rd15, [quantize_mmq_q8_1_nd5_param_7];
	ld.param.u32 	%r2, [quantize_mmq_q8_1_nd5_param_8];
	ld.param.u32 	%r3, [quantize_mmq_q8_1_nd5_param_9];
	ld.param.u32 	%r4, [quantize_mmq_q8_1_nd5_param_10];
	ld.param.u64 	%rd18, [quantize_mmq_q8_1_nd5_param_11];
	cvta.to.global.u64 	%rd1, %rd17;
	cvta.to.global.u64 	%rd2, %rd16;
	mov.u32 	%r5, %ntid.x;
	cvt.u64.u32 	%rd3, %r5;
	mov.u32 	%r6, %ctaid.y;
	mul.wide.u32 	%rd19, %r5, %r6;
	mov.u32 	%r7, %tid.x;
	cvt.u64.u32 	%rd20, %r7;
	add.s64 	%rd4, %rd19, %rd20;
	shl.b64 	%rd5, %rd4, 2;
	setp.ge.s64 	%p1, %rd5, %rd18;
	@%p1 bra 	$L__BB7_15;

	mov.u32 	%r8, %ctaid.x;
	cvt.u64.u32 	%rd6, %r8;
	mov.u32 	%r1, %ctaid.z;
	div.u32 	%r9, %r1, %r3;
	mul.lo.s32 	%r10, %r9, %r3;
	sub.s32 	%r11, %r1, %r10;
	cvt.u64.u32 	%rd21, %r11;
	rem.u32 	%r12, %r9, %r2;
	cvt.u64.u32 	%rd22, %r12;
	mul.lo.s32 	%r13, %r3, %r2;
	div.u32 	%r14, %r1, %r13;
	cvt.u64.u32 	%rd23, %r14;
	mul.lo.s64 	%rd24, %rd23, %rd11;
	mul.lo.s64 	%rd25, %rd22, %rd12;
	mul.lo.s64 	%rd26, %rd21, %rd13;
	mul.lo.s64 	%rd27, %rd6, %rd14;
	mul.lo.s64 	%rd28, %rd5, %rd15;
	add.s64 	%rd29, %rd28, %rd27;
	add.s64 	%rd30, %rd29, %rd26;
	add.s64 	%rd31, %rd30, %rd25;
	add.s64 	%rd7, %rd31, %rd24;
	setp.ge.s64 	%p2, %rd5, %rd10;
	mov.f32 	%f61, 0f00000000;
	mov.f32 	%f60, %f61;
	@%p2 bra 	$L__BB7_3;

	shl.b64 	%rd32, %rd7, 2;
	add.s64 	%rd33, %rd2, %rd32;
	ld.global.nc.f32 	%f60, [%rd33];

$L__BB7_3:
	add.s64 	%rd34, %rd5, 1;
	setp.ge.s64 	%p3, %rd34, %rd10;
	@%p3 bra 	$L__BB7_5;

	add.s64 	%rd35, %rd7, %rd15;
	shl.b64 	%rd36, %rd35, 2;
	add.s64 	%rd37, %rd2, %rd36;
	ld.global.nc.f32 	%f61, [%rd37];

$L__BB7_5:
	add.s64 	%rd38, %rd5, 2;
	setp.ge.s64 	%p4, %rd38, %rd10;
	mov.f32 	%f63, 0f00000000;
	mov.f32 	%f62, %f63;
	@%p4 bra 	$L__BB7_7;

	shl.b64 	%rd39, %rd15, 1;
	add.s64 	%rd40, %rd7, %rd39;
	shl.b64 	%rd41, %rd40, 2;
	add.s64 	%rd42, %rd2, %rd41;
	ld.global.nc.f32 	%f62, [%rd42];

$L__BB7_7:
	add.s64 	%rd43, %rd5, 3;
	setp.ge.s64 	%p5, %rd43, %rd10;
	@%p5 bra 	$L__BB7_9;

	mul.lo.s64 	%rd44, %rd15, 3;
	add.s64 	%rd45, %rd7, %rd44;
	shl.b64 	%rd46, %rd45, 2;
	add.s64 	%rd47, %rd2, %rd46;
	ld.global.nc.f32 	%f63, [%rd47];

$L__BB7_9:
	mov.u32 	%r15, %nctaid.x;
	mov.u32 	%r16, %nctaid.y;
	mul.wide.u32 	%rd48, %r15, %r16;
	mul.lo.s64 	%rd49, %rd48, %rd3;
	shr.u64 	%rd50, %rd49, 5;
	cvt.u64.u32 	%rd51, %r1;
	mul.lo.s64 	%rd52, %rd50, %rd51;
	cvt.s64.s32 	%rd53, %r4;
	shr.u64 	%rd54, %rd4, 5;
	mul.lo.s64 	%rd55, %rd54, %rd53;
	add.s64 	%rd56, %rd52, %rd6;
	and.b64  	%rd8, %rd5, 124;
	abs.f32 	%f20, %f61;
	abs.f32 	%f21, %f60;
	max.f32 	%f22, %f21, %f20;
	abs.f32 	%f23, %f62;
	max.f32 	%f24, %f22, %f23;
	abs.f32 	%f25, %f63;
	max.f32 	%f26, %f24, %f25;
	mov.b32 	%r17, %f26;
	mov.u32 	%r18, 31;
	mov.u32 	%r19, 4;
	mov.u32 	%r20, -1;
	shfl.sync.bfly.b32 	%r21|%p6, %r17, %r19, %r18, %r20;
	mov.b32 	%f27, %r21;
	max.f32 	%f28, %f26, %f27;
	mov.b32 	%r22, %f28;
	mov.u32 	%r23, 2;
	shfl.sync.bfly.b32 	%r24|%p7, %r22, %r23, %r18, %r20;
	mov.b32 	%f29, %r24;
	max.f32 	%f30, %f28, %f29;
	mov.b32 	%r25, %f30;
	mov.u32 	%r26, 1;
	shfl.sync.bfly.b32 	%r27|%p8, %r25, %r26, %r18, %r20;
	mov.b32 	%f31, %r27;
	max.f32 	%f9, %f30, %f31;
	add.s64 	%rd9, %rd56, %rd55;
	add.f32 	%f32, %f60, %f61;
	add.f32 	%f33, %f32, %f62;
	add.f32 	%f34, %f33, %f63;
	mov.b32 	%r28, %f34;
	shfl.sync.bfly.b32 	%r29|%p9, %r28, %r19, %r18, %r20;
	mov.b32 	%f35, %r29;
	add.f32 	%f36, %f34, %f35;
	mov.b32 	%r30, %f36;
	shfl.sync.bfly.b32 	%r31|%p10, %r30, %r23, %r18, %r20;
	mov.b32 	%f37, %r31;
	add.f32 	%f38, %f36, %f37;
	mov.b32 	%r32, %f38;
	shfl.sync.bfly.b32 	%r33|%p11, %r32, %r26, %r18, %r20;
	mov.b32 	%f39, %r33;
	add.f32 	%f10, %f38, %f39;
	setp.leu.f32 	%p12, %f9, 0f00000000;
	mov.f32 	%f64, 0f00000000;
	@%p12 bra 	$L__BB7_11;

	mov.f32 	%f40, 0f42FE0000;
	div.rn.f32 	%f64, %f40, %f9;

$L__BB7_11:
	mul.f32 	%f41, %f60, %f64;
	mov.b32 	%r34, %f41;
	and.b32  	%r35, %r34, -2147483648;
	or.b32  	%r36, %r35, 1056964608;
	mov.b32 	%f42, %r36;
	add.rz.f32 	%f43, %f41, %f42;
	cvt.rzi.f32.f32 	%f44, %f43;
	cvt.rzi.s32.f32 	%r37, %f44;
	mul.f32 	%f45, %f61, %f64;
	mov.b32 	%r38, %f45;
	and.b32  	%r39, %r38, -2147483648;
	or.b32  	%r40, %r39, 1056964608;
	mov.b32 	%f46, %r40;
	add.rz.f32 	%f47, %f45, %f46;
	cvt.rzi.f32.f32 	%f48, %f47;
	cvt.rzi.s32.f32 	%r41, %f48;
	mul.f32 	%f49, %f62, %f64;
	mov.b32 	%r42, %f49;
	and.b32  	%r43, %r42, -2147483648;
	or.b32  	%r44, %r43, 1056964608;
	mov.b32 	%f50, %r44;
	add.rz.f32 	%f51, %f49, %f50;
	cvt.rzi.f32.f32 	%f52, %f51;
	cvt.rzi.s32.f32 	%r45, %f52;
	mul.f32 	%f53, %f63, %f64;
	mov.b32 	%r46, %f53;
	and.b32  	%r47, %r46, -2147483648;
	or.b32  	%r48, %r47, 1056964608;
	mov.b32 	%f54, %r48;
	add.rz.f32 	%f55, %f53, %f54;
	cvt.rzi.f32.f32 	%f56, %f55;
	cvt.rzi.s32.f32 	%r49, %f56;
	mul.lo.s64 	%rd57, %rd9, 144;
	add.s64 	%rd58, %rd1, %rd57;
	add.s64 	%rd59, %rd58, %rd8;
	cvt.u16.u32 	%rs1, %r49;
	cvt.u16.u32 	%rs2, %r45;
	cvt.u16.u32 	%rs3, %r41;
	cvt.u16.u32 	%rs4, %r37;
	st.global.v4.u8 	[%rd59+16], {%rs4, %rs3, %rs2, %rs1};
	and.b64  	%rd60, %rd4, 7;
	setp.ne.s64 	%p13, %rd60, 0;
	@%p13 bra 	$L__BB7_15;

	setp.leu.f32 	%p14, %f64, 0f00000000;
	mov.f32 	%f65, 0f00000000;
	@%p14 bra 	$L__BB7_14;

	rcp.rn.f32 	%f65, %f64;

$L__BB7_14:
	shr.u64 	%rd63, %rd8, 3;
	and.b64  	%rd64, %rd63, 12;
	add.s64 	%rd65, %rd58, %rd64;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs6, %f10;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs5, %f65;}

	// end inline asm
	st.global.v2.u16 	[%rd65], {%rs5, %rs6};

$L__BB7_15:
	ret;

}
	// .globl	quantize_q8_1_nd2
.visible .entry quantize_q8_1_nd2(
	.param .u64 quantize_q8_1_nd2_param_0,
	.param .u64 quantize_q8_1_nd2_param_1,
	.param .u64 quantize_q8_1_nd2_param_2,
	.param .u64 quantize_q8_1_nd2_param_3,
	.param .u64 quantize_q8_1_nd2_param_4,
	.param .u64 quantize_q8_1_nd2_param_5
)
{
	.reg .pred 	%p<15>;
	.reg .b16 	%rs<3>;
	.reg .f32 	%f<36>;
	.reg .b32 	%r<36>;
	.reg .b64 	%rd<32>;


	ld.param.u64 	%rd5, [quantize_q8_1_nd2_param_0];
	ld.param.u64 	%rd6, [quantize_q8_1_nd2_param_1];
	ld.param.u64 	%rd7, [quantize_q8_1_nd2_param_2];
	ld.param.u64 	%rd8, [quantize_q8_1_nd2_param_3];
	ld.param.u64 	%rd9, [quantize_q8_1_nd2_param_4];
	ld.param.u64 	%rd10, [quantize_q8_1_nd2_param_5];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r2, %ctaid.x;
	mul.wide.u32 	%rd11, %r1, %r2;
	mov.u32 	%r3, %tid.x;
	cvt.u64.u32 	%rd12, %r3;
	add.s64 	%rd1, %rd11, %rd12;
	setp.ge.s64 	%p1, %rd1, %rd10;
	@%p1 bra 	$L__BB8_7;

	mov.u32 	%r4, %ctaid.y;
	cvt.u64.u32 	%rd2, %r4;
	setp.ge.s64 	%p2, %rd1, %rd7;
	mov.f32 	%f35, 0f00000000;
	mov.f32 	%f34, %f35;
	@%p2 bra 	$L__BB8_3;

	mul.lo.s64 	%rd13, %rd2, %rd8;
	mul.lo.s64 	%rd14, %rd1, %rd9;
	add.s64 	%rd15, %rd14, %rd13;
	cvta.to.global.u64 	%rd16, %rd5;
	shl.b64 	%rd17, %rd15, 2;
	add.s64 	%rd18, %rd16, %rd17;
	ld.global.nc.f32 	%f34, [%rd18];

$L__BB8_3:
	mul.lo.s64 	%rd19, %rd2, %rd10;
	add.s64 	%rd20, %rd19, %rd1;
	shr.s64 	%rd21, %rd20, 63;
	shr.u64 	%rd22, %rd21, 59;
	add.s64 	%rd23, %rd20, %rd22;
	shr.s64 	%rd3, %rd23, 5;
	abs.f32 	%f9, %f34;
	mov.b32 	%r5, %f9;
	mov.u32 	%r6, 31;
	mov.u32 	%r7, 16;
	mov.u32 	%r8, -1;
	shfl.sync.bfly.b32 	%r9|%p3, %r5, %r7, %r6, %r8;
	mov.b32 	%f10, %r9;
	max.f32 	%f11, %f9, %f10;
	mov.b32 	%r10, %f11;
	mov.u32 	%r11, 8;
	shfl.sync.bfly.b32 	%r12|%p4, %r10, %r11, %r6, %r8;
	mov.b32 	%f12, %r12;
	max.f32 	%f13, %f11, %f12;
	mov.b32 	%r13, %f13;
	mov.u32 	%r14, 4;
	shfl.sync.bfly.b32 	%r15|%p5, %r13, %r14, %r6, %r8;
	mov.b32 	%f14, %r15;
	max.f32 	%f15, %f13, %f14;
	mov.b32 	%r16, %f15;
	mov.u32 	%r17, 2;
	shfl.sync.bfly.b32 	%r18|%p6, %r16, %r17, %r6, %r8;
	mov.b32 	%f16, %r18;
	max.f32 	%f17, %f15, %f16;
	mov.b32 	%r19, %f17;
	mov.u32 	%r20, 1;
	shfl.sync.bfly.b32 	%r21|%p7, %r19, %r20, %r6, %r8;
	mov.b32 	%f18, %r21;
	max.f32 	%f19, %f17, %f18;
	and.b64  	%rd24, %rd23, -32;
	sub.s64 	%rd4, %rd20, %rd24;
	mov.b32 	%r22, %f34;
	shfl.sync.bfly.b32 	%r23|%p8, %r22, %r7, %r6, %r8;
	mov.b32 	%f20, %r23;
	add.f32 	%f21, %f34, %f20;
	mov.b32 	%r24, %f21;
	shfl.sync.bfly.b32 	%r25|%p9, %r24, %r11, %r6, %r8;
	mov.b32 	%f22, %r25;
	add.f32 	%f23, %f21, %f22;
	mov.b32 	%r26, %f23;
	shfl.sync.bfly.b32 	%r27|%p10, %r26, %r14, %r6, %r8;
	mov.b32 	%f24, %r27;
	add.f32 	%f25, %f23, %f24;
	mov.b32 	%r28, %f25;
	shfl.sync.bfly.b32 	%r29|%p11, %r28, %r17, %r6, %r8;
	mov.b32 	%f26, %r29;
	add.f32 	%f27, %f25, %f26;
	mov.b32 	%r30, %f27;
	shfl.sync.bfly.b32 	%r31|%p12, %r30, %r20, %r6, %r8;
	mov.b32 	%f28, %r31;
	add.f32 	%f3, %f27, %f28;
	div.rn.f32 	%f4, %f19, 0f42FE0000;
	setp.eq.f32 	%p13, %f19, 0f00000000;
	@%p13 bra 	$L__BB8_5;

	div.rn.f32 	%f29, %f34, %f4;
	mov.b32 	%r32, %f29;
	and.b32  	%r33, %r32, -2147483648;
	or.b32  	%r34, %r33, 1056964608;
	mov.b32 	%f30, %r34;
	add.rz.f32 	%f31, %f29, %f30;
	cvt.rzi.f32.f32 	%f35, %f31;

$L__BB8_5:
	cvta.to.global.u64 	%rd25, %rd6;
	cvt.rzi.s32.f32 	%r35, %f35;
	mul.lo.s64 	%rd26, %rd3, 36;
	add.s64 	%rd27, %rd25, %rd26;
	add.s64 	%rd28, %rd27, %rd4;
	st.global.u8 	[%rd28+4], %r35;
	setp.gt.s64 	%p14, %rd4, 0;
	@%p14 bra 	$L__BB8_7;

	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f4;}

	// end inline asm
	st.global.u16 	[%rd27], %rs1;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs2, %f3;}

	// end inline asm
	st.global.u16 	[%rd27+2], %rs2;

$L__BB8_7:
	ret;

}
	// .globl	quantize_q8_1_nd3
.visible .entry quantize_q8_1_nd3(
	.param .u64 quantize_q8_1_nd3_param_0,
	.param .u64 quantize_q8_1_nd3_param_1,
	.param .u64 quantize_q8_1_nd3_param_2,
	.param .u64 quantize_q8_1_nd3_param_3,
	.param .u64 quantize_q8_1_nd3_param_4,
	.param .u64 quantize_q8_1_nd3_param_5,
	.param .u64 quantize_q8_1_nd3_param_6,
	.param .u64 quantize_q8_1_nd3_param_7
)
{
	.reg .pred 	%p<15>;
	.reg .b16 	%rs<3>;
	.reg .f32 	%f<36>;
	.reg .b32 	%r<37>;
	.reg .b64 	%rd<39>;


	ld.param.u64 	%rd6, [quantize_q8_1_nd3_param_0];
	ld.param.u64 	%rd7, [quantize_q8_1_nd3_param_1];
	ld.param.u64 	%rd8, [quantize_q8_1_nd3_param_2];
	ld.param.u64 	%rd9, [quantize_q8_1_nd3_param_3];
	ld.param.u64 	%rd10, [quantize_q8_1_nd3_param_4];
	ld.param.u64 	%rd11, [quantize_q8_1_nd3_param_5];
	ld.param.u64 	%rd12, [quantize_q8_1_nd3_param_6];
	ld.param.u64 	%rd13, [quantize_q8_1_nd3_param_7];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r2, %ctaid.x;
	mul.wide.u32 	%rd14, %r1, %r2;
	mov.u32 	%r3, %tid.x;
	cvt.u64.u32 	%rd15, %r3;
	add.s64 	%rd1, %rd14, %rd15;
	setp.ge.s64 	%p1, %rd1, %rd13;
	@%p1 bra 	$L__BB9_7;

	mov.u32 	%r4, %ctaid.y;
	cvt.u64.u32 	%rd2, %r4;
	mov.u32 	%r5, %ctaid.z;
	cvt.u64.u32 	%rd3, %r5;
	setp.ge.s64 	%p2, %rd1, %rd8;
	mov.f32 	%f35, 0f00000000;
	mov.f32 	%f34, %f35;
	@%p2 bra 	$L__BB9_3;

	mul.lo.s64 	%rd16, %rd3, %rd9;
	mul.lo.s64 	%rd17, %rd2, %rd10;
	add.s64 	%rd18, %rd17, %rd16;
	mul.lo.s64 	%rd19, %rd1, %rd11;
	add.s64 	%rd20, %rd18, %rd19;
	cvta.to.global.u64 	%rd21, %rd6;
	shl.b64 	%rd22, %rd20, 2;
	add.s64 	%rd23, %rd21, %rd22;
	ld.global.nc.f32 	%f34, [%rd23];

$L__BB9_3:
	mul.lo.s64 	%rd24, %rd3, %rd12;
	add.s64 	%rd25, %rd24, %rd2;
	mul.lo.s64 	%rd26, %rd25, %rd13;
	add.s64 	%rd27, %rd26, %rd1;
	shr.s64 	%rd28, %rd27, 63;
	shr.u64 	%rd29, %rd28, 59;
	add.s64 	%rd30, %rd27, %rd29;
	shr.s64 	%rd4, %rd30, 5;
	abs.f32 	%f9, %f34;
	mov.b32 	%r6, %f9;
	mov.u32 	%r7, 31;
	mov.u32 	%r8, 16;
	mov.u32 	%r9, -1;
	shfl.sync.bfly.b32 	%r10|%p3, %r6, %r8, %r7, %r9;
	mov.b32 	%f10, %r10;
	max.f32 	%f11, %f9, %f10;
	mov.b32 	%r11, %f11;
	mov.u32 	%r12, 8;
	shfl.sync.bfly.b32 	%r13|%p4, %r11, %r12, %r7, %r9;
	mov.b32 	%f12, %r13;
	max.f32 	%f13, %f11, %f12;
	mov.b32 	%r14, %f13;
	mov.u32 	%r15, 4;
	shfl.sync.bfly.b32 	%r16|%p5, %r14, %r15, %r7, %r9;
	mov.b32 	%f14, %r16;
	max.f32 	%f15, %f13, %f14;
	mov.b32 	%r17, %f15;
	mov.u32 	%r18, 2;
	shfl.sync.bfly.b32 	%r19|%p6, %r17, %r18, %r7, %r9;
	mov.b32 	%f16, %r19;
	max.f32 	%f17, %f15, %f16;
	mov.b32 	%r20, %f17;
	mov.u32 	%r21, 1;
	shfl.sync.bfly.b32 	%r22|%p7, %r20, %r21, %r7, %r9;
	mov.b32 	%f18, %r22;
	max.f32 	%f19, %f17, %f18;
	and.b64  	%rd31, %rd30, -32;
	sub.s64 	%rd5, %rd27, %rd31;
	mov.b32 	%r23, %f34;
	shfl.sync.bfly.b32 	%r24|%p8, %r23, %r8, %r7, %r9;
	mov.b32 	%f20, %r24;
	add.f32 	%f21, %f34, %f20;
	mov.b32 	%r25, %f21;
	shfl.sync.bfly.b32 	%r26|%p9, %r25, %r12, %r7, %r9;
	mov.b32 	%f22, %r26;
	add.f32 	%f23, %f21, %f22;
	mov.b32 	%r27, %f23;
	shfl.sync.bfly.b32 	%r28|%p10, %r27, %r15, %r7, %r9;
	mov.b32 	%f24, %r28;
	add.f32 	%f25, %f23, %f24;
	mov.b32 	%r29, %f25;
	shfl.sync.bfly.b32 	%r30|%p11, %r29, %r18, %r7, %r9;
	mov.b32 	%f26, %r30;
	add.f32 	%f27, %f25, %f26;
	mov.b32 	%r31, %f27;
	shfl.sync.bfly.b32 	%r32|%p12, %r31, %r21, %r7, %r9;
	mov.b32 	%f28, %r32;
	add.f32 	%f3, %f27, %f28;
	div.rn.f32 	%f4, %f19, 0f42FE0000;
	setp.eq.f32 	%p13, %f19, 0f00000000;
	@%p13 bra 	$L__BB9_5;

	div.rn.f32 	%f29, %f34, %f4;
	mov.b32 	%r33, %f29;
	and.b32  	%r34, %r33, -2147483648;
	or.b32  	%r35, %r34, 1056964608;
	mov.b32 	%f30, %r35;
	add.rz.f32 	%f31, %f29, %f30;
	cvt.rzi.f32.f32 	%f35, %f31;

$L__BB9_5:
	cvta.to.global.u64 	%rd32, %rd7;
	cvt.rzi.s32.f32 	%r36, %f35;
	mul.lo.s64 	%rd33, %rd4, 36;
	add.s64 	%rd34, %rd32, %rd33;
	add.s64 	%rd35, %rd34, %rd5;
	st.global.u8 	[%rd35+4], %r36;
	setp.gt.s64 	%p14, %rd5, 0;
	@%p14 bra 	$L__BB9_7;

	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f4;}

	// end inline asm
	st.global.u16 	[%rd34], %rs1;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs2, %f3;}

	// end inline asm
	st.global.u16 	[%rd34+2], %rs2;

$L__BB9_7:
	ret;

}
	// .globl	quantize_q8_1_nd4
.visible .entry quantize_q8_1_nd4(
	.param .u64 quantize_q8_1_nd4_param_0,
	.param .u64 quantize_q8_1_nd4_param_1,
	.param .u64 quantize_q8_1_nd4_param_2,
	.param .u64 quantize_q8_1_nd4_param_3,
	.param .u64 quantize_q8_1_nd4_param_4,
	.param .u64 quantize_q8_1_nd4_param_5,
	.param .u64 quantize_q8_1_nd4_param_6,
	.param .u64 quantize_q8_1_nd4_param_7,
	.param .u64 quantize_q8_1_nd4_param_8,
	.param .u64 quantize_q8_1_nd4_param_9
)
{
	.reg .pred 	%p<16>;
	.reg .b16 	%rs<3>;
	.reg .f32 	%f<36>;
	.reg .b32 	%r<43>;
	.reg .b64 	%rd<56>;


	ld.param.u64 	%rd11, [quantize_q8_1_nd4_param_0];
	ld.param.u64 	%rd12, [quantize_q8_1_nd4_param_1];
	ld.param.u64 	%rd13, [quantize_q8_1_nd4_param_2];
	ld.param.u64 	%rd14, [quantize_q8_1_nd4_param_3];
	ld.param.u64 	%rd15, [quantize_q8_1_nd4_param_4];
	ld.param.u64 	%rd16, [quantize_q8_1_nd4_param_5];
	ld.param.u64 	%rd17, [quantize_q8_1_nd4_param_6];
	ld.param.u64 	%rd18, [quantize_q8_1_nd4_param_7];
	ld.param.u64 	%rd19, [quantize_q8_1_nd4_param_8];
	ld.param.u64 	%rd20, [quantize_q8_1_nd4_param_9];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r2, %ctaid.x;
	mul.wide.u32 	%rd21, %r1, %r2;
	mov.u32 	%r3, %tid.x;
	cvt.u64.u32 	%rd22, %r3;
	add.s64 	%rd1, %rd21, %rd22;
	setp.ge.s64 	%p1, %rd1, %rd20;
	@%p1 bra 	$L__BB10_10;

	mov.u32 	%r4, %ctaid.z;
	cvt.u64.u32 	%rd2, %r4;
	and.b64  	%rd23, %rd18, -4294967296;
	setp.eq.s64 	%p2, %rd23, 0;
	@%p2 bra 	$L__BB10_3;

	div.s64 	%rd54, %rd2, %rd18;
	mul.lo.s64 	%rd24, %rd54, %rd18;
	sub.s64 	%rd55, %rd2, %rd24;
	bra.uni 	$L__BB10_4;

$L__BB10_3:
	cvt.u32.u64 	%r5, %rd18;
	cvt.u32.u64 	%r6, %rd2;
	div.u32 	%r7, %r6, %r5;
	mul.lo.s32 	%r8, %r7, %r5;
	sub.s32 	%r9, %r6, %r8;
	cvt.u64.u32 	%rd54, %r7;
	cvt.u64.u32 	%rd55, %r9;

$L__BB10_4:
	setp.ge.s64 	%p3, %rd1, %rd13;
	mov.f32 	%f35, 0f00000000;
	mov.f32 	%f34, %f35;
	@%p3 bra 	$L__BB10_6;

	mul.lo.s64 	%rd25, %rd54, %rd14;
	mov.u32 	%r10, %ctaid.y;
	cvt.u64.u32 	%rd26, %r10;
	mul.lo.s64 	%rd27, %rd26, %rd16;
	mul.lo.s64 	%rd28, %rd1, %rd17;
	add.s64 	%rd29, %rd28, %rd27;
	mul.lo.s64 	%rd30, %rd55, %rd15;
	add.s64 	%rd31, %rd29, %rd30;
	add.s64 	%rd32, %rd31, %rd25;
	cvta.to.global.u64 	%rd33, %rd11;
	shl.b64 	%rd34, %rd32, 2;
	add.s64 	%rd35, %rd33, %rd34;
	ld.global.nc.f32 	%f34, [%rd35];

$L__BB10_6:
	mov.u32 	%r11, %ctaid.y;
	cvt.u64.u32 	%rd36, %r11;
	mul.lo.s64 	%rd37, %rd54, %rd18;
	add.s64 	%rd38, %rd37, %rd55;
	mul.lo.s64 	%rd39, %rd38, %rd19;
	add.s64 	%rd40, %rd39, %rd36;
	mul.lo.s64 	%rd41, %rd40, %rd20;
	add.s64 	%rd42, %rd41, %rd1;
	shr.s64 	%rd43, %rd42, 63;
	shr.u64 	%rd44, %rd43, 59;
	add.s64 	%rd45, %rd42, %rd44;
	shr.s64 	%rd9, %rd45, 5;
	abs.f32 	%f9, %f34;
	mov.b32 	%r12, %f9;
	mov.u32 	%r13, 31;
	mov.u32 	%r14, 16;
	mov.u32 	%r15, -1;
	shfl.sync.bfly.b32 	%r16|%p4, %r12, %r14, %r13, %r15;
	mov.b32 	%f10, %r16;
	max.f32 	%f11, %f9, %f10;
	mov.b32 	%r17, %f11;
	mov.u32 	%r18, 8;
	shfl.sync.bfly.b32 	%r19|%p5, %r17, %r18, %r13, %r15;
	mov.b32 	%f12, %r19;
	max.f32 	%f13, %f11, %f12;
	mov.b32 	%r20, %f13;
	mov.u32 	%r21, 4;
	shfl.sync.bfly.b32 	%r22|%p6, %r20, %r21, %r13, %r15;
	mov.b32 	%f14, %r22;
	max.f32 	%f15, %f13, %f14;
	mov.b32 	%r23, %f15;
	mov.u32 	%r24, 2;
	shfl.sync.bfly.b32 	%r25|%p7, %r23, %r24, %r13, %r15;
	mov.b32 	%f16, %r25;
	max.f32 	%f17, %f15, %f16;
	mov.b32 	%r26, %f17;
	mov.u32 	%r27, 1;
	shfl.sync.bfly.b32 	%r28|%p8, %r26, %r27, %r13, %r15;
	mov.b32 	%f18, %r28;
	max.f32 	%f19, %f17, %f18;
	and.b64  	%rd46, %rd45, -32;
	sub.s64 	%rd10, %rd42, %rd46;
	mov.b32 	%r29, %f34;
	shfl.sync.bfly.b32 	%r30|%p9, %r29, %r14, %r13, %r15;
	mov.b32 	%f20, %r30;
	add.f32 	%f21, %f34, %f20;
	mov.b32 	%r31, %f21;
	shfl.sync.bfly.b32 	%r32|%p10, %r31, %r18, %r13, %r15;
	mov.b32 	%f22, %r32;
	add.f32 	%f23, %f21, %f22;
	mov.b32 	%r33, %f23;
	shfl.sync.bfly.b32 	%r34|%p11, %r33, %r21, %r13, %r15;
	mov.b32 	%f24, %r34;
	add.f32 	%f25, %f23, %f24;
	mov.b32 	%r35, %f25;
	shfl.sync.bfly.b32 	%r36|%p12, %r35, %r24, %r13, %r15;
	mov.b32 	%f26, %r36;
	add.f32 	%f27, %f25, %f26;
	mov.b32 	%r37, %f27;
	shfl.sync.bfly.b32 	%r38|%p13, %r37, %r27, %r13, %r15;
	mov.b32 	%f28, %r38;
	add.f32 	%f3, %f27, %f28;
	div.rn.f32 	%f4, %f19, 0f42FE0000;
	setp.eq.f32 	%p14, %f19, 0f00000000;
	@%p14 bra 	$L__BB10_8;

	div.rn.f32 	%f29, %f34, %f4;
	mov.b32 	%r39, %f29;
	and.b32  	%r40, %r39, -2147483648;
	or.b32  	%r41, %r40, 1056964608;
	mov.b32 	%f30, %r41;
	add.rz.f32 	%f31, %f29, %f30;
	cvt.rzi.f32.f32 	%f35, %f31;

$L__BB10_8:
	cvta.to.global.u64 	%rd47, %rd12;
	cvt.rzi.s32.f32 	%r42, %f35;
	mul.lo.s64 	%rd48, %rd9, 36;
	add.s64 	%rd49, %rd47, %rd48;
	add.s64 	%rd50, %rd49, %rd10;
	st.global.u8 	[%rd50+4], %r42;
	setp.gt.s64 	%p15, %rd10, 0;
	@%p15 bra 	$L__BB10_10;

	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f4;}

	// end inline asm
	st.global.u16 	[%rd49], %rs1;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs2, %f3;}

	// end inline asm
	st.global.u16 	[%rd49+2], %rs2;

$L__BB10_10:
	ret;

}
	// .globl	quantize_q8_1_nd5
.visible .entry quantize_q8_1_nd5(
	.param .u64 quantize_q8_1_nd5_param_0,
	.param .u64 quantize_q8_1_nd5_param_1,
	.param .u64 quantize_q8_1_nd5_param_2,
	.param .u64 quantize_q8_1_nd5_param_3,
	.param .u64 quantize_q8_1_nd5_param_4,
	.param .u64 quantize_q8_1_nd5_param_5,
	.param .u64 quantize_q8_1_nd5_param_6,
	.param .u64 quantize_q8_1_nd5_param_7,
	.param .u64 quantize_q8_1_nd5_param_8,
	.param .u64 quantize_q8_1_nd5_param_9,
	.param .u64 quantize_q8_1_nd5_param_10,
	.param .u64 quantize_q8_1_nd5_param_11
)
{
	.reg .pred 	%p<18>;
	.reg .b16 	%rs<3>;
	.reg .f32 	%f<36>;
	.reg .b32 	%r<56>;
	.reg .b64 	%rd<81>;


	ld.param.u64 	%rd18, [quantize_q8_1_nd5_param_0];
	ld.param.u64 	%rd19, [quantize_q8_1_nd5_param_1];
	ld.param.u64 	%rd20, [quantize_q8_1_nd5_param_2];
	ld.param.u64 	%rd21, [quantize_q8_1_nd5_param_3];
	ld.param.u64 	%rd22, [quantize_q8_1_nd5_param_4];
	ld.param.u64 	%rd23, [quantize_q8_1_nd5_param_5];
	ld.param.u64 	%rd24, [quantize_q8_1_nd5_param_6];
	ld.param.u64 	%rd25, [quantize_q8_1_nd5_param_7];
	ld.param.u64 	%rd26, [quantize_q8_1_nd5_param_8];
	ld.param.u64 	%rd27, [quantize_q8_1_nd5_param_9];
	ld.param.u64 	%rd28, [quantize_q8_1_nd5_param_10];
	ld.param.u64 	%rd29, [quantize_q8_1_nd5_param_11];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r2, %ctaid.x;
	mul.wide.u32 	%rd30, %r1, %r2;
	mov.u32 	%r3, %tid.x;
	cvt.u64.u32 	%rd31, %r3;
	add.s64 	%rd32, %rd30, %rd31;
	setp.ge.s64 	%p1, %rd32, %rd29;
	@%p1 bra 	$L__BB11_16;

	mov.u32 	%r4, %ctaid.z;
	cvt.u64.u32 	%rd1, %r4;
	and.b64  	%rd33, %rd27, -4294967296;
	setp.eq.s64 	%p2, %rd33, 0;
	@%p2 bra 	$L__BB11_3;

	div.s64 	%rd77, %rd1, %rd27;
	mul.lo.s64 	%rd34, %rd77, %rd27;
	sub.s64 	%rd78, %rd1, %rd34;
	bra.uni 	$L__BB11_4;

$L__BB11_3:
	cvt.u32.u64 	%r5, %rd27;
	cvt.u32.u64 	%r6, %rd1;
	div.u32 	%r7, %r6, %r5;
	mul.lo.s32 	%r8, %r7, %r5;
	sub.s32 	%r9, %r6, %r8;
	cvt.u64.u32 	%rd77, %r7;
	cvt.u64.u32 	%rd78, %r9;

$L__BB11_4:
	or.b64  	%rd35, %rd77, %rd26;
	and.b64  	%rd36, %rd35, -4294967296;
	setp.eq.s64 	%p3, %rd36, 0;
	@%p3 bra 	$L__BB11_6;

	rem.s64 	%rd79, %rd77, %rd26;
	bra.uni 	$L__BB11_7;

$L__BB11_6:
	cvt.u32.u64 	%r10, %rd26;
	cvt.u32.u64 	%r11, %rd77;
	rem.u32 	%r12, %r11, %r10;
	cvt.u64.u32 	%rd79, %r12;

$L__BB11_7:
	mul.lo.s64 	%rd11, %rd27, %rd26;
	and.b64  	%rd37, %rd11, -4294967296;
	setp.eq.s64 	%p4, %rd37, 0;
	@%p4 bra 	$L__BB11_9;

	div.s64 	%rd80, %rd1, %rd11;
	bra.uni 	$L__BB11_10;

$L__BB11_9:
	cvt.u32.u64 	%r14, %rd11;
	div.u32 	%r16, %r4, %r14;
	cvt.u64.u32 	%rd80, %r16;

$L__BB11_10:
	setp.ge.s64 	%p5, %rd32, %rd20;
	mov.f32 	%f35, 0f00000000;
	mov.f32 	%f34, %f35;
	@%p5 bra 	$L__BB11_12;

	mul.lo.s64 	%rd41, %rd80, %rd25;
	mov.u32 	%r20, %ctaid.y;
	cvt.u64.u32 	%rd42, %r20;
	mul.lo.s64 	%rd43, %rd42, %rd22;
	mul.lo.s64 	%rd47, %rd32, %rd21;
	add.s64 	%rd48, %rd43, %rd47;
	mul.lo.s64 	%rd49, %rd78, %rd23;
	add.s64 	%rd50, %rd48, %rd49;
	mul.lo.s64 	%rd51, %rd79, %rd24;
	add.s64 	%rd52, %rd50, %rd51;
	add.s64 	%rd53, %rd52, %rd41;
	cvta.to.global.u64 	%rd54, %rd18;
	shl.b64 	%rd55, %rd53, 2;
	add.s64 	%rd56, %rd54, %rd55;
	ld.global.nc.f32 	%f34, [%rd56];

$L__BB11_12:
	mov.u32 	%r24, %ctaid.y;
	cvt.u64.u32 	%rd57, %r24;
	mul.lo.s64 	%rd58, %rd80, %rd26;
	add.s64 	%rd59, %rd58, %rd79;
	mul.lo.s64 	%rd60, %rd59, %rd27;
	add.s64 	%rd61, %rd60, %rd78;
	mul.lo.s64 	%rd62, %rd61, %rd28;
	add.s64 	%rd63, %rd62, %rd57;
	mul.lo.s64 	%rd64, %rd63, %rd29;
	add.s64 	%rd65, %rd64, %rd32;
	shr.s64 	%rd66, %rd65, 63;
	shr.u64 	%rd67, %rd66, 59;
	add.s64 	%rd68, %rd65, %rd67;
	shr.s64 	%rd16, %rd68, 5;
	abs.f32 	%f9, %f34;
	mov.b32 	%r25, %f9;
	mov.u32 	%r26, 31;
	mov.u32 	%r27, 16;
	mov.u32 	%r28, -1;
	shfl.sync.bfly.b32 	%r29|%p6, %r25, %r27, %r26, %r28;
	mov.b32 	%f10, %r29;
	max.f32 	%f11, %f9, %f10;
	mov.b32 	%r30, %f11;
	mov.u32 	%r31, 8;
	shfl.sync.bfly.b32 	%r32|%p7, %r30, %r31, %r26, %r28;
	mov.b32 	%f12, %r32;
	max.f32 	%f13, %f11, %f12;
	mov.b32 	%r33, %f13;
	mov.u32 	%r34, 4;
	shfl.sync.bfly.b32 	%r35|%p8, %r33, %r34, %r26, %r28;
	mov.b32 	%f14, %r35;
	max.f32 	%f15, %f13, %f14;
	mov.b32 	%r36, %f15;
	mov.u32 	%r37, 2;
	shfl.sync.bfly.b32 	%r38|%p9, %r36, %r37, %r26, %r28;
	mov.b32 	%f16, %r38;
	max.f32 	%f17, %f15, %f16;
	mov.b32 	%r39, %f17;
	mov.u32 	%r40, 1;
	shfl.sync.bfly.b32 	%r41|%p10, %r39, %r40, %r26, %r28;
	mov.b32 	%f18, %r41;
	max.f32 	%f19, %f17, %f18;
	and.b64  	%rd69, %rd68, -32;
	sub.s64 	%rd17, %rd65, %rd69;
	mov.b32 	%r42, %f34;
	shfl.sync.bfly.b32 	%r43|%p11, %r42, %r27, %r26, %r28;
	mov.b32 	%f20, %r43;
	add.f32 	%f21, %f34, %f20;
	mov.b32 	%r44, %f21;
	shfl.sync.bfly.b32 	%r45|%p12, %r44, %r31, %r26, %r28;
	mov.b32 	%f22, %r45;
	add.f32 	%f23, %f21, %f22;
	mov.b32 	%r46, %f23;
	shfl.sync.bfly.b32 	%r47|%p13, %r46, %r34, %r26, %r28;
	mov.b32 	%f24, %r47;
	add.f32 	%f25, %f23, %f24;
	mov.b32 	%r48, %f25;
	shfl.sync.bfly.b32 	%r49|%p14, %r48, %r37, %r26, %r28;
	mov.b32 	%f26, %r49;
	add.f32 	%f27, %f25, %f26;
	mov.b32 	%r50, %f27;
	shfl.sync.bfly.b32 	%r51|%p15, %r50, %r40, %r26, %r28;
	mov.b32 	%f28, %r51;
	add.f32 	%f3, %f27, %f28;
	div.rn.f32 	%f4, %f19, 0f42FE0000;
	setp.eq.f32 	%p16, %f19, 0f00000000;
	@%p16 bra 	$L__BB11_14;

	div.rn.f32 	%f29, %f34, %f4;
	mov.b32 	%r52, %f29;
	and.b32  	%r53, %r52, -2147483648;
	or.b32  	%r54, %r53, 1056964608;
	mov.b32 	%f30, %r54;
	add.rz.f32 	%f31, %f29, %f30;
	cvt.rzi.f32.f32 	%f35, %f31;

$L__BB11_14:
	cvta.to.global.u64 	%rd70, %rd19;
	cvt.rzi.s32.f32 	%r55, %f35;
	mul.lo.s64 	%rd71, %rd16, 36;
	add.s64 	%rd72, %rd70, %rd71;
	add.s64 	%rd73, %rd72, %rd17;
	st.global.u8 	[%rd73+4], %r55;
	setp.gt.s64 	%p17, %rd17, 0;
	@%p17 bra 	$L__BB11_16;

	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f4;}

	// end inline asm
	st.global.u16 	[%rd72], %rs1;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs2, %f3;}

	// end inline asm
	st.global.u16 	[%rd72+2], %rs2;

$L__BB11_16:
	ret;

}

