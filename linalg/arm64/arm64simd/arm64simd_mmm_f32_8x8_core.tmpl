// vim: ft=arm

// x20..x27 are used, callee-preserved

// C tile regs: v16 to v31, (scratch)
// 
//      v16[0] v18[0] v20[0] v22[0] v24[0] v26[0] v28[0] v30[0]
//      v16[1] v18[1] 
//      v16[2] v18[2] 
//      v16[3] v18[3]
//                     
//      v17[0] v19[0] v21[0] v23[0] v25[0] v27[0] v29[0] v31[0]
//      v17[1] v19[1] 
//      v17[2] v19[2] 
//      v17[3] v19[3] 

// v8 is used, d8 (lower half) must preserved
// v0-v7 (scratch registers)
//  packed A buffering (2x8 values): alternating v0, v1 with v2, v3
//  packed B buffering (2x8 values): alternating v4, v5 with v6, v7

.text
.align 4

.cpu generic+fp+simd
.global {{G}}arm64simd_mmm_f32_8x8_{{core}}_{{suffix}}
{{G}}arm64simd_mmm_f32_8x8_{{core}}_{{suffix}}:

    stp         x20, x21, [sp, #-16]!
    stp         x22, x23, [sp, #-16]!
    stp         x24, x25, [sp, #-16]!
    stp         x26, x27, [sp, #-16]!

    str         q8, [sp, #-16]!

{% for r in (16..31) %}
    eor         v{{r}}.8b, v{{r}}.8b, v{{r}}.8b
{% endfor %}

{% include "dispatcher.tmpliq" %}

.add_mat_mul:
    ldr         x8, [x0, #24]       // b
    ldp         x3, x1, [x0, #8]    // k, a
    ldp         x4, x2, [x8]        // b disc, first arg

    cmp         x3, #0
    beq         .non_linear_loop

    cmp         x4, #0
    beq         .packed_packed

.packed_tops_and_offsets:
    // x2 is row_byte_offsets
    ldr         x8, [ x8, #16]          // cols ptr ptr
    ldr         x4, [ x2 ], #8          // fist row offset

    ldp         x20, x21, [x8], #16     // heads of cols ptrs
    ldp         x22, x23, [x8], #16
    ldp         x24, x25, [x8], #16
    ldp         x26, x27, [x8], #16

    cmp         x3, #4
    blt         .packed_tops_and_offsets_loop_1


{% capture packed_tops_and_offset_loop %}
    ld1         { v0.4s, v1.4s }, [ x1 ], #32

    add         x8, x4, x20
    ld1         {v4.s}[0], [ x8 ]
    add         x9, x4, x21
    ld1         {v4.s}[1], [ x9 ]
    add         x10, x4, x22
    ld1         {v4.s}[2], [ x10 ]
    add         x11, x4, x23
    ld1         {v4.s}[3], [ x11 ]
    add         x12, x4, x24
    ld1         {v5.s}[0], [ x12 ]
    add         x13, x4, x25
    ld1         {v5.s}[1], [ x13 ]
    add         x14, x4, x26
    ld1         {v5.s}[2], [ x14 ]
    add         x15, x4, x27
    ld1         {v5.s}[3], [ x15 ]

    ldr         x4, [ x2 ], #8

    fmla        v16.4s, v0.4s, v4.s[0]
    fmla        v17.4s, v1.4s, v4.s[0]
    fmla        v18.4s, v0.4s, v4.s[1]
    fmla        v19.4s, v1.4s, v4.s[1]
    fmla        v20.4s, v0.4s, v4.s[2]
    fmla        v21.4s, v1.4s, v4.s[2]
    fmla        v22.4s, v0.4s, v4.s[3]
    fmla        v23.4s, v1.4s, v4.s[3]

    fmla        v24.4s, v0.4s, v5.s[0]
    fmla        v25.4s, v1.4s, v5.s[0]
    fmla        v26.4s, v0.4s, v5.s[1]
    fmla        v27.4s, v1.4s, v5.s[1]
    fmla        v28.4s, v0.4s, v5.s[2]
    fmla        v29.4s, v1.4s, v5.s[2]
    fmla        v30.4s, v0.4s, v5.s[3]
    fmla        v31.4s, v1.4s, v5.s[3]
{% endcapture %}

.p2align 4
.packed_tops_and_offsets_loop_4:
    {{ packed_tops_and_offset_loop }}
    {{ packed_tops_and_offset_loop }}
    {{ packed_tops_and_offset_loop }}
    {{ packed_tops_and_offset_loop }}

    sub x3, x3, #4
    cmp x3, #4
    bge .packed_tops_and_offsets_loop_4

    cmp x3, #0
    beq .non_linear_loop

.p2align 4
.packed_tops_and_offsets_loop_1:
    {{ packed_tops_and_offset_loop }}

    subs        x3, x3, #1
    bne         .packed_tops_and_offsets_loop_1

    b           .non_linear_loop

.packed_packed:
    ld1         { v0.4s, v1.4s }, [ x1 ], #32
    ld1         { v4.4s, v5.4s }, [ x2 ], #32

{% capture packed_packed_loop1 %}
{% if core == "a53" %}
        {% include "arm64simd_mmm_f32_8x8/packed_packed_loop1/ldr_w_preload.tmpli" %}
    {% else %}
        {% include "arm64simd_mmm_f32_8x8/packed_packed_loop1/naive.tmpli" %}
    {% endif %}
{% endcapture %}

    cmp         x3, #4
    blt         .packed_packed_loop_1

.p2align 4
.packed_packed_loop_4:
    {{ packed_packed_loop1 }}
    {{ packed_packed_loop1 }}
    {{ packed_packed_loop1 }}
    {{ packed_packed_loop1 }}

    sub x3, x3, #4
    cmp x3, #4
    bge .packed_packed_loop_4

    cmp x3, #0
    beq .non_linear_loop

.p2align 4
.packed_packed_loop_1:
    {{ packed_packed_loop1 }}
    subs        x3, x3, #1
    bne .packed_packed_loop_1

    b .non_linear_loop

{% include "arm64simd_mmm_f32_scalars.tmpliq" from:16, to:31%}
{% include "arm64simd_mmm_f32_per_rows.tmpliq" mr:8, from:16, to:31 %}
{% include "arm64simd_mmm_f32_per_cols.tmpliq" mr:8, from:16, to:31 %}

.add_unicast:
    ldp         x5, x6, [x0, #8]
    ldp         x7, x8, [x0, #24]

    {% for col in (8..15) %}
        mov x4, x5
        {% for reg in (0..1) %}
            {% for lane in (0..3) %}
                ld1 {v0.s}[{{lane}}], [ x4 ], x6
            {% endfor %}
            fadd v{{col | times:2 | plus: reg}}.4s, v{{col | times:2 | plus: reg}}.4s, v0.4s
        {% endfor %}
        add x5, x5, x7
    {% endfor %}

    b           .non_linear_loop

.add_row_col_products:
    ldr     x2, [x0, #8]
    ldr     x3, [x0, #16]

    ld1         { v0.4s, v1.4s }, [ x2 ], #32
    ld1         { v4.4s, v5.4s }, [ x3 ], #32

    fmla        v16.4s, v0.4s, v4.s[0]
    fmla        v17.4s, v1.4s, v4.s[0]
    fmla        v18.4s, v0.4s, v4.s[1]
    fmla        v19.4s, v1.4s, v4.s[1]
    fmla        v20.4s, v0.4s, v4.s[2]
    fmla        v21.4s, v1.4s, v4.s[2]
    fmla        v22.4s, v0.4s, v4.s[3]
    fmla        v23.4s, v1.4s, v4.s[3]

    fmla        v24.4s, v0.4s, v5.s[0]
    fmla        v25.4s, v1.4s, v5.s[0]
    fmla        v26.4s, v0.4s, v5.s[1]
    fmla        v27.4s, v1.4s, v5.s[1]
    fmla        v28.4s, v0.4s, v5.s[2]
    fmla        v29.4s, v1.4s, v5.s[2]
    fmla        v30.4s, v0.4s, v5.s[3]
    fmla        v31.4s, v1.4s, v5.s[3]

    b           .non_linear_loop

.store:
    ldp         x5, x6, [x0, #8]            // c base ptr, rsc
    ldp         x7, x8, [x0, #24]           // csc, item_size

    cmp         x6, #4
    bne         .store_strides_generic

    {% for col in (8..15) %}
        str q{{col | times:2 }}, [ x5 ]
        str q{{col | times:2 | plus: 1}}, [ x5, #16 ]
        add x5, x5, x7
    {% endfor %}

    b           .non_linear_loop

.store_strides_generic:

    {% for col in (8..15) %}
        mov x4, x5
        {% for reg in (0..1) %}
            {% for lane in (0..3) %}
                st1 { v{{col | times:2 | plus: reg}}.s }[{{lane}}], [ x4 ], x6
            {% endfor %}
        {% endfor %}
        add x5, x5, x7
    {% endfor %}

    b           .non_linear_loop

.q_scale:
    b .unsupported

.return:
    ldr         q8, [sp], #16

    ldp         x26, x27, [sp], #16
    ldp         x24, x25, [sp], #16
    ldp         x22, x23, [sp], #16
    ldp         x20, x21, [sp], #16

    ret
