// vim: ft=arm

// C tile regs:
// - x19-x29 to preserve (but x19, x28, x29 not used) 
// - d8..d15 to preserve
// - v16 to v31, no need to preserve
// 
//      v16[0] v18[0] v20[0] v22[0] v24[0] v26[0] v28[0] v30[0]
//      v16[1] v18[1] 
//      v16[2] v18[2] 
//      v16[3] v18[3]
//                     
//      v17[0] v19[0] v21[0] v23[0] v25[0] v27[0] v29[0] v31[0]
//      v17[1] v19[1] 
//      v17[2] v19[2] 
//      v17[3] v19[3] 

// packed A buffering (2x8 values): alternating v0, v1 with v2, v3
// packed B buffering (2x8 values): alternating v4, v5 with v6, v7

.text
.align 4

.cpu generic+fp+simd
.global {{G}}arm64simd_mmm_f32_32x1_{{core}}_{{suffix}}
{{G}}arm64simd_mmm_f32_32x1_{{core}}_{{suffix}}:

    stp         x20, x21, [sp, #-16]!
    stp         x22, x23, [sp, #-16]!
    stp         x24, x25, [sp, #-16]!
    stp         x26, x27, [sp, #-16]!

    stp         d8, d9, [sp, #-16]!
    stp         d10, d11, [sp, #-16]!
    stp         d12, d13, [sp, #-16]!
    stp         d14, d15, [sp, #-16]!

{% include "dispatcher.tmpliq" %}

.add_mat_mul:
    ldp         x2, x4, [x0, #24]   // b, packing
    ldp         x3, x1, [x0, #8]    // k, a

    cmp         x3, #0
    beq         .non_linear_loop

    cmp         x4, #1
    beq         .q4f16se
    cmp         x4, #2
    beq         .q4f32se

    sub         x3, x3, #1

.p2align 4
.packed_packed_loop_1:
    ld1         { v8.s }[0], [ x2 ], #4
    ld1         { v0.4s, v1.4s, v2.4s, v3.4s }, [ x1 ], #64
    ld1         { v4.4s, v5.4s, v6.4s, v7.4s }, [ x1 ], #64

    fmla        v24.4s, v0.4s, v8.s[0]
    fmla        v25.4s, v1.4s, v8.s[0]
    fmla        v26.4s, v2.4s, v8.s[0]
    fmla        v27.4s, v3.4s, v8.s[0]
    fmla        v28.4s, v4.4s, v8.s[0]
    fmla        v29.4s, v5.4s, v8.s[0]
    fmla        v30.4s, v6.4s, v8.s[0]
    fmla        v31.4s, v7.4s, v8.s[0]

    subs        x3, x3, #1
    bge         .packed_packed_loop_1

    b           .non_linear_loop

.p2align 8
.q40f16_const:
    .byte 0xc8, 0xc7, 0xc6, 0xc5, 0xc4, 0xc2, 0xc0, 0xbc
    .byte 0x00, 0x3c, 0x40, 0x42, 0x44, 0x45, 0x46, 0x47

.q4f16se:
    adr      x4, .q40f16_const
    movi     v15.16b, 15
    ld1      {v13.16b}, [ x4 ]
    eor      v12.16b, v12.16b, v12.16b

.q4f16se_outerloop:
{% for i in (0..7) %}
    eor      v{{i|plus:16}}.16b, v{{i|plus:16}}.16b, v{{i|plus:16}}.16b
{% endfor %}
    mov         x4, #32

.p2align 4
.q4f16se_innerloop:
        ld1      { v10.16b }, [ x1 ], #16
        ld1      { v11.h }[0], [ x2 ], #2

        and      v9.16b, v10.16b, v15.16b
        ushr     v10.16b, v10.16b, 4

        tbl      v9.16b, { v13.16b }, v9.16b
        tbl      v10.16b, { v13.16b }, v10.16b

        zip1     v0.16b, v12.16b, v9.16b
        zip2     v2.16b, v12.16b, v9.16b
        zip1     v4.16b, v12.16b, v10.16b
        zip2     v6.16b, v12.16b, v10.16b

        fcvtl    v11.4s, v11.4h

        fcvtl2   v1.4s, v0.8h
        fcvtl2   v3.4s, v2.8h
        fcvtl2   v5.4s, v4.8h
        fcvtl2   v7.4s, v6.8h
        fcvtl    v0.4s, v0.4h
        fcvtl    v2.4s, v2.4h
        fcvtl    v4.4s, v4.4h
        fcvtl    v6.4s, v6.4h

{% for i in (0..7) %}
        fmla        v{{ i|plus: 16 }}.4s, v{{i}}.4s, v11.s[0]
{% endfor %}

    subs        x4, x4, #1
    bne         .q4f16se_innerloop

    // scales
    ld1         { v0.8h-v3.8h }, [ x1 ], #64

    fcvtl       v4.4s, v0.4h
    fcvtl2      v5.4s, v0.8h
    fcvtl       v6.4s, v1.4h
    fcvtl2      v7.4s, v1.8h
    fcvtl       v8.4s, v2.4h
    fcvtl2      v9.4s, v2.8h
    fcvtl       v10.4s, v3.4h
    fcvtl2      v11.4s, v3.8h

{% for i in (0..7) %}
       fmla     v{{i|plus:24}}.4s, v{{i|plus:4}}.4s, v{{i|plus:16}}.4s
{% endfor %}

    subs        x3, x3, #32
    bne         .q4f16se_outerloop

    b           .non_linear_loop

.q4f32se:
    adr      x4, .q40f16_const
    movi     v15.16b, 15
    ld1      {v13.16b}, [ x4 ]
    eor      v12.16b, v12.16b, v12.16b

.q4f32se_outerloop:
{% for i in (0..7) %}
    eor      v{{i|plus:16}}.16b, v{{i|plus:16}}.16b, v{{i|plus:16}}.16b
{% endfor %}
    mov         x4, #32

.p2align 4
.q4f32se_innerloop:
        ld1      { v10.16b }, [ x1 ], #16
        ld1      { v11.s }[0], [ x2 ], #4

        and      v9.16b, v10.16b, v15.16b
        ushr     v10.16b, v10.16b, 4

        tbl      v9.16b, { v13.16b }, v9.16b
        tbl      v10.16b, { v13.16b }, v10.16b

        zip1     v0.16b, v12.16b, v9.16b
        zip2     v2.16b, v12.16b, v9.16b
        zip1     v4.16b, v12.16b, v10.16b
        zip2     v6.16b, v12.16b, v10.16b

        fcvtl2   v1.4s, v0.8h
        fcvtl2   v3.4s, v2.8h
        fcvtl2   v5.4s, v4.8h
        fcvtl2   v7.4s, v6.8h
        fcvtl    v0.4s, v0.4h
        fcvtl    v2.4s, v2.4h
        fcvtl    v4.4s, v4.4h
        fcvtl    v6.4s, v6.4h

{% for i in (0..7) %}
        fmla        v{{ i|plus: 16 }}.4s, v{{i}}.4s, v11.s[0]
{% endfor %}

    subs        x4, x4, #1
    bne         .q4f32se_innerloop

    // scales
    ld1         { v0.8h-v3.8h }, [ x1 ], #64

    fcvtl       v4.4s, v0.4h
    fcvtl2      v5.4s, v0.8h
    fcvtl       v6.4s, v1.4h
    fcvtl2      v7.4s, v1.8h
    fcvtl       v8.4s, v2.4h
    fcvtl2      v9.4s, v2.8h
    fcvtl       v10.4s, v3.4h
    fcvtl2      v11.4s, v3.8h

{% for i in (0..7) %}
       fmla     v{{i|plus:24}}.4s, v{{i|plus:4}}.4s, v{{i|plus:16}}.4s
{% endfor %}

    subs        x3, x3, #32
    bne         .q4f32se_outerloop

    b           .non_linear_loop

{% include "arm64simd_mmm_f32_scalars.tmpliq" from:24, to:31%}
{% include "arm64simd_mmm_f32_per_rows.tmpliq" mr:32, from:24, to:31%}
{% include "arm64simd_mmm_f32_per_cols.tmpliq" mr:32, from:24, to:31%}
{% include "arm64simd_mmm_load_tile.tmpliq" from:24, to:31 %}

.add_unicast:
    ldp         x5, x6, [x0, #8]           // c base ptr, rsc
    cmp         x6, #4
    beq         .do_per_row_add

    {% for reg in (24..31) %}
        {% for lane in (0..3) %}
            ld1 {v0.s}[{{lane}}], [ x5 ], x6
        {% endfor %}
        fadd v{{reg}}.4s, v{{reg}}.4s, v0.4s
    {% endfor %}

    b           .non_linear_loop

.do_per_row_add:
    ld1     {v0.4s-v3.4s}, [x5], #64
    ld1     {v4.4s-v7.4s}, [x5], #64

    {% for r in (0..7) %}
        fadd v{{r| plus: 24}}.4s, v{{r | plus: 24}}.4s, v{{r}}.4s
    {% endfor %}

    b           .non_linear_loop

.add_row_col_products:
    ldr     x3, [x0, #16]
    ldr     x2, [x0, #8]

    ld1         {v8.s}[0], [ x3 ]
    ld1         { v0.4s, v1.4s, v2.4s, v3.4s }, [ x2 ], #64
    ld1         { v4.4s, v5.4s, v6.4s, v7.4s }, [ x2 ], #64

    fmla        v24.4s, v0.4s, v8.s[0]
    fmla        v25.4s, v1.4s, v8.s[0]
    fmla        v26.4s, v2.4s, v8.s[0]
    fmla        v27.4s, v3.4s, v8.s[0]
    fmla        v28.4s, v4.4s, v8.s[0]
    fmla        v29.4s, v5.4s, v8.s[0]
    fmla        v30.4s, v6.4s, v8.s[0]
    fmla        v31.4s, v7.4s, v8.s[0]

    b           .non_linear_loop

.store:
    ldp         x5, x6, [x0, #8]                // c base ptr, rsc

    cmp         x6, #4
    beq         .store_strides_contig

    {% for reg in (24..31) %}
        {% for lane in (0..3) %}
            st1 { v{{reg}}.s }[{{lane}}], [ x5 ], x6
        {% endfor %}
    {% endfor %}
    b           .non_linear_loop

.store_strides_contig:

    {% for reg in (24..31) %}
        st1 { v{{reg}}.4s }, [ x5 ], #16
    {% endfor %}
    b           .non_linear_loop

.return:

    ldp         d14, d15, [sp], #16
    ldp         d12, d13, [sp], #16
    ldp         d10, d11, [sp], #16
    ldp         d8, d9, [sp], #16

    ldp         x26, x27, [sp], #16
    ldp         x24, x25, [sp], #16
    ldp         x22, x23, [sp], #16
    ldp         x20, x21, [sp], #16

    ret

