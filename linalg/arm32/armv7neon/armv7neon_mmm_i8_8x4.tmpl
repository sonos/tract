// vim: ft=arm

// C tile regs
// 
//      q8[0]    q10[0]   q12[0]    q14[0]
//      q8[1]    q10[1]   q12[1]    q14[1]
//      q8[2]    q10[2]   q12[2]    q14[2]
//      q8[3]    q10[3]   q12[3]    q14[3]
//
//      q9[0]    q11[0]   q13[0]    q15[0]
//      q9[1]    q11[1]   q13[1]    q15[1]
//      q9[2]    q11[2]   q13[2]    q15[2]
//      q9[3]    q11[3]   q13[3]    q15[3]

    .arm
    .text
    .global armv7neon_mmm_i8_8x4
    .type armv7neon_mmm_i8_8x4, %function

armv7neon_mmm_i8_8x4:

    pld     [r0]
    push    { r4-r12 }
    vpush   { q4-q7 }

    veor    q8, q8 ,q8
    veor    q9, q9 ,q9
    veor    q10, q10 ,q10
    veor    q11, q11 ,q11
    veor    q12, q12 ,q12
    veor    q13, q13 ,q13
    veor    q14, q14 ,q14
    veor    q15, q15 ,q15

    ldm     r0, { r7, r8, r9, r10 }      // a, b, c, lin
    ldm     r7, { r1, r2 }
    pld     [r10]
    pld     [r8]
    // check a->discriminant == 1 (packed)
    cmp     r1, #1
    bne     .unsupported
    mov     r1, r2 // packed A ptr
    pld     [r1]

    // check linear
    ldm     r10, {r5, r6}
    cmp     r5, #0
    bne     .unsupported
    cmp     r6, #0
    beq     .non_linear

    mov     r3, r6 // k

    // B
    ldm     r8, { r4, r5, r6 }
    cmp     r4, #1
    beq     .packed_packed
    cmp     r4, #2
    beq     .packed_tops_and_offsets
    cmp     r4, #3
    beq     .packed_strides_vec
    b       .unsupported

    .packed_tops_and_offsets:   // r5: rows offsets ptr, r6: cols ptr ptr
    mov             r2, r5                  // r2 <- rows offsets ptr
    ldm             r6, { r7, r8, r9, r10 } // cols ptr
    eor             r5, r5                  // previous offset to sub
    pld             [r2]

    cmp r3, #4
    blt .packed_tops_and_offsets_loop_1

    .packed_tops_and_offsets_loop_4:

    ldm             r2!, { r4 }
    vldmia          r1!, { d0 }
    sub             r6, r4, r5
    mov             r5, r4
    ldm             r2!, { r4 }

    add             r7, r7, r6
    add             r8, r8, r6
    add             r9, r9, r6
    add             r10, r10, r6

    vmovl.s8        q0, d0          // d0, d1: 8 as

    vld1.s8         d4[0], [ r7 ]
    vld1.s8         d4[1], [ r8 ]
    vld1.s8         d4[2], [ r9 ]
    vld1.s8         d4[3], [ r10 ]
    vmovl.s8        q2, d4

    vmlal.s16       q8, d0, d4[0]
    vmlal.s16       q9, d1, d4[0]

    vmlal.s16       q10, d0, d4[1]
    vmlal.s16       q11, d1, d4[1]

    vmlal.s16       q12, d0, d4[2]
    vmlal.s16       q13, d1, d4[2]

    vmlal.s16       q14, d0, d4[3]
    vmlal.s16       q15, d1, d4[3]

    // 1
    sub             r6, r4, r5
    mov             r5, r4
    vldmia          r1!, { d0 }
    ldm             r2!, { r4 }

    add             r7, r7, r6
    add             r8, r8, r6
    add             r9, r9, r6
    add             r10, r10, r6

    vmovl.s8        q0, d0          // d0, d1: 8 as

    vld1.s8         d4[0], [ r7 ]
    vld1.s8         d4[1], [ r8 ]
    vld1.s8         d4[2], [ r9 ]
    vld1.s8         d4[3], [ r10 ]
    vmovl.s8        q2, d4

    vmlal.s16       q8, d0, d4[0]
    vmlal.s16       q9, d1, d4[0]

    vmlal.s16       q10, d0, d4[1]
    vmlal.s16       q11, d1, d4[1]

    vmlal.s16       q12, d0, d4[2]
    vmlal.s16       q13, d1, d4[2]

    vmlal.s16       q14, d0, d4[3]
    vmlal.s16       q15, d1, d4[3]

    // 2
    sub             r6, r4, r5
    mov             r5, r4
    vldmia          r1!, { d0 }
    ldm             r2!, { r4 }

    add             r7, r7, r6
    add             r8, r8, r6
    add             r9, r9, r6
    add             r10, r10, r6

    vmovl.s8        q0, d0          // d0, d1: 8 as

    vld1.s8         d4[0], [ r7 ]
    vld1.s8         d4[1], [ r8 ]
    vld1.s8         d4[2], [ r9 ]
    vld1.s8         d4[3], [ r10 ]
    vmovl.s8        q2, d4

    vmlal.s16       q8, d0, d4[0]
    vmlal.s16       q9, d1, d4[0]

    vmlal.s16       q10, d0, d4[1]
    vmlal.s16       q11, d1, d4[1]

    vmlal.s16       q12, d0, d4[2]
    vmlal.s16       q13, d1, d4[2]

    vmlal.s16       q14, d0, d4[3]
    vmlal.s16       q15, d1, d4[3]

    // 3
    sub             r6, r4, r5
    mov             r5, r4
    vldmia          r1!, { d0 }

    add             r7, r7, r6
    add             r8, r8, r6
    add             r9, r9, r6
    add             r10, r10, r6

    vmovl.s8        q0, d0          // d0, d1: 8 as

    vld1.s8         d4[0], [ r7 ]
    vld1.s8         d4[1], [ r8 ]
    vld1.s8         d4[2], [ r9 ]
    vld1.s8         d4[3], [ r10 ]
    vmovl.s8        q2, d4

    vmlal.s16       q8, d0, d4[0]
    vmlal.s16       q9, d1, d4[0]

    vmlal.s16       q10, d0, d4[1]
    vmlal.s16       q11, d1, d4[1]

    vmlal.s16       q12, d0, d4[2]
    vmlal.s16       q13, d1, d4[2]

    vmlal.s16       q14, d0, d4[3]
    vmlal.s16       q15, d1, d4[3]

    sub r3, r3, #4
    cmp r3, #4
    bge .packed_tops_and_offsets_loop_4

    cmp r3, #0
    beq .packed_tops_and_offsets_end

    .packed_tops_and_offsets_loop_1:

    ldm             r2!, { r4 }     // r4 <- next row ptr
    vldmia          r1!, { d0 }     // d0 <- 8 bytes
    sub             r6, r4, r5      // r6 <- offset increment
    mov             r5, r4          // r5 <- current offset

    add             r7, r7, r6
    add             r8, r8, r6
    add             r9, r9, r6
    add             r10, r10, r6

    vld1.s8         d4[0], [ r7 ]
    vld1.s8         d4[1], [ r8 ]
    vld1.s8         d4[2], [ r9 ]
    vld1.s8         d4[3], [ r10 ]

    vmovl.s8        q0, d0          // d0, d1: 8 as
    vmovl.s8        q2, d4

    vmlal.s16       q8, d0, d4[0]
    vmlal.s16       q9, d1, d4[0]

    vmlal.s16       q10, d0, d4[1]
    vmlal.s16       q11, d1, d4[1]

    vmlal.s16       q12, d0, d4[2]
    vmlal.s16       q13, d1, d4[2]

    vmlal.s16       q14, d0, d4[3]
    vmlal.s16       q15, d1, d4[3]

    subs            r3, r3, #1
    bne .packed_tops_and_offsets_loop_1

    .packed_tops_and_offsets_end:
    b   .non_linear

    .packed_packed:
    pld     [r5]                           // packed B ptr       

    cmp r3, #4
    blt .packed_packed_loop_1

    .packed_packed_loop_4:
    pld             [r1, #64]
    pld             [r5, #64]

    // q2: d4 -> d4,d5 A even cols (from r1)
    // q3: d6 -> d6,d7 A odd cols (from r1)
    // q0: s0 -> d0 : B even lines (from r5)
    // q1: s4 -> d2 : B odd lines (from r5)

    // 0
    vldmia          r1!, { d4 }
    vldmia          r5!, { s0 }

    vmovl.s8        q2, d4
    vmovl.s8        q0, d0

    vmlal.s16       q8, d4, d0[0]
    vmlal.s16       q9, d5, d0[0]

    vldmia          r1!, { d6 }

    vmlal.s16       q10, d4, d0[1]
    vmlal.s16       q11, d5, d0[1]

    vldmia          r5!, { s4 }

    vmlal.s16       q12, d4, d0[2]
    vmlal.s16       q13, d5, d0[2]

    vmlal.s16       q14, d4, d0[3]
    vmlal.s16       q15, d5, d0[3]

    // 1
    vmovl.s8        q3, d6
    vmovl.s8        q1, d2

    vmlal.s16       q8, d6, d2[0]
    vldmia          r1!, { d4 }
    vmlal.s16       q9, d7, d2[0]
    vldmia          r5!, { s0 }

    vmlal.s16       q10, d6, d2[1]
    vmlal.s16       q11, d7, d2[1]

    vmlal.s16       q12, d6, d2[2]
    vmlal.s16       q13, d7, d2[2]

    vmlal.s16       q14, d6, d2[3]
    vmlal.s16       q15, d7, d2[3]

    // 2
    vmovl.s8        q2, d4
    vmovl.s8        q0, d0

    vmlal.s16       q8, d4, d0[0]
    vmlal.s16       q9, d5, d0[0]

    vldmia          r1!, { d6 }

    vmlal.s16       q10, d4, d0[1]
    vmlal.s16       q11, d5, d0[1]

    vldmia          r5!, { s4 }

    vmlal.s16       q12, d4, d0[2]
    vmlal.s16       q13, d5, d0[2]

    vmlal.s16       q14, d4, d0[3]
    vmlal.s16       q15, d5, d0[3]

    // 3
    vmovl.s8        q3, d6
    vmovl.s8        q1, d2

    vmlal.s16       q8, d6, d2[0]
    vmlal.s16       q9, d7, d2[0]

    vmlal.s16       q10, d6, d2[1]
    vmlal.s16       q11, d7, d2[1]

    vmlal.s16       q12, d6, d2[2]
    vmlal.s16       q13, d7, d2[2]

    vmlal.s16       q14, d6, d2[3]
    vmlal.s16       q15, d7, d2[3]

    sub r3, r3, #4
    cmp r3, #4
    bge .packed_packed_loop_4

    cmp r3, #0
    beq .non_linear

    .packed_packed_loop_1:

    vldmia          r1!, { s0, s1 }
    vmovl.s8        q0, d0
    vldmia          r5!, { s4 }
    vmovl.s8        q1, d2

    vmlal.s16       q8, d0, d2[0]
    vmlal.s16       q9, d1, d2[0]

    vmlal.s16       q10, d0, d2[1]
    vmlal.s16       q11, d1, d2[1]

    vmlal.s16       q12, d0, d2[2]
    vmlal.s16       q13, d1, d2[2]

    vmlal.s16       q14, d0, d2[3]
    vmlal.s16       q15, d1, d2[3]

    subs r3, r3, #1
    bne .packed_packed_loop_1
    b   .non_linear

    .packed_strides_vec:
    // r5 -> b ptr, r6 -> b stride

    cmp r3, #4
    blt .packed_strides_vec_loop_1

/*
    cmp r6, #4
    bne .packed_strides_vec_loop_4

    .packed_strides_vec_loop_4_contig:

    vldmia          r1!, { q0, q1, q2, q3 }
    vldmia          r5!, { q4 }

    vmla.f32        q8, q0, d8[0]
    vldmia          r1!, { q0 }
    vmla.f32        q9, q1, d8[0]
    vldmia          r1!, { q1 }
    vmla.f32        q10, q2, d8[1]
    vldmia          r1!, { q2 }
    vmla.f32        q11, q3, d8[1]

    vldmia          r1!, { q3 }

    vmla.f32        q8, q0, d9[0]
    vmla.f32        q9, q1, d9[0]
    vmla.f32        q10, q2, d9[1]
    vmla.f32        q11, q3, d9[1]

    sub r3, r3, #4
    cmp r3, #4
    bge .packed_strides_vec_loop_4_contig
    b .packed_strides_vec_loop_4_end
*/

    .packed_strides_vec_loop_4:

    vldmia          r1!, { q4, q5 }
    vld1.s8         d0[0], [r5], r6
    vld1.s8         d0[1], [r5], r6
    vld1.s8         d0[2], [r5], r6
    vld1.s8         d0[3], [r5], r6

    vmovl.s8        q7, d11
    vmovl.s8        q6, d10
    vmovl.s8        q5, d9
    vmovl.s8        q4, d8

    vmovl.s8        q0, d0

    vmlal.s16       q8, d8, d0[0]
    vmlal.s16       q9, d9, d0[0]
    vmlal.s16       q10, d10, d0[1]
    vmlal.s16       q11, d11, d0[1]
    vmlal.s16       q12, d12, d0[2]
    vmlal.s16       q13, d13, d0[2]
    vmlal.s16       q14, d14, d0[3]
    vmlal.s16       q15, d15, d0[3]

    sub r3, r3, #4
    cmp r3, #4
    bge .packed_strides_vec_loop_4

    vadd.s32        q8, q8, q10
    vadd.s32        q9, q9, q11
    vadd.s32        q8, q8, q12
    vadd.s32        q9, q9, q13
    vadd.s32        q8, q8, q14
    vadd.s32        q9, q9, q15

    .packed_strides_vec_loop_4_end:

    cmp r3, #0
    beq .non_linear

    .packed_strides_vec_loop_1:
    vldmia          r1!, { s0, s1 }     // d0: 8 s8
    vld1.s8         d2[0], [r5], r6     // d2[0]: 1 s8
    vmovl.s8        q0, d0              // q0: 8 s16 <- d0
    vmovl.s8        q1, d2              // q1: 1 s16 <- d1

    vmlal.s16       q8, d0, d2[0]
    vmlal.s16       q9, d1, d2[0]

    subs r3, r3, #1
    bne .packed_strides_vec
    b   .non_linear

.non_linear:

    ldr     r1, [r0, #16]
    cmp     r1, #0
    bne     .non_linear_loop_entry

.store:
    ldr     r3, [r0, #8]
    ldm     r3, { r4, r5, r6, r7 } // discr, ...
    cmp     r4, #0
    beq     .store_strides
    cmp     r4, #3
    beq     .store_vec_strides

    b       .unsupported

.store_strides:
    ldr     r8, [ r3, #16 ]
    cmp     r8, #4
    beq     .store_strides_i32

    // r5,r6,r7 are c,src,csc
    {% for reg in (8..15) %}
        vmovn.s32 d{{reg | times: 2}}, q{{reg}}
        vmovn.s16 d{{reg | times: 2}}, q{{reg}}
    {% endfor %}
    {% for col in (0..3) %}
        mov         r8, r5
        {% for reg in (0..1) %}
            {%capture d%}{{col | times: 2 | plus: reg | times: 2 | plus: 16}}{%endcapture%}
            vst1.s8     d{{d}}[0], [ r8 ], r6
            vst1.s8     d{{d}}[1], [ r8 ], r6
            vst1.s8     d{{d}}[2], [ r8 ], r6
            vst1.s8     d{{d}}[3], [ r8 ], r6
        {% endfor %}
        {% if col < 3 %}
            add r5, r5, r7
        {% endif %}
    {% endfor %}

    mov         r0,     #0
    b           .return

.store_strides_i32:

    {% for col in (0..3) %}
        mov         r8, r5
        {% for reg in (0..3) %}
            {% for lane in (0..1) %}
                vst1.s32     d{{col | times: 4 | plus: reg | plus: 16}}[{{lane}}], [ r8 ], r6
            {% endfor %}
        {% endfor %}
        {% if col < 3 %}
            add r5, r5, r7
        {% endif %}
    {% endfor %}

    mov         r0,     #0
    b           .return

.store_vec_strides:
    // r5, r6, r7: ptr, row_byte_stride, item_stride

    cmp r7, #4
    beq .store_vec_strides_i32

    vmovn.s32   d16, q8
    vmovn.s16   d16, q8
    vmovn.s32   d18, q9
    vmovn.s16   d18, q9
    // r5 c ptr, r6 c stride
    vst1.s8     d16[0], [r5], r6
    vst1.s8     d16[1], [r5], r6
    vst1.s8     d16[2], [r5], r6
    vst1.s8     d16[3], [r5], r6

    vst1.s8     d18[0], [r5], r6
    vst1.s8     d18[1], [r5], r6
    vst1.s8     d18[2], [r5], r6
    vst1.s8     d18[3], [r5]

    mov         r0,     #0
    b           .return

.store_vec_strides_i32:

    {% for reg in (0..3) %}
        {% for lane in (0..1) %}
            vst1.s32     d{{reg | plus: 16}}[{{lane}}], [ r5 ], r6
        {% endfor %}
    {% endfor %}

    mov         r0,     #0
    b           .return

.return:
    vpop        { q4-q7 }
    pop         { r4-r12 }

    bx          lr

.non_linear_loop_entry:
    sub     r1, #12

.non_linear_loop:
    add     r1, #12
    ldr     r2, [r1]
    cmp     r2, #0
    beq     .store
    cmp     r2, #1
    beq     .min
    cmp     r2, #2
    beq     .max
    cmp     r2, #3
    beq     .non_linear_addc
    cmp     r2, #4
    beq     .per_row_mul
    cmp     r2, #5
    beq     .per_row_add
    cmp     r2, #6
    beq     .per_col_mul
    cmp     r2, #7
    beq     .per_col_add
    cmp     r2, #8
    beq     .add_row_col_product
    cmp     r2, #9
    beq     .scalar_mul
    cmp     r2, #10
    beq     .scalar_add
    cmp     r2, #11
    beq     .q_towards_even
    cmp     r2, #12
    beq     .q_towards_plusinf

    b .unsupported

.non_linear_addc:

    ldr     r3, [r0, #8]
    ldr     r4, [r3]
    cmp     r4, #0
    bne     .unsupported

    ldr     r4, [r3, #4]   // C ptr
    ldr     r5, [r3, #8]   // rsc
    ldr     r6, [r3, #12]  // csc
    ldr     r7, [r3, #16]  // item size

    cmp     r7, #4
    beq     .non_linear_addc_i32

    {% for col in (0..3) %}
        mov         r7, r4
        {% for reg in (0..3) %}
            vld1.s8     d0[0], [ r7 ], r5
            vld1.s8     d0[1], [ r7 ], r5
            vmovl.s8    q0, d0
            vmovl.s16   q0, d0
            vadd.i32    d{{col | times: 4 | plus: reg | plus : 16}}, d0
        {% endfor %}
        add r4, r4, r6
    {% endfor %}

    b .non_linear_loop

.non_linear_addc_i32:

    {% for col in (0..3) %}
        mov         r7, r4
        {% for reg in (0..3) %}
            vld1.s32    d0[0], [ r7 ], r5
            vld1.s32    d0[1], [ r7 ], r5
            vadd.i32    d{{col | times: 4 | plus: reg | plus : 16}}, d0
        {% endfor %}
        {% if col < 3 %}
            add r4, r4, r6
        {% endif %}
    {% endfor %}

    b .non_linear_loop

.max:
    vldr            s0, [r1, #4]
    vdup.32         q0, d0[0]
    {% for reg in (8..15) %}
        vmax.s32    q{{reg}}, q{{reg}}, q0
    {% endfor %}

    b .non_linear_loop

.min:
    vldr            s0, [r1, #4]
    vdup.32         q0, d0[0]
    {% for reg in (8..15) %}
        vmin.s32    q{{reg}}, q{{reg}}, q0
    {% endfor %}
    b .non_linear_loop

.per_row_add:
    ldr     r2, [r1, #4]
    vldmia  r2!, { q0, q1 }
    {% for col in (0..3) %}
        vadd.s32 q{{col|times:2|plus:8}}, q{{col|times:2|plus:8}}, q0
        vadd.s32 q{{col|times:2|plus:9}}, q{{col|times:2|plus:9}}, q1
    {% endfor %}

    b .non_linear_loop

.per_row_mul:
    ldr     r2, [r1, #4]
    vldmia  r2!, { q0, q1 }
    {% for col in (0..3) %}
        vmul.s32 q{{col|times:2|plus:8}}, q{{col|times:2|plus:8}}, q0
        vmul.s32 q{{col|times:2|plus:9}}, q{{col|times:2|plus:9}}, q1
    {% endfor %}

    b .non_linear_loop

.per_col_add:
    ldr         r2, [r1, #4]
    vldm        r2, { q0 }
    vdup.s32    q3, d1[1]
    vdup.s32    q2, d1[0]
    vdup.s32    q1, d0[1]
    vdup.s32    q0, d0[0]
    {% for col in (0..3) %}
        vadd.s32 q{{col|times:2|plus:8}}, q{{col|times:2|plus:8}}, q{{col}}
        vadd.s32 q{{col|times:2|plus:9}}, q{{col|times:2|plus:9}}, q{{col}}
    {% endfor %}

    b .non_linear_loop

.per_col_mul:
    ldr         r2, [r1, #4]
    vldm        r2, { q0 }
    vdup.s32    q3, d1[1]
    vdup.s32    q2, d1[0]
    vdup.s32    q1, d0[1]
    vdup.s32    q0, d0[0]
    {% for col in (0..3) %}
        vmul.s32 q{{col|times:2|plus:8}}, q{{col|times:2|plus:8}}, q{{col}}
        vmul.s32 q{{col|times:2|plus:9}}, q{{col|times:2|plus:9}}, q{{col}}
    {% endfor %}

    b .non_linear_loop

.add_row_col_product:
    ldr     r2, [r1, #4]
    ldr     r3, [r1, #8]

    vldmia          r2!, { q0, q1 }
    vldmia          r3!, { q4 }

    vmla.s32        q8, q0, d8[0]
    vmla.s32        q9, q1, d8[0]

    vmla.s32        q10, q0, d8[1]
    vmla.s32        q11, q1, d8[1]

    vmla.s32        q12, q0, d9[0]
    vmla.s32        q13, q1, d9[0]

    vmla.s32        q14, q0, d9[1]
    vmla.s32        q15, q1, d9[1]

    b .non_linear_loop

.scalar_mul:
    vldr        s0, [r1, #4]
    vdup.s32    q0, d0[0]

    {% for q in (8..15) %}
        vmul.s32 q{{q}}, q{{q}}, q0
    {% endfor %}

    b .non_linear_loop

.scalar_add:
    vldr        s0, [r1, #4]
    vdup.s32    q0, d0[0]

    {% for q in (8..15) %}
        vadd.s32 q{{q}}, q{{q}}, q0
    {% endfor %}

    b .non_linear_loop

.q_towards_even:
    b .unsupported
/*
    vldr.s32    s0, [r1, #4]
    ldr         r2, [r1, #8]
    neg         r2, r2
    add         r2, #3
    vmov.s8     d2[0], r2
    vdup.s8     q1, d2[0]
    vneg.s8     q1, q1                      // q1 = initial shift
    mov         r2, #3
    vmov.s8     d6[0], r2
    vdup.s8     q3, d6[0]                   // q3 = 0x11
    mov         r2, #1
    vmov.s8     d10[0], r2
    vdup.s8     q5, d10[0]                  // q5 = 1
    {% for q in (8..15) %}
        vqdmulh.s32    q{{q}}, q{{q}}, d0[0]
    {% endfor %}
    {% for q in (8..15) %}
        vrshl.s8    q{{q}}, q{{q}}, q1
        vclt.s8     q2, q{{q}}, #0          // q2=is_neg(q)
        vqabs.s8    q{{q}}, q{{q}}          // q=abs(q)
        vand.s8     q4, q{{q}}, q3          // q4=abs(q) & 0x11
        vceq.s8     q4, q4, q3              // q4=(abs(q) & 0x11) == 0x11
        vshr.u8     q4, q4, #7              // q4=nudge
        vadd.s8     q{{q}}, q{{q}}, q4
        vshr.s8     q{{q}}, q{{q}}, #1
        vneg.s8     q4, q{{q}}
        vbit        q{{q}}, q4, q2
    {% endfor %}

    b .non_linear_loop
*/

.q_towards_plusinf:
    vldr        s0, [r1, #4]
    vldr        s4, [r1, #8]
    vdup.s8     q1, d2[0]
    vneg.s8     q1, q1
    {% for q in (8..15) %}
        vqrdmulh.s32    q{{q}}, q{{q}}, d0[0]
    {% endfor %}
    {% for q in (8..15) %}
        vqrshl.s32  q{{q}}, q{{q}}, q1
    {% endfor %}

    b .non_linear_loop

.unsupported:
    mov         r0,     #1
    b           .return

