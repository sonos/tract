// vim: ft=arm

// C tile regs
// 
//      q8[0] 
//      q8[1] 
//      q8[2] 
//      q8[3] 
//    
//      ....
//
//      q15[0]
//      q15[1]
//      q15[2]
//      q15[3]

    .arm
    .text
    .global armv7neon_mmm_f32_32x1_{{core}}_{{suffix}}
    .type armv7neon_mmm_f32_32x1_{{core}}_{{suffix}}, %function

armv7neon_mmm_f32_32x1_{{core}}_{{suffix}}:

    pld     [r0]
    push    { r4-r12 }
    vpush   { q4-q7 }

    veor    q8, q8 ,q8
    veor    q9, q9 ,q9
    veor    q10, q10 ,q10
    veor    q11, q11 ,q11
    veor    q12, q12 ,q12
    veor    q13, q13 ,q13
    veor    q14, q14 ,q14
    veor    q15, q15 ,q15

{% include "dispatcher.tmpliq" %}

.add_mat_mul:

    cmp     r3, #0
    beq     .non_linear_loop

    ldm     r5, { r6, r7 } //  b disc, b ptr
    mov     r1, r4 // packed A ptr
    pld     [r3]
    pld     [r7]

    pld     [r1, #128]
    pld     [r1, #192]
    pld     [r1, #256]
    pld     [r1, #320]
    pld     [r1, #384]
    pld     [r1, #448]
    pld     [r1, #512]

    cmp     r6, #0
    beq     .packed_packed

.packed_tops_and_offsets:
    mov             r2, r7                  // &row_byte_offsets
    ldr             r7, [r5,#8]
    ldr             r6, [r7]                // cols tops ptr
    pld             [r5]

.packed_tops_and_offsets_loop_1:
    ldr             r4, [ r2 ], #4

    add             r7, r6, r4
    vldr            s16, [r7]

    vldmia          r1!, { q0-q3 }

    vmla.f32        q8, q0, d8[0]
    vmla.f32        q9, q1, d8[0]

    vldmia          r1!, { q0-q1 }

    vmla.f32        q10, q2, d8[0]
    vmla.f32        q11, q3, d8[0]

    vldmia          r1!, { q2-q3 }
    vmla.f32        q12, q0, d8[0]
    vmla.f32        q13, q1, d8[0]

    vmla.f32        q14, q2, d8[0]
    vmla.f32        q15, q3, d8[0]

    subs            r3, r3, #1
    bne .packed_tops_and_offsets_loop_1

    .packed_tops_and_offsets_end:
    b   .non_linear_loop

    .packed_packed:
    pld     [r5]                           // packed B ptr

    cmp r3, #4
    blt .packed_packed_loop_1

    .packed_packed_loop_1:

{% if core == "cortexa7" %}

    vldr            d0, [r1]
    vldr            d1, [r1, #8]
    vldr            d2, [r1, #16]
    vldr            d3, [r1, #24]
    vldr            d4, [r1, #32]
    vldr            d5, [r1, #40]
    vldr            d6, [r1, #48]
    vldr            d7, [r1, #56]
    vldr            d8, [r1, #64]
    vldr            d9, [r1, #72]
    vldr            d10, [r1, #80]
    vldr            d11, [r1, #88]
    vldr            s30, [r7]

    pld             [r1, #512]
    pld             [r1, #576]
    pld             [r7, #64]

    vmla.f32        q8, q0, d15[0]
    vmla.f32        q9, q1, d15[0]

    vldr            d0, [r1, #96]
    vldr            d1, [r1, #104]
    vldr            d2, [r1, #112]
    vldr            d3, [r1, #120]

    vmla.f32        q10, q2, d15[0]
    vmla.f32        q11, q3, d15[0]

    vmla.f32        q12, q4, d15[0]
    vmla.f32        q13, q5, d15[0]

    vmla.f32        q14, q0, d15[0]
    vmla.f32        q15, q1, d15[0]

    add             r1, #128
    add             r7, #4

{% elsif core == "cortexa9" %}

    vld1.64         {d0-d3}, [r1]!
    vld1.64         {d4-d7}, [r1]!
    pld             [r1, #512]
    pld             [r1, #576]
    vld1.64         {d8-d11}, [r1]!
    vld1.f32        d15[0], [r7]!
    pld             [r7, #64]

    vmla.f32        q8, q0, d15[0]
    vmla.f32        q9, q1, d15[0]
    vld1.64         {d0-d3}, [r1]!

    vmla.f32        q10, q2, d15[0]
    vmla.f32        q11, q3, d15[0]

    vmla.f32        q12, q4, d15[0]
    vmla.f32        q13, q5, d15[0]

    vmla.f32        q14, q0, d15[0]
    vmla.f32        q15, q1, d15[0]

{% else %}

    vldmia          r1!, { q0-q3 }
    vldmia          r7!, { s30 }

    vmla.f32        q8, q0, d15[0]
    vmla.f32        q9, q1, d15[0]
    vldmia          r1!, { q0-q1 }

    vmla.f32        q10, q2, d15[0]
    vmla.f32        q11, q3, d15[0]
    vldmia          r1!, { q2-q3 }

    vmla.f32        q12, q0, d15[0]
    vmla.f32        q13, q1, d15[0]

    vmla.f32        q14, q2, d15[0]
    vmla.f32        q15, q3, d15[0]

{% endif %}

    subs            r3, r3, #1
    bne .packed_packed_loop_1

    b   .non_linear_loop

{% include "armv7neon_mmm_f32_scalars.tmpliq" from:8, to:15 %}
{% include "armv7neon_mmm_f32_per_rows.tmpliq" mr:32, from:8, to:15 %}
{% include "armv7neon_mmm_f32_per_cols.tmpliq" mr:32, from:8, to:15 %}

.add_unicast:
    {% for reg in (0..15) %}
        vld1.f32    d{{reg}}[0], [ r3 ], r4
        vld1.f32    d{{reg}}[1], [ r3 ], r4
    {% endfor %}
    {% for reg in (0..7) %}
        vadd.f32 q{{reg|plus:8}}, q{{reg|plus:8}}, q{{reg}}
    {% endfor %}

    b .non_linear_loop

.add_row_col_products:
    vld1.f32        d0[0], [ r4 ]
    vldmia          r3!, { q4-q7 }

    vmla.f32        q8, q4, d0[0]
    vmla.f32        q9, q5, d0[0]

    vmla.f32        q10, q6, d0[0]
    vmla.f32        q11, q7, d0[0]

    vldmia          r3!, { q4-q7 }

    vmla.f32        q12, q4, d0[0]
    vmla.f32        q13, q5, d0[0]

    vmla.f32        q14, q6, d0[0]
    vmla.f32        q15, q7, d0[0]

    b .non_linear_loop

.q_scale:
    b   .unsupported

.store:
    // r3, r4 <- ptr, rsc
    cmp     r4, #4
    bne     .store_generic

    vst1.f64    {d16-d19}, [r3]!
    vst1.f64    {d20-d23}, [r3]!
    vst1.f64    {d24-d27}, [r3]!
    vst1.f64    {d28-d31}, [r3]!

    b .non_linear_loop

.store_generic:

    {% for reg in (16..31) %}
        vst1.f32    d{{reg}}[0], [r3], r4
        vst1.f32    d{{reg}}[1], [r3], r4
    {% endfor %}

    b .non_linear_loop

.return:
    vpop        { q4-q7 }
    pop         { r4-r12 }

    bx          lr

