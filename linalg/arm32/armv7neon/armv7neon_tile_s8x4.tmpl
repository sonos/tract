// vim: ft=arm

// C tile regs
// 
//      q8[0]    q10[0]   q12[0]    q14[0]
//      q8[1]    q10[1]   q12[1]    q14[1]
//      q8[2]    q10[2]   q12[2]    q14[2]
//      q8[3]    q10[3]   q12[3]    q14[3]
//
//      q9[0]    q11[0]   q13[0]    q15[0]
//      q9[1]    q11[1]   q13[1]    q15[1]
//      q9[2]    q11[2]   q13[2]    q15[2]
//      q9[3]    q11[3]   q13[3]    q15[3]

// packed A buffering (2x8 values): alternating q0, q1 with q2, q3
// packed B buffering (2x4 values): alternating q4 with q5

    .arm
    .text
    .global neon_stile8x4
    .type neon_stile8x4, %function

neon_stile8x4:

    pld     [r0]
    push    { r4-r12 }
    vpush   { q4-q7 }

    veor    q8, q8 ,q8
    veor    q9, q9 ,q9
    veor    q10, q10 ,q10
    veor    q11, q11 ,q11
    veor    q12, q12 ,q12
    veor    q13, q13 ,q13
    veor    q14, q14 ,q14
    veor    q15, q15 ,q15

    ldm     r0, { r7, r8, r9, r10 }      // a, b, c, lin
    ldm     r7, { r1, r2 }
    pld     [r10]
    pld     [r8]
    // check a->discriminant == 1 (packed)
    cmp     r1, #1
    bne     .unsupported
    mov     r1, r2 // packed A ptr
    pld     [r1]

    // check linear
    ldm     r10, {r5, r6}
    cmp     r5, #0
    bne     .unsupported
    cmp     r6, #0
    beq     .non_linear

    mov     r3, r6 // k

    // B
    ldm     r8, { r4, r5, r6 }
    cmp     r4, #1
    beq     .packed_packed
    cmp     r4, #2
    beq     .packed_tops_and_offsets
    b       .unsupported

    .packed_tops_and_offsets:
    push            { r0 }
    ldm             r5!, { r0 }
    mov             r4, r1
    mov             r2, r5                  // rows offsets
    ldm             r6, {r5, r6, r7, r8}    // cols tops ptr
    pld             [r5]

    cmp r3, #4
    blt .packed_tops_and_offsets_loop_1

    .packed_tops_and_offsets_loop_4:
    // 1
    vldmia          r4!, { q0, q1, q2, q3 }
    ldm             r2!, { r1 }

    add             r9, r5, r0
    add             r10, r6, r0
    add             r11, r7, r0
    add             r12, r8, r0

    ldm             r2!, { r0 }

    vldr            s16, [r9]
    add             r9, r5, r1
    vldr            s17, [r10]
    add             r10, r6, r1
    vldr            s18, [r11]
    add             r11, r7, r1
    vldr            s19, [r12]
    add             r12, r8, r1

    ldm             r2!, { r1 }
    vmla.f32        q8, q0, d8[0]
    vldr            s20, [r9]
    vmla.f32        q9, q1, d8[0]
    vldr            s21, [r10]

    vmla.f32        q10, q0, d8[1]
    vldr            s22, [r11]
    vmla.f32        q11, q1, d8[1]
    vldr            s23, [r12]

    vmla.f32        q12, q0, d9[0]
    add             r9, r5, r0
    vmla.f32        q13, q1, d9[0]
    add             r10, r6, r0

    vmla.f32        q14, q0, d9[1]
    add             r11, r7, r0
    vmla.f32        q15, q1, d9[1]
    add             r12, r8, r0

    vldmia          r4!, { q0, q1 }

    // 2
    vmla.f32        q8, q2, d10[0]
    ldm             r2!, { r0 }
    vmla.f32        q9, q3, d10[0]
    vldr            s16, [r9]

    vmla.f32        q10, q2, d10[1]
    vldr            s17, [r10]
    vmla.f32        q11, q3, d10[1]
    vldr            s18, [r11]

    vmla.f32        q12, q2, d11[0]
    vldr            s19, [r12]
    vmla.f32        q13, q3, d11[0]
    add             r9, r5, r1

    vmla.f32        q14, q2, d11[1]
    add             r10, r6, r1
    vmla.f32        q15, q3, d11[1]
    add             r11, r7, r1

    // 3

    vmla.f32        q8, q0, d8[0]
    add             r12, r8, r1
    vmla.f32        q9, q1, d8[0]
    vldmia          r4!, { q2, q3 }

    vmla.f32        q10, q0, d8[1]
    vldr            s20, [r9]
    vmla.f32        q11, q1, d8[1]
    vldr            s21, [r10]

    vmla.f32        q12, q0, d9[0]
    vldr            s22, [r11]
    vmla.f32        q13, q1, d9[0]
    vldr            s23, [r12]

    vmla.f32        q14, q0, d9[1]
//    pld             [r4, #64]
    vmla.f32        q15, q1, d9[1]
//    pld             [r2, #64]

    // 4

    vmla.f32        q8, q2, d10[0]
    vmla.f32        q9, q3, d10[0]

    vmla.f32        q10, q2, d10[1]
    vmla.f32        q11, q3, d10[1]

    vmla.f32        q12, q2, d11[0]
    vmla.f32        q13, q3, d11[0]

    vmla.f32        q14, q2, d11[1]
    vmla.f32        q15, q3, d11[1]
    sub r3, r3, #4

    cmp r3, #4
    bge .packed_tops_and_offsets_loop_4

    cmp r3, #0
    beq .packed_tops_and_offsets_end

    .packed_tops_and_offsets_loop_1:
    vldmia          r4!, { q0, q1 }

    add             r9, r5, r0
    add             r10, r6, r0
    add             r11, r7, r0
    add             r12, r8, r0

    ldm             r2!, { r0 }

    vldr            s16, [r9]
    vldr            s17, [r10]
    vldr            s18, [r11]
    vldr            s19, [r12]

    vmla.f32        q8, q0, d8[0]
    vmla.f32        q9, q1, d8[0]

    vmla.f32        q10, q0, d8[1]
    vmla.f32        q11, q1, d8[1]

    vmla.f32        q12, q0, d9[0]
    vmla.f32        q13, q1, d9[0]

    vmla.f32        q14, q0, d9[1]
    vmla.f32        q15, q1, d9[1]

    subs            r3, r3, #1
    bne .packed_tops_and_offsets_loop_1

    .packed_tops_and_offsets_end:
    pop             { r0 }
    b   .non_linear

    .packed_packed:
    pld     [r5]                           // packed B ptr       

    cmp r3, #4
    blt .packed_packed_loop_1

    .packed_packed_loop_4:
    pld             [r1, #64]
    pld             [r5, #64]

    // 1
    vldmia          r1!, { q0, q1 }
    vldmia          r5!, { q4 }

    vmla.f32        q8, q0, d8[0]
    vmla.f32        q9, q1, d8[0]

    vldmia          r1!, { q2, q3 }

    vmla.f32        q10, q0, d8[1]
    vmla.f32        q11, q1, d8[1]

    vldmia          r5!, { q5 }

    vmla.f32        q12, q0, d9[0]
    vmla.f32        q13, q1, d9[0]

    vmla.f32        q14, q0, d9[1]
    vmla.f32        q15, q1, d9[1]

    // 2
    vldmia          r1!, { q0, q1 }

    vmla.f32        q8, q2, d10[0]
    vmla.f32        q9, q3, d10[0]

    vldmia          r5!, { q4 }

    vmla.f32        q10, q2, d10[1]
    vmla.f32        q11, q3, d10[1]

    vmla.f32        q12, q2, d11[0]
    vmla.f32        q13, q3, d11[0]

    vmla.f32        q14, q2, d11[1]
    vmla.f32        q15, q3, d11[1]

    // 3
    vldmia          r1!, { q2, q3 }

    vmla.f32        q8, q0, d8[0]
    vmla.f32        q9, q1, d8[0]

    vldmia          r5!, { q5 }

    vmla.f32        q10, q0, d8[1]
    vmla.f32        q11, q1, d8[1]

    vmla.f32        q12, q0, d9[0]
    vmla.f32        q13, q1, d9[0]

    vmla.f32        q14, q0, d9[1]
    vmla.f32        q15, q1, d9[1]

    // 4
    vmla.f32        q8, q2, d10[0]
    vmla.f32        q9, q3, d10[0]

    vmla.f32        q10, q2, d10[1]
    vmla.f32        q11, q3, d10[1]

    vmla.f32        q12, q2, d11[0]
    vmla.f32        q13, q3, d11[0]

    vmla.f32        q14, q2, d11[1]
    vmla.f32        q15, q3, d11[1]

    sub r3, r3, #4
    cmp r3, #4
    bge .packed_packed_loop_4

    cmp r3, #0
    beq .non_linear

    .packed_packed_loop_1:

    vldmia          r1!, { q0, q1 }
    vldmia          r5!, { q4 }

    vmla.f32        q8, q0, d8[0]
    vmla.f32        q9, q1, d8[0]

    vmla.f32        q10, q0, d8[1]
    vmla.f32        q11, q1, d8[1]

    vmla.f32        q12, q0, d9[0]
    vmla.f32        q13, q1, d9[0]

    vmla.f32        q14, q0, d9[1]
    vmla.f32        q15, q1, d9[1]

    subs r3, r3, #1
    bne .packed_packed_loop_1

.non_linear:

    ldr     r1, [r0, #16]
    cmp     r1, #0
    bne     .non_linear_loop_entry

.store:
    ldr     r3, [r0, #8]
    ldm     r3, { r4, r5, r6, r7 } // discr, ...
    cmp     r4, #0
    bne     .unsupported

    // r5,r6,r7 are c,src,csc

    {% for col in (0..3) %}
        mov         r8, r5
        {% for reg in (0..3) %}
            vst1.f32    d{{col | times: 4 | plus: reg | plus : 16}}[0], [ r8 ]
            add         r8, r8, r6
            vst1.f32    d{{col | times: 4 | plus: reg | plus : 16}}[1], [ r8 ]
            {% if reg < 3 %}
                add         r8, r8, r6
            {% endif %}
        {% endfor %}
        {% if col < 3 %}
            add r5, r5, r7
        {% endif %}
    {% endfor %}

    mov         r0,     #0

.return:
    vpop        { q4-q7 }
    pop         { r4-r12 }

    bx          lr

.non_linear_loop_entry:
    sub     r1, #8

.non_linear_loop:
    add     r1, #8
    ldr     r2, [r1]
    cmp     r2, #0
    beq     .store
    cmp     r2, #3
    beq     .non_linear_addc

    b .unsupported

.non_linear_addc:

    ldr     r3, [r0, #8]
    ldr     r4, [r3]
    cmp     r4, #0
    bne     .unsupported

    ldr     r4, [r3, #4]   // C ptr
    ldr     r5, [r3, #8]   // rsc
    ldr     r6, [r3, #12]  // csc

    {% for col in (0..3) %}
        mov         r7, r4
        {% for reg in (0..3) %}
            vld1.f32    d0[0], [ r7 ]
            add         r7, r7, r5
            vld1.f32    d0[1], [ r7 ]
            add         r7, r7, r5
            vadd.f32    d{{col | times: 4 | plus: reg | plus : 16}}, d0
        {% endfor %}
        add r4, r4, r6
    {% endfor %}

    b .non_linear_loop

.unsupported:
    mov         r0,     #1
    b           .return

