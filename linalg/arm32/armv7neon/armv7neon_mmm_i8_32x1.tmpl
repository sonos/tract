// vim: ft=arm

// C tile regs: q8..q16

    .arm
    .text
    .global armv7neon_mmm_i8_32x1_{{suffix}}
    .type armv7neon_mmm_i8_32x1_{{suffix}}, %function

armv7neon_mmm_i8_32x1_{{suffix}}:

    pld     [r0]
    push    { r4-r12 }
    vpush   { q4-q7 }

    veor    q8, q8 ,q8
    veor    q9, q9 ,q9
    veor    q10, q10 ,q10
    veor    q11, q11 ,q11
    veor    q12, q12 ,q12
    veor    q13, q13 ,q13
    veor    q14, q14 ,q14
    veor    q15, q15 ,q15

    ldm     r0, { r7, r8, r9, r10 }      // a, b, c, lin
    ldm     r7, { r1, r2 }
    pld     [r10]
    pld     [r8]
    // check a->discriminant == 1 (packed)
    cmp     r1, #1
    bne     .unsupported
    mov     r1, r2 // packed A ptr
    pld     [r1]

    // check linear
    ldm     r10, {r5, r6}
    cmp     r5, #0
    bne     .unsupported
    cmp     r6, #0
    beq     .non_linear

    mov     r3, r6 // k

    // B
    ldm     r8, { r4, r5, r6 }
    cmp     r4, #1
    beq     .packed_packed
    cmp     r4, #2
    beq     .packed_tops_and_offsets
    b       .unsupported

    .packed_tops_and_offsets:   // r5: rows offsets ptr, r6: cols ptr ptr
    mov             r2, r5                  // r2 <- rows offsets ptr
    ldm             r6, { r7 }              // r7 col ptr
    pld             [r2]

    .packed_tops_and_offsets_loop_1:
    vldmia          r1!, { q4-q5 }

    ldm             r2!, { r4 }     // r4 <- next row ptr
    add             r8, r7, r4
    vld1.s8         d0[0], [ r8 ]
    vmovl.s8        q0, d0

    vmovl.s8        q1, d8
    vmlal.s16       q8, d2, d0[0]
    vmlal.s16       q9, d3, d0[0]

    vmovl.s8        q1, d9
    vmlal.s16       q10, d2, d0[0]
    vmlal.s16       q11, d3, d0[0]

    vmovl.s8        q1, d10
    vmlal.s16       q12, d2, d0[0]
    vmlal.s16       q13, d3, d0[0]

    vmovl.s8        q1, d11
    vmlal.s16       q14, d2, d0[0]
    vmlal.s16       q15, d3, d0[0]

    subs            r3, r3, #1
    bne .packed_tops_and_offsets_loop_1

    .packed_tops_and_offsets_end:
    b   .non_linear

    .packed_packed:
    pld     [r5]                           // packed B ptr       

    .packed_packed_loop_1:
    vldmia          r1!, { q4-q5 }

    vld1.8          { d0[0] }, [ r5 ]!
    vmovl.s8        q0, d0

    vmovl.s8        q1, d8
    vmlal.s16       q8, d2, d0[0]
    vmlal.s16       q9, d3, d0[0]

    vmovl.s8        q1, d9
    vmlal.s16       q10, d2, d0[0]
    vmlal.s16       q11, d3, d0[0]

    vmovl.s8        q1, d10
    vmlal.s16       q12, d2, d0[0]
    vmlal.s16       q13, d3, d0[0]

    vmovl.s8        q1, d11
    vmlal.s16       q14, d2, d0[0]
    vmlal.s16       q15, d3, d0[0]

    subs r3, r3, #1
    bne .packed_packed_loop_1
    b   .non_linear

{% include "dispatcher.tmpliq" %}

.add_unicast:
    // r3, r4, r5, r6 <- ptr, rsc, csc, size

    cmp     r6, #4
    beq     .non_linear_addc_i32

    {% for reg in (16..31) %}
        vld1.s8     d0[0], [ r3 ], r4
        vld1.s8     d0[1], [ r3 ], r4
        vmovl.s8    q0, d0
        vmovl.s16   q0, d0
        vadd.i32    d{{reg}}, d0
    {% endfor %}

    b .non_linear_loop

.non_linear_addc_i32:
    {% for reg in (16..31) %}
        vld1.s32    d0[0], [ r3 ], r4
        vld1.s32    d0[1], [ r3 ], r4
        vadd.i32    d{{reg}}, d0
    {% endfor %}
    b .non_linear_loop

.max:
    vmov            s0, r3
    vdup.32         q0, d0[0]
    {% for reg in (8..15) %}
        vmax.s32    q{{reg}}, q{{reg}}, q0
    {% endfor %}

    b .non_linear_loop

.min:
    vmov            s0, r3
    vdup.32         q0, d0[0]
    {% for reg in (8..15) %}
        vmin.s32    q{{reg}}, q{{reg}}, q0
    {% endfor %}
    b .non_linear_loop

.per_row_add:
    vldmia  r3!, { q0-q3 }
    vldmia  r3!, { q4-q7 }

    {% for reg in (0..7) %}
        vadd.s32 q{{reg|plus:8}}, q{{reg|plus:8}}, q{{reg}}
    {% endfor %}

    b .non_linear_loop

.per_row_mul:
    vldmia  r3!, { q0-q3 }
    vldmia  r3!, { q4-q7 }

    {% for reg in (0..7) %}
        vmul.s32 q{{reg|plus:8}}, q{{reg|plus:8}}, q{{reg}}
    {% endfor %}

    b .non_linear_loop

.per_col_add:
    vldm        r3, { s0 }
    vdup.s32    q0, d0[0]
    {% for reg in (8..15) %}
        vadd.s32 q{{reg}}, q{{reg}}, q0
    {% endfor %}

    b .non_linear_loop

.per_col_mul:
    vldm        r3, { s0 }
    vdup.s32    q0, d0[0]
    {% for reg in (8..15) %}
        vmul.s32 q{{reg}}, q{{reg}}, q0
    {% endfor %}

    b .non_linear_loop

.add_row_col_product:
    vldm    	r3, { s0 }

    vldmia          r4!, { q4-q7 }

    vmla.s32        q8, q4, d0[0]
    vmla.s32        q9, q5, d0[0]

    vmla.s32        q10, q6, d0[0]
    vmla.s32        q11, q7, d0[0]

    vldmia          r4!, { q4-q7 }

    vmla.s32        q12, q4, d0[0]
    vmla.s32        q13, q5, d0[0]

    vmla.s32        q14, q6, d0[0]
    vmla.s32        q15, q7, d0[0]

    b .non_linear_loop

.scalar_mul:
    vmov            s0, r3
    vdup.s32    q0, d0[0]

    {% for q in (8..15) %}
        vmul.s32 q{{q}}, q{{q}}, q0
    {% endfor %}

    b .non_linear_loop

.scalar_add:
    vmov            s0, r3
    vdup.s32    q0, d0[0]

    {% for q in (8..15) %}
        vadd.s32 q{{q}}, q{{q}}, q0
    {% endfor %}

    b .non_linear_loop

    {% include "armv7neon_mmm_i8_scale_q8_q15.tmpliq" %}

.store:
    // r3, r4, r5, r6 <- ptr, rsc, csc, size
    cmp     r6, #4
    beq     .store_strides_i32

    {% for reg in (8..15) %}
        vmovn.s32 d{{reg | times: 2}}, q{{reg}}
        vmovn.s16 d{{reg | times: 2}}, q{{reg}}
    {% endfor %}
    {% for reg in (8..15) %}
        {%capture d%}{{reg | times: 2 }}{%endcapture%}
        vst1.s8     d{{d}}[0], [ r3 ], r4
        vst1.s8     d{{d}}[1], [ r3 ], r4
        vst1.s8     d{{d}}[2], [ r3 ], r4
        vst1.s8     d{{d}}[3], [ r3 ], r4
    {% endfor %}

    b .non_linear_loop

.store_strides_i32:
    {% for reg in (8..15) %}
        {%capture d%}{{reg | times: 2}}{%endcapture%}
        vst1.s32    d{{d}}[0], [ r3 ], r4
        vst1.s32    d{{d}}[1], [ r3 ], r4
        vst1.s32    d{{d|plus:1}}[0], [ r3 ], r4
        vst1.s32    d{{d|plus:1}}[1], [ r3 ], r4
    {% endfor %}

    b .non_linear_loop

.return:
    vpop        { q4-q7 }
    pop         { r4-r12 }

    bx          lr

