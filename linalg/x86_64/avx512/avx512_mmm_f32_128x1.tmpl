{% comment %}
// vim: set syntax=asm :

/* mmm 128 x 1

    zmm0
    zmm1
    ...
    zmm7

System V ABI:
    args: rdi, rsi, rdx, rcx, r8, r9
    preserve: rbx, rsp, rbp, r12, r13, r14, r15
    scratch: rax, rdi, rsi, rdx, rcx, r8, r9, r10, r11
    return: rax (+rdx)

Windows ABI:
    args: RCX, RDX, R8, R9
    preserve: RBX, RBP, RDI, RSI, RSP, R12, R13, R14, R15, and XMM6-15
    scratch: RAX, RCX, RDX, R8, R9, R10, R11, XMM0-5, and the upper portions of ZMM0-15 and ZMM0-15
    return: rax (+rdx)
*/
{% endcomment %}

{% include "preamble.tmpliq" size:"128x1", suffix:suffix, G:G, arch:"avx512" %}

{{L}}clear:
    vzeroall
    jmp     {{L}}non_linear_loop

{{L}}add_mat_mul:
    mov     rcx,    [rdi + 24]   // B
    mov     rax,    [rdi + 16]   // A

    mov     rbx,    [rdi + 8]    // k
    mov     r8,     [rdi + 32]   // packing
    test    rbx,    rbx
    jz      {{L}}non_linear_loop

    cmp     r8, 1
    jz      {{L}}q40f32

{{align}} 16
{{L}}f32f32:
	{% include "8x1/packed_packed_loop1/avx-512.tmpli" %}

    sub             rbx, 1
    jnz             {{L}}f32f32

    jmp             {{L}}non_linear_loop

{{L}}q40f32_mask:
{% if msvc %}
    {{long}} 0F0F0F0Fh
{% else %}
    {{long}} 0x0F0F0F0F
{% endif %}

{{L}}q40f32_eight:
    {{long}} 8

{{L}}q40f32_perm:
    {{quad}} 2
    {{quad}} 3
    {{quad}} 4
    {{quad}} 5
    {{quad}} 6
    {{quad}} 7
    {{quad}} 0 // we dont care what's rolling in from the right
    {{quad}} 0

{{L}}q40f32:
    // zmm0-7: acc
    // zmm8-16: scales
    // zmm30: 8
    // zmm29: mask
    // zmm31: b value
    vbroadcastss    zmm29, dword ptr [{{offset}} {{L}}q40f32_mask]
    vbroadcastss    zmm30, dword ptr [{{offset}} {{L}}q40f32_eight]
    vmovups         zmm28, [{{offset}} {{L}}q40f32_perm]

{{L}}q40f32_outerloop:
    // scales
    {% for i in (0..7) %}
        vmovaps         ymm{{i|plus:8}}, [rax + {{i|times:32}}]
    {% endfor %}
    {% for i in (0..7) %}
        vcvtph2ps       zmm{{i|plus:8}}, ymm{{i|plus:8}}
    {% endfor %}
    add             rax, 256

    mov             rdx, 32

{{L}}q40f32_innerloop:
    vbroadcastss    zmm31, dword ptr [rcx]
    vmovaps         zmm27, [rax]            // 128 nibbles

    vpandq           zmm26, zmm27, zmm29     // 64 bytes

    vpmovzxbd       zmm16, xmm26            // 16 u32
    vpermt2q        zmm26, zmm28, zmm26
    vpmovzxbd       zmm17, xmm26            // 16 u32
    vpermt2q        zmm26, zmm28, zmm26
    vpmovzxbd       zmm18, xmm26            // 16 u32
    vpermt2q        zmm26, zmm28, zmm26
    vpmovzxbd       zmm19, xmm26            // 16 u32

    vpsrlw          zmm27, zmm27, 4
    vpandq          zmm26, zmm27, zmm29     // 64 bytes

    vpmovzxbd       zmm20, xmm26            // 16 u32
    vpermt2q        zmm26, zmm28, zmm26
    vpmovzxbd       zmm21, xmm26            // 16 u32
    vpermt2q        zmm26, zmm28, zmm26
    vpmovzxbd       zmm22, xmm26            // 16 u32
    vpermt2q        zmm26, zmm28, zmm26
    vpmovzxbd       zmm23, xmm26            // 16 u32


    {% for i in (16..23) %}
        vpsubd          zmm{{i}}, zmm{{i}}, zmm30
    {% endfor %}

    {% for i in (16..23) %}
        vcvtdq2ps       zmm{{i}}, zmm{{i}}
    {% endfor %}

    {% for i in (0..7) %}
        vmulps      zmm{{i|plus:16}}, zmm{{i|plus:16}}, zmm{{i|plus:8}}
    {% endfor %}
    
    {% for i in (0..7) %}
        vfmadd231ps zmm{{i}}, zmm{{i|plus:16}}, zmm31
    {% endfor %}

    add             rax, 64
    add             rcx, 4
    sub             rdx, 1
    jnz             {{L}}q40f32_innerloop

    sub             rbx, 32
    jnz             {{L}}q40f32_outerloop

    jmp             {{L}}non_linear_loop

{% include "f32_scalars.tmpliq" from:0, to:7 %}
{% include "f32_per_rows.tmpliq" mr:128, from:0, to:7 %}
{% include "f32_per_cols.tmpliq" mr:128, from:0, to:7 %}
{% include "avx512_mmm_load_tile.tmpliq" from:0, to:7 %}

{{L}}add_unicast:
    mov     r10,    [rdi + 8]           // c ptr
    mov     rsi,    [rdi + 16]          // row stride

    {% for row in (0..7) %}
        vaddps zmm{{row}}, zmm{{row}}, [ r10 + {{row|times:64}} ]
    {% endfor %}

    jmp    {{L}}non_linear_loop

{{L}}add_row_col_products:
    mov             rax, [ rdi + 8 ]
    mov             rbx, [ rdi + 16 ]

    vbroadcastss    zmm14, dword ptr [rbx]

{% for i in (0..7) %}
    vmovups         zmm12,  [rax + {{i|times:64}}]
    vfmadd231ps     zmm{{i}}, zmm12, zmm14
{% endfor %}
    jmp    {{L}}non_linear_loop

{{L}}store:
    mov     r8,     [rdi + 8]           // c ptr
    mov     rsi,    [rdi + 16]          // row stride

    cmp     rsi, 4
    jne      {{L}}store_noncontiguous

	test r8, 63
	jnz {{L}}store_unaligned

	{% for row in (0..7) %}
        vmovaps [r8 + {{row|times:64}}], zmm{{row}}
    {% endfor %}

    jmp     {{L}}non_linear_loop

{{L}}store_unaligned:
	{% for row in (0..7) %}
        vmovups [r8 + {{row|times:64}}], zmm{{row}}
    {% endfor %}

    jmp     {{L}}non_linear_loop

{{L}}store_noncontiguous:
    {% for r in (0..7) %}
        {% for quarter in (0..3) %}
            vextractf32x4 xmm8, zmm{{r}}, {{quarter}}
            {% for row in (0..3) %}
                vextractps  dword ptr [r8], xmm8, {{row}}
                add         r8, rsi
            {% endfor %}
        {% endfor %}
    {% endfor %}
    jmp     {{L}}non_linear_loop

{% include "postamble.tmpliq" size:"128x1", suffix:suffix, G:G, L:L, arch:"avx512" %}
