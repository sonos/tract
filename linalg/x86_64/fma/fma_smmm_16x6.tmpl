{% comment %}
/* vim: set syntax=asm : */

/* mmm 16 x 6:

    ymm0 ymm2 ymm4 ymm6 ymm8 ymm10
    ymm1 ymm3 ymm5 ymm7 ymm9 ymm11

System V ABI:
    args: rdi, rsi, rdx, rcx, r8, r9
    preserve: rbx, rsp, rbp, r12, r13, r14, r15
    scratch: rax, rdi, rsi, rdx, rcx, r8, r9, r10, r11
    return: rax (+rdx)

Windows ABI:
    args: RCX, RDX, R8, R9
    preserve: RBX, RBP, RDI, RSI, RSP, R12, R13, R14, R15, and XMM6-15
    scratch: RAX, RCX, RDX, R8, R9, R10, R11, XMM0-5, and the upper portions of YMM0-15 and ZMM0-15
    return: rax (+rdx)
*/
{% endcomment %}

{% if os == "macos" %}

.intel_syntax noprefix
.text
.p2align 5
.globl _fma_smmm16x6
_fma_smmm16x6:

{% elsif family == "unix" %}

.intel_syntax noprefix
.text
.p2align 5
.globl fma_smmm16x6
fma_smmm16x6:

{% elsif family == "windows" %}

_text segment
fma_smmm16x6 proc

{% endif %}

    push        rbp
    mov         rbp, rsp

{% if family == "windows" %}
// https://www.agner.org/optimize/calling_conventions.pdf xmm6-15 are not scratch
// https://stackoverflow.com/questions/43358429/save-value-of-xmm-registers
    and rsp,-16
    lea rsp,[rsp-160]
    vmovaps [rsp], xmm6
    vmovaps [rsp+16*1],xmm7
    vmovaps [rsp+16*2],xmm8
    vmovaps [rsp+16*3],xmm9
    vmovaps [rsp+16*4],xmm10
    vmovaps [rsp+16*5],xmm11
    vmovaps [rsp+16*6],xmm12
    vmovaps [rsp+16*7],xmm13
    vmovaps [rsp+16*8],xmm14
    vmovaps [rsp+16*9],xmm15

    push        rdi
    push        rsi

    mov         rdi, rcx

{% endif %}

    push        rbx
    push        r12
    push        r13
    push        r14
    push        r15

    sub         rsp, 8
    stmxcsr     [rsp + 4]
{% if family == "windows" %}
    mov         rax, 1FC0h
{% else %}
    mov         rax, 0x1FC0
{% endif %}
    mov         [rsp], eax
    ldmxcsr     [rsp]

    vzeroall

    mov     rax,    [rdi]       // A
    mov     rbx,    [rdi + 8]   // B

    mov     rcx,    [rdi + 24]  // Linear spec
    mov     rcx,    [rcx + 8]   // k
    test    rcx,    rcx

    je      {{L}}non_linear

    mov     rsi, [rbx]   // B discriminant
    cmp     rsi,  1
    je      {{L}}packed_packed
    cmp     rsi,  2
    je      {{L}}packed_tops_and_offsets
    cmp     rsi,  3
    je      {{L}}packed_vec

    jmp     {{L}}unimplemented

{{L}}packed_tops_and_offsets:
    mov     rax,    [rax + 8]   // A
    mov     rsi,    [rbx + 16]  // B cols head
    mov     rbx,    [rbx + 8]   // rbx: current row offset ptr

    mov     r8,     [rsi]
    mov     r9,     [rsi + 8]
    mov     r10,    [rsi + 16]
    mov     r11,    [rsi + 24]
    mov     r12,    [rsi + 32]
    mov     r13,    [rsi + 40]

{{L}}main_loop_packed_tops_and_offsets:
    mov             rsi,    [rbx]   // rsi: current row offset

    vmovaps         ymm12,  [rax]
    vmovaps         ymm13,  [rax + 32]

    vbroadcastss    ymm14,  dword ptr [r8 + rsi]
    vbroadcastss    ymm15,  dword ptr [r9 + rsi]

    vfmadd231ps     ymm0,   ymm12, ymm14
    vfmadd231ps     ymm1,   ymm13, ymm14

    vbroadcastss    ymm14,  dword ptr [r10 + rsi]

    vfmadd231ps     ymm2,   ymm12, ymm15
    vfmadd231ps     ymm3,   ymm13, ymm15

    vbroadcastss    ymm15,  dword ptr [r11 + rsi]

    vfmadd231ps     ymm4,   ymm12, ymm14
    vfmadd231ps     ymm5,   ymm13, ymm14

    vbroadcastss    ymm14,  dword ptr [r12 + rsi]

    vfmadd231ps     ymm6,   ymm12, ymm15
    vfmadd231ps     ymm7,   ymm13, ymm15

    vbroadcastss    ymm15,  dword ptr [r13 + rsi]

    vfmadd231ps     ymm8,   ymm12, ymm14
    vfmadd231ps     ymm9,   ymm13, ymm14

    vfmadd231ps     ymm10,   ymm12, ymm15
    vfmadd231ps     ymm11,   ymm13, ymm15

    add             rbx,    8
    add             rax,    64
    dec             rcx
    jnz             {{L}}main_loop_packed_tops_and_offsets

    jmp             {{L}}non_linear

{{L}}packed_packed:

    mov     rax,   [rax + 8] // A
    mov     rbx,   [rbx + 8] // B 

{{L}}main_loop_packed_packed:
    vbroadcastss    ymm14,  dword ptr [rbx]
    vbroadcastss    ymm15,  dword ptr [rbx + 4]

    vmovaps         ymm12,  [rax]
    vmovaps         ymm13,  [rax + 32]

    vfmadd231ps     ymm0,   ymm12, ymm14
    vfmadd231ps     ymm1,   ymm13, ymm14

    vbroadcastss    ymm14,  dword ptr [rbx + 8]

    vfmadd231ps     ymm2,   ymm12, ymm15
    vfmadd231ps     ymm3,   ymm13, ymm15

    vbroadcastss    ymm15,  dword ptr [rbx + 12]

    vfmadd231ps     ymm4,   ymm12, ymm14
    vfmadd231ps     ymm5,   ymm13, ymm14

    vbroadcastss    ymm14,  dword ptr [rbx + 16]

    vfmadd231ps     ymm6,   ymm12, ymm15
    vfmadd231ps     ymm7,   ymm13, ymm15

    vbroadcastss    ymm15,  dword ptr [rbx + 20]

    vfmadd231ps     ymm8,   ymm12, ymm14
    vfmadd231ps     ymm9,   ymm13, ymm14

    vfmadd231ps     ymm10,   ymm12, ymm15
    vfmadd231ps     ymm11,   ymm13, ymm15

    add             rbx,    24
    add             rax,    64
    dec             rcx
    jnz             {{L}}main_loop_packed_packed

    jmp             {{L}}non_linear

{{L}}packed_vec:
    mov     rax,   [rax + 8]    // A
    mov     rsi,   [rbx + 16]   // B stride
    mov     rbx,   [rbx + 8]    // B ptr

{{L}}packed_vec_loop:
    vbroadcastss    ymm14,  dword ptr [rbx]
    vmovaps         ymm12,  [rax]
    vmovaps         ymm13,  [rax + 32]

    vfmadd231ps     ymm0,   ymm12, ymm14
    vfmadd231ps     ymm1,   ymm13, ymm14

    add             rbx,    rsi
    add             rax,    64
    dec             rcx
    jnz             {{L}}packed_vec_loop

{{L}}non_linear:

    mov     rcx,    [rdi + 32]          // non linear spec
    test    rcx,    rcx
    jnz     {{L}}non_linear_loop_enter

{{L}}store:
    mov     rcx,    [rdi + 16]
    mov     rsi,    [rcx]

    cmp     rsi,  0
    je      {{L}}store_strides
    cmp     rsi,  3
    je      {{L}}store_vec_strides
    mov     rax, 1
    jmp     {{L}}return

{{L}}store_strides:

    mov     r8,     [rcx + 8]           // c ptr
    mov     rsi,    [rcx + 16]          // row stride
    mov     rbx,    [rcx + 24]          // col stride

    // tops of cols
    lea     r9,     [ r8 + rbx ]
    lea     r10,    [ r8 + 2 * rbx ]
    lea     r12,    [ r8 + 4 * rbx ]
    lea     r11,    [ r10 + rbx ]
    lea     r13,    [ r12 + rbx ]

    {% for quarter in (0..3) %}
        {% if quarter != 0 %}
            // move next four rows at top (xmm0,2,..10)
            vperm2f128  ymm0,   ymm0,   ymm1,  {{quarter}}
            vperm2f128  ymm2,   ymm2,   ymm3,  {{quarter}}
            vperm2f128  ymm4,   ymm4,   ymm5,  {{quarter}}
            vperm2f128  ymm6,   ymm6,   ymm7,  {{quarter}}
            vperm2f128  ymm8,   ymm8,   ymm9,  {{quarter}}
            vperm2f128  ymm10,  ymm10,  ymm11, {{quarter}}
        {% endif %}
        {% for row in (0..3) %}
            {% for i in (0..5) %}
                vextractps  dword ptr [r{{i | plus: 8}}], xmm{{i | times:2}}, {{row}}
                add         r{{i | plus: 8}}, rsi
            {% endfor %}
        {% endfor %}
    {% endfor %}

    mov     rax,    0
    jmp     {{L}}return

{{L}}store_vec_strides:

    mov     r8,     [rcx + 8]           // c ptr
    mov     rsi,    [rcx + 16]          // stride

    {% for quarter in (0..3) %}
        {% if quarter != 0 %}
            // move next four rows at top (xmm0,2,..10)
            vperm2f128  ymm0,   ymm0,   ymm1,  {{quarter}}
        {% endif %}
        {% for row in (0..3) %}
            vextractps  dword ptr [r8], xmm0, {{row}}
            add         r8, rsi
        {% endfor %}
    {% endfor %}

    mov     rax,    0

{{L}}return:
    ldmxcsr     [rsp + 4]
    add         rsp, 8

    pop r15
    pop r14
    pop r13
    pop r12
    pop rbx

{% if family == "windows" %}
    pop rsi
    pop rdi

    vmovaps xmm15, dword ptr [rsp+16*9]
    vmovaps xmm14, dword ptr [rsp+16*8]
    vmovaps xmm13, dword ptr [rsp+16*7]
    vmovaps xmm12, dword ptr [rsp+16*6]
    vmovaps xmm11, dword ptr [rsp+16*5]
    vmovaps xmm10, dword ptr [rsp+16*4]
    vmovaps xmm9, dword ptr [rsp+16*3]
    vmovaps xmm8, dword ptr [rsp+16*2]
    vmovaps xmm7, dword ptr [rsp+16*1]
    vmovaps xmm6, dword ptr [rsp]
{% endif %}

    mov rsp, rbp
    pop rbp
    ret

{{L}}unimplemented:
    mov     rax,    1
    jmp     {{L}}return

// NON LINEAR LOOP

{{L}}non_linear_loop_enter:
    sub     rcx,    16
{{L}}non_linear_loop:
    add     rcx,    16
    mov     rax,    [rcx]

    cmp     rax,    0
    je      {{L}}store

    cmp     rax,    1
    je      {{L}}min

    cmp     rax,    2
    je      {{L}}max

    cmp     rax,    3
    je      {{L}}non_linear_addc

    cmp     rax,    4
    je      {{L}}per_row_mul

    cmp     rax,    5
    je      {{L}}per_row_add

    cmp     rax,    6
    je      {{L}}per_col_mul

    cmp     rax,    7
    je      {{L}}per_col_add

    jmp     {{L}}unimplemented

// NON LINEAR / ADDC

{{L}}non_linear_addc:
    mov     rax,    [rdi + 16]

    // FIXME: assume Strides storage
    mov     r10,    [rax + 8]           // c ptr
    mov     rsi,    [rax + 16]          // row stride
    mov     rbx,    [rax + 24]          // col stride

    mov     eax,    0
{% for i in (0..3) %}
    pinsrd  xmm14, eax, {{i}}
    add     eax,    esi
{% endfor %}
{% for i in (0..3) %}
    pinsrd  xmm15, eax, {{i}}
    add     eax,    esi
{% endfor %}

    vperm2f128      ymm14,  ymm14, ymm15,         32 // ymm14 <- xmm14::xmm15

    lea             r8, [ r10 + rsi * 8 ]

{% for i in (0..5) %}
    vpcmpeqd        ymm15,  ymm15, ymm15
    vgatherdps      ymm12,  [ r10 + ymm14 ],      ymm15
    vpcmpeqd        ymm15,  ymm15, ymm15
    vgatherdps      ymm13,  [ r8  + ymm14 ],      ymm15
    add     r10, rbx
    add     r8, rbx
    vaddps          ymm{{i | times:2 }},   ymm{{i | times:2}},   ymm12
    vaddps          ymm{{i | times:2 | plus: 1}}, ymm{{i | times:2 | plus:1 }},   ymm13
{% endfor %}

    jmp    {{L}}non_linear_loop

// NON LINEAR / MAX

{{L}}max:
    vbroadcastss    ymm12, dword ptr [rcx + 8]
{% for i in (0..11) %}
    vmaxps          ymm{{i}}, ymm{{i}}, ymm12
{% endfor %}
    jmp    {{L}}non_linear_loop

// NON LINEAR / MIN

{{L}}min:
    vbroadcastss    ymm12, dword ptr [rcx + 8]
{% for i in (0..11) %}
    vminps          ymm{{i}}, ymm{{i}}, ymm12
{% endfor %}
    jmp    {{L}}non_linear_loop

// NON LINEAR / PER ROW MUL

{{L}}per_row_mul:
    mov             rax, [ rcx + 8 ]

    vmovups         ymm12,  [rax]
    vmovups         ymm13,  [rax + 32]

{% for i in (0..5) %}
    vmulps          ymm{{i|times:2}}, ymm{{i|times:2}}, ymm12
    vmulps          ymm{{i|times:2|plus:1}}, ymm{{i|times:2|plus:1}}, ymm13
{% endfor %}

    jmp    {{L}}non_linear_loop

// NON LINEAR / PER ROW ADD

{{L}}per_row_add:
    mov             rax, [ rcx + 8 ]

    vmovups         ymm12,  [rax]
    vmovups         ymm13,  [rax + 32]

{% for i in (0..5) %}
    vaddps          ymm{{i|times:2}}, ymm{{i|times:2}}, ymm12
    vaddps          ymm{{i|times:2|plus:1}}, ymm{{i|times:2|plus:1}}, ymm13
{% endfor %}

    jmp    {{L}}non_linear_loop

// NON LINEAR / PER COL MUL

{{L}}per_col_mul:
    mov             rax, [ rcx + 8 ]

{% for i in (0..5) %}
    vbroadcastss    ymm12, dword ptr [rax + {{i|times:4}}]
    vmulps          ymm{{i|times:2}}, ymm{{i|times:2}}, ymm12
    vmulps          ymm{{i|times:2|plus:1}}, ymm{{i|times:2|plus:1}}, ymm12
{% endfor %}

    jmp    {{L}}non_linear_loop

// NON LINEAR / PER COL ADD

{{L}}per_col_add:
    mov             rax, [ rcx + 8 ]

{% for i in (0..5) %}
    vbroadcastss    ymm12, dword ptr [rax + {{i|times:4}}]
    vaddps          ymm{{i|times:2}}, ymm{{i|times:2}}, ymm12
    vaddps          ymm{{i|times:2|plus:1}}, ymm{{i|times:2|plus:1}}, ymm12
{% endfor %}

    jmp    {{L}}non_linear_loop


{% if family == "windows" %}
fma_smmm16x6 endp
_text ends
end
{% endif %}
